qid	question	answer	context	document_ids	question_factuality	question_premise	question_phrasing	question_linguistic_variation	user_expertise-categorization	generation_timestamp	question_length	answer_length	context_length
1	what defines behavioral asset pricing model difference from traditional model reading economics paper	The basic difference between behavioral and traditional asset-pricing models lies in their assumptions about agents' preferences, particularly those of a hypothetical representative agent. Behavioral models depart from the assumption of time-stationary expected utility (Savage rationality) and may incorporate concepts like loss aversion, hyperbolic discounting, or state-dependent utility functions.	"[""آیا مدل های رفتاری قیمت گذاری دارایی، ساختاری است؟\n|کد مقاله||سال انتشار||مقاله انگلیسی||ترجمه فارسی||تعداد کلمات|\n|10775||2002||14 صفحه PDF||سفارش دهید||5988 کلمه|\nPublisher : Elsevier - Science Direct (الزویر - ساینس دایرکت)\nJournal : Journal of Monetary Economics, Volume 49, Issue 1, January 2002, Pages 215–228\nThe recent increase in interest in so-called behavioral models of asset-pricing is motivated partly by the desire to have models that appear realistic in light of experimental evidence, and partly by their success in moment-matching exercises. This paper argues that the attention given to these two criteria misses perhaps the most important aspect of the modeling exercises. That is, the search for parameters that are invariant to changes in the economic environment. It is precisely this invariance that motivates the use of a tightly parameterized general equilibrium model. Assessing a model on this dimension is difficult and, as the paper argues through the use of suggestive examples, will undoubtedly require strong subjective judgments about the reasonableness of preference assumptions. Such judgments are routinely made about the reasonableness of assumptions about stochastic endowments. The paper suggests that more effort be applied to understanding aggregation in these models and to the exploration of behavioral assumptions in a less flexible but less corruptible time-stationary recursive class of preferences.\nThe recent successes of behavioral asset-pricing models provide new hope for the quantitative research program started by Mehra and Prescott (1985) following the theoretical work of Lucas (1978). That is, there is a renewed interest in the ability of a tightly parameterized, representative-agent, general-equilibrium model to explain the salient features of historical asset-market data (e.g., large equity premium, excess volatility, etc.). What makes an asset-pricing model “behavioral” can itself be the subject of debate. For the purposes of this paper, I will lump all asset-pricing models that endow agents with preferences that do not adhere to the assumption of time-stationary expected utility (i.e., “Savage rationality”), into the category of “behavioral”. Many of these preference assumptions are directly motivated by evidence from experimental psychology and behavioral decision theory, e.g., loss aversion (Epstein and Zin, 1990; Benartzi and Thaler, 1995; Barberis et al., 1999), or hyperbolic discounting (Luttmer and Mariotti, 2000; Krusell and Smith, 2000). Also falling within this broad definition, however, are models that may depart from classical assumption by allowing for state-dependent utility functions, but that are less formally motivated by behavioral evidence, e.g., habit formation (Abel, 1990; Constantinides, 1990; Campbell and Cochrane, 1995; Wachter, 2001). These examples are suggestive and are in no way an exhaustive list of behavioral asset-pricing models. Indeed as more experimental evidence filters into economics from various fields of psychology, this list continues to grow at a rapid rate. This paper takes a sympathetic view of these recent behavioral approaches and tries to identify what these models have yet to accomplish before they can claim success and presumably supplant more traditional approaches. Particular attention is paid to the need for structural models, and whether behavioral models are more or less likely to achieve the sort of “deep structural excavation” called for by the rational expectations revolution in dynamic macroeconomics. The methodological guidelines laid out by Friedman and Lucas in the two quotes above, cast a very different light on the current debate about the usefulness of behavioral versus more traditional models of asset markets, than what one might hear in academic circles and even in the popular press. From this perspective, traditional models cannot be viewed as inherently better because of their reliance on well-understood Savage rationality. Likewise, behavioral models cannot claim superiority simply based on experimental evidence of individual departures from this definition of rationality, and the sense of modeling realism that this evidence invokes. If this debate can be settled, then following Lucas’ advice, it can only be settled by determining which approach to building a mechanical, imitation economy provides “better imitations” of real asset markets. It is now common for behavioral models to adopt the dynamic stochastic general equilibrium approach of Lucas (1978) and Mehra and Prescott (1985) as a framework for understanding the consequences of alternative behavioral assumptions on observables in asset markets. In other words, the basic difference between the two approaches can be thought of as differences in assumptions about agents’ preferences — most often a hypothetical representative agent. Therefore, the common use of dynamic general equilibrium endowment economies by both approaches provides a common framework for comparison. The use of these general equilibrium models highlights an implicit desire to obtain structural models. In this case, “structural” is used to differentiate between purely statistical descriptions of empirical evidence (which may or may not use economic theory to suggest functional forms and factors), from models that derive their empirical predictions directly from the structure of a parametric version of an economic theory. The obvious implication being that like more traditional models, behavioral models can potentially provide useful guidance for understanding the likely consequences of changes in the economic environment. That is, they can be used to make forecasts about situations in which we have no (or at least very little) historical evidence. If that was not the desire of behavioralists in finance, then behavioral arguments could be safely relegated to a relatively minor role in the design and interpretation of reduced-form econometric models. Predicting responses to changes in the economic environment, e.g., changes in government policy, therefore, are precisely the “particular questions” for which we seek “better imitations”, using Lucas’ words, or the “sufficiently accurate predictions”, using Friedman's words. The primary reason for using tightly parameterized general equilibrium models to characterize asset-market outcomes is precisely the need for identifying policy-invariant structural parameters. Behavioral models have an obvious and important advantage over traditional models: their parameters can be calibrated so that various moments of particular interest of the distribution of asset prices generated by these models, will closely match their sample counterparts in historical data. That is, they do a better job imitating the large equity premiums, volatility, and persistent dynamics that are so puzzling from the perspective of traditional models. Clearly, this data-fitting exercise seems like a necessary condition for evaluating the usefulness of any equilibrium model. We would have little confidence in any model's ability to forecast outcomes in new environments when it is incapable of forecasting in the current environment. These conclusions, however, require a fair bit of judgment on the part of the modeler. Section 2 outlines judgments that modeler's working in this area typically make about the reasonableness of assumptions about the stochastic properties of exogenous variables. A simple example demonstrates that poorly fitting Savage-rational models can be made to fit empirical evidence by making assumptions about higher-order moments of the distribution of endowments. Most people would object to this strategy, however, since these assumptions might not seem reasonable given their prior beliefs. Where these beliefs come from is unspecified. They may derive from beliefs about the deeper micro-foundations of production, or they may derive from personal experience, or they may be purely whimsical. Direct statistical evidence about these higher-order-moment assumptions is likely to be misleading, or at best inconclusive, so we are left with non-sample-based judgments about the reasonableness of these assumptions. Section 3 of the paper looks at a similar sort of reasonableness criterion for preference assumptions. Unfortunately, unlike the case of assumptions about endowments, we are not yet at the stage where there is a consensus about what types of preference assumptions are unreasonable. The examples in Section 3 demonstrate that virtually any well-fitting, reduced-form empirical model of asset pricing can be incorporated into the representative agent's preferences so that a purely statistical model could be viewed as the outcome of what one could claim to be a structural general equilibrium model. Naturally, there is no guarantee that a model constructed in this way will have parameters that are invariant to the types of structural change for which these models must provide guidance. Analogous to the case for reasonableness of endowments, the modeler is forced to make judgments about the reasonableness of the preference assumptions. It is clear that fitting historical data is not a sufficient criterion for determining the usefulness of particular preference assumptions for delivering a structurally stable model. Likewise, without some explicit aggregation results, individual-level experimental evidence will also be insufficient. The main conclusion of this paper is straightforward: the parameters of asset-pricing models including behavioral models must be invariant to changes in the economic environment. This is not a very original conclusion and it is not likely to be very controversial. What is controversial and quite difficult, is assessing whether this goal has been met. Econometric testing for structural stability is notoriously problematic, especially in small samples. Moreover if the type of structural change under investigation has no historical counterpart, then purely statistical testing will be uninformative. The examples in 2 and 3 of the paper suggest that any assessment of the structural stability of a model will require the use of both non-sample information and the researcher's judgment. Accounting for historical evidence is not enough. The researcher is forced to evaluate the reasonableness of the assumptions on preferences and technologies under which the asset-pricing model can both account for historical evidence and maintain its basic structure in the face of significant changes in the economic environment. If aggregation results are available, then preferences of the representative agent that appear unreasonable given individual-level evidence would certainly be a cause for concern. These results also suggest that maintaining time-consistency as a basic axiom for preferences, and avoiding the introduction of arbitrary state variables in the utility function, will help eliminate much of the scope for well-fitting but non-structural empirical asset-pricing models to pass as deeper structural models.\nنتیجه گیری انگلیسی\nThe purpose of trying to characterize asset-market data using a tightly parameterized, representative-agent, general-equilibrium model is to try to uncover deep structural parameters. This is equally true of behavioral models as it is of more traditionally expected utility models. This is not a simple task and, as the examples in this paper suggest, fitting historical data is not sufficient to insure structural stability. Evaluating a model in this dimension will always require an element of subjective judgment of the reasonableness of the assumptions of the model. This is perhaps even more true of behavioral asset-pricing models than more traditionally expected utility models. The reason is that behavioral evidence may suggest the inclusion of state variables in the utility function and the relaxation of the stationarity assumption of intertemporal preferences. This exposes these models to the risk of being reverse engineered to fit the data, without serious consideration of whether the parameters of the model can be deemed structural. Econometric testing for structural stability is likely to be problematic, especially in small samples. In addition, statistical test will be uninformative if the types of change to the economic environment being contemplated has no natural analog in historical experience. A better understanding of how behavioral models aggregate provides some hope for reaching a consensus about reasonableness, since this will allow inference about the representative agent's preferences based on individual-level experimental evidence. Finally, since the use of behavioral models often opens the door for claims of reverse engineering, it seems prudent to work harder to avoid these criticisms. Maintaining assumptions on recursivity and time-stationarity of intertemporal preferences, while incorporating behavioral concepts is both feasible, as shown in Epstein and Zin (1990), and desirable given the discipline that this will naturally enforce on the modeling exercise.""]"	['<urn:uuid:417fbc69-80f9-4b53-8b5c-1c9cdd2bb0e9>']	factoid	with-premise	long-search-query	similar-to-document	novice	2025-04-22T14:43:01.810270	13	48	1901
2	As an architectural photographer, I notice my building shots often have exposure issues with bright skies. What's the best way to handle this common problem with auto settings?	When the camera automatically exposes for the sky, it can leave the building too dark. To solve this, you have two options: if your camera has exposure adjustment, try increasing the exposure by about one stop and take another shot. If that doesn't resolve the issue, try taking the picture from a different angle while minimizing the amount of sky in the frame.	"['Learning patterns/Improving your building photography/ja\nWhat problem does this solve?\nYour photos of buildings don\'t seem to come out as well as you\'d hoped. Is it your camera? Is it the way you take the photos?\nWhat\'s the solution?\nHere\'s the good news: the quality of those photos can be greatly improved if you keep in mind a few simple tips. You don\'t need an expensive camera to take good shots – just a little knowledge and care when taking the image.\nよくある失敗 - カメラの構え方\nTake care to keep the camera straight. Spending ten seconds on composition (how the visual elements are arranged in the photo) and alignment can greatly improve your picture quality. And in spite of what you may see others doing, always hold your camera or your mobile phone in both hands when composing and taking the picture.\nCamera tilt is the single biggest problem with mobile phone uploads, and it\'s worth repeating: always hold your camera or your mobile phone in both hands when composing and taking the picture.\nWhile there are applications for later editing pictures by rotating them, it\'s much better to get it right in the first place.\nBuilding partly obscured\nTry to avoid objects that obstruct the view of your subject. Choose a different angle, or (if the obstruction – especially parked cars – might go away) come back later. If you absolutely can\'t avoid cars, for privacy reasons ensure that numberplates are not visible (the number plates had to be blurred out to display this image).\nIf your camera creates a timestamp watermark, please turn off that feature beforehand.\nDon\'t chop off the top of the building or, as some people like to say: ""make sure you get the cross on top of the steeple"". It can ruin a good picture if the viewer expects to see something that has been chopped off.\nThink about how far the edges of the building are from the frame. If the entire building won\'t fit in the frame, walk back if you can, or try a different angle. If there\'s nowhere from which you can take a good picture, concentrate on something else instead, such as some of the smaller architectural details.\nDon\'t chop off the bottom of the building unless it\'s unavoidable. It\'s easy to forget the bottom when you point your camera up to get the top of the building.\nIf the entire building won\'t fit in the frame, move back if you can, or try a different angle. If there\'s nowhere from which you can take a good picture, concentrate on something else instead such as some of the smaller architectural details.\nAvoid pointing the camera too close to the sun. The amount of flare in this photo makes the picture unusable, even though the sun itself is outside the frame.\nTo avoid camera movement, especially in dark church interiors, use the font or a pillar as a support. Blurred images such as this are of little use. Even outside, occasionally we\'re lucky enough to have something solid like a bench or fence conveniently located; if so, try resting your camera on it for greater stability, and see if the composition and alignment work from that angle.\nThis picture seems to have been taken one-handed; remember the rule about always holding your camera in two hands? Notice the annoying chopped-off candle holder on the left; again, think about each element in relation to the edges of your photo.\nCommon errors – auto settings\nThe camera has automatically exposed the sky correctly, but that leaves the building much too dark. If your camera has exposure adjustment, increase the exposure by about one stop and try again. If that doesn\'t solve the problem, take the picture from another angle and try to keep the amount of sky to a minimum.\nOverexposure like this is not very common on modern cameras. If you see it, check your camera settings: the ""exposure compensation"" may have been set incorrectly.\nCheck what your camera is focusing on\nHere, the camera autofocus has latched on to the lamp to the left, rather than the building.\nCommon errors – subjects\nPlease don\'t take photos of signs\nPlease don\'t upload photographs of posters, noticeboards, signs, modern murals, or anything else having text or two-dimensional images that might be copyright-protected. That applies even to text or images in a public place – even if everyone else is taking pictures of them. Ancient wall paintings in churches are fine: they have no copyright protection.\nHere, the copyright-protected text and images have been blurred out. It\'s OK if your photo of a building incidentally includes an unobtrusive sign that appears small in the final image, but if the sign appears large or prominent, please take the photo from another angle.\nNo posing please\nPlease don\'t upload images of people posing, even if there\'s a nice building in the background!\nThat looks a lot better!\nThe signs are small in the composition and don\'t cause a copyright problem. Maybe you can do even better than this?\nWhen to use\n- Individually uploaded photos of local buildings\n- Wiki Loves Monuments, \'Wiki Takes ...\' events and \'Wiki Scavenger Hunts\'\n- Compositional techniques, in the English Wikipedia.\n- ""Picture composition"" in the Danish Wikipedia (Google translation into English).\n- ""Composition"" in the Finnish Wikipedia, which goes into greater detail concerning lines and directionality (Google translation into English).']"	['<urn:uuid:05ff742a-91b3-47fb-a038-253dc0b9430c>']	open-ended	with-premise	verbose-and-natural	similar-to-document	expert	2025-04-22T14:43:01.810270	28	63	901
3	what methods does phoenix youth performing arts use respect hip hop authenticity	Rising Youth Theatre in Phoenix ensures hip-hop authenticity through several key methods: They partner with CYPHERS the Center for Urban Arts, which provides expertise in the five elements of hip-hop (B-boying, MC-ing, DJ-ing, Graffiti Art, and Knowledge). They involve the hip-hop community directly through story circles and require community approval of scripts. They perform in public spaces creating a 'Block Party' setting that pays homage to original hip-hop street battles from the 1970s Bronx. Their dramaturgy involves experts from multiple hip-hop disciplines and focuses on all elements of hip-hop culture rather than using it merely as a trendy addition.	['Respecting Hip Hop in Rising Youth Theatre’s production of antonia\na latina hip-hop Antigone\n“Rap is something you do, Hip-Hop is something you live.”—KRS-One\nThe mission of Rising Youth Theatre is to create youth driven theatre that is not only riveting and relevant, but also challenges audiences to hear new stories, start conversations, and participate in their communities. Heading into their fifth year anniversary, Co-Artistic Directors Xanthia Angel Walker and Sarah Sullivan have successfully promoted these ideals with a number of diverse communities throughout Phoenix, Arizona.\nRising Youth’s programming has touched on several important issues affecting today’s youth, such as high school dropout rates, juvenile incarceration, the foster care system, and immigration. For the first show of its 2015/2016 season, the organization reached out to the youth of the hip-hop community. With every show Rising Youth produces, the creative team engages with youth in the community through story circle interviews that focus on the community’s relationship to the issue being explored. From this interaction the playwright involved creates a play. For the first show of the season, the resulting project was an adaptation of a Sophocles’ play titled antonia: a latina hip-hop Antigone. Both the playwright and director felt that the themes in the play, such as power, fate vs. free will, rules vs. order, and feminism vs. gender, were relatable to hip-hop culture and the lives of the youth participating in the project.\nThe success of a TYA hip-hop performance is dependent on a communal form of dramaturgy that is similar to hip-hop’s cypher, or artistic gathering.\nIn the process of creating antonia, Rising Youth has made a conscious effort to respect hip-hop culture by avoiding the appropriation of hip-hop, which is present in numerous productions of theatre for young audience (TYA) across the United States. This cultural appropriation has manifested in hip-hop theatre, lending itself to bad and disingenuous theatrical experiences. The danger of unauthentic hip-hop theatre is that it is a false reflection of what hip-hop is and works towards keeping the culture marginalized. Expanding on this sentiment, Rising Youth’s goal with antonia is to create theatre that explores hip-hop and its origins. Director Xanthia Angel Walker states, “In the field of TYA, I have observed ‘hip-hop’ being inserted into plays as a trendy way of being ‘accessible’ to young people. There is little to no regard for the elements and principles of hip-hop that serve as a powerful community building tool and emancipatory force. This force is often disregarded and disrespected in ways that continually feed the dominant narrative of the Caucasian, middle class youth experience.”\nWith this production of antonia, Rising Youth decided to perform the piece in a public space, which promotes the idea of making theatre accessible to those who have little to no knowledge of theatre and/or hip-hop by creating a gathering space where audiences participate in a “Block Party” setting. As a result, Rising Youth breaks away from the traditional proscenium stage, while paying homage to the origins of hip-hop culture by mimicking the “battles” that occurred on the streets in the early 1970’s in The Boogie Down Bronx. Therefore, creating a community space where ritual becomes the driving force. The audience does not only watch a play, but also participates in an experience.\nI recommend that TYA theatres take note of Rising Youth’s process to mitigate the risks of creating unauthentic hip-hop theatre. I believe antonia models how to effectively and ethically create and produce hip-hop theatre. Three factors needed for producing hip-hop theatre are partnerships, dramaturgy, and community.\nFor antonia, Rising Youth is partnering with CYPHERS the Center for Urban Arts, founded by Danny “Skooby” Morales. This organization is dedicated to educating youth about the five elements of hip-hop: B-boying, MC-ing, DJ-ing, Graffiti Art, and Knowledge. This relationship was created with the understanding that CYPHERS offers an expertise of hip-hop culture that Rising Youth lacks. This partnership also ensures that the interests of youth participants are protected. Furthermore, the goal is to establish a common vocabulary that will allow both partners to benefit from the collaboration in a meaningful fashion, while educating each other in their respective mediums. TYA theatres wishing to produce hip-hop theatre should seek out partnerships with organizations that promote and maintain hip-hop culture. Without these types of partnerships, the risk of hip-hop culture being exploited on stage increases.\nIn the field of TYA, I have observed ‘hip-hop’ being inserted into plays as a trendy way of being ‘accessible’ to young people. There is little to no regard for the elements and principles of hip-hop that serve as a powerful community building tool and emancipatory force. —Xanthia Angel Walker\nThe approach to dramaturgy is another important factor in the production of hip-hop theatre. In antonia, Rising Youth focuses on how the themes in the original source material connect with the elements of hip-hop. The creative team includes a dramaturg and experts from multiple disciplines within the hip-hop community. TYA theatres must focus on dramaturgy that considers the interdisciplinary nature of hip-hop and its improvisational foundation. Dramaturgy should not focus on one element, but rather all elements of hip-hop. This essentially calls for a creative collaboration and exchange of ideas that goes beyond the formal director-dramaturg relationship. The success of a TYA hip-hop performance is dependent on a communal form of dramaturgy that is similar to hip-hop’s cypher, or artistic gathering.\nLastly, acknowledging the importance of community members is key in the process because the hip-hop community continually gets misrepresented. This directive serves the production of antonia quite well, but should be a goal for any TYA theatre wishing to reach wider audiences like traditionally marginalized groups. Rising Youth holds itself accountable to the community-at-large. It does not produce a script of which community participants do not approve. This theatre believes that the community of youth will directly shape the content of its work. Rising Youth hopes to create art that converses with community members, instead of talking down to them. In regards to hip-hop culture, TYA theatres must evaluate how they approach this culturally sensitive subject. TYA theatres must also acknowledge that they have a moral obligation to do so.']	['<urn:uuid:9dceb9b3-ea9b-4ccc-b3f5-a5c828007a62>']	open-ended	direct	long-search-query	distant-from-document	novice	2025-04-22T14:43:01.810270	12	99	1024
4	need help when can person legally die peacefully oregon first usa state allow	Oregon became the first state in the United States with an open practice of aid in dying (AID) in 1998. This occurred after Oregon voters adopted the first AID statute in 1994, though implementation was delayed for several years due to a lawsuit challenging its validity.	"['California adopted the End of Life Option Act (EOLOA) in October 2015, establishing statutory permission for aid in dying (AID), the medical practice in which a physician provides a mentally competent terminally ill patient with a prescription for medication that the patient may ingest to achieve a peaceful death. California was the fourth state to adopt such a statute, following Oregon, Washington and Vermont. Recently, Colorado and the District of Columbia adopted similar measures. All impose a number of burdens and restrictions on accessing AID.\nThe first state to vote on a measure intended to empower terminally ill patients with physician-assisted dying was Washington in 1991. California unsuccessfully tried a similar measure the next year. Opponents argued that there were insufficient ""safeguards"". Voters were swayed by this concern, despite strong public support for AID. Learning from these attempts, Oregonians embraced a \'throw in the kitchen sink\' approach and put forth a measure thick with ""safeguards."" In 1994 Oregon voters adopted the first AID statute in the United States. Implementation of Oregon’s law was delayed for several years, until a lawsuit challenging its validity was resolved late in 1997. Beginning in 1998 Oregon became the first state with an open practice of AID.\nIn 2015 California legislators adopted an Oregon-style AID measure, though slightly more restrictive. While in some ways the EOLOA can be considered a step forward, in others it represents a step backwards. The ""step forward"" perspective recognizes that California is the largest, most populous and most demographically diverse state to adopt an AID law. The ""step backward"" can be seen in the fact the EOLOA imposes all of the burdens and limitations as does the original Oregon Measure and goes beyond in imposing both an additional restriction and a sunset provision.\nOregon’s nearly 20 years experience with AID has been closely watched and studied. The State annually reports on implementation of the program, and a plethora of articles examining and discussing the data have been published. The consensus has emerged that availability of AID improves end-of-life care and presents no risk.\nAccordingly, it would be appropriate for the practice to become more normalized within the practice of medicine, with less government oversight and regulation. The regulation of the practice of medicine is not commonly governed by statute; rather it is primarily governed by professional practice standards as they evolve in practice; in other words—by standard of care. However, the EOLOA is ""Oregon Plus"": imposing all of the intrusive, burdensome government involvement into medical practice that the Oregon statute requires and adding some additional burden and limitation. The obstruction faced by patients is apparent in accounts of families of patients who have attempted to run the gauntlet of requirements in order for the dying patient to achieve a more peaceful death through aid in dying.\nPrior to enactment of the EOLOA, two cases had been filed in California seeking clarification of the reach of California’s statute that makes a crime of ""assisting"" a ""suicide."" Brody v. Harris, Donorovich-O\'Donnell v. Harris. Plaintiffs, patients and physicians, argued that this law should not apply to physicians providing AID because the choice of a dying patient for a peaceful death is not “suicide”; alternatively, plaintiffs argued that the choice at issue deserved protection under California\'s constitutional guarantees of privacy and equal protection. Adoption of the EOLOA mooted the statutory scope claim, as it made clear that physicians who provide aid in dying cannot be prosecuted for ‘assisted suicide.’\nNeither of these cases reached the California Supreme Court. Hence the question of whether state constitutional protection extends to the choice of a dying patient for a more peaceful death through AID has not been resolved. It is this commentator\'s view that the Donorovich court applied law incorrectly and its opinion ought not be persuasive to the California Supreme Court.\nCalifornia courts have repeatedly held that the scope of California\'s constitutional right of personal autonomy privacy is greater than that protected by the federal Constitution. California’s privacy jurisprudence has often been ahead of that of the SCOTUS. California\'s Supreme Court recognized “[t]he fundamental right of [a] woman to choose whether to bear children” in invalidating its ban on abortion four years before the SCOTUS recognized this right in Roe v. Wade.\nJurisprudence concerning reproductive freedom may be especially relevant to AID. California decisions have held the right of a woman to control her body and to choose to terminate a previable pregnancy as ""clearly among the most intimate and fundamental of all constitutional rights."" The right of a woman to control her body has been grounded in the right of privacy. The decision of a dying patient about how much suffering to endure prior to death is of a similar profoundly personal nature, concerning one’s own body, medical treatment, and life course. Because the only life at issue in AID is that of the dying patient, the greatest complexity present in the abortion context—the potential life of the fetus—is absent. Respecting this decision does not result in a reality where doing so ends a potential life with no voice in the decision. Principles that compel respect for a woman’s decision to control her body through abortion apply with equal force to the decision of an individual regarding how much suffering to endure in the final throes of terminal illness, and hence whether to choose AID.\nUnder this approach, the California Supreme Court ought find a privacy or equal protection interest in choosing AID, and hold legislation constraining exercise of this choice to the standard articulated in American Academy of Pediatrics. The State would be required to prove that it has a compelling interest at stake, that it achieves its interest in the least intrusive way, and that the restrictions are narrowly drawn to impinge upon the constitutionally protected area no more than is necessary. It is unlikely that several of the EOLOA provisions could meet this exacting test, including the fifteen-day waiting period. The state might argue that it has an interest in ensuring that a patient’s choice of AID is carefully considered and enduring, and that this interest is served by the waiting period. In the reproductive rights arena at the federal level waiting periods of much shorter duration have been allowed—most commonly twenty-four hours. And, notably, a medical emergency exception to mandatory waiting periods are included, which the EOLOA lacks.\nIn Planned Parenthood v. Casey, the court upheld a state’s twenty-four hour abortion waiting period stating that “only where state regulation imposes an undue burden on a woman\'s ability to make this decision does the power of the State reach into the heart of the liberty protected by the Due Process Clause.” The court stated that an undue burden exists if “a state regulation has the purpose or effect of placing a substantial obstacle in the path of a woman seeking an abortion of a nonviable fetus.” The Court elaborated on the “substantial obstacle” test, stating “the means chosen by the State to further the interest in potential life must be calculated to inform the woman\'s free choice, not hinder it.”\nThe fact that the statute contained an emergency exception to the waiting period was significant, as was absence of findings that a twenty-four hour waiting period created “appreciable health risks.” The EOLOA’s failure to include an emergency exception renders it vulnerable, and certainly imposing a wait period causes the patient harm, as additional suffering must be endured or, even worse, sometimes the patient dies exactly the sort of death hoped to be avoided during the long waiting period.\nA state might assert an interest in ensuring that a patient choosing AID is not acting impulsively and that this interest is served by a waiting period —though why, specifically, the state has an interest in this is unclear. Even if the state establishes an interest, it could be served by a much shorter period. Patients choosing aid in dying do so because they find themselves in an unbearable dying process, where the cumulative burden of suffering imposed by their end stage terminal illness is overwhelming. Forcing them to suffer longer, and possibly be forever denied the right to access aid in dying, seems unjustifiable.\nIf the California Supreme Court reviews restrictions imposed by the EOLOA under a strict scrutiny standard, its jurisprudential history suggests it would almost certainly invalidate a fifteen-day waiting period, and likely other of the EOLOA’s burdensome restrictions. Under the strict scrutiny standard laid out in Roe v. Wade, prior to the undue burden standard expressed in Casey, the court recognized only two state interests as sufficiently compelling to justify governmental restrictions on abortion: the state\'s interests “in preserving and protecting the health of the pregnant woman” and “in protecting the potentiality of human life.” Neither of these sorts of interests are addressed by the restrictions on AID. There is no “health” to protect, as terminal illness by definition destroys health, and there is no potential life interest at stake. Under a strict scrutiny standard, AID regulations would not survive. Even if the more permissive undue burden analysis were to be applied, the fifteen-day waiting period is likely to fail.\nOther elements of the EOLOA may not survive scrutiny: requiring patients to make a written request in addition to the two oral requests, and that they see a consulting physician to confirm diagnosis, prognosis, and competence, delay access to AID. Delays always force additional unwanted suffering and sometimes prevent patients from access entirely, given the progressive nature of their illnesses.\nThe restrictions imposed on AID by the EOLOA will likely be challenged and ultimately examined by the California Supreme Court, which ought find this choice to be protected by privacy and/or equal protection guarantees under the California constitution, and that various restrictions contained in the EOLOA do not survive constitutional scrutiny. Those who believe it is the individual who ought be vested with autonomy to make their own informed decisions about profoundly personal matters involving their own body, life course and medical treatment will applaud such an outcome.\nKathryn L. Tucker, JD, is Executive Director of the End of Life Liberty Project (ELLP), which she founded during her tenure as Executive Director of the Disability Rights Legal Center, the nation\'s oldest disability rights advocacy organization. She has held faculty appointments at Loyola, the University of Washington, Seattle University and Lewis & Clark Schools of Law, teaching in the areas of law, medicine and ethics, with a focus on the end of life. Tucker engages multidimensional advocacy to protect and expand the rights of terminally ill patients nationwide.\nSuggested citation: Kathryn L. Tucker, End of Life Liberty in CA, JURIST - Professional Commentary, Feb. 21, 2017, http://jurist.org/professional/2016/12/kathryn-tucker-end-of-life-liberty-in-california.php.\nThis article was prepared for publication by Henna Bagga, an Assistant Editor for JURIST Commentary. Please direct any questions or comments to her at']"	['<urn:uuid:bed58ebf-446e-4813-a0e4-d96ee1b3b3da>']	factoid	with-premise	long-search-query	distant-from-document	novice	2025-04-22T14:43:01.810270	13	46	1800
5	I heard that gastroenteritis is caused by viruses - can you tell me which specific viruses are responsible for causing it?	Viral gastroenteritis can be caused by several viruses including rotaviruses, toroviruses, adenoviruses, caliciviruses, and astroviruses. Rotavirus is a major cause of gastroenteritis worldwide, while norovirus is a major cause in the United States. About 75% to 90% of gastroenteritis cases are caused by viruses, with the remaining 5% caused by parasites.	"[""Gastroenteritis is one of the serious illnesses that have a significant impact on infants and babies. It occurs when the stomach and intestines are inflamed. In the case of viral gastroenteritis, inflammation is caused by infection with a virus, often accompanied by vomiting or diarrhea. Learn about infantile gastroenteritis.\nGastroenteritis is an infectious disease that affects the gastrointestinal tract. It occurs when pathogens attack the gastrointestinal tract and cause inflammation of the digestive system. This causes the gastrointestinal dysfunction leading to gastroenteritis.\nInfection, especially from the stomach to the colon, can affect all parts of the digestive system. A myriad of bacteria and viruses infect babies.\nA gastroenteritis-causing virus\nViral gastroenteritis is not caused by influenza viruses. Instead, it can be caused by a variety of viruses including rotaviruses, toroviruses, adenoviruses, caliciviruses, and astroviruses.\nAs a result, only about 75% to 90% of the cases of gastroenteritis caused by the virus alone occur. Among them, rotavirus is a major cause of gastroenteritis worldwide. Norovirus is a major example of gastroenteritis in the United States. And the other 5% is caused by parasites.\nThe main pathogens of gastroenteritis are contaminated food and water. There is also a greater risk of infection among babies who consume a variety of other foods than those who breast-feed. Pathogens are usually excreted through the stool when food and water come in contact with infected stools.\nAlso, contaminated items such as toys and toilet seats also carry viruses that cause gastroenteritis. The virus flourishes on these surfaces for several days. And if someone in the family has gastroenteritis, touching the baby 's goods without washing their hands can be transmitted. In conclusion, when infants are infected with pathogens or viruses from all these factors, symptoms of the disease soon appear.\nViral gastroenteritis, however, is not a very serious disease. Even if most of them are sick, they usually recover soon after a certain period of time without long-term problems or side effects. In children, the rate of recovery can vary depending on which virus you have.\nHowever, in order to replace vomiting or diarrhea caused by sickness, a sufficient amount of water should be available for drinking. Otherwise, it can develop into a serious disease. Especially those who can not take care of themselves because of their babies, the elderly, or the disabled are at high risk of dehydration and should pay more attention.\nPeople with impaired immune systems may also have more severe symptoms than healthy people. Vomiting or diarrhea is more frequent. This can also lead to dehydration risk. If some symptoms are severe, you may need to be hospitalized to prevent dehydration.\nViral Gastroenteritis Treatment\nViral gastroenteritis in children and adults is usually treated with dehydration prevention. Dehydration means a lot of body fluids are lost. When a child has enteritis symptoms, parents should ask their doctor how to prevent dehydration and get advice.\nHowever, water is not always a sufficient remedy. Of course water is good in itself, but in some cases it may not be enough. For example, water can not replace electrolytes such as salt, sugar, and minerals that can be lost when the body is dehydrated. Beverages that can replace salt or minerals can be purchased at pharmacies, such as electrolytic solutions or oral water supply solutions. Some sports drinks also have the ability to replace electrolytes. Of course, it contains a lot of sugar, but it does not give much to most school-aged children.\nOn the other hand, milk should be kept away. Milk can make gastroenteritis worse. In addition, it is preferable to avoid beverages or caffeinated beverages containing a lot of acid because they can cause discomfort.\nAnd when you feed such a drink containing water and various electrolytes, it is advisable not to enter too quickly. If it is absorbed into the body too soon, it can worsen vomiting and should be taken as slowly as possible. The best way to do this is with a teaspoon every 4-5 minutes.\nFood should also be adjusted to eat slowly. Once you have had a good drink, you should try adding soft foods such as banana, bread, rice, apple sauce or toast. Chicken soup and noodles with noodles are also good. And if the condition improves gradually, you can feed the cooked meat or vegetables. However, fried or spicy foods, greasy or sour foods should not be eaten for a while.\nIt is good to skip medicines during the treatment period. Although you may think you should take a prescription-free medicine, gastroenteritis can eventually be resolved over time. Most medications also have a different risk of worsening the condition. With vomiting and diarrhea, the body fights infection while eliminating everything in the body. It is desirable to make the process self-healing by retaining sufficient water without stopping with the drug.""]"	['<urn:uuid:d7c6cf83-351d-40b8-8216-34098c592495>']	factoid	with-premise	verbose-and-natural	similar-to-document	novice	2025-04-22T14:43:01.810270	21	51	803
6	turbofan engines market dominance 1970s widebody	The introduction of widebody jets in the 1970s, including the Boeing 747, Douglas DC-10, and Lockheed Tristar, marked a significant shift in the engine market. This period saw GE and Rolls-Royce entering the market, and it marked the beginning of high bypass turbofans. Previously, Pratt & Whitney's JT8D turbofan had dominated the short-haul airliner market, while their JT3D controlled the long-range market.	['By Bjorn Fehrm\nSeptember 15, 2022, © Leeham News: Last week, we looked at how Pratt & Whitney’s JT8D turbofan came to dominate short-haul airliners while the JT3D had the long-range market.\nThe introduction of the widebody jets in the 1970s with Boeing 747, Douglas DC-10, and Lockheed Tristar brought GE and Rolls-Royce into the market. It was the start of the high bypass turbofans.\nBy Bjorn Fehrm\nSeptember 8, 2022, © Leeham News: Last week, we analyzed the change from turbojets to turbofans for civil air transport. The jet engine was developed for high-speed military fighters and was not ideal for subsonic airliner use.\nWe also dwelled on why the three major engine OEMs came to different solutions for the first-generation turbofans. Now we look at the engine that made turbofans mainstream, the Pratt & Whitney JT8.\nBy Vincent Valery\nJune 20, 2022, © Leeham News: New airplane programs used to come to market in four years. Now, the launch-to-entry-into-service period has been seven years or more. (Chinese and Russian programs take even longer.)\nBoeing launched the 787 in December 2003. EIS was October 2011. Airbus’ A350, launched in response to the 787 in 2004, went through several iterations which added time to the program. Delays added more time. EIS was in January 2015.\nBombardier’s C Series was launched in 2008. EIS was in July 2016. The Boeing 777X was launched in 2013. EIS is now targeted for 2025. Boeing launched the 747-8 in 2005. EIS was in 2011. The Boeing 737 MAX was launched in July 2011. EIS was May 2017. Airbus’ A320neo was launched in December 2010. EIS was in January 2016.\nBoeing has been discussing the New Midmarket Airplane (or whatever it was called throughout changing nomenclature) since 2012. It still hasn’t launched the program. Once it does, how long will it take to enter service?\nAny new program is a multi-year, multi-million investment that, in the worst case, can take decades before recovering the initial development and production ramp-up expenditures.\nSeveral recent programs, notably the 777X, have faced significant delays between the envisioned and actual start of deliveries to airlines.\nBoeing claims that advances in manufacturing techniques will reduce the time required to develop the next aircraft program. However, regulatory scrutiny is higher nowadays and the aircraft built are more complex than in previous generations.\nLNA analyzes how the time between the program launch and entry into service has evolved since the beginning of the Jet Age. The goal is to find whether there is a trend and in what direction. The analysis focuses on Airbus, Boeing, Lockheed, and McDonnell Douglas.\nBy Vincent Valery\nJuly 29, 2021, © Leeham News: Last week, LNA compared the performance of the 777F against the A350F, launched today. As a follow-up, we thought it relevant to look at the history of freighter aircraft derived from passenger jets at the major OEMs.\nShortly after the dawn of the jet age, Boeing and McDonnell Douglas started selling freighter variants of their 707 and DC-8, respectively. Most aircraft families developed later at both OEMs would receive a freighter variant in one form or another.\nWe will stick for our analysis to Freighter aircraft delivered off the assembly line at the world’s Western OEMs: Airbus, Boeing, Lockheed, and McDonnell Douglas.\nBy Scott Correa\nSpecial to Leeham News\nMay 27, 2021, © Leeham News: Forty-two years ago this week, I puked at work.\nOn May 25, 1979, an American Airlines McDonnell Douglas DC-10 crashed on take off from Chicago’s O’Hare International Airport. Within minutes, it was known that the No. 1 engine separated from the airplane just after the airplane was committed.\nThe aircraft gained a few hundred feet before rolling over on its left wing, crashing into a trailer park. All 271 on board and two people on the ground were killed.\nThe Federal Aviation Administration immediately grounded all DC-10s in the US because of the engine separation. Regulators elsewhere in the world followed suit.\nMay 10, 2021, © Leeham News: The COVID-19 pandemic prompted airlines to ground more than 8,000 aircraft at the peak.\nAmong widebodies, no aircraft was hit harder than the Airbus A330ceo.\nTraffic within China, the US and Asia recovers with narrowbody airplanes. European short- and medium-haul traffic is not recovering as quickly due to continued boarder closings. International traffic, for the same reason, remains awful.\nBut in chaos some see opportunities.\nJep Thornton, managing partner of the boutique lessor Aerolease, last week said the A330-300 could be a great trading opportunity.\nAt April 1, there were 267 -300s and 286 A330-200s (of all types) in storage, according to data reviewed by LNA.\nDec. 22, 2020, © Leeham News: If you get a chance over the next few weeks – in between binge-watching The Queen’s Gambit, putting up the 79 extra feet of Christmas lights you ordered this year and figuring out how to buy surprise Christmas gifts for your spouse when you have a joint Amazon account – you should take 90 minutes to watch this video from our friends at the International Association of Machinists District Lodge 751.\nThe Machinists on Dec. 8 hosted (on Zoom, of course) a high-level panel discussion about the state of the aerospace industry and Washington state’s role in it, featuring a whole bunch of Brand-Name People Who are Smarter Than Me(c).\nThey shared their insights for those of us coffee-drinkers who are trying to read the tea leaves to divine what Boeing’s next moves should be as it tries to get back on its feet – and what the implications are for its home state.\nThe problems for Boeing are obvious, and the solutions are pretty clear – but doing the smart thing would require a major cultural shift from an executive team that’s locked into a 1990s vision of how business gets done.\nJune 29, 2020, © Leeham News: Boeing may be set to begin recertification flights of the 737 MAX as early as today, The Seattle Times reported last week.\nTesting will take three days, if all goes well. But Boeing still has a lot of work to do to fully satisfy regulators.\nAccording to The Times, Transport Canada and Europe’s EASA require additional modifications to enhance safety on the MAX. The additional changes may not be required for certification but must be done within a year, the paper reports. The MAX 10 must have the changes before it is certified.\nBy Vincent Valery\nJune 1, 2020, © Leeham News: As airlines slashed capacity in the aftermath of the COVID-19 outbreak, some took the opportunity to accelerate aircraft retirements.\nOlder generation twin-aisle aircraft, notably the Airbus A340, older A330s, Boeing 747 and 767, have exited numerous carrier’s fleet early. Several Airbus A380 operators put their Superjumbos in long-term storage, wondering whether these will ever fly in passenger service again.\nMajor crises tend to accelerate existing trends. The move away from large twin-aisle aircraft is a case in point. In the context of subdued demand for several years, airlines will be under pressure to reduce expenses. Streamlining fleets is an obvious target.\nThe Airbus A320 and Boeing 737 families dominated the single-aisle market for decades. The picture has historically been far more fragmented for twin-aisle aircraft. Airbus and Boeing still have three widebody aircraft families apiece with significant numbers of passenger aircraft in service.\nLNA analyzes in two-part articles why the picture will likely change for the widebody market in the 2020s. In the first part, we will take a historical detour to analyze why twin-aisle fleets are still so fragmented nowadays.\nMarch 9, 2020, © Leeham News: Commercial aviation accidents are high profile news events.\nThese happen rarely. Many times, a lot of people are killed. (It should be noted that often survivors may outnumber those killed as safety improved.)\nIn this era of 24/7 cable news and minute-by minute social media, everyone wants instant answers as to causes.\nFinding answers is not simple. A typical accident investigation usually takes 12-18 months before the investigators issue a final report with a probable cause.\nOne reason for this is that sometimes, the cause of an accident comes down to a single bolt, or even a single cotter pin.\nThis is where the new book, Flight Failure, Investigating the Nuts and Bolts of Air Disasters and Aviation Safety, serves to remind us of just how intricate accident investigation is.']	['<urn:uuid:decf2ad2-7813-4548-9ffa-360165da1603>']	open-ended	direct	short-search-query	similar-to-document	expert	2025-04-22T14:43:01.810270	6	62	1398
7	What solution is recommended when implementing ITIL processes in phases to handle missing process inputs?	To handle missing inputs during a phased introduction of ITIL processes, a generic process directory for the IT organization can be used. This directory provides a structured framework for defining process links even when only a subset of ITIL processes is initially defined, allowing additional processes to be plugged into the model later as needed.	['ITIL Implementation - Process Interfaces\n|Step 6: Definition of ITIL Process Interfaces|\nThis step determines which inputs each ITIL process receives from other processes, and which outputs it must produce so that subsequent processes are able to function.\n- Definition of the interfaces for all ITIL processes which are to be introduced\nThese inputs and outputs are also called ITIL information objects: Structured sets of data, like e.g. an Incident Record, which serves to describe a service interruption.\nJust how great the importance of process interfaces is for the design of optimal work procedures frequently becomes apparent during the analysis of as-is processes:\nWeaknesses in processes often occur at those points where one process ends and another one begins. In many cases one will find interrupted information flows or media breaks – so that the required information is not exchanged as intended.\nThe definition of the process interfaces is taken care of as a separate project step, before dealing with the innards of the processes in detail. Obviously, before being able to define the detailed activities, it must be clear what inputs a process can expect from preceding ones, and which outputs it must produce.\nThe ITIL Process Map applies a rigorous approach to the definition of interfaces: Information objects may be picked from a central ITIL glossary (see figure 1: Index of data objects (.pdf)) to define the inputs and outputs in a precise way. Every information object contains a short definition to avoid any ambiguities about the expected process results.\nA challenge during the definition of the ITIL interfaces lies in the fact that, as a rule, not all ITIL processes are introduced at once, which often means that some of the required inputs for a process are missing.\nIn order to circumvent this problem, which inevitably springs up during a phased introduction of ITIL, a generic process directory for the IT organization as a whole can be used.\nThe generic directory offers a structured framework for the definition of process links even if, initially, only a sub-set of the ITIL processes is defined in detail.\nAdditional ITIL processes can thus be plugged into the process model at a later point in time as needed.\n- Structure of the ITIL processes to be introduced\n- ITIL information objects (ITIL glossary terms) as inputs and outputs\n- Interfaces of the ITIL processes to be introduced:\n- with each other\n- with other service management processes\n- with customer and supplier processes\n- It must be avoided that the newly introduced processes represent an isolated solution; the interfaces to the other processes within the IT organization and beyond it must therefore be considered.\n- The documentation of the interfaces should be clearly structured, showing details only when required. This calls for overview diagrams showing the big picture and separate detailed interface diagrams for each process.\nRelevant views of the ITIL Process Map\nThe ITIL Process Map contains two types of models which, in combination with each other, are used for the definition of the process interfaces:\n- Process overviews (see ITIL implementation step 5 - figure 2), which illustrate the interrelations of several processes on one single page\n- Detailed process interface diagrams with all inputs and outputs (see figure 2)\n-  To-be process structure: Generic ITIL process structure (.pdf)\n-  ITIL inputs and outputs: ITIL information objects (ITIL glossary terms)\n-  Index of Data Objects - Example (.pdf)\nFollowing project activity\n→ ITIL Implementation - Step 7: Establishing Process Control']	['<urn:uuid:1b7bce94-d953-4f32-aed9-1222dbbf1b59>']	factoid	direct	verbose-and-natural	similar-to-document	expert	2025-04-22T14:43:01.810270	15	55	581
8	what happened britain after roman rule ended economy society	After Roman rule ended in Britain, the army and civil service workers stopped receiving payment, which triggered a decline in craft and service industries. While it was traditionally believed that most of the population turned to subsistence farming and the administrative system broke down into local fiefdoms, evidence from places like Chedworth Villa suggests the decline was more gradual, particularly in the West Country where the Romanized way of life was sustained longer.	"['\'Tremendously exciting\' 5th century Roman mosaic found in Britain\nArchaeologists have uncovered Britain\'s first 5th century Roman mosaic -- a find of ""enormous"" historical significance which could change the way historians view the period it dates back to.\nThe mosaic floor was first discovered inside ""room 28"" at the Chedworth Roman Villa in Gloucestershire, England in 2017, but testing to verify the age of the floor has just been completed, UK conservation charity the National Trust said in a press release Thursday.\nThe date is significant because it had been believed towns and villas fell into decay after being abandoned following an economic crash in the 4th century.\nMartin Papworth, National Trust archaeologist, said the 5th century marks the end of the Roman era in Britain and the beginning of the Dark Ages -- when society is thought by some historians to have deteriorated in western Europe as a result of the Roman withdrawal.\nThe Dark Ages, according to the National Trust, was also a period from which few documents survive, with archaeological evidence scarce.\nNational Trust researchers used radiocarbon dating -- testing the level of carbon in charcoal and bone found in the area where the mosaic was found -- and pottery analysis to pin down the ""unexpected"" age of the mosaic at the villa.\nChedworth Roman Villa is one of the largest Roman villas known in the country and one of the best preserved, according to the National Trust.\nAfter the end of Roman rule in Britain, the army and civil service workers stopped being paid, which in turn triggered ""production decline"" among the craft and service industries, Papworth said in a statement.\nThe quality of the mosaic design possibly reflects this, as the National Trust said it was of ""poorer quality"" than those created in the 4th century.\nHowever, its existence also indicates that society did not decline as rapidly as first thought, and that ""sophisticated life"" carried on for longer, particularly in southwestern England where the mosaic was found.\n""It has generally been believed that most of the population turned to subsistence farming to sustain themselves and, after the break with Rome, Britannia\'s administrative system broke down into a series of local fiefdoms,"" Papworth added.\n""What is so exciting about the dating of this mosaic at Chedworth is that it is evidence for a more gradual decline. The creation of a new room and the laying of a new floor suggests wealth, and a mosaic industry continuing 50 years later than had been expected.""\nThe mosaic was uncovered as part of a six-year program of archaeological research and digs at the Chedworth Roman Villa.\nPapworth said: ""It is interesting to speculate why Chedworth Villa\'s owners were still living in this style well into the 5th century. It seems that in the West Country, the Romanised way of life was sustained for a while.\n""Many large, richly decorated Roman Villas have been found in the countryside around Cirencester, which is around 8 miles from Chedworth.""\nStephen Cosh, a Roman mosaic expert, said in a statement: ""I am still reeling from the shock of this dating.""\n""There are very late Roman mosaics in the area for which archeology can only ever say they must be later than a particular date, without being able to say how much later,"" Cosh added.\n""But none has ever been suspected to be this late. It will be important to research further sites in the region to see whether we can demonstrate a similar refurbishment at other villas which continued to be occupied in the 5th century. But there is no question that this find at Chedworth is of enormous significance -- it\'s tremendously exciting.""']"	['<urn:uuid:0ac83b5f-81ed-4283-a8a7-ea794e35ed7d>']	open-ended	direct	long-search-query	similar-to-document	novice	2025-04-22T14:43:01.810270	9	73	612
9	What role does internationalization play in modern education systems, and how is technology being used to enhance international collaboration and learning opportunities?	Internationalization plays a vital role in modern education by promoting student, teacher, researcher, and knowledge mobility through various means including scholarships, staff exchanges, and cross-border qualification recognition. Technology, particularly Information and Communications Technology (ICT), significantly enhances internationalization by enabling new forms of teaching, learning, and knowledge sharing across global educational settings. The focus is on creating learning environments that facilitate collaboration in our interconnected world and integrating international and intercultural dimensions into all education levels. Special attention is given to internationalization at home practices, which help develop students' intercultural and global competencies while providing equal learning opportunities for all, especially considering the challenges posed by the digital divide.	"['G20 Education Ministers\' Communiqué\nVideoconference, September 5, 2020\n- We, the G20 Ministers of Education, met virtually on September 5, 2020, to affirm the central role of education in enabling all people to realize the opportunities of the 21st century.\n- As articulated in the G20 Education Ministers\' Statement on COVID-19 of June 27, 2020, we support the individual and collective efforts to mitigate the unprecedented impact of the COVID-19 pandemic on education and acknowledge the importance of ensuring education continuity and safety for all in times of crises.\n- Following the 2018 G20 Education Ministers\' Declaration, we reaffirm that education is a human right and a basis for the realization of other rights, as well as ""the foundation of personal development as it provides children, youth and adults with the knowledge, skills, values, and attitudes necessary to reach their full potential"".\n- In line with the United Nations 2030 Agenda, we reaffirm our commitment to ensuring inclusive and equitable quality education and promoting lifelong learning opportunities for all.\n- We emphasize the vital role of education and skills training in addressing social, cultural, and economic challenges, and, therefore, further our commitment to encouraging international collaboration and the sharing of best practices to advance education systems across the globe. In this way, we will contribute to broader aims, including reducing poverty and inequality; promoting inclusive and sustainable economic growth; advancing access to quality education for all, especially girls, and empowering women, youth and vulnerable groups.\n- We highlight the importance of improving access to quality Early Childhood Education (ECE) as a foundation for the development of current and future generations and as a fundamental part of promoting equity and inclusion in education and encouraging lifelong learning.\n- We recognize the value of fostering internationalization in education as a means of improving the quality of education at all levels and cultivating global citizens who are prepared for an increasingly interconnected world.\n[back to top]\nEduction Continuity in Times of Crisis\n- We support the sharing of best practices and experiences as we explore approaches to building more resilient education systems. We encourage the development of policies and measures to prioritize the continuity of teaching and learning during and after the pandemic and the health and safety of the education community at large, students, teachers, educators, staff and parents, as appropriate in national, regional and local contexts.\n- Werecognizethevalueofdistanceandblendedteachingandlearning and underscore the importance of enhancing access to high-quality education, professional development for educators, digital infrastructure and content, cybersecurity awareness, appropriate teaching methodologies and active learning, while recognizing that these approaches complement face-to-face learning. We stress the importance of research and data to assess the learning outcomes and quality of distance learning.\n[back to top]\nEarly Childhood Education (ECE)\n- We acknowledge the fundamental role that equitable access to quality ECE plays in stimulating children\'s holistic development, which is the basis of their acquisition of literacy, numeracy, and social and emotional skills, to lay the foundation for future learning and well-being.\n- We emphasize the importance of improving the accessibility and affordability of quality ECE for all children, especially those in vulnerable groups.\n- We assert the importance of ECE that focuses on children\'s experiences, development and well-being and fosters positive interactions among ECE personnel, children, families and their communities.\n- We also acknowledge the need to raise family and community awareness of the vital role of quality ECE that is delivered in accordance with children\'s developmental needs at each stage.\n- We underscore the significance of building and retaining an appropriately qualified ECE workforce based on teachers, educators, staff and leaders of ECE institutions who have the knowledge, skills and competencies to work with young children, and on professional training to upskill and reskill them throughout their career.\n- We highlight the value of a smooth transition from pre-primary settings to primary schools. Therefore, we encourage cooperation and collaboration across these educational levels, in accordance with country contexts, so that the benefits of quality ECE are realized and sustained.\n- We recognize that leveraging digital technologies can increase children\'s access to quality ECE and enable families, teachers and educators to create developmentally-appropriate learning experiences for all children. We acknowledge the need to reduce the digital divide by providing the education community, including vulnerable groups, with the support and education necessary to enable effective interaction with technological devices. We also emphasize the importance of studying the effects of such exposure on young children\'s development, learning, and well-being to identify opportunities and mitigate potential risks.\n[back to top]\nInternationalization in Education\n- We emphasize the importance of international collaborations and partnerships in the field of education, especially in the context of global crises such as the COVID-19 pandemic. While respecting national and sub-national laws, rules and policies, we support the promotion of internationalization in education for all through student, teacher, researcher, and knowledge mobility; the provision of scholarships; the exchange of teachers, educators and staff; information-sharing for the facilitation of cross-border recognition of qualifications; the use of Information and Communications Technology (ICT); and international research, knowledge production and technological development.\n- We acknowledge the need for learning environments that enable students, teachers, and educators to collaborate and engage in our interconnected world. We encourage the integration of international and intercultural dimensions into all levels of general (K-12), higher and vocational education and training, where appropriate, to ensure effective learning outcomes.\n- While considering the opportunities and challenges of digitalization, we recognize the significant role that ICT can play in enhancing internationalization in education by shaping new forms of teaching, learning, knowledge sharing and exchange within and among educational settings across the globe. To achieve that role, we will work to support access for the most vulnerable and to reduce the digital divide.\n- We encourage the sharing of best practices in internationalization in education and the adaptation of such practices at the local, national, and international levels, as appropriate. We support advancing the discussion on internationalization at the K-12 level, and considering the COVID-19 pandemic, we recognize the impact of internationalization at home (internationalization practices in a country) in broadening students\' intercultural and global competencies and in providing equal learning opportunities and experiences for all.\n[back to top]\n- We will continue to work on and support knowledge-sharing across G20 members in the areas of education continuity, early childhood education and internationalization in education so that we can all learn together and advance our education systems. We also agree to consider future collaboration and research to examine the impact of COVID-19 on education.\n- We extend our gratitude to the Saudi G20 Presidency for its determined efforts and leadership. We also thank the Arab Bureau of Education for the Gulf States (ABEGS), Islamic Development Bank (IsDB), the Organisation for Economic Co-operation and Development (OECD), the United Nations Educational, Scientific and Cultural Organization (UNESCO), the United Nations Children\'s Fund (UNICEF), and the World Bank Group (WBG) for their valuable contributions to our work.\n- We will submit this Communique to the G20 Leaders\' 2020 Summit and will continue our cooperation towards Italy\'s G20 Presidency in 2021 and thereafter.\n[back to top]\nSource: Official website of the Saudi G20 Presidency\nAll contents copyright © 2021. University of Toronto unless otherwise stated. All rights reserved.']"	['<urn:uuid:37c49f91-85ea-477d-b36d-52d217ff7a6d>']	open-ended	direct	verbose-and-natural	similar-to-document	expert	2025-04-22T14:43:01.810270	22	108	1208
10	how many glasses water prevent kidney stones	It is recommended to drink eight (8) glasses of water each day to help reduce your chances of developing kidney stones.	"[""How Your Kidneys WorkMost people have two kidneys. The kidneys have several very important jobs which include:\n- Clean waste products from your blood.\n- Maintain normal fluid balance.\n- Helps keep the blood pressure in normal range.\n- The kidneys produce a hormone called erythropoieten to help stimulate red blood cell production.\n- The kidneys help keep your bones healthy by maintaining normal levels of certain chemicals in your blood.\nIf you have Chronic Kidney Disease it is important that you do the following:\n- If you are diabetic keep your blood sugar under control.\n- Keep your blood pressure within normal range.\n- Keep appointments with your doctor.\n- It is recommended that you see a kidney doctor (Nephrologist) if your glomerular filtration rate (GFR) is less than 45 or less than 60 if you have protein in your urine.\nLeading Causes of Kidney Disease\n- High Blood Pressure\n- Polycystic Kidney Disease\nChronic Kidney Disease (CKD)Chronic Kidney Disease is defined as the presence of one or more of the following:\n- Kidney damage for > 3 months, as defined by structural or functional abnormalities of the kidney, with or without decreased glomerular filtration rate (GFR).\n- GFR < 60mL/min/1.73m2 for > 3months, with or without kidney damage.\nAcute Renal FailureAcute renal failure occurs rapidly, usually within days, and is typically related to a serious illness, surgery, exposure to toxic substances, or trauma to the kidney. This condition may be reversible with treatment. Patients may need temporary dialysis treatments until the kidney resumes normal function.\nKidney StonesKidney stones include a variety of small, rock-like particles that form in your kidneys. Our nephrologists specialize in identifying the type of stone causing you discomfort and recommending treatment to prevent formation of more stones. The most common stone type is calcium oxalate.\nThings you can do to reduce your chances for developing new kidney stones of this type include:\n- Following a low oxalate diet.\n- Increasing your fluid intake to the recommended eight (8) glasses of water each day.\n- Maintaining the recommended dietary intake of calcium each day.\nNutrition and DietWhen a person's kidney function declines to a stage that they are no longer able to balance specific electrolyte and mineral levels in their blood certain dietary changes may be required.\nThe kidney doctor (Nephrologist) will assess your particular needs based on your lab work and physical exam. Common dietary restrictions for patients with Chronic Kidney Disease (CKD) are Sodium, Potassium and Phosphorus. For people with Kidney Stones (Nephrolithiasis) a low Oxalate diet might be recommended.\nFor people who have high blood pressure (Hypertension) it is recommended that they limit the amount of sodium in their diet. A 2 Gram sodium diet is often recommended. Limiting sodium intake will help maintain blood pressure at a more normal level and decrease fluid retention and swelling (Edema).\nKidneys that are functioning normally will remove excess Potassium through the urine. Patients who have Chronic Kidney Disease may have an elevated level of potassium that stays in their blood. By limiting the amount of high potassium foods that are eaten it will help keep the potassium level in a normal range. Excessively high levels of potassium in the blood require emergency treatment and can be life threatening.\nPhosphorus is a mineral that is found in many foods. It is important to keep the blood phosphorus level within a normal range to help protect the bones. Sometimes it is necessary to take medications that bind to the phosphorus in the foods that are eaten to keep this level in normal range.\nLists of foods for Sodium, Potassium and Oxalate:\n- Sodium - Good Choices and Poor Choices\n- Potassium - Acceptable Potassium Foods\n- Foods High In Oxalates\n- Foods Low In Oxalates\nImportant Minerals in Your DietGood nutrition habits can be built upon a better understanding of the minerals in your blood that your nephrologist will monitor over time. CLICK HERE for a general overview on the importance of these key minerals in your diet.\nMaking Wise Food ChoicesHealthy nutrition is critical for your care. Learn more by visiting our Food Lists to Support Healthy Kidney Functions.\nFrequently Asked Questions about Kidney DietsPlease visit our Nutrition FAQ page\nRecipe LinksFor healthy recipies, please visit:\n- The Recipe Center at www.freseniusmedicalcare.com\n- The Nutrition tab at myfoodcoach.kidney.org/signin\nAccess ServicesDialysis Access Management\nAt Nephrology Physicians, we offer the full scope of care for patients with ESRD and specialize in the following procedures:\n- Vein Mapping\n- Angioplasty, Stenting, and Fistula Salvage\n- Thrombectomy & Trhombolysis\n- Dialysis Catheter Placement and Removal\n- Peritoneal Dialysis Catheter Placement and Repositioning\nGeneral InformationNephrology Physicians is the only practice in the Michiana area to offer all options of dialysis modalities through our association with Fresenius Medical Care. To learn more about these services visit www.freseniusmedicalcare.com\nHemodialysisHemodialysis removes wastes and extra fluid from your blood. During this process, blood is pumped though a filter called a dialyzer. As your blood is filtered only a small amount is outside of your body at a given moment and your filtered blood is retuned to your body. A special access is created, typically in your arm, to allow faster and safer connection to the equipment on a routine basis.\nHome HemodialysisIn 2006 Nephrology Physicians initiated the Michiana area's first option for patients to dialyze at home and our program continues to be highly regarded as an innovative program.\nHome dialysis is very similar to more traditional in-center dialysis except the machine to cleanse your blood is more compact. Home dialysis offers many opportunities for flexibility in scheduling your treatments at times that meet your personal lifestyle. Home Dialysis does require a committed partner to assist you in performing the procedure safely. Your nephrologist will discuss this option with you and your partner to identify if this option is suitable for your needs.\nOur program uses the NxStage System One Cycler to perform your dialysis treatments. You can learn more about this system at http://www.nxstage.com.\nPrior to starting home dialysis, you will receive your treatments at an in-center facility to understand the specific processes that occur during dialysis. This is a time when you will become proficient in self-cannulation techniques to enable you to hook up to your home machine. Several weeks are typically required to work with specialized home dialysis nurses on all of the critical aspects to completing this procedure routinely in a safe and healthy manner.\nPeritoneal DialysisThis type of dialysis is done at home. The lining of the abdomen acts as a natural filter to clean the blood with a solution called dialysate. Dialysate is infused into the peritoneal cavity through a catheter. Waste products and excess fluid are drawn into the dialysate from the blood and drained from the body when the fluid is drained out through the catheter.\nPeritoneal Dialysis Fact Sheet\nThere are two types of peritoneal dialysis:\n- Continuous Ambulatory Peritoneal Dialysis (CAPD): Exchanges are done 4-6 times a day.\n- Continuous Cycling Peritoneal Dialysis (CCPD): A machine called a cycler does the exchanges automatically at night while the person sleeps.\nNocturnal DialysisNocturnal hemodialysis treatment takes place for an average of 8 hours a night, 3 times a week at the dialysis facility. Dialysis professionals monitor you throughout the night while you sleep.\nWhile traditional hemodialysis provides effective treatment, nocturnal hemodialysis offers a longer, slower treatment for patients who need additional time to remove fluids.\nThe only local nocturnal program is offered at the Fresenius Medical Care - South Bend facility at 320 St. Joseph, South Bend, IN\nDo I still see my nephrologist while I am on dialysis?Your nephrologist manages your dialysis treatment prescriptions. Your doctor will meet with you during your dialysis treatment on a monthly basis. This rounding includes the whole care team including nursing staff, a social worker, and dietician to better understand how you are adjusting to the treatments and address any of your questions. Your doctor may make adjustments to your medications or your dialysis prescription to maximize your health and comfort level. Our Nurse Practitioners assist the nephrologist in managing your treatment by rounding up to three additional times per month to monitor your progress, discuss changes to your care, and assist in helping you find other resources to manage your disease.\nPatient Education Programs\nNephWiseThis is an informative, introductory class for patients who have recently been diagnosed with CKD Stage 3. This group session is held monthly at our office and last approximately 1 hour. For additional information or to reserve a space, please contact our office at 574-273-6767.\nMIPPAThis educational program abbreviation references the Medicare Improvements for Patients and Providers Act. This service is available for Medicare beneficiaries with a diagnosis of CKD Stage 4. This is a series of one-on-one visits at the office with a nephologist and/or Nurse Practitioner.\nTOPsThis educational program abbeviation references the Treatment Options Program offered by Fresenius Medical Care. This free class is provided for patients with a CKD Stage 4 or 5 diagnosis who want to paticipate in learning about dialysis options. For additional background information visit www.ultracare-dialysis.com\nInternet Education LinksNational Kidney Foundation (NKF): Information for organ donors and recipients, for patients and professionals, m0eetings and events and support. An AZ guide for kidney disease.\nAmerican Kidney Foundation (AKF): Information about this national voluntary health organization, as well as kidney disease facts.\nAmerican Association of Kidney Patients (AAKP): Have you recently been diagnosed with reduced kidney function? View information on resources for those with chronic kidney disease (CKD) or at risk.\nKidney School is an interactive, web-based learning program designed to help people learn what they need to know to understand kidney disease.\nUnderstanding Kidney Transplant Information and Advice for PatientsPlease visit: www.transplantexperience.com\nOur Physicians and Staff are dedicated to taking care of patients with Chronic Kidney Disease. If you have been diagnosed with Chronic Kidney Disease, Kidney Stones or Hypertension talk to your doctor about a referral to Nephrology Physicians. We offer the most comprehensive care for kidney disease in the Michiana area. To refer yourself, go to https://www.nephinc.com/patient-referral.asp or call 574-273-6767 ext. 1205.""]"	['<urn:uuid:9f19c992-c917-4a56-8594-9581f7b71d0b>']	factoid	direct	short-search-query	similar-to-document	novice	2025-04-22T14:43:01.810270	7	21	1685
11	euro step variations reverse one handed execution nba players	There are two main variations of the euro step: the reverse euro step and the one-handed euro step. The reverse euro step involves quickly changing direction opposite to the original move for increased deception. The one-handed euro step involves dribbling with only one hand while performing the movement, allowing for maximum agility and quickness. Notable NBA players who have mastered the euro step include James Harden, Giannis Antetokounmpo, and Nikola Jokic. Some memorable moments include Harden's game-winning layup against Oklahoma City Thunder in the 2017 playoffs and LeBron James's winning basket against Andre Iguodala in Game 6 of the 2016 NBA finals.	['The euro step is an exciting and unique move seen in basketball, utilized by some of the greatest players to ever play the game. It has become a signature move for many of today’s professional athletes and can be used to create space between a defender and shooter. With its roots in European basketball, the euro step looks like a cross between a crossover dribble and a spin move. It has become a favorite move among many players and coaches because it gives the offensive player an advantage over defenders. The euro step is used to great effect by many of today’s players, including LeBron James, Stephen Curry, and Kyrie Irving.\nWhat is the Euro Step?\nThe euro step is a move used in basketball that involves a player taking two steps in one direction and then quickly changing direction, usually with the intent of creating an open shot. It often looks like a cross between a crossover dribble and a spin move, and has become popular among many of today’s professional athletes due to its effectiveness at creating space between the shooter and defender. Originating from European basketball, the euro step has become a signature move for many of today’s athletes, including LeBron James, Stephen Curry and Kyrie Irving.\nHistory and origin of the euro step\nThe euro step originated in Europe, where it is known as a “step-back” move. It was first popularized by Italian basketball player Dino Meneghin, who used the move to create space between himself and his defender. The move then spread to other countries throughout Europe before making its way to the United States where it has become a staple of the game. It was popularized in the US by NBA players such as Kobe Bryant, who showcased the move during his career.\nBenefits of the euro step\nIncreased efficiency in scoring\nOne of the main benefits of using the euro step is its increased efficiency in scoring. By quickly changing direction, players are able to create space between themselves and their defenders, allowing them to have more time and space to get a shot off. Additionally, this move can be used to create openings for cuts or drives towards the basket, creating more opportunities for points. This makes it particularly useful for players looking to score quickly and efficiently.\nAbility to maneuver around defenders\nThe euro step is also effective at maneuvering around defenders. By quickly changing direction, the offensive player can throw off their defender and create open space for a shot or drive towards the basket. This deceptive move allows players to take advantage of their opponents and create opportunities for themselves, allowing them to gain an edge over their opponents.\nHow to perform the euro step\nSteps and breakdown of the move\nStep 1: Begin with a crossover dribble, moving the ball from one hand to the other while shifting your body weight. As you do this, make sure that you are pushing off of your back foot and stepping in towards your defender.\nStep 2: As soon as you finish your crossover, take a second step in the same direction as the first one. This will help to create space between you and your defender.\nStep 3: As soon as you take the second step, quickly shift your body weight toward the other direction and plant your outside foot. You can also add a slight hop to further throw off your defender.\nStep 4: Once you have planted your outside foot, begin dribbling in the opposite direction that you were originally going. This will create an even bigger opening between you and your defender.\nPractice drills to improve euro step technique\nPractice drills to improve euro step technique are important for players who want to sharpen their skills and become more comfortable with the move. These drills can help a player develop strength, agility, and muscle memory so that they can utilize the move in game situations. One drill involves taking two dribbles forward and then quickly changing direction by planting the outside foot and pivoting in the opposite direction while dribbling the ball. This drill can be done multiple times in a row to help players develop quickness and accuracy with their euro steps. Additionally, drills that involve two or more players are beneficial as they allow players to practice their moves against live defense and learn how to read and react in real game situations.\nExamples of the euro step in action\nNBA players who use the euro step\nNBA players who use the euro step are some of the most talented players in the league. Notable examples include Houston Rockets guard James Harden, Milwaukee Bucks forward Giannis Antetokounmpo, and Denver Nuggets center Nikola Jokic. These players use the move to create openings for themselves on offense and score efficiently against their opponents. Additionally, these players have mastered how to maneuver around their defenders with the move, allowing them to create even more space for themselves on the court.\nMemorable moments featuring the euro step\nMemorable moments featuring the euro step are numerous within the NBA. One of the most iconic examples occurred during the 2017 NBA playoffs when James Harden of the Houston Rockets employed a successful euro step to evade Russell Westbrook and sink a game-winning layup against the Oklahoma City Thunder. Another memorable moment came during Game 6 of the 2016 NBA finals when LeBron James used a Euro step to drive past Andre Iguodala and score the winning basket for the Cleveland Cavaliers. These moments will forever be remembered as some of the most iconic plays in basketball history.\nVariations of the euro step\nReverse euro step\nThe reverse euro step is a variation of the standard euro step move that gives players an even greater edge against their defenders. This variation involves quickly changing direction in the opposite direction of the original move, which allows for more deception and misdirection. To perform this move, start by taking two dribbles in one direction and then quickly planting your outside foot and pivoting in the opposite direction. This can be a great way to create an even bigger opening between you and your defender.\nOne-handed euro step\nThe one-handed euro step is a powerful variation of the standard euro step move that allows for maximum agility and quickness on the court. This move involves taking two dribbles, planting your outside foot, and then quickly shifting your weight in the opposite direction while dribbling the ball with only one hand. This can be an effective way to create even more space between you and your defender as they may not expect the ball to be in your hand.\nTips for executing the euro step effectively\nTiming and footwork\nTiming and footwork are essential when performing the euro step. The move needs to be executed with quickness and precision and must be done in a split second. Players should practice their timing and footwork before trying the move during a game. This is best done by doing drills that involve two or more players, as this allows for live defense and an opportunity to develop muscle memory.\nReading the defender\nReading the defender is an important skill to master when executing a euro step. Players must be able to read and react quickly to their opponents’ movements in order to gain an advantage on offense. To do this, players should take note of their opponent’s body language, such as foot positioning and arm placement. By doing so, players can anticipate their next move and use the euro step to get around them.\nThe euro step is a powerful move that gives players an edge on offense and allows them to create more space for themselves. Notable examples of NBA greats who have mastered the move are James Harden, Giannis Antetokounmpo, and Nikola Jokic. There are also several variations of the move such as the reverse euro step and one-handed euro step that offer even more misdirection and agility on the court. To execute the move effectively, players should practice their timing and footwork as well as read and react quickly to their opponents’ movements.\nMastering the euro step is essential for any basketball player looking to elevate their game to the next level. Not only does it allow players to create more space on offense, but it also gives them an advantage over defenders who may not be expecting the move. This is especially true when coupled with variations like the reverse euro step or one-handed euro step, which can be used to even greater effect. With practice and dedication, the euro step can become a go-to move for any basketball player looking to gain an edge on offense.']	['<urn:uuid:8a492973-35b5-4799-90f7-f10fa2b33fae>']	open-ended	direct	long-search-query	similar-to-document	expert	2025-04-22T14:43:01.810270	9	102	1453
12	How much electricity can a marine turbine with 30-foot rotor blades produce?	A marine turbine with 30 foot long rotor blades can produce 300 kilowatts of electricity.	"[""People are becoming aware of the importance of ocean energy, which will lead to a new era of alternative energy. Because of this knowledge, tidal energy has great potential and bright prospects in future.\nLen Calderone for | AltEnergyMag\nGravity plays an important role in our lives. It keeps our feet on the ground and the planets in order. Gravity is very noticeable on our planet, specifically, it is responsible for the rise and fall of the ocean’s tides worldwide.\nTo understand how the tides work it’s important to understand the relationship between the motion of our planet and its moon. The Moon's gravitational forces are strong enough to cause the oceans to bulge, which causes a high tide. Because of the rotation of the Earth a high tide also occurs on the other side of the planet. As the moon moves around the Earth, the bulge (high tide) moves with it. The moon’s movement creates the waves that are part of the tidal energy.\nTides are more predictable than wind energy and solar power. The main problem with tidal power is its comparatively high cost and limited accessibility of locations that have suitably high tidal ranges or flow velocities.\nTidal energy is one of the oldest forms of energy dating back to the seventh century. The tides turned waterwheels, producing power in order to mill grain, similar to what is used in streams.\nThe emission of gases responsible for global warming and acid rain is eliminated by the use of tidal energy.\nThe Rotech tidal turbine has been designed to be uncomplicated and tough, keeping costs and maintenance down. With the turbine unit attached to the seabed, the rotor blades are designed to work in either tide direction so that the whole unit does not need to be rotated, or the pitch of the blades changed when the tide turns.\nThe Venturi effect is the reduction in fluid pressure that results when a fluid flows through a constricted section of pipe. A Venturi shaped duct is used to channel and increase the speed of the tidal stream toward the rotor. By using a Venturi duct, more energy can be extracted from the same amount of water with smaller diameter rotor blades. This design is suitable for slow tide movement, and a wide angle diffuser will help if the turbine axis is not aligned with the tidal flow, as turbine assemblies are bi-directional or multidirectional.\nA tidal barrage uses the energy of the water, captured at high tide behind a dam in an estuary. Water flows to the lower level as the tide recedes, driving turbines which in turn drive electric generators. The system could work both ways. A typical tidal power barrage may look like this:\nA barrage is similar to a hydroelectric dam, and is essentially a large concrete structure that spans an estuary basin, bay or river with sluices that enable waves to move in and out of the dammed area. At high tide, the sluices are closed; but when the tide shifts directions and ebbs, the sluices are opened allowing the higher water levels in the basin to flow through the barrage and pass the hydroelectric turbine on its way out to sea. The basin is refilled when the tide flows back to shore.\nAnother form of tidal energy is dynamic tidal power that requires the construction of a long dam perpendicular to the coast that could be over 15 miles long, since the structure has to be long enough to influence tidal patterns and cause high and low tides to occur simultaneously on opposite sides of the structure.\nBelow is a cut-away of a dam caisson showing a turbine that generates electricity with the flow of water between one side of the dam and the other.\nDynamic tidal power does not require the enclosure of a basin, which reduces its environmental impact. This long caisson would interfere with the coastal parallel tidal wave hydrodynamics, creating water level differences on opposite sides of the barrier which drive a series of bi-directional turbines, which are installed in the caisson. Oscillating tidal waves which run along the coasts of the continental shelves contain powerful hydraulic currents.\nIt is estimated that some of the largest dams could accommodate over 15 GW (15000 MW) of installed capacity. If the average American household consumes 12,000 KWh per year, one dynamic tidal power dam could supply energy for about 1.5 million households.\nA marine current turbine looks like a smaller version of a land windmill and the principle is precisely the same. But whereas a windmill draws energy from the movement of air, the marine turbine uses currents in the water. A single 30 foot long rotor blade will be able to produce 300 kilowatts of electricity.\nSince the ocean currents are more reliable than wind, marine turbines are a rival to wind power. They are also less obtrusive, since the structure is built on the seabed and projects just a few feet above the surface. Fish can feel safe because the blades rotate slowly at only 20 revolutions per minute.\nThe wave snake is a device that is a big red cylindrical tube that is 426.5 feet long, 13 feet in diameter, weighing around 750 tons. The snake has a life expectancy of up to 20 years, with tubes that are connected by hinges so that they float like a snake in the water. The snake rises up and down as the passing waves tug on the hinges, which are resisted by hydraulic rams, which pump high-pressure fluid through hydraulic motors and turn electrical generators to produce electricity. The energy, which comes from the joints on each wave-energy conversion, is fed via a cable to a central undersea export cable, which carries the combined power generated from a nest of snakes to shore.\nOcean thermal energy conversion (OTEC) refers to technologies that utilize the temperature differential between the upper and lower layers of the ocean to drive a heat engine. In geographical areas with warm surface water and cold deep water, the temperature difference of at least 38 degrees Fahrenheit can be leveraged to drive a steam cycle that turns a turbine and produces power. Warm surface sea water passes through a heat exchanger, vaporizing a low boiling point working fluid to drive a turbine generator, producing electricity. These power plants use the difference in temperature to make energy.\nThe water on the ocean’s surface is used to heat a pressurized liquid, usually ammonia, which boils at a temperature slightly below that of warm seawater. That liquid becomes gas, which powers a turbine generator. Cold water is then pumped from the ocean’s depths through a long submerged pipe in order to condense the gas back into a liquid, and the cycle is repeated.\nThe best ocean thermal resources are located in tropical and subtropical deep waters. Ocean thermal energy systems have three cost fundamentals, which are the platform, the cold water pipe, and the heat exchangers. Prefabricated cold water pipes made from inexpensive, lightweight composite materials, have the potential to appreciably lower the cost of OTEC technology, along with new platform construction techniques.\nWith the increasing demand for energy, the traditional fossil energy sources, such as coal, oil, natural gas, will eventually be depleted. Plus these same fossil fuels lead to many environmental issues. People are becoming aware of the importance of ocean energy, which will lead to a new era of alternative energy. Because of this knowledge, tidal energy has great potential and bright prospects in future.\nFor further information:\nThe content & opinions in this article are the author’s and do not necessarily represent the views of AltEnergyMag\nThis post does not have any comments. Be the first to leave a comment below.\nPost A Comment\nYou must be logged in before you can post a comment. Login now.""]"	['<urn:uuid:8580c709-782b-42aa-ac5b-cd640eb33cc1>']	factoid	with-premise	concise-and-natural	similar-to-document	expert	2025-04-22T14:43:01.810270	12	15	1310
13	explain main differences between computer simulations and bayesian statistics method	Stochastic simulation samples from an unconditional probability distribution and does not assert a prior. In contrast, Bayesian analysis often uses the Metropolis Hastings method to approximate the marginal distribution function and involves the accept-reject test to sample from a complicated target distribution. While stochastic processes are not inherently part of Bayesian analysis, if you formulate the stochastic process in terms of evolution of probability density functions, with the prior being the historical density function, then the problem formulation becomes similar.	"[""Deep probabilistic programming (DPP) combines three fields: Bayesian statistics and machine learning, deep learning (DL), and probabilistic programming. In this webinar, our expert panel discussed DPP tools and related theory relevant for Bayesian forecasting and decision making with financial time series data and other types of financial data (e.g. limit order books, news etc).\n- How does DPP differ conceptually from frequentist statistics and machine learning?\n- Why represent probabilistic models as a computational graph?\n- What are the DPP tools, methodologies and applications that are most important for finance?\n- Is DPP the future for risk modeling using complex datasets?\nHear it from the experts...\nThe panel summarize their answers to the audience Q&A.\nIs deep learning a type of deep probabilistic programming method?\nStrictly, deep learning by itself, as it commonly known is not a deep probabilistic programming method. Deep learning is an example of a deterministic method - it is purely algorithmic and not probabilistic. However, the types of data representations that deep learning permits are central constructs in DPP. DPP is really a combination of deep learning and probabilistic programming.\nHow do you model graphical relationships in financial data?\nGraphical relationships are based on subjective causal relationships. X caused Y. In many cases, this causal relationship requires fundamental knowledge of the asset. For example, the effects of increased oil pipeline maintenance costs on the price of WTI crude. See here for further examples: https://kuscholarworks.ku.edu/bitstream/handle/1808/161/CF99.pdf;jsessionid=0ACADE1EA67D04B14C25BC8060F7B0F0?sequence=1\nIt also possible to infer the graphical relationship (identification from a set of different structures) through a maximum likelihood estimate. This, of course, is challenging and will lead to a non-unique solution. Riccardo Rebonata recently wrote a book on coherent stress testing covering this area: http://onlinelibrary.wiley.com/book/10.1002/9781118374719\nDo you have any suggestions when choosing a suitable online platform to do deep learning?\nCloud hosted services with a jupyter notebook and tensorflow are powerful and flexible. In general, it's important to be able to have programmatic control of the data input and manipulation, rather than using a GUI. An important aspect of machine learning is how to provide the input to the machine and this is difficult to scale with GUIs.\nWhat are your thoughts on combining RNNs and financial time series modelling with DPP? Do you foresee an advantage in training Baysian RNNs vs classical time series models?\nThis is an excellent direction. The main advantage is having uncertainty estimates with the time series prediction, rather than just predicting the expected prices. DPP enables a probabilistic representation of the time series prediction.\nHow can we trust our predictive uncertainty estimates/posterior predictive distributions in a practical financial setting, given well-known problems with common approximate inference algorithms like VI and HMC ? (mode-seeking, high variance, difficulty in capturing multi-model posteriors etc.)\nThis is an important question and the problems that you allude to present many challenges for researchers in this area. The short answer to your question is that one must perform statistical tests to check that the posterior distribution captured the tails correctly. QQ-plots, KS, Shapiro-Wilks etc are some of the tests that can be performed on the marginals.\nHow is bayesian analyses which has a prior different from a stochastic simulation, which also tries to spans the search space?\nStochastic simulation samples from an unconditional probability distribution and does not assert a prior. Bayesian analysis often uses the Metropolis Hastings method to approximate the marginal distribution function and involves the use of the accept-reject test to sample from a much more complicated target distribution. There are a whole suite of so called variational Bayesian methods which are well known to be equivalent, under certain restrictive assumptions, to other techniques, e.g. particle-filter methods etc. Also Bayesian analysis isn't about using stochastic processes per se (which is often how monte carlo methods are used in quant finance), although if you formulate the stochastic process in terms of evolution of probability density functions, with the prior being chosen as the historical density function, then the problem formulation seems very similar.""]"	['<urn:uuid:360f9128-caf5-4fbe-9579-1d42870aab65>']	open-ended	direct	long-search-query	distant-from-document	novice	2025-04-22T14:43:01.810270	10	80	664
14	urban bees nyc red honey cause 2010	In the summer of 2010, mysterious red honey appeared in New York City bee hives. Initially thought to be from sumac plants, testing revealed the honey contained Red Dye No. 40. The bees had been collecting sugar syrup from a Maraschino cherry factory in Brooklyn, rather than foraging on flowers. The honey was originally intended for a local restaurant before the artificial source was discovered.	['Journal of Urban Ecology\nThe Journal of Urban Ecology covers all aspects of urban environments. This includes the biology of the organisms that inhabit urban areas, human social issues encountered within\nMysterious red honey began to appear in the hives of New York City bees in the summer of 2010. At first, beekeepers thought their bees were foraging on some strange plant, possibly sumac. But after more beekeepers began to find red honey in their hives, they decided to get their honey tested. As it turned out, the honey was filled with Red Dye No. 40, and instead of foraging on flowers, the bees had been collecting sugar syrup from a Maraschino cherry factory in Brooklyn.\nThe story of New York’s red honey struck a chord with those already concerned about honey bee health. Bees have been hit hard by a host of challenges ranging from parasitic mites to neonicotenoid pesticides—but could red honey be another sign of bee decline? Could artificial flavors and chemicals in human foods be toxic to bees? Could we be at risk if we eat “local honey”?\nRooftop hive at the American Tobacco Campus in Durham, NC, managed by Bee Downtown. Photo by Lauren Nichols and used with permission.\nAs people were asking questions about New York’s bees, my lab group was studying another insect in New York—ants. Over 8.9 million people live in New York, but there are at least 16 billion ants. That means for each person living in New York there are nearly 2,000 ants. In an area that is almost 90% concrete, how could so many ants survive? The secret, we thought, might lie in what was happening with New York’s bees. Rather than feeding on dead insects and other “natural” foods, we guessed ants might be switching to human foods.\nThe average person living in a city produces 1,000 pounds of garbage each year, and of that, 15% is food waste. With over half the world’s population now living in cities, this amounts to 250 million tons of food thrown out in cities each year, which represents a massive potential resource for urban animals. We found that much of this food in New York was making it into urban ant colonies. This was especially true in the most urban areas of the city, like the sidewalks running down Broadway. A follow-up study found that ants living on Broadway alone consumed the equivalent of 60,000 hotdogs per year—more than city rats or birds consumed in the same area.\nBut what about urban bees? Anecdotes about red honey aside, no serious investigation had been carried out on the diet of urban bees. Over the last decade, cities began to change local ordinances that had previously outlawed beekeeping inside city limits, and urban beekeeping has become increasingly popular. Along with backyard chickens and rooftop gardens, urban beekeeping has become a major part of the local food movement. In fact, the red honey discovered in New York was intended for a local restaurant until it turned out to be recycled sugar syrup. Despite growing interests in urban bees, there was still no clear answer as to whether bees were sticking to flower nectar or finding new sugar sources in cities.\nUsing the same techniques we used to study ants in New York, we began studying the diet of bees in Raleigh, North Carolina. Human-produced sugars, like sugarcane and high-fructose corn syrup, have a characteristic carbon isotope signature that can be used to determine if bees are feeding on flower nectar or, say, someone’s leftover soda. Bees in rural areas should only have access to flower nectar, but if city bees are feeding on human food sources, then their carbon isotope signature should show it.\nTo our surprise, we found no difference in carbon isotopes between bees living in downtown Raleigh and those living outside the city. In both habitats, bees were sticking to flower nectar and largely avoiding human sugar sources. This is good news for urban beekeepers and people who buy local honey, and it also shows that urban flowers play a major role in maintaining healthy pollinator populations in cities.\nIn the future, we plan to partner with urban beekeepers in larger cities, like New York and Tokyo, to see if bees are still able to sustain their colonies on local flowers. Will we find bees similar to those in Raleigh, or will we uncover new mysterious shades of honey inside the hives of big-city bees?']	['<urn:uuid:76c368d9-5231-4f76-80bd-dae395e35288>']	open-ended	with-premise	short-search-query	similar-to-document	expert	2025-04-22T14:43:01.810270	7	65	744
15	tools needed for efficient quran translation process	Translation memories and term bases are essential tools for efficient translation of Qur'anic Studies sources. Without these tools, maximum efficiency in translating from/into Persian cannot be achieved, as they facilitate decision-making in the translation process.	['A Model for Crowdsourcing Development of Databases for Qur’anic Studies Sources\nTechnology has become an integral part of the translation task. Nevertheless, few translation memories and term bases are available for translating Qur’anic Studies sources. Without them, attaining maximum efficiency in this field is not possible because such tools facilitate decision-making in the translation process from/into Persian. There is an imperative need for developing such databases. Creating parallel corpora and aligning them to come up with translation memories and term banks can help improve the quantity and quality of translations of Qur’anic Studies sources from/into Persian. However, this task cannot be carried out by a single person. Using crowdsourcing in developing TMs and TBs for Qur’anic Studies sources is an alternative that can expedite the task. Nonetheless, crowdsourcing in developing such databases is a relatively unattended research area. Examining existing models revealed that no pre-existing Translation Studies model suited the needs of this study. With the motive of filling this gap, the researchers opted for developing and validating a model for human resource management in Translation Studies through adopting a crowdsourcing model (the Metropolis Model) and adapting it for their specific conditions (developingthe Jāmiʿ model). Findings of this research indicate that the Jāmiʿ Model is adequate for developing TMs and TBs.\nBarlas, Y., & Carpenter, S. (1990). Philosophical Roots of Model Validation: Two Paradigms. System Dynamics Review, Vol. 6, No. 2, pp. 148–166.\nBowker, L. (2015). Computer-Aided Translation Translator Training. In Chan Sin-Wai (ed.), the Routledge Encyclopedia of Translation Technology. London and New York: Routledge.\nBrabham, D. C. (2008). Crowdsourcing as a model for problem solving: An introduction and cases. Convergence: The International Journal of Research into New Media Technologies, 14, 75–90.\nCronin, M. (2010).The translation crowd. Revista tradumàtica, 8.\nDePalma, D. A. & N. Kelly. (2011). ‘Project Management for Crowdsourced Translation: How User-translated Content Projects Work in Real Life’, in K. J. Dunne & E. S. Dunne (Eds.) Translation and Localization Project Management: The Art of the Possible (pp. 379–407). Amsterdam and Philadelphia: John Benjamins.\nDésilets, A. (2007). Translation Wikified: How Will Massive Online Collaboration Impact the World of Translation? In Proceedings of the Translating and the Computer 29 Conference, 29–30 November 2007, ASLIB, London, UK.\nDombek, M. (2014). A Study into the Motivations of Internet Users Contributing to Translation Crowdsourcing: The Case of Polish Facebook User-Translators. Unpublished Doctoral Thesis. Dublin City University.\nDunne, K. & E. S. Dunne. (2011). Translation and Localization Project Management: The Art of the Possible. Amsterdam and Philadelphia: John Benjamins.\nGarcia, I. (2015). Cloud Marketplaces: Procurement of Translators in the Age of Social Media, the Journal of Specialised Translation, No. 23, pp 18 - 38.\nHowe, J. (2006). The Rise of Crowdsourcing. Wired 14 (6): 176–183. http://www.wired.com /wired/archive/14.06/crowds.html\nJiménez-Crespo, M. A. (2017). How much would you like to pay? Reframing and expanding the notion of translation quality through crowdsourcing and volunteer approaches. Perspectives. Studies in Translation Theory and Practice, 25(3). 478–491.\nKazman, R., & Chen, H. (2009). The Metropolis Model A New Logic For Development Of Crowdsourced Systems. Communications of the ACM, 52(7), 76–84.\nMcDonough, J. (2012).Analyzing the Crowdsourcing Model and Its Impact on Public Perceptions of Translation. The Translator, 18 (2), 167–191.\nO’Hagan, M. 2016. Massively Open Translation: Unpacking the Relationship Between Technology and Translation in the 21st Century. International Journal of Communication, 10, 929–946 1932–8036/20160005.\nSurowiecki, J. (2004). The Wisdom of Crowds: Why the Many are Smarter than the Few and How Collective Wisdom Shapes Business, Economies, Societies, and Nations. New York: Doubleday.\nViitamaki, S. (2007). The FLIRT Model of Crowdsourcing Collective Customer Collaboration. Retrieved from http://www.samiviitamaki.com/2007/02/16/the-flirt-model']	['<urn:uuid:318b4217-be47-42e5-b54a-3097a587087c>']	factoid	with-premise	short-search-query	distant-from-document	expert	2025-04-22T14:43:01.810270	7	35	595
16	How big is the tide variation in French Polynesia?	The tidal variation in French Polynesia is only about 1-2 feet.	['Many sailors crossing the Pacific choose not to stop in many of the Tuamotos atolls due to the challenge of navigating the reef passes — the currents can be extremely dangerous, flowing up to 10 knots, occasionally creating standing waves to 2-3 feet.\nPredicting when it is safe to navigate a reef pass can be tricky. Sailors often talk about “timing the slack tide” in Tuamotos passes, but this is somewhat misleading. What is really important is the overall current in the pass– which is caused by a combination of tide, swell and wind.\nThe tide has the most obvious effect on the current. Luckily the tidal variation in French Polynesia is only about 1-2ft, but the atolls are so large that a formidable amount of water still needs to move in and out of the lagoon twice a day.\nSlack tide is normally the calmest time in a reef pass – assuming there is no wind and swell. The wind can seriously disturb the pass when it is moving in the opposite direction to a strong current, resulting in confused standing waves. So we avoid transiting the pass when wind and current are in opposition.\nAn even more significant hazard for the pass, however, are the swell conditions. If there’s a moderate to large swell (2m+) there is a huge amount of water that is poured into the lagoon. Satellite images and nautical charts make it seem like the atolls are a complete ring of land, but the majority of the ring is actually submerged reef with a peppering of motus ( little islands of land in the barrier reef).\nWaves can come from groundswell (far away storms, with large periods) or windswell (localized strong winds with short periods) such as the ‘Maraamu’ South-East trade winds that blow 25knots+. The extra water these swells push into the lagoon must escape through the reef pass. This creates an additional outgoing current of 1-6 knots.\nWhen this swell-driven outgoing current combines with an outflowing tide, a river of water can flow towards the ocean at an incredible 8-10 knots! Even a big ship might have a hard time during such conditions.\nThe best time to enter a pass during such a swell event is usually during the peak of an incoming tide, so that it neutralizes the outgoing current. Then, the outgoing current may be a reasonable 1-2 knots. Therefore, in the case of swell events, slack current is NOT slack tide.\nNote: swell can also create large waves that break next to narrow reef passes making them difficult to impossible – like Maupiti in the western Society Islands. This is an entirely different problem, but still relevant for navigation!\nNaturally, it’s best to travel across reef passes when the swell and wind is moderate, and tide is slack. But we don’t always have that luxury. Being able to factor for the effects of swell and wind is critical for the safe navigation of reef passes in less-than-ideal conditions.\nPHOTO: shows the Tiputa pass in Rangiroa. It is a 40 mile wide atoll with a huge amount of water moving in and out of the pass, generating large standing waves which can be seen on satellite images. This famously attracts dolphins, which divers come to swim with. We entered Tiputa pass after waiting an hour for the tide to shift, and still the outgoing current was nearly 6 knots. However we were blessed with an amazing moment when a huge dolphin jumped directly in front of Aldebaran as we were barely moving forward with the engine at full throttle!']	['<urn:uuid:f03ad46a-96b1-4be4-81b7-3ad0e0266080>']	factoid	direct	concise-and-natural	similar-to-document	novice	2025-04-22T14:43:01.810270	9	11	598
17	food supplements vs natural foods for bones	While supplements like vitamin D, calcium, magnesium, and vitamin K2 are commonly taken for bone health, getting nutrients from natural food sources is ideal. For vitamin B12, which is crucial for bone health, the best natural sources include wild-caught Alaskan salmon, raw milk, pastured free-range eggs, grass-fed beef, and beef liver. However, people over 50 may have difficulty absorbing B12 from food due to decreased stomach acid production, so they might benefit from sublingual supplements or injections. Oral B12 supplements are generally ineffective due to poor absorption of the large B12 molecule.	['Vitamin B12 Deficiency and Bone Health\nVitamin B12 Deficiency and Bone Health\nStudies from the US Framingham trial show nearly two-fifths of the US population may have suboptimal blood levels of vitamin B12.1 And the criteria they use to make this assessment is 6-700 pg/ml, so it may be the majority of people who are vitamin B12 deficient.\nThis is important to be aware of, and correct if it applies to you, as vitamin B12 is important for the formation of red blood cells, the maintenance of your central nervous system, and plays a role in the production of DNA and RNA.\nVitamin B12 is also fittingly known as the energy vitamin, and your body requires it for a number of vital functions, including energy production. Much less is known about its role in bone health, although it’s emerging as an important player.\nEven though vitamin B12 is water-soluble, it doesn’t exit your body quickly in your urine like other water-soluble vitamins. Instead, B12 is stored in your liver, kidneys, and other body tissues.\nAs a result, a deficiency may not show itself until up to seven years later, and by this time damage to your bones may have already set in…\nVitamin B12 Deficiency May Harm Your Bones\nResearch published in the New England Journal of Medicine (NEJM) revealed that mice deficient in vitamin B12 have growth retardation and fewer osteoblasts (cells responsible for bone formation).2\nThe researchers suggested that lack of vitamin B12 may interfere with growth signaling in the liver and its “downstream effect” on the osteoblasts.\nRecent research also suggests low vitamin B12 status may increase the risk for bone fractures in older men.3 This risk remained even after taking into account other important factors such as smoking, vitamin D status, and calcium intake. As reported by MedicineNet:4\n“Men in the group with the lowest B-12 levels were about 70 percent more likely to have suffered a fracture than others in the study. This increased risk was primarily due to fractures in the lumbar spine, where there was an up to 120 percent greater chance of fractures.”\nOlder women with low levels of vitamin B12 (below 208 pg/ml) also experienced significantly more rapid hip bone loss – a sign of osteoporosis – than women with higher levels of B12 in a separate study.5\nElevated homocysteine levels (an amino acid) and low vitamin B12 have also been associated with deteriorated bone health,6 and this may be one avenue by which B12 influences bone health (B vitamins are known to suppress homocysteine). As explained by the Linus Pauling Institute:7\n“High homocysteine levels may affect bone remodeling by increasing bone resorption (breakdown), decreasing bone formation, and reducing bone blood flow.\nAnother proposed mechanism involves the binding of homocysteine to the collagenous matrix of bone, which may modify collagen properties and reduce bone strength.\nSince vitamin B12 is a determinant of homocysteine metabolism, it was suggested that the risk of osteoporotic fractures in older subjects might be enhanced by vitamin B12 deficiency.”\nIndeed, a meta-analysis found that raising vitamin B12 levels in older individuals lead to a reduction in fracture risk.8\nMove Over Vitamin D and Calcium, Are You Getting Enough Vitamin B12?\nVitamin D, calcium, magnesium, and vitamin K2 are crucial for bone health, but also are among the most common supplements taken by older adults for this very reason. Vitamin B12, on the other hand, is often overlooked.\nIf you’re a vegan who does not eat animal products, you are at high risk of deficiency, as B12 is readily bioavailable in its natural form only in animal food sources. This doesn’t necessarily have to be meat — eggs and dairy are options also. Top foods to include are:\n- Wild-caught Alaskan salmon\n- Raw milk\n- Pastured free-range eggs\nIf you do consume animal products, then consider adding these foods that are even higher in vitamin B12:\n- Grass-fed beef and beef liver\n- Pastured organic free-range chicken\nHowever, keep in mind that even if you do eat animal foods, a supplement can be beneficial if your body’s ability to absorb the vitamin from food is compromised, which is especially prevalent as you age.\nWhen you get older, the lining of your stomach gradually loses its ability to produce hydrochloric acid (the stomach acid suppressed by proton pump inhibitors), which releases vitamin B12 from your food. If you’re over 50, it’s safe to assume you are not absorbing vitamin B12 at an optimal level.\nHowever, just because you are over 50, it doesn’t mean you are deficient, it only means your risk increases. If you are eating a healthy diet, you can easily maintain healthy levels. The only way to know for sure is to get your blood tested.\nNormal ranges of B12 are 200-1,100 pg/ml. Even though the lower level of normal is 200, if you are below 600, you might be suffering from B12 deficiency. There is one problem with supplementation however, and it’s related to the poor absorbability of oral vitamin B12 supplements.\nVitamin B12 is the largest vitamin molecule known. Because of its large size, it is not easily absorbed passively like most supplements. This is why many, if not most, oral B12 supplements are grossly ineffective. The only effective form of B12 supplementation is IM injections or sublingual administration.\nThat said, B12 supplements are exceptionally safe, with virtually no known side effects. Just avoid oral B12 supplements, as they will not be readily absorbed. Injections or a sublingual (under your tongue) spray work far better, as they allow the large B12 molecule to be absorbed directly into your bloodstream.\nWhat Else Do You Risk by Not Getting Enough Vitamin B12?\nYour risk of fracture might increase, but that’s not all. Some of the initial signs of B12 deficiency will often include mood changes, such as lack of motivation or feelings of apathy. Low levels can also lead to mental fogginess, memory troubles, muscle weakness, and — one of the hallmark signs — fatigue. Vitamin B12 also plays a role in:\n|Proper digestion, food absorption, iron use, and carbohydrate and fat metabolism||Healthy nervous system function||Promotion of normal nerve growth and development|\n|Help with regulation of the formation of red blood cells||Cell formation and longevity||Proper circulation|\n|Adrenal hormone production||Healthy immune system function||Support of female reproductive health and pregnancy|\n|Feelings of well-being and mood regulation||Mental clarity, concentration, and memory function||Physical, emotional, and mental energy|\nVitamin B12’s role in brain health and mental health is particularly significant. According to a small Finnish study published in the journal Neurology, people who consume foods rich in B12 may reduce their risk of Alzheimer’s in their later years.9 For each unit increase in the marker of vitamin B12 (holotranscobalamin), the risk of developing Alzheimer’s was reduced by 2 percent.\nMeanwhile, B group vitamins may slow brain shrinkage by as much as seven-fold in brain regions specifically known to be most impacted by Alzheimer’s disease.10 Among participants taking high doses of folic acid and vitamins B6 and B12, blood levels of homocysteine were lowered, as was the associated brain shrinkage – by up to 90 percent. As discussed by Dr. Kelly Brogan, MD in the video below, vitamin B12 deficiency can even cause a range of neurological disturbances that mimic serious mental illness.\nThe Best ‘Recipe’ for Bone Health\nOne of the important strategies for healthy bones is to eat the right kind of foods. A diet full of processed foods will produce biochemical and metabolic conditions in your body that will decrease your bone density, so avoiding processed foods is definitely the first step in the right direction. This goes far beyond calcium, which is the first nutrient many people think of concerning their bones. Your bones are actually composed of several different minerals, and if you focus on calcium alone, you will likely weaken your bones and increase your risk of osteoporosis, as Dr. Robert Thompson explains in his book, The Calcium Lie.\nCalcium, vitamins D and K2, and magnesium work synergistically together to promote strong, healthy bones, and your sodium to potassium ratio also plays an important role in maintaining your bone mass (larger amounts of potassium in relation to sodium is optimal for your bone health and your overall health). Ideally, you’d get all or most of these nutrients, including vitamin B12, from your diet (with the exception of vitamin D). This includes:\n- Plant-derived calcium: Raw milk from pasture-raised cows (who eat the plants), leafy green vegetables, the pith of citrus fruits, carob, and sesame seeds\n- Magnesium: Raw organic cacao and supplemental magnesium threonate if need be\n- Vitamin K2: Grass-fed organic animal products (i.e. eggs, butter, and dairy), certain fermented foods such as natto, or vegetables fermented using a starter culture of vitamin K2-producing bacteria, and certain cheeses such as Brie and Gouda\n- Trace minerals: Himalayan Crystal Salt, which contains all 84 elements found in your body, or other natural, unprocessed salt (NOT regular table salt!)\n- Vitamin D: Ideally from appropriate sun exposure (or a safe tanning bed), as it’s virtually impossible to get sufficient amounts from food. As a last resort, you could use a supplement, but if you do, you may also need to supplement with vitamin K2 to maintain ideal ratios\nRemember to Exercise Regularly and Use Strength Training\nAlthough the focus of this article is on nutrition, the other component you can’t ignore if you want strong, healthy bones is weight-bearing exercises like strength training. Bone building is a dynamic process, so you want to make sure you exert enough force on your bones to stimulate the osteoblasts to build new bone. Further, bone is living tissue that requires regular physical activity in order to renew and rebuild itself, it is important to make exercise a lifelong commitment. Peak bone mass is achieved in adulthood and then begins a slow decline, but exercise can help you to maintain healthy bone mass as you get older, and should be viewed as a bone-building partner to your healthy diet.\nWeight-bearing exercise is actually one of the most effective remedies against osteoporosis, because as you put more tension on your muscles it puts more pressure on your bones, which then respond by continuously creating fresh, new bone. In addition, as you build more muscle, and make the muscle that you already have stronger, you also put more constant pressure on your bones. A good weight-bearing exercise to incorporate into your routine (depending on your current level of fitness, of course) is a walking lunge, as it helps build bone density in your hips, even without any additional weights.\nIn addition, Acceleration Training, a.k.a. Whole Body Vibrational Training (WBVT) using a Power Plate, has also been shown to be a safe, natural way to ward off osteoporosis, and it’s gentle enough even for the disabled and elderly. Research shows vibrational training may help to produce a significant increase in bone density in postmenopausal women,11 making it another valuable tool for bone health.']	['<urn:uuid:a5187e6e-7406-45d5-9c72-3f614d90064a>']	open-ended	direct	short-search-query	distant-from-document	novice	2025-04-22T14:43:01.810270	7	92	1830
18	I keep hearing about some new bus service coming to Indianapolis that's supposed to be special. What makes it different from regular bus lines?	Indianapolis is building the nation's first all-electric Bus Rapid Transit (BRT) system called the Red Line. This $96 million project will feature dedicated lanes, pre-board ticket purchase, and buses running every ten minutes during the day for 20 hours daily, seven days a week. The route will connect the University of Indianapolis to Broad Ripple, coming within a quarter mile of more than 50,000 residents and nearly 150,000 jobs.	['As home of “The Greatest Spectacle in Racing” and the “Crossroads of America”, Indianapolis is reinventing itself in the 21st century with a batch of high-profile projects that are catching the attention of the rest of the country. As Governor Eric Holcomb has stated, Indy is quickly becoming somewhere “people can’t afford to miss”.\nWhile Indianapolis hasn’t built over 35 stories in nearly 30 years, that is poised to change in a big way. Huge chunks of downtown-adjacent property that have sat derelict for years will soon be replaced by neighborhood-sized projects that are sure to reinvent Center Township.\nAnd with the nation’s first all-electric BRT service finally under construction, the third decade of the 21st century is shaping up to be an exciting time for metropolitan Hoosiers.\nWhite River Vision Plan\nDragon Boat races in the White River? What about urban kayaking tours from Broad Ripple to downtown — along with an activated riverfront with unbridled recreation opportunities and improved pedestrian accessibility, teeming with tourists and locals alike? This may just be a possibility in store if the White River Vision Plan is implemented. After the progress of DigIndy, a massive tunneling project diverting storm water from the White River, the White River is yet again safe for recreation, and the possibilities are endless.\nAccording to mywhiteriver.org, the White River Vision Plan is a “joint effort between the City of Indianapolis and Hamilton County Tourism, Inc. in partnership with Visit Indy’s philanthropic arm, Tourism Tomorrow, Inc. to develop a comprehensive and coordinated regional, community-driven plan to enhance 58 miles of the White River in Marion and Hamilton counties. The goal of the vision plan is to create an accessible, recreational, and cultural environment that encourages a unique sense of place for the community as a whole”.\nOver 4,000 residents responded to a recent survey on how to develop the river, and early responses were in favor of enhanced connectivity and recreational opportunity. The project is currently in the ‘Envisioning’ stage, and concepts and designs are being developed. There are public meetings upcoming in both Hamilton and Marion counties and you can take the White River Vision Plan Survey here.\nPhoto: White River Vision Plan | mywhiteriver.org\nFormer Angie’s List Campus Redevelopment on East Washington Street\nStatus: Proposed/Under Construction\nAngie’s List announcing a leave from their campus on East Washington Street came as a surprise to some — but their merger with HomeAdvisor was a long time coming. With Angie’s hemorrhaging financially for months and enduring intense layoffs, questions with what to do with their sprawling 17.5 acre campus immediately came to the forefront in the aftermath of their exodus.\nHowever, the loss may be a blessing in disguise for the burgeoning Holy Cross neighborhood. Often times, the collection of buildings sandwiched between Market Street and Washington just east of Interstate 65/70 became a ghost town after Angie’s workers left. And while the company’s restoration and use of older buildings was novel in concept, without active use by the public, the buildings acted more as caricatures of what they were — rather than actually providing real use.\nEnter Bill Oesterle, co-founder of Angie’s List. Head of Fred Abel LLC, an investment group, Oesterle acquired the 25-building campus early in 2018 and sees the area becoming a mixed-use neighborhood, with tech start-ups, apartments, and lively shops. Given the parcel’s location between downtown and an impoverished east-side, his plan has a chance to rejuvenate and rebuild a historic business node that was perhaps underutilized by Angie’s List. Add in the proposed Blue Line BRT along Washington Street, redevelopment of the nearby old Ford plant, as well as increasing density in the Market East district and you have the recipe for one of Indy’s coolest new n’hoods.\nAccording to Indy Star, Oesterle hopes to start attracting tenants by the end of 2018.\nPhoto: East Washington St.| IndyStar\nPan Am Plaza Redevelopment\nSafe to say, one of the ripest sites for redevelopment downtown Indianapolis has been Pan Am Plaza. Originally built for the 1987 Pan Am games, the parcel sits adjacent to both the Indiana Convention Center and revitalized Georgia Street. While it may have hosted ESPN during Indy’s Super Bowl, the plaza remains vastly underutilized, and its prime location alongside Illinois Street has long made it a candidate for various development proposals. Finally, Kite Realty Group Trust broke through with a proposal that stands an excellent chance of getting done.\nAdding a combined 1,400 hotel rooms along with a publicly-funded $120 million ballroom expansion of the Indiana Convention Center, the major element of Kite’s proposal is a Ratio-designed 38 story, 800 room Hilton-brand hotel that would either be the third or fourth tallest building in Indianapolis once completed.\nKite also plans to build a second, smaller Hilton high-rise hotel on the southeast edge of the site, which would offer around 600 rooms. The expansion of the convention center, a publicly-funded 50,000 sq. ft. ballroom — the largest in the city — would connect to both hotels and be directly linked to the rest of the convention center via skyway. City leaders have repeatedly stated that expanding the convention center is paramount to attracting and retaining the money-making conventions which have called Indianapolis home, such as FFA and GenCon.\nIf Kite’s hotel plan seemingly isn’t as ‘transformative’ as others on this list, it belongs due to the potential impact the 38-story hotel will have on Indy’s skyline. While building up has seen to be a hindrance for Indy developers, this project, along with CityWay’s 11 story expansion, will further grow the skyline in a southern direction. The end result will be a fuller block of towers, alongside the big three of Regions, OneAmerica, and Salesforce. It’s nice to see Indy “make it” so to say, and join some of its regional cities in building taller.\nWhile the project will require tearing down of the existing underground parking garage, site clearance and acquisition, and city approvals, it seems there is good faith this gets done — GenCon announced a renewal of its contract with Indianapolis through 2023, shortly after the Kite proposal became public.\nPhoto: 38 Story Hotel Proposal | Kite Realty\nStatus: Under Construction\nTo call the Bottleworks project transformative would be an understatement. The 12-acre site aims to create a wholly new neighborhood at the corner of Mass Ave and College Avenue downtown — this is ambition at its finest.\nThe $300 million mixed-use development calls not only for West Elm’s first boutique hotel (with a rooftop bar), but a food hall with dozens of vendors, 240 condominium and apartment units, an eight-screen dine-in film theater, nearly 200,000 sq. ft. of office, and 200,000 sq. ft. of retail. Developer Hendricks Commercial Properties claims the project will bring over 3,000 jobs and attract over 2 million visitors annually. Add in the retrofitting of the former Coca-Cola bottling plant and you have a project which is not only transformative, but massive in scope.\nImportantly for connectivity, the project calls for restoring a street grid to what was formerly an IPS bus parking lot, extending 9th Street and creating a street arcade along Carrolton Avenue.\nWhile retailers and tenants are currently being secured, ground broke earlier in 2018. According to Hendricks, Bottleworks will be built in phases, with street retail opening in 2020. Recently, Indianapolis tech company High Alpha announced they would be moving to the district — the first of what should be many such announcements.\nPhoto: Bottleworks seen from Mass and College | Ratio Architects\nTwin Aire Justice Complex\nStatus: Under Construction\n140 acres of the former Citizens Energy Coke & Gas Plant will become Marion County’s new courthouse and jail, replacing the antiquated and un-streamlined hodgepodge of criminal justice facilities currently downtown.\nAtop continuing development of Twin Aire, just to the east of Fountain Square and part of Indy’s Great Places 2020, the Justice Center promises to bring newfound vitality to the struggling southeast-side neighborhood whose industrial anchorage long ago floated away.\nThe nearly $600 million project will house a 3,000-bed jail, a new courthouse, the sheriff’s office, and a community center which aims to remediate those with mental illness. National architectural firm HOK was in charge of designing the campus, which includes an 11-story courts building surrounding a shorter building housing the jail.\nAs part of the deal, Citizens Energy will be planting 1,000 trees, and the entrance to the complex will feature a large roundabout akin to Monument Circle.\nThere is a hefty amount of bail-bonds, court offices and businesses centered on Delaware Street near the current courts downtown. Many expect these businesses to relocate once the new justice center opens. While some question what the future of Delaware Street will be if and when all the lawyers leave, an exciting prospect exists to think what a reimagined Delaware Street looks like –come alive from the bonds of bailsman and court-catering traffic and adjacent to the trendy Market East district.\nThe center is looking to open in 2021, though delays are always possible with a project of this magnitude.\nPhoto: Criminal Justice Center | City of Indianapolis\nStatus: Proposed, Under Construction\nThis massive, 60 acre, 6 million sq.-ft. public-private-partnership between the City of Indianapolis and Indiana University northwest of downtown off Indiana Avenue and 16th Street is coined as a “innovation-based community for researchers, entrepreneurs, and established companies”.\nEssentially creating a neighborhood, 16Tech will be a great addition to Indy’s exploding tech scene, and surely must have taken part of the city’s Amazon bid. With proximity to major hospitals, IUPUI, and recreational trails, 16Tech looks poised to attract and retain creative workers.\nWhen complete, the neighborhood will include a 120,000 sq. ft. anchor building, a 250-unit apartment complex, renovated office space formerly occupied by Citizens Energy Group; restaurant and retail space, and a number of infrastructure improvements, including a central greenway and a new bridge across Fall Creek that will connect the neighborhood with IUPUI, Eskenazi and Riley hospitals.\nThe anchor building is expected to be complete in 2020, to become home to Indiana Biosciences Research Institute, a $360-million independent research facility. While the magnitude of 16Tech is immense, progress is occurring slowly but surely. In March 2018, 16Tech announced a $38 million grant from Eli Lilly to fund an initial stage, and that Indy-based commercial real estate firm Browning will spend over $120 million to build three new buildings and renovate an existing structure on 11 acres along Waterway Boulevard.\nPhoto: 16Tech | Browning\nAmbrose Redevelopment of GM Stamping Plant (Waterside)\nThis would get the top billing on this list if not for the larger impact of another project, but Ambrose Property Group’s Waterside neighborhood easily tops the list for most potential impact by a single real estate development.\nThe numbers are staggering: a $1.3 billion dollar investment offering 1,350 residential units, 620 hotel rooms, 2.75 million sq. ft. of office space, 100,000 sq. ft. of retail and 12,000 jobs. If Bottleworks is ambitious, then Waterside is ridiculous – not just in its chutzpah, but in its sheer scope.\nWhile initial investment on the site was earlier estimated around $500 million, Ambrose recently upped that number to $1.3 billion due to an expanded vision of the 103+ acre area. Beside restoring the Albert Kahn-designed crane bay to its former glory, Ambrose intends to redevelop all the surrounding land over the next 15 years, creating an entirely new mixed-use neighborhood of walkable multi-modal streets, mid-rises, apartments, single and multi-family housing, office space, and abundant recreation opportunities alongside the White River.\nThe third of Indy’s neighborhood-sized mega-projects alongside 16Tech and Bottleworks, Waterside has long been eyed for Indy’s Amazon bid – although, it is important to note that Ambrose has said repeatedly that Waterside is getting done with or without Mr. Bezos. By incorporating a street grid and fostering connectivity through a pedestrian bridge over the river, Ambrose is overtly attempting to seam together Waterside with the nearby and overlooked neighborhood, The Valley.\nWhile the planning and approval process takes shape, infrastructure improvements are slated to begin in earnest during 2019, at which point White River Parkway will be rerouted through the middle of the development, allowing its former riverside right-of-way to be repurposed for recreation and pedestrian accessibility.\nWhile the dollars haven’t flowed across the river during downtown’s building boom, that is about to change in a big way. And while decades of deindustrialization, crime, and disinvestment on the Westside may have scared developers away, Waterside, along with the proposed Blue Line BRT, will assuredly rewrite that narrative.\nGet ready, folks — here comes the Westside.\nPhoto: Waterside | Ambrose Property Group\nRed Line Bus Rapid Transit\nStatus: Under Construction\nWelcome to the 21st century, Indianapolis. In a city that has long suffered with one of the least reliable transit systems in the U.S., the paradigm is quickly shifting. Soon, Indianapolis will not only be home to “The Greatest Spectacle in Racing” but also the first all-electric BRT system in the country. And it’s suddenly looking bright for those who desire multi-modal options in car-centric Central Indiana.\nThe $96 million project, slated to open in 2019 and featuring frequent bus service from the University of Indianapolis to Broad Ripple, will use dedicated lanes, pre-board ticket purchase and street reconfiguration to expedite service, in which buses will generally come every ten minutes during the day. According to IndyGo, the route “will come within a quarter mile of more than 50,000 residents and nearly 150,000 jobs – a quarter of all jobs in Marion County”. The service will operate for 20 hours a day, seven days a week.\nKnow this, the Red Line is not just a big deal — it is an immense deal. In a city that has struggled with transit mobility since the decline of streetcar networks, the Red Line is poised to rebrand Indy as a transit-friendly community. The Red Line will act as the backbone of Central Indiana’s transit network, connecting neighborhoods, creating economic opportunities, and allow transit-oriented-development (TOD) to densify and rebuild historic urban nodes along the corridor. It will bring access to jobs for low-income residents and connect some of the largest employers in the state to each other.\nThe battle for BRT hasn’t been easy — many years of planning, securing grants, and plotting a successful referendum (which passed in 2016), all culminated in Indy’s success in finally bringing its residents adequate transit. Things haven’t always been rosy. Threats of lawsuit from north-side neighbors still permeate discussions about the Red Line and securing grants became an uphill battle after President Trump determined to shut down the Small Starts program that provided its funding.\nBut, for now, construction has started and transit fans will finally have their day. And despite the huge economic impact of some of the other projects on this list, the Red Line offers the most potential to remediate deepening social inequities and improve the quality of life for residents of Indianapolis.\nPhoto: Red Line in Fountain Square | IndyGo\n© Jeffery Tompkins 2018']	['<urn:uuid:f6448e3b-a750-4e11-8ebf-affad8276f5d>']	factoid	with-premise	verbose-and-natural	distant-from-document	novice	2025-04-22T14:43:01.810270	24	69	2501
19	how brain imaging technology changes understanding cognitive processes compared to behavioral studies	Brain imaging technology is providing a direct observational window into cognitive functions that was previously impossible. While behaviorism focused only on observable motor behavior due to lack of access to cognitive processes, current imaging technology allows us to see computerized representations of the working brain. Although current imaging requires laboratory settings, future portable imaging technologies are expected to show brain activity during normal environmental interactions. This represents a shift toward the same kind of direct, intimate understanding we have of our motor system.	['Observing Mobility. The recent death of the renowned evolutionary theorist, Stephen J. Gould, recalls his intriguing comment that we’re inside-out crustaceans. A crustacean’s skeleton is on the outside, ours is on the inside. Our soft tissue and appendages are out where we can readily observe them.\nLlinas (2002) expands briefly on this concept in his excellent discussion of the evolution and nature of our brain, but it’s something that’s pleasant to explore further.\nHaving an internal skeleton means that we have a direct, intimate, sensory knowledge of how our external motor system functions. From birth on, we can observe and feel muscular contractions and their relationship to body movements. We’ve created tools that accurately measure the properties of our marvelous movement system. Further, we’ve always celebrated this basic universal understanding and awe of our motor system through performance and competition.\nWell, why not? Our motor system is perhaps the definitive element of our biological self. Compare the two major biological groups, plants and animals. Plants don’t have a brain and animals do. Plants don’t have a brain because they’re not going anywhere — and if you’re not going anywhere, you don’t even need to know where you are. What’s the advantage for a rooted tree to realize that other trees are better situated, or to be able to observe approaching loggers?\nOn the other hand, if you have legs/wings/fins/etc that permit mobility, you need a sensory system to tell you about here and there. Then you need a make-up-your-mind system to decide if there is better than here, or here is better than there. Finally, you need to activate your motor system to move to there, if you’ve decided it’s better than here.\nWe spend much of our extended juvenile development period informally observing and exploring our motor system. We have to learn how to regulate and predict its movements and the movements of others (and of moving objects). It’s a complex system that must be activated for thousands of hours to reach the adult proficiency levels of complex movements. We’ve turned much of this juvenile practice activity into enjoyable games.\nOur mobility systems can even get us beyond direct physical movement. For example, our vocal apparatus can rhythmically move air molecules that hit the eardrums of others at a distance and create brain-to-brain language connections. Mastering the movements involved in spoken (and written) language is thus another major childhood task.\nMy March 2002 Brain Connection column (From Video Games to the Internet) discussed how important it is for young children to get on a tricycle at three if they hope to drive a car at 16, and to similarly begin with video games at an early age if they later hope to effectively travel the Internet.\nWe’re fascinated by those who move (or move objects) at virtuoso levels. The whole world gathers every two years to discover who can jump the highest, throw things the farthest, run or skate the fastest, ski the best. We attend concerts to observe others sing or play musical instruments, and sporting events to watch others throw balls through hoops or hit them with bats. It may seem kind of foolish, but it’s also quite human.\nObserving Cognition. Although we all develop an excellent common understanding of movement via our continuous observation of its dynamics, our brain’s processing systems (that regulate movement among other things) are located within a hidden bony skull and spine. So from our brain’s perspective, it’s sort of like we have a crustacean brain— the soft cognitive tissue is on the inside.\nWe thus don’t have the direct observational access to what’s occurring within our skull/spine that we have of the actions of our motor system. For example, we can’t hear the sounds active neurons make or smell our brain, and our brain has no pain receptors.\nThis lack of direct sensory access to cognitive processes led to the development of many competing speculations and theories about how our brain/mind functions. Indeed, Behaviorism, which dominated psychology for much of the past century, focused on the observable motor behavior that emerges out of cognition, rather than on inaccessible cognitive activity.\nBrain imaging technology is now finally providing this direct observational window into our cognitive functions. It provides an observable computerized representation of our working brain. Unfortunately, the current imaging technology requires the imaged subject to function within a laboratory setting, but we can anticipate the development of powerful portable imaging technologies that can depict the brain activity of subjects who are interacting with objects/events in a normal environment. We will then have moved toward the direct, intimate, observable relationship we’ve long enjoyed with our motor system.\nThe recent emergence of biologically based theories of consciousness (Damasio, 1999. Edelman, 2000) are good examples of the shift towards a more direct understanding of our brain’s mysterious processing systems. We can expect this process to escalate in the years ahead. Imagine what it might be like to finally understand our brain’s thinking activities at the same level that we now enjoy for movement.']	['<urn:uuid:47209421-1f3a-45f4-b467-bb5624a20c62>']	open-ended	direct	long-search-query	similar-to-document	expert	2025-04-22T14:43:01.810270	12	83	839
20	foreign licenses space activities valid nz	Under the Outer Space and High-altitude Activities Act 2017, the responsible minister has the authority to recognize overseas licenses and permits as meeting some New Zealand requirements. However, it's important to note that New Zealand's regulatory regime and international obligations are extraterritorial, meaning they also apply to New Zealand nationals or entities conducting space launches or satellite activities from other countries.	"[""Our regulatory regime\nOur regulatory regime supports the growth of a safe, responsible and secure space industry that meets our international obligations and manages any liability arising from our obligations as a launching state.\nThe Outer Space and High-altitude Activities Act 2017 (OSHAA)\nThe OSHAA Act came into force in December 2017. The Act regulates — through licences or permits — launches into outer space, launch facilities, high-altitude vehicles (HAVs) and payloads. It's administered by the New Zealand Space Agency within the Ministry of Business, Innovation and Employment (MBIE).\nGranting licences or permits\nThe Act contains requirements that applicants must satisfy to be granted licences or permits. These include:\n- the technical capability to safely conduct the proposed activity — for example, a safe launch, or safe operation of the payload\n- an orbital debris mitigation plan that meets any prescribed requirements\n- that the proposed activity is consistent with New Zealand’s international obligations.\nEven if these requirements are met, the responsible minister responsible may still decline to grant a licence or permit if — for example —they’re not satisfied the proposed operation is in the national interest, or if national security risks associated with the licence/permit application have been identified.\nAll activities will also need to comply with all other applicable New Zealand legislative requirements, such as resource consents, health and safety and environmental requirements.\nOverseas licences and permits\nThe legislation allows the responsible minister to recognise overseas licences and permits as satisfying some of the New Zealand requirements.\nOur regulatory regime and international obligations are extraterritorial — meaning they also apply to New Zealand nationals (or New Zealand entities) carrying out space launches or satellite activities from other countries.\nBackground to the Act\nThe documents below provide a background to the development and passing of the Outer Space and High-altitude Activities Act 2017.\n- Outer Space and High-altitude Activities Act 2017 Regulations — Regulatory Impact Statement (Aug 2017) [PDF, 414 KB]\n- Outer Space and High-altitude Activities Bill (introduced 19 September 2016)(external link)\n- Outer Space and High-altitude Activities Bill (June 2016) — Regulatory Impact Statement [PDF, 625 KB]\n- Outer Space and High-altitude Activities Bill: Final Policy Decisions (June 2016) — Cabinet paper [PDF, 620 KB]\n- Outer Space and High-altitude Activities Bill: Final Policy Decisions (June 2016) — Cabinet minute [PDF 579KB]\n- The Scope of Space Policy and a Lead Space Agency (April 2016) - Cabinet paper [PDF, 629 KB]\n- The Scope of Space Policy and a Lead Space Agency (April 2016) — Cabinet minute [PDF, 575 KB]\nThe regulations to support the Outer Space and High-altitude Activities Act 2017 came into force in December 2017. They are the:\n- Outer Space and High-altitude Activities (Licences and Permits) Regulations 2017(external link)\n- Outer Space and High-altitude Activities (Definition of High-altitude Vehicle) Regulations 2017(external link)\nThe Act contains broad regulation-making powers, however not all of them have to be used straight away — they’ve been built into the Act to future-proof it.\nThe regulations that were necessary to implement the Act when it came into force included:\n- requirements for licences and permits — particularly the information that applicants provide\n- requirements for an orbital debris mitigation plan\n- requirements for safety cases for launch licences, launch facility licences and (non-aircraft) high-altitude vehicles\n- clarification regarding which vehicles that go (or are capable of going) into high-altitude are not high-altitude vehicles (HAVs) for the purposes of the Act, and hence won’t require a licence.\nWhy HAVs are part of the regulatory regime\nSome high-altitude technologies have similar functions to satellites, such as for earth monitoring, communications and internet connectivity.\nWe already have high-altitude activity happening from New Zealand. These range from small, uncontrolled balloons launched for the purpose of collecting weather data or educating students, to large controllable balloons carrying sophisticated imaging and communications equipment for scientific research.\nIncluding high-altitude vehicles (HAVs) in the regulatory regime is intended to:\n- future-proof the legislation for advances in technology\n- ensure that different technologies performing similar functions are treated in a consistent manner.""]"	['<urn:uuid:f5660ade-0f5d-4352-bead-b513f096c8d8>']	open-ended	with-premise	short-search-query	distant-from-document	novice	2025-04-22T14:43:01.810270	6	61	676
