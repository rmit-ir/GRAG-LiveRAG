qid	question	answer	context	document_ids	question_factuality	question_premise	question_phrasing	question_linguistic_variation	user_expertise-categorization	generation_timestamp	question_length	answer_length	context_length
1	what defines behavioral asset pricing model difference from traditional model reading economics paper	The basic difference between behavioral and traditional asset-pricing models lies in their assumptions about agents' preferences, particularly those of a hypothetical representative agent. Behavioral models depart from the assumption of time-stationary expected utility (Savage rationality) and may incorporate concepts like loss aversion, hyperbolic discounting, or state-dependent utility functions.	"[""آیا مدل های رفتاری قیمت گذاری دارایی، ساختاری است؟\n|کد مقاله||سال انتشار||مقاله انگلیسی||ترجمه فارسی||تعداد کلمات|\n|10775||2002||14 صفحه PDF||سفارش دهید||5988 کلمه|\nPublisher : Elsevier - Science Direct (الزویر - ساینس دایرکت)\nJournal : Journal of Monetary Economics, Volume 49, Issue 1, January 2002, Pages 215–228\nThe recent increase in interest in so-called behavioral models of asset-pricing is motivated partly by the desire to have models that appear realistic in light of experimental evidence, and partly by their success in moment-matching exercises. This paper argues that the attention given to these two criteria misses perhaps the most important aspect of the modeling exercises. That is, the search for parameters that are invariant to changes in the economic environment. It is precisely this invariance that motivates the use of a tightly parameterized general equilibrium model. Assessing a model on this dimension is difficult and, as the paper argues through the use of suggestive examples, will undoubtedly require strong subjective judgments about the reasonableness of preference assumptions. Such judgments are routinely made about the reasonableness of assumptions about stochastic endowments. The paper suggests that more effort be applied to understanding aggregation in these models and to the exploration of behavioral assumptions in a less flexible but less corruptible time-stationary recursive class of preferences.\nThe recent successes of behavioral asset-pricing models provide new hope for the quantitative research program started by Mehra and Prescott (1985) following the theoretical work of Lucas (1978). That is, there is a renewed interest in the ability of a tightly parameterized, representative-agent, general-equilibrium model to explain the salient features of historical asset-market data (e.g., large equity premium, excess volatility, etc.). What makes an asset-pricing model “behavioral” can itself be the subject of debate. For the purposes of this paper, I will lump all asset-pricing models that endow agents with preferences that do not adhere to the assumption of time-stationary expected utility (i.e., “Savage rationality”), into the category of “behavioral”. Many of these preference assumptions are directly motivated by evidence from experimental psychology and behavioral decision theory, e.g., loss aversion (Epstein and Zin, 1990; Benartzi and Thaler, 1995; Barberis et al., 1999), or hyperbolic discounting (Luttmer and Mariotti, 2000; Krusell and Smith, 2000). Also falling within this broad definition, however, are models that may depart from classical assumption by allowing for state-dependent utility functions, but that are less formally motivated by behavioral evidence, e.g., habit formation (Abel, 1990; Constantinides, 1990; Campbell and Cochrane, 1995; Wachter, 2001). These examples are suggestive and are in no way an exhaustive list of behavioral asset-pricing models. Indeed as more experimental evidence filters into economics from various fields of psychology, this list continues to grow at a rapid rate. This paper takes a sympathetic view of these recent behavioral approaches and tries to identify what these models have yet to accomplish before they can claim success and presumably supplant more traditional approaches. Particular attention is paid to the need for structural models, and whether behavioral models are more or less likely to achieve the sort of “deep structural excavation” called for by the rational expectations revolution in dynamic macroeconomics. The methodological guidelines laid out by Friedman and Lucas in the two quotes above, cast a very different light on the current debate about the usefulness of behavioral versus more traditional models of asset markets, than what one might hear in academic circles and even in the popular press. From this perspective, traditional models cannot be viewed as inherently better because of their reliance on well-understood Savage rationality. Likewise, behavioral models cannot claim superiority simply based on experimental evidence of individual departures from this definition of rationality, and the sense of modeling realism that this evidence invokes. If this debate can be settled, then following Lucas’ advice, it can only be settled by determining which approach to building a mechanical, imitation economy provides “better imitations” of real asset markets. It is now common for behavioral models to adopt the dynamic stochastic general equilibrium approach of Lucas (1978) and Mehra and Prescott (1985) as a framework for understanding the consequences of alternative behavioral assumptions on observables in asset markets. In other words, the basic difference between the two approaches can be thought of as differences in assumptions about agents’ preferences — most often a hypothetical representative agent. Therefore, the common use of dynamic general equilibrium endowment economies by both approaches provides a common framework for comparison. The use of these general equilibrium models highlights an implicit desire to obtain structural models. In this case, “structural” is used to differentiate between purely statistical descriptions of empirical evidence (which may or may not use economic theory to suggest functional forms and factors), from models that derive their empirical predictions directly from the structure of a parametric version of an economic theory. The obvious implication being that like more traditional models, behavioral models can potentially provide useful guidance for understanding the likely consequences of changes in the economic environment. That is, they can be used to make forecasts about situations in which we have no (or at least very little) historical evidence. If that was not the desire of behavioralists in finance, then behavioral arguments could be safely relegated to a relatively minor role in the design and interpretation of reduced-form econometric models. Predicting responses to changes in the economic environment, e.g., changes in government policy, therefore, are precisely the “particular questions” for which we seek “better imitations”, using Lucas’ words, or the “sufficiently accurate predictions”, using Friedman's words. The primary reason for using tightly parameterized general equilibrium models to characterize asset-market outcomes is precisely the need for identifying policy-invariant structural parameters. Behavioral models have an obvious and important advantage over traditional models: their parameters can be calibrated so that various moments of particular interest of the distribution of asset prices generated by these models, will closely match their sample counterparts in historical data. That is, they do a better job imitating the large equity premiums, volatility, and persistent dynamics that are so puzzling from the perspective of traditional models. Clearly, this data-fitting exercise seems like a necessary condition for evaluating the usefulness of any equilibrium model. We would have little confidence in any model's ability to forecast outcomes in new environments when it is incapable of forecasting in the current environment. These conclusions, however, require a fair bit of judgment on the part of the modeler. Section 2 outlines judgments that modeler's working in this area typically make about the reasonableness of assumptions about the stochastic properties of exogenous variables. A simple example demonstrates that poorly fitting Savage-rational models can be made to fit empirical evidence by making assumptions about higher-order moments of the distribution of endowments. Most people would object to this strategy, however, since these assumptions might not seem reasonable given their prior beliefs. Where these beliefs come from is unspecified. They may derive from beliefs about the deeper micro-foundations of production, or they may derive from personal experience, or they may be purely whimsical. Direct statistical evidence about these higher-order-moment assumptions is likely to be misleading, or at best inconclusive, so we are left with non-sample-based judgments about the reasonableness of these assumptions. Section 3 of the paper looks at a similar sort of reasonableness criterion for preference assumptions. Unfortunately, unlike the case of assumptions about endowments, we are not yet at the stage where there is a consensus about what types of preference assumptions are unreasonable. The examples in Section 3 demonstrate that virtually any well-fitting, reduced-form empirical model of asset pricing can be incorporated into the representative agent's preferences so that a purely statistical model could be viewed as the outcome of what one could claim to be a structural general equilibrium model. Naturally, there is no guarantee that a model constructed in this way will have parameters that are invariant to the types of structural change for which these models must provide guidance. Analogous to the case for reasonableness of endowments, the modeler is forced to make judgments about the reasonableness of the preference assumptions. It is clear that fitting historical data is not a sufficient criterion for determining the usefulness of particular preference assumptions for delivering a structurally stable model. Likewise, without some explicit aggregation results, individual-level experimental evidence will also be insufficient. The main conclusion of this paper is straightforward: the parameters of asset-pricing models including behavioral models must be invariant to changes in the economic environment. This is not a very original conclusion and it is not likely to be very controversial. What is controversial and quite difficult, is assessing whether this goal has been met. Econometric testing for structural stability is notoriously problematic, especially in small samples. Moreover if the type of structural change under investigation has no historical counterpart, then purely statistical testing will be uninformative. The examples in 2 and 3 of the paper suggest that any assessment of the structural stability of a model will require the use of both non-sample information and the researcher's judgment. Accounting for historical evidence is not enough. The researcher is forced to evaluate the reasonableness of the assumptions on preferences and technologies under which the asset-pricing model can both account for historical evidence and maintain its basic structure in the face of significant changes in the economic environment. If aggregation results are available, then preferences of the representative agent that appear unreasonable given individual-level evidence would certainly be a cause for concern. These results also suggest that maintaining time-consistency as a basic axiom for preferences, and avoiding the introduction of arbitrary state variables in the utility function, will help eliminate much of the scope for well-fitting but non-structural empirical asset-pricing models to pass as deeper structural models.\nنتیجه گیری انگلیسی\nThe purpose of trying to characterize asset-market data using a tightly parameterized, representative-agent, general-equilibrium model is to try to uncover deep structural parameters. This is equally true of behavioral models as it is of more traditionally expected utility models. This is not a simple task and, as the examples in this paper suggest, fitting historical data is not sufficient to insure structural stability. Evaluating a model in this dimension will always require an element of subjective judgment of the reasonableness of the assumptions of the model. This is perhaps even more true of behavioral asset-pricing models than more traditionally expected utility models. The reason is that behavioral evidence may suggest the inclusion of state variables in the utility function and the relaxation of the stationarity assumption of intertemporal preferences. This exposes these models to the risk of being reverse engineered to fit the data, without serious consideration of whether the parameters of the model can be deemed structural. Econometric testing for structural stability is likely to be problematic, especially in small samples. In addition, statistical test will be uninformative if the types of change to the economic environment being contemplated has no natural analog in historical experience. A better understanding of how behavioral models aggregate provides some hope for reaching a consensus about reasonableness, since this will allow inference about the representative agent's preferences based on individual-level experimental evidence. Finally, since the use of behavioral models often opens the door for claims of reverse engineering, it seems prudent to work harder to avoid these criticisms. Maintaining assumptions on recursivity and time-stationarity of intertemporal preferences, while incorporating behavioral concepts is both feasible, as shown in Epstein and Zin (1990), and desirable given the discipline that this will naturally enforce on the modeling exercise.""]"	['<urn:uuid:417fbc69-80f9-4b53-8b5c-1c9cdd2bb0e9>']	factoid	with-premise	long-search-query	similar-to-document	novice	2025-04-14T18:34:38.591736	101	401	13105
2	As an architectural photographer, I notice my building shots often have exposure issues with bright skies. What's the best way to handle this common problem with auto settings?	When the camera automatically exposes for the sky, it can leave the building too dark. To solve this, you have two options: if your camera has exposure adjustment, try increasing the exposure by about one stop and take another shot. If that doesn't resolve the issue, try taking the picture from a different angle while minimizing the amount of sky in the frame.	"['Learning patterns/Improving your building photography/ja\nWhat problem does this solve?\nYour photos of buildings don\'t seem to come out as well as you\'d hoped. Is it your camera? Is it the way you take the photos?\nWhat\'s the solution?\nHere\'s the good news: the quality of those photos can be greatly improved if you keep in mind a few simple tips. You don\'t need an expensive camera to take good shots – just a little knowledge and care when taking the image.\nよくある失敗 - カメラの構え方\nTake care to keep the camera straight. Spending ten seconds on composition (how the visual elements are arranged in the photo) and alignment can greatly improve your picture quality. And in spite of what you may see others doing, always hold your camera or your mobile phone in both hands when composing and taking the picture.\nCamera tilt is the single biggest problem with mobile phone uploads, and it\'s worth repeating: always hold your camera or your mobile phone in both hands when composing and taking the picture.\nWhile there are applications for later editing pictures by rotating them, it\'s much better to get it right in the first place.\nBuilding partly obscured\nTry to avoid objects that obstruct the view of your subject. Choose a different angle, or (if the obstruction – especially parked cars – might go away) come back later. If you absolutely can\'t avoid cars, for privacy reasons ensure that numberplates are not visible (the number plates had to be blurred out to display this image).\nIf your camera creates a timestamp watermark, please turn off that feature beforehand.\nDon\'t chop off the top of the building or, as some people like to say: ""make sure you get the cross on top of the steeple"". It can ruin a good picture if the viewer expects to see something that has been chopped off.\nThink about how far the edges of the building are from the frame. If the entire building won\'t fit in the frame, walk back if you can, or try a different angle. If there\'s nowhere from which you can take a good picture, concentrate on something else instead, such as some of the smaller architectural details.\nDon\'t chop off the bottom of the building unless it\'s unavoidable. It\'s easy to forget the bottom when you point your camera up to get the top of the building.\nIf the entire building won\'t fit in the frame, move back if you can, or try a different angle. If there\'s nowhere from which you can take a good picture, concentrate on something else instead such as some of the smaller architectural details.\nAvoid pointing the camera too close to the sun. The amount of flare in this photo makes the picture unusable, even though the sun itself is outside the frame.\nTo avoid camera movement, especially in dark church interiors, use the font or a pillar as a support. Blurred images such as this are of little use. Even outside, occasionally we\'re lucky enough to have something solid like a bench or fence conveniently located; if so, try resting your camera on it for greater stability, and see if the composition and alignment work from that angle.\nThis picture seems to have been taken one-handed; remember the rule about always holding your camera in two hands? Notice the annoying chopped-off candle holder on the left; again, think about each element in relation to the edges of your photo.\nCommon errors – auto settings\nThe camera has automatically exposed the sky correctly, but that leaves the building much too dark. If your camera has exposure adjustment, increase the exposure by about one stop and try again. If that doesn\'t solve the problem, take the picture from another angle and try to keep the amount of sky to a minimum.\nOverexposure like this is not very common on modern cameras. If you see it, check your camera settings: the ""exposure compensation"" may have been set incorrectly.\nCheck what your camera is focusing on\nHere, the camera autofocus has latched on to the lamp to the left, rather than the building.\nCommon errors – subjects\nPlease don\'t take photos of signs\nPlease don\'t upload photographs of posters, noticeboards, signs, modern murals, or anything else having text or two-dimensional images that might be copyright-protected. That applies even to text or images in a public place – even if everyone else is taking pictures of them. Ancient wall paintings in churches are fine: they have no copyright protection.\nHere, the copyright-protected text and images have been blurred out. It\'s OK if your photo of a building incidentally includes an unobtrusive sign that appears small in the final image, but if the sign appears large or prominent, please take the photo from another angle.\nNo posing please\nPlease don\'t upload images of people posing, even if there\'s a nice building in the background!\nThat looks a lot better!\nThe signs are small in the composition and don\'t cause a copyright problem. Maybe you can do even better than this?\nWhen to use\n- Individually uploaded photos of local buildings\n- Wiki Loves Monuments, \'Wiki Takes ...\' events and \'Wiki Scavenger Hunts\'\n- Compositional techniques, in the English Wikipedia.\n- ""Picture composition"" in the Danish Wikipedia (Google translation into English).\n- ""Composition"" in the Finnish Wikipedia, which goes into greater detail concerning lines and directionality (Google translation into English).']"	['<urn:uuid:05ff742a-91b3-47fb-a038-253dc0b9430c>']	open-ended	with-premise	verbose-and-natural	similar-to-document	expert	2025-04-14T18:34:38.591736	176	362	5284
3	what methods does phoenix youth performing arts use respect hip hop authenticity	Rising Youth Theatre in Phoenix ensures hip-hop authenticity through several key methods: They partner with CYPHERS the Center for Urban Arts, which provides expertise in the five elements of hip-hop (B-boying, MC-ing, DJ-ing, Graffiti Art, and Knowledge). They involve the hip-hop community directly through story circles and require community approval of scripts. They perform in public spaces creating a 'Block Party' setting that pays homage to original hip-hop street battles from the 1970s Bronx. Their dramaturgy involves experts from multiple hip-hop disciplines and focuses on all elements of hip-hop culture rather than using it merely as a trendy addition.	['Respecting Hip Hop in Rising Youth Theatre’s production of antonia\na latina hip-hop Antigone\n“Rap is something you do, Hip-Hop is something you live.”—KRS-One\nThe mission of Rising Youth Theatre is to create youth driven theatre that is not only riveting and relevant, but also challenges audiences to hear new stories, start conversations, and participate in their communities. Heading into their fifth year anniversary, Co-Artistic Directors Xanthia Angel Walker and Sarah Sullivan have successfully promoted these ideals with a number of diverse communities throughout Phoenix, Arizona.\nRising Youth’s programming has touched on several important issues affecting today’s youth, such as high school dropout rates, juvenile incarceration, the foster care system, and immigration. For the first show of its 2015/2016 season, the organization reached out to the youth of the hip-hop community. With every show Rising Youth produces, the creative team engages with youth in the community through story circle interviews that focus on the community’s relationship to the issue being explored. From this interaction the playwright involved creates a play. For the first show of the season, the resulting project was an adaptation of a Sophocles’ play titled antonia: a latina hip-hop Antigone. Both the playwright and director felt that the themes in the play, such as power, fate vs. free will, rules vs. order, and feminism vs. gender, were relatable to hip-hop culture and the lives of the youth participating in the project.\nThe success of a TYA hip-hop performance is dependent on a communal form of dramaturgy that is similar to hip-hop’s cypher, or artistic gathering.\nIn the process of creating antonia, Rising Youth has made a conscious effort to respect hip-hop culture by avoiding the appropriation of hip-hop, which is present in numerous productions of theatre for young audience (TYA) across the United States. This cultural appropriation has manifested in hip-hop theatre, lending itself to bad and disingenuous theatrical experiences. The danger of unauthentic hip-hop theatre is that it is a false reflection of what hip-hop is and works towards keeping the culture marginalized. Expanding on this sentiment, Rising Youth’s goal with antonia is to create theatre that explores hip-hop and its origins. Director Xanthia Angel Walker states, “In the field of TYA, I have observed ‘hip-hop’ being inserted into plays as a trendy way of being ‘accessible’ to young people. There is little to no regard for the elements and principles of hip-hop that serve as a powerful community building tool and emancipatory force. This force is often disregarded and disrespected in ways that continually feed the dominant narrative of the Caucasian, middle class youth experience.”\nWith this production of antonia, Rising Youth decided to perform the piece in a public space, which promotes the idea of making theatre accessible to those who have little to no knowledge of theatre and/or hip-hop by creating a gathering space where audiences participate in a “Block Party” setting. As a result, Rising Youth breaks away from the traditional proscenium stage, while paying homage to the origins of hip-hop culture by mimicking the “battles” that occurred on the streets in the early 1970’s in The Boogie Down Bronx. Therefore, creating a community space where ritual becomes the driving force. The audience does not only watch a play, but also participates in an experience.\nI recommend that TYA theatres take note of Rising Youth’s process to mitigate the risks of creating unauthentic hip-hop theatre. I believe antonia models how to effectively and ethically create and produce hip-hop theatre. Three factors needed for producing hip-hop theatre are partnerships, dramaturgy, and community.\nFor antonia, Rising Youth is partnering with CYPHERS the Center for Urban Arts, founded by Danny “Skooby” Morales. This organization is dedicated to educating youth about the five elements of hip-hop: B-boying, MC-ing, DJ-ing, Graffiti Art, and Knowledge. This relationship was created with the understanding that CYPHERS offers an expertise of hip-hop culture that Rising Youth lacks. This partnership also ensures that the interests of youth participants are protected. Furthermore, the goal is to establish a common vocabulary that will allow both partners to benefit from the collaboration in a meaningful fashion, while educating each other in their respective mediums. TYA theatres wishing to produce hip-hop theatre should seek out partnerships with organizations that promote and maintain hip-hop culture. Without these types of partnerships, the risk of hip-hop culture being exploited on stage increases.\nIn the field of TYA, I have observed ‘hip-hop’ being inserted into plays as a trendy way of being ‘accessible’ to young people. There is little to no regard for the elements and principles of hip-hop that serve as a powerful community building tool and emancipatory force. —Xanthia Angel Walker\nThe approach to dramaturgy is another important factor in the production of hip-hop theatre. In antonia, Rising Youth focuses on how the themes in the original source material connect with the elements of hip-hop. The creative team includes a dramaturg and experts from multiple disciplines within the hip-hop community. TYA theatres must focus on dramaturgy that considers the interdisciplinary nature of hip-hop and its improvisational foundation. Dramaturgy should not focus on one element, but rather all elements of hip-hop. This essentially calls for a creative collaboration and exchange of ideas that goes beyond the formal director-dramaturg relationship. The success of a TYA hip-hop performance is dependent on a communal form of dramaturgy that is similar to hip-hop’s cypher, or artistic gathering.\nLastly, acknowledging the importance of community members is key in the process because the hip-hop community continually gets misrepresented. This directive serves the production of antonia quite well, but should be a goal for any TYA theatre wishing to reach wider audiences like traditionally marginalized groups. Rising Youth holds itself accountable to the community-at-large. It does not produce a script of which community participants do not approve. This theatre believes that the community of youth will directly shape the content of its work. Rising Youth hopes to create art that converses with community members, instead of talking down to them. In regards to hip-hop culture, TYA theatres must evaluate how they approach this culturally sensitive subject. TYA theatres must also acknowledge that they have a moral obligation to do so.']	['<urn:uuid:9dceb9b3-ea9b-4ccc-b3f5-a5c828007a62>']	open-ended	direct	long-search-query	distant-from-document	novice	2025-04-14T18:34:38.591736	80	667	6661
4	need help when can person legally die peacefully oregon first usa state allow	Oregon became the first state in the United States with an open practice of aid in dying (AID) in 1998. This occurred after Oregon voters adopted the first AID statute in 1994, though implementation was delayed for several years due to a lawsuit challenging its validity.	"['California adopted the End of Life Option Act (EOLOA) in October 2015, establishing statutory permission for aid in dying (AID), the medical practice in which a physician provides a mentally competent terminally ill patient with a prescription for medication that the patient may ingest to achieve a peaceful death. California was the fourth state to adopt such a statute, following Oregon, Washington and Vermont. Recently, Colorado and the District of Columbia adopted similar measures. All impose a number of burdens and restrictions on accessing AID.\nThe first state to vote on a measure intended to empower terminally ill patients with physician-assisted dying was Washington in 1991. California unsuccessfully tried a similar measure the next year. Opponents argued that there were insufficient ""safeguards"". Voters were swayed by this concern, despite strong public support for AID. Learning from these attempts, Oregonians embraced a \'throw in the kitchen sink\' approach and put forth a measure thick with ""safeguards."" In 1994 Oregon voters adopted the first AID statute in the United States. Implementation of Oregon’s law was delayed for several years, until a lawsuit challenging its validity was resolved late in 1997. Beginning in 1998 Oregon became the first state with an open practice of AID.\nIn 2015 California legislators adopted an Oregon-style AID measure, though slightly more restrictive. While in some ways the EOLOA can be considered a step forward, in others it represents a step backwards. The ""step forward"" perspective recognizes that California is the largest, most populous and most demographically diverse state to adopt an AID law. The ""step backward"" can be seen in the fact the EOLOA imposes all of the burdens and limitations as does the original Oregon Measure and goes beyond in imposing both an additional restriction and a sunset provision.\nOregon’s nearly 20 years experience with AID has been closely watched and studied. The State annually reports on implementation of the program, and a plethora of articles examining and discussing the data have been published. The consensus has emerged that availability of AID improves end-of-life care and presents no risk.\nAccordingly, it would be appropriate for the practice to become more normalized within the practice of medicine, with less government oversight and regulation. The regulation of the practice of medicine is not commonly governed by statute; rather it is primarily governed by professional practice standards as they evolve in practice; in other words—by standard of care. However, the EOLOA is ""Oregon Plus"": imposing all of the intrusive, burdensome government involvement into medical practice that the Oregon statute requires and adding some additional burden and limitation. The obstruction faced by patients is apparent in accounts of families of patients who have attempted to run the gauntlet of requirements in order for the dying patient to achieve a more peaceful death through aid in dying.\nPrior to enactment of the EOLOA, two cases had been filed in California seeking clarification of the reach of California’s statute that makes a crime of ""assisting"" a ""suicide."" Brody v. Harris, Donorovich-O\'Donnell v. Harris. Plaintiffs, patients and physicians, argued that this law should not apply to physicians providing AID because the choice of a dying patient for a peaceful death is not “suicide”; alternatively, plaintiffs argued that the choice at issue deserved protection under California\'s constitutional guarantees of privacy and equal protection. Adoption of the EOLOA mooted the statutory scope claim, as it made clear that physicians who provide aid in dying cannot be prosecuted for ‘assisted suicide.’\nNeither of these cases reached the California Supreme Court. Hence the question of whether state constitutional protection extends to the choice of a dying patient for a more peaceful death through AID has not been resolved. It is this commentator\'s view that the Donorovich court applied law incorrectly and its opinion ought not be persuasive to the California Supreme Court.\nCalifornia courts have repeatedly held that the scope of California\'s constitutional right of personal autonomy privacy is greater than that protected by the federal Constitution. California’s privacy jurisprudence has often been ahead of that of the SCOTUS. California\'s Supreme Court recognized “[t]he fundamental right of [a] woman to choose whether to bear children” in invalidating its ban on abortion four years before the SCOTUS recognized this right in Roe v. Wade.\nJurisprudence concerning reproductive freedom may be especially relevant to AID. California decisions have held the right of a woman to control her body and to choose to terminate a previable pregnancy as ""clearly among the most intimate and fundamental of all constitutional rights."" The right of a woman to control her body has been grounded in the right of privacy. The decision of a dying patient about how much suffering to endure prior to death is of a similar profoundly personal nature, concerning one’s own body, medical treatment, and life course. Because the only life at issue in AID is that of the dying patient, the greatest complexity present in the abortion context—the potential life of the fetus—is absent. Respecting this decision does not result in a reality where doing so ends a potential life with no voice in the decision. Principles that compel respect for a woman’s decision to control her body through abortion apply with equal force to the decision of an individual regarding how much suffering to endure in the final throes of terminal illness, and hence whether to choose AID.\nUnder this approach, the California Supreme Court ought find a privacy or equal protection interest in choosing AID, and hold legislation constraining exercise of this choice to the standard articulated in American Academy of Pediatrics. The State would be required to prove that it has a compelling interest at stake, that it achieves its interest in the least intrusive way, and that the restrictions are narrowly drawn to impinge upon the constitutionally protected area no more than is necessary. It is unlikely that several of the EOLOA provisions could meet this exacting test, including the fifteen-day waiting period. The state might argue that it has an interest in ensuring that a patient’s choice of AID is carefully considered and enduring, and that this interest is served by the waiting period. In the reproductive rights arena at the federal level waiting periods of much shorter duration have been allowed—most commonly twenty-four hours. And, notably, a medical emergency exception to mandatory waiting periods are included, which the EOLOA lacks.\nIn Planned Parenthood v. Casey, the court upheld a state’s twenty-four hour abortion waiting period stating that “only where state regulation imposes an undue burden on a woman\'s ability to make this decision does the power of the State reach into the heart of the liberty protected by the Due Process Clause.” The court stated that an undue burden exists if “a state regulation has the purpose or effect of placing a substantial obstacle in the path of a woman seeking an abortion of a nonviable fetus.” The Court elaborated on the “substantial obstacle” test, stating “the means chosen by the State to further the interest in potential life must be calculated to inform the woman\'s free choice, not hinder it.”\nThe fact that the statute contained an emergency exception to the waiting period was significant, as was absence of findings that a twenty-four hour waiting period created “appreciable health risks.” The EOLOA’s failure to include an emergency exception renders it vulnerable, and certainly imposing a wait period causes the patient harm, as additional suffering must be endured or, even worse, sometimes the patient dies exactly the sort of death hoped to be avoided during the long waiting period.\nA state might assert an interest in ensuring that a patient choosing AID is not acting impulsively and that this interest is served by a waiting period —though why, specifically, the state has an interest in this is unclear. Even if the state establishes an interest, it could be served by a much shorter period. Patients choosing aid in dying do so because they find themselves in an unbearable dying process, where the cumulative burden of suffering imposed by their end stage terminal illness is overwhelming. Forcing them to suffer longer, and possibly be forever denied the right to access aid in dying, seems unjustifiable.\nIf the California Supreme Court reviews restrictions imposed by the EOLOA under a strict scrutiny standard, its jurisprudential history suggests it would almost certainly invalidate a fifteen-day waiting period, and likely other of the EOLOA’s burdensome restrictions. Under the strict scrutiny standard laid out in Roe v. Wade, prior to the undue burden standard expressed in Casey, the court recognized only two state interests as sufficiently compelling to justify governmental restrictions on abortion: the state\'s interests “in preserving and protecting the health of the pregnant woman” and “in protecting the potentiality of human life.” Neither of these sorts of interests are addressed by the restrictions on AID. There is no “health” to protect, as terminal illness by definition destroys health, and there is no potential life interest at stake. Under a strict scrutiny standard, AID regulations would not survive. Even if the more permissive undue burden analysis were to be applied, the fifteen-day waiting period is likely to fail.\nOther elements of the EOLOA may not survive scrutiny: requiring patients to make a written request in addition to the two oral requests, and that they see a consulting physician to confirm diagnosis, prognosis, and competence, delay access to AID. Delays always force additional unwanted suffering and sometimes prevent patients from access entirely, given the progressive nature of their illnesses.\nThe restrictions imposed on AID by the EOLOA will likely be challenged and ultimately examined by the California Supreme Court, which ought find this choice to be protected by privacy and/or equal protection guarantees under the California constitution, and that various restrictions contained in the EOLOA do not survive constitutional scrutiny. Those who believe it is the individual who ought be vested with autonomy to make their own informed decisions about profoundly personal matters involving their own body, life course and medical treatment will applaud such an outcome.\nKathryn L. Tucker, JD, is Executive Director of the End of Life Liberty Project (ELLP), which she founded during her tenure as Executive Director of the Disability Rights Legal Center, the nation\'s oldest disability rights advocacy organization. She has held faculty appointments at Loyola, the University of Washington, Seattle University and Lewis & Clark Schools of Law, teaching in the areas of law, medicine and ethics, with a focus on the end of life. Tucker engages multidimensional advocacy to protect and expand the rights of terminally ill patients nationwide.\nSuggested citation: Kathryn L. Tucker, End of Life Liberty in CA, JURIST - Professional Commentary, Feb. 21, 2017, http://jurist.org/professional/2016/12/kathryn-tucker-end-of-life-liberty-in-california.php.\nThis article was prepared for publication by Henna Bagga, an Assistant Editor for JURIST Commentary. Please direct any questions or comments to her at']"	['<urn:uuid:bed58ebf-446e-4813-a0e4-d96ee1b3b3da>']	factoid	with-premise	long-search-query	distant-from-document	novice	2025-04-14T18:34:38.591736	77	271	11552
5	I heard that gastroenteritis is caused by viruses - can you tell me which specific viruses are responsible for causing it?	Viral gastroenteritis can be caused by several viruses including rotaviruses, toroviruses, adenoviruses, caliciviruses, and astroviruses. Rotavirus is a major cause of gastroenteritis worldwide, while norovirus is a major cause in the United States. About 75% to 90% of gastroenteritis cases are caused by viruses, with the remaining 5% caused by parasites.	"[""Gastroenteritis is one of the serious illnesses that have a significant impact on infants and babies. It occurs when the stomach and intestines are inflamed. In the case of viral gastroenteritis, inflammation is caused by infection with a virus, often accompanied by vomiting or diarrhea. Learn about infantile gastroenteritis.\nGastroenteritis is an infectious disease that affects the gastrointestinal tract. It occurs when pathogens attack the gastrointestinal tract and cause inflammation of the digestive system. This causes the gastrointestinal dysfunction leading to gastroenteritis.\nInfection, especially from the stomach to the colon, can affect all parts of the digestive system. A myriad of bacteria and viruses infect babies.\nA gastroenteritis-causing virus\nViral gastroenteritis is not caused by influenza viruses. Instead, it can be caused by a variety of viruses including rotaviruses, toroviruses, adenoviruses, caliciviruses, and astroviruses.\nAs a result, only about 75% to 90% of the cases of gastroenteritis caused by the virus alone occur. Among them, rotavirus is a major cause of gastroenteritis worldwide. Norovirus is a major example of gastroenteritis in the United States. And the other 5% is caused by parasites.\nThe main pathogens of gastroenteritis are contaminated food and water. There is also a greater risk of infection among babies who consume a variety of other foods than those who breast-feed. Pathogens are usually excreted through the stool when food and water come in contact with infected stools.\nAlso, contaminated items such as toys and toilet seats also carry viruses that cause gastroenteritis. The virus flourishes on these surfaces for several days. And if someone in the family has gastroenteritis, touching the baby 's goods without washing their hands can be transmitted. In conclusion, when infants are infected with pathogens or viruses from all these factors, symptoms of the disease soon appear.\nViral gastroenteritis, however, is not a very serious disease. Even if most of them are sick, they usually recover soon after a certain period of time without long-term problems or side effects. In children, the rate of recovery can vary depending on which virus you have.\nHowever, in order to replace vomiting or diarrhea caused by sickness, a sufficient amount of water should be available for drinking. Otherwise, it can develop into a serious disease. Especially those who can not take care of themselves because of their babies, the elderly, or the disabled are at high risk of dehydration and should pay more attention.\nPeople with impaired immune systems may also have more severe symptoms than healthy people. Vomiting or diarrhea is more frequent. This can also lead to dehydration risk. If some symptoms are severe, you may need to be hospitalized to prevent dehydration.\nViral Gastroenteritis Treatment\nViral gastroenteritis in children and adults is usually treated with dehydration prevention. Dehydration means a lot of body fluids are lost. When a child has enteritis symptoms, parents should ask their doctor how to prevent dehydration and get advice.\nHowever, water is not always a sufficient remedy. Of course water is good in itself, but in some cases it may not be enough. For example, water can not replace electrolytes such as salt, sugar, and minerals that can be lost when the body is dehydrated. Beverages that can replace salt or minerals can be purchased at pharmacies, such as electrolytic solutions or oral water supply solutions. Some sports drinks also have the ability to replace electrolytes. Of course, it contains a lot of sugar, but it does not give much to most school-aged children.\nOn the other hand, milk should be kept away. Milk can make gastroenteritis worse. In addition, it is preferable to avoid beverages or caffeinated beverages containing a lot of acid because they can cause discomfort.\nAnd when you feed such a drink containing water and various electrolytes, it is advisable not to enter too quickly. If it is absorbed into the body too soon, it can worsen vomiting and should be taken as slowly as possible. The best way to do this is with a teaspoon every 4-5 minutes.\nFood should also be adjusted to eat slowly. Once you have had a good drink, you should try adding soft foods such as banana, bread, rice, apple sauce or toast. Chicken soup and noodles with noodles are also good. And if the condition improves gradually, you can feed the cooked meat or vegetables. However, fried or spicy foods, greasy or sour foods should not be eaten for a while.\nIt is good to skip medicines during the treatment period. Although you may think you should take a prescription-free medicine, gastroenteritis can eventually be resolved over time. Most medications also have a different risk of worsening the condition. With vomiting and diarrhea, the body fights infection while eliminating everything in the body. It is desirable to make the process self-healing by retaining sufficient water without stopping with the drug.""]"	['<urn:uuid:d7c6cf83-351d-40b8-8216-34098c592495>']	factoid	with-premise	verbose-and-natural	similar-to-document	novice	2025-04-14T18:34:38.591736	122	357	5015
6	turbofan engines market dominance 1970s widebody	The introduction of widebody jets in the 1970s, including the Boeing 747, Douglas DC-10, and Lockheed Tristar, marked a significant shift in the engine market. This period saw GE and Rolls-Royce entering the market, and it marked the beginning of high bypass turbofans. Previously, Pratt & Whitney's JT8D turbofan had dominated the short-haul airliner market, while their JT3D controlled the long-range market.	['By Bjorn Fehrm\nSeptember 15, 2022, © Leeham News: Last week, we looked at how Pratt & Whitney’s JT8D turbofan came to dominate short-haul airliners while the JT3D had the long-range market.\nThe introduction of the widebody jets in the 1970s with Boeing 747, Douglas DC-10, and Lockheed Tristar brought GE and Rolls-Royce into the market. It was the start of the high bypass turbofans.\nBy Bjorn Fehrm\nSeptember 8, 2022, © Leeham News: Last week, we analyzed the change from turbojets to turbofans for civil air transport. The jet engine was developed for high-speed military fighters and was not ideal for subsonic airliner use.\nWe also dwelled on why the three major engine OEMs came to different solutions for the first-generation turbofans. Now we look at the engine that made turbofans mainstream, the Pratt & Whitney JT8.\nBy Vincent Valery\nJune 20, 2022, © Leeham News: New airplane programs used to come to market in four years. Now, the launch-to-entry-into-service period has been seven years or more. (Chinese and Russian programs take even longer.)\nBoeing launched the 787 in December 2003. EIS was October 2011. Airbus’ A350, launched in response to the 787 in 2004, went through several iterations which added time to the program. Delays added more time. EIS was in January 2015.\nBombardier’s C Series was launched in 2008. EIS was in July 2016. The Boeing 777X was launched in 2013. EIS is now targeted for 2025. Boeing launched the 747-8 in 2005. EIS was in 2011. The Boeing 737 MAX was launched in July 2011. EIS was May 2017. Airbus’ A320neo was launched in December 2010. EIS was in January 2016.\nBoeing has been discussing the New Midmarket Airplane (or whatever it was called throughout changing nomenclature) since 2012. It still hasn’t launched the program. Once it does, how long will it take to enter service?\nAny new program is a multi-year, multi-million investment that, in the worst case, can take decades before recovering the initial development and production ramp-up expenditures.\nSeveral recent programs, notably the 777X, have faced significant delays between the envisioned and actual start of deliveries to airlines.\nBoeing claims that advances in manufacturing techniques will reduce the time required to develop the next aircraft program. However, regulatory scrutiny is higher nowadays and the aircraft built are more complex than in previous generations.\nLNA analyzes how the time between the program launch and entry into service has evolved since the beginning of the Jet Age. The goal is to find whether there is a trend and in what direction. The analysis focuses on Airbus, Boeing, Lockheed, and McDonnell Douglas.\nBy Vincent Valery\nJuly 29, 2021, © Leeham News: Last week, LNA compared the performance of the 777F against the A350F, launched today. As a follow-up, we thought it relevant to look at the history of freighter aircraft derived from passenger jets at the major OEMs.\nShortly after the dawn of the jet age, Boeing and McDonnell Douglas started selling freighter variants of their 707 and DC-8, respectively. Most aircraft families developed later at both OEMs would receive a freighter variant in one form or another.\nWe will stick for our analysis to Freighter aircraft delivered off the assembly line at the world’s Western OEMs: Airbus, Boeing, Lockheed, and McDonnell Douglas.\nBy Scott Correa\nSpecial to Leeham News\nMay 27, 2021, © Leeham News: Forty-two years ago this week, I puked at work.\nOn May 25, 1979, an American Airlines McDonnell Douglas DC-10 crashed on take off from Chicago’s O’Hare International Airport. Within minutes, it was known that the No. 1 engine separated from the airplane just after the airplane was committed.\nThe aircraft gained a few hundred feet before rolling over on its left wing, crashing into a trailer park. All 271 on board and two people on the ground were killed.\nThe Federal Aviation Administration immediately grounded all DC-10s in the US because of the engine separation. Regulators elsewhere in the world followed suit.\nMay 10, 2021, © Leeham News: The COVID-19 pandemic prompted airlines to ground more than 8,000 aircraft at the peak.\nAmong widebodies, no aircraft was hit harder than the Airbus A330ceo.\nTraffic within China, the US and Asia recovers with narrowbody airplanes. European short- and medium-haul traffic is not recovering as quickly due to continued boarder closings. International traffic, for the same reason, remains awful.\nBut in chaos some see opportunities.\nJep Thornton, managing partner of the boutique lessor Aerolease, last week said the A330-300 could be a great trading opportunity.\nAt April 1, there were 267 -300s and 286 A330-200s (of all types) in storage, according to data reviewed by LNA.\nDec. 22, 2020, © Leeham News: If you get a chance over the next few weeks – in between binge-watching The Queen’s Gambit, putting up the 79 extra feet of Christmas lights you ordered this year and figuring out how to buy surprise Christmas gifts for your spouse when you have a joint Amazon account – you should take 90 minutes to watch this video from our friends at the International Association of Machinists District Lodge 751.\nThe Machinists on Dec. 8 hosted (on Zoom, of course) a high-level panel discussion about the state of the aerospace industry and Washington state’s role in it, featuring a whole bunch of Brand-Name People Who are Smarter Than Me(c).\nThey shared their insights for those of us coffee-drinkers who are trying to read the tea leaves to divine what Boeing’s next moves should be as it tries to get back on its feet – and what the implications are for its home state.\nThe problems for Boeing are obvious, and the solutions are pretty clear – but doing the smart thing would require a major cultural shift from an executive team that’s locked into a 1990s vision of how business gets done.\nJune 29, 2020, © Leeham News: Boeing may be set to begin recertification flights of the 737 MAX as early as today, The Seattle Times reported last week.\nTesting will take three days, if all goes well. But Boeing still has a lot of work to do to fully satisfy regulators.\nAccording to The Times, Transport Canada and Europe’s EASA require additional modifications to enhance safety on the MAX. The additional changes may not be required for certification but must be done within a year, the paper reports. The MAX 10 must have the changes before it is certified.\nBy Vincent Valery\nJune 1, 2020, © Leeham News: As airlines slashed capacity in the aftermath of the COVID-19 outbreak, some took the opportunity to accelerate aircraft retirements.\nOlder generation twin-aisle aircraft, notably the Airbus A340, older A330s, Boeing 747 and 767, have exited numerous carrier’s fleet early. Several Airbus A380 operators put their Superjumbos in long-term storage, wondering whether these will ever fly in passenger service again.\nMajor crises tend to accelerate existing trends. The move away from large twin-aisle aircraft is a case in point. In the context of subdued demand for several years, airlines will be under pressure to reduce expenses. Streamlining fleets is an obvious target.\nThe Airbus A320 and Boeing 737 families dominated the single-aisle market for decades. The picture has historically been far more fragmented for twin-aisle aircraft. Airbus and Boeing still have three widebody aircraft families apiece with significant numbers of passenger aircraft in service.\nLNA analyzes in two-part articles why the picture will likely change for the widebody market in the 2020s. In the first part, we will take a historical detour to analyze why twin-aisle fleets are still so fragmented nowadays.\nMarch 9, 2020, © Leeham News: Commercial aviation accidents are high profile news events.\nThese happen rarely. Many times, a lot of people are killed. (It should be noted that often survivors may outnumber those killed as safety improved.)\nIn this era of 24/7 cable news and minute-by minute social media, everyone wants instant answers as to causes.\nFinding answers is not simple. A typical accident investigation usually takes 12-18 months before the investigators issue a final report with a probable cause.\nOne reason for this is that sometimes, the cause of an accident comes down to a single bolt, or even a single cotter pin.\nThis is where the new book, Flight Failure, Investigating the Nuts and Bolts of Air Disasters and Aviation Safety, serves to remind us of just how intricate accident investigation is.']	['<urn:uuid:decf2ad2-7813-4548-9ffa-360165da1603>']	open-ended	direct	short-search-query	similar-to-document	expert	2025-04-14T18:34:38.591736	48	410	8465
7	What solution is recommended when implementing ITIL processes in phases to handle missing process inputs?	To handle missing inputs during a phased introduction of ITIL processes, a generic process directory for the IT organization can be used. This directory provides a structured framework for defining process links even when only a subset of ITIL processes is initially defined, allowing additional processes to be plugged into the model later as needed.	['ITIL Implementation - Process Interfaces\n|Step 6: Definition of ITIL Process Interfaces|\nThis step determines which inputs each ITIL process receives from other processes, and which outputs it must produce so that subsequent processes are able to function.\n- Definition of the interfaces for all ITIL processes which are to be introduced\nThese inputs and outputs are also called ITIL information objects: Structured sets of data, like e.g. an Incident Record, which serves to describe a service interruption.\nJust how great the importance of process interfaces is for the design of optimal work procedures frequently becomes apparent during the analysis of as-is processes:\nWeaknesses in processes often occur at those points where one process ends and another one begins. In many cases one will find interrupted information flows or media breaks – so that the required information is not exchanged as intended.\nThe definition of the process interfaces is taken care of as a separate project step, before dealing with the innards of the processes in detail. Obviously, before being able to define the detailed activities, it must be clear what inputs a process can expect from preceding ones, and which outputs it must produce.\nThe ITIL Process Map applies a rigorous approach to the definition of interfaces: Information objects may be picked from a central ITIL glossary (see figure 1: Index of data objects (.pdf)) to define the inputs and outputs in a precise way. Every information object contains a short definition to avoid any ambiguities about the expected process results.\nA challenge during the definition of the ITIL interfaces lies in the fact that, as a rule, not all ITIL processes are introduced at once, which often means that some of the required inputs for a process are missing.\nIn order to circumvent this problem, which inevitably springs up during a phased introduction of ITIL, a generic process directory for the IT organization as a whole can be used.\nThe generic directory offers a structured framework for the definition of process links even if, initially, only a sub-set of the ITIL processes is defined in detail.\nAdditional ITIL processes can thus be plugged into the process model at a later point in time as needed.\n- Structure of the ITIL processes to be introduced\n- ITIL information objects (ITIL glossary terms) as inputs and outputs\n- Interfaces of the ITIL processes to be introduced:\n- with each other\n- with other service management processes\n- with customer and supplier processes\n- It must be avoided that the newly introduced processes represent an isolated solution; the interfaces to the other processes within the IT organization and beyond it must therefore be considered.\n- The documentation of the interfaces should be clearly structured, showing details only when required. This calls for overview diagrams showing the big picture and separate detailed interface diagrams for each process.\nRelevant views of the ITIL Process Map\nThe ITIL Process Map contains two types of models which, in combination with each other, are used for the definition of the process interfaces:\n- Process overviews (see ITIL implementation step 5 - figure 2), which illustrate the interrelations of several processes on one single page\n- Detailed process interface diagrams with all inputs and outputs (see figure 2)\n-  To-be process structure: Generic ITIL process structure (.pdf)\n-  ITIL inputs and outputs: ITIL information objects (ITIL glossary terms)\n-  Index of Data Objects - Example (.pdf)\nFollowing project activity\n→ ITIL Implementation - Step 7: Establishing Process Control']	['<urn:uuid:1b7bce94-d953-4f32-aed9-1222dbbf1b59>']	factoid	direct	verbose-and-natural	similar-to-document	expert	2025-04-14T18:34:38.591736	105	351	3617
8	what happened britain after roman rule ended economy society	After Roman rule ended in Britain, the army and civil service workers stopped receiving payment, which triggered a decline in craft and service industries. While it was traditionally believed that most of the population turned to subsistence farming and the administrative system broke down into local fiefdoms, evidence from places like Chedworth Villa suggests the decline was more gradual, particularly in the West Country where the Romanized way of life was sustained longer.	"['\'Tremendously exciting\' 5th century Roman mosaic found in Britain\nArchaeologists have uncovered Britain\'s first 5th century Roman mosaic -- a find of ""enormous"" historical significance which could change the way historians view the period it dates back to.\nThe mosaic floor was first discovered inside ""room 28"" at the Chedworth Roman Villa in Gloucestershire, England in 2017, but testing to verify the age of the floor has just been completed, UK conservation charity the National Trust said in a press release Thursday.\nThe date is significant because it had been believed towns and villas fell into decay after being abandoned following an economic crash in the 4th century.\nMartin Papworth, National Trust archaeologist, said the 5th century marks the end of the Roman era in Britain and the beginning of the Dark Ages -- when society is thought by some historians to have deteriorated in western Europe as a result of the Roman withdrawal.\nThe Dark Ages, according to the National Trust, was also a period from which few documents survive, with archaeological evidence scarce.\nNational Trust researchers used radiocarbon dating -- testing the level of carbon in charcoal and bone found in the area where the mosaic was found -- and pottery analysis to pin down the ""unexpected"" age of the mosaic at the villa.\nChedworth Roman Villa is one of the largest Roman villas known in the country and one of the best preserved, according to the National Trust.\nAfter the end of Roman rule in Britain, the army and civil service workers stopped being paid, which in turn triggered ""production decline"" among the craft and service industries, Papworth said in a statement.\nThe quality of the mosaic design possibly reflects this, as the National Trust said it was of ""poorer quality"" than those created in the 4th century.\nHowever, its existence also indicates that society did not decline as rapidly as first thought, and that ""sophisticated life"" carried on for longer, particularly in southwestern England where the mosaic was found.\n""It has generally been believed that most of the population turned to subsistence farming to sustain themselves and, after the break with Rome, Britannia\'s administrative system broke down into a series of local fiefdoms,"" Papworth added.\n""What is so exciting about the dating of this mosaic at Chedworth is that it is evidence for a more gradual decline. The creation of a new room and the laying of a new floor suggests wealth, and a mosaic industry continuing 50 years later than had been expected.""\nThe mosaic was uncovered as part of a six-year program of archaeological research and digs at the Chedworth Roman Villa.\nPapworth said: ""It is interesting to speculate why Chedworth Villa\'s owners were still living in this style well into the 5th century. It seems that in the West Country, the Romanised way of life was sustained for a while.\n""Many large, richly decorated Roman Villas have been found in the countryside around Cirencester, which is around 8 miles from Chedworth.""\nStephen Cosh, a Roman mosaic expert, said in a statement: ""I am still reeling from the shock of this dating.""\n""There are very late Roman mosaics in the area for which archeology can only ever say they must be later than a particular date, without being able to say how much later,"" Cosh added.\n""But none has ever been suspected to be this late. It will be important to research further sites in the region to see whether we can demonstrate a similar refurbishment at other villas which continued to be occupied in the 5th century. But there is no question that this find at Chedworth is of enormous significance -- it\'s tremendously exciting.""']"	['<urn:uuid:0ac83b5f-81ed-4283-a8a7-ea794e35ed7d>']	open-ended	direct	long-search-query	similar-to-document	novice	2025-04-14T18:34:38.591736	60	479	3661
9	What role does internationalization play in modern education systems, and how is technology being used to enhance international collaboration and learning opportunities?	Internationalization plays a vital role in modern education by promoting student, teacher, researcher, and knowledge mobility through various means including scholarships, staff exchanges, and cross-border qualification recognition. Technology, particularly Information and Communications Technology (ICT), significantly enhances internationalization by enabling new forms of teaching, learning, and knowledge sharing across global educational settings. The focus is on creating learning environments that facilitate collaboration in our interconnected world and integrating international and intercultural dimensions into all education levels. Special attention is given to internationalization at home practices, which help develop students' intercultural and global competencies while providing equal learning opportunities for all, especially considering the challenges posed by the digital divide.	"['G20 Education Ministers\' Communiqué\nVideoconference, September 5, 2020\n- We, the G20 Ministers of Education, met virtually on September 5, 2020, to affirm the central role of education in enabling all people to realize the opportunities of the 21st century.\n- As articulated in the G20 Education Ministers\' Statement on COVID-19 of June 27, 2020, we support the individual and collective efforts to mitigate the unprecedented impact of the COVID-19 pandemic on education and acknowledge the importance of ensuring education continuity and safety for all in times of crises.\n- Following the 2018 G20 Education Ministers\' Declaration, we reaffirm that education is a human right and a basis for the realization of other rights, as well as ""the foundation of personal development as it provides children, youth and adults with the knowledge, skills, values, and attitudes necessary to reach their full potential"".\n- In line with the United Nations 2030 Agenda, we reaffirm our commitment to ensuring inclusive and equitable quality education and promoting lifelong learning opportunities for all.\n- We emphasize the vital role of education and skills training in addressing social, cultural, and economic challenges, and, therefore, further our commitment to encouraging international collaboration and the sharing of best practices to advance education systems across the globe. In this way, we will contribute to broader aims, including reducing poverty and inequality; promoting inclusive and sustainable economic growth; advancing access to quality education for all, especially girls, and empowering women, youth and vulnerable groups.\n- We highlight the importance of improving access to quality Early Childhood Education (ECE) as a foundation for the development of current and future generations and as a fundamental part of promoting equity and inclusion in education and encouraging lifelong learning.\n- We recognize the value of fostering internationalization in education as a means of improving the quality of education at all levels and cultivating global citizens who are prepared for an increasingly interconnected world.\n[back to top]\nEduction Continuity in Times of Crisis\n- We support the sharing of best practices and experiences as we explore approaches to building more resilient education systems. We encourage the development of policies and measures to prioritize the continuity of teaching and learning during and after the pandemic and the health and safety of the education community at large, students, teachers, educators, staff and parents, as appropriate in national, regional and local contexts.\n- Werecognizethevalueofdistanceandblendedteachingandlearning and underscore the importance of enhancing access to high-quality education, professional development for educators, digital infrastructure and content, cybersecurity awareness, appropriate teaching methodologies and active learning, while recognizing that these approaches complement face-to-face learning. We stress the importance of research and data to assess the learning outcomes and quality of distance learning.\n[back to top]\nEarly Childhood Education (ECE)\n- We acknowledge the fundamental role that equitable access to quality ECE plays in stimulating children\'s holistic development, which is the basis of their acquisition of literacy, numeracy, and social and emotional skills, to lay the foundation for future learning and well-being.\n- We emphasize the importance of improving the accessibility and affordability of quality ECE for all children, especially those in vulnerable groups.\n- We assert the importance of ECE that focuses on children\'s experiences, development and well-being and fosters positive interactions among ECE personnel, children, families and their communities.\n- We also acknowledge the need to raise family and community awareness of the vital role of quality ECE that is delivered in accordance with children\'s developmental needs at each stage.\n- We underscore the significance of building and retaining an appropriately qualified ECE workforce based on teachers, educators, staff and leaders of ECE institutions who have the knowledge, skills and competencies to work with young children, and on professional training to upskill and reskill them throughout their career.\n- We highlight the value of a smooth transition from pre-primary settings to primary schools. Therefore, we encourage cooperation and collaboration across these educational levels, in accordance with country contexts, so that the benefits of quality ECE are realized and sustained.\n- We recognize that leveraging digital technologies can increase children\'s access to quality ECE and enable families, teachers and educators to create developmentally-appropriate learning experiences for all children. We acknowledge the need to reduce the digital divide by providing the education community, including vulnerable groups, with the support and education necessary to enable effective interaction with technological devices. We also emphasize the importance of studying the effects of such exposure on young children\'s development, learning, and well-being to identify opportunities and mitigate potential risks.\n[back to top]\nInternationalization in Education\n- We emphasize the importance of international collaborations and partnerships in the field of education, especially in the context of global crises such as the COVID-19 pandemic. While respecting national and sub-national laws, rules and policies, we support the promotion of internationalization in education for all through student, teacher, researcher, and knowledge mobility; the provision of scholarships; the exchange of teachers, educators and staff; information-sharing for the facilitation of cross-border recognition of qualifications; the use of Information and Communications Technology (ICT); and international research, knowledge production and technological development.\n- We acknowledge the need for learning environments that enable students, teachers, and educators to collaborate and engage in our interconnected world. We encourage the integration of international and intercultural dimensions into all levels of general (K-12), higher and vocational education and training, where appropriate, to ensure effective learning outcomes.\n- While considering the opportunities and challenges of digitalization, we recognize the significant role that ICT can play in enhancing internationalization in education by shaping new forms of teaching, learning, knowledge sharing and exchange within and among educational settings across the globe. To achieve that role, we will work to support access for the most vulnerable and to reduce the digital divide.\n- We encourage the sharing of best practices in internationalization in education and the adaptation of such practices at the local, national, and international levels, as appropriate. We support advancing the discussion on internationalization at the K-12 level, and considering the COVID-19 pandemic, we recognize the impact of internationalization at home (internationalization practices in a country) in broadening students\' intercultural and global competencies and in providing equal learning opportunities and experiences for all.\n[back to top]\n- We will continue to work on and support knowledge-sharing across G20 members in the areas of education continuity, early childhood education and internationalization in education so that we can all learn together and advance our education systems. We also agree to consider future collaboration and research to examine the impact of COVID-19 on education.\n- We extend our gratitude to the Saudi G20 Presidency for its determined efforts and leadership. We also thank the Arab Bureau of Education for the Gulf States (ABEGS), Islamic Development Bank (IsDB), the Organisation for Economic Co-operation and Development (OECD), the United Nations Educational, Scientific and Cultural Organization (UNESCO), the United Nations Children\'s Fund (UNICEF), and the World Bank Group (WBG) for their valuable contributions to our work.\n- We will submit this Communique to the G20 Leaders\' 2020 Summit and will continue our cooperation towards Italy\'s G20 Presidency in 2021 and thereafter.\n[back to top]\nSource: Official website of the Saudi G20 Presidency\nAll contents copyright © 2021. University of Toronto unless otherwise stated. All rights reserved.']"	['<urn:uuid:37c49f91-85ea-477d-b36d-52d217ff7a6d>']	open-ended	direct	verbose-and-natural	similar-to-document	expert	2025-04-14T18:34:38.591736	169	902	8423
10	how many glasses water prevent kidney stones	It is recommended to drink eight (8) glasses of water each day to help reduce your chances of developing kidney stones.	"[""How Your Kidneys WorkMost people have two kidneys. The kidneys have several very important jobs which include:\n- Clean waste products from your blood.\n- Maintain normal fluid balance.\n- Helps keep the blood pressure in normal range.\n- The kidneys produce a hormone called erythropoieten to help stimulate red blood cell production.\n- The kidneys help keep your bones healthy by maintaining normal levels of certain chemicals in your blood.\nIf you have Chronic Kidney Disease it is important that you do the following:\n- If you are diabetic keep your blood sugar under control.\n- Keep your blood pressure within normal range.\n- Keep appointments with your doctor.\n- It is recommended that you see a kidney doctor (Nephrologist) if your glomerular filtration rate (GFR) is less than 45 or less than 60 if you have protein in your urine.\nLeading Causes of Kidney Disease\n- High Blood Pressure\n- Polycystic Kidney Disease\nChronic Kidney Disease (CKD)Chronic Kidney Disease is defined as the presence of one or more of the following:\n- Kidney damage for > 3 months, as defined by structural or functional abnormalities of the kidney, with or without decreased glomerular filtration rate (GFR).\n- GFR < 60mL/min/1.73m2 for > 3months, with or without kidney damage.\nAcute Renal FailureAcute renal failure occurs rapidly, usually within days, and is typically related to a serious illness, surgery, exposure to toxic substances, or trauma to the kidney. This condition may be reversible with treatment. Patients may need temporary dialysis treatments until the kidney resumes normal function.\nKidney StonesKidney stones include a variety of small, rock-like particles that form in your kidneys. Our nephrologists specialize in identifying the type of stone causing you discomfort and recommending treatment to prevent formation of more stones. The most common stone type is calcium oxalate.\nThings you can do to reduce your chances for developing new kidney stones of this type include:\n- Following a low oxalate diet.\n- Increasing your fluid intake to the recommended eight (8) glasses of water each day.\n- Maintaining the recommended dietary intake of calcium each day.\nNutrition and DietWhen a person's kidney function declines to a stage that they are no longer able to balance specific electrolyte and mineral levels in their blood certain dietary changes may be required.\nThe kidney doctor (Nephrologist) will assess your particular needs based on your lab work and physical exam. Common dietary restrictions for patients with Chronic Kidney Disease (CKD) are Sodium, Potassium and Phosphorus. For people with Kidney Stones (Nephrolithiasis) a low Oxalate diet might be recommended.\nFor people who have high blood pressure (Hypertension) it is recommended that they limit the amount of sodium in their diet. A 2 Gram sodium diet is often recommended. Limiting sodium intake will help maintain blood pressure at a more normal level and decrease fluid retention and swelling (Edema).\nKidneys that are functioning normally will remove excess Potassium through the urine. Patients who have Chronic Kidney Disease may have an elevated level of potassium that stays in their blood. By limiting the amount of high potassium foods that are eaten it will help keep the potassium level in a normal range. Excessively high levels of potassium in the blood require emergency treatment and can be life threatening.\nPhosphorus is a mineral that is found in many foods. It is important to keep the blood phosphorus level within a normal range to help protect the bones. Sometimes it is necessary to take medications that bind to the phosphorus in the foods that are eaten to keep this level in normal range.\nLists of foods for Sodium, Potassium and Oxalate:\n- Sodium - Good Choices and Poor Choices\n- Potassium - Acceptable Potassium Foods\n- Foods High In Oxalates\n- Foods Low In Oxalates\nImportant Minerals in Your DietGood nutrition habits can be built upon a better understanding of the minerals in your blood that your nephrologist will monitor over time. CLICK HERE for a general overview on the importance of these key minerals in your diet.\nMaking Wise Food ChoicesHealthy nutrition is critical for your care. Learn more by visiting our Food Lists to Support Healthy Kidney Functions.\nFrequently Asked Questions about Kidney DietsPlease visit our Nutrition FAQ page\nRecipe LinksFor healthy recipies, please visit:\n- The Recipe Center at www.freseniusmedicalcare.com\n- The Nutrition tab at myfoodcoach.kidney.org/signin\nAccess ServicesDialysis Access Management\nAt Nephrology Physicians, we offer the full scope of care for patients with ESRD and specialize in the following procedures:\n- Vein Mapping\n- Angioplasty, Stenting, and Fistula Salvage\n- Thrombectomy & Trhombolysis\n- Dialysis Catheter Placement and Removal\n- Peritoneal Dialysis Catheter Placement and Repositioning\nGeneral InformationNephrology Physicians is the only practice in the Michiana area to offer all options of dialysis modalities through our association with Fresenius Medical Care. To learn more about these services visit www.freseniusmedicalcare.com\nHemodialysisHemodialysis removes wastes and extra fluid from your blood. During this process, blood is pumped though a filter called a dialyzer. As your blood is filtered only a small amount is outside of your body at a given moment and your filtered blood is retuned to your body. A special access is created, typically in your arm, to allow faster and safer connection to the equipment on a routine basis.\nHome HemodialysisIn 2006 Nephrology Physicians initiated the Michiana area's first option for patients to dialyze at home and our program continues to be highly regarded as an innovative program.\nHome dialysis is very similar to more traditional in-center dialysis except the machine to cleanse your blood is more compact. Home dialysis offers many opportunities for flexibility in scheduling your treatments at times that meet your personal lifestyle. Home Dialysis does require a committed partner to assist you in performing the procedure safely. Your nephrologist will discuss this option with you and your partner to identify if this option is suitable for your needs.\nOur program uses the NxStage System One Cycler to perform your dialysis treatments. You can learn more about this system at http://www.nxstage.com.\nPrior to starting home dialysis, you will receive your treatments at an in-center facility to understand the specific processes that occur during dialysis. This is a time when you will become proficient in self-cannulation techniques to enable you to hook up to your home machine. Several weeks are typically required to work with specialized home dialysis nurses on all of the critical aspects to completing this procedure routinely in a safe and healthy manner.\nPeritoneal DialysisThis type of dialysis is done at home. The lining of the abdomen acts as a natural filter to clean the blood with a solution called dialysate. Dialysate is infused into the peritoneal cavity through a catheter. Waste products and excess fluid are drawn into the dialysate from the blood and drained from the body when the fluid is drained out through the catheter.\nPeritoneal Dialysis Fact Sheet\nThere are two types of peritoneal dialysis:\n- Continuous Ambulatory Peritoneal Dialysis (CAPD): Exchanges are done 4-6 times a day.\n- Continuous Cycling Peritoneal Dialysis (CCPD): A machine called a cycler does the exchanges automatically at night while the person sleeps.\nNocturnal DialysisNocturnal hemodialysis treatment takes place for an average of 8 hours a night, 3 times a week at the dialysis facility. Dialysis professionals monitor you throughout the night while you sleep.\nWhile traditional hemodialysis provides effective treatment, nocturnal hemodialysis offers a longer, slower treatment for patients who need additional time to remove fluids.\nThe only local nocturnal program is offered at the Fresenius Medical Care - South Bend facility at 320 St. Joseph, South Bend, IN\nDo I still see my nephrologist while I am on dialysis?Your nephrologist manages your dialysis treatment prescriptions. Your doctor will meet with you during your dialysis treatment on a monthly basis. This rounding includes the whole care team including nursing staff, a social worker, and dietician to better understand how you are adjusting to the treatments and address any of your questions. Your doctor may make adjustments to your medications or your dialysis prescription to maximize your health and comfort level. Our Nurse Practitioners assist the nephrologist in managing your treatment by rounding up to three additional times per month to monitor your progress, discuss changes to your care, and assist in helping you find other resources to manage your disease.\nPatient Education Programs\nNephWiseThis is an informative, introductory class for patients who have recently been diagnosed with CKD Stage 3. This group session is held monthly at our office and last approximately 1 hour. For additional information or to reserve a space, please contact our office at 574-273-6767.\nMIPPAThis educational program abbreviation references the Medicare Improvements for Patients and Providers Act. This service is available for Medicare beneficiaries with a diagnosis of CKD Stage 4. This is a series of one-on-one visits at the office with a nephologist and/or Nurse Practitioner.\nTOPsThis educational program abbeviation references the Treatment Options Program offered by Fresenius Medical Care. This free class is provided for patients with a CKD Stage 4 or 5 diagnosis who want to paticipate in learning about dialysis options. For additional background information visit www.ultracare-dialysis.com\nInternet Education LinksNational Kidney Foundation (NKF): Information for organ donors and recipients, for patients and professionals, m0eetings and events and support. An AZ guide for kidney disease.\nAmerican Kidney Foundation (AKF): Information about this national voluntary health organization, as well as kidney disease facts.\nAmerican Association of Kidney Patients (AAKP): Have you recently been diagnosed with reduced kidney function? View information on resources for those with chronic kidney disease (CKD) or at risk.\nKidney School is an interactive, web-based learning program designed to help people learn what they need to know to understand kidney disease.\nUnderstanding Kidney Transplant Information and Advice for PatientsPlease visit: www.transplantexperience.com\nOur Physicians and Staff are dedicated to taking care of patients with Chronic Kidney Disease. If you have been diagnosed with Chronic Kidney Disease, Kidney Stones or Hypertension talk to your doctor about a referral to Nephrology Physicians. We offer the most comprehensive care for kidney disease in the Michiana area. To refer yourself, go to https://www.nephinc.com/patient-referral.asp or call 574-273-6767 ext. 1205.""]"	['<urn:uuid:9f19c992-c917-4a56-8594-9581f7b71d0b>']	factoid	direct	short-search-query	similar-to-document	novice	2025-04-14T18:34:38.591736	44	119	10928
11	euro step variations reverse one handed execution nba players	There are two main variations of the euro step: the reverse euro step and the one-handed euro step. The reverse euro step involves quickly changing direction opposite to the original move for increased deception. The one-handed euro step involves dribbling with only one hand while performing the movement, allowing for maximum agility and quickness. Notable NBA players who have mastered the euro step include James Harden, Giannis Antetokounmpo, and Nikola Jokic. Some memorable moments include Harden's game-winning layup against Oklahoma City Thunder in the 2017 playoffs and LeBron James's winning basket against Andre Iguodala in Game 6 of the 2016 NBA finals.	['The euro step is an exciting and unique move seen in basketball, utilized by some of the greatest players to ever play the game. It has become a signature move for many of today’s professional athletes and can be used to create space between a defender and shooter. With its roots in European basketball, the euro step looks like a cross between a crossover dribble and a spin move. It has become a favorite move among many players and coaches because it gives the offensive player an advantage over defenders. The euro step is used to great effect by many of today’s players, including LeBron James, Stephen Curry, and Kyrie Irving.\nWhat is the Euro Step?\nThe euro step is a move used in basketball that involves a player taking two steps in one direction and then quickly changing direction, usually with the intent of creating an open shot. It often looks like a cross between a crossover dribble and a spin move, and has become popular among many of today’s professional athletes due to its effectiveness at creating space between the shooter and defender. Originating from European basketball, the euro step has become a signature move for many of today’s athletes, including LeBron James, Stephen Curry and Kyrie Irving.\nHistory and origin of the euro step\nThe euro step originated in Europe, where it is known as a “step-back” move. It was first popularized by Italian basketball player Dino Meneghin, who used the move to create space between himself and his defender. The move then spread to other countries throughout Europe before making its way to the United States where it has become a staple of the game. It was popularized in the US by NBA players such as Kobe Bryant, who showcased the move during his career.\nBenefits of the euro step\nIncreased efficiency in scoring\nOne of the main benefits of using the euro step is its increased efficiency in scoring. By quickly changing direction, players are able to create space between themselves and their defenders, allowing them to have more time and space to get a shot off. Additionally, this move can be used to create openings for cuts or drives towards the basket, creating more opportunities for points. This makes it particularly useful for players looking to score quickly and efficiently.\nAbility to maneuver around defenders\nThe euro step is also effective at maneuvering around defenders. By quickly changing direction, the offensive player can throw off their defender and create open space for a shot or drive towards the basket. This deceptive move allows players to take advantage of their opponents and create opportunities for themselves, allowing them to gain an edge over their opponents.\nHow to perform the euro step\nSteps and breakdown of the move\nStep 1: Begin with a crossover dribble, moving the ball from one hand to the other while shifting your body weight. As you do this, make sure that you are pushing off of your back foot and stepping in towards your defender.\nStep 2: As soon as you finish your crossover, take a second step in the same direction as the first one. This will help to create space between you and your defender.\nStep 3: As soon as you take the second step, quickly shift your body weight toward the other direction and plant your outside foot. You can also add a slight hop to further throw off your defender.\nStep 4: Once you have planted your outside foot, begin dribbling in the opposite direction that you were originally going. This will create an even bigger opening between you and your defender.\nPractice drills to improve euro step technique\nPractice drills to improve euro step technique are important for players who want to sharpen their skills and become more comfortable with the move. These drills can help a player develop strength, agility, and muscle memory so that they can utilize the move in game situations. One drill involves taking two dribbles forward and then quickly changing direction by planting the outside foot and pivoting in the opposite direction while dribbling the ball. This drill can be done multiple times in a row to help players develop quickness and accuracy with their euro steps. Additionally, drills that involve two or more players are beneficial as they allow players to practice their moves against live defense and learn how to read and react in real game situations.\nExamples of the euro step in action\nNBA players who use the euro step\nNBA players who use the euro step are some of the most talented players in the league. Notable examples include Houston Rockets guard James Harden, Milwaukee Bucks forward Giannis Antetokounmpo, and Denver Nuggets center Nikola Jokic. These players use the move to create openings for themselves on offense and score efficiently against their opponents. Additionally, these players have mastered how to maneuver around their defenders with the move, allowing them to create even more space for themselves on the court.\nMemorable moments featuring the euro step\nMemorable moments featuring the euro step are numerous within the NBA. One of the most iconic examples occurred during the 2017 NBA playoffs when James Harden of the Houston Rockets employed a successful euro step to evade Russell Westbrook and sink a game-winning layup against the Oklahoma City Thunder. Another memorable moment came during Game 6 of the 2016 NBA finals when LeBron James used a Euro step to drive past Andre Iguodala and score the winning basket for the Cleveland Cavaliers. These moments will forever be remembered as some of the most iconic plays in basketball history.\nVariations of the euro step\nReverse euro step\nThe reverse euro step is a variation of the standard euro step move that gives players an even greater edge against their defenders. This variation involves quickly changing direction in the opposite direction of the original move, which allows for more deception and misdirection. To perform this move, start by taking two dribbles in one direction and then quickly planting your outside foot and pivoting in the opposite direction. This can be a great way to create an even bigger opening between you and your defender.\nOne-handed euro step\nThe one-handed euro step is a powerful variation of the standard euro step move that allows for maximum agility and quickness on the court. This move involves taking two dribbles, planting your outside foot, and then quickly shifting your weight in the opposite direction while dribbling the ball with only one hand. This can be an effective way to create even more space between you and your defender as they may not expect the ball to be in your hand.\nTips for executing the euro step effectively\nTiming and footwork\nTiming and footwork are essential when performing the euro step. The move needs to be executed with quickness and precision and must be done in a split second. Players should practice their timing and footwork before trying the move during a game. This is best done by doing drills that involve two or more players, as this allows for live defense and an opportunity to develop muscle memory.\nReading the defender\nReading the defender is an important skill to master when executing a euro step. Players must be able to read and react quickly to their opponents’ movements in order to gain an advantage on offense. To do this, players should take note of their opponent’s body language, such as foot positioning and arm placement. By doing so, players can anticipate their next move and use the euro step to get around them.\nThe euro step is a powerful move that gives players an edge on offense and allows them to create more space for themselves. Notable examples of NBA greats who have mastered the move are James Harden, Giannis Antetokounmpo, and Nikola Jokic. There are also several variations of the move such as the reverse euro step and one-handed euro step that offer even more misdirection and agility on the court. To execute the move effectively, players should practice their timing and footwork as well as read and react quickly to their opponents’ movements.\nMastering the euro step is essential for any basketball player looking to elevate their game to the next level. Not only does it allow players to create more space on offense, but it also gives them an advantage over defenders who may not be expecting the move. This is especially true when coupled with variations like the reverse euro step or one-handed euro step, which can be used to even greater effect. With practice and dedication, the euro step can become a go-to move for any basketball player looking to gain an edge on offense.']	['<urn:uuid:8a492973-35b5-4799-90f7-f10fa2b33fae>']	open-ended	direct	long-search-query	similar-to-document	expert	2025-04-14T18:34:38.591736	61	666	8570
12	How much electricity can a marine turbine with 30-foot rotor blades produce?	A marine turbine with 30 foot long rotor blades can produce 300 kilowatts of electricity.	"[""People are becoming aware of the importance of ocean energy, which will lead to a new era of alternative energy. Because of this knowledge, tidal energy has great potential and bright prospects in future.\nLen Calderone for | AltEnergyMag\nGravity plays an important role in our lives. It keeps our feet on the ground and the planets in order. Gravity is very noticeable on our planet, specifically, it is responsible for the rise and fall of the ocean’s tides worldwide.\nTo understand how the tides work it’s important to understand the relationship between the motion of our planet and its moon. The Moon's gravitational forces are strong enough to cause the oceans to bulge, which causes a high tide. Because of the rotation of the Earth a high tide also occurs on the other side of the planet. As the moon moves around the Earth, the bulge (high tide) moves with it. The moon’s movement creates the waves that are part of the tidal energy.\nTides are more predictable than wind energy and solar power. The main problem with tidal power is its comparatively high cost and limited accessibility of locations that have suitably high tidal ranges or flow velocities.\nTidal energy is one of the oldest forms of energy dating back to the seventh century. The tides turned waterwheels, producing power in order to mill grain, similar to what is used in streams.\nThe emission of gases responsible for global warming and acid rain is eliminated by the use of tidal energy.\nThe Rotech tidal turbine has been designed to be uncomplicated and tough, keeping costs and maintenance down. With the turbine unit attached to the seabed, the rotor blades are designed to work in either tide direction so that the whole unit does not need to be rotated, or the pitch of the blades changed when the tide turns.\nThe Venturi effect is the reduction in fluid pressure that results when a fluid flows through a constricted section of pipe. A Venturi shaped duct is used to channel and increase the speed of the tidal stream toward the rotor. By using a Venturi duct, more energy can be extracted from the same amount of water with smaller diameter rotor blades. This design is suitable for slow tide movement, and a wide angle diffuser will help if the turbine axis is not aligned with the tidal flow, as turbine assemblies are bi-directional or multidirectional.\nA tidal barrage uses the energy of the water, captured at high tide behind a dam in an estuary. Water flows to the lower level as the tide recedes, driving turbines which in turn drive electric generators. The system could work both ways. A typical tidal power barrage may look like this:\nA barrage is similar to a hydroelectric dam, and is essentially a large concrete structure that spans an estuary basin, bay or river with sluices that enable waves to move in and out of the dammed area. At high tide, the sluices are closed; but when the tide shifts directions and ebbs, the sluices are opened allowing the higher water levels in the basin to flow through the barrage and pass the hydroelectric turbine on its way out to sea. The basin is refilled when the tide flows back to shore.\nAnother form of tidal energy is dynamic tidal power that requires the construction of a long dam perpendicular to the coast that could be over 15 miles long, since the structure has to be long enough to influence tidal patterns and cause high and low tides to occur simultaneously on opposite sides of the structure.\nBelow is a cut-away of a dam caisson showing a turbine that generates electricity with the flow of water between one side of the dam and the other.\nDynamic tidal power does not require the enclosure of a basin, which reduces its environmental impact. This long caisson would interfere with the coastal parallel tidal wave hydrodynamics, creating water level differences on opposite sides of the barrier which drive a series of bi-directional turbines, which are installed in the caisson. Oscillating tidal waves which run along the coasts of the continental shelves contain powerful hydraulic currents.\nIt is estimated that some of the largest dams could accommodate over 15 GW (15000 MW) of installed capacity. If the average American household consumes 12,000 KWh per year, one dynamic tidal power dam could supply energy for about 1.5 million households.\nA marine current turbine looks like a smaller version of a land windmill and the principle is precisely the same. But whereas a windmill draws energy from the movement of air, the marine turbine uses currents in the water. A single 30 foot long rotor blade will be able to produce 300 kilowatts of electricity.\nSince the ocean currents are more reliable than wind, marine turbines are a rival to wind power. They are also less obtrusive, since the structure is built on the seabed and projects just a few feet above the surface. Fish can feel safe because the blades rotate slowly at only 20 revolutions per minute.\nThe wave snake is a device that is a big red cylindrical tube that is 426.5 feet long, 13 feet in diameter, weighing around 750 tons. The snake has a life expectancy of up to 20 years, with tubes that are connected by hinges so that they float like a snake in the water. The snake rises up and down as the passing waves tug on the hinges, which are resisted by hydraulic rams, which pump high-pressure fluid through hydraulic motors and turn electrical generators to produce electricity. The energy, which comes from the joints on each wave-energy conversion, is fed via a cable to a central undersea export cable, which carries the combined power generated from a nest of snakes to shore.\nOcean thermal energy conversion (OTEC) refers to technologies that utilize the temperature differential between the upper and lower layers of the ocean to drive a heat engine. In geographical areas with warm surface water and cold deep water, the temperature difference of at least 38 degrees Fahrenheit can be leveraged to drive a steam cycle that turns a turbine and produces power. Warm surface sea water passes through a heat exchanger, vaporizing a low boiling point working fluid to drive a turbine generator, producing electricity. These power plants use the difference in temperature to make energy.\nThe water on the ocean’s surface is used to heat a pressurized liquid, usually ammonia, which boils at a temperature slightly below that of warm seawater. That liquid becomes gas, which powers a turbine generator. Cold water is then pumped from the ocean’s depths through a long submerged pipe in order to condense the gas back into a liquid, and the cycle is repeated.\nThe best ocean thermal resources are located in tropical and subtropical deep waters. Ocean thermal energy systems have three cost fundamentals, which are the platform, the cold water pipe, and the heat exchangers. Prefabricated cold water pipes made from inexpensive, lightweight composite materials, have the potential to appreciably lower the cost of OTEC technology, along with new platform construction techniques.\nWith the increasing demand for energy, the traditional fossil energy sources, such as coal, oil, natural gas, will eventually be depleted. Plus these same fossil fuels lead to many environmental issues. People are becoming aware of the importance of ocean energy, which will lead to a new era of alternative energy. Because of this knowledge, tidal energy has great potential and bright prospects in future.\nFor further information:\nThe content & opinions in this article are the author’s and do not necessarily represent the views of AltEnergyMag\nThis post does not have any comments. Be the first to leave a comment below.\nPost A Comment\nYou must be logged in before you can post a comment. Login now.""]"	['<urn:uuid:8580c709-782b-42aa-ac5b-cd640eb33cc1>']	factoid	with-premise	concise-and-natural	similar-to-document	expert	2025-04-14T18:34:38.591736	76	89	7711
13	explain main differences between computer simulations and bayesian statistics method	Stochastic simulation samples from an unconditional probability distribution and does not assert a prior. In contrast, Bayesian analysis often uses the Metropolis Hastings method to approximate the marginal distribution function and involves the accept-reject test to sample from a complicated target distribution. While stochastic processes are not inherently part of Bayesian analysis, if you formulate the stochastic process in terms of evolution of probability density functions, with the prior being the historical density function, then the problem formulation becomes similar.	"[""Deep probabilistic programming (DPP) combines three fields: Bayesian statistics and machine learning, deep learning (DL), and probabilistic programming. In this webinar, our expert panel discussed DPP tools and related theory relevant for Bayesian forecasting and decision making with financial time series data and other types of financial data (e.g. limit order books, news etc).\n- How does DPP differ conceptually from frequentist statistics and machine learning?\n- Why represent probabilistic models as a computational graph?\n- What are the DPP tools, methodologies and applications that are most important for finance?\n- Is DPP the future for risk modeling using complex datasets?\nHear it from the experts...\nThe panel summarize their answers to the audience Q&A.\nIs deep learning a type of deep probabilistic programming method?\nStrictly, deep learning by itself, as it commonly known is not a deep probabilistic programming method. Deep learning is an example of a deterministic method - it is purely algorithmic and not probabilistic. However, the types of data representations that deep learning permits are central constructs in DPP. DPP is really a combination of deep learning and probabilistic programming.\nHow do you model graphical relationships in financial data?\nGraphical relationships are based on subjective causal relationships. X caused Y. In many cases, this causal relationship requires fundamental knowledge of the asset. For example, the effects of increased oil pipeline maintenance costs on the price of WTI crude. See here for further examples: https://kuscholarworks.ku.edu/bitstream/handle/1808/161/CF99.pdf;jsessionid=0ACADE1EA67D04B14C25BC8060F7B0F0?sequence=1\nIt also possible to infer the graphical relationship (identification from a set of different structures) through a maximum likelihood estimate. This, of course, is challenging and will lead to a non-unique solution. Riccardo Rebonata recently wrote a book on coherent stress testing covering this area: http://onlinelibrary.wiley.com/book/10.1002/9781118374719\nDo you have any suggestions when choosing a suitable online platform to do deep learning?\nCloud hosted services with a jupyter notebook and tensorflow are powerful and flexible. In general, it's important to be able to have programmatic control of the data input and manipulation, rather than using a GUI. An important aspect of machine learning is how to provide the input to the machine and this is difficult to scale with GUIs.\nWhat are your thoughts on combining RNNs and financial time series modelling with DPP? Do you foresee an advantage in training Baysian RNNs vs classical time series models?\nThis is an excellent direction. The main advantage is having uncertainty estimates with the time series prediction, rather than just predicting the expected prices. DPP enables a probabilistic representation of the time series prediction.\nHow can we trust our predictive uncertainty estimates/posterior predictive distributions in a practical financial setting, given well-known problems with common approximate inference algorithms like VI and HMC ? (mode-seeking, high variance, difficulty in capturing multi-model posteriors etc.)\nThis is an important question and the problems that you allude to present many challenges for researchers in this area. The short answer to your question is that one must perform statistical tests to check that the posterior distribution captured the tails correctly. QQ-plots, KS, Shapiro-Wilks etc are some of the tests that can be performed on the marginals.\nHow is bayesian analyses which has a prior different from a stochastic simulation, which also tries to spans the search space?\nStochastic simulation samples from an unconditional probability distribution and does not assert a prior. Bayesian analysis often uses the Metropolis Hastings method to approximate the marginal distribution function and involves the use of the accept-reject test to sample from a much more complicated target distribution. There are a whole suite of so called variational Bayesian methods which are well known to be equivalent, under certain restrictive assumptions, to other techniques, e.g. particle-filter methods etc. Also Bayesian analysis isn't about using stochastic processes per se (which is often how monte carlo methods are used in quant finance), although if you formulate the stochastic process in terms of evolution of probability density functions, with the prior being chosen as the historical density function, then the problem formulation seems very similar.""]"	['<urn:uuid:360f9128-caf5-4fbe-9579-1d42870aab65>']	open-ended	direct	long-search-query	distant-from-document	novice	2025-04-14T18:34:38.591736	84	583	4558
14	urban bees nyc red honey cause 2010	In the summer of 2010, mysterious red honey appeared in New York City bee hives. Initially thought to be from sumac plants, testing revealed the honey contained Red Dye No. 40. The bees had been collecting sugar syrup from a Maraschino cherry factory in Brooklyn, rather than foraging on flowers. The honey was originally intended for a local restaurant before the artificial source was discovered.	['Journal of Urban Ecology\nThe Journal of Urban Ecology covers all aspects of urban environments. This includes the biology of the organisms that inhabit urban areas, human social issues encountered within\nMysterious red honey began to appear in the hives of New York City bees in the summer of 2010. At first, beekeepers thought their bees were foraging on some strange plant, possibly sumac. But after more beekeepers began to find red honey in their hives, they decided to get their honey tested. As it turned out, the honey was filled with Red Dye No. 40, and instead of foraging on flowers, the bees had been collecting sugar syrup from a Maraschino cherry factory in Brooklyn.\nThe story of New York’s red honey struck a chord with those already concerned about honey bee health. Bees have been hit hard by a host of challenges ranging from parasitic mites to neonicotenoid pesticides—but could red honey be another sign of bee decline? Could artificial flavors and chemicals in human foods be toxic to bees? Could we be at risk if we eat “local honey”?\nRooftop hive at the American Tobacco Campus in Durham, NC, managed by Bee Downtown. Photo by Lauren Nichols and used with permission.\nAs people were asking questions about New York’s bees, my lab group was studying another insect in New York—ants. Over 8.9 million people live in New York, but there are at least 16 billion ants. That means for each person living in New York there are nearly 2,000 ants. In an area that is almost 90% concrete, how could so many ants survive? The secret, we thought, might lie in what was happening with New York’s bees. Rather than feeding on dead insects and other “natural” foods, we guessed ants might be switching to human foods.\nThe average person living in a city produces 1,000 pounds of garbage each year, and of that, 15% is food waste. With over half the world’s population now living in cities, this amounts to 250 million tons of food thrown out in cities each year, which represents a massive potential resource for urban animals. We found that much of this food in New York was making it into urban ant colonies. This was especially true in the most urban areas of the city, like the sidewalks running down Broadway. A follow-up study found that ants living on Broadway alone consumed the equivalent of 60,000 hotdogs per year—more than city rats or birds consumed in the same area.\nBut what about urban bees? Anecdotes about red honey aside, no serious investigation had been carried out on the diet of urban bees. Over the last decade, cities began to change local ordinances that had previously outlawed beekeeping inside city limits, and urban beekeeping has become increasingly popular. Along with backyard chickens and rooftop gardens, urban beekeeping has become a major part of the local food movement. In fact, the red honey discovered in New York was intended for a local restaurant until it turned out to be recycled sugar syrup. Despite growing interests in urban bees, there was still no clear answer as to whether bees were sticking to flower nectar or finding new sugar sources in cities.\nUsing the same techniques we used to study ants in New York, we began studying the diet of bees in Raleigh, North Carolina. Human-produced sugars, like sugarcane and high-fructose corn syrup, have a characteristic carbon isotope signature that can be used to determine if bees are feeding on flower nectar or, say, someone’s leftover soda. Bees in rural areas should only have access to flower nectar, but if city bees are feeding on human food sources, then their carbon isotope signature should show it.\nTo our surprise, we found no difference in carbon isotopes between bees living in downtown Raleigh and those living outside the city. In both habitats, bees were sticking to flower nectar and largely avoiding human sugar sources. This is good news for urban beekeepers and people who buy local honey, and it also shows that urban flowers play a major role in maintaining healthy pollinator populations in cities.\nIn the future, we plan to partner with urban beekeepers in larger cities, like New York and Tokyo, to see if bees are still able to sustain their colonies on local flowers. Will we find bees similar to those in Raleigh, or will we uncover new mysterious shades of honey inside the hives of big-city bees?']	['<urn:uuid:76c368d9-5231-4f76-80bd-dae395e35288>']	open-ended	with-premise	short-search-query	similar-to-document	expert	2025-04-14T18:34:38.591736	35	398	4333
15	tools needed for efficient quran translation process	Translation memories and term bases are essential tools for efficient translation of Qur'anic Studies sources. Without these tools, maximum efficiency in translating from/into Persian cannot be achieved, as they facilitate decision-making in the translation process.	['A Model for Crowdsourcing Development of Databases for Qur’anic Studies Sources\nTechnology has become an integral part of the translation task. Nevertheless, few translation memories and term bases are available for translating Qur’anic Studies sources. Without them, attaining maximum efficiency in this field is not possible because such tools facilitate decision-making in the translation process from/into Persian. There is an imperative need for developing such databases. Creating parallel corpora and aligning them to come up with translation memories and term banks can help improve the quantity and quality of translations of Qur’anic Studies sources from/into Persian. However, this task cannot be carried out by a single person. Using crowdsourcing in developing TMs and TBs for Qur’anic Studies sources is an alternative that can expedite the task. Nonetheless, crowdsourcing in developing such databases is a relatively unattended research area. Examining existing models revealed that no pre-existing Translation Studies model suited the needs of this study. With the motive of filling this gap, the researchers opted for developing and validating a model for human resource management in Translation Studies through adopting a crowdsourcing model (the Metropolis Model) and adapting it for their specific conditions (developingthe Jāmiʿ model). Findings of this research indicate that the Jāmiʿ Model is adequate for developing TMs and TBs.\nBarlas, Y., & Carpenter, S. (1990). Philosophical Roots of Model Validation: Two Paradigms. System Dynamics Review, Vol. 6, No. 2, pp. 148–166.\nBowker, L. (2015). Computer-Aided Translation Translator Training. In Chan Sin-Wai (ed.), the Routledge Encyclopedia of Translation Technology. London and New York: Routledge.\nBrabham, D. C. (2008). Crowdsourcing as a model for problem solving: An introduction and cases. Convergence: The International Journal of Research into New Media Technologies, 14, 75–90.\nCronin, M. (2010).The translation crowd. Revista tradumàtica, 8.\nDePalma, D. A. & N. Kelly. (2011). ‘Project Management for Crowdsourced Translation: How User-translated Content Projects Work in Real Life’, in K. J. Dunne & E. S. Dunne (Eds.) Translation and Localization Project Management: The Art of the Possible (pp. 379–407). Amsterdam and Philadelphia: John Benjamins.\nDésilets, A. (2007). Translation Wikified: How Will Massive Online Collaboration Impact the World of Translation? In Proceedings of the Translating and the Computer 29 Conference, 29–30 November 2007, ASLIB, London, UK.\nDombek, M. (2014). A Study into the Motivations of Internet Users Contributing to Translation Crowdsourcing: The Case of Polish Facebook User-Translators. Unpublished Doctoral Thesis. Dublin City University.\nDunne, K. & E. S. Dunne. (2011). Translation and Localization Project Management: The Art of the Possible. Amsterdam and Philadelphia: John Benjamins.\nGarcia, I. (2015). Cloud Marketplaces: Procurement of Translators in the Age of Social Media, the Journal of Specialised Translation, No. 23, pp 18 - 38.\nHowe, J. (2006). The Rise of Crowdsourcing. Wired 14 (6): 176–183. http://www.wired.com /wired/archive/14.06/crowds.html\nJiménez-Crespo, M. A. (2017). How much would you like to pay? Reframing and expanding the notion of translation quality through crowdsourcing and volunteer approaches. Perspectives. Studies in Translation Theory and Practice, 25(3). 478–491.\nKazman, R., & Chen, H. (2009). The Metropolis Model A New Logic For Development Of Crowdsourced Systems. Communications of the ACM, 52(7), 76–84.\nMcDonough, J. (2012).Analyzing the Crowdsourcing Model and Its Impact on Public Perceptions of Translation. The Translator, 18 (2), 167–191.\nO’Hagan, M. 2016. Massively Open Translation: Unpacking the Relationship Between Technology and Translation in the 21st Century. International Journal of Communication, 10, 929–946 1932–8036/20160005.\nSurowiecki, J. (2004). The Wisdom of Crowds: Why the Many are Smarter than the Few and How Collective Wisdom Shapes Business, Economies, Societies, and Nations. New York: Doubleday.\nViitamaki, S. (2007). The FLIRT Model of Crowdsourcing Collective Customer Collaboration. Retrieved from http://www.samiviitamaki.com/2007/02/16/the-flirt-model']	['<urn:uuid:318b4217-be47-42e5-b54a-3097a587087c>']	factoid	with-premise	short-search-query	distant-from-document	expert	2025-04-14T18:34:38.591736	52	266	4264
16	How big is the tide variation in French Polynesia?	The tidal variation in French Polynesia is only about 1-2 feet.	['Many sailors crossing the Pacific choose not to stop in many of the Tuamotos atolls due to the challenge of navigating the reef passes — the currents can be extremely dangerous, flowing up to 10 knots, occasionally creating standing waves to 2-3 feet.\nPredicting when it is safe to navigate a reef pass can be tricky. Sailors often talk about “timing the slack tide” in Tuamotos passes, but this is somewhat misleading. What is really important is the overall current in the pass– which is caused by a combination of tide, swell and wind.\nThe tide has the most obvious effect on the current. Luckily the tidal variation in French Polynesia is only about 1-2ft, but the atolls are so large that a formidable amount of water still needs to move in and out of the lagoon twice a day.\nSlack tide is normally the calmest time in a reef pass – assuming there is no wind and swell. The wind can seriously disturb the pass when it is moving in the opposite direction to a strong current, resulting in confused standing waves. So we avoid transiting the pass when wind and current are in opposition.\nAn even more significant hazard for the pass, however, are the swell conditions. If there’s a moderate to large swell (2m+) there is a huge amount of water that is poured into the lagoon. Satellite images and nautical charts make it seem like the atolls are a complete ring of land, but the majority of the ring is actually submerged reef with a peppering of motus ( little islands of land in the barrier reef).\nWaves can come from groundswell (far away storms, with large periods) or windswell (localized strong winds with short periods) such as the ‘Maraamu’ South-East trade winds that blow 25knots+. The extra water these swells push into the lagoon must escape through the reef pass. This creates an additional outgoing current of 1-6 knots.\nWhen this swell-driven outgoing current combines with an outflowing tide, a river of water can flow towards the ocean at an incredible 8-10 knots! Even a big ship might have a hard time during such conditions.\nThe best time to enter a pass during such a swell event is usually during the peak of an incoming tide, so that it neutralizes the outgoing current. Then, the outgoing current may be a reasonable 1-2 knots. Therefore, in the case of swell events, slack current is NOT slack tide.\nNote: swell can also create large waves that break next to narrow reef passes making them difficult to impossible – like Maupiti in the western Society Islands. This is an entirely different problem, but still relevant for navigation!\nNaturally, it’s best to travel across reef passes when the swell and wind is moderate, and tide is slack. But we don’t always have that luxury. Being able to factor for the effects of swell and wind is critical for the safe navigation of reef passes in less-than-ideal conditions.\nPHOTO: shows the Tiputa pass in Rangiroa. It is a 40 mile wide atoll with a huge amount of water moving in and out of the pass, generating large standing waves which can be seen on satellite images. This famously attracts dolphins, which divers come to swim with. We entered Tiputa pass after waiting an hour for the tide to shift, and still the outgoing current was nearly 6 knots. However we were blessed with an amazing moment when a huge dolphin jumped directly in front of Aldebaran as we were barely moving forward with the engine at full throttle!']	['<urn:uuid:f03ad46a-96b1-4be4-81b7-3ad0e0266080>']	factoid	direct	concise-and-natural	similar-to-document	novice	2025-04-14T18:34:38.591736	50	63	3398
17	food supplements vs natural foods for bones	While supplements like vitamin D, calcium, magnesium, and vitamin K2 are commonly taken for bone health, getting nutrients from natural food sources is ideal. For vitamin B12, which is crucial for bone health, the best natural sources include wild-caught Alaskan salmon, raw milk, pastured free-range eggs, grass-fed beef, and beef liver. However, people over 50 may have difficulty absorbing B12 from food due to decreased stomach acid production, so they might benefit from sublingual supplements or injections. Oral B12 supplements are generally ineffective due to poor absorption of the large B12 molecule.	['Vitamin B12 Deficiency and Bone Health\nVitamin B12 Deficiency and Bone Health\nStudies from the US Framingham trial show nearly two-fifths of the US population may have suboptimal blood levels of vitamin B12.1 And the criteria they use to make this assessment is 6-700 pg/ml, so it may be the majority of people who are vitamin B12 deficient.\nThis is important to be aware of, and correct if it applies to you, as vitamin B12 is important for the formation of red blood cells, the maintenance of your central nervous system, and plays a role in the production of DNA and RNA.\nVitamin B12 is also fittingly known as the energy vitamin, and your body requires it for a number of vital functions, including energy production. Much less is known about its role in bone health, although it’s emerging as an important player.\nEven though vitamin B12 is water-soluble, it doesn’t exit your body quickly in your urine like other water-soluble vitamins. Instead, B12 is stored in your liver, kidneys, and other body tissues.\nAs a result, a deficiency may not show itself until up to seven years later, and by this time damage to your bones may have already set in…\nVitamin B12 Deficiency May Harm Your Bones\nResearch published in the New England Journal of Medicine (NEJM) revealed that mice deficient in vitamin B12 have growth retardation and fewer osteoblasts (cells responsible for bone formation).2\nThe researchers suggested that lack of vitamin B12 may interfere with growth signaling in the liver and its “downstream effect” on the osteoblasts.\nRecent research also suggests low vitamin B12 status may increase the risk for bone fractures in older men.3 This risk remained even after taking into account other important factors such as smoking, vitamin D status, and calcium intake. As reported by MedicineNet:4\n“Men in the group with the lowest B-12 levels were about 70 percent more likely to have suffered a fracture than others in the study. This increased risk was primarily due to fractures in the lumbar spine, where there was an up to 120 percent greater chance of fractures.”\nOlder women with low levels of vitamin B12 (below 208 pg/ml) also experienced significantly more rapid hip bone loss – a sign of osteoporosis – than women with higher levels of B12 in a separate study.5\nElevated homocysteine levels (an amino acid) and low vitamin B12 have also been associated with deteriorated bone health,6 and this may be one avenue by which B12 influences bone health (B vitamins are known to suppress homocysteine). As explained by the Linus Pauling Institute:7\n“High homocysteine levels may affect bone remodeling by increasing bone resorption (breakdown), decreasing bone formation, and reducing bone blood flow.\nAnother proposed mechanism involves the binding of homocysteine to the collagenous matrix of bone, which may modify collagen properties and reduce bone strength.\nSince vitamin B12 is a determinant of homocysteine metabolism, it was suggested that the risk of osteoporotic fractures in older subjects might be enhanced by vitamin B12 deficiency.”\nIndeed, a meta-analysis found that raising vitamin B12 levels in older individuals lead to a reduction in fracture risk.8\nMove Over Vitamin D and Calcium, Are You Getting Enough Vitamin B12?\nVitamin D, calcium, magnesium, and vitamin K2 are crucial for bone health, but also are among the most common supplements taken by older adults for this very reason. Vitamin B12, on the other hand, is often overlooked.\nIf you’re a vegan who does not eat animal products, you are at high risk of deficiency, as B12 is readily bioavailable in its natural form only in animal food sources. This doesn’t necessarily have to be meat — eggs and dairy are options also. Top foods to include are:\n- Wild-caught Alaskan salmon\n- Raw milk\n- Pastured free-range eggs\nIf you do consume animal products, then consider adding these foods that are even higher in vitamin B12:\n- Grass-fed beef and beef liver\n- Pastured organic free-range chicken\nHowever, keep in mind that even if you do eat animal foods, a supplement can be beneficial if your body’s ability to absorb the vitamin from food is compromised, which is especially prevalent as you age.\nWhen you get older, the lining of your stomach gradually loses its ability to produce hydrochloric acid (the stomach acid suppressed by proton pump inhibitors), which releases vitamin B12 from your food. If you’re over 50, it’s safe to assume you are not absorbing vitamin B12 at an optimal level.\nHowever, just because you are over 50, it doesn’t mean you are deficient, it only means your risk increases. If you are eating a healthy diet, you can easily maintain healthy levels. The only way to know for sure is to get your blood tested.\nNormal ranges of B12 are 200-1,100 pg/ml. Even though the lower level of normal is 200, if you are below 600, you might be suffering from B12 deficiency. There is one problem with supplementation however, and it’s related to the poor absorbability of oral vitamin B12 supplements.\nVitamin B12 is the largest vitamin molecule known. Because of its large size, it is not easily absorbed passively like most supplements. This is why many, if not most, oral B12 supplements are grossly ineffective. The only effective form of B12 supplementation is IM injections or sublingual administration.\nThat said, B12 supplements are exceptionally safe, with virtually no known side effects. Just avoid oral B12 supplements, as they will not be readily absorbed. Injections or a sublingual (under your tongue) spray work far better, as they allow the large B12 molecule to be absorbed directly into your bloodstream.\nWhat Else Do You Risk by Not Getting Enough Vitamin B12?\nYour risk of fracture might increase, but that’s not all. Some of the initial signs of B12 deficiency will often include mood changes, such as lack of motivation or feelings of apathy. Low levels can also lead to mental fogginess, memory troubles, muscle weakness, and — one of the hallmark signs — fatigue. Vitamin B12 also plays a role in:\n|Proper digestion, food absorption, iron use, and carbohydrate and fat metabolism||Healthy nervous system function||Promotion of normal nerve growth and development|\n|Help with regulation of the formation of red blood cells||Cell formation and longevity||Proper circulation|\n|Adrenal hormone production||Healthy immune system function||Support of female reproductive health and pregnancy|\n|Feelings of well-being and mood regulation||Mental clarity, concentration, and memory function||Physical, emotional, and mental energy|\nVitamin B12’s role in brain health and mental health is particularly significant. According to a small Finnish study published in the journal Neurology, people who consume foods rich in B12 may reduce their risk of Alzheimer’s in their later years.9 For each unit increase in the marker of vitamin B12 (holotranscobalamin), the risk of developing Alzheimer’s was reduced by 2 percent.\nMeanwhile, B group vitamins may slow brain shrinkage by as much as seven-fold in brain regions specifically known to be most impacted by Alzheimer’s disease.10 Among participants taking high doses of folic acid and vitamins B6 and B12, blood levels of homocysteine were lowered, as was the associated brain shrinkage – by up to 90 percent. As discussed by Dr. Kelly Brogan, MD in the video below, vitamin B12 deficiency can even cause a range of neurological disturbances that mimic serious mental illness.\nThe Best ‘Recipe’ for Bone Health\nOne of the important strategies for healthy bones is to eat the right kind of foods. A diet full of processed foods will produce biochemical and metabolic conditions in your body that will decrease your bone density, so avoiding processed foods is definitely the first step in the right direction. This goes far beyond calcium, which is the first nutrient many people think of concerning their bones. Your bones are actually composed of several different minerals, and if you focus on calcium alone, you will likely weaken your bones and increase your risk of osteoporosis, as Dr. Robert Thompson explains in his book, The Calcium Lie.\nCalcium, vitamins D and K2, and magnesium work synergistically together to promote strong, healthy bones, and your sodium to potassium ratio also plays an important role in maintaining your bone mass (larger amounts of potassium in relation to sodium is optimal for your bone health and your overall health). Ideally, you’d get all or most of these nutrients, including vitamin B12, from your diet (with the exception of vitamin D). This includes:\n- Plant-derived calcium: Raw milk from pasture-raised cows (who eat the plants), leafy green vegetables, the pith of citrus fruits, carob, and sesame seeds\n- Magnesium: Raw organic cacao and supplemental magnesium threonate if need be\n- Vitamin K2: Grass-fed organic animal products (i.e. eggs, butter, and dairy), certain fermented foods such as natto, or vegetables fermented using a starter culture of vitamin K2-producing bacteria, and certain cheeses such as Brie and Gouda\n- Trace minerals: Himalayan Crystal Salt, which contains all 84 elements found in your body, or other natural, unprocessed salt (NOT regular table salt!)\n- Vitamin D: Ideally from appropriate sun exposure (or a safe tanning bed), as it’s virtually impossible to get sufficient amounts from food. As a last resort, you could use a supplement, but if you do, you may also need to supplement with vitamin K2 to maintain ideal ratios\nRemember to Exercise Regularly and Use Strength Training\nAlthough the focus of this article is on nutrition, the other component you can’t ignore if you want strong, healthy bones is weight-bearing exercises like strength training. Bone building is a dynamic process, so you want to make sure you exert enough force on your bones to stimulate the osteoblasts to build new bone. Further, bone is living tissue that requires regular physical activity in order to renew and rebuild itself, it is important to make exercise a lifelong commitment. Peak bone mass is achieved in adulthood and then begins a slow decline, but exercise can help you to maintain healthy bone mass as you get older, and should be viewed as a bone-building partner to your healthy diet.\nWeight-bearing exercise is actually one of the most effective remedies against osteoporosis, because as you put more tension on your muscles it puts more pressure on your bones, which then respond by continuously creating fresh, new bone. In addition, as you build more muscle, and make the muscle that you already have stronger, you also put more constant pressure on your bones. A good weight-bearing exercise to incorporate into your routine (depending on your current level of fitness, of course) is a walking lunge, as it helps build bone density in your hips, even without any additional weights.\nIn addition, Acceleration Training, a.k.a. Whole Body Vibrational Training (WBVT) using a Power Plate, has also been shown to be a safe, natural way to ward off osteoporosis, and it’s gentle enough even for the disabled and elderly. Research shows vibrational training may help to produce a significant increase in bone density in postmenopausal women,11 making it another valuable tool for bone health.']	['<urn:uuid:a5187e6e-7406-45d5-9c72-3f614d90064a>']	open-ended	direct	short-search-query	distant-from-document	novice	2025-04-14T18:34:38.591736	43	610	11243
18	I keep hearing about some new bus service coming to Indianapolis that's supposed to be special. What makes it different from regular bus lines?	Indianapolis is building the nation's first all-electric Bus Rapid Transit (BRT) system called the Red Line. This $96 million project will feature dedicated lanes, pre-board ticket purchase, and buses running every ten minutes during the day for 20 hours daily, seven days a week. The route will connect the University of Indianapolis to Broad Ripple, coming within a quarter mile of more than 50,000 residents and nearly 150,000 jobs.	['As home of “The Greatest Spectacle in Racing” and the “Crossroads of America”, Indianapolis is reinventing itself in the 21st century with a batch of high-profile projects that are catching the attention of the rest of the country. As Governor Eric Holcomb has stated, Indy is quickly becoming somewhere “people can’t afford to miss”.\nWhile Indianapolis hasn’t built over 35 stories in nearly 30 years, that is poised to change in a big way. Huge chunks of downtown-adjacent property that have sat derelict for years will soon be replaced by neighborhood-sized projects that are sure to reinvent Center Township.\nAnd with the nation’s first all-electric BRT service finally under construction, the third decade of the 21st century is shaping up to be an exciting time for metropolitan Hoosiers.\nWhite River Vision Plan\nDragon Boat races in the White River? What about urban kayaking tours from Broad Ripple to downtown — along with an activated riverfront with unbridled recreation opportunities and improved pedestrian accessibility, teeming with tourists and locals alike? This may just be a possibility in store if the White River Vision Plan is implemented. After the progress of DigIndy, a massive tunneling project diverting storm water from the White River, the White River is yet again safe for recreation, and the possibilities are endless.\nAccording to mywhiteriver.org, the White River Vision Plan is a “joint effort between the City of Indianapolis and Hamilton County Tourism, Inc. in partnership with Visit Indy’s philanthropic arm, Tourism Tomorrow, Inc. to develop a comprehensive and coordinated regional, community-driven plan to enhance 58 miles of the White River in Marion and Hamilton counties. The goal of the vision plan is to create an accessible, recreational, and cultural environment that encourages a unique sense of place for the community as a whole”.\nOver 4,000 residents responded to a recent survey on how to develop the river, and early responses were in favor of enhanced connectivity and recreational opportunity. The project is currently in the ‘Envisioning’ stage, and concepts and designs are being developed. There are public meetings upcoming in both Hamilton and Marion counties and you can take the White River Vision Plan Survey here.\nPhoto: White River Vision Plan | mywhiteriver.org\nFormer Angie’s List Campus Redevelopment on East Washington Street\nStatus: Proposed/Under Construction\nAngie’s List announcing a leave from their campus on East Washington Street came as a surprise to some — but their merger with HomeAdvisor was a long time coming. With Angie’s hemorrhaging financially for months and enduring intense layoffs, questions with what to do with their sprawling 17.5 acre campus immediately came to the forefront in the aftermath of their exodus.\nHowever, the loss may be a blessing in disguise for the burgeoning Holy Cross neighborhood. Often times, the collection of buildings sandwiched between Market Street and Washington just east of Interstate 65/70 became a ghost town after Angie’s workers left. And while the company’s restoration and use of older buildings was novel in concept, without active use by the public, the buildings acted more as caricatures of what they were — rather than actually providing real use.\nEnter Bill Oesterle, co-founder of Angie’s List. Head of Fred Abel LLC, an investment group, Oesterle acquired the 25-building campus early in 2018 and sees the area becoming a mixed-use neighborhood, with tech start-ups, apartments, and lively shops. Given the parcel’s location between downtown and an impoverished east-side, his plan has a chance to rejuvenate and rebuild a historic business node that was perhaps underutilized by Angie’s List. Add in the proposed Blue Line BRT along Washington Street, redevelopment of the nearby old Ford plant, as well as increasing density in the Market East district and you have the recipe for one of Indy’s coolest new n’hoods.\nAccording to Indy Star, Oesterle hopes to start attracting tenants by the end of 2018.\nPhoto: East Washington St.| IndyStar\nPan Am Plaza Redevelopment\nSafe to say, one of the ripest sites for redevelopment downtown Indianapolis has been Pan Am Plaza. Originally built for the 1987 Pan Am games, the parcel sits adjacent to both the Indiana Convention Center and revitalized Georgia Street. While it may have hosted ESPN during Indy’s Super Bowl, the plaza remains vastly underutilized, and its prime location alongside Illinois Street has long made it a candidate for various development proposals. Finally, Kite Realty Group Trust broke through with a proposal that stands an excellent chance of getting done.\nAdding a combined 1,400 hotel rooms along with a publicly-funded $120 million ballroom expansion of the Indiana Convention Center, the major element of Kite’s proposal is a Ratio-designed 38 story, 800 room Hilton-brand hotel that would either be the third or fourth tallest building in Indianapolis once completed.\nKite also plans to build a second, smaller Hilton high-rise hotel on the southeast edge of the site, which would offer around 600 rooms. The expansion of the convention center, a publicly-funded 50,000 sq. ft. ballroom — the largest in the city — would connect to both hotels and be directly linked to the rest of the convention center via skyway. City leaders have repeatedly stated that expanding the convention center is paramount to attracting and retaining the money-making conventions which have called Indianapolis home, such as FFA and GenCon.\nIf Kite’s hotel plan seemingly isn’t as ‘transformative’ as others on this list, it belongs due to the potential impact the 38-story hotel will have on Indy’s skyline. While building up has seen to be a hindrance for Indy developers, this project, along with CityWay’s 11 story expansion, will further grow the skyline in a southern direction. The end result will be a fuller block of towers, alongside the big three of Regions, OneAmerica, and Salesforce. It’s nice to see Indy “make it” so to say, and join some of its regional cities in building taller.\nWhile the project will require tearing down of the existing underground parking garage, site clearance and acquisition, and city approvals, it seems there is good faith this gets done — GenCon announced a renewal of its contract with Indianapolis through 2023, shortly after the Kite proposal became public.\nPhoto: 38 Story Hotel Proposal | Kite Realty\nStatus: Under Construction\nTo call the Bottleworks project transformative would be an understatement. The 12-acre site aims to create a wholly new neighborhood at the corner of Mass Ave and College Avenue downtown — this is ambition at its finest.\nThe $300 million mixed-use development calls not only for West Elm’s first boutique hotel (with a rooftop bar), but a food hall with dozens of vendors, 240 condominium and apartment units, an eight-screen dine-in film theater, nearly 200,000 sq. ft. of office, and 200,000 sq. ft. of retail. Developer Hendricks Commercial Properties claims the project will bring over 3,000 jobs and attract over 2 million visitors annually. Add in the retrofitting of the former Coca-Cola bottling plant and you have a project which is not only transformative, but massive in scope.\nImportantly for connectivity, the project calls for restoring a street grid to what was formerly an IPS bus parking lot, extending 9th Street and creating a street arcade along Carrolton Avenue.\nWhile retailers and tenants are currently being secured, ground broke earlier in 2018. According to Hendricks, Bottleworks will be built in phases, with street retail opening in 2020. Recently, Indianapolis tech company High Alpha announced they would be moving to the district — the first of what should be many such announcements.\nPhoto: Bottleworks seen from Mass and College | Ratio Architects\nTwin Aire Justice Complex\nStatus: Under Construction\n140 acres of the former Citizens Energy Coke & Gas Plant will become Marion County’s new courthouse and jail, replacing the antiquated and un-streamlined hodgepodge of criminal justice facilities currently downtown.\nAtop continuing development of Twin Aire, just to the east of Fountain Square and part of Indy’s Great Places 2020, the Justice Center promises to bring newfound vitality to the struggling southeast-side neighborhood whose industrial anchorage long ago floated away.\nThe nearly $600 million project will house a 3,000-bed jail, a new courthouse, the sheriff’s office, and a community center which aims to remediate those with mental illness. National architectural firm HOK was in charge of designing the campus, which includes an 11-story courts building surrounding a shorter building housing the jail.\nAs part of the deal, Citizens Energy will be planting 1,000 trees, and the entrance to the complex will feature a large roundabout akin to Monument Circle.\nThere is a hefty amount of bail-bonds, court offices and businesses centered on Delaware Street near the current courts downtown. Many expect these businesses to relocate once the new justice center opens. While some question what the future of Delaware Street will be if and when all the lawyers leave, an exciting prospect exists to think what a reimagined Delaware Street looks like –come alive from the bonds of bailsman and court-catering traffic and adjacent to the trendy Market East district.\nThe center is looking to open in 2021, though delays are always possible with a project of this magnitude.\nPhoto: Criminal Justice Center | City of Indianapolis\nStatus: Proposed, Under Construction\nThis massive, 60 acre, 6 million sq.-ft. public-private-partnership between the City of Indianapolis and Indiana University northwest of downtown off Indiana Avenue and 16th Street is coined as a “innovation-based community for researchers, entrepreneurs, and established companies”.\nEssentially creating a neighborhood, 16Tech will be a great addition to Indy’s exploding tech scene, and surely must have taken part of the city’s Amazon bid. With proximity to major hospitals, IUPUI, and recreational trails, 16Tech looks poised to attract and retain creative workers.\nWhen complete, the neighborhood will include a 120,000 sq. ft. anchor building, a 250-unit apartment complex, renovated office space formerly occupied by Citizens Energy Group; restaurant and retail space, and a number of infrastructure improvements, including a central greenway and a new bridge across Fall Creek that will connect the neighborhood with IUPUI, Eskenazi and Riley hospitals.\nThe anchor building is expected to be complete in 2020, to become home to Indiana Biosciences Research Institute, a $360-million independent research facility. While the magnitude of 16Tech is immense, progress is occurring slowly but surely. In March 2018, 16Tech announced a $38 million grant from Eli Lilly to fund an initial stage, and that Indy-based commercial real estate firm Browning will spend over $120 million to build three new buildings and renovate an existing structure on 11 acres along Waterway Boulevard.\nPhoto: 16Tech | Browning\nAmbrose Redevelopment of GM Stamping Plant (Waterside)\nThis would get the top billing on this list if not for the larger impact of another project, but Ambrose Property Group’s Waterside neighborhood easily tops the list for most potential impact by a single real estate development.\nThe numbers are staggering: a $1.3 billion dollar investment offering 1,350 residential units, 620 hotel rooms, 2.75 million sq. ft. of office space, 100,000 sq. ft. of retail and 12,000 jobs. If Bottleworks is ambitious, then Waterside is ridiculous – not just in its chutzpah, but in its sheer scope.\nWhile initial investment on the site was earlier estimated around $500 million, Ambrose recently upped that number to $1.3 billion due to an expanded vision of the 103+ acre area. Beside restoring the Albert Kahn-designed crane bay to its former glory, Ambrose intends to redevelop all the surrounding land over the next 15 years, creating an entirely new mixed-use neighborhood of walkable multi-modal streets, mid-rises, apartments, single and multi-family housing, office space, and abundant recreation opportunities alongside the White River.\nThe third of Indy’s neighborhood-sized mega-projects alongside 16Tech and Bottleworks, Waterside has long been eyed for Indy’s Amazon bid – although, it is important to note that Ambrose has said repeatedly that Waterside is getting done with or without Mr. Bezos. By incorporating a street grid and fostering connectivity through a pedestrian bridge over the river, Ambrose is overtly attempting to seam together Waterside with the nearby and overlooked neighborhood, The Valley.\nWhile the planning and approval process takes shape, infrastructure improvements are slated to begin in earnest during 2019, at which point White River Parkway will be rerouted through the middle of the development, allowing its former riverside right-of-way to be repurposed for recreation and pedestrian accessibility.\nWhile the dollars haven’t flowed across the river during downtown’s building boom, that is about to change in a big way. And while decades of deindustrialization, crime, and disinvestment on the Westside may have scared developers away, Waterside, along with the proposed Blue Line BRT, will assuredly rewrite that narrative.\nGet ready, folks — here comes the Westside.\nPhoto: Waterside | Ambrose Property Group\nRed Line Bus Rapid Transit\nStatus: Under Construction\nWelcome to the 21st century, Indianapolis. In a city that has long suffered with one of the least reliable transit systems in the U.S., the paradigm is quickly shifting. Soon, Indianapolis will not only be home to “The Greatest Spectacle in Racing” but also the first all-electric BRT system in the country. And it’s suddenly looking bright for those who desire multi-modal options in car-centric Central Indiana.\nThe $96 million project, slated to open in 2019 and featuring frequent bus service from the University of Indianapolis to Broad Ripple, will use dedicated lanes, pre-board ticket purchase and street reconfiguration to expedite service, in which buses will generally come every ten minutes during the day. According to IndyGo, the route “will come within a quarter mile of more than 50,000 residents and nearly 150,000 jobs – a quarter of all jobs in Marion County”. The service will operate for 20 hours a day, seven days a week.\nKnow this, the Red Line is not just a big deal — it is an immense deal. In a city that has struggled with transit mobility since the decline of streetcar networks, the Red Line is poised to rebrand Indy as a transit-friendly community. The Red Line will act as the backbone of Central Indiana’s transit network, connecting neighborhoods, creating economic opportunities, and allow transit-oriented-development (TOD) to densify and rebuild historic urban nodes along the corridor. It will bring access to jobs for low-income residents and connect some of the largest employers in the state to each other.\nThe battle for BRT hasn’t been easy — many years of planning, securing grants, and plotting a successful referendum (which passed in 2016), all culminated in Indy’s success in finally bringing its residents adequate transit. Things haven’t always been rosy. Threats of lawsuit from north-side neighbors still permeate discussions about the Red Line and securing grants became an uphill battle after President Trump determined to shut down the Small Starts program that provided its funding.\nBut, for now, construction has started and transit fans will finally have their day. And despite the huge economic impact of some of the other projects on this list, the Red Line offers the most potential to remediate deepening social inequities and improve the quality of life for residents of Indianapolis.\nPhoto: Red Line in Fountain Square | IndyGo\n© Jeffery Tompkins 2018']	['<urn:uuid:f6448e3b-a750-4e11-8ebf-affad8276f5d>']	factoid	with-premise	verbose-and-natural	distant-from-document	novice	2025-04-14T18:34:38.591736	143	435	15914
19	how brain imaging technology changes understanding cognitive processes compared to behavioral studies	Brain imaging technology is providing a direct observational window into cognitive functions that was previously impossible. While behaviorism focused only on observable motor behavior due to lack of access to cognitive processes, current imaging technology allows us to see computerized representations of the working brain. Although current imaging requires laboratory settings, future portable imaging technologies are expected to show brain activity during normal environmental interactions. This represents a shift toward the same kind of direct, intimate understanding we have of our motor system.	['Observing Mobility. The recent death of the renowned evolutionary theorist, Stephen J. Gould, recalls his intriguing comment that we’re inside-out crustaceans. A crustacean’s skeleton is on the outside, ours is on the inside. Our soft tissue and appendages are out where we can readily observe them.\nLlinas (2002) expands briefly on this concept in his excellent discussion of the evolution and nature of our brain, but it’s something that’s pleasant to explore further.\nHaving an internal skeleton means that we have a direct, intimate, sensory knowledge of how our external motor system functions. From birth on, we can observe and feel muscular contractions and their relationship to body movements. We’ve created tools that accurately measure the properties of our marvelous movement system. Further, we’ve always celebrated this basic universal understanding and awe of our motor system through performance and competition.\nWell, why not? Our motor system is perhaps the definitive element of our biological self. Compare the two major biological groups, plants and animals. Plants don’t have a brain and animals do. Plants don’t have a brain because they’re not going anywhere — and if you’re not going anywhere, you don’t even need to know where you are. What’s the advantage for a rooted tree to realize that other trees are better situated, or to be able to observe approaching loggers?\nOn the other hand, if you have legs/wings/fins/etc that permit mobility, you need a sensory system to tell you about here and there. Then you need a make-up-your-mind system to decide if there is better than here, or here is better than there. Finally, you need to activate your motor system to move to there, if you’ve decided it’s better than here.\nWe spend much of our extended juvenile development period informally observing and exploring our motor system. We have to learn how to regulate and predict its movements and the movements of others (and of moving objects). It’s a complex system that must be activated for thousands of hours to reach the adult proficiency levels of complex movements. We’ve turned much of this juvenile practice activity into enjoyable games.\nOur mobility systems can even get us beyond direct physical movement. For example, our vocal apparatus can rhythmically move air molecules that hit the eardrums of others at a distance and create brain-to-brain language connections. Mastering the movements involved in spoken (and written) language is thus another major childhood task.\nMy March 2002 Brain Connection column (From Video Games to the Internet) discussed how important it is for young children to get on a tricycle at three if they hope to drive a car at 16, and to similarly begin with video games at an early age if they later hope to effectively travel the Internet.\nWe’re fascinated by those who move (or move objects) at virtuoso levels. The whole world gathers every two years to discover who can jump the highest, throw things the farthest, run or skate the fastest, ski the best. We attend concerts to observe others sing or play musical instruments, and sporting events to watch others throw balls through hoops or hit them with bats. It may seem kind of foolish, but it’s also quite human.\nObserving Cognition. Although we all develop an excellent common understanding of movement via our continuous observation of its dynamics, our brain’s processing systems (that regulate movement among other things) are located within a hidden bony skull and spine. So from our brain’s perspective, it’s sort of like we have a crustacean brain— the soft cognitive tissue is on the inside.\nWe thus don’t have the direct observational access to what’s occurring within our skull/spine that we have of the actions of our motor system. For example, we can’t hear the sounds active neurons make or smell our brain, and our brain has no pain receptors.\nThis lack of direct sensory access to cognitive processes led to the development of many competing speculations and theories about how our brain/mind functions. Indeed, Behaviorism, which dominated psychology for much of the past century, focused on the observable motor behavior that emerges out of cognition, rather than on inaccessible cognitive activity.\nBrain imaging technology is now finally providing this direct observational window into our cognitive functions. It provides an observable computerized representation of our working brain. Unfortunately, the current imaging technology requires the imaged subject to function within a laboratory setting, but we can anticipate the development of powerful portable imaging technologies that can depict the brain activity of subjects who are interacting with objects/events in a normal environment. We will then have moved toward the direct, intimate, observable relationship we’ve long enjoyed with our motor system.\nThe recent emergence of biologically based theories of consciousness (Damasio, 1999. Edelman, 2000) are good examples of the shift towards a more direct understanding of our brain’s mysterious processing systems. We can expect this process to escalate in the years ahead. Imagine what it might be like to finally understand our brain’s thinking activities at the same level that we now enjoy for movement.']	['<urn:uuid:47209421-1f3a-45f4-b467-bb5624a20c62>']	open-ended	direct	long-search-query	similar-to-document	expert	2025-04-14T18:34:38.591736	101	603	5262
20	foreign licenses space activities valid nz	Under the Outer Space and High-altitude Activities Act 2017, the responsible minister has the authority to recognize overseas licenses and permits as meeting some New Zealand requirements. However, it's important to note that New Zealand's regulatory regime and international obligations are extraterritorial, meaning they also apply to New Zealand nationals or entities conducting space launches or satellite activities from other countries.	"[""Our regulatory regime\nOur regulatory regime supports the growth of a safe, responsible and secure space industry that meets our international obligations and manages any liability arising from our obligations as a launching state.\nThe Outer Space and High-altitude Activities Act 2017 (OSHAA)\nThe OSHAA Act came into force in December 2017. The Act regulates — through licences or permits — launches into outer space, launch facilities, high-altitude vehicles (HAVs) and payloads. It's administered by the New Zealand Space Agency within the Ministry of Business, Innovation and Employment (MBIE).\nGranting licences or permits\nThe Act contains requirements that applicants must satisfy to be granted licences or permits. These include:\n- the technical capability to safely conduct the proposed activity — for example, a safe launch, or safe operation of the payload\n- an orbital debris mitigation plan that meets any prescribed requirements\n- that the proposed activity is consistent with New Zealand’s international obligations.\nEven if these requirements are met, the responsible minister responsible may still decline to grant a licence or permit if — for example —they’re not satisfied the proposed operation is in the national interest, or if national security risks associated with the licence/permit application have been identified.\nAll activities will also need to comply with all other applicable New Zealand legislative requirements, such as resource consents, health and safety and environmental requirements.\nOverseas licences and permits\nThe legislation allows the responsible minister to recognise overseas licences and permits as satisfying some of the New Zealand requirements.\nOur regulatory regime and international obligations are extraterritorial — meaning they also apply to New Zealand nationals (or New Zealand entities) carrying out space launches or satellite activities from other countries.\nBackground to the Act\nThe documents below provide a background to the development and passing of the Outer Space and High-altitude Activities Act 2017.\n- Outer Space and High-altitude Activities Act 2017 Regulations — Regulatory Impact Statement (Aug 2017) [PDF, 414 KB]\n- Outer Space and High-altitude Activities Bill (introduced 19 September 2016)(external link)\n- Outer Space and High-altitude Activities Bill (June 2016) — Regulatory Impact Statement [PDF, 625 KB]\n- Outer Space and High-altitude Activities Bill: Final Policy Decisions (June 2016) — Cabinet paper [PDF, 620 KB]\n- Outer Space and High-altitude Activities Bill: Final Policy Decisions (June 2016) — Cabinet minute [PDF 579KB]\n- The Scope of Space Policy and a Lead Space Agency (April 2016) - Cabinet paper [PDF, 629 KB]\n- The Scope of Space Policy and a Lead Space Agency (April 2016) — Cabinet minute [PDF, 575 KB]\nThe regulations to support the Outer Space and High-altitude Activities Act 2017 came into force in December 2017. They are the:\n- Outer Space and High-altitude Activities (Licences and Permits) Regulations 2017(external link)\n- Outer Space and High-altitude Activities (Definition of High-altitude Vehicle) Regulations 2017(external link)\nThe Act contains broad regulation-making powers, however not all of them have to be used straight away — they’ve been built into the Act to future-proof it.\nThe regulations that were necessary to implement the Act when it came into force included:\n- requirements for licences and permits — particularly the information that applicants provide\n- requirements for an orbital debris mitigation plan\n- requirements for safety cases for launch licences, launch facility licences and (non-aircraft) high-altitude vehicles\n- clarification regarding which vehicles that go (or are capable of going) into high-altitude are not high-altitude vehicles (HAVs) for the purposes of the Act, and hence won’t require a licence.\nWhy HAVs are part of the regulatory regime\nSome high-altitude technologies have similar functions to satellites, such as for earth monitoring, communications and internet connectivity.\nWe already have high-altitude activity happening from New Zealand. These range from small, uncontrolled balloons launched for the purpose of collecting weather data or educating students, to large controllable balloons carrying sophisticated imaging and communications equipment for scientific research.\nIncluding high-altitude vehicles (HAVs) in the regulatory regime is intended to:\n- future-proof the legislation for advances in technology\n- ensure that different technologies performing similar functions are treated in a consistent manner.""]"	['<urn:uuid:f5660ade-0f5d-4352-bead-b513f096c8d8>']	open-ended	with-premise	short-search-query	distant-from-document	novice	2025-04-14T18:34:38.591736	42	442	4587
21	How much diversity exists in Neanderthal mitochondrial DNA?	Neanderthal mitochondrial genomes show relatively low genetic diversity, with an average difference of 20.4 bases between individuals. This diversity is only 1/3 of what is found in modern humans. The low diversity might indicate a small population size, possibly due to competition with modern humans expanding into their territory.	"[""- Human Evolution Research\n- Climate and Human Evolution\n- Asian Research Projects\n- East African Research Projects\n- Human Origins Program Team\n- What's Hot In Human Origins?\n- Fossil Forensics: Interactive\n- E. A. Mammal Dentition Database\n- Human Evolution Evidence\n- 3D Collection\n- Human Fossils\n- Human Family Tree\n- Timeline Interactive\n- Human Characteristics\n- About Us\nSequencing Neanderthal DNA\nChallenges in Extracting Ancient DNA\nWorking with ancient DNA is very challenging, both in terms of finding sufficient material to work with after decomposition has occurred, and in terms of eliminating modern human contamination. Distinguishing between modern human and ancient genetic material is particularly difficult when the ancient DNA comes from close relatives of modern humans.\nOrganisms decompose after death. Water, oxygen and microbes break down DNA. Within 100,000 years, all DNA is destroyed. Ancient DNA tends to be found in small quantities. The DNA that is extracted is generally fragmentary and damaged. Some damage results in changes to the DNA sequence. Cytosine can change to uracil, which is read by copying enzymes as thymine, resulting in a C to T transition. Changes from G to A also occur. DNA errors are very common at the ends of molecules.\nContamination by modern DNA is a particularly difficult problem to solve. Labs and chemicals may be contaminated by the DNA of the people working in them, while many fossils have been handled by researchers for years. Contamination is difficult to detect because Neanderthals and humans share much of their genetic material, making some DNA sequences indistinguishable. Researchers have developed ways to analyze the results of ancient DNA sequencing efforts to determine whether contamination is likely and how much has occurred. Analysis of the results and efforts to keep labs and specimens free of modern DNA is very important as some researchers believe that the early studies of Neanderthal DNA included modern contaminants.\nSequencing the Complete Neanderthal Mitochondrial Genome\nAfter successfully sequencing large amounts of DNA and devising strategies to deal with potential contamination, a team led by Svante Pääbo from the Max Planck Institute, reported the first complete mtDNA sequence for a Neanderthal (Green et al. 2008). The 0.3 gram sample was taken from a 38,000 year old Neanderthal from Vindija Cave, Croatia. Complete Neanderthal mtDNA sequences give researchers more information about the relationship between modern humans and Neanderthals, as well as information about Neanderthal population size.\nThe complete mtDNA sequence shows that Neanderthals were outside the range of modern human mtDNA variation. Researchers compared the mtDNA sequence with that of modern humans. They compared sequence changes that resulted in nonsynonymous amino acid changes with synonymous changes. They found a larger number of nonsynonymous changes in the Neanderthal lineage, possibly implying that Neanderthals had a small population size with weaker purifying selection (Green et al. 2008).\nLater, Svante Pääbo’s lab sequenced the entire mitochondrial genome of five Neanderthals (Briggs et al. 2009). Sequences came from two individuals from the Neander Valley in Germany, Mezmaiskaya Cave in Russia, El Sidrón Cave in Spain and Vindija Cave in Croatia. Though the Neanderthal sample comes from a wide geographic area, the Neanderthal mtDNA sequences were not particularly genetically diverse. The most divergent Neanderthal sequence came from the Mezmaiskaya Cave Neanderthal from Russia, which the oldest and eastern-most specimen. To look at whether age or geographic location contributed to genetic differences, the team sequenced part of the DNA of another Mezmaiskaya Cave Neanderthal that dated to 41,000 years ago. This more recent specimen grouped with the other Neanderthals, possibly showing that age was the cause of the sequence differences (Briggs et al. 2009). Other studies show the existence of eastern, western and southern groups of Neanderthals (Fabre et al. 2009).\nOn average, Neanderthal mtDNA genomes differ from each other by 20.4 bases and are only 1/3 as diverse as modern humans (Briggs et al. 2009). The low diversity might signal a small population size, possibly due to the incursions of modern humans into their range (Briggs et al. 2009).\nSequencing the Neanderthal Nuclear Genome\nRecently, there have been efforts to sequence Neanderthal nuclear genes. Two studies, one by Svante Pääbo’s team and one by Edward Rubin, have sequenced large amount of Neanderthal nuclear DNA using different methods. Their results were announced in 2006. Given their success in sequencing some nuclear DNA, both labs launched projects to sequence the entire Neanderthal genome. Nuclear genomic sequences from Neanderthals show differences between modern humans and Neanderthals, and illustrate aspects of Neanderthal biology.\nOne Million Base Pairs of the Neanderthal Sequence\nSvante Pääbo’s team from the Max Planck Institute for Evolutionary Anthropology in Germany announced the sequencing of one million base pairs of nuclear DNA of a Neanderthal specimen in 2006 (Green et al. 2006). After a long search for specimens with a sufficient amount of undamaged DNA to sequence and for the ones with the least evidence of contamination, they focused on Vindija 80, a Neanderthal discovered in Croatia in 1980 that is approximately 38,000 years old.\nThey estimated that 7.9% of the changes in human DNA compared with that of the chimpanzee occurred after the split with Neanderthals. They dated the split between the ancestors of modern humans and Neanderthals to 465,000 to 569,000 years ago. They also found that the effective population size of the Neanderthals was small. Their success in sequencing this amount of DNA indicated that a large-scale project to sequence the Neanderthal genome is possible.\nRubin's Neanderthal Nuclear DNA\nEdward Rubin’s team from the Lawrence Berkeley National Laboratory in California also sequenced Neanderthal nuclear DNA (Noonan et al. 2006). They sequenced about 65,000 base pairs from the 38,000 year old Vindija, Croatia specimen. The technique used here produces a copy of the Neanderthal sequence that can be retained forever, reducing the need for repeated destructive sampling. The DNA is then cloned in bacteria.\nThe average split time between the Neanderthal and modern human populations was around 370,000 years ago. They used the sequence to look at the possibility of interbreeding between Neanderthals and moderns. Admixture would be seen as derived alleles that are found in Neanderthals and in low frequencies among modern humans. They did not detect this in their sample. A simulation to test the Neanderthal contribution to the human genome found a 0% chance of Neanderthal input with a 0% to 20% confidence range. With this data, the authors cannot definitively rule out admixture (Noonan et al. 2006).\nSome aspects of the two sets of nuclear DNA do not fit together, possibly because of contamination and sequencing errors, especially in the Green et al. (2006) study (Wall and Kim 2007). This has led the researchers to develop new methods of detecting and preventing contamination to ensure that only ancient DNA is being sequenced.\nA Draft Sequence of the Neanderthal Genome\nIn 2010, Svante Pääbo’s lab announced a draft sequence of the Neanderthal genome (Green et al. 2010). This new study has produced evidence consistent with interbreeding between Neanderthals and anatomically modern Homo sapiens and points to aspects of the human genome that may have changed since the split between humans and Neanderthals.\nDNA was extracted from three Neanderthal bones from Vindija Cave, Croatia. By comparing sequences from their mtDNA and their nuclear DNA, scientists determined that the three bones came from different individuals, although two of them might be related on their mother’s side. The researchers used several methods to ensure that the DNA they were sequencing was derived from the Neanderthal specimens rather than from contamination by modern humans in the lab.\nThe Neanderthal sequence was compared to those of five modern humans from France, China, Papua New Guinea, as well as Africans from the San and Yoruba groups. Tests indicated that Neanderthals shared more derived alleles with non-African modern humans than with African modern humans. They compared parts of the Neanderthal genome with pairs of modern humans. While the European and Asian pairs had similar amounts of derived material compared with the Neanderthal, Neanderthals had more similarities with non-African humans than with Africans. The simplest explanation for these results is gene flow from Neanderthals into modern humans. Gene flow could also have occurred from modern humans into Neanderthals. Interbreeding events between Neanderthals and modern humans might be obscured if the modern human population was large.\nNeanderthals have contributed approximately 1% to 4% to the genomes of non-African modern humans. This evidence of interbreeding sheds light on how we think of the expansion of modern humans out of Africa. It refutes the strictest scenario in which anatomically modern humans replaced archaic hominins completely without any interbreeding. However, even with some interbreeding between moderns and archaic hominins, most of our genome still derives from Africa.\nThe data also points to the time when interbreeding might have taken place. Since the Neanderthal DNA was equally related to that of the modern samples from France, China and Papua New Guinea, admixture between moderns and Neanderthals must have occurred before the Eurasian populations split off from each other. Remains of both modern humans and Neanderthals dating to around 100,000 years ago have been found in the Middle East. A few interbreeding events during this period could have produced the results found in this study.\nThe sequence of our close hominin relative also shows us how humans are unique. Researchers found 78 sequence differences that would have affected proteins in which Neanderthals had the ancestral state and modern humans had a newer, derived state. Five genes had more than one sequence change that affected the protein structure. These proteins include SPAG17, which is involved in the movement of sperm, PCD16, which may be involved in wound healing, TTF1, which is involved in ribosomal gene transcription, and RPTN, which is found in the skin, hair and sweat glands. Scientists do not know the function of the CAN15 protein, which was also one of the differences. Other changes may affect regulatory regions in the human sequence. Some changes are in regions that code for microRNA molecules that regulate protein manufacture.\nThe comparison also pointed out regions that might have been under positive selection in modern humans. Though some of the genomic areas that may have been positively selected for in modern humans may have coded for structural or regulatory regions, others may have been associated with energy metabolism, cognitive development and the morphology of the head and upper body.""]"	['<urn:uuid:8078d8a5-0700-4b28-bdd5-663c2c173c71>']	open-ended	with-premise	concise-and-natural	distant-from-document	expert	2025-04-14T18:34:38.591736	59	333	11168
22	explain value creation financial risk principles	Finance theory prescribes that a firm should only take on projects that increase shareholder value. However, firm managers cannot create value for shareholders by taking on projects that shareholders could do for themselves at the same cost. Financial risk management requires identifying sources of risk, measuring it, and developing plans to address them. This can be done through both qualitative and quantitative approaches, focusing on when and how to hedge using financial instruments to manage costly risk exposures.	"['Financial risk management\nFinancial risk management is the practice of economic value in a firm by using financial instruments to manage exposure to risk, particularly credit risk and market risk. Other types include Foreign exchange risk, Shape risk, Volatility risk, Sector risk, Liquidity risk, Inflation risk, etc. Similar to general risk management, financial risk management requires identifying its sources, measuring it, and plans to address them.\nFinancial risk management can be qualitative and quantitative. As a specialization of risk management, financial risk management focuses on when and how to hedge using financial instruments to manage costly exposures to risk.\nWhen to use financial risk management\nFinance theory (i.e., financial economics) prescribes that a firm should take on a project when it increases shareholder value. Finance theory also shows that firm managers cannot create value for shareholders, also called its investors, by taking on projects that shareholders could do for themselves at the same cost.\nWhen applied to financial risk management, this implies that firm managers should not hedge risks that investors can hedge for themselves at the same cost. This notion was captured by the so-called ""hedging irrelevance proposition"": In a perfect market, the firm cannot create value by hedging a risk when the price of bearing that risk within the firm is the same as the price of bearing it outside of the firm. In practice, financial markets are not likely to be perfect markets.\nThis suggests that firm managers likely have many opportunities to create value for shareholders using financial risk management, wherein they have to determine which risks are cheaper for the firm to manage than the shareholders. Market risks that result in unique risks for the firm are commonly the best candidates for financial risk management.\nThe concepts of financial risk management change dramatically in the international realm. Multinational Corporations are faced with many different obstacles in overcoming these challenges. There has been some research on the risks firms must consider when operating in many countries, such as the three kinds of foreign exchange exposure for various future time horizons: transactions exposure, accounting exposure, and economic exposure.\n- Crockford, Neil (1986). An Introduction to Risk Management (2nd ed.). Woodhead-Faulkner. ISBN 0-85941-332-2.\n- Charles, Tapiero (2004). Risk and Financial Management: Mathematical and Computational Methods. John Wiley & Son. ISBN 0-470-84908-8.\n- Conti, Cesare & Mauri, Arnaldo (2008). ""Corporate Financial Risk Management: Governance and Disclosure post IFRS 7"", Icfai Journal of Financial Risk Management, ISSN 0972-916X, Vol. V, n. 2, pp. 20–27.\n- Lam, James (2003). Enterprise Risk Management: From Incentives to Controls. John Wiley. ISBN 978-0-471-43000-1.\n- McNeil, Alexander J.; Frey, Rüdiger; Embrechts, Paul (2005), Quantitative Risk Management. Concepts, Techniques and Tools, Princeton Series in Finance, Princeton, NJ: Princeton University Press, ISBN 0-691-12255-5, MR 2175089, Zbl 1089.91037\n- van Deventer; Donald R.; Kenji Imai; Mark Mesler (2004). Advanced Financial Risk Management: Tools and Techniques for Integrated Credit Risk and Interest Rate Risk Management. John Wiley. ISBN 978-0-470-82126-8.\n- Peter F. Christoffersen (22 November 2011). Elements of Financial Risk Management. Academic Press. ISBN 978-0-12-374448-7.\n- Allan M. Malz (13 September 2011). Financial Risk Management: Models, History, and Institutions. John Wiley & Sons. ISBN 978-1-118-02291-7.\n- Van Deventer, Donald R., and Kenji Imai. Credit risk models and the Basel Accords. Singapore: John Wiley & Sons (Asia), 2003.\n- Drumond, Ines. ""Bank capital requirements, business cycle fluctuations and the Basel Accords: a synthesis."" Journal of Economic Surveys 23.5 (2009): 798-830.\n- EMMANUEL ATTAH KUMAH. COST OF CAPITAL (A FINANCIAL TOOL TO CREATE AND MAXIMIZE SHAREHOLDER VALUE). Lulu.com. pp. 39–. ISBN 978-1-304-26045-1.\n- KRISHNAMURTI CHANDRASEKHAR; Krishnamurti & Viswanath (eds.) ""; Vishwanath S. R. Advanced Corporate Finance. PHI Learning Pvt. Ltd. pp. 178–. ISBN 978-81-203-3611-7.\n- John J. Hampton (1982). Modern Financial Theory: Perfect and Imperfect Markets. Reston Publishing Company. ISBN 978-0-8359-4553-0.\n- Zahirul Hoque (2005). Handbook of Cost and Management Accounting. Spiramus Press Ltd. pp. 201–. ISBN 978-1-904905-01-1.\n- Kirt C. Butler (28 August 2012). Multinational Finance: Evaluating Opportunities, Costs, and Risks of Operations. John Wiley & Sons. pp. 37–. ISBN 978-1-118-28276-2.\n- Dietmar Franzen (6 December 2012). Design of Master Agreements for OTC Derivatives. Springer Science & Business Media. pp. 7–. ISBN 978-3-642-56932-6.\n- Corporate Finance: Part I. Bookboon. pp. 32–. ISBN 978-87-7681-568-4.\n- http://www.emeraldinsight.com/Insight/viewContentItem.do;jsessionid=EFA8D4FB63329F2C94F48279646551BF?contentType=Article&contentId=1649008 (contrary to conventional wisdom it may be rational to hedge translation exposure. Empirical evidence of agency costs and the managerial tendency to report higher levels of translated income, based on the early adoption of Financial Accounting Standard No. 52).\n- Aggarwal, Raj, ""The Translation Problem in International Accounting: Insights for Financial Management."" Management International Review 15 (Nos. 2-3, 1975): 67-79. (Proposed accounting framework for evaluating and developing translation procedures for multinational corporations).\n- http://www.iijournals.com/doi/abs/10.3905/jpm.1997.409611 (Discusses the benefits for hedging in foreign currencies for MNCs).\n- CERA - The Chartered Enterprise Risk Analyst Credential - Society of Actuaries (SOA)\n- Financial Risk Manager Certification Program - Global Association of Risk Professional (GARP)\n- Professional Risk Manager Certification Program - Professional Risk Managers\' International Association (PRMIA)\n- Managing a portfolio of stock and risk-free investments: a tutorial for risk-sensitive investors\n- Risk Journals']"	['<urn:uuid:d70e5a40-726a-450c-afae-345166438945>']	open-ended	with-premise	short-search-query	distant-from-document	expert	2025-04-14T18:34:38.591736	48	523	6074
23	hoa board architectural restrictions outdated covenants not enforcing legal consequences	When HOA boards encounter outdated covenants, they have only two legal options: 1) enforce the covenants exactly as written, or 2) present a declaration amendment to the community to approve removal of the outdated requirement. Deciding not to enforce a covenant is legally problematic unless it violates State or Federal law. Boards have a duty to enforce all restrictions as written, and residents are legally entitled to rely on and insist on enforcement of these covenants. Failing to enforce restrictions can expose the association to liability.	['For those of you who have served, or currently serve on a board of directors or an architectural review committee, have you ever felt like the evil parent telling your child “no”? No matter how you put it, the child continues to argue that he/she should be able to carry out the “forbidden” activity, until everything comes to a head and the child is sent off to his/her room.\nUnfortunately, when it comes to the architectural review process for associations, resolution of a dispute is not as easy as putting your foot down and sending an owner to his/her room. To the contrary, the architectural process is closely scrutinized by courts and owners in the communities. Therefore, before denying (or approving) a request, it is imperative that the architectural review committee dot its i’s and cross its t’s.\nBelow, you will find examples of common pitfalls our office has seen that impaired rights of associations to enforce their architectural restrictions.\nFailure to be consistent\nWhen it comes to enforcement of restrictions and approval of proposed exterior modifications, consistency is key. In fact, it has been our experience that more and more judges are demanding consistent enforcement of covenants and rules before allowing associations to enforce such covenants and rules through the court process.\nConsistent enforcement means an association is treating all owners the same and requiring all owners to comply with the same standards. Consistency does not mean an association can never grant variances, but if an association does grant a variance, it must be able to demonstrate why the particular property needed the variance and distinguish it from the other properties that did not receive variances. If an association cannot do this, a court will treat the variance as unequal treatment of owners and a failure to be consistent.\nHow can an association ensure it is consistent? By creating written criteria (i.e. a checklist) that will be used to evaluate each proposed modification and using only those criteria with respect to every proposed modification. For example, when it comes to installation of gazebos, the association could create a checklist that sets forth appropriate locations, maximum dimensions, appropriate materials, acceptable colors, acceptable architectural types, etc. As the ARC reviews each proposed gazebo request, it can check off each criterion to mark compliance. At the end of the evaluation, if all criteria were met, the gazebo is approved. If one or more of the criteria is not met, the structure is denied.\nFailure to follow process\nOftentimes, a specific process is set forth in the governing documents or architectural guidelines which governs ARC submissions and reviews. The process may involve requirements for owners to complete specific forms and provide copies of certain materials to the ARC. The ARC, on the other hand, may be required to send a notice of receipt of an ARC submission and perhaps some other acknowledgment or documentation.\nIf the process is not followed by the owner, the ARC will likely have a basis on which to deny the request. If the ARC fails to follow the process and denies the proposed improvement, the owner may have a legal argument that the denial was unlawful. Even if the ARC approves the proposed improvement, but fails to follow its process, an angry neighbor who dislikes the improvement may have an argument that the ARC breached its duty by allowing this improvement and failing to follow its own process for doing so.\nBased on the above, it is imperative that architectural review committees (and board) be aware of and follow their processes closely.\nFailure to timely respond to ARC request\nSome declarations (especially older ones), contain provisions requiring ARC’s to respond to exterior modification requests within a certain amount of time, or the request is deemed approved. For example, it is not unusual to see a provision indicating that failure of the committee to respond to a request for modification within 30 days of receipt is deemed an approval of the request.\nIf your governing documents contain such a provision, it is imperative for the association to have some sort of process in place that ensures all ARC requests are reviewed and responded to within the 30 day time frame.\nFailure by an association to comply with such requirement may lead to installation of a modification that is prohibited by the governing documents. In turn, such installation could expose the association to liability based on breach of duty (i.e. failure to timely respond).\nFailure to enforce restrictions as written\nA board of directors has a duty to act in the best interests of the community, which includes the duty to enforce all restrictions and covenants as they are written. However, as communities age, some of the covenants that may have originally made sense, may no longer benefit a community.\nFor example, 30 years ago the requirement that all homes paint their picket fences white may have been consistent with the overall aesthetics of the association. However, in 2013 white picket fences are no longer viewed as positive attributes. In this scenario, a board may decide to simply stop enforcing the white picket fence requirement and allow owners to paint their fences other colors.\nAlthough this may seem harmless at first, what happens when Bob Smith (a lover of white picket fences) moves into the neighborhood and starts complaining to the board that his neighbors’ fences are painted brown? Legally, Bob Smith is entitled to rely on the covenants as they are written and is entitled to insist on enforcement of those covenants. If the board refused to enforce the restrictions as they are written, the association faces liability exposure.\nSo, what’s a board to do in this type of situation? There are two options: 1) enforce the covenants as they are written, or 2) present a declaration amendment to the community to approve removal of the outdated requirement. Making a decision not to enforce a covenant is never a good idea unless the covenant violates a State or Federal law.\nFailure to timely file legal action\nThe Colorado Common Interest Ownership Act (“CCIOA”) prohibits associations from commencing legal actions against owners to enforce violations of building restrictions, unless such actions are brought within one year of when the Association knew or should have known of the violation (a/k/a statute of limitations).\nA common mistake made by boards (and ARC’s) is to send demand letters to, and impose fines against, the violating owners and believe that such letters stop the one year clock. The only action stopping the statute of limitations is filing a complaint in court.\nAnother nuance of the one year statute of limitations is figuring out when the association should have known of the violation. Generally speaking, if a violation is clearly visible from the street, the statute of limitations beginS when the modification is completed. On the other hand, if the violation is not visible from the street (i.e. it’s in the backyard where only neighbors can see); the one year statute of limitations may be extended until such time as the association received a complaint about the violation. However, the specific circumstances of each violation are different and legal counsel will need to perform a case-by-case analysis of each violation.\nFor more information on the architectural review process, click here.']	['<urn:uuid:ea771ed7-b08d-4c30-bba2-bb90b9183df6>']	open-ended	with-premise	long-search-query	similar-to-document	expert	2025-04-14T18:34:38.591736	88	550	7456
24	atomic mass unit size grams conversion	One atomic mass unit (amu) is equal to 1.66 · 10^-24 grams.	['Weight is a way of measuring force, which Newton’s 2nd rules of movement identified as the item of size multiplied from the velocity. 8 yards) for each and every second squared. For this reason somebody’s lbs depends on gravity, and you may was other if mentioned toward Moon; mass, on the other hand, is the identical throughout the market. Provided their invariable really worth, researchers typically cam with regards to bulk in lieu of weight.\nWeight changes as the a reaction to the latest gravitational eliminate of the planet, moonlight, or other human body about what it’s mentioned. Which men weighs in at faster on Moon, just like the Moon and it has faster bulk than just Environment, and you may, hence, exerts smaller gravitational force. Thus, it will be easier towards the Moon in order to elevator a man throughout the floor, it will be zero better to disperse that individual off a sleep updates, or to prevent her or him off moving. It is because the individual’s mass, and hence their effectiveness inertia, have not altered.\nNuclear Mass Units\nChemists don’t always contract within the highest products of mass, like the size out-of a looks – which, however, are counted during the kilograms. Instead, the new chemist’s tasks are will concerned with size of bulk to possess the tiniest sort of amount: molecules, atoms, or other primary particles. To measure these types of inside terms of grams (0.001 kilogram) was absurd: one atom of carbon, as an instance, have a mass of 1.99 · ten ? 23 grams. Simply put, a great gram is approximately fifty,000,100000,100,000,000,000,100 moments bigger than a carbon dioxide atom – hardly good usable research.\nAs an alternative, chemists have fun with a keen atom size equipment (abbreviated amu), that’s equivalent to step 1.66 · ten ? twenty-four grams. In spite of this, is difficult to assume deciding the newest size out-of single atoms into the a regular basis, thus chemists use figures to the mediocre atomic size off a specific feature. The average nuclear size out of carbon, such as, are amu. As well as the situation having any mediocre, because of this certain atoms – various other isotopes from carbon dioxide – u is still credible. Additional average atomic bulk figures for different issues try just like the follows:\n- Hydrogen (H): step one.008 amu\n- Helium (He): 4.003 amu\n- Lithium (Li): six.941 amu\n- Nitrogen (N): amu\n- Clean air (O):\n- Aluminum (Al):\n- Chlorine (Cl): amu\n- Gold (Au): 197.0 amu\n- Hassium (Hs): [265 amu]\nNew figure to own hassium, having an atomic amount of 108, is given from inside the brackets because this matter is the bulk to own the newest longest-stayed isotope. The common value of bulk towards the particles during the confirmed substance normally made in terms of nuclear bulk products: water (H 2 O) particles, for-instance, keeps the common mass out of amu. Molecules out-of magnesium oxide (MgO), that is obtained from sea water and you can utilized in while making ceramics, keeps the average bulk a lot higher compared to h2o: amu.\nSuch philosophy try obtained by simply including those of the new atoms as part of the molecule: once the drinking water has two hydrogen atoms plus one oxygen, the common unit bulk try acquired by multiplying the common atomic mass from hydrogen because of the one or two, and you will adding it towards average nuclear size of outdoors. Regarding magnesium oxide, the latest oxygen was bonded to at least one almost every other atom – however, magnesium, that have the typical atomic reddit Tucson hookup bulk out-of , weighs more than just hydrogen.\nIt’s been very important to an excellent chemist knowing just how of many atoms are located in confirmed shot, particularly in happening of a substance effect between a few or even more samples. Of course, it is impossible to help you number atoms and other basic particles, but there is however ways to determine whether a couple of activities – no matter what issues or compounds in it – have the same number of basic dirt. This technique utilizes the fresh rates getting average nuclear size that have been created per ability.']	['<urn:uuid:88efbbc6-f93d-4a6f-a880-32205ef34cee>']	factoid	with-premise	short-search-query	distant-from-document	expert	2025-04-14T18:34:38.591736	38	59	4133
25	difference windows apple vs linux software updates and fixes	Open-source systems like Linux tend to be improved more regularly than closed source ones like Windows and Apple OS X. This is because with open-source programs, users can identify bugs and fix them, while closed source systems are modified only by organization personnel.	['Author: Henry George\nLet’s say you are building a website and you are using jQuery plugins, CSS frameworks or libraries. Managing all these resources one by one can be very complicated at times. Instead you can use Bower. Bower is a package manager that you can use in your web development projects, for example. It facilitates the task of downloading packages from libraries and frameworks such as jQuery, Bootstrap, FontAwesome, etc. You can download them one by one or list them all and download them directly in your project, then you just have to link them to the pages that will make use of these resources in your project.\nBower by definition is a package manager for the web, now that in ordinary Spanish means, well, it is a tool that allows us to define which libraries we need to use in our web application and the system detects the dependencies of the required packages.\nWhen bower checks all the dependencies of the requested packages and finds any inconsistencies, it presents us with a minus so that we can choose which version we want to use.\nNow you will wonder what kind of packages are available, well the packages range from jQuery to Bootstrap through Linux, it all depends on what our project needs, currently bower has almost fourteen thousand packages.\nHow to install Bower\nBower is an npm package therefore we need NodeJS and with the following command we can install it.\nHow to use Bower\nOnce Bower is installed we can use it immediately in our application to download the packages we want, let’s imagine that we need jQuery in our project for which we only need to use the following command.\nAs we have seen, it is quite easy to add new components, but it depends on someone’s memory to install the required packages, in order to define all the packages required by our project we use the bower.json file.\nFortunately, bower provides us with a command that allows us to create a basic bower.json file, followed by an example of the interactive command execution $ bower init .\nNow what advantage we have with this bower.js file, the first thing would be portability since we only have to version this file in our version control system.\nIn this case, bower will detect the existence of a bower.json file and proceed to install the requested packages, allowing us to have our version control system cleaner and thus avoid having 50 versions of jQuery stored when in the end the one that interests us is probably the one last.\nSpecify packet destination\nAs we have already seen, the default destination for the packages is the bower_components folder , but it is very likely that this location is not to our liking.\nWhich will make the packages be saved in the web / vendor folder or whatever we want.\nI hope it has meet your expectations.…\nPrograms, including operating systems and applications, are created using programming code. Programmers write the source code, which is then compiled into executable code, creating the product that you ultimately run on your computer. There are many operating systems for personal use, for business and industrial contexts, and some of them are closed source, and some are open source. In general, an open-source system is one in which the source code is visible to the user, while in open systems, the code cannot be seen.\nThe principle of an open-source program is that, as a user, you have the right to access the source code. If you are using a closed source system, you do not have access, even if you have purchased it with a commercial license. In addition to being able to see how a program has been implemented, many open source licenses allow you to improve and redistribute the program. However, this often depends on the type of license you use.\nHaving access to the source code is not the only difference between open source and closed source systems. While closed source operating systems are typically developed by corporations and modified only by organization personnel, open-source systems tend to be developed by large communities. Because open-source programs have visible code, users and consumers of the program can identify bugs and even fix them. For this reason, open-source systems tend to be improved more regularly than closed source ones.\nThere are many closed source systems in use. The Microsoft Windows and Apple OS X operating systems are closed source, and both platforms provide programs for a variety of computing contexts, including personal, server, and mobile. The most widely used operating systems for personal computers, particularly for desktop PCs, tend to be closed source, although open source alternatives are on the rise.\nThere is a variety of open-source operating systems for desktop, server, and mobile devices. Although most PCs run on closed source systems, many Web servers run on open source platforms, particularly Linux, which became the most common open-source system in 2011. The rise of computers Mobile has also caused an increase in open-source operating systems, including Ubuntu Linux and Google’s Android and Chrome systems.\nIn addition to the fundamentally different principles involved when choosing an open or closed source operating system, there are some major differences between the systems available to users. Although Linux systems are free, many users are not comfortable using them since they can be more difficult for those who are not experts. However, with mobile computers, the use of open-source platforms is exposing these technologies to new users.…']	['<urn:uuid:634b9406-6cf2-4b5c-b591-f2af99928a78>']	factoid	direct	long-search-query	distant-from-document	novice	2025-04-14T18:34:38.591736	60	272	5499
26	how chinese letters added to computer text	Creating Chinese fonts for computers is highly complex due to the large number of characters involved. A complete GB 18030 Chinese font contains about 30,000 glyphs plus another 8,000 design variants for Taiwan and about 3,500 for Hong Kong. Due to this incredible amount of design and production work, Chinese fonts must be created by a team rather than an individual. The Chinese ideograms developed completely independently from Latin script and require special considerations for both horizontal and vertical writing.	['Mourad Boutros interviews Peter Rosenfeld, whose 35 years’ work in type influences how we read the same way in different scripts\nPeter Rosenfeld has crafted a career in designing fonts. After finishing his business studies (marketing & sales) in 1980, Rosenfeld started work as a junior sales manager for fonts at Dr. Hell in Kiel, Germany. During his two-year stay, he was trained in typography as well as in digital font production. In 1982, he left Dr. Hell to start a new position at URW Software & Type, a young software-development company focusing on digital font design and production in Hamburg, Germany. Within a few years, Rosenfeld had become the manager for the URW font department, responsible for both the font production and font marketing and sales for OEM clients as well as users of a URW proprietary sign-making system (SIGNUS). During this period and under his guidance, URW developed well over 1,000 digital fonts for the URW library and hundreds of fonts for IKARUS customers such as Letraset, Monotype, ITC, Scangraphic, and many more. In 1995, Rosenfeld and his partner Dr. Jürgen Willrodt founded URW++ after having acquired URW’s IKARUS software system for font design and production as well as the legendary URW font library of about 2,500 ultra-high quality outline fonts, which both of them had helped to build. Ever since, Rosenfeld, Jürgen, and the entire URW++ team of type experts have expanded the font library, especially relating to non-Latin fonts.\nLanguage Magazine: What are global fonts? Are they a family of fonts that would enable a global brand to present a consistent typographical image across all countries, languages, and writing systems, or are they merely a series of font bundles comprising a broad range of writing systems that can be used on a global basis?\nPeter Rosenfeld: The development and expansion of global markets and new technologies have brought new challenges and demands for font manufacturers. New products keep pushing into the Near East and Asian markets. In order to be successful, such products must be equipped with Asian writing-systems fonts, such as Arabic, Hebrew, Cyrillic, Devanagari, Chinese, Japanese, Korean, and many more. The world has become a global village during the last two decades. The internet has locally connected completely separated regions and information is traveling in seconds around the world. English has been the lingua franca of the internet in its early days, but now communication is happening in all languages. Look at Wikipedia, available in more than 100 languages and viewable in their native scripts. Multilingual fonts are an important building block of the architecture of the World Wide Web.\nGlobal fonts in general cover all languages and writing systems commercially relevant and demanded. URW global fonts contain up to 65,000 characters per style, the physical boundary of TrueType. All scripts within a global font should be harmonized in weight, style, and flavor. Last but not least, a true global font must contain OpenType language features for complex scripts, such as Arabic and Indic scripts. URW’s global fonts are indeed designed to enable a global brand to present a consistent typographical image across all countries, languages, and writing systems. Nimbus Sans Global [pictured left] and Nimbus Roman Global, URW’s two global font families, are available off the shelf representing a standard grotesque (sans serif) and roman (serif). However, it is even possible to design and produce an exclusive, customized global corporate font family with all 65,000 glyphs per style exclusively designed as a corporate typeface (Siemens, Daimler, Nokia).\nLM: Is the former actually possible and, if so, what possible advantages could there be? For instance, why would a global brand manager want to do this at the level of the written word rather than simply the brand logotype itself? Is it not sufficient for the logotype on a print advertisement to be converted into another language following the same colors, style, and layout of the original? What advantage could there be for the text to be consistent and would anyone other than a typographer notice?\nPR: Type is a relevant part of corporate design, next to the logo and color. The internal communication as well as the company image is determined by it. Type can generate a high level of brand recognition and thus provides identification with the company, both internally and externally. The fact that it is not consciously perceived as often as the company name and the company logo does not diminish its significance. It is a proven fact that it greatly influences the unconscious perception of a company.\nLM: Could you tell us something about the technology involved in creating a global font? What are the obstacles to overcome?\nPR: Not only is the design of foreign language scripts a challenge, but the production of these large and complex fonts is a huge task which requires a lot of know-how. One has to understand the required technology and also to understand the syntax of the language. Ideographic language systems like Chinese contain an incredible number of glyphs; alphasyllabary scripts like the Indian scripts or the Southeast Asian scripts are following really complex rules. The OpenType technology to represent and process these complexities on a computer is about 15 years old and creates for the first time the opportunity to communicate world wide.\nLM: Are there any languages that have to be excluded from the concept of a global font?\nPR: No, the only boundary is the technical two-byte restriction of TrueType with a maximum of 65,536 glyphs in a font.\nLM: Obviously, some languages will be creatively and technologically less demanding than others. Could you give us some idea of the hierarchy of these languages, i.e., which are the easiest and which are the most difficult?\nPR: URW++ has designed and produced thousands of Latin typefaces and hundreds of Greek and Cyrillic alphabets. Obviously, these closely related scripts are creatively and technologically less demanding than other more ‘exotic’ scripts like Arabic, Indic, or Chinese for us. In order to be able to develop global fonts, you cannot but get really into it. Of course, we understand and appreciate that non-Latin scripts need to be designed by or at least under the control of native type designers from the respective countries or areas of the world. We started to build a global network of type designers, partners, and joint ventures in the early 1980s in order to learn about the design and production of all the non-Latin scripts. Chinese is very complex in terms of numbers of ideograms. A complete GB 18030 Chinese font contains about 30,000 glyphs plus another 8,000 design variants for Taiwan and about 3,500 for Hong Kong. This is an incredible amount of design and production work. It is easy to understand that Chinese fonts must be created by a team, rather than by one individual alone. Chinese is not related to Latin at all. The Chinese ideograms have developed completely independent from Latin. The same [degree of difficulty] is true for Japan, with a combination of kana (syllables) and about 8,500 kanji (Chinese ideograms adopted by Japan in medieval times and adjusted over time to better reflect the Japanese culture). Korean hangul, with over 11,000 syllables, is another design and production challenge. Although the basic design of the syllables is not as complex as in Chinese, the challenge lies in the combination of the syllabic blocks.\nLike in Latin, white space is of major importance, and every single glyph needs design attention. Chinese and Japanese are written both horizontally and vertically, which requires specially designed characters, e.g., punctuation, for vertical setting and according language features. Even more complex, relating to language features, are scripts like Arabic. This beautifully flowing script requires every character to have variants for initial, medial, and final setting depending on its position in a word. Aside from designing all characters three or four times for individual position, the fonts must carry the corresponding language features for positioning (GPOS) and subsetting (GSUB). At this level, a fully equipped global font becomes more of a program, much more complex than single-byte fonts from the past. Finally, a global font needs to come with the major Indic scripts. Indic scripts are even more complex than Arabic, because not only are there different character shapes depending on position, but on top of that, characters can move vertically and horizontally besides changing form — incredibly complex. We have been working on a set of ten Indic scripts for the last three years, and we are not yet done.\nLM: Do global fonts have anything to do with creativity and design? Are they not more geared toward technological information systems that can process data in different writing systems? In other words, are not global fonts designed for reading and processing orders, usage instructions, packaging, and other embedded devices rather than for designers and typographers?\nPR: Yes, the initial demand was for a set of technically perfect fonts with all different writing systems including language features for embedded devices of all kinds, translation systems, user manuals, packaging, and so on. However, pretty soon, clients like Daimler, BMW, and Siemens, who are selling their products around the world, started to ask for real global fonts individually designed to fit their existing sets of corporate fonts. Ever since, we have designed a number of additional, partly exclusively designed sets of global fonts. Look at the corresponding websites, advertisements, annual reports, etc. All designed and created by typographers and designers for such clients. The demand for individual, exclusive global fonts is growing, resulting from ever more companies selling products globally. Think of companies selling fashion, sports, luxury goods, you name it. Many of them will have a need for corporate global fonts completing the corporate design.\nLM: Is there not a danger that the drive to achieve consistency on a global basis will undermine creativity and individuality on a local basis?\nPR: No, not at all. Driven by the economic strength of the Asian countries, there is much more interest in type design and typography in these countries, and we are seeing a lot more new typefaces from local designers than before. Type conferences are held in India, China, and Japan, manifesting the growing interest in local font design and typography. Also, type designers from Asia see the potential of working for Western clients. We at URW are used to working with designs and experts from around the world. It is fascinating and educative at the same time. Today, global fonts are primarily used by global players. However, as stated before, the world has become a global village during the last two decades. Multilingual fonts are an important building block of the architecture of the web, and we can expect to see a lot more designers and typographers around the world using global fonts for web design and web typography.\nMourad Boutros, of Boutros Fonts, has been designing type since 1966, and is author of Talking about Arabic and Arabic for Designers. For more on Boutros’s work as a type designer and designing Arabic type, please see the August 2011 issue of Language Magazine.']	['<urn:uuid:68525b83-5024-4c51-8650-0430cded69e4>']	open-ended	direct	short-search-query	distant-from-document	novice	2025-04-14T18:34:38.591736	42	521	11460
27	I'm interested in crime-solving games but want something more educational - are there any games that teach you real skills while you investigate cases?	Where in the World is Carmen Sandiego? is a notable example of an educational crime-solving game series. While solving cases, players need to use real history and geography knowledge and employ genuine research and deduction skills. The game follows a procedural pattern of heist, investigation, and capturing thieves, and while its crime-solving may not be as realistic as other police games, it requires players to learn and apply real knowledge to progress.	"['Procedural success stories\nIn the 1980s, the PC was a platform for RPGs and adventure games. The technology for realistic simulations didn\'t exist yet, but Police Quest did the best it could with a command line and then Sierra\'s SCI engine.\n""The adventure game genre at the time was very linear,"" Jim Walls wrote. ""Police Quest fit well because police work in itself is linear (you follow the trail of clues and see where they lead). The design of Police Quest was intended to present the player with an array of police activities from the mundane, attending a briefing, to beating back a deadly assault, such as a shootout. I think we did a fairly good job in presenting both ends of the spectrum. Even the mundane briefing was fun because the player could pick up hints, clues and a little humor while being briefed.""\nPolice Quest retrospectives often aren\'t too kind to the series, criticizing the mundanity of its procedural tasks or that the stories played out in a logical way. But Police Quest was early proof that police procedurals could be playable, and the series was popular enough to see four adventure games before it evolved into the first-person shooter series SWAT.\nFew game developers attempted to make procedurals in the 90s, but the PC was a more common platform for simulation-oriented games than the console. FMV games like Spycraft: The Great Game again pushed technology to depict police work (or, in this case, CIA work) as realistically as possible.\nAs I struggled to find more police procedurals in gaming history, I softened my requirements. There are dozens, hundreds of detective games, often point-and-clicks like David Gilbert\'s The Shivah , or Westwood\'s classic Blade Runner , or Gemini Rue . Gilbert cited Discworld Noir as an inspiration. None of those are procedurals, but then I stumbled onto a game that was. It doesn\'t star a police officer, but it may be the most successful procedural game series ever made.\nWhere in the World is Carmen Sandiego? is absolutely a procedural game: each case follows the same pattern of heist, investigation, and thief-napping. Carmen Sandiego\'s crime solving may not be as realistic as Police Quest, but the games use real history, geography, and deduction to solve cases. The underlying system can be gamed, but playing Carmen Sandiego right requires genuine research and learning.\n2011\'s L.A. Noire may be the most high-profile procedural game ever, but it came at a cost—after a five year dev cycle, developer Team Bondi shut down a few months after release. For noir fans, though, the game was a home run. It was a unique police procedural, set in a time period few games have touched, with a lengthy story and amazing attention to detail. It sold more than five million copies in three years—surely that\'s proof there\'s an audience for police procedural games.\nSo why, in 2014, are there still so few? Indie darling Papers, Please uses a procedural structure to depict the life of an underpaid border agent dictating who immigrates into his country. It\'s remarkable for how it integrates the mechanics of its gameplay into the theme—shuffling papers and desperately flipping through a rulebook evoke the physical acts, even when done with a mouse pointer. The game also perfectly captures the monotony of performing the same tasks day-to-day. It\'s affecting, but not fun in the same way procedural TV shows are.\nLaw & Order and CSI both have licensed games (CSI, in particular, has quite a few), but they\'re all clearly low budget games aimed at hardcore fans of the shows. They definitely are procedurals—they focus almost completely on the procedure of investigative work, with clues and items to discover and witnesses to interview for various cases. But they don\'t hold the same appeal as the shows.\nIf the basic elements are there, why aren\'t they more popular, when each TV series has lasted for hundreds of episodes? The answer, I think, is that the procedure of police work isn\'t actually why we watch procedurals at all.\nThe secret to procedural popularity\nSome of the most popular procedurals, like Law & Order and NCIS, barely even attempt realism. Others like Bones and Castle focus more on banter between characters than real crime solving.\nJim Walls has a theory that seeing the process of police work lends a comforting sense of order to the world. ""Investigative techniques, such as, forensics, fingerprinting etc. seem to almost guarantee the bad guy gets caught,"" Walls wrote. ""We are inundated on a daily basis with mayhem, chaos and craziness all around the world. It\'s nice to know there are people working very hard trying to get a handle on it. I believe this is why television procedurals are so popular. They offer a story, a sense of order and closure.""\nWalt Williams offered another answer: it\'s the structure of the show, not the content, that always brings us back. ""I think the procedural thing, at least in the form of television, has a dual meaning,"" he said. ""[There\'s] the procedural aspect of: this is the procedure that these people have to go through in order to solve a mystery and convict this person. But also...every time you tune in to watch a show, the show itself is procedural. It\'s always going to be a stand-alone episode that starts with this and ends with this and goes by the book. Like House, I think, is an example of a procedural, because every episode is ultimately the same thing. So you\'ve got that dual meaning with procedural, which I think makes it a bit trickier to translate over to a video game procedural.""\nReliably formulaic plots make procedurals TV\'s greatest comfort food. We know what to expect. Procedurals rarely challenge us to think too hard, though there are exceptions—The Wire, for example, is beloved for its nuanced writing and acting, not its formula. ""The Wire could be considered almost the ultimate procedural, in that it basically takes one case that has a domino effect over five seasons and shows every little intricate aspect of how it spreads out through all the people who touch it, even remotely,"" Williams said.\nIf we watch most procedurals for the comforting familiarity, then, how can that essence of the genre translate to games? By Williams\' logic, Papers, Please is almost the anti -procedural game. It uses the structure of the procedural to tell a story of grinding monotony, rather than pleasant predictability. For a game to match the popularity of the police or court procedural, it may have to come at things from a different angle.\n""We have a different gateway, in games, than other mediums have to bring in new ideas,"" said Williams. ""The Last of Us is basically, at its core, a story of a single father taking care of a teenage girl, and the struggles that coincide. It could be a story that takes place in the real world. They\'re homeless and [Joel would be] an alcoholic father who can\'t keep a job and they\'re going across the country. The exact same story, basically. But it\'s video games, so we needed zombies. I feel like, with a procedural type of game, you could bring that to the forefront and make that something very popular and desired by gamers, but you couldn\'t come straight in and make it a straightforward, realistic, cop procedural. You need to come in from that video game angle.""\nThe future for procedural games\nIndie developer Dave Gedarovich has added a third meaning to ""procedural"" with his crime game Noir Syndrome. It\'s a noir mystery procedural that\'s procedurally generated—every case has a randomly selected murderer, randomly generated clues, and NPCs offering hints about the crime. It took him about three months to figure out how to make the investigative aspects of the game fun.\n""When I first started making the game, I kept thinking, how can I make this fun?"" Gedarovich said. ""People wanted more solving a mystery. They didn\'t come for the action, or walking around collecting food. They wanted to feel smart that they unraveled the mystery and caught the killer. I really had to expand on the notebook, all the clues, I had to make them intertwine and really point toward the killer but not be too obvious so that people still feel good about capturing them.""\nGedarovich sees procedural generation as a potential solution to the challenge of making repetitive police practices fun. Episodic delivery is another format that seems perfect for procedural gaming.\n""When I play some episodic games I think about how powerful the format can be, how you get attached to these characters over a long period of time,"" said David Gilbert. ""[Walking Dead] would not be nearly as effective if it was played all in one sitting. But because it\'s released monthly, these characters sit in your head for a little bit of time, and it\'s like reconnecting with them each episode.""\nWilliams agreed, but pointed out that a procedural wouldn\'t necessarily work within the same kind of episodic setup as The Walking Dead.\n""I think the season pass model kind of kills it, to be honest. When you buy a season pass, you\'re expecting some kind of arc. If, instead of putting out a season pass, you were just like, this is the game that we\'re going to be doing, and we\'re going to put out an episode every couple months as we finish them, and we\'ll keep putting it out as long as you guys keep enjoying it and wanting to buy it. You could work in bigger story structure stuff that connected each episode. The season pass, I feel like, instantly gives the consumer a mindset of, I\'m buying a big connected experience, versus the episodic procedural-type format.""\nFrom a business perspective, of course, game developers would rather get the money up front to fund development, hence the season pass model. Gilbert also said that the gaming audience just doesn\'t have faith yet in episodic delivery—from developers other than Telltale, that is. Both he and Williams see that as an enticing future for story-driven games.\nJim Walls, who is still working on the design for Police Quest revival Precinct, sees another, perhaps simpler future for a police procedural.\n""My take on game design has not changed over the course of my career,"" Walls wrote. ""I still want to make the most realistic police game possible. Having said that, I do believe today\'s technology is becoming more and more in line with the way I have always wanted to design a police game...technologies of interface design are forever evolving. That\'s why we are excited about new tech like Oculus Rift, which puts the player into a more physical space of the experience. Virtual reality would be the ultimate step forward for the police procedural.""']"	['<urn:uuid:0de73eca-9722-484e-a48b-b75a60516639>']	open-ended	with-premise	verbose-and-natural	distant-from-document	novice	2025-04-14T18:34:38.591736	151	460	10604
28	fastest mechanical pet speed	The Spot Mini, a four-legged robot that resembles a cross between a dog and a giraffe, can run at speeds of up to 28 miles per hour.	['There are a lot of robots in EngiNerds. (And I mean a lot.) Mostly they’re just hungry, but they can do more than eat. They can walk and talk and, well, do the sorts of things we humans usually do after we eat . . . But maybe even more incredibly, the robots in EngiNerds show the potential to learn – to pick up new skills over time.\nReading about these bots, you might find yourself wondering whether any of this stuff could happen outside of a book. Just how smart and skilled can a robot really get? Well, the sorts of machines we think of today as “robots” have been appearing in science fiction stories for decades, and pretty much from the get-go people have been wondering these same things.\nRobotics is now one of the most thrilling and fastest-growing fields in the rapidly paced worlds of science and engineering. And whether you realize it or not, it’s possible you already have a robot in your life – and maybe even in your home at this very moment!\nMaybe you’ve got an “autonomous robotic vacuum cleaner,” like the Roomba, that scoots around your house on the weekend sucking up dirt and dust so that you don’t have to. Or maybe you have your very own CHiP, the “Lovable Robot Dog” who can learn how to follow you around, how to play soccer, and even how not to do its robotic business in the house. Or maybe you’ve got a robotic teacher’s assistant in your classroom at school.\nAnd even if you don’t have a robot in your day-to-day life, you’ve surely come across one at some point. Robots are playing increasingly critical roles in our hospitals, factories, and military, and in places like Japan, robots can be found nearly everywhere else, too – restaurants, hotels, offices, banks, and stores.\nBut to return to the question that started all of this (and one of the questions that got me writing EngiNerds in the first place): just how smart and skilled can a robot really get? To help answer this question, I’ve created a list of some of the most jaw-droppingly impressive robots around today. Read about them below — and prepare for that jaw of yours to drop.\nNASA’s Valkyrie is, without a doubt, the most forward-looking robot on this list – and maybe the most forward-looking robot currently in existence. Valkyrie was designed – and has since then been continually modified – with an incredibly lofty goal in mind: to help humans build a larger, more permanent presence in outer space, and to make that process as safe as possible. Valkyrie could, for instance, assist with the colonization of Mars. Several other, non-humanoid bots have already traveled up to the Red Planet, learning as much about the place as they can. Should a humanoid robot such as Valkyrie make it up there, it could, say, build sound and secure human habitats, eliminating a tremendous amount of risk and making humans’ time on Mars safer and more efficient. It may seem like something out of a science fiction story, but with both NASA and private companies investing more and more money and brainpower in missions to Mars, this future may not actually be too far off.\nIf you’re so inclined to fear the future – especially a future in which we’re surrounded by robots – you might just want to skip this section. Because South Korea’s Hankook Mirae Technology has built the bot that most closely resembles something that might’ve come out of a brilliant villain’s secret laboratory. The 13-foot tall Method-2 is a humanoid robot that can be controlled by a human “pilot” seated inside the cockpit-like cavity in its torso. If the pilot raises his or her arm, the robot raises its (300-pound) arm. If the pilot makes a fist, the robot makes a (metal-skinned, beach ball-sized) fist. And if you weren’t worried before, but kind of are now, don’t be. It’s likely that the giant bot will be used for good – such as for cleaning up wreckage and more quickly reaching survivors in the wake of a natural disaster.\nYou might’ve seen Honda’s Asimo on the news a few years back. In 2014, during a visit to Japan, President Barack Obama famously played soccer with the bot. But that’s not all Asimo can do – not even close. Its balance and agility – it can hop up and down on one foot, run smoothly and fairly fast, and climb stairs – are unmatched. And its movements are so clean and natural that, watching the bot do its thing, you’d be forgiven for wondering if the massive motor company hasn’t just shoved a small, very devoted actor into a robot suit. Asimo can talk, too, and even interact with humans. The bot can respond to calls, obey commands, and even recognize faces and specific postures and gestures. It’s not hard to imagine Asimo in a home or workplace, serving as a sort of robotic butler and handling all kinds of chores. And it doesn’t hurt that Asimo is polite. Approach the bot, and it’ll probably ask you to shake hands.\nLike Method-2, Atlas is a humanoid robot. This one, however, was designed here in the United States, and actually right in my backyard, at Boston Dynamics in Waltham, Massachusetts. Also unlike Method-2, Atlas isn’t gargantuan, but built on a more familiar scale. The bot stands just shy of six feet and tips the scales at 330 pounds – which, in case you’re wondering, would be like if you took a guy the size of Shaquille O’Neal and squashed him down about 14 inches. Like a lot of the robots on this list, Atlas remains a work in progress. The engineers at Boston Dynamics are continually tweaking the bot so that it becomes better at fulfilling its intended purpose – to lend a hand during emergencies and perform potentially dangerous search and rescue missions. Every time Boston Dynamics unveils a new version of Atlas, the bot’s got a batch of brand-new skills, and has usually thoroughly mastered all his old ones. The bot can quickly navigate rough, uneven terrain. It can open doors and lift, carry, and accurately place objects. Watching Atlas’s demonstration videos, you can’t help but feel that robots, made right, might one day do a tremendous amount of good in the world.\nClick here to watch Atlas in action.\nThe Spot Mini is another of Boston Dynamics’s creations, and in my opinion, it’s the coolest. A cross between a big dog and a baby giraffe, this four-legged, long-necked bot can do an astounding number of things. It can fetch you a can of seltzer, say, then stick around until you’re done so it can put your glass in the dishwasher and your empty can in the recycling bin. It can also effortlessly climb stairs, duck and dodge obstacles while navigating a house or other environment, and, should it slip and fall, it can quickly climb back up and continue on with whatever it’d been doing. And last but certainly not least, the Spot Mini can run. It has clocked speeds of up to 28 miles per hour, meaning it might be more appropriate to call the bot a cross between a dog, a giraffe, and a gazelle.\nClick here to see just what Spot Mini can do.\n. . .\nNot impressed? Well, that’s crazy. But if so, rest assured that in the years (and probably even days) to come, these robots will only get smarter and more skilled. You might even end up with an Asimo or a Spot Mini of your own, hanging out in your house waiting to be bossed around. The trick there is just to, you know, keep the things from eating everything in sight and kinda sorta trying to destroy your town . . .']	['<urn:uuid:4305dbda-50f9-466a-a229-5f83b3f5cb6d>']	factoid	direct	short-search-query	distant-from-document	novice	2025-04-14T18:34:38.591736	28	132	7323
29	What picture format should I use for my photos?	JPEG (or JPG) is the best format for photographs. JPEG files preserve most image and color detail while reducing file size significantly from bitmap formats.	"['There are two ways to store information about a graphic element (photograph or clip art): as a mathematical expression or as a description of dots of color.\nIn a vector format, the picture is represented by a mathematical description, typically only possible in clip art or illustrations.\nFor example, a vector format file decribing a piece of clip art of the moon might describe it as a circle with black line and white fill.\nAs another example, a vector format file describing a company logo would describe each line and curve mathematically, and indicate that the fill color is red.\nTypical file types for vector formats include:\n- Windows Metafile (WMF) - typical clip art format on a Windows system.\n- Enhanced PostScript (EPS) - typical file format from graphic artists using Adobe Illustrator.\nYou may have to experiment with file types to determine what looks good for a particular project. For example, some vector graphics look fantastic when printed, but the way Windows displays them on screen make them a poor choice for an on-screen presentation. Other vector graphics, like most clip art, look perfect when printed or displayed on-screen.\nIn a raster or ""bitmap"" format, the picture is represented by dots (pixels) arranged in rows and columns.\nFor example, a raster format file describing the company logo would describe each dot of red and, possibly, white.\n- Bitmap (BMP or TIFF) - an uncompressed and therefore very large raster image… archenemy #1 of PowerPoint presentations.\n- JPEG (JPEG or JPG) - a raster image compressed with the JPEG compression scheme. JPEG files preserve most image and color detail while reducing file size significantly from bitmap formats. JPEG is the best format for high-color, detailed graphics, including photos. Digital cameras typically store their images as JPEG files.\n- GIF - a raster image compressed with the GIF compression scheme. GIF files preserve all image detail and minimize file size at the expense of color detail. GIF images typically have a low number of colors (from 2 to 256). Low-color images such as logos and simple art are best saved as GIFs. GIFs have the ability to support transparency, meaning that one color can be designated astransparent. Logos often use white as the transparent color in a transparent GIF, so that the logo can be placed without a white \'box\' surrounding the logo.\nYou should try to use raster graphics (.GIF, .JPG, .TIF, .BMP) only when the picture was produced with the correct size in the first place, so you are not having to scale the picture.\nRaster versions of high-contrast art such as a logo can look jagged on a computer monitor, even in their correct/original sizes. That\'s why raster graphics are often antialiased to soften the edges. Antialiasing makes a high-contrast graphic look good on a monitor, but the soft edges look very poor when printed on paper.\nNote the blurry edges in the company logo above, which are particularly noticeable as the object is enlarged. These edges, even in the smaller graphic, would be highly apparent if printed on a quality laser printer.\n- GIF is the best file format for logos and other low-color, high-contrast, crisp images.\n- JPG is the best file format for photographs.\n- Anti-aliasing high-contrast art is useful for viewing on a monitor, but does not look good on paper.\n- If creating a raster image, it is best to produce the image in the size it will be used, so that it does not have to be scaled (enlarged or reduced).']"	['<urn:uuid:dca9d76b-b687-4a89-bc00-5342ef239a65>']	factoid	direct	concise-and-natural	distant-from-document	novice	2025-04-14T18:34:38.591736	47	157	3480
30	ways birds help garden ecosystem	Birds help gardens by eating pest insects like caterpillars and mosquitoes, helping with pollination (especially hummingbirds moving pollen between flowers), and facilitating seed dispersal by consuming and transporting seeds.	['Garden Lesson Plan: Birds and the Garden Ecosystem\nDownload: Birds and the Garden Ecosystem\nOverview: Birds are important and beneficial members of the garden ecosystem. They eat common garden pests, help with pollination, and aid in seed distribution. This lesson explores ways we can design our gardens to provide a supportive habitat for our feathered friends.\nGrade Level/Range: 6- 8th Grade\n- Learn that birds play an important role in our ecosystem.\n- Discover that bird populations are in decline and need our help to protect and restore their habitats.\n- Explore ways to design and plant bird-friendly habitats.\nTime: 2 hours\n- Interact access\n- Paper and pencils\nFound in a range of colors and sizes, birds fill gardens with song and activity. They are working hard for your garden, too. Birds:\n- Eat insects that we might find garden pests, such as caterpillars chomping on your lettuce or tomato plants and mosquitoes that are feasting on young gardeners.\n- Help with pollination. Birds that feast on nectar, such as hummingbirds, aid in the movement of pollen from flower to flower.\n- Facilitate seed dispersal. Fruit and seeds are a favorite food of birds. Some seeds will actually survive the digestive system of birds when consumed. They get a free flight to a new home and are dropped off surrounded by a “care package” of organic matter. In fact some seeds with hard seed coats will benefit from acid treatment of the digestive process, which will help with their germination. Other seeds hitch a ride on birds by getting stuck on beaks or in their feathers.\nUnfortunately, like many animal and plant species in our environment, bird populations are on the decline. National Geographic recently shared an article published in the journal Science that found bird populations have decreased by 2.9 billion birds since 1970 (which equates to a 29% reduction in the size of the population). They suggest many reasons for the decline. Including\n- loss of habitat\n- exposure to toxic pesticides\n- decrease in insects to consume\n- collisions with glass and vehicles\n- domestic cats\nFrom this list, loss of habitat is considered the top reason for the decline. Fortunately, this is something our gardens can help with. By choosing plants that offer shelter and food sources for local bird populations, we can help our gardens become a safe haven for these garden helpers. The following lesson will guide your class through the process of identifying the birds living in your community and then use that information to help turn your garden into a supportive habitat for them.\nLaying the Groundwork\nAsk students to read the article National Geographic article Three billion birds have been lost in North America since 1970 by Jason Bittel.\nAsk them to answer the following questions about the article:\n- Approximately how many different kinds of birds can be found in the United States and Canada?\nAnswer: 760 bird species\n- What methods did scientists use to discover the decline in bird populations?\nAnswer: population surveys and weather radar data\n- What are some of the reasons they list that have contributed to decreasing populations?\nAnswer: habitat loss, pesticides, decrease in insects, collisions with glass, domestic cat consumption\n- What are some important jobs that birds do in our ecosystem?\nAnswer: decrease insect pests, distribute seeds, eat dead animals, pollinate plants\n- What are some ways they suggest people can help birds?\nAnswer: Plant native plants, minimize impact of windows, keep cats inside\n- The first step in helping your local bird populations is identifying which bird species live in your area. Many birds migrate seasonally as the weather changes, so this is an activity that your students should conduct multiple times during the school year to compare their findings. Also, different bird species are active at different times of day. If possible, conduct one inventory first thing in the morning, and perhaps one mid-day.\n- First ask students to make a list of all the birds they have seen in your area. Next, take the class on a field trip to your garden, schoolyard or a nearby park or natural area to complete a bird checklist or inventory. Before heading outside, remind students to:\n- Respect all life in the garden. Observe living creatures with your eyes, not your hands.\n- Write down or draw as many details as possible. You may also want to bring a digital camera to help record your findings.\nBring a bird identification resource with you to see if you can identify some of the birds as you see them in the field. The Cornell Lab of Ornthology has created an amazing bird identification app called the Merlin Bird ID App. There are also numerous field guides available specifically designed for young observers.\nDepending on the skill level of your students, you can also opt to create ahead of time a pictorial checklist of common birds you may see to aid in the identification process. You may want to check with your local Audubon Society to see if they have regional checklists available for you to use.\nYou may also want to review these tips from the National Audubon Society before you begin.\n- To encourage more detailed exploration, in addition to identifying different types of birds, also have the students record how many of each they find. You can also ask them to take additional notes, such as to describe what they see them doing.\n- Before heading back in, take a look a look at your garden, schoolyard or natural area and complete an inventory of the different habitat elements you find, including plants that provide food, water sources, shelter, etc.\n- When you return to the classroom, begin by completing your identification and make an inventory list of all of the birds you observed.\n- Next, as a large group or in small groups or as an individual activity, research the habitat needs of each of these bird species. What foods do they like to eat? What kind of shelter do they prefer? Where do they like to build their homes? Do they migrate or do they live in your area year-round? What kind of water sources do they prefer?\n- Compare the resources you have available in your garden with the needs of the birds. Does you garden or greenspace currently provide what they need? Is there anything missing? Can you add these missing elements to your space?\n- Create a design to make your space more bird friendly. For example, identify native plants you can add to your garden that would provide food for your native birds. Brainstorm additional features you could add, such as shallow water sources, protective structures or feeders you could use to help provide supplemental winter food sources. If your region experiences freezing temperatures, consider a birdbath with a birdbath heater to ensure a continuous and reliable supply of water.\n- Participate in a local or national bird inventory. Here are a couple events to consider:\nOctober Big Day: https://ebird.org/octoberbigday\nThe Great Backyard Bird Count (February): http://gbbc.birdcount.org/\n- Write stories about the garden from the perspective of one of its newly identified residents.\n- Use your findings to create a field guide specific to your schoolyard garden to share with fellow students and community members.\n- Invite a local naturalist or wildlife expert to give a guest presentation about birds in your area. Based on their own school garden discoveries, have students prepare some questions for the speaker in advance.\nLink to Standards\nMS-LS2 Ecosystems: Interactions, Energy, and Dynamics\nMS-LS2-1. Analyze and interpret data to provide evidence for the effects of resource availability on organisms and populations of organisms in an ecosystem.\nMS-LS2-4. Construct an argument supported by empirical evidence that changes to physical or biological components of an ecosystem affect populations.\nMS-LS2-5. Evaluate competing design solutions for maintaining biodiversity and ecosystem services.\nMS-ESS3 Earth and Human Activity\nMS-ESS3-3. Apply scientific principles to design a method for monitoring and minimizing a human impact on the environment.']	['<urn:uuid:908dc2f1-732a-4f7c-aba5-4c7538d0527b>']	factoid	with-premise	short-search-query	similar-to-document	expert	2025-04-14T18:34:38.591736	32	226	8105
31	replacement ship old aurora australis details cost	The Nuyina replaces the Aurora Australis, which served from 1989 to 2020. The federal government has invested $1.9 billion to cover the design, construction and operation of the new vessel for the next 30 years. The Romanian-built ship was named through a school children's competition, with 'nuyina' meaning 'southern lights' in palawa kani, the language of Tasmanian Aboriginals.	['Australia’s state-of-the-art Antarctic icebreaker, Nuyina, has begun its six-week journey across the planet to its new home port of Hobart.\n- Nuyina replaces Aurora Australis, which was in service between 1989 and 2020\n- The new ship boasts vastly improved capabilities and a hefty price tag\n- The word ‘nuyina’ means ‘southern lights’ in palawa kani, the language of Tasmanian Aboriginals\nThe RSV (research and supply vessel) Nuyina departed Vlissingen in the Netherlands last night after completing final testing, and will arrive in Hobart in October after a 24,000-kilometre trip.\nThe federal government has invested $1.9 billion to cover the design, construction and operation of the 160-metre-long icebreaker for the next 30 years.\nFederal Environment Minister Sussan Ley said the vessel, which has been 10 years in the making and can withstand 14-metre seas and temperatures up to minus 30 degrees Celsius, will be “the backbone” of Australia’s Antarctic program.\n“She will be a floating platform for our Antarctic research, marine research, climate research, our million-year ice core in Antarctica, and is critical to our mission and she will carry our expeditioners and our scientists to and from the continent,” Ms Ley said.\n“The cost of the ship demonstrates the seriousness of our investment, $1.9 billion to cover its design and operation over 30 years.\n“This is a ship that will establish our Antarctic legacy for generations to come.”\nMs Ley said Nuyina is equipped with state-of-the-art technology, and can support voyages of up to 90 days at a time.\n“Being able to support voyages of 90 days at a time does make a difference because the window of travel between the port of Hobart and the destination in Antarctica is carefully planned because of weather and it’s good to have that buffer zone,” she said.\n“When you look at a more advanced vessel, you get advances in technology that allow the scientific program to continue unimpeded, you get better forecasting, you get better ability to withstand different sea states and weather conditions.\n“This vessel will be the most advanced of its kind to be deployed in the Southern Ocean.\n“It’s a really exciting moment for Tasmania, for Hobart, for our Antarctic expeditioners and also for the whole of Australia.”\nNuyina began sea trials late last year.\nDelivery voyage leader Vic Doust said he was looking forward to bringing the “game-changer” ship to its new home.\n“It’s not just another ship, it’s a game changer, particularly for research. There’s none other in the world.”\nNuyina replaces the Aurora Australis, which completed its last expedition for the Australian Antarctic Division (AAD) last year before leaving its Hobart port bound for Singapore.\nThe Romanian-built Nuyina, named by schoolchildren as part of a competition, was expected to arrive in Hobart last year, but impacts of the COVID pandemic caused delays in its delivery to the AAD.\nThe AAD had been using the MPV Everest as an interim icebreaker vessel, but it has been out of action since an engine fire broke out on board during a resupply mission in April.\nThe AAD was adamant that the ship “can be accommodated and operated at the Port of Hobart using current infrastructure”, with TasPorts saying work would be completed prior to the Nuyina’s arrival.\nOnce it arrives in Hobart in October, Nuyina will undergo testing, commissioning and certification, including ice trials in Antarctica before it departs on its first scientific mission to the continent.']	['<urn:uuid:64e2b2e1-13a9-407e-be70-00f3aee1ec88>']	open-ended	with-premise	short-search-query	distant-from-document	expert	2025-04-14T18:34:38.591736	50	381	3497
32	which are the variables that predict happiness levels top countries world	Six key variables help predict levels of happiness in countries: GDP per capita, healthy years of life expectancy, social support, perceived lack of corruption, freedom to make life decisions, and generosity (as measured by donation levels).	['The recently published World Happiness Report shows New Zealand ranks eighth in the world for happiness, behind Norway, Denmark, Iceland, Switzerland, Finland, the Netherlands, and Canada. Six key variables help predict levels of happiness – GDP per capita, healthy years of life expectancy, social support, perceived lack of corruption, freedom to make life decisions, and generosity, as measured by donation levels. Helen Borne asked two University of Auckland academics for their response to the report.\nWhy we should not be happy about feeling so happy\nThe recent World Happiness Report makes for interesting but somewhat complacent reading. It is all too easy to miss the fact that in all of the top countries, high levels of happiness are supported by high levels of long-term ecological recklessness and a widespread blindness to the inherent inequities of the global order.\nThe World Happiness Report would be better read alongside two related reports – the Living Planet Report and the Happy Planet Report. The first of these analyses the ecological footprint of a nation’s lifestyle and finds that all of those deemed to be happiest are enjoying life well beyond the pale of sustainable living. The second integrates each nation’s ecological footprint as a moderating factor to rank countries according to the relative efficiency with which happiness is achieved. Thus, if a country manages to produce high levels of felt thriving with few resources and little pollution, it rises in the ranking. In New Zealand’s case, we fall from 8th to 38th place as the environmental price of our happiness is taken into account.\nAs with ignoring the ethical requirements to generate happiness in ecologically responsible ways, the World Happiness Report similarly ignores the ethical requirements of justice and equity. It is important to remember that most New Zealanders belong to the global elite in terms of our material lifestyles and that the miserable conditions endured by the billions who live in the world’s slums are maintained by our (and other happily advantaged nations) continuing to hold onto the lion’s share of the world’s bounty. Just as our happiness is bought at the expense of future generations, it is also bought at the cost of those we are willing to exclude from the benefits of an advantaged lifestyle.\nIn the end then, it is not only how happy a nation might be that matters but also how happy we should be with how our relative felicity is secured. How happy ought we to be to live in a country where the major industries (both tourism and dairying) add to dangerous levels of greenhouse gases – and how happy should we be to live a nation that corners wealth in a world crying out to be included in a more humane global order?\nDr Ross McDonald is a national award-winning teacher working in the areas of well-being, ethics and sustainability. He is a Senior Lecturer in Management and International Business at the Business School.\nIs it really measuring happiness?\nNew Zealand being ranked as one of the top 10 nations in terms of the World Happiness Report makes me “happy”. The situation in New Zealand is very different from my country of birth, the Philippines, which tops world lists for corruption, human rights violations, journalists murdered and cataclysmic disasters. Digging deeper though, I have doubts as to how accurate this “Happiness Report” is. Is it really measuring happiness?\nThe World Happiness Report uses variables which are important in living a materially comfortable and safe life, but not a genuinely happy life, characterized by self-acceptance, meaning and contentment.\nI know many people who are physically healthy, not wanting in comfort, material possessions and external safety but suffering internally from self-doubt, disconnection, anxiety and lack of meaning. On the other hand, I have met materially deprived people living in huts, unsure of where their next meal will come from, who are content, at peace and have a deep sense of purpose in their lives.\nLast year, whilst travelling in Myanmar, I had a casual chat with a 14-year-old Buddhist novice. He was practising his English whilst I was being a nosy psychiatrist. I asked him about details of his life, his worries as well as his dreams. Throughout the chat, he exuded this aura of warmth, peace, and openness. He said that he was very content and thankful for what he has (two sets of robes, toiletries, jandals, study materials and a cracked cell phone). I pushed him to tell me if he wanted anything else in his life. I even asked if he wants sex (Buddhist monks choose to be celibate), normal clothes or a faster internet. He kept on saying, with a smile, “not really. I am happy”. I insisted that surely there is something else he wants in his life. Finally he gave in. I felt that I won. He said, “Oh yes, there’s one thing. I wish I can wake up earlier so that I am not late for our morning meditation”.\nI probably would get a different response from a 14-year-old from Norway, Denmark, Switzerland or New Zealand.\nDr Tony Fernando is a Senior Lecturer in Psychological Medicine, Faculty of Medical and Health Sciences, and “a perpetual student”.\nThis article was originally published in Ingenio (the University of Auckland Alumni Magazine) and was republished with permission.\nDisclaimer: The ideas expressed in this article reflect the author’s views and not necessarily the views of The Big Q.']	['<urn:uuid:090b58c3-6518-47f9-9ba2-befd5d1fb4b2>']	open-ended	with-premise	long-search-query	similar-to-document	novice	2025-04-14T18:34:38.591736	73	241	5429
33	When was the first example of written language created?	The first examples of the written word appeared around 3000 B.C. in the form of receipts for trade transactions in Mesopotamia.	"['The invention of written language is perhaps the most important milestone in human history, and we owe it to the accountants. Mesopotamia was a wild place at the dawn of the Bronze age. The Tigris and Euphrates rivers created a fertile stripe of perfect agricultural land arcing from the Mediterranean Sea to the Persian Gulf. Human civilization had flourished since the development of metal tools around 4500 B.C., and the potter’s spinning wheel had recently made storage and trade of agricultural produce possible. In this hotbed of development, trade between Mesopotamia’s many emerging city-states became a staple of the urbanizing culture, and so around 3000 B.C. we see the first examples of the written word — as receipts.\nStorage vessels were sealed with clay, and stamped with marks of ownership, and cylinder seals were rolled into clay tablets to designate the quantities of sheep or grain to be exchanged. This earliest form of writing is known now as cuneiform, and while the earliest writings recorded transactions, it was quickly adapted to the recording of ideas. Luckily for historians, cuneiform was finally deciphered successfully by the French scholar Eugène Burnouf in 1836 — opening the nearly two million discovered cuneiform tablets to translation. Only a fraction of these writings have been translated, but already we\'ve found stories, prayers, poetry and songs of the Mesopotamian world.\nThe rich and rapid social growth of the Mesopotamian city states brought war. From 3000 B.C. to the region’s final conquest by Alexander the Great in 332 B.C., Mesopotamia was awash in bloody conflict. To understand the politics of the time, it’s important to understand the geography. Mesopotamia was home to many urban centers, each home to their own unique culture and lineage of kings. Surrounding these cities was farmland, kept fertile by irrigation ditches — so the more farmland and irrigation a city controlled, the more powerful it became. For 3000 years these city’s fought, traded, taxed and murdered each other. For 1000 years the Sumerians from Sumer were dominant, until temporary defeat by the Akkadians from Akkad. Babylon would rise to power in 1900 B.C. and competed with the Hittites from 1450-1200 B.C. And so on…\nBut in this violent, messy time some extraordinary things happened. In 2140 B.C., Gudea of Lagash, a Sumerian king, realized he could control his domain more effectively with words than with the sword. Gudea understood that stable power grows from order, so he funded massive public works — temples to the popular local gods Ningirsu, Nanshe, Ningishzida and Geshtinanna. These imposing structures emphasized the values of peace and religion, and reinforced the Gudea’s authority. Finally, and most brilliantly, Gudea commissioned small, moveable statues of himself to be delivered to cities across his kingdom, making his face a fixture throughout Mesopotamia and becoming the first superstar, the first celebrity.\n380 years later the sixth Babylonian king, Hammurabi, would create another defining feature of human civilization — the first unified code of law. Engraved on a seven foot tall basalt stone the 282 laws establish terms of judgement for business transactions, proper wages, and relationships, including divorce and inheritance. The laws also defined a strict set of punishments for breaking the laws, including the now infamous lex talionis, or eye-for-an-eye system of retaliatory punishment. Hammurabi’s Law Code was not the first set of laws in Mesopotamia, but it was the first to be compiled by legal experts, reviewed, edited, and codified into a universal definition of law — setting a standard for governmental justice that’s in place to this day.\nIn the midst of Umma Entemena overthrew Urlumma and killed him. He left behind 60 soldiers of his force dead on the bank of the canal.\nI am Nin-jirsu. No country can bear my fierce stare, nobody escapes my outstretched arms.\nThe day was for supplication, the night was for prayer. The moonlight ... early morning, its master.\n""Wake me early in the morning, I must not be late, or my teacher will cane me.""\nYour kingship is good for the people. The people spend their days in abundance thanks to you.\nIn order to find sweetness in the bed on the joyous coverlet, my lady bathes her holy thighs.\nThe First Law of the Land: if any one ensnare another, then he that ensnared him shall be put to death\nAction and inaction will both destroy you in the end']"	['<urn:uuid:47230668-154a-4d82-87e6-af54a6fddea8>']	factoid	direct	concise-and-natural	similar-to-document	novice	2025-04-14T18:34:38.591736	55	127	4471
34	What are some of the reasons why young people find it easier to learn from peer educators rather than authority figures?	Young people find it easier to learn from peer educators because they share similar backgrounds and interests, such as taste in music, popular celebrities, use of language, and family themes. Peer educators are not seen as authority figures 'preaching' about behavior, but rather like receiving advice from a friend 'in the know' who understands what it's like to be young and has similar concerns.	['What is Peer Education?\nPeer education proposes education among equals and aims to encourage the active involvement and development of young people through information-sharing, debate and Interaction. Peer education is an innovative method for working with young people. The aim of this programme is to introduce a peer education model based on shared practice as an instrument for working with young people in need of support\nInvolvement in peer groups encourages the following in young people:\n_ Active participation\n_ Involvement and familiarity with the group and society\n_ Interest in social experience\n_ Independence and assertiveness\n_ improved relationship with themselves, peers, family and other areas of society\n_ Greater awareness of their own potential and limitations (strengths and weaknesses)\n_ Development of skills\nIn the context of this programme, peer education is the process whereby well-trained and motivated young people undertake informal or organised educational activities with their peers (those similar to themselves in age, background or interests) over a period of time, aimed at developing their knowledge, attitudes, beliefs and skills and enabling them to be responsible for and protect their own health.\nPeer education can take place in small groups or through individual contact and in a variety of settings: in schools and universities, clubs, churches, workplaces, on the street or wherever young people gather.\nA young person’s peer group has a great influence on the way he or she behaves. This is true of both risky and safe behaviour. Peer education makes use of peer influence in a positive way. The credibility of peer educators in the eyes of their target group is indeed an important base upon which peer education can be built.\nYoung people who have taken part in peer education initiatives often praise the fact that information is transmitted more easily because of the educator’s and the audience’s shared background and interests in areas such as taste in music and popular celebrities, use of the language, family themes (brother and sister issues, struggle for independence, etc.) and role demands (student, team member, etc.). Youth peer educators are less likely to be seen as authority figures ‘preaching’ about how others should behave from a judgemental position. Rather, the process of peer education is perceived more like receiving advice from a friend ‘in the know’, who has similar concerns and an understanding of what it’s like to be a young person.\nPeer education is also a way to empower young people: it offers them the opportunity to participate in activities that affect them and to access the information and services they need to protect their health.\nOur ‘Healthy Life Choices Programme’ works in harmony with the Walk Tall and SPHE in school, it is delivered to 6th Class National School students in school or out of school context. The teacher is involved in planning and implementing the programme.\n· Organised sessions with students in a secondary and primary school or in a Parish or Youth club setting, using interactive techniques such as quizzes, role plays or stories;\n· Trained facilitators with on-going training and support\n· Involving other services and agencies from the community\nThe Peer leaders\nThe Peer leaders are often in Transition Year in school or are Leaders in a Youth Club or teenagers from a parish.\nThey enter in to a scheduled training programme over 2-3 days or 8 Sessions\nThe training programme involves\n· The Ethos of Drug Education\n· Facilitation & Presentation Skills\n· How Groups develop\n· Self Esteem in young people\n· Values & Attitudes\n· Conflict Resolution / Problem Solving\n· Information & Facts on substance misuse\n· Legal & First aid\n· Four session Healthy Life style Programme\nThe Peers then work together in chosen groups / pairs and prepare to facilitate the 6th Class students\nThe students are then brought together in groups of 12-15 and will have a minimum of two Peer leaders and an Adult present while the programme is delivered.\nThe Programme is delivered in four sessions\n· Assertiveness – Saying No and refusal skills\n· Listening skills\n· Staying safe – mobile & Internet awareness\n· Effects & consequences\n· Legal / Illegal Drugs\n· Decision Making\n· Natural Buzz\n· Recipe for a friend\nThere is a pack supplied to all groups involved in facilitating the programme which includes:\n· Peer Trainers book\n· Facilitators Book\n· Activity / Worksheets\nWhy does it work?\nYoung people appreciate and are influenced in positive ways by a peer-led intervention if it is well-designed and properly supervised;\nServing as a peer educator provides a challenging, rewarding opportunity to young people to develop their leadership skills, gain the respect of their peers, and improve their own knowledge base and skills. Peer educators often change their own behaviour after becoming a peer educator;\nShould you require any further information on this programme please contact:\nMobile: 00 353 (0) 87 790 1461\nOffice: 00 353 (0) 1- 5053044']	['<urn:uuid:c8fdf202-3614-4bc6-af6d-897173cab7a9>']	factoid	direct	verbose-and-natural	similar-to-document	novice	2025-04-14T18:34:38.591736	120	398	5048
35	When dining in France, where should I keep my hands?	In France, you should keep your hands on the table, not in your lap, to avoid committing a faux pas at the dining table.	"[""Body language of the world: Body signs to avoid in travel...\nBody language of the world: A traveller's guide to avoiding faux pas\nSarah Bennett & Lee Slater\nAugust 10, 201110:21AM\nIn the Middle East always eat with your right hand. Eating with your left is considered unclean. Picture: Flickr user hiyori13\nIn Japan bowing is a sign of respect. Picture: Flickr user alf melin\nEVERY country has its cultural quirks. To avoid being the subject of tut tuts and disapproving looks while you're travelling, we take you through the top taboos from Japan to the Middle East.\nThe most common greeting in Japan is the bow; the timing, posture and movement of which should reflect sincerity, respect and graciousness. A beautiful bow is often compared to a ripe rice stalk swaying in the wind: the more mature the person, the deeper the head is lowered. An improper bow hints at a lack of education and maturity. As a foreign visitor you are not expected to emulate this ritual faithfully - a gentle nod will do. -Lonely Planet Japanese Phrasebook\nPersonal space boundaries vary from country to country, but in Latin America they are set closer than in Anglo-Saxon countries. People stand closer when talking to one another, and casual touching of the arm or shoulder during conversation is not unusual. Good friends will typically greet each other with an abrazo (hug) or beso (kiss), and it’s quite normal to see people of the same sex walking down the street arm in arm.\n-Lonely Planet Latin American Spanish Phrasebook\nIn Mexico, when paying for something, place your cash or credit card directly into the hand of the person you’re dealing with. This applies in cafes and restaurants, as well as hotels and shops. Leaving payment on the counter can be interpreted as a sign that you don’t respect the person enough to have contact with them. -Lonely Planet Mexican Spanish Phrasebook\nThe traditional greeting of New Zealand Maori is the simultaneous pressing of noses and forehead, known as the hongi. The word directly translates as smell or sniff, but is more evocatively described as an exchange of ha, the breath of life. Such greetings are commonplace on marae, the open space in front of a Maori meetinghouse where visitors are welcomed, nowadays with a handshake at the same time as the hongi. -Hirini Moko Mead, Tikanga Maori (Huia, 2003) & Lonely Planet South Pacific Phrasebook\nThe hongi is a traditional New Zealand Maori greeting. Picture: Lonely Planet\nWhen ‘no’ gets mistaken for ‘yes’ there can be all sorts of trouble, but you can always rely on a shake of the head or a nod, can’t you? Not in Bulgaria, where the nod means no and the shake means yes. To complicate matters further, polite Bulgarians will often try to compensate by reversing their normal habit. For absolute verification, familiarise yourself with the words da (yes) and ne (no). -Lonely Planet Bulgarian Phrasebook\nBefore you whip the big thumbs up out of your pocket to convey that ‘all is well’ or ‘I’d like to hitch a ride’, be warned that you may be about to make a boo-boo. In some parts of the Middle East you may as well be flipping the bird, and the gesture has a similarly negative meaning in Nigeria and parts of South America. In Germany, you will be indicating the numeral one, which will be fine unless you really want to order two.\nMeanwhile, eating with your hand is acceptable in the Middle East, as long as it’s the right one! The left hand is considered unclean. Practicing this custom is particularly important at communal dinners, where many hands may come into contact with shared food, but it’s also important when shaking hands or giving and receiving gifts. -Roger Axtel, Gestures: The do’s and taboos of body language around the world\nJust don't raise your thumb in the Middle East. Picture: Lonely Planet\nAs you would expect in a country that came up with the word etiquette, it is easy for foreigners to commit faux pas in France. At the dining table the odds increase dramatically. To avoid offence, keep your hands on the table, not in your lap. Break up your bread roll into nibbles, rather than shoving the whole thing in your gob, but beyond that try to avoid eating anything with your fingers. Peel fruit with a knife and eat it with a fork; avoid man-handling sandwiches if you can. -Lonely Planet French Phrasebook\nIn India - as in much of Asia - it is the feet that are considered unclean. Do your utmost to avoid touching any part of someone else’s body with your foot or shoes, and if you do so, apologise straight away. Pointing the soles of your feet at someone is also offensive, so don’t prop your feet on chairs or tables while sitting, and take care how you arrange yourself when sitting on the floor. -Lonely Planet Hindi, Urdu & Bengali Phrasebook\nThe ultimate beau geste (gracious gesture) is one that is used in every country on earth. Although in some cultures in certain circumstances it can have negative connotations, it is seldom misunderstood and can be used in many situations. It conveys an array of positive emotions, and as such is the great bridge builder between peoples of the world. It involves only the eyes and the mouth, and so requires minimal effort. It is particularly useful in sticky situations. It is so powerful it is thought to release endorphins into the body that generate a feeling of euphoria. It is, of course, the smile.""]"	['<urn:uuid:43448c08-edca-41cd-af2f-7d6b11e809df>']	factoid	with-premise	concise-and-natural	distant-from-document	expert	2025-04-14T18:34:38.591736	52	120	5378
36	what cities has richard prince art been shown	Richard Prince's art has been exhibited in many major cities including New York (Whitney Museum, Metropolitan Museum, Museum of Modern Art), San Francisco (Museum of Modern Art), Rotterdam (Museum Boymans-Van Beuningen), Munich (Haus der Kunst), Basel (Museum für Gegenwartskunst), Oslo (Astrup Fearnley Museum), Paris (Bibliothèque nationale de France), and London (Serpentine Gallery, Victoria and Albert Museum), among others.	['Mining images from mass media, advertising and entertainment since the late 1970s, Richard Prince has redefined the concepts of authorship, ownership, and aura. Applying his understanding of the complex transactions of representation to the making of art, he evolved a unique signature filled with echoes of other signatures yet that is unquestionably his own. An avid collector and perceptive chronicler of American subcultures and vernaculars and their role in the construction of American identity, he has probed the depths of racism, sexism, and psychosis in mainstream humor; the mythical status of cowboys, bikers, customized cars, and celebrities; and most recently, the push–pull allure of pulp fiction and soft porn, producing such unlikely icons as the highly coveted Nurse paintings.\nRichard Prince was born in 1949 in the Panama Canal Zone. Prince’s work has been the subject of major solo exhibitions, including the Whitney Museum of American Art, New York (1992); San Francisco Museum of Modern Art, California (1993); “Fotos, Schilderijen, Objecten,” Museum Boymans–Van Beuningen, Rotterdam (1993); Haus der Kunst / Süddeutsche Zeitung, Munich (1996); Museum Haus Lange / Museum Haus Esters, Germany (1997); “4x4,” MAK Center for Art and Architecture, Vienna (2000); “Upstate,” MAK Center for Art and Architecture, Schindler House, Los Angeles (2000); Museum für Gegenwartskunst, Basel (2001, traveled to Kunsthalle Zurich, Switzerland; and Kunstmuseum Wolfsburg, Germany); “American Dream, Collecting Richard Prince for 27 Years,” Rubell Family Collection, Miami (2004); “Canaries in the Coal Mine,” Astrup Fearnley Museum, Oslo (2006); “The Early Works,” Neuberger Museum of Art, New York (2007); Solomon R. Guggenheim Museum, New York (2007, traveled to Walker Art Center, Minneapolis; and Serpentine Gallery, London, through 2008); “American Prayer,” Bibliothèque nationale de France, Paris (2011); “Prince/Picasso,” Picasso Museum, Spain (2012); and “It’s a Free Concert,” Kunsthaus Bregenz, Austria (2014). Prince’s works are in the public collections of the Metropolitan Museum of Art, New York; Modern Art Museum of Fort Worth, Texas; Museum of Fine Arts Collection, Boston; Museum of Modern Art, New York; and the Victoria and Albert Museum, London.\nPrince currently lives and works in New York.\nPhoto: Gordon M. Grant/The New York Times/Redux\nExtended through December 19, 2018\nNovember 1–December 19, 2018\nWest 21st Street, New York\nPicture Books: Percival Everett and Brandon Taylor\nThe second installment of Picture Books, an imprint organized by Emma Cline and Gagosian, presents author Percival Everett’s novella Grand Canyon, Inc. alongside Untitled (Original Cowboy), a photograph by Richard Prince. In celebration of the publication, Everett met with author Brandon Taylor to discuss the novella, the role of history in the writing process, and the similarity in methodologies for science and literature.\nRichard Prince: Cowboy\nOn the occasion of the publication of Richard Prince: Cowboy, a major monograph on the artist’s preoccupation with the mythic American West, Lucy Sante tracks the archetype through mass media, advertising, and the art of Richard Prince to illuminate the cowboy’s enduring appeal.\nGagosian Quarterly Summer 2020\nThe Summer 2020 issue of Gagosian Quarterly is now available, featuring Joan Jonas’s Mirror Piece 1 (1969) on its cover.\nThe Right Time\nNatasha Stagg on influencers, the loss of the it-girl, and the “promotional life.”\nGagosian Quarterly Spring 2020\nThe Spring 2020 issue of Gagosian Quarterly is now available, featuring Cindy Sherman’s Untitled #412 (2003) on its cover.\nCast of Characters\nJames Lawrence explores how contemporary artists have grappled with the subject of the library.\nVisions of the Self: Jenny Saville on Rembrandt\nJenny Saville reveals the process behind her new self-portrait, painted in response to Rembrandt’s masterpiece Self-Portrait with Two Circles.\nText by Richard Hell.\nFairs, Events & Announcements\nTEFAF New York 2023\nRichard Prince, Jeff Koons, Peter Halley\nMay 12–16, 2023, booth 350\nPark Avenue Armory, New York\nGagosian is pleased to announce its participation in TEFAF New York 2023, with a special presentation of works by Richard Prince, Jeff Koons, and Peter Halley. Made between 1989 and 1994, they employ strategies of appropriation, figuration, and abstraction, responding with wry humor to the eclectic postmodernism and moral panics of a culturally volatile era. Three decades later, these works remain provocative and represent a pivotal development in the career of each artist.\nGagosian’s booth at TEFAF New York 2023. Artwork, left to right: © Jeff Koons, © Peter Halley. Photo: Sebastiano Pellion di Persano\nArt Basel Miami Beach 2022\nDecember 1–3, 2022, booth D5\nMiami Beach Convention Center\nGagosian is pleased to present a selection of modern and contemporary works at Art Basel Miami Beach 2022. Returning to Miami for the fair’s twentieth anniversary, the gallery is honored to have participated each year the fair has been held.\nGagosian’s booth at Art Basel Miami Beach 2022. Artwork, left to right: © Gerhard Richter; © Amoako Boafo; © Richard Prince; © 2022 Judd Foundation/Artists Rights Society (ARS), New York; © Richard Diebenkorn Foundation; © The Andy Warhol Foundation for the Visual Arts, Inc./Artists Rights Society (ARS), New York; © Stanley Whitney. Photo: Sebastiano Pellion di Persano\nWest Bund Art & Design 2022\nNovember 11–13, 2022, booth A102\nWest Bund Art Center, Shanghai\nGagosian is pleased to participate in the ninth edition of West Bund Art & Design. The gallery will present new works made for the fair by Georg Baselitz, Roe Ethridge, Thomas Houseago, Alex Israel, Harmony Korine, Adam McEwen, Jim Shaw, Alexandria Smith, Spencer Sweeney, and Tatiana Trouvé, alongside works by Ashley Bickerton, Urs Fischer, Katharina Grosse, Damien Hirst, Takashi Murakami, Nam June Paik, Richard Prince, Ugo Rondinone, Ed Ruscha, Richard Wright, and Zeng Fanzhi.\nGagosian’s booth at West Bund Art & Design 2022. Artwork, left to right: © Adam McEwen, © Roe Ethridge, © Alex Israel, © Harmony Korine. Photo: JJYPHOTO\nNovember 17, 2022–May 7, 2023\nLouisiana Museum of Modern Art, Humlebaek, Denmark\nRichard Prince is a perceptive chronicler of American subcultures and vernaculars and their role in the construction of American identity. In his paintings, he probes the depths of racism, sexism, and psychosis in mainstream humor; the mythical status of cowboys, bikers, customized cars, and celebrities; and the push-pull allure of pulp fiction and soft porn, producing such unlikely icons as the Nurse paintings. Prince manages to identify and sample visual codes and finely tune them so that they become seductive and strange despite their banality. This exhibition—part of the Louisiana on Paper series, which focuses on drawings—also includes a number of important paintings, photographs, and sculptures encompassing many different bodies of work from throughout Prince’s career.\nRichard Prince, Untitled (Cowboy), 2016 © Richard Prince Studio\nArtists Inspired by Music\nJanuary 30–February 13, 2022\nLos Angeles County Museum of Art\nTo mark the thirtieth anniversary of Interscope Records, the company invited artists to select albums and songs from Interscope’s groundbreaking catalogue and fostered exchanges between artists and musicians to generate resonant pairings. The exhibition, which includes more than fifty works, brings an intergenerational group of visual artists into dialogue with iconic musicians from the last three decades, providing a fresh perspective on influential music for the present moment. Work by John Currin, Jennifer Guidi, Damien Hirst, Titus Kaphar, Takashi Murakami, Richard Prince, Ed Ruscha, and Anna Weyant is included.\nJennifer Guidi, Seeking Hearts (Black MT, Pink Sand, Pink CS, Pink Ground), 2021 © Jennifer Guidi. Photo: Brica Wilcox\nArt of the Eighties\nOctober 17, 2021–February 13, 2022\nAlbertina Modern, Vienna\nSome consider the 1980s to be the most important decade for the art of our age. For the first time, art was no longer determined by a dominant style, such as abstraction or Pop, but rather embodied an unprecedented stylistic pluralism that was a hallmark of postmodernism. This exhibition, curated by Albertina Modern director Angela Stief, examines the variety of artistic approaches and strategies that defined the era. Work by Jean-Michel Basquiat, Albert Oehlen, Richard Prince, Andy Warhol, and Franz West is included.\nInstallation view, The 80s: Art of the Eighties, Albertina Modern, Vienna, October 17, 2021–February 13, 2022. Artwork, left to right: © Jiří Georg Dokoupil, © Hubert Schmalix, © Albert Oehlen. Photo: © Ana Paula Franco/Albertina, Wien 2021\nA Labour of Love\nJuly 1–October 17, 2021\nTarmak22, Gstaad Saanen Airport, Switzerland\nDécoupage presents more than 150 cut-paper works made in the nineteenth and twentieth century by Swiss masters. Tracing the origins and development of the paper cutout, the exhibition honors the craft’s history and its Swiss heritage while creating a dialogue with a selection of contemporary artworks brought to Gstaad by collaborators including Gagosian. Work by Balthus, Richard Prince, and Setsuko is included.\nBalthus, Paysage de Monte Calvello, 1978 © Balthus']	['<urn:uuid:65a14755-a829-4f9a-a2b5-6a916ec3c2c5>']	open-ended	with-premise	short-search-query	similar-to-document	novice	2025-04-14T18:34:38.591736	45	429	9317
37	vault sculpture melbourne yellow peril location	The public sculpture Vault (known informally as The Yellow Peril) is currently located outside ACCA. It was originally placed in the City Square but was later removed due to public outcry over its modern style and perceived lack of harmony with surrounding architecture.	"['Just wandering the streets of the city will bring you in contact with numbers of public sculptures, murals and other art works.\nMelbourne was the name given to the European settlement commenced by Batman and Fawkner in 1835. Aboriginal people lived in the area tens of thousands of years before the arrival of Europeans and had a complex culture and art. We leave commentary on Aboriginal art in this period to those better placed to do so. However we thoroughly recommended a visit to the Aboriginal Gallery at the National Gallery (Australian Collection) to all visitors to Melbourne.\nThere are many parts of our built environment that display great craft and are artistically pleasing but are not necessarily labelled as \'art\'. On the other hand there are many objects which show little craft or depth of thinking but are funded by an arts budget or are labelled by their creators as \'art\'. (It is worth noting that anything that has to label itself xxxx Art such as Mural Art or Graffiti Art is already indicating that it has not generally been recognised as art and therefore using xxxx Art as a marketing label.) Some art destroys the art that went before it (see Melbourne\'s Disappearing Hidden Gems from our Melbourne newsletter No.151). So what constitutes public art?\nSince long before the existence of Melbourne the question ""what is art?"" has sparked fierce debate. At White Hat we don\'t claim to have a definitive answer and will leave it for you to decide what belongs under the label Public Art.\nIn Melbourne there are public memorials, often in the form of statues, to people directly associated with Melbourne such as Governor Charles La Trobe, Tommy Bent, Justice Higinbotham, Adam Lindsay Gordon, Burke & Wills, , Sir Rupert Hamer, John Cain, Sir Henry Bolte, Weary Dunlop, Sir Redmond Barry, Francis Ormond, Sidney Myer, John Batman, John Fawkner. George Marshall-Hall and many others. Photos of these monuments can be found at many of the pages linked above.\nIn addition, reflecting Victoria\'s roots as a British Colony and the fact that the monarch of Britain is still Australia\'s head of state, there are various monuments and memorials to monarchs and \'heroes of the empire\' who never visited Melbourne. These include Queen Victoria, King Edward VII, Governor Gordon of Khartoum and Nurse Cavell.\nThere are also memorials and monuments to people of international significance including John F Kennedy and Raoul Wallenberg.\nSome monuments have an important part in Melbourne history but often go un-noticed. You can read some of these at The White Hat Guide to 7 Monuments of Melbourne.\nBecause Melbourne is a very young city and much of it was created by private rather than public initiative, there is little surprise that many of the public memorials are a result of individuals or the wider public contributing funds to create monuments to ensure that the life and work of a particular individual should not be forgotten. There is thus a certain irony in the current fashion for publicly funded conceptual art to denigrate the people who wished these achievements to be remembered and, similarly in the number of these monuments which have become covered with \'unofficial street art\' in the form of graffiti.\nOther public monuments commemorate events or heritage associated with the site. Like all Australian cities, Melbourne has numbers of memorials to the First and Second World Wars, many of them in the precinct of the Shrine of Remembrance. Some other memorials include those to Victoria\'s women pioneers and the Old Melbourne Cemetery.\nNot all public memorials are good art - that is often not their purpose and the public artist has to satisfy almost as many conflicting requirements as artists working under patronage in the Renaissance courts. The monument needs to reflect the attitudes and mores of the time and the funders. Most of Melbourne\'s nineteenth century monuments were the result of private benefactors and public subscriptions and the artist would usually be expected to submit and modify designs for the people who were paying for the monument. By the earlier part of the the twentieth century some monuments were funded by governments using public money and the artist would usually be expected to create a monument that was acceptable to the public at large and/or the government of the day. In more recent times the artist may not have to please the government or the public but to please the bodies and committees that dispense public monies for such art.\nIn addition the artist needs to be aware of the language and iconography of western art -who should appear on horseback and in what position and so on. A walk through the Domain presents number of examples of public art making use of this language and symbolism. Even if the artist chooses to thumb his/her nose at such artistic language they need to know what impact that will have on the informed viewer.\nWhen you observe the public memorials in central Melbourne, there were only two until the 1920s that were for a women (and both of them English). Later a small bust of Nellie Melba appeared, and now (reflecting current values) the only significant new additions have been of sportswomen such as Betty Cuthbert. Even though we have listed numbers of women associated with Melbourne in our 200 Significant Australians (such as Caroline Chisholm, Louise Hanson-Dyer and Mary MacKillop) you won\'t find a major memorial to any of them in the centre of Melbourne.\nThe lack of Aboriginal people represented in such memorials is not surprising as many Aboriginal cultures forbid the representation of the deceased. However there is a representation of Bungaree.\nRonald Ridley\'s book Melbourne Monuments is the best available guide to public monuments in Melbourne. It is well researched, well written with numbers of insights into Melbourne\'s history, and provides a suggested self-guided walking tour of these monuments. Those interested in public monuments and their history can always take a Melbourne by Lamplight tour which examines a number of monuments in the vicinity of Parliament House and the Treasury Building as well as the stories behind them. For a different style of monument you can take a guided walking tour of Melbourne General Cemetery.\nThe public sculpture Vault (known unkindly by many in Melbourne as The Yellow Peril) can be found outside ACCA. This sculpture caused an outcry when first placed in the City Square - firstly because it was modern and confronting to Middle Melbourne and secondly because it didn\'t seem in sympathy with the surrounding architecture. It was subsequently banished from the centre of the city and publicly funded art in Melbourne has since been restricted much more to the whimsical or smaller scale politically correct decoration or temporary installations. Laudable though these projects may be in supporting artists still learning their craft, the better public art in recent time has usually come from commissions from corporations, larger organisations and public benefactors who can be more concerned with artistic merit and less with being re-elected or satisfying the agendas of funding bodies.\nThe ones which seem to best characterise Melbourne today have a sense of whimsy. The Three Businessmen Who Brought Their Own Lunch, Larry Latrobe, weather vanes, The Public Purse, Brunswick street street signs.\nStatuary outside State Library\nSome public art in Melbourne was not specifically designed for the place in which it stands. Stand alone sculptures were often purchased by the city burgers or donated by public spirited citizens and erected outside public buildings or in our parks. Good examples can be found outside the State Library (dating from the time it was also our art gallery) and in our city parks and gardens.\nMuch public art in Melbourne has been commissioned, created or chosen to form part of the urban landscape and enhance the experience of living, working or passing through a particular precinct.\nWhen the Docklands area was redeveloped starting in the 1990s, developers were required to set aside a portion of their budget for public art. As a result this area has some of the greatest concentration of public art works in Melbourne. Outside the Telstra Dome are the works Threaded Field and Art Wall, while close by is John Kelly\'s Cow up a Tree with its oblique reference to William Dobell. The shiny white sculptures at Newquay are Silence by Adrian Mauriks.\nMany buildings choose to enhance their public spaces - forecourts, atriums, etc - with sculpture and other art.\nGood architecture is of course a form of public art and has been referred to as ""inhabited sculpture"". However a number of Melbourne buildings, particularly those dating from the nineteenth century, incorporate sculpture into their facades and architectural decorations. Good (and bad) examples can be found in much of Collins Street as well as on our two major cathedrals. (One has a gargoyle from the 1990s with its mouth wide open which purportedly represents a premier and art minister of that time). You can find more recent examples in the large bronze figures that Nonda Katsalidis has integrated into the base of his city apartment buildings and in mobile sculptures.\nPerhaps the most common form of art incorporated into the structure of a building is the mural. At best they complement the structure and at their worst they seem \'stuck on\'. There is a large mural at Eastern Hill Fire Station. You will find a Napier Waller fresco above the door at the T&G Building (now KPMG House). You can find a characteristic Mirka Mora mural at the Princes Bridge end of Flinders Street Station in Swanston Street. as well as in the restaurant Tollarno in St Kilda. For more information on Murals see The White Hat Guide to 7 Murals of Melbourne.\nThe modern Republic Tower (designed by Nonda Katsalidis) on the corner of Queen & Latrobe streets is unusual in that incorporates a large public art display area into the facade of the building. The artworks change four times a year.\nMuch public art in Melbourne is in buildings which may be only open to the public at certain times and under certain conditions. Public buildings such as Parliament House may have restricted access depending on their daily functions. Theatres and sporting arenas may only be open to paying customers. Licensed premises such as Young & Jackson\'s (the home of Chloe) are likely to have age and dress requirements. Commercial buildings are unlikely to welcome tour groups that disrupt their daily business. Certain clubs may only be open to guests of members and certain corporate and government areas may require an appropriate dress and behaviour code and introduction from a trusted client. Thus there is quite a continuum of what may or may not be considered as \'public\'.\nThe mural Symmetry of Sport and Italian glass mosaic tiles both designed by Leonard French can be seen in the trophy hall of Beaurepaire Centre at Melbourne University.\nJeffrey Smart\'s Container Train in Landscape was commissioned in 1983 to hang in the foyer of the Fairfax Theatre of the Melbourne Arts Centre. The painting comes into view at the top of the entrance stairs, and by cunning use of forced perspective the train appears to remain at the same distance as the viewer descends the stairs. It is painted on five panels. As a consummate craftsman, Smart integrates the painting with the colours and columns of the foyer, and as a consummate artist uses numbers of visual devices to extract beauty form a seemingly unpromising subject.\nThose who know the painting well may notice that the above image (from the website of the organisation selling the print) is in fact a mirror image of the original. Never fear - if you buy the print it is the right way round.\nEureka Stockade by Sidney Nolan\nThis large mural (20m by 3.6 m) is made up of 66 panels of jewellery enamel on copper. The mural is in the form of a ""line drawing"" which Nolan applied to the enamel with finger and thumb before the copper sheets were then fired. The subject is the Eureka Stockade civil uprising on the Ballarat goldfields in 1854 which forms an important part of Australia\'s history. The work was created in collaboration with the enamellers Patrick Furse and Robin Banks. Even though the foyer where it is located is usually closed at night, the mural is illuminated and clearly visible through the windows during the evening.\nLocation: Ground Floor Foyer, Reserve Bank of Australia, 60 Collins Street, Melbourne\nIn recent years, government bodies such as the City of Melbourne have funded numbers of temporary installations - usually in the style of Conceptual Art. Below are some current examples:\nsee also a recent encounter\nIf none of the above is to your taste you can search out the council’s art in the laneways projects. For instance in Hosiers Lane and off Centre Place you are likely to find examples of stencil art and cultural jamming. Cultural jamming is the technique of using images or icons from a given culture but changing the message conveyed. Much of what is being ‘jammed’ in the alleys is commercial and popular culture, but as none of us from White Hat has ever decided to immerse ourselves in commercial culture (we would be useless in a trivia quiz that required us to know advertising jingles) the messages are largely lost on us.\n- Various statues in The Domain and connected gardens show people on horseback. How can you tell the status of a person from a 19th century equestrian statue?\n|To find the answers to questions like these ask a White Hat Accredited Guide or subscribe to our free newsletter where similar questions and answers are provided each week. For a sample of previous questions see The White Hat Quiz.|\nMr Felton\'s Bequest']"	['<urn:uuid:83bd1a99-eeee-4bd8-9f7a-dfc1c6e59074>']	factoid	with-premise	short-search-query	similar-to-document	expert	2025-04-14T18:34:38.591736	47	270	13782
38	hi what is the highest temperature rare earth minerals studied	Studies of rare earth elements include experiments up to 150°C, as shown in research on rare earth element phosphates solubility from 23 to 150°C.	['Theoretical and experimental studies of the thermodynamics, kinetics and molecular mechanisms of aqueous processes and water-rock interactions, especially at elevated temperatures and pressures. Particular emphasis on metals and their speciation and transport. Solubility, calorimetric, spectroscopic, electrochemical, phase equilibrium and other experimental studies. Field-based studies of ore deposits, geothermal systems and the environment. Equilibrium and reaction-path modeling of geochemical processes. Applications to ore deposits, hydrometallurgy, exploration geochemistry, environmental geochemistry, radioactive waste disposal and medical geology. Particular interest in rare earth (REE) and platinum group elements (PGE).\nRecent Publications (*former student; #former post-doc)\nArmstrong*, C.R. and Wood, S.A. (2012) Effect of fulvic acid on neodymium uptake by goethite: Jour. Colloid and Interfacial Sci. (DOI: 10.1016/j.jcis.2012.07.060).\nTaunton*, A.E., Gunter, M.E., Druschel, G.K., and Wood, S.A. (2010) Geochemistry in the lung: Reaction-path modeling and experimental examination of rock-forming minerals under physiologic conditions: Amer. Mineral. 95, 1624-1635.\nKruszewski*, J.M. and Wood, S.A. (2009) Experimental measurement of the solubility of bismuth phases in water vapor from 220 ºC to 300 ºC: Implications for ore formation. Appl. Geochem. 24, 493-503.\nParker, S.R., Gammons, C.H., Pedrozo, F.L., and Wood, S.A. (2008) Diel changes in metal concentrations in a geogenically acidic river: Rio Agrio, Argentina. Jour. Volcanol. Geotherm. Res. 178, 213-223 (doi: 0.1016/j.jvolgeores.2008.06.029).\nMigdisov, Art.A., Williams-Jones, A.E., Normand#, C., and Wood, S.A. (2008) A spectrophotometric study of samarium (III) speciation in chloride solutions at elevated temperatures. Geochim. Cosmochim. Acta 72, 1611-1625.\nConnon, S., Koski*, A., Neal, A., Wood, S., and Magnuson, T. (2008) Ecophysiology and geochemistry of microbial arsenic oxidation within a high arsenic, circumneutral hot spring system of the Alvord Desert. FEMS Microbiol. Ecol. 64, 117-128.\nWood, S.A. and Norman#, C. (2008) Mobility of palladium chloride complexes in mafic rocks: insights from a flow-through experiment at 25 °C using air-saturated, acidic, and Cl-rich solutions. Mineral. Petrol. 92, 81-97 (Published electronically on July 13, 2007; DOI 10.1007/s00710-007-0193-5).\nRedkin#, A.F. and Wood, S.A. (2007) Uranium(VI) in aqueous solutions at 25°C and a pressure of 1 bar: Insight from experiments and calculations. Geochem. Int. 45, 1111-1123.\nWood, S.A., Taunton*, A.E., Normand#, C., and Gunter, M.E. (2006) Fluid-mineral interaction in the lungs: Insights from reaction-path modeling. Inhal. Toxicol. 18, 975-984.\nXiong*, Y., Wood, S.A., and Kruszewski*, J. (2006) Hydrothermal transport and deposition of rhenium under subcritical conditions revisited. Econ. Geol. 101, 471-478.\nWood, S.A. (2006) The behavior of rare earth elements in naturally and anthropogenically acidified waters. Jour. Alloys Comps. 418, 161-165.\nWood, S.A. (2006) Rare earth element systematics of acidic geothermal waters from the Taupo Volcanic Zone, New Zealand. Jour. Geochem. Explor. 89, 424-427.\nWood, S.A. and Samson, I.M. (2006) The aqueous geochemistry of gallium, germanium, indium and scandium. Ore Geol. Rev. 28, 57-102.\nGammons, C.H., Wood, S.A., Pedrozo, F., Varekamp, J.C., Nelson, B.J., Shope, C.L., and Baffico, G. (2005) Hydrogeochemistry and rare earth element behavior in a volcanically acidified watershed in Patagonia, Argentina. Chem. Geol. 222, 249-267.\nGammons, C.H., Wood, S.A., and Nimick, D.A. (2005) Diel behavior of rare earth elements in a mountain stream with acidic to neutral pH. Geochim. Cosmochim. Acta 69, 3747-3758.\nCetiner*, Z.S., Wood, S.A., and Gammons, C.H. (2005) The aqueous geochemistry of the rare earth elements. Part XIV. The solubility of rare earth element phosphates from 23 to 150 ºC. Chem. Geol. 217, 147-169.\nSamson, I.M., Wood, S.A., and Finucane*, K.G. (2004) Fluid inclusion characteristics and genesis of the fluorite-parisite mineralization in the Snowbird Deposit, Montana. Econ. Geol. 99, 1727-1744.\nNelson*, B.J., Wood, S.A. and Osiensky, J.L. (2004) Rare earth element geochemistry of groundwater in the Palouse Basin, Northern Idaho–Eastern Washington. Geochem. Explor. Environ. Anal. 4, 227-242.\nWood, S.A. and van Middlesworth*, J. (2004) The influence of acetate and oxalate as simple organic ligands on the behavior of palladium in surface environments. Can. Min. 42, 411-421.\nNelson*, B.J., Wood, S.A. and Osiensky, J.L. (2003) Partitioning of REE between solution and particulate matter in natural waters: a filtration study. Jour. Solid State Chem. 171, 51-56.\nWood, S.A. and Shannon, W.M. (2003) Rare-earth elements in geothermal waters from Oregon, Nevada, and California. Jour. Solid State Chem. 171, 246-253.\nGammons, C.H., Wood, S.A., Jonas, J.P., and Madison, J.P. (2003) Geochemistry of the rare-earth elements and uranium in the acidic Berkeley Pit lake, Butte, Montana. Chem. Geol. 198, 269-288.\nNicholson#, K.N. and Wood, S.A. (2002) Aqueous geochemistry of rare earth elements and yttrium. XII: Potentiometric stability constant determination of Bis-Tris complexes with La, Nd, Eu, Gd, Yb, Dy, Er, Lu, and Y. Jour. Sol. Chem. 31, 703-717.\nWood, S.A., Tait, C.D., and Janecky, D.R. (2002) A Raman spectroscopic study of arsenite and thioarsenite species in aqueous solution at 25 °C. Geochem. Trans. 3(4), 31-39.\nXiong*, Y. and Wood, S.A. (2002) Experimental determination of the hydrothermal solubility of ReS2 and the Re–ReO2ffer assemblage and transport of rhenium under supercritical conditions. Geochem. Trans. 3(1), 1-10.\nXiong*, Y. and Wood, S.A. (2001) Hydrothermal transport and deposition of rhenium under subcritical conditions (up to 200°C) in light of experimental studies. Econ. Geol. 96, 1429-1444.\nNicholson#, K.N., Twamley, B. and Wood, S.A. (2001) [Bis(2-hydroxyethyl)amino]-tris(hydroxymethyl)methane (Bis–Tris), an important complexing agent. Acta Cryst. E57, o1133-o1135.\nPalmer, D.A., Bénézeth, P., Wesolowski, D.J., Wood, S.A., and Xiao, C. (2000) The solubility of metal oxides and hydroxides at high temperatures: Results and implications of recent ORNL measurements. Power Plant Chem. 2, 517-521.\nWood, S.A. and Ricketts*, A. (2000) Allanite-(Ce) from the Eocene Casto granite, Idaho: response to hydrothermal alteration. Canadian Mineralogist 38, 81-100.\nWood, S.A., Wesolowski, D.J. and Palmer, D.A. (2000) The aqueous geochemistry of the rare earth elements. IX. A potentiometric study of Nd3+ complexation with acetate in 0.1 molal NaCl solution from 25°C to 225°C. Chemical Geology 167, 231-253.\nGammons, C.H. and Wood, S.A. (2000) The aqueous geochemistry of REE: Part 8. Solubility of ytterbium oxalate and the stability of Yb(III)-oxalate complexes in water at 25°C to 80°C. Chemical Geology 166, 103-124.\nXiong*, Y. and Wood, S.A. (2000) Experimental quantification of hydrothermal solubility of platinum-group elements with special reference to porphyry copper environments. Mineralogy and Petrology 68, 1-28.\nWood, S.A. and Samson, I.M. (2000) The hydrothermal geochemistry of tungsten in granitoid environments: I. Relative solubilities of ferberite and scheelite as a function of T, P, pH and mNaCl. Econ. Geol. 95, 143-182.\nKulik, D.A., Aja#, S.U., Sinitsyn, V.A., and Wood, S.A. (2000) Acid/base surface chemistry and sorption of some lanthanides on K+ - saturated Marblehead illite: II. A multisite-surface complexation modeling. Geochim. Cosmochim. Acta 64, 195-213.\nSinitsyn, V.A., Aja#, S.U., Kulik, D.A., and Wood, S.A. (2000) Acid/base surface chemistry and sorption of some lanthanides on K+ - saturated Marblehead illite: I. Results of an experimental investigation. Geochim. Cosmochim. Acta 64, 185-194.\nBaker#, L.L., Agenbroad*, D.J. and Wood, S.A. (2000) Experimental hydrothermal alteration of a martian analog basalt: Implications for martian meteorites. Meteoritics & Planetary Science 35, 31-38.']	['<urn:uuid:c2375337-b622-43ec-9f26-d06db9bb0637>']	factoid	with-premise	short-search-query	distant-from-document	novice	2025-04-14T18:34:38.591736	62	146	8023
39	research scientist seeking tb sputum test info	In sputum analysis for TB diagnosis, samples are examined in laboratories where bacteriologists look for acid-fast bacilli using special stains and attempt to grow bacteria from the sample. The time needed to grow M. tuberculosis increases as the proportion of lipid body-positive acid-fast bacilli in the sputum increases.	"['They have identified for the first time that the TB bug lays down body fat that may help it survive passing from one person to another and, in the process, the bacteria increase their resistance to the action of anti-TB drugs.\nThis finding challenges the established view that the TB bacteria coughed up in sputum by infected individuals are rapidly multiplying.\nLead investigator Professor Mike Barer, Professor of Clinical Microbiology in the Department of Infection, Immunity and Inflammation at the University of Leicester said: “Strenuous efforts are being made to reduce the global burden of tuberculosis, a disease which kills four people every minute. Our success so far has been limited for many reasons; one of these is our failure to control the spread of TB from one person to another. Very little is known about this vital part of the bacterium’s life cycle.\n“If scientists could understand more about the transmission of TB between people, they might identify new therapeutic and preventative targets.”\nThe Leicester team discovered that, unlike TB bacteria growing in test tubes, many of the bugs in sputum are loaded with fat droplets. They went on to show with their colleagues in London that these ‘fat bacilli’ were in an inert non-growing state, a condition in which they are more likely to survive the process of passing from one person to another.\nThe findings are published today (April 1st) in the freely available Journal, Public Library of Science Medicine (www.plosmedicine.org). Additional funding for the study was provided by the British Lung Foundation and the Henry Smith Charity.\nThe discovery sheds light on the story of “persister bacteria” in TB - a mysterious population believed by many to be the reason why TB patients have to be treated for at least six months.\nProfessor Barer said: “These surprising findings have opened the door for us to develop new ways to stop TB from spreading and to treat it more effectively. We hope that our new ability to monitor these sleepy and resistant bacteria in sputum will enable us to treat the disease more quickly.\n“This work has taken more than ten years to come to fruition and has taken dedicated work from the teams in Leicester and London. I am particularly delighted for my team in Leicester who fought long and hard to bring this story together.”\nProfessor Philip Butcher and his team at St George’s, University of London have exploited the genome sequence information of the TB bacteria generated by the Pathogen Sequencing Unit at the Wellcome Trust Sanger Institute. They studied all the genes that are expressed by the bacteria in sputum having being coughed up from the lungs of TB patients using microarrays or gene-chips, made available through the Wellcome Trust funded Bacterial Microarray Group at St George’s (B?G@S; http://bugs.sgul.ac.uk). Importantly the St George’s team have developed a novel way to study the small numbers of bacteria present in sputum and this discovery will open the way to investigate why bacteria in TB lungs are so hard to kill with antibiotics. “This work forms the foundation to develop a new drug that works effectively against these fat and lazy bacteria” said Professor Butcher.\nProfessor Barer added: “In the University of Leicester study we examined TB in sputum samples from infected patients to get a snapshot of the disease at the point of its transmission to a new person and ask how the characteristics of these bacilli compare with those of TB growing in the laboratory.”\nThe researchers found the presence of a fat deposits and related gene expression patterns which may help the TB bacterium to survive during transmission and establish a new infection.\nPlease find below an Editors’ Summary taken from the Journal Public Library of Science MedicineBackground.\nDiagnostic tests include chest X-rays, the tuberculin skin test, and sputum analysis. For the last of these tests, a sample of sputum (mucus and other matter brought up from the lungs by coughing) is collected and then taken to a laboratory where bacteriologists look for M. tuberculosis using special stains—tuberculosis-positive sputum contains ‘‘acid-fast bacilli’’—and also try to grow bacteria from the sample.\nTuberculosis can be cured by taking several powerful antibiotics for several months. It is very important that this treatment is completed to ensure that all the M. tuberculosis bacteria in the body are killed and to prevent the emergence of drug-resistant bacteria.Why Was This Study Done?\nNext, the researchers showed that M. tuberculosis grown in the laboratory under hypoxic conditions, which induce the bacteria to enter an antibiotic tolerant condition called a ‘‘nonreplicating persistent’’ (NRP) state, also accumulated lipid bodies. This result suggests that the lipid body– positive acid-fast bacilli in sputum might be in an NRP state. To test this idea, the researchers compared the pattern of mRNAs (the templates from which proteins are produced; the pattern of mRNAs is called the transcriptome and gives an idea of which proteins a cell is making under given conditions) made by growing cultures of M. tuberculosis, by M. tuberculosis maintained in the NRP state, and by the acid-fast bacilli in several sputum samples. The transcriptome of the sputum sample revealed production of many proteins made in the NRP state.\nFinally, the researchers showed that the time needed to grow M. tuberculosis from sputum samples increased as the proportion of lipid body–positive acidfast bacilli in the sputum increased, just as one would suspect if the presence lipid bodies signifies nongrowing cells.What Do These Findings Mean?\nAther Mirza | alfa\nMicrogel powder fights infection and helps wounds heal\n14.11.2018 | Michigan Technological University\nSpread of deadly eye cancer halted in cells and animals\n13.11.2018 | Johns Hopkins Medicine\nBiochips have been developed at TU Wien (Vienna), on which tissue can be produced and examined. This allows supplying the tissue with different substances in a very controlled way.\nCultivating human cells in the Petri dish is not a big challenge today. Producing artificial tissue, however, permeated by fine blood vessels, is a much more...\nFaster and secure data communication: This is the goal of a new joint project involving physicists from the University of Würzburg. The German Federal Ministry of Education and Research funds the project with 14.8 million euro.\nIn our digital world data security and secure communication are becoming more and more important. Quantum communication is a promising approach to achieve...\nOn Saturday, 10 November 2018, the research icebreaker Polarstern will leave its homeport of Bremerhaven, bound for Cape Town, South Africa.\nWhen choosing materials to make something, trade-offs need to be made between a host of properties, such as thickness, stiffness and weight. Depending on the application in question, finding just the right balance is the difference between success and failure\nNow, a team of Penn Engineers has demonstrated a new material they call ""nanocardboard,"" an ultrathin equivalent of corrugated paper cardboard. A square...\nPhysicists at ETH Zurich demonstrate how errors that occur during the manipulation of quantum system can be monitored and corrected on the fly\nThe field of quantum computation has seen tremendous progress in recent years. Bit by bit, quantum devices start to challenge conventional computers, at least...\n09.11.2018 | Event News\n06.11.2018 | Event News\n23.10.2018 | Event News\n14.11.2018 | Materials Sciences\n14.11.2018 | Health and Medicine\n14.11.2018 | Life Sciences']"	['<urn:uuid:0bc6fb96-7569-4c98-9f4a-a6ae18bba2cb>']	factoid	with-premise	short-search-query	distant-from-document	expert	2025-04-14T18:34:38.591736	46	323	7620
40	need info earliest postwar change exhibition organization who curated shows	After the Second World War, the most significant shows of contemporary art were increasingly organised by professional exhibition-makers instead of artists. Harald Szeemann was one such curator, known for his major exhibition 'When Attitudes Become Form'.	['Beyond the commercial\nLike museums, fairs are learning how to educate their audiences\nBy Bruce Altshuler. From Frieze daily edition\nPublished online: 18 October 2013\nAsk people what they think of when the words “Armory Show” are mentioned, and many will say a commercial art fair that takes place in New York on two piers in the Hudson River each spring. I am always amused, because the name of course stems from the 1913 exhibition in which Modern art was introduced to a broad American public, first in New York and then in Chicago and Boston. But lately I have been thinking about analogies between the two Armory Shows, similarities that situate them, and contemporary art fairs, within a more general history of exhibitions and art institutions.\nThe 1913 Armory Show was organised by a group of artists to display and sell their work; titled the International Exhibition of Modern Art, its Manhattan venue was the 69th Regiment Armory on Lexington Avenue. Inspired by the 1912 Cologne Sonderbund exhibition, the show’s organisers brought to New York a large and outstanding group of European Modern works of art, which generated controversy and garnered most of the attention. One participant, Stuart Davis, described it in retrospect as “a masochistic reception whereat the naïve hosts are trampled and stomped by the European guests at the buffet”.\nThe Armory Show art fair was also created by a self-organised group intent on introducing its wares to a larger public. Put together by the dealers Pat Hearn, Colin de Land, Paul Morris and Matthew Marks, it was first held in 1994 at the Gramercy Park Hotel as the Gramercy International Art Fair. As in Chelsea Hotel precedents such as a 1965 Daniel Spoerri exhibition, the works were displayed in small hotel rooms that were little-changed for the occasion. The new name came in 1999 with the fair’s relocation to the site of the 1913 Armory Show, and remained despite the event’s move two years later to the Hudson River piers.\nHaving been organised by a group of artists to present their own work and that of their fellows, the Armory Show typifies a critical feature of many important shows of the classic avant-garde. Examples range from the Impressionist exhibitions of the 1870s and 1880s in Paris to the “First Exhibition of the Editors of the Blaue Reiter” (Munich, 1911), the “First International Dada Fair” (Berlin, 1920) and the “International Exposition of Surrealism” (Paris, 1938). After the Second World War, however, the most significant shows of contemporary art were increasingly organised by professional exhibition-makers. The self-organisation of the Armory art fair connects it to its namesake in this regard. And, as with major exhibitions in general, the fair’s future lay with institutional growth and professional organisers.\nThe professionalisation of exhibition-making is essentially tied to the development of the group thematic exhibition. Although a principal function of large international exhibitions was to display the new, with the profusion of artistic innovation in the 1960s and 1970s, it was felt that there was a need for exhibitions to facilitate audience comprehension by providing some conceptual framing. The demand was filled by curators such as Harald Szeemann, whose first major exhibition, “When Attitudes Become Form”, has been restaged this year by the Prada Foundation alongside the Venice Biennale. Complex exhibition narratives proliferated with the growth of international biennials outside the Euro-American centres, generating ambitious conference, publication and commissioning programmes. And as biennials and other major exhibitions have taken on a highly thematic character, one might argue that contemporary art fairs have become the primary places to go to see what is new.\nAncillary programming is now also found at all key art fairs, as with Frieze Talks, Frieze Projects and the curated Armory Focus exhibitions. Some consider such events to be merely high-cultural amenities to elevate the tone of commercial enterprises, much like works of art in new condominium complexes or art lectures on Cunard cruises. But these programmes also connect the art fair to changes in other art-world institutions, many of which have moved from conceiving of themselves solely as sites of display to becoming platforms for the exploration of broad issues. The presentational function of art fairs remains central. Yet as fairs have joined museums and biennials as places that attract wide-ranging audiences, it is not surprising to see them move into the educational and the discursive. To understand them fully, we must think about their participation in such cultural developments and convergences.\nBruce Altshuler is in conversation with Vivian Sky Rehberg, Frieze Talks, Friday 18 October, 5pm\nSubmit a comment\nAll comments are moderated. If you would like your comment to be approved, please use your real name, not a pseudonym. We ask for your email address in case we wish to contact you - it will not be\nmade public and we do not use it for any other purpose.\nWant to write a longer comment to this article? Email email@example.com']	['<urn:uuid:260a0b5a-f8e0-4cc7-9550-49a4de42a697>']	factoid	with-premise	long-search-query	distant-from-document	expert	2025-04-14T18:34:38.591736	75	255	5149
41	What materials are usually used for outdoor barn walls?	The most common siding material for pole buildings is rolled-rib 29-gauge enameled steel, which comes in 32-or-36-inch widths and is attached using color-matched screws with rubber washers to seal the holes. However, other standard siding materials can also be used, including T1-11, vinyl, lap siding, cedar, and even brick. When using materials other than metal, you may need to first install sheathing such as plywood, oriented strand board, or boards.	"['Pole building framing\nPole framing or post-frame construction (pole building framing, pole building, pole barn) is a simplified building technique adapted from the labor-intensive traditional timber framing technique. It uses large poles or posts buried in the ground or on a foundation to provide the vertical structural support, along with girts to provide horizontal support. The method was developed and matured during the 1930s as agricultural practices changed, including the shift toward engine-powered farm equipment and the demand for cheaper, larger barns and storage areas.\nPole building design was pioneered in the 1930s in the United States originally using utility poles for horse barns and agricultural buildings. The depressed value of agricultural products in the 1920s and 1930s and the emergence of large, corporate farming in the 1930s created a demand for larger, cheaper agricultural buildings. As the practice took hold, rather than using utility poles, materials such as pole barn nails were developed specifically for this type of construction, making the process more affordable and reliable. Today, almost any low-rise structure can be quickly built using the post-frame construction method.\nPole barn construction was a quick and economical method of adding outbuildings on a farm as agriculture shifted to equipment-dependent and capital-intensive agriculture—necessitating shelter for tractors, harvesters, wagons and the like in much greater quantities and sizes. Around North America, many pole-built structures are still readily seen in rural and industrial areas.\nPoles, from which these buildings get their name, are natural shaped or round wooden timbers 4 to 12 inches (102 to 305 mm) in diameter. The structural frame of a pole building is made of tree trunks, utility poles, engineered lumber or chemically pressure treated squared timbers which may be buried in the ground or anchored to a concrete slab. Generally the posts are evenly spaced 8 to 12 feet (2.44 to 3.66 m) apart except to allow for doors. Buried posts have the benefit of providing lateral stability so no braces are needed. Buried posts may be driven into the ground or set in holes then filled with soil, crushed stone, or concrete.\nPole buildings may not have walls but be open shelters, such as for farm animals or equipment or for use as picnic shelters.\nEnclosed pole buildings have exterior curtain walls formed by girts fastened to the exterior of the posts at intervals about 2 feet (0.61 m) on center that carry the siding and any interior load. The walls may be designed as a shear wall to provide structural stability. Other girt systems include framing in between the posts rather than on the outer side of the posts. Siding materials for a pole building are most commonly rolled-rib 29-gauge enameled steel cut to length in 32-or-36-inch (813 or 914 mm) widths attached using color-matched screws with rubber washers to seal the holes. However, any standard siding can be used, including T1-11, vinyl, lap siding, cedar and even brick. Using sidings other than metal may require first installing sheathing, such plywood, oriented strand board or boards.\nOn two walls, usually the long walls, the dimensional lumber girts at the top of the walls are doubled, one on the inside and one on the outside of the posts, and usually through-bolted with large carriage bolts to support the roof load. The roof structure is frequently a truss roof supporting purlins or laths, or built using common rafters. Wide buildings with common rafters need interior rows of posts. Sometimes rafters may be attached directly to the poles. The roof pitch of pole buildings is usually low and the roof form is usually gable or lean-to. Metal roofing is commonly used as the roofing and siding material on pole buildings.\nThe floor may be soil, concrete slab, or framed of wood.\nIn modern developments the pole barns of the 1930s have become pole buildings for use as housing, commercial use, churches, picnic shelters or storage buildings. In the process more often than not, the poles have become posts of squared-off, pressure-treated timbers. These structures have the potential to replicate the functionality of other buildings, but they may be more affordable and require less time to construct. The most common use for pole buildings is storage buildings as it was on the farms, but today they may be for the storage of automobiles or boats along with many other household items that would normally be found in a residential garage, or commercially as the surroundings for a light industry or small corporate offices with attached shops.\nGallery of modern uses\n- Kern, Barbara, and Ken Kern. The owner-built pole frame house. New York: Scribner, 1981. Print. ISBN 978-0684167671\n- Merrilees, Doug, and Ralph Wolfe. Low-cost pole building construction. paperback ed. Pownal, VT: Storey Communications, 1991. Print.\n- Seddon, Leigh W.. Practical pole building construction. Charlotte, VT: Williamson, 1985. Print.\n- Burch, Monte. Monte Burch\'s Pole building projects: over 25 low-cost plans. Pownal, Vt.: Storey Communications, 1993. Print.\n- Agriculture, U.S. Department of.. The Encyclopedia of Wood. New York: Skyhorse Publishing, Inc., 2011. 16-4 to 16-6. Print.\n- Vlach, John Michael. Barns. New York: W. W. Norton & Co. ;, 2003. 21–22. Print.\n- National Frame Builders Association\n- ""Pole."" def. 3. Merriam-Webster.com. Merriam-Webster, n.d. Web. 26 Jan. 2014. <http://www.merriam-webster.com/dictionary/pole>.\n- Stalnaker, Judith J., and Ernest C. Harris. Structural design in wood. New York: Van Nostrand Reinhold, 1989. 319. Print.\n- Quality Steel Buildings, Inc. – What is a pole building?\n- Pole Building Glossary. Terminology – Pole buildings Archived 2010-03-27 at the Wayback Machine., Retrieved 28 April 2010.\n- An example of building code requirements for a pole building in the U.S.\n- Examples of pole barns from Iowa State University\n- National Frame Building Association official website\n|Wikimedia Commons has media related to Pole barns.|']"	['<urn:uuid:d8905133-7898-4dea-bb30-d707cfcaf029>']	open-ended	with-premise	concise-and-natural	distant-from-document	novice	2025-04-14T18:34:38.591736	55	455	6062
42	I'm dealing with some mental health issues alongside my trauma and wondering if CPT could help - what other benefits can this therapy provide besides trauma treatment?	CPT sessions are designed to help improve overall quality of life by alleviating comorbid mental illnesses such as depression, substance abuse, and dissociation. Additionally, the therapy helps people trust themselves and others again, improve their self-esteem, and increase their ability to be intimate. The ultimate goal is to improve how you feel day to day, without trauma dictating your mood or behaviors.	['Trauma in any form is hard to shake.\nBut when we hold onto our unhealthy thoughts about ourselves and our lives, we give trauma even more power than it already has.\nAnd let’s be real, it doesn’t deserve any more real estate in your brain.\nThat’s where Cognitive Processing Therapy (CPT) comes in.\nCPT is a specific type of trauma-focused cognitive behavioral (TF-CBT) psychotherapy that helps individuals recover from post-traumatic stress disorder (PTSD) and related events such as child abuse, combat (military and veterans), rape, and natural disasters.\nIt has been found to be the most successful in alleviating trauma-related PTSD symptoms while maintaining long term recovery.\nThis therapy approach teaches you to become aware of your thoughts and feelings surrounding your trauma, and to create a new understanding and interpretation of the event in order to lessen its negative impact on your life.\nCPT is also used to help you change and modify unhealthy beliefs related to your trauma, and manage your memories effectively by gaining a more in depth understanding of how your symptoms can affect your thoughts about yourself and the world around you.\nOur ultimate goal is to improve the way you feel on a day to day basis, without your trauma dictating your mood or behaviors.\nCPT was originally developed to help process the emotions (especially fear) caused by PTSD, and to help manage them in a way that’s conducive to a happier and more functional life.\nAccording to the underlying theories of CPT, exposure to your traumatic event—in a safe environment—is a crucial step in accepting, processing, and giving less power to your fear.\nHOW CPT WORKS\nWhile the title “Cognitive Processing Therapy” may seem scientific and impersonal, your sessions will actually be very interactive as we discuss ways to shift your negative thoughts surrounding your trauma.\nThroughout our sessions, we will work together to consider other ways of thinking about your situation and beliefs about yourself that have surfaced in the aftermath of your traumatic experience.\nWe will use a wide range of different techniques during our sessions, and we will even give you tools to practice at home in your own time.\nThe process will begin with some psychoeducation, where we teach you about the symptoms and impacts of PTSD.\nThis will make you aware of your “automatic thoughts” that may be perpetuating your PTSD symptoms.\nThen, you will learn to process your trauma more directly by verbally talking about it or writing it down on paper.\nAnd of course, will be there to help you navigate your thoughts and feelings along the way.\nWhile facing trauma may seem like a daunting and intimidating process, we will be there to guide you through the pain and into a greater sense of healing, safety, and peace.\nWe will continue to work with you in getting rid of your unhelpful thoughts and beliefs, helping you transform self-blame and self-hatred into self-compassion and self-worth.\nOnce you are able to identify your automatic, negative thoughts you will continue to use your newfound skills to modify your traumatic thoughts and beliefs.\nCPT also involves work outside of session so that you gain the greatest benefits possible from treatment by integrating practical exercises into your life.\nWe will supply you with worksheets that will help you implement your newly-learned strategies in the real world.\nOur therapists understand that everyone who experiences trauma is unique, and that your experience is extremely personal.\nThat’s why we will develop a plan of action that is tailored to your individual needs.\nIt’s also important to note that CPT’s efficacy may differ for different populations, and may be less effective for combat-induced PTSD (you might want to look into EMDR or TRM for combat-induced trauma, please follow the links to visit our dedicated pages and learn more.)\nOur CPT sessions are designed to help patients learn skills to improve their quality of life, and help alleviate comorbid mental illnesses such as depression, substance abuse, and dissociation.\nAt the same time, you will learn to trust yourself and those around you, improve your self-esteem, and increase your ability to be intimate again.\nAt My La Therapy, we are ready and prepared to help you take control of your story, manage your trauma, and get your life back.\nUsing Cognitive Processing Therapy, we will challenge the cognitive distortions surrounding your trauma, so you can experience greater freedom, peace, and vitality.\nOUR THERAPY METHODS\nTherapy can successfully improve your life by helping you minimize the anxiety in your life, identifying and changing underlying thought and behavioral patterns that contribute to your struggles, and providing you with strategies to decrease discomfort while restoring an overall sense of peace.\nACT is an evidence-based, scientifically proven intervention that is demonstrated by research to be effective in addressing a wide range of psychological issues including trauma, depression, anxiety, and relationship issues.\nLearn more about our empirically based therapy modalities by visiting our Methods page.\nWANT TO TALK? SPEAK WITH A CPT EXPERT NOW\nIf you have any questions, contact one of our CPT specialists for a free consultation any time.']	['<urn:uuid:1fb45c51-0d8a-4025-8a48-9b75bee3f65a>']	factoid	with-premise	verbose-and-natural	similar-to-document	novice	2025-04-14T18:34:38.591736	167	411	5279
43	psychiatrist researching frequency mood disorders expecting mothers occurrence rate depression anxiety pregnancy	The CDC estimates that almost 20 percent of women experience some type of depressive episode during or after pregnancy. Additionally, the prevalence of anxiety disorders during the prenatal and postpartum period equals or surpasses that of depression during pregnancy.	"[""The recent HealthStream Webinar, “Maternal Mental Health Risk Assessment and Intervention Before, During, and After Pregnancy,” featured Susan Kendig, an attorney and nurse practitioner with more than 35 years of experience in the healthcare industry. This blog post is the first in a series based on this webinar that will focus on maternal mental health.\nThe CDC estimates that almost 20 percent of women will experience some type of depressive episode during or after pregnancy. Untreated, this can have a devastating effect on women and their families. Importantly, maternal suicide within a year of giving birth is emerging as a significant cause of maternal mortality, and it's probably under-reported in the United States. States are now looking at maternal mortality reviews and incorporating review of suicidality during pregnancy or the first year postpartum. Both in Colorado and Illinois, they have found that suicides, when you look at maternal deaths both during pregnancy and that first year postpartum, from suicide or drug overdose, seem to be outpacing all medical conditions combined and are emerging as one of the top causes of maternal mortality. Certainly, this staggering statistic is something to consider in terms of prevention strategies and recognition strategies. There are significant implications, especially that women who are having issues in terms of depression and anxiety may not adhere to their prescribed medical regimen.\nHigh-risk Patients and Depression Are a Dangerous Mix\nHigh-risk patients are those women who have severe hypertensive disorders, diabetes, etc. Having an overlay of mental health conditions may actually contribute to less than optimal outcomes. Certainly, there are issues with the newborn, not the least of which would be a potential for failure to thrive, increased risk of pediatric issues, and at the very least missing those really important pediatric appointments and immunizations.\nAnxiety should also be considered during the prenatal and postpartum period because the prenatal and postpartum prevalence, particularly when you look at the full spectrum of anxiety disorders, equals or surpasses that of depression during pregnancy, and prenatal anxiety is really a very strong predictor of perinatal depression as well. Outcomes tend to be similar.\nRecommendations for Screening for Depression and Anxiety\nBecause of the statistics and the recognition of the importance of depression and anxiety and its impact on maternal and women's health outcomes as well as newborn outcomes, several professional groups have issued recommendations around screening for depression and anxiety.\nAmerican Academy of Paediatrics Advises Screening During Newborn Visits\nThe American Academy of Paediatrics was the first to come on board with this, recommending that pediatricians screen mothers for depressive symptoms at child visits at one, two, and four months. They have since updated that recommendation to continue those during the first year. The important thing to remember is if your pediatricians are implementing this protocol, there needs to be some type of feedback loop back to the women's health care provider as well as her primary care provider. The woman is screened during that pediatric encounter, because the theory here is many women will attend the newborn visits. However, about 40% of women across the board miss their postpartum visits. So, having pediatricians screen is an excellent idea. The next step would be having that feedback loop because if she does come to her women's health care provider or her primary care provider for a variety of reasons and they don't know that she has had a screen that indicates risks, there again maybe a missed opportunity.\nACOG and Others Also Endorse Screening\nIn May 2015, The American College of Obstetricians and Gynecologists (ACOG) published a committee opinion, the American College of Obstetricians and Gynecologists, to screen at least once during the prenatal period and the US Preventive Services Task Force followed that with a statement including depression screening for pregnant women both in the antepartum and postpartum period. A significant body of research recommended that screening occur during the prenatal and postpartum periods, which led the Council on Patient Safety to develop a maternal mental health patient safety bundle and widely recognized that there are multiple conditions that come under that umbrella of mental health, certainly things like bipolar disorder, schizophrenia, etc.\nFuture blog posts will examine the maternal mental health patient safety bundle.\nAccess the full Webinar recording here.\nHealthStream’s learning management system and comprehensive suite of competency management tools empower your healthcare workforce to deliver the best patient care.View All Learning & Performance\nWhen you enact HealthStream's quality compliance solutions, you can do so with the confidence your healthcare organization will meet all standards of care.View All Quality & Compliance\nFulfill compliance requirements with a variety of programs and courseware designed to address critical regulatory requirements as well as educate staff to recognize and mitigate risks.View All Products\nHealthStream offers professional training and education on how to best optimize your reimbursement process within your healthcare organization.View All Reimbursement\nImprove the preparedness of your staff, increase survival rates, and cut costs with the advanced resuscitation training services from HealthStream.View All Resuscitation\nExpand the decision-making skills and effectiveness of your healthcare workforce with HealthStream's clinical development programs and services.View All Clinical Development\nDelivers everything you need to request, gather, and validate information about a provider to create a single source of truth for downstream processes.View All Credentialing\nMake sure your healthcare staff can schedule out appointments and work schedules with ease using HealthStream's line of software solutions.View All Scheduling & Capacity Management""]"	['<urn:uuid:343c3c29-945c-4cbc-ac8b-eb26baddc5bc>']	factoid	with-premise	long-search-query	distant-from-document	expert	2025-04-14T18:34:38.591736	112	268	6101
44	im concerned about environment what big companies doing reduce carbon emissions 2021	2021 was a significant year for climate action among large companies. Through the Climate Action 100+ initiative, which includes over 615 investors managing $60 trillion in assets, many of the world's largest greenhouse gas emitters made new net-zero commitments. In fact, 111 major emitting companies have now set net-zero targets. Companies faced increased pressure from shareholders through climate-related resolutions, with 147 such resolutions filed in the U.S. alone, achieving an average 40% support rate.	['Article originally published in November 2021 issue of ProxyInsight. Written by Stephanie Maier, Global Head of Sustainable and Impact Investment at GAM Investments and current chair of Climate Action 100+ global Steering Committee, and Anne Simpson, Managing Investment Director, Board Governance & Sustainability at CalPERS and member of the Climate Action 100+ global Steering Committee.\n2021 has been a remarkable year for shareholder action on climate change. Investors secured historic results, accelerating the decarbonization of the world’s largest corporate greenhouse gas emitters through new net-zero commitments, replacing directors, and aligning political lobbying with the Paris Agreement. The progress shows what investors can achieve and Climate Action 100+ is raising its game.\nAs investors, we know we can and we must turn the financial tide and drive meaningful decarbonization by stepping up the intensity of our engagements with companies. We are not alone in this viewpoint. The Climate Action 100+ initiative now includes more than 615 investors responsible for over $60 trillion in assets under management.\nA PROXY SEASON TO REMEMBER\nThere was extraordinary growth in climate-related shareholder resolutions in the 2021 proxy season, as investors demonstrated their willingness to use these as an effective means of securing more ambitious targets from companies. In the U.S., there were over 147 climate-related resolutions filed, with 47 going to a vote and support averaging 40%. Six out of 14 shareholder proposals filed by Climate Action 100+ investor signatories and flagged by the initiative won majority support, according to Proxy Insight Online data.\nTURNING COMMITMENT INTO ACTION\nTo provide a transparent and objective way of measuring company progress, Climate Action 100+ launched the first iteration of its Net-Zero Company Benchmark in March. This provides the foundation for company engagement coordinated through the initiative, and the assessments have become a widely-cited demonstration of the progress companies are making in aligning to a net-zero future.\nTo date, 111 of the systemically-important emitters on the Climate Action 100+ focus list have responded to this call from from investors by setting net-zero targets. But there is still much more work to do and investor signatories are engaging intensely with companies on improving their alignment ahead of the next round of assessments in March 2022.\nClimate Action 100+ launched a new sectoral program this year, looking at the actions that companies, industries as a whole and investors in carbon-intensive sectors must take to accelerate the pace of the net-zero transition. But decarbonisation isn’t a ‘one size fits all’ approach. The strategies published so far for the aviation, steel, food and beverage, and electric utilities sectors aim to provide sector specific detail and clarity. In the coming months we’ll be working on sector strategies for the remaining sectors that are vital to the transition.\nRAISING THE BAR IN 2022\nClimate Action 100+ recently published an updated framework and indicators for the next iteration of the Net- Zero Company Benchmark company assessments, which will be released in March 2022. The new and updated indicators reflect the evolving priorities of investors, with a new climate accounting and audit indicator representing the increased focus around incorporating climate risk in financial accounts. In addition, the inclusion of a just transition indicator assesses the impact of a company’s net-zero transition on its workforce, communities, and supply chains.\nBuilding on the strong results from this year, we expect there will be a laser focus on how company net-zero goals are being met with short-, medium-, and long-term emissions reduction targets, and we anticipate more resolutions on integrating climate risk into financial accounts. We also expect to see more companies producing transition plans and putting these to shareholder advisory votes.\nWhere we don’t see the required progress being made from companies, the next step is to ask directors to respond to these challenges and bring about the required change. This year, investors demonstrated their willingness to hold boards to account on climate change, in particular with the election of three new directors at Exxon Mobil.\nNone of these are small challenges but if we can build on the momentum that was achieved this year, we are hopeful we can drive further alignment with the goals of the Paris Agreement. As fiduciaries, with a duty of prudence, loyalty, and care, we expect nothing less than the companies across our portfolios to manage the risk and find the opportunity that the transition to net-zero will bring.']	['<urn:uuid:93d0cda3-e3d8-4b2c-8cb5-de200e414922>']	open-ended	with-premise	long-search-query	distant-from-document	novice	2025-04-14T18:34:38.591736	84	512	4743
45	urgent care facilities services provided	Urgent Care Centers treat acute but minor medical conditions like cough, fever, sore throat, urinary infections, and ankle sprains. They provide flu shots and school physicals, have onsite labs and x-ray machines, and are typically open 7 days per week until 8 or 9 pm.	['Hospital ER, Freestanding ER, or Urgent Care- How to choose the right care.\nLet’s say your child has a high fever, things are getting worse and you need quick medical attention. Where do you take them?\nIf you go to urgent care your provider might suggest you be transferred to an ER for higher level of care. You can’t afford to pay for two sets of bills, but your child needs medical attention now. What do you do?\nMany people base their decision on cost. Both hospitals and Freestanding ERs can handle everything that an urgent care can, but that “one-stop shop” convenience usually comes with a hefty price tag.\nWhile there are many sides to this debate, there are some solid guidelines that can help you determine which venue is best for your acute medical need.\nChoosing Between an ER and Urgent Care\nFreestanding ERs and Hospital ERs both provide great emergency care. The biggest decision is when to choose an ER versus an urgent care. Really, price can’t factor into this call. The decision should be driven only by the severity of the medical problem. If there is a definite medical emergency call 911 or go to your nearest emergency room. Serious bleeding, head injuries, major fractures, ingestion of poison, chest pain, stroke like symptoms – these types of conditions all need immediate and serious attention and there is no substitute for the ER in these circumstances.\nHospital ERs are a trusted and reliable choice, but as we know they are not always convenient and they definitely are not cheap. The good news is that According to an independent survey, only 1/3rd of people attending an E.R needed actual emergency care! If you know loss of a limb or life are not on the line, spare your pocket book. There are better choices out there.\nSo What are Urgent Care Centers For?\nUrgent Care Centers bridge the gap between the services provided in an emergency room and services provided in a primary care physician office. They treat a broad range of acute, but minor medical conditions. Examples include, cough, fever, sore throat, urinary infections, and ankle sprains. They often provide flu shots and school physicals as well. The majority of urgent care centers are open 7 days per week and appointments are not required. Wait times vary, but are generally shorter than at a hospital ER. Most all are open until 8 or 9 pm and some as late as 11pm. They are usually staffed by mid-level providers or family care physicians. They can provide most of the same services available in a local physician’s office with the addition of onsite labs and x-ray machines. They usually do not have the specialized medical equipment or emergency trained doctors needed for life-threatening medical conditions. Cost is the biggest benefit here. They often offer cash-pay options if you do not have insurance and most are in-network with major insurance plans. This means they have a contract with your insurance company so there are no surprise battles to fight later.\nFreestanding versus Hospital Based ERs\nWhether you have a cold or have suffered a gunshot wound, both facilities are capable of providing quality and emergency care 24 hours a day. While people understandably want convenient access to medical care whenever they need it, both types of ERs should really be reserved for real emergencies. This is especially true for hospital ERs where the same ER doctors treating coughs and urinary infections are usually simultaneously entrenched treating heart attacks, strokes, and trauma victims. Those critical patients will require the physician’s attention and may cause you to wait longer to be seen if you are more stable.\nBecause of the myriad of complaints and their sheer volume hospitals they have to use a strict triage system to decide who gets seen first. Time is important to everyone. In order to judge if a freestanding may be a better alternative, it is helpful to understand the hospital’s triage system. Triage usually consists of 5 levels based on severity. Level 1s are the most critical patient’s who can die without immediate life-saving interventions. This includes acute cardiac arrest patients who are receiving CPR. Their hearts have actually stopped beating and all the hospitals resources are brought to bear in the attempt to save the patient’s life. They garner the immediate attention of everyone on the team. Level 2s are the next most critical patients who should receive some medical attention within the first 15 minutes of arriving to an ER. These patient’s usually have abnormal vital signs or life threating complaints like chest pain and shortness of breath. Without quick treatment, these patients have the potential to become level 1s! They go to a room as soon as possible. Level 4s and 5s are much less sick and the most stable patients. These patients usually require no more than 1 X-ray or 1 lab test. Examples include ankle sprains, flu symptoms and bug bites. These are shuffled to the ER’s ‘fast track’ area where wait times are typically around an hour. Basically 1s, 2s, and even 4s and 5s all get seen pretty quickly. The problem is the guys in the middle – the dreaded level 3s. These are the abdominal pains, high fevers, headaches, complex lacerations, broken bones, concussions, dehydration and asthma exacerbations. Abdominal pain could be something serious like appendicitis, but it could also be constipation or gas. An asthma attack can turn deadly serious but the patient may just need a breathing treatment or a med refill. When everyone needs the doctor ‘now’ triage is the answer and Level 3s go to the back of the line and they wait the longest. They are too sick for an urgent care or the fast track area, but not sick enough to get sent quickly to a room. Getting triaged to Level 3 means going to the purgatory of the ER. Put simply, if there is a four hour wait, and you are designated a ‘level 3’ you will wait in the lobby for the entire four hours.\n“So, what do I do if I may be a level 3?”\nOne option is a freestanding ER (preferably with a board-certified ER doctor who has trained in hospital ERs). Many Freestanding ERs employ the same board-certified ER doctors as the major hospitals, and they can provide the same initial stabilization and treatment of most anything that walks through their door. They have CT scanners, Ultrasounds, X-ray and on-sight, hospital-quality labs. They can test for heart attacks and meningitis. They can safely sedate patients for painful procedures like setting broken bones. They are even equipped to place someone on temporary life support in preparation for transport to a hospital if needed. If someone needs admission to a hospital they can often have them transferred directly to a room in that hospital faster than if they had gone to that hospital’s own ER. In-short they are a great solution for the level 3s.\nThe primary concern with freestanding facilities is that they bill similar to hospital ERs and some are located in shopping centers and get mistaken for retail urgent care facilities. The trick is to look for the word “Emergency” on the building and to use them for emergencies only. Level 3s are true emergencies. They can turn into fatal situations and no one plans for these.\nAnother complaint is that the freestanding facilities are often out of network with insurers. Fortunately, insurers are required by law to cover all legitimate ER visits under their in-network plans regardless of whether the visit was at a freestanding ER or located in a hospital. It is important to note that many hospitals and/or their ER doctors are ‘out-of-network’ as well. Insurance companies and the state government recognize that no one has time to check the network status of an ER in the middle of an emergency.\nTo make it even easier, some freestanding ERs actually partner with an on-site urgent care that provides both services under one roof taking out ALL of the guess work.\nIf you have questions about your local freestanding ER, give them a call. Ask if their doctors are board-certified in emergency medicine. Ask if they balance bill. Ask if they have an urgent care on site for minor complaints.\nIn summary, all three types of facilities address the need for quick medical assistance. If you would typically visit your doctor, but they are unavailable due to the time or day, then urgent care is usually the next best place to seek medical assistance quickly. If you think you may have a level 3 complaint give your local freestanding ER a try. If you know you or your loved one needs immediate lifesaving care call 911.\nFOLLOW US ON SOCIAL FOR MORE RELEVANT,\nLOCAL HEALTH INFORMATION']	['<urn:uuid:9a678d9e-ed7c-4734-afb9-eebab14aa212>']	factoid	direct	short-search-query	similar-to-document	expert	2025-04-14T18:34:38.591736	40	269	8622
46	psychology student looking for who wrote chapter genetics genomics post traumatic stress disorder	Chapter 11 on 'Genetics and Genomics of Post-traumatic Stress Disorder' was written by Monica Uddin, Ananda B. Amstadter, Nicole R. Nugent, and Karestan C. Koenen.	"[""The Oxford handbook of traumatic stress disorders\n- edited by J. Gayle Beck, Denise M. Sloan.\n- Oxford ; New York : Oxford University Press, c2012.\n- Physical description\n- xxi, 548 p. : ill ; 26 cm.\n- Oxford library of psychology.\n- Includes bibliographical references and index.\n- Part One: Introduction to the volume -- 1. Traumatic Stress Disorders: Historical context and current focus -- J. Gayle Beck and Denise M. Sloan -- Part Two: Classification and Phenomenology -- 2. Defining traumatic events: Research findings and controversies -- Jesse R. Cougle, Dean G. Kilpatrick, and Heidi Resnick -- 3. Classification of Acute Stress Disorder -- Maria L. Pacella and Douglas L. Delahanty -- 4. Classification of Posttraumatic Stress Disorder -- Tali Manber Ball and Murray B. Stein -- 5. Future of Classification in Post Traumatic Stress Disorder -- Terence M. Keane, Ph.D. & Mark W. Miller, Ph.D. -- Part Three: Epidemiology and Special Populations -- 6. The Epidemiology of Acute Stress Disorder and Other Early Responses to Trauma in Adults -- Quinn M. Biggs, Jennifer M. Guimond, Carol S. Fullerton, Robert J. Ursano, ASD Workgroup* Christine Gray, Matthew Goldenberg, Dori Reissman, James E. McCarroll, Patcho Santiago, Mary P. Tyler -- 7. Epidemiology of Post-Traumatic Stress Disorder in Adults -- Naomi Breslau -- 8. Traumatic Stress Disorders in Children and Adolescents -- Annette M. La Greca, Cortney J. Taylor, Whitney M. Herge -- 9. Traumatic Stress in Older Adults -- Joan M. Cook, Tatyana Biyanova, Diane L. Elmore -- 10. Traumatic Stress in Special Populations -- Kim T. Mueser and Weili Lu -- Part Four: Contributions from Theory -- 11. Genetics and Genomics of Post-traumatic Stress Disorder -- Monica Uddin, Ananda B. Amstadter, Nicole R. Nugent, Karestan C. Koenen -- 12. Biological Contributions to PTSD: Differentiating normative from pathological response -- Rachel Yehuda, Laura Pratchett, Michelle Pelcovitz -- 13. Learning Models of PTSD -- Shmuel Lissek and Christian Grillon -- 14. Information Processing in Posttraumatic Stress Disorder -- Anke Ehlers, Thomas Ehring, and Birgit Kleim -- 15. Family Models of Posttraumatic Stress Disorder -- Candice M. Monson, Steffany J. Fredman, Rachel Dekel, Alexandra Macdonald -- Part Five: Assessment -- 16. Assessing PTSD Symptoms -- Michelle J. Bovin and Frank W. Weathers -- 17. Assessing Acute Traumatic Stress Symptoms -- Richard A. Bryant, PhD -- 18. Assessing trauma-related symptoms in children and adolescents -- Sonja March, Alexandra De Young, Belinda Dow, and Justin Kenardy -- 19. Psychometric concerns in the assessment of trauma-related symptoms in older adults -- Willeke H. van Zelst and Aartjan T.F. Beekman -- 20. Assessment of PTSD in Non-Western Cultures: The Need for New Contextual and Complex Perspectives -- Boris Droz?ek, John P. Wilson, and Silvana Turkovic, Ph.D. -- 21. Assessing PTSD-related Functional Impairment and Quality of Life -- Darren W. Holowka and Brian P. Marx -- Part Six: Prevention/Early Intervention -- 22. Risk and Protective Factors for Traumatic Stress Disorders -- Lynda A. King, Anica P. Pless, Jennifer L. Schuster, Carrie M. Potter, Crystal L. Park, Avron Spiro, III, Daniel W. King -- 23. Community-Based Early Intervention with Trauma Survivors -- Josef I. Ruzek, Ph.D. -- 24. Individual Approaches to Prevention and Early Intervention -- Teresa M. Au, Caroline Silva1, Eileen M. Delaney, and Brett T. Litz -- 25. Prevention and Early Intervention Programs for Children and Adolescents -- Melissa J. Brymer, Alan M. Steinberg, Patricia J. Watson, and Robert S. Pynoos -- 26. Prevention and Early Intervention Programs for Older Adults -- Martha Strachan, Lisa S. Elwood, Ananda B. Amstadter, & Ron Acierno -- 27. Prevention and Early Intervention Programs for Special Populations -- Heather E. Baldwin & B. Heidi Ellis -- Part Seven: Treatment -- 28. PTSD Treatment Research: An Overview and Evaluation -- Jessica L. Hamblen, Erin R. Barnett, Barbara A. Hermann, & Paula P. Schnurr -- 29. Empirically supported psychological treatments: Prolonged Exposure (PE) -- Nisha Nayak, Mark B. Powers, & Edna B. Foa -- 30. Empirically supported psychological treatments: Cognitive processing therapy -- Kathleen M. Chard, Jennifer L. Schuster, Patricia A. Resick -- 31. Empirically supported psychological treatments: EMDR -- C. Richard Spates and Sophie Rubin -- 32. Promising Psychological Treatments -- Megan C. Kearns, Ph.D. and Barbara Olasov Rothbaum, Ph.D. -- 33. Treating Trauma-Related Symptoms in Children and Adolescents -- Esther Deblinger, Elisabeth Pollio, and Felicia Neubauer -- 34. PTSD at Late Life: Context and Treatment -- Lee Hyer and Catherine A. Yeager -- 35. Treating trauma-related symptoms in special populations -- Devon E Hinton and Angela Nickerson -- 36. Pharmacotherapy for PTSD -- Matthew J. Friedman MD, PhD -- Part Eight: Conclusions/Summary -- 37. Traumatic Stress Disorders: Looking back and moving forward -- Denise M. Sloan and J. Gayle Beck.\n- (source: Nielsen Book Data)9780195399066 20160608\n- Publisher's Summary\n- The experience of traumatic events is a near-universal, albeit unfortunate, part of the human experience. Given how many individuals are exposed to trauma, it is interesting to question why some individuals are resilient in the face of trauma while others go on to develop chronic post-traumatic stress. Throughout the relatively brief history of the psychological study of trauma, a number of themes have consistently emerged; many of these themes remain essential elements within our current study of traumatic stress disorders, as summarized within this volume. The Oxford Handbook of Traumatic Stress Disorders addresses the current landscape of research and clinical knowledge surrounding traumatic stress disorders. Bringing together a group of highly-regarded experts, this volume is divided into six sections, together summarizing the current state of knowledge about 1) classification and phenomenology, 2) epidemiology and special populations, 3) contributions from theory, 4) assessment, 5) prevention and early intervention efforts, and 6) treatment of individuals with post-trauma mental health symptoms. Throughout the volume, attention is paid to identifying current controversies in the literature and highlighting directions that hold promise for future work.\n(source: Nielsen Book Data)9780195399066 20160608\n- Publication date\n- Oxford Library of psychology\n- 9780195399066 (acid-free paper)\n- 0195399064 (acid-free paper)\nBrowse related items\nStart at call number:""]"	['<urn:uuid:085c7c5a-edbc-4a18-af37-439c2caf273a>']	factoid	with-premise	long-search-query	similar-to-document	novice	2025-04-14T18:34:38.591736	97	163	6560
47	cuba role non aligned movement nam years secretary general fidel castro duration	Fidel Castro served as the secretary general of the Non-Aligned Movement (NAM) between 1979 and 1983	['With the death of Fidel Castro, the last of the iconic revolutionary figures of the 20th century is now no more. The word, “revolutionary” is a bit too easily bandied out these days to describe leaders, but there is no better description to encapsulate the 90-year-old Cuban leader’s life and achievements.The son of a rich landowner, Fidel — as he has always been called by his compatriots, his fellow Cubans and many in the Third World — began his political career as a militant student leader committed to social justice and the establishment of a corruption-free government in Cuba. Later, he became part of movements that sought to overthrow Cuban dictator Fulgencio Batista who came to power in 1952 through a coup.\nBatista was presiding over a system that promoted “casino capitalism”, and oversaw widespread corruption even as the country’s economy was dependent largely on one crop — sugar and had high unemployment and rural poverty. Fidel’s first foray into armed revolution was the attack on the Moncada Barracks in Santiago de Cuba in July 1953, which failed spectacularly but set the stage for his future revolutionary movement that was named the 26th of July movement.\nSoon, in the mid-1950s, Fidel, after his release from prison, along with his revolutionary comrades, Ernesto “Che” Guevara, brother Raul Castro, Camilo Cienfuegos, Juan Almeida, among several others sailed from Mexico to the Sierra Maestra to launch a guerrilla struggle. It took them close to half-a-decade and several setbacks and victories later, Fidel was able to attain power after Batista went into exile in 1959.\nCuba also played a major role in the Non-Aligned Movement (NAM), having sent Che Guevara to New Delhi to discuss its formation and later when Fidel was the secretary general between 1979 and 1983. Fidel came up with the clearest enunciation of the NAM’s aims as an anti-imperial, anti-racist organisation. Fidel had re-invented himself as a Third World internationalist and an anti-imperialist who spoke for the developing world, inspiring anti-colonial struggles. He sent Cuban forces to participate in anti-colonial wars in countries like Angola and resulting in the independence of Namibia – then South Western Africa and was revered by leaders such as Nelson Mandela for these actions and his voice against Apartheid.\nFidel’s Cuba always enjoyed good ties with India, with both countries supporting multilateralism internationally and need for a more democratised United Nations. Former Prime Minister Manmohan Singh described Fidel during his visit to Havana for a NAM summit in 2006, “I had gone there only to greet him, but he engaged me in intense discussion. We covered a whole range of issues, including the future of the international financial system, the future role of NAM, India’s development prospects and how we are dealing with our population, food and energy problems… I felt I was in the presence of one of the greatest men of our times.”\nFidel lived through a five-and-a-half decade-long Cuban economic embargo imposed by the U.S. Cuba’s socialist system emphasised investment in free education and health for its largely peasant population while discouraging free enterprise and nationalising most foreign assets in the country. This emphasis resulted in a mixed legacy. By the 21st century, Cuba had among the most advanced health care systems in the world, a largely well educated and socially conscious population, but a battered economy characterised by low wages and little diversification. Partially, the collapse of the Soviet Union was responsible for the dire straits that Cuba found itself in the 1990s, resulting in severe shortages of essential goods and supplies, but Fidel refused to give up on socialism, persisting with the social development model till he stepped down provisionally in 2006 and handed over powers to his brother Raul in 2008.']	['<urn:uuid:f01f59fa-9e1b-41f0-855d-309fd11b960a>']	factoid	with-premise	long-search-query	similar-to-document	expert	2025-04-14T18:34:38.591736	80	100	3885
48	I'm planning a fishing trip to Karumba and I've heard you need to be really careful with the weather there - what should I watch out for when I'm out on a boat?	In Karumba, you need to be very cautious with the weather, especially during the wet season. The afternoons consistently bring northwesterly tradewinds, and storms can appear suddenly with fierce electrical activity. Never take calm weather for granted. If you see a storm on the horizon, stay close to safety, and if lightning starts, avoid using graphite fishing rods. The weather patterns are particularly tricky because large thunderstorms often move from west to east over the ocean, and the humid conditions can be intense. It's especially dangerous to venture far from shore early in the day, as it can lead to a dangerous return trip.	['Wait for rain – it’s the talk of the town was a song written by the 80’s Aussie rock band Goanna, and it had to be written about Karumba in the lead up to the wet! The weather in the Gulf of Capentaria at this time of year is all anyone can talk about.\nThe Gulf mornings are so humid you are more wet after a shower than in one. Large thunderheads and crack lightening loom ominously over the ocean, moving from west to east but never really hitting the town to give respite to the incessant heat and humidity.\nThe humid northwesterly tradewinds feed into large storms that tower into the sky and tease every afternoon from the south. They always seem to miss the coastal fringe except, of course, when you are out in a boat. Welcome to the wet season build up.\nThere is no doubt that there can be some good fishing at Karumba at this time of year, however, care must be taken. Don’t ever take a lull in the weather for granted as the weather patterns have been happening forever and you cannot outsmart it!\nEvery afternoon produces some northwesterly tradewind action. Some days it starts early and some days it can be late, especially after some early morning storm action, and other days it doesn’t stop. Don’t be lured into a false sense of security and head for the horizon early as it can mean an uncomfortable and dangerous trip home.\nThe last of the run-out tide occurs around the middle of the day at this time of year. It’s about the only constant except the afternoon will see a run-in tide.\nStorms can also come from nowhere so beware. The place can cook up some real doozies and at sea in a small boat is not the best place to experience one. Electrical storms can be fierce with plenty of bolts choosing to earth out, ending in some spectacular light and sound shows, as long as you’re not in the immediate vicinity.\nIf you see a storm brewing on the horizon make sure you are not far from some form of safety and if the lightning starts, put down the expensive graphite composite lightning conductors you are fishing with.\nThe best fish to target this month is the king salmon. Some good fishing can be had by those prepared to spend a few hours on the beach at Karumba point targeting jewfish and salmon in the deepwater just off the beach.\nFishing the last of the run-in tide on the flats in the early hours of the morning can be very rewarding but be prepared to share the spot with other fishers and plenty of mosquitos. As the big spring tides exceed over 4m and start to push water up onto the flats, the mosquito population explodes.\nFavourite targets of the night fishers at Karumba point are the king or threadfin salmon. The fish are eagerly awaited every year at this time, not only as a sportfish but also as a good feed. They are an excellent backstop to the barramundi that are, of course, off the list. Make no mistake you will still catch barra and probably plenty, but all have to be released back the water immediately unharmed.\nFish the start of the run-in tide with live baits on the shallow flats area. Also try targeting fish in really dirty water using mullet fillet instead of live mullet. The oily consistency of mullet makes it great bait for dirty water and increases the chance of tangling with a big XOS black jew.\nFor night fishers, focus on the edge of the light cast off by the numerous wharves in the Norman River. The king salmon become quite veracious as the bait is forced along with the strong current and are a viable target on surface lures and fly gear. Hooking a big threadfin on an 8# in strong current is great fun. Big barra also hunt the edge of the light as the tide strength drops a little so expect a few surprises.\nOther popular places to try are the deep bank from the point to Shelly Beach with live bait on the start of the run-in tides, the mouths of 4 or 6 mile creek and the mouth of Twin Creeks a bit further up the river. On the neap tides you can also troll big gold lures for success – big threadfin salmon seem to like these.\nKing salmon are one of our finest estuary eating fish but at this time of year any fish you catch must be looked after and iced down pretty quick to stop the quality of the flesh from dropping. King salmon are hard to clean when they get bigger due to the large knobs of bone that grow along their spine. Just use the point of the knife to cut around the lump and then tear the fillet away.\nNow is the time to hit up the big fat bloke in the red outfit and get a quality rod and reel for next year’s big trip to the fishing destination of a lifetime. It pains me that people spend thousands of dollars to get somewhere fishing and then pull out the old favourite with the mono line that was bought a hundred years ago. It’s a bit like paying thousands for that U-beaut set of golf clubs and then putting a squash ball on the tee.\nAll say a few quiet prayers for a good wet and get ready. – QFMReads: 2105']	['<urn:uuid:89b14d05-6a96-4b3d-ba8c-f33cff56149b>']	open-ended	with-premise	verbose-and-natural	similar-to-document	novice	2025-04-14T18:34:38.591736	160	642	4905
49	What are the dangers of current weather changes?	The increased frequency and intensity of extreme weather events such as heatwaves, droughts, wildfires, heavy rain, and floods will significantly affect millions of people worldwide. This could lead to displacement, loss of livelihoods, and increased mortality rates. For example, wildfires can destroy homes and farmlands, leading to displacement and food insecurity.	"['According to the UN World Meteorological Organization (WMO), 2023 is likely to be the hottest year on record, with the global mean temperature currently at 1.43 degrees Celsius above pre-industrial levels. ""That is why WMO is committed to the Early Warnings For All initiative to save lives and minimize economic losses,"" added Petteri Taalas, head of WMO. Source: news.un.org\nThe WMO\'s prediction of 2023 being the hottest year on record indicates a significant acceleration in global climate change, which could have major implications for billions of people and numerous ecosystems. The increased frequency and intensity of extreme weather events could lead to displacement, loss of livelihoods, and increased mortality rates. However, there is some uncertainty in these predictions, and the impacts of climate change will vary by region. Overall, this report highlights the urgent need for action to mitigate climate change and adapt to its impacts.\n- Global climate change: The report indicates a significant increase in global temperatures, which could have a substantial impact on the lives of billions of people. This aligns with the \'7-8\' rating criteria as it affects more than a billion people and has major implications for social, economic, and political systems. The rise in temperatures can lead to more extreme weather events, affecting agriculture, water supply, and human health. For example, heatwaves can cause widespread health issues and even death, particularly in regions that are not equipped to deal with such temperatures.\n- Extreme weather events: The increased frequency and intensity of extreme weather events such as heatwaves, droughts, wildfires, heavy rain, and floods will significantly affect millions of people worldwide. This could lead to displacement, loss of livelihoods, and increased mortality rates. For instance, wildfires can destroy homes and farmlands, leading to displacement and food insecurity.\n- Impact on ecosystems: The record low levels of sea ice in the Antarctic could have severe implications for marine ecosystems, affecting millions of species. This could disrupt the food chain and lead to the extinction of certain species, which would have further knock-on effects on other species and ecosystems.\n- Human activities and climate change: The report highlights the role of human activities in exacerbating climate change, specifically through the release of heat-trapping greenhouse gases. This indicates a need for systemic changes in how societies function and interact with the environment, affecting billions of people and future generations.\n- Uncertainty in predictions: While the WMO predicts that 2023 will be the hottest year on record, there is always a degree of uncertainty in such forecasts. Climate models are complex and depend on numerous variables, some of which are not fully understood. Therefore, while the trend is concerning, the exact outcomes may vary.\n- Regional variations: The impacts of climate change and extreme weather events will not be evenly distributed. Some regions may experience more severe effects than others, which could lead to disparities in the impacts experienced by different populations.\n- Potential for mitigation: The article does not discuss potential mitigation strategies or efforts to reduce greenhouse gas emissions. If significant steps are taken to address climate change, the predicted impacts could be lessened.']"	['<urn:uuid:9b425c1c-0744-4287-b68e-34e9ced27c11>']	open-ended	direct	concise-and-natural	distant-from-document	novice	2025-04-14T18:34:38.591736	48	368	3430
50	what data used identify high risk buildings collapse baltimore row houses	Building footprint data was used to divide city blocks into contiguous sub-blockfaces, accounting for alleys and previous demolitions. The ends of these sub-blockfaces were identified as having the highest likelihood of collapse, and these buildings were then visually verified against ConnectExplorer aerial photography.	['Breaking the Divestment Cycle: Predicting Abandonment and Fostering Neighborhood Revitalization in Baltimore\nSince the 1970’s, a certain urban pessimism has pervaded both academic research and public policy on Baltimore. Nothing, it seemed, could be done to stem the tide of divestment. Policy conversations focused on ‘right-sizing’ and even ‘mothballing’ the urban landscape.\nRecently however, there has been a groundswell of enthusiasm for the potential of American cities in general, and Baltimore city in particular. After decades of naysaying, policymakers have begun to realize that urban revitalization is possible. The city has seen a profound shift, away from downtown redevelopment and urban renewal to neighborhood based efforts and infill redevelopment that has shown significant promise.\nDespite this optimism, there remain significant gaps in our knowledge base. Due primarily to data limitations, the dynamics of divestment and reinvestment are not well understood. Surprisingly little is known, for example, about when and why particular properties are abandoned and the degree to which abandonment has a contagious effect on adjacent properties.\nIn Baltimore, these questions take on particular urgency. Not only has decades of decline resulted in 17,000 vacant and abandoned properties, but the nature of Baltimore’s housing stock – primarily brick rowhomes – means that costs of revitalization, stabilization, and demolition are high. By modeling urban divestment and investment, we hope to not only expand our knowledge-base, but to aid the City in effective and efficient program implementation.\nTo close the gap, we created a unique database\nWith seed funding from the Institute of Data-Intensive Engineering and Science (IDIES) we created a rich parcel-level longitudinal database of Baltimore city’s housing stock merging administrative data provided by the Housing Department of Baltimore City. This data allows us not only to consider the efficacy of various housing interventions, such as the city’s Vacants to Value initiative, but also to address larger urban questions – examining empirically the mechanisms of property divestment, neighborhood decline, and renewal.\nThe 21st Century Cities Initiative at the Johns Hopkins University recently provided funding to continue the development of this unique resource and to apply this data to a wide range of urban questions. Researchers from JHU’s Applied Mathematics & Statistics and Sociology departments have come together with the Office of Code Enforcement of Baltimore City to tackle this interdisciplinary project.\nIn the spirit of Jim Gray’s “20 questions”, a set of key problems were identified early on that set the long term goals and spell out the immediate needs to drive our research directions.\nPhase 1 – Predicting Abandonment Investment\nUrban researchers have long hypothesized that property abandonment spreads through a city following a contagion logic: the abandonment of one property in a neighborhood increases the likelihood that proximal properties will also be abandoned (over and above the neighborhood’s market position). Modeling this dynamic process, however, is far more difficult and requires longitudinal parcel-level data that until recently has been unavailable.\nOur approach builds on the previously developed custom database of the geometries of all parcels and buildings in Baltimore, to which new layers of pertinent information can be joined for exploring vacant housing dynamics. Importantly the use of water usage data has allowed us to create a proxy for building occupancy – a key missing piece in modeling abandonment.\nPhase 2 – Evaluating Policy\nBaltimore city’s innovative blight remediation program, Vacants to Value (V2V), was designed to facilitate transfer of blighted properties back into productive use via receivership and disposition. V2V appears to have had a positive effect in certain areas, but separating the effects of the initiative from larger population and housing market trends remains a nontrivial problem. Urban researchers often struggle to employ rigorous statistical analyses in the context of neighborhood revitalization in part because the level of spatial aggregation available to them (the census tract or neighborhood) is generally too coarse to make plausible comparisons. Our data, in contrast, have the advantage of being scale-independent and measured consistently through time, allowing us to employ more robust matching techniques to better approximate causal inference.\nPhase 3 – Modeling Neighborhood Revitalization\nThe foundation we are laying will allow our inquiry to go further than these two phases. As the project progresses, the dataset will allow us to model all manner of urban transitions, not just measure the effects of the V2V interventions.\nRecently Baltimore suffered a fatality caused by a collapsing vacant house. Our team helped to expedite the emergency inspections prompted by this tragedy.\nBuilding on our existing data solution, we integrated building footprint data to look at the divestment from a completely new angle. Using the footprint data, we divided the city’s blocks of rowhomes into contiguous “sub-blockfaces,” accounting for alleys and previous demolitions which leave spaces between rowhomes on the same blocks. The ends of these sub-blockfaces have the highest likelihood of collapse, and were not flagged in any existing database. We visually checked every high-risk building against the ConnectExplorer aerial photography. Over the first weekend, 80 houses were identified as imminent threats and were subsequently demolished.\nSuch emergency demolitions are like putting out fires: they have to be done and done fast. Baltimore’s Planning and Housing Departments are also at the forefront of systematically scheduling demos across the city. When earlier this year, Maryland’s Governor Larry Hogan announced Project C.O.R.E., a new program to address blight, Baltimore City’s strategic demolition program schedule received a welcomed boost.\nIn light of the new funding, Baltimore City began carefully selecting the next round of vacant houses for demolition. We were able to streamline this process by identifying groups of adjacent properties that were entirely abandoned — an ideal demolition scenario that increases cost effectiveness and requires no residential relocation. We identified these properties using a sophisticated database query, which folds in all the required constraints and determines the spatial colocation of row houses.\nToday, as city officials carefully discussed each property, considering community feedback, architectural preservation, strategic planning, and cost effectiveness, they projected a map containing property information culled from diverse administrative dataset including our identification of standalone clusters.\nOf course, the targets identified by our team are only the beginning. All aspects of urban planning need to be considered before tearing down a property. But one building after another, Baltimore City’s strategic demolition program partnering with the Governor’s Project C.O.R.E. will clear the way for new green space, housing and community development.\nMeet the Team\nTamás Budavári is Assistant Professor of Applied Mathematics & Statistics in the Whiting School of Engineering at The Johns Hopkins University, where he focuses on mathematical and computational challenges of big data. He is builder of the Sloan Digital Sky Survey and founding editor of the Astronomy & Computing journal.\nPhilip Garboden is a doctoral student in Sociology and Applied Mathematics & Statistics at The Johns Hopkins University. He holds a masters degree in Public Policy from the same institution. He works as a researcher at the Poverty and Inequality Research Lab. His work focuses on the intersection of housing and urban poverty.\nMichael Braverman is Deputy Commissioner for Permits and Code Enforcement at Baltimore Housing. In that role, he oversees the strategic code enforcement piece of Mayor Rawlings-Blake’s Vacants to Value initiative, leading its innovative receivership and targeted demolition programs. Michael has been asked to share his expertise and passion for well-managed, data-driven government with a variety of cities and with organizations including the Federal Reserve Bank Board of Governors, the Center for Community Progress, and the Clinton Global Initiative. He has a J.D from the City University of New York and a B.A. from the Johns Hopkins University.\nJohn David Evans is Director of Analytics for Permits and Code Enforcement at Baltimore Housing. He develops analytic tools for the evaluation and management of housing programs in Baltimore. John David holds a masters degree in Public Policy from the University of Maryland.\nTalented undergraduate students Surya Ram (Applied Mathematics / Computer Science) and Kevin Wells (Sociology) are responsible for much for the code development and data management.\nLeave your comment below, or reply to others.\nPlease note that this comment section is for thoughtful, on-topic discussions. Admin approval is required for all comments. Your comment may be edited if it contains grammatical errors. Low effort, self-promotional, or impolite comments will be deleted.\nRead more from the Meeting of the Minds Blog\nSpotlighting innovations in urban sustainability and connected technology\nToday, over 2 million Americans are living without access to clean, running water. The newly released ‘Close The Water Gap’ report by DigDeep and the US Water Alliance pulls back the veil on America’s hidden water crisis.\nThis is the first-ever comprehensive look at indoor water access across the United States, and its findings are explosive: Race is the strongest predictor of vulnerability. In six states (plus Puerto Rico), progress is actually backsliding. More than 44 million Americans are served by water systems with recent violations of the Safe Drinking Water Act.\nWhen thinking about conserving water, we should also be focusing on how more efficient water use correlates with energy savings. Studies show that when households participate in water savings programs, they also conserve energy and reduce strain on the power grid during peak demand periods while saving consumers money on their utility bills.\nWater utilities can also dramatically increase their energy efficiency and reduce overall energy usage by adopting locally based solutions. For many municipal governments, drinking water and wastewater treatment plants are typically the largest energy consumers, often accounting for 30 to 40 percent of total energy consumed. Overall, drinking water and wastewater systems account for approximately two percent of energy use in the United States, adding over 45 million tons of greenhouse gases annually.\nAddressing the impact of heat on health is well-aligned with MCDPH’s vision and mission “to make healthy lives possible” by protecting and promoting the health and well-being of MC residents and visitors. The climate has significant impacts on our community’s health. Through extensive surveillance and community surveys, we have demonstrated the importance of local public health data to increase buy-in from new and existing partners and obtain funding to address this significant public health issue. We encourage other health departments to consider the power of data and collaboration as they seek methods for protecting the public’s health from a changing climate.']	['<urn:uuid:10d2e4be-d70b-4c0f-bd72-7e368914721e>']	factoid	direct	long-search-query	similar-to-document	expert	2025-04-14T18:34:38.591736	73	321	11500
51	night mode camera slow shutter speed problems	When using night mode, the very slow shutter speed can cause two problems: ghosting if the subject moves even slightly, and starbursts from streetlamps in the image.	['Night portraiture can be tricky because there are a lot of things to think about, and exposure, flash, camera shake, and colour balance are just the tip of the iceberg. Most modern cameras have settings to help you take great nighttime shots, but why do some turn out looking great while others are a blurry or bright mess?\nTo better understand what’s going on inside the camera, we headed out with a Nikon D90 with a 28mm, f/2.8 lens to try to take the best night portrait shot possible. There’s always more than one method, so we tried several techniques.\nDaytime exposure with the flash turned on\nIf you use a camera’s daytime settings at night, you’ll end up with an underexposed image. The obvious solution is to turn on the flash. In manual mode, when you turn the flash on, it will go off for every shot, regardless of whether it’s needed in the frame.\nPosing near San Francisco City Hall for nighttime shots taken with a Nikon D90 and a 28mm, f/2.8 lens. The bad photo (top) uses a daytime exposure with flash at ISO 320 at f/2.8, with a shutter speed of 1/200 and flash on TTL. The good picture (bottom) has a manual setting of ISO 320 at f/2.8, with a shutter speed of 1/20 and the flash on TTL. The shutter speed made all the difference\nThrough-the-lens (TTL) is the default flash setting for most cameras. It examines the scene before deciding how strong the flash needs to be. This usually means that your foreground will be properly lit, but the camera will light the background based on the exposure settings you set previously.\nAuto mode will take a clean, clear photo of your subject, but it won’t pay any attention to the background. While it may include data from the whole scene, its priority is to expose the point of focus. Auto mode works fine when the subject and background are in the same light, or where the point of focus is what’s important in the frame, but not for a nighttime portrait in front of, for example, a landmark.\nNight portrait mode\nThe night setting on your camera will slow down the shutter speed drastically and set the flash to auto plus slow sync – this tells the camera that you’re taking a slow-exposure photo with flash. The idea behind these settings is that, although the shutter speed is very slow, the burst of the flash will freeze the subject. While this photo will correctly expose both the background and the subject, the colour balance may be too warm. Also, the shutter speed will be so slow that if your subject moves even slightly, there will be ghosting. Another possible problem with such a shot is the starbursts coming from streetlamps – a byproduct of the slow shutter speed.\nA good rule of thumb for manual flash photography is to expose the background, set your flash for TTL and then make the shutter speed faster by two stops. In this case, we instead made the ISO lower, though this achieved the same goal of allowing enough light into the frame to expose the background, yet not enough to blow out the foreground. Because the shutter speed was slightly higher than for the night portrait mode, we were able to avoid starbursts, and the lower ISO minimised noise.']	['<urn:uuid:9bfb60c1-c2b2-4d31-bc66-ca05b014ae58>']	factoid	with-premise	short-search-query	similar-to-document	novice	2025-04-14T18:34:38.591736	45	165	3135
52	What gives natural gas its distinctive rotten egg smell?	The distinctive 'rotten egg' smell of natural gas comes from an odorant called mercaptan that is added to the gas before delivery to help detect any leaks.	['Natural gas is a vital component of the world’s supply of energy. It is one of the cleanest, safest, and most useful of all energy sources. Despite its importance, however, there are many misconceptions about natural gas. For instance, the word ‘gas’ itself has a variety of different uses, and meanings. When we fuel our car, we put ‘gas’ in it. However, the gasoline that goes into your vehicle, while a fossil fuel itself, is very different from natural gas. The ‘gas’ in the common barbecue is actually propane, which, while closely associated and commonly found in natural gas, is not really natural gas itself. While commonly grouped in with other fossil fuels and sources of energy, there are many characteristics of natural gas that make it unique. Below is a bit of background information about natural gas, what it is exactly, how it is formed, and where it is found in nature.\nWhat is Natural Gas?\n|A Natural Gas Wellhead|\n|Source: Duke Energy|\nNatural gas, in itself, might be considered an uninteresting gas – it is colorless, shapeless, and odorless in its pure form. Quite uninteresting – except that natural gas is combustible, abundant in the United States, and when burned it gives off a great deal of energy with fewer emissions than many other sources. Compared to other fossil fuels, natural gas is cleaner burning and emits lower levels of potentially harmful byproducts into the air. We require an ever-increasing supply of energy to heat our homes, cook our food, and generate our electricity. It is this need for energy that has elevated natural gas to such a level of importance in our society, and in our lives.\nNatural gas is a combustible mixture of hydrocarbon gases. While natural gas is formed primarily of methane, it can also include ethane, propane, butane and pentane. The composition of natural gas can vary widely, but below is a chart outlining the typical makeup of natural gas before it is refined.\n|Typical Composition of Natural Gas|\n|Rare gases||A, He, Ne, Xe||trace|\nIn its purest form, such as the natural gas that is delivered to your home, it is almost pure methane. Methane is a molecule made up of one carbon atom and four hydrogen atoms, and is referred to as CH4. The distinctive “rotten egg” smell that we often associate with natural gas is actually an odorant called mercaptan that is added to the gas before it is delivered to the end-user. Mercaptan aids in detecting any leaks.\n|A Methane molecule, CH4|\nNatural gas is considered ‘dry’ when it is almost pure methane, having had most of the other commonly associated hydrocarbons removed. When other hydrocarbons are present, the natural gas is ‘wet’.\nNatural gas has many uses in residential, commercial, and industrial applications; see more on the multiple uses of natural gas here.\nFound in reservoirs underneath the earth, natural gas is often associated with oil deposits. Production companies search for evidence of these reservoirs by using sophisticated technology that helps to find the location of the natural gas, and drill wells in the earth where it is likely to be found. Click on the link to learn more about the new technologies and their environmental impact. Once brought from underground, the natural gas is refined to remove impurities such as water, other gases, sand, and other compounds. Some hydrocarbons are removed and sold separately, including propane and butane. Other impurities are also removed, such as hydrogen sulfide (the refining of which can produce sulfur, which is then sold separately). After refining, the clean natural gas is transmitted through a network of pipelines, thousands of miles of which exist in the United States alone. From these pipelines, natural gas is delivered to its point of use. For more information on how natural gas gets from underneath the ground to its final destination, click here.\nNatural gas can be measured in a number of different ways. As a gas, it can be measured by the volume it takes up at normal temperatures and pressures, commonly expressed in cubic feet. Production and distribution companies commonly measure natural gas in thousands of cubic feet (Mcf), millions of cubic feet (MMcf), or trillions of cubic feet (Tcf). While measuring by volume is useful, natural gas can also be measured by potential energy output. Like other forms of energy, natural gas is commonly measured and expressed in British thermal units (Btu). One Btu is the amount of natural gas that will produce enough energy to heat one pound of water by one degree at normal pressure. To give an idea, one cubic foot of natural gas contains about 1,027 Btus. When natural gas is delivered to a residence, it is measured by the gas utility in ‘therms’ for billing purposes. A therm is equivalent to 100,000 Btu, or just over 97 cubic feet, of natural gas.\nThe Formation of Natural Gas\nNatural gas is a fossil fuel. Like oil and coal, this means that it is, essentially, the remains of plants and animals and microorganisms that lived millions and millions of years ago. But how do these once living organisms become an inanimate mixture of gases?\nThere are many different theories as to the origins of fossil fuels. The most widely accepted theory says that fossil fuels are formed when organic matter (such as the remains of a plant or animal) is compressed under the earth, at very high pressure for a very long time. This is referred to as thermogenic methane. Similar to the formation of oil, thermogenic methane is formed from organic particles that are covered in mud and other sediment. Over time, more and more sediment and mud and other debris are piled on top of the organic matter. This sediment and debris puts a great deal of pressure on the organic matter, which compresses it.\nThis compression, combined with high temperatures found deep underneath the earth, breaks down the carbon bonds in the organic matter. As one gets deeper and deeper under the earth’s crust, the temperature gets higher and higher. At low temperatures (shallower deposits), more oil is produced relative to natural gas. At higher temperatures, however, more natural gas is created, as opposed to oil. That is why natural gas is usually associated with oil in deposits that are 1 to 2 miles below the earth’s crust. Deeper deposits, very far underground, usually contain primarily natural gas, and in many cases, pure methane.\nNatural gas can also be formed through the transformation of organic matter by tiny microorganisms. This type of methane is referred to as biogenic methane. Methanogens, tiny methane-producing microorganisms, chemically break down organic matter to produce methane. These microorganisms are commonly found in areas near the surface of the earth that are void of oxygen. These microorganisms also live in the intestines of most animals, including humans. Formation of methane in this manner usually takes place close to the surface of the earth, and the methane produced is usually lost into the atmosphere. In certain circumstances, however, this methane can be trapped underground, recoverable as natural gas. An example of biogenic methane is landfill gas. Waste-containing landfills produce a relatively large amount of natural gas from the decomposition of the waste materials that they contain. New technologies are allowing this gas to be harvested and used to add to the supply of natural gas.\nA third way in which methane (and natural gas) may be formed is through abiogenic processes. Extremely deep under the earth’s crust, there exist hydrogen-rich gases and carbon molecules. As these gases gradually rise towards the surface of the earth, they may interact with minerals that also exist underground, in the absence of oxygen. This interaction may result in a reaction, forming elements and compounds that are found in the atmosphere (including nitrogen, oxygen, carbon dioxide, argon, and water). If these gases are under very high pressure as they move toward the surface of the earth, they are likely to form methane deposits, similar to thermogenic methane.\nNatural Gas Under the Earth\n|Source: U.S. Energy Information\nAlthough there are several ways that methane, and thus natural gas, may be formed, it is usually found underneath the surface of the earth. As natural gas has a low density, once formed it will rise toward the surface of the earth through loose, shale type rock and other material. Some of this methane will simply rise to the surface and dissipate into the air. However, a great deal of this methane will rise up into geological formations that ‘trap’ the gas under the ground. These formations are made up of layers of porous, sedimentary rock (kind of like a sponge that soaks up and contains the gas), with a denser, impermeable layer of rock on top.\nThis impermeable rock traps the natural gas under the ground. If these formations are large enough, they can trap a great deal of natural gas underground, in what is known as a reservoir. There are a number of different types of these formations, but the most common is created when the impermeable sedimentary rock forms a ‘dome’ shape, like an umbrella that catches all of the natural gas that is floating to the surface.\nThere are a number of ways that this sort of ‘dome’ may be formed. For instance, faults are a common location for oil and natural gas deposits to exist. A fault occurs when the normal sedimentary layers ‘split’ vertically, so that impermeable rock shifts down to trap natural gas in the more permeable limestone or sandstone layers. Essentially, the geological formation, which layers impermeable rock over more porous, oil- and gas-rich sediment, has the potential to form a reservoir. The picture below shows how natural gas and oil can be trapped under impermeable sedimentary rock, in what is known as an anticlinal formation. To successfully bring these fossil fuels to the surface, a hole must be drilled through the impermeable rock to release the fossil fuels under pressure. Note that in reservoirs that contain oil and gas, the gas, being the least dense, is found closest to the surface, with the oil beneath it, typically followed by a certain amount of water. With natural gas trapped under the earth in this fashion, it can be recovered by drilling a hole through the impermeable rock. Gas in these reservoirs is typically under pressure, allowing it to escape from the reservoir on its own.\nIn addition to being found in a traditional reservoir such as the one shown above, natural gas may also be found in other ‘unconventional’ formations. Click here to learn more about unconventional natural gas formations such as shale, which may be seen in a graphic on the right.\nNow that the basics of natural gas as a fossil fuel have been discussed, proceed onto information on the history of natural gas.']	['<urn:uuid:78df3a11-902f-44c6-a638-d530b6546218>']	factoid	with-premise	concise-and-natural	similar-to-document	expert	2025-04-14T18:34:38.591736	56	155	10806
53	best time to explore national parks australia avoid crowds walking trails	It is best to visit national parks in spring and autumn. You should avoid the coast from Christmas to the end of January as well as Easter, as these are school holidays and access to hiking trails is restricted.	"[""You can visit Australia in any season, even if it is better choose the date of your departure according to the climate of the region visited. You can visit any region of Australia in spring and autumn: the climate there will be all the more pleasant. Summer, from December to February, is a very pleasant period in the south, and the winter climate is very pleasant in different regions. Check the climate of your specific destination before you leave.\nThe Northern Territory\nPrefer the dry season from April to September to visit the north and centre of Australia. The north of the country offers exuberant animal life and is the home of Kakadu National Park\n. You will discover the famous city of Alice Springs, a unique centre in the heart of the desert, and its many historical sites. It is the driest and least populated territory of Australia, which makes it absolutely fascinating.\nThis is the region of the surfing beaches of the Gold Coast\nand the tropical forests populated by wild animals! You can visit Queensland in any season. Nevertheless, prefer the European spring and autumn: the temperatures are fine and warm and you can enjoy the beach, whereas box jellyfish make bathing dangerous from December to March. There may also be abundant rainfall during this period. The months of June to September are windy but are the ideal time to observe migrating whales.\nSouthern Australia, Victoria and Tasmania\nIt is best to avoid the coast from Christmas to the end of January as well as at Easter: these are school holidays so the beaches are crowded and access to hiking trails is restricted. The climate is ideal in February and April, as well as in November when the flowers grace the countryside around Melbourne. You can visit Tasmania year round\n. A contributor to world gastronomy, the island abounds with small local productions and offers a very pleasant tour for amateurs of fine cuisine. It can be crowded in December and January: so prefer the months of February, March, April, October and November. During the rest of the year, snowfall can close down some trails.\nNew South Wales\nSydney, the sophisticated, sunshine city and economic capital of Australia, has a lot to offer: you'll love it. Visit the oldest town and the most diverse city of Australia in spring or autumn if you have the choice. There can be a lot of rain between November and March. Admire the architecture of the very famous opera of the most populated city in Australia, discover its skyscrapers but also its many parks and all the other treasures that the city is bursting with.\nAustralia is a vast territory which deserves to be explored from coast to coast, especially for its many wild natural regions. Close to Sydney, the Blue Mountains National Park is a must: a succession of canyons and cliffs as far as the eye can see and eucalyptus forests make for a magnificent landscape. The Flinders Ranges National Park in South Australia is also fascinating. In Tasmania, you can visit Cradle Mountain-Lake St Clair National Park, and observe the wildlife in the Sturt National Park in New South Wales. As always, prefer the spring and autumn for hiking in these beautiful landscapes.\nCascades and waterfalls\nObviously, the Blue Mountains falls to the west of Sydney are the first to come to mind. Don't miss these huge cascades with their wonderful display of rainbows under any pretext. In Tasmania, the less-known Liffey Falls take more time to reach but offer an opportunity for a quiet walk, off the beaten tourist paths. You might even come across some opossums! Still in Tasmania, you will shiver with fright on the suspension bridge of the superb Montezuma Falls. Prefer spring and autumn to discover these falls. Florence Falls and Edith Falls are two must-see waterfalls in the Northern Territory: for the latter, go during the dry season between April and September.\nHead for Western Australia during the dry season, i.e. between April and September. Between wild beaches and absolutely superb nature, the region will take your heart away. Its landscapes are highly diversified and adventurers will jump at the opportunity to trek through this magnificent part of Australia. You can cycle, walk and even climb the trees to observe the nature, there is something for everybody!\nBeaches and sun\nOf course, Bondi Beach in Sydney inevitably comes to mind, where you can dive into the waves or just relax. Wineglass Bay in Tasmania requires a little effort: a climb will take you up to this beautiful expanse of sand. In Queensland, Whitehaven Beach is one of the most notable beaches. In New South Wales, Crowdy Head offers golden sandy beaches as far as the eye can see... You will have understood by now that Australia is a paradise for surfing or swimming. Prefer the summer for this type of trip, i.e. the months of December, January and February, when the sun hits hard and the waters are warm.""]"	['<urn:uuid:75f6ad47-7e7b-4921-86e0-2eb349cc9c56>']	factoid	direct	long-search-query	distant-from-document	novice	2025-04-14T18:34:38.591736	73	211	4902
54	traditional function voices of ancestors udu drums igbo nigeria ceremonies	In Igbo traditions of Nigeria, Udu drums were considered sacred items that served as a medium for spirits, ancestors, and gods. During ritual ceremonies, the deep haunting sounds of the Udu drums were believed to be the 'voices of ancestors'. Traditionally, only Igbo women were allowed to produce Udus and other pottery, as it was considered too dangerous for men. The clay for making these drums had to be collected from sacred locations, and a man's presence in these spots was considered a serious offense that could cause impotence. Today, however, the instrument is widely used in churches and entertainment venues by both men and women, as most Igbos are now Christian.	"['Udu Drum (Kuzeh) Homayoon By DOYEK UDHN\n- Low stock - 10 items left\n- Inventory on the way\nUdu Drum Homayoon Nasiri\'s Signature By DOYEK UDHN\n- Large size\n- Flat Bottom\n- Maker: DOYEK\n- Weight: 2.8 kg\n- It comes with Padded bag and black hoop (Circle)\nIn some African tribes, India & also Iran, clay pots are used as percussion. The clay pot, partly idiophone & partly aerophone, is the same pot that has traditionally been used for carrying & keeping water but has also had the function of a musical instrument. Despite its simple shape, it has significant capability in playing various kinds of rhythms.\nIn Baluchestan, two Kuzehs are played with a metal platter. This combination, the oldest idiophone instrument in Baluchestan, is known as Kuzak & Tal. The Kuzeh is never played separately. Rather, it is always accompanied by a platter. The player can change the pitch by adding water into these clay pots.\nThe Kuzeh of Kerman is Spindle-like with no handhold. They are made in small & large sizes to accompany folk songs. Because of the uneven bottom, the player uses special holders or holds it by hands or legs.\nSince time immemorial, the white clay pot has been common in Hormozgan province. Called Jahleh, this pot is completely round & spherical. To play Jahleh, one palm vertically beats on the throat while the other hand strikes on the body. The sound of the throat is thick while the sound of the body is relatively dry. The combination of these two sounds is marvelous. Jahlehs are played singly or together by a group, though each player never uses more than one Jahleh at a time. No water is added into Jahleh as there is no need to change the pitch.\nThe word Udu means both pottery & peace in the Igbo (a tribe in southeastern Nigeria) language. It has many different names in Nigeria, depending on tribal areas & ceremonies in which it is used. ""Abang mbre"" or ""pot for playing"" is the name generally ascribed to it. The pottery & other sacred items become a medium for spirits, ancestors & gods. The Igbo perform prayers & sacrifices before them to please the spirits & ask them for help & guidance. During these rituals, the deep haunting sounds of the Udu drums are believed to be the ""voices of ancestors"". Traditionally, only Igbo women produce Udus & other pottery, as pottery is too dangerous for men. The needed clay is collected in sacred locations. The presence of a man in those secret spots would be a serious offense & cause him to become impotent. Today, the sounds of musical pots praise God in churches, because nowadays most Igbos are Christian. Even their men fearlessly play it to entertain audiences in bars.\nGhatam in Sanskrit means pot & is one of the oldest percussion instruments used in the Carnatic Music of South India. Its analogue in Rajasthan is known as madga & pani mataqa (water jug). Although the ghatam is the same shape as an ordinary Indian domestic clay pot, it is made specifically to be played as an instrument.\nThe player uses the fingers, thumbs, palms, & heels of the hands to strike the outer surface of the ghatam. It is also suitable for playing fast rhythmic patterns.\nMany famous percussionists all around the world have noticed the peaceful & mystical sound of this instrument. As far as the traditional Iranian music is concerned, the Kuzeh has recently found its way into remarkable positions in various musical blends played by some modern bands, whereby the sound of the Kuzeh reaches the ear more & more frequently.\nAlong with Kuzeh-makers in many different countries, Iranian craftsmen have also begun, in recent years, to manufacture their own as a novel experience. Undoubtedly, the Kuzeh, as a musical instrument with its particular features & capabilities is still far from known to many musicians in the country. Our objective, therefore, is to introduce & popularize it as widely as possible by producing a greater variety of pots with higher musical quality & offering them to those interested in pots, the percussionists in particular.']"	['<urn:uuid:ced7292a-e30d-4ba6-a0ef-913c65b82cfb>']	open-ended	with-premise	long-search-query	similar-to-document	expert	2025-04-14T18:34:38.591736	74	676	4018
55	I drive through congested city traffic every day during rush hour - what's the most effective way to minimize my exposure to harmful emissions?	Avoiding rush hour is the most effective way to lower your exposure to pollution. Sitting in congested traffic leads to high doses of pollution. Alternative options include walking or cycling along quieter backstreets, which also provides additional health benefits.	"['It\'s official - air pollution causes cancer and a QUT researcher is calling on Australian policymakers to do more to save lives.\nThe World Health Organization has reclassified outdoor air pollution as carcinogenic to humans, after their International Agency for Research on Cancer (IARC) concluded there was enough evidence to show it caused lung cancer and increased risks of bladder cancer.\nQuoted in the latest edition of Medical Journal of Australia, QUT Faculty of Health Associate Professor Adrian Barnett said the reclassification was a wake-up call to the Australian government.\n""There is a generally complacent attitude to air quality in Australia, which is based on our air being cleaner than other countries, such as China, and the fact that most air pollutants are invisible and odourless,"" Professor Barnett was quoted as saying in the Medical Journal of Australia\'s November edition. ""But just because we can\'t see the pollutants doesn\'t mean they aren\'t harming us. A lack of any serious policy action has meant that there\'s been no clear improvement in air quality in the last decade in Australia\'s major cities for the two important air pollutants of ozone and particulate matter. Traffic is the major source of pollution in Australian cities, so if we want cleaner air then we either need cleaner cars or fewer cars.""\nProfessor Barnett said it was important to bring the issue to the attention of the public through publications such as the MJA.\n""Australia is relatively clean compared with other countries, but there are places in Australia that will have dangerously high levels of pollutants at times. For example, a busy intersection in the city,"" he said.\n""Air pollution is a complex mix (or soup) of toxins. Some of the more prevalent ones are nitrogen dioxide, ozone, carbon monoxide and particulate matter, and all of these have been associated with negative health effects.\n""An article in last year\'s European Respiratory Journal identified ozone and particulate matter as the two of greatest concern. That\'s worrying for Australia as these are two pollutants that have remained at constant levels over the last decade, whereas other pollutants, such as carbon monoxide, have decreased over time.\n""Clean air is vitally important for health and any new project or infrastructure that makes the air less clean needs to be very carefully examined. We need action on specific dirty sites, but we also need action on common sources such as vehicles which create most of the pollution in our cities.""\nHowever, he said there were actions people could take to reduce the impact of pollution.\n""Avoiding rush hour is probably the biggest thing that the average Australian could do to lower their exposure,"" he said.\n""Sitting in congested traffic is one of the surest ways to get a high dose of pollution. Walking or cycling along quieter backstreets is another option that also has other health benefits.\n""The other bit of advice is keeping yourself healthy and fit in general, as those who suffer from the effects of air pollution are often those with a weakened cardiovascular or respiratory system.""\nThe IRAC report Air Pollution and Cancer reviewed the latest scientific literature to make the reclassification. The IRAC also separately examined findings surrounding particulate matter - a major component of air pollution - and concluded that it was also a ""Group 1"" carcinogen to humans. It also concluded that the findings apply to ""all regions of the world"", no matter the composition of that area\'s pollution.\nMore information: www.mja.com.au/careers/199/9/t… action-air-pollution']"	['<urn:uuid:341294c0-6658-47f2-861a-50001e90e22a>']	factoid	with-premise	verbose-and-natural	distant-from-document	expert	2025-04-14T18:34:38.591736	143	266	3606
56	what makes old fashioned plane engines continue working even without power from battery	Aircraft engines use magnetos, which are self-contained electrical generators that can operate independently from the rest of the electrical system. A magneto consists of coils of wire turning inside a magnetic field, producing high voltage electricity needed for engine ignition. Modern magnetos can generate around 24,000 volts and include components like coils, points, and distributors to create and deliver timed electrical pulses to spark plugs. This independence from the aircraft's electrical system, combined with their potential for high reliability, has made magnetos a long-standing choice for aircraft engine ignition systems.	['The engine the Wright brothers used in their first aeroplane, 113 years ago, had no throttle, spark plugs or carburettor. But it did have one component still found on modern piston engines: a magneto.\nMagnetos are arguably the ultimate legacy aviation component. At heart, it is a simple electrical generator that the 19th century electricity pioneer Michael Faraday would have recognised—engine-driven coils of wire turning inside a magnetic field provided by two horseshoe magnets.\nAs this early spinning-field coil and fixed horseshoe magnet configuration evolved to give the significantly higher voltages required to fire another new invention, the spark plug, the spinning bits and the stationary bits were swapped around. Today’s aircraft magnetos use stationary breaker points (opened by a rotating cam) and a rotating multi-poled ‘permanent’ magnet generating rapid flux changes in a stationary coil.\nThe Wright’s magneto was a simple generator which produced a constant 10 volts at 4 amps, with ignition spark and timing accomplished mechanically at each cylinder. A modern magneto can put out something like 24,000 volts with negligible amps.\nThis simplicity has long made magnetos attractive for aircraft engine ignition systems because it bestows the twin virtues of independence from the rest of the electrical system, and potentially, very high reliability. Modern aircraft magnetos are in effect self-contained high tension ignition modules, incorporating a coil, points and distributor that step up the voltage to a spark-generating level, turn it into timed high tension electrical pulses, and sends it to the appropriate spark plug.\nDespite their essential simplicity, magnetos go wrong often enough for CASA to issue periodic updates to Airworthiness Bulletin 74-005, which is dedicated to identifying defects and failures in aircraft magneto ignition systems in order to keep industry informed. The latest update took place in March.\nAWB 74-005 Issue 4 continues to emphasise a simple precaution for engineers. Make sure the magneto installed on an engine is the correct one for that specific engine. Examples continue to appear of people attempting to install or having installed an incorrect magneto for a particular engine and position.\nFor example, magnetos are typically marked on the manufacturer’s data plate with either ‘L’ or ‘R’. The L or R does not refer to the position on the engine, but to the magnetos direction of rotation. In one instance, a left-hand rotation magneto was installed in the position where a right-hand rotation was specified. The result was a rough running engine with reduced power and the associated possibility of destructive detonation.\nIt is vital to only install serviceable magnetos with the correct part, model and configuration details for the intended engine. The correct magneto configuration should be verified against the approved technical data.\nTheir essential simplicity means magnetos can be overhauled and refurbished many times but some of the components have inherently limited lives. The plastic gears that turn the distributor are among these and eventually, due to heat and fatigue, will suffer from brittleness and susceptibility to fracture as the plastic degrades. When they snap, shear, or degrade into dust, the magneto stops.\nA failed distributor gear can create another problem within the magneto, of electrical arcing, when the mechanism stops turning. This is because the high-tension electricity is still being generated as the magneto continues to operate, and if the electrical energy cannot discharge at the spark plug, it will seek (by arcing and/or burning) another path to earth.\nAny event which places a thermal or impact shock on the engine, such as overheating or prop strike, has the potential to also damage the magneto. Oil contamination can enter a magneto through worn or inadequate magneto drive seals or in mist form, from an engine that has crankcase ventilation problems. Once inside a magneto, engine oil accelerates its failure.\nMagneto drive rubbers or cushions can become hard and brittle over many hours and years of normal operation. Also, it has been found that abnormal torsional engine vibration (e.g. de-tuned crankshaft dampers) may cause magneto drive rubbers to fragment.\nHigh cycle fatigue cracking can begin from small corrosion pits in the magneto shaft or in the area of the Woodruff key. This shaft can also respond to engine vibration which, under certain conditions, may induce a bending or wave motion response typical of shafts rotating at critical speed, making the shafts vulnerable to any surface defect. Shear failure often soon follows.\nMagnetos contain capacitors which are essential to store electric current briefly each time the breaker point opens. Age and/or high temperature may cause the capacitor to change value or break down. The result can include a partial short, which can lower the voltage in the primary coil. Signs of high temperature on the contact spring or severe breaker point erosion are signs of a failing capacitor.\nMagneto repair and overhaul is a specialised craft, requiring intimate knowledge of systems and individual types, along with high standards of inspection, repair and assembly. When having your aircraft’s magnetos overhauled, it pays to ask around and be sure your maintenance repair and overhaul facility is in the top rank. One question you may want to ask is if the shop can re-magnetise the magneto’s ‘permanent’ magnet rotor, because the magnetic power of the rotating magnet declines over time.\nWith various forms of electronic ignition systems now challenging the traditional aircraft engine magneto, everything is set to change again, with almost everything that now moves in the classic magneto becoming stationary.\nIt is conceivable that the recent Part 23 reforms introduced by the US Federal Aviation Administration may free up general aviation engine development and allow the development of a redundant aviation version of the capacitor discharge ignition systems found on most car engines since the 1980s (they are also on some of the Rotax engines popular in recreational aircraft).\nUntil then, the magneto, a link to the Wright brothers found under almost every general aviation cowling, remains a reliable source of ignition, so long as each pair is thoroughly inspected and diligently maintained.']	['<urn:uuid:64b18582-cc51-4ef8-b779-de51bdfe96d0>']	open-ended	direct	long-search-query	distant-from-document	novice	2025-04-14T18:34:38.591736	87	639	6385
57	experienced pier builder here need details lake wingra restoration project cost timing workforce	The pier restoration project at Lake Wingra was funded with nearly $22,000 raised by the USA's Madison Area Conservation Dinner. More than 30 union volunteers from various trades unions donated nearly 200 hours to rebuild and install the formerly abandoned pier. The work was conducted over the course of a cold Wisconsin winter, during which repairs were made including new decking installation, construction of a pier abutment, new sidewalk and steps, and installation of a hand railing.	"[""Dedication Ceremony Set for the New Accessible Pier at Vilas Park\nThursday, September 13, 2018 - 10:22am\nThe City of Madison, the Building Trades Council of South Central Wisconsin (BTC), the Union Sportsmen’s Alliance (USA) and Madison Parks invite the public to a dedication ceremony at 1:00pm on Friday, September 14 at the new accessible fishing pier at Vilas Park, 1501 Vilas Park Drive.\nUnion volunteers from the BTC teamed up with the USA to renovate a formerly abandoned pier and install the renovated, accessible fishing pier on Lake Wingra.\n“This project was a great opportunity for multiple union trades to come together and benefit our local community,” said project leader and BTC of South Central Wisconsin President/Executive Director Dave Branson. “It's rewarding to know that this revitalized pier will provide safe and easy access for all to participate in the sport of fishing.”\nUtilizing nearly $22,000 in funds raised by the USA’s Madison Area Conservation Dinner, union volunteers partnered with the USA and Madison Parks to take the original floating fishing pier, which was sitting in a state of disrepair in one of the City’s materials yards, and restore it for the public’s use.\nEric Knepp, Madison Parks Superintendent conveyed, “Madison Parks and the City of Madison are grateful for the significant efforts by the Building Trades and Union Sportsmen’s Alliance to make our park system more accessible to all residents and visitors.”\nVolunteers coordinated transportation of the pier to one of the local union shops where over the course of the cold, harsh Wisconsin winter repairs were made, including the installation of new decking. In preparation for installation of the renovated, now handicap accessible fishing pier, volunteers and union contractors also designed and constructed a pier abutment as well as a new sidewalk and steps on the edge of Lake Wingra in Madison's Vilas Park, which has greatly increased accessibility to the fishing pier.\nAfter completing the restoration of the pier, it was transported and installed at its new location at Vilas Park. Volunteers installed a new hand railing on the pier to complete this project.\n“This project is an excellent example of the impact that USA’s skilled union volunteers bring to the future of conservation and preserving our outdoor heritage,” said USA Conservation Manager Robert Stroede. “If it weren’t for their dedication to conservation and their community, there is a very good chance that this pier would have never made its way back to the water for the public’s use.”\nMore than 30 union volunteers from Ironworkers (IW) Local 383, International Union of Bricklayers and Allied Craftworkers (BAC) Local 13, United Brotherhood of Carpenters and Joiners of America (UBC) Local 314, International Association of Heat and Frost Insulators and Allied Workers (IAHFIAW) Local 18, International Union of Elevator Constructors (IUEC) Local 132, International Union of Painters and Allied Trades (IUPAT) District Council 7, Laborers’ International Union of North America (LiUNA) Locals 113 and 330 and Association of Sheet Metal, Air, Rail and Transportation Workers (SMART) Local 18 donated nearly 200 hours to rebuild and install the formerly abandoned pier.\n- Media Contact: Ann Shea, Parks Public Information Officer, (608)266-5949, firstname.lastname@example.org""]"	['<urn:uuid:df99a1a3-72e6-4337-a94d-b11a4d58b184>']	open-ended	with-premise	long-search-query	distant-from-document	expert	2025-04-14T18:34:38.591736	96	489	3358
58	How are some states addressing the problem of electric vehicles not paying gas tax while still using the roads?	The State of Washington now taxes EVs $100 per year to offset the lack of gas tax revenue, while Oregon is conducting a large scale test of the Vehicle Mileage Tax, allowing 5000 volunteers to pay by the mile and have their gas tax rebated.	['At the beginning of this series, we described Wardrop’s First Principle, of User Equilibrium. He also had a second principle, of System Optimality, which says: “At equilibrium the average journey time is minimum.” To achieve this requires every traveler to act in accordance with society’s best interest, which as we noted in part 2, is not generally calculable by an individual. This ratio of the total system travel time associated with a user equilibrium traffic pattern and the system optimal travel pattern has been dubbed “The Price of Anarchy” by Tim Roughgarden, who has applied this to computer networks. This number indicates the inefficiency of autonomous (or selfish) control in a system, compared to a theoretically best central control.\nWhen choosing a route, selfish users see the costs they incur, but not the costs they impose on others. This is analogous to the difference between average and social marginal costs in economics. If we somehow persuaded travelers to make route decisions considering the cost they impose on others, their marginal cost, we could achieve a minimal total cost for the system. In economics, the classic theoretical mechanism for this is called a Pigouvian Tax, which charges the polluter for the negative externalities imposed on the pollutee (the difference between the social marginal cost and social average cost). In this case the externality is congestion, or travel time imposed by a vehicle on all other vehicles in excess of what would be borne in the vehicle’s absence. The Pigouvian Tax gains its name from Arthur Pigou, a British economist from the 1920s, who discusses the idea in his text The Economics of Welfare.\nTravelers facing travel times and a Pigouvian Tax might choose a route that satisfies both of Wardrop’s Principles. The User Equilibrium (UE) solution would equal the Social Optimal (SO).\nUsing traffic assignment models we compared system optimal and user equilibrium flows and travel times for the Minneapolis – Saint Paul regional planning network, assuming total traffic flow between origins and destinations were fixed (i.e. unaffected by our distortion of route prices). We found the SO assignment had a 1.7% overall time savings, and a slightly higher average speed (63.2 km/h vs. 61.8 km/h). Perhaps surprisingly, it also had somewhat more total vehicle kilometers traveled (9.37M vs. 9.33M), as drivers had to take longer routes to avoid imposing congestion on others.\nWhile the SO result is better than the UE (it cannot be worse), we might ask “SO What?”. The price of anarchy, letting drivers choose their own routes rather than being centrally directed, is relatively small, under 2 percent. It turns out it is much more important to get people to choose an efficient time of day than to worry about micro-managing which route they select.\nWe could post time-varying prices (just like the HOT lanes of Part 3, or many transit systems which have peak and off-peak fares) to discourage demand when it is highest, and encourage demand at off-peak periods. This is done on some toll facilities now, and other schemes, like the London Congestion Charge, have two prices: free or tolled, depending on time of day. But this can be as refined as we want it, with prices changing every hour, every five minutes, or even continuously. The prices might change in real-time, or change according to a fixed and posted schedule.\nNobel-winning Economist William Vickery developed the first version of the bottleneck model, which showed how varying prices would allow people to trade-off being on-time (at a higher toll) or being early or late (at a lower toll, but a higher cost in what transportation researchers call “schedule delay”).\nThe simplest version of this has two players1. Imagine two boats racing for a canal lock, or, as in the image, two weightlifters trying to get through a narrow door on the London Underground. When they arrive at the same time, only one can make it through first, the other has to wait. The one who makes it through imposed schedule delay on the one who waited. But if they arrived at different times, there would be no direct schedule delay, though one might not get into the canal (or through the door) at their preferred time. If we appropriately price simultaneous arrivals, we will discourage them. While with two players this may be feasible to coordinate with direct communication by saying don’t arrive when the other guy arrives, and negotiating, for 2000 people instead of 2, coordination is better through posted price signals than conversation and negotiation. Prices varying by time-of-day is what congestion pricing is about, putting a higher price on times which are most desired, and lower prices on the less desired times.\nThere are perhaps other ways to achieve this end. On most roads, it is assumed no one owns the travel time, and so we get congestion. If there were some kind of property in the right to travel at a given time, we could auction off this right to the highest bidder, and similarly avoid congestion. This would more closely follow a strategy of establishing property rights to avoid externalities, as suggested by British-American economist Ronald Coase (who is still talking about economics at the age of 102). In the transportation literature, this has come to be known as reservation pricing. Just as one does not expect to be seated when showing up unexpectedly at a popular restaurant that takes reservations, one should not expect to use a high-demand bottleneck facility on the transportation network without making arrangements in advance. Of course it is much more complicated with a real-time system like transportation, and to maximize throughput, it is likely that some queueing is required. This queue ensures there is someone waiting to take advantage of the next gap that opens. The alternative would be that the facility remains under-utilized for part of the time, which has its own costs. Even restaurants that reserve tables sometimes make you wait a little bit, for their immediate convenience, not yours, maximizing the productivity of their staff.\nUnfortunately congestion pricing in any form remains more in the realm of theory than practice. While there are a few Congestion Charging programs: notably Singapore, London, and Stockholm, they are not over a large enough area, or variable enough in prices, to produce an end to congestion. Once many of these are implemented, I expect many cities will look at their peers and copy them, and it will become standard in all large metropolitan areas. But to date, the cases are fairly exceptional: Central Singapore, a city-state governed by a strong Prime Minister, whose family has been in power for five decades; Central London, a city governed at the time by “Red Ken” Livingstone, a radical thinker who was willing to take the political heat for the decision; and Stockholm, which conducted a trial experiment before holding an election to allow residents to vote up or down. Technically the systems all work well, and certainly do reduce congestion compared to the unpriced alternative. Politically they have been difficult to emulate. New York City tried and failed2, and no other US city has been willing to do something quite so radical.\nAnother possible deployment path for congestion pricing is through what is variously called a Vehicle Mileage Tax, or a Mileage-based User Fee. Gas tax revenues, which provide a large share of road funding, have been declining for a long time in the US, both due to leveling off of demand for driving, as well as better fuel economy. The simplest solution is to raise the gas tax, which solves an immediate problem, but not the longer term one. While hybrid gasoline-electric vehicles (like the Toyota Prius) still pay some gas tax, plug-in electrics (like the Tesla, Chevy Volt, or Nissan Leaf) pay almost none. Yet they still use the roads. Although they are presently a small share of the market, that share is likely to grow. Some states are beginning to think about how to charge EVs for the use of roads, just as gasoline-powered vehicles are charged based on a gas tax. Once a device is placed in cars tracking miles traveled (basically just the odometer, though possibly with some locational data to allow prices to vary by location (urban vs. rural) (although it technically feasible to ensure privacy, by not tracking which specific miles are traveled, no one will believe government protestations isn’t tracking them anymore, anyway), that device can also track when those miles are traveled, and vary the rate by time-of-day. The State of Washington now taxes EVs $100 per year to offset the lack of gas tax revenue. Oregon is conducting a large scale test of the Vehicle Mileage Tax, allowing 5000 volunteers to pay by the mile and have their gas tax rebated.\nWe are getting to the point where we can provide incentives and disincentives to efficiently manage road use. The technology exists, it is probably accurate enough. The cost of collecting a new road fee is non-trivial (especially compared with the gas tax, which simply requires an annual check of refinery sales), but the costs should drop with widespread deployment. The benefits are a significant improvement in the management of road use, so that drivers who do not need to travel when roads are congested, will have incentives to avoid those times.\nIf applied correctly, the resulting changes in route choices it will reveal where roads are overbuilt, and where demand, even after pricing, is sufficient to justify new capacity. The most cost-effective thing we can do in transportation is to get the prices right, all else will follow. This requires above all else, field experiments where different strategies are tested and evaluated, and deployment by replicating the successful experiments.\n- Levinson, David (2005) Micro-foundations of Congestion and Pricing: A Game Theory Perspective. Transportation Research part A Volume 39, Issues 7-9 , August-November 2005, Pages 691-704.\n- Zou, Xi and David Levinson (2006) A Multi-Agent Congestion and Pricing Model. Transportmetrica Vol.2, No.3, 2006 pp.237-249.\n- Schaller, Bruce (2005) New York City’s congestion pricing experience and implications for road pricing acceptance in the United States. Transport Policy 17(4) 266-273.']	['<urn:uuid:1967ff8d-542c-4334-a294-0ea9d09e1bc9>']	factoid	direct	verbose-and-natural	similar-to-document	novice	2025-04-14T18:34:38.591736	111	240	10321
59	How many treatment stages does the water purification process involve?	The water waste undergoes three depuration cycles: first a biological treatment through an active sludge aerobic reactor, then ultrafiltration through an MBR (Membrane Biological Reactor), and finally reverse osmosis to remove remaining dissolved salts.	['A deep-rooted need that is still topical: closing the water cycle. A prospect, which means reducing costs, waste and the impact on the territory. As a solution, an innovative treatment plant designed for a paper mill at the heart of Tuscany, which is capable of reducing ground water withdrawals by 50%.\nThe path followed over the years by the Sofidel Group stems from a business concept, which cannot be considered in isolation from the environment where the group works, but rather has to be integrated in this environment respecting it and using its resources in the best way possible. We have gone back over a part of this path together with Riccardo Balducci, who is Sofidel Corporate Environmental Manager. He told us about the recent start of the first «Waste Water Reuse» plant of the group, which was built in the Delicarta paper mill in Porcari, Tuscany (Italy) and produces tissue paper reels.\nA traditional propensity\nPorcari is a small town in the province of Lucca, which is characterised by the presence of several paper mills. The area traditional propensity to this type of production is also due to the large quantities of water available, which is indispensable for the papermaking process. Yet, the strong concentration of industrial activities and decades of exploitation have highlighted the pressing need to reduce the pressure on the environment.\nIn line with the environmental policies of the Sofidel Group to preserve natural resources, the paper mill soon set itself the objective to reduce draining, i.e. ground water withdrawals. «Delicarta had attempted to close the water cycle in the past», says Mr Balducci, «by making full use of available traditional technologies with various tricks, which also implied raising the awareness of its personnel. Once a level of optimization was reached, beyond which traditional methods could no longer be used, we had to resort to more complex technologies, which had in the meantime become more mature and reliable».\nHence the idea of resorting to a treatment plant capable of reusing the production cycle of the waste waters, which had been up to that moment disposed of in the industrial sewage system. Well before the measure, the improvements implemented over the years had made it possible to register water consumption levels at the Delicarta plant below 6 litres drained water per kg paper produced, this being a very good figure even when compared to the Sofidel Group average of 7.6 l/kg in 2013. «Innovative measures were necessary, if we wanted to go beyond these results».\nThreefold water purification\n«In 2012, it became technologically possible to build the water waste reuse» with an overall investment of about 3 million Euro. With the new solution, the water waste outgoing from the cycle undergoes three depuration cycles. «First of all, biological treatment occurs through an active sludge aerobic reactor, which triggers the abatement of the polluting organic load; the water purifier is equipped with an innovative section, which makes it possible to filter water through an ultrafiltration plant». The biological reactor and this ultrafiltration section, which represents the core of the entire plant, make up for the MBR (Membrane Biological Reactor), whose task is to separate solid from liquid waste. «The water coming out of these two steps is already very clean and free from suspended solids, and yet it undergoes a third step. This is represented by reverse osmosis, whose task is to remove the remaining dissolved salts that normally accumulate during the paper production process and whose concentration make the latter very difficult. Removing the salty fraction is thus necessary». Water is now finally purified and its «quality is equal to, or even better than, the quality of water withdrawn from a well or a river» and only at this stage can it be used again in the production cycle.\nLess withdrawals and better quality\nAlthough the plant was started only at the end of 2014, it has already shown tangible benefits. It is estimated that thanks to the new treatment plant the amount of drained water can be reduced by 50%, whereby water consumption has already been optimized. This way, the amount of drained water will go down from 700 000 to 350 000 m3/year.\nAt the same time, the impact on waste water has dramatically diminished bringing about a significant reduction in the costs of the purifier and positive repercussions on the sewage system of the Lucca plain, which thanks to the waste water recovered now bears a much lighter burden with a further positive effect on the area. January data has shown a quantity of drained water, which is already below 4 litres per kg paper, whereby in the paper mill it is hoped that this result is not only made tangible, but even further improved.\nYet, the installation of the Waste Water Reuse technology does not only bring about advantages as regards water supply, but it also exercises considerable influence also in terms of product quality. As Mr. Balducci explains, very hard groundwater is found in the Lucca plain due to chalk deposits, which require the introduction of chemical additives to be solved. The water flowing out from the new purifier, instead, is much fresher and free of salts. This makes it possible to limit the use of chemical products on the one hand and to make the paper production step much more stable on the other. «The entire cycle will thus be cleaner with the advantage that some typical problems in paper production, i.e. fouling or chalk deposits of the paper machine, or even the formation of stains on the final products, can be solved». These results can be, however, verified after at least six months of continuous operation of the machine.\nIn the design stage the paper mill also had to take the issue of increased power consumption into account, with more electricity needed to power the new plant. This increased power consumption affects the overall consumption of the production site. Mr. Balducci says that «although the purifier does not consume a high energy load, it undoubtedly contributes to increasing the energy consumption of the whole site. Its extra-consumptions are below 3%, however, so the power increase is measurable but not so significant». Delicarta had moreover provided to significantly reduce said consumption, with the plant already being brought to a level of considerable efficiency. The paper mill is furthermore equipped with combined power and steam generation plant, which provides for general energy savings and the reduction of emissions in the atmosphere. Furthermore, for some years the plant has consumed the power produced by the power photovoltaic roof cover, which is made up of 30 000 m2 solar photovoltaic panels capable of producing about 2 MW electricity.\nLarger consumption will certainly affect the investment payback time, which will be long, yet this cost has been duly calculated. As Mr Balducci says, after all «our main intention was to experiment a new technology and protect the local environment».\nThe Waste Water Reuse plant at the Porcari site is the very first featuring this type of technology innovation that has been installed in the Sofidel Group, with the owners are currently planning to replicate it in other sites of the group. «We have given us a couple of months for experimentation», says the company manager. «The plant has just been started and we would like first and foremost to check how reliable the technology and its results are; once the necessary checks are made, we will start thinking about a similar plant elsewhere».\nThe project will obviously have to be implemented with the necessary modifications, enabling it to meet the specific needs of each individual site. The existence of any paper mill depends of course on water resources, however «our aim is to replicate the plant first of all in sites, where the cost of water is very high and actual problems of water availability exist – in some countries, water scarcity is a particularly serious problem». A water purification plant like the one in Porcari would enable to reduce both the risk of excessive economic expenditure, and the problem of reduced water availability.\nA balanced choice\nBuilding the plant has been a complex operation, which required overcoming a number of difficulties. Timing has been necessarily long: three years have elapsed from the project idea, which was developed at the beginning of 2012 and followed by the innovation of the economic and financial model, to the start of the plant, which took place at the end of 2014. In this period of time, an important step consisted in the selection of the supplier with whom the project was to be developed. We carried out a careful evaluation, as «we wanted to make sure that we selected a player capable of providing the best technology and the most advanced skills». Our choice went to Degremont who was our main contractor for the entire project and provided guidance to other subcontractors in the project implementation. Subcontractors included also General Electric, which produced the systems for the MBR ultrafiltration section, i.e. the most important part of the entire plant. Degremont has matured experience mainly in the oil, gas and energy sector. The work done with Delicarta was Degremont’s first experience in the tissue paper industry. The company provided for the main engineering part of the plant, whose construction started in August 2013. Times could have been shorter, but the construction phase was slowed down due to the sudden crisis of the building company that had been entrusted with the construction of the reinforced concrete structure; works then resumed with the structure finalized and the building site closed in June of the following year. The start up thus took place at the end of 2014. Since January 2015 the plant has been fully operational.']	['<urn:uuid:f7b94694-13e4-4227-9b9a-615b955c17ee>']	factoid	direct	concise-and-natural	distant-from-document	expert	2025-04-14T18:34:38.591736	70	253	9885
60	What vision did St. Andrew have in the Blachernae Church?	While praying in the Church of the Blachernae in Constantinople, St. Andrew saw the dome of the church open and the Virgin enter. After weeping and praying for the world, she spread her veil over the congregation. The vision showed the Virgin exchanging letters with Christ, interceding on behalf of humanity, while saints and angels led by John the Baptist joined in prayer.	"[""Description In this traditional composition, celebrating the October 1 church feast of the Protecting Veil (Pokrov) of the Mother of God, the 10th-century Byzantine saint Andrew the Fool for Christ's Sake (at the lower right, arm raised) points out to his disciple Epiphanius a vision that came to him while praying in the Church of the Blachernae in Constantinople. He saw the dome of the church open and the Virgin enter and, after weeping and praying for the world, spread her veil over the congregation. In the upper part of the icon, the Virgin, veil in hand, exchanges letters with Christ, interceding with him on behalf of humanity, while a crowd of saints and angels, led by John the Baptist, joins in her prayer. Behind them are the towers of the Church of the Blachernae. The composition differs slightly from the traditional iconography, in which the Virgin occupies the center (rather than the left side, as she does here), with angels holding her veil above her. Although St. Andrew's vision took place in Byzantium, the feast of Pokrov is celebrated mainly in Russia, where it was introduced in the late 12th century and became the most important event of the liturgical year after the twelve major Christological feasts. Because his feast day is also October 1, St. Romanus the Melodist frequently appears in Pokrov compositions. Romanus was a deacon in a church dedicated to the Virgin, and although he had many virtues, he was a poor singer. The other deacons often derided Romanus for his voice, and when one year he was assigned to be the lead singer in the Christmas Vigil, responsible not only for singing but for the composition of the music, he was in great despair. The Virgin, however, appeared to him in a dream, instructing him to swallow a scroll that she gave him (in the lower left corner of the icon). The next day, everyone was amazed at Romanus' angelic voice as he sang beautiful, poetic hymns. This was the beginning of a celebrated and prolific career as a hymnographer. In the lower center of the icon, Romanus stands in the pulpit, holding the text of his hymn to the Virgin, surrounded by the emperor Leo the Great, Patriarch Tarasius of Constantinople, and other saints. The icon's style imitates that of the Stroganov school of icon-painting of the late 16th-early 17th centuries.\n- A Millennium of Christianity: Russian Art from The Walters Art Gallery. The Walters Art Gallery, Baltimore. 1988-1989.\n- Realms of Faith: Medieval and Byzantine Art from the Walters Art Museum. Frist Center for the Visual Arts, Nashville. 2001-2002.\n- Realms of Faith: Medieval and Byzantine Art from the Walters Art Museum. Frist Center for the Visual Arts, Nashville. 2002-2005.\n- Seeing Music in Medieval Manuscripts. The Walters Art Museum, Baltimore. 2014.\nProvenance Henry Walters, Baltimore, prior to 1909, by purchase; Walters Art Museum, 1931, by bequest.\nCredit Acquired by Henry Walters, before 1909\nDownload Image Add to Collection Share on Twitter Share on Facebook Creative Commons License""]"	['<urn:uuid:f7077f6a-d08a-4611-9bd3-23ee5a1e2db0>']	open-ended	with-premise	concise-and-natural	similar-to-document	expert	2025-04-14T18:34:38.591736	57	375	3027
61	How has the Colorado River Delta ecosystem changed over time?	The Colorado River Delta ecosystem has experienced significant degradation. Historically, it consisted of 1.5 million acres of wetlands where the river met the Gulf of California. However, due to supply and demand imbalance, this ecosystem was largely eliminated. Currently, Raise the River, a binational coalition of NGOs, is working with the United States and Mexican federal governments to restore a small fragment of the ecosystem to benefit birds and other wildlife.	['Crisis is brewing on the Colorado River, the result of decades of water use exceeding the river’s water supply, leaving some communities and ecosystems vulnerable—including the Grand Canyon. Climate change is making things worse, shrinking the river. NASA recently released satellite imagery documenting the decline of water stored in Lake Mead, a visual reminder that water users have drained nearly all their reserves and have no choice but to reduce demands. It’s not yet clear whether the seven Colorado River basin states and water users will reach collaborative agreements for voluntary and compensated reductions, whether federal leaders will have to make the hard choices, or whether courts will be the ultimate arbiters deciding who gets to use the water that’s left. It is clear the impact will be a significant reduction in water use, because water to supply all historic uses does not exist.\nThe Colorado River isn’t just a plumbing system for municipalities and agricultural economies, it’s an ecosystem, a living river home to myriad species. Before the reservoirs started to dry up, the clearest indication that supply and demand were out of balance was the elimination of the Colorado River Delta ecosystem, 1.5 million acres of wetlands where the river is supposed to meet the Gulf of California. That story is now a half century old, and Raise the River, a binational coalition of NGOs, has partnered with the United States and Mexican federal governments to restore a small fragment of that ecosystem to improve outcomes for birds and other wildlife.\nThis summer we learned that another Colorado River ecosystem is in trouble. The Grand Canyon sits between the two largest Colorado River reservoirs (in fact, the two largest in the country), uniquely exposed to the water supply crisis. The Grand Canyon may be one of the most iconic of the United States national parks, but it too is vulnerable to the brewing crisis.\nThe Colorado River within the Grand Canyon is managed under the 1992 Grand Canyon Protection Act. The U.S. Bureau of Reclamation and National Park Service, working with partner and stakeholder agencies, have collaborated for decades. Results of this collaboration include improved sediment flows that help maintain sandy beaches used by plants and animals that dwell in the floodplain, as well as by people traveling the canyon by boat. Results also include creation of in-river conditions conducive to maintaining the largest remaining population of Humpback Chub, a fish endemic to the whitewater reaches of deep canyons in the Colorado River Basin. In 1967 the Humpback Chub was listed as an endangered species, and in 2021 it was downlisted to threatened status, indicative of the remarkable efforts to help the species recover.\nYet the Humpback Chub may be in trouble again, vulnerable to predators including smallmouth bass and green sunfish. Water flowing from the shrinking Lake Powell into the Grand Canyon is getting warmer as the reservoir’s water surface drops closer to the turbine intakes on the face of the Glen Canyon Dam. Until recently the water flowing through those intakes, buried deep below the reservoir’s surface, was cold enough to keep predator fish away from passage into the Grand Canyon. In June 2022 research teams first reported detections of the predator fish downstream from Glen Canyon Dam.\nIf you have read this far and are asking yourself why you should be concerned about an endangered fish in the Grand Canyon while water supply challenges threaten drinking water and agricultural production at an unprecedented scale, consider this: as the water surface at Lake Powell continues to drop, there’s a mounting risk that it gets too low for any water to pass through the turbine intakes. Several bypass tubes sit below the turbine intakes, but Reclamation has expressed uncertainty about their ability to pass Colorado River water downstream on an extended basis. Left unchecked, the crisis brewing on the Colorado River won’t just threaten the Humpback Chub, it will threaten every living thing in the Grand Canyon. That includes not only fish, but also a diversity of wildlife including California Condors, Southwestern Willow Flycatchers, bald eagles, and herons, none of which will survive in the Grand Canyon if the river disappears. Already, Colorado River Basin States and Reclamation have agreed to emergency provisions to raise Lake Powell’s elevation through emergency releases from upstream reservoirs and reduced releases from the Glen Canyon Dam, but these measures are temporary and do nothing to balance supply and demand on the Colorado River.\nConsider this: if the Colorado River disappears in the Grand Canyon, there won’t be any float trips through those hallowed red rock walls.\nConsider this: if the Colorado River disappears in the Grand Canyon, there won’t be a Colorado River flowing into Lake Mead, and soon there won’t be a river flowing downstream from Hoover Dam either. Then the crisis is “day zero” with no Colorado River supply downstream in Arizona, California and Mexico, threatening every water user regardless of the seniority of their water right, the economic value of their water use, or their willingness to pay for water.\nHow we adapt to climate change in the Colorado River Basin is of enormous consequence to everyone and everything that depends on the river:\nIn the near term, Reclamation and the National Park Service should develop strategies to protect the incredible progress of the Humpback Chub, relying on physical removal of predator fish, and develop longer-term plans for mechanical strategies such as an engineered temperature control device. While the volume of water to be released from the Glen Canyon Dam cannot be modified for fish, and this year’s crisis situation at the Glen Canyon Dam precludes the flexibility to modify the timing of water releases (the hydrograph), the agencies have an opportunity to explore how 2023 operations could help limit predator fish migration into the territory of the Humpback Chub.\nIn the near term, states and water users should find ways to reduce water use that minimizes harm to vulnerable communities and ecosystems, and develop longer-term strategies that increase operational flexibility to respond to changing conditions in consideration of the full range of values supported by the Colorado River. There is also ample opportunity to use resources provided in the 2021 Bipartisan Infrastructure Law to increase system resilience, including improvements to reduce consumptive uses of water like irrigation infrastructure improvements that help farmers and ranchers and investments in urban conservation and reuse, and investments in native habitat.\nConsider this: we have no choice at the moment but to adapt to the reduced water supply in the Colorado River Basin, as climate change impacts are here for the foreseeable future. But we do have an opportunity to limit how much climate changes in the future, and that requires all of us to insist that our elected leaders not just provide the resources to adapt to this terrible crisis on the Colorado River, but to enact comprehensive policy approaches to reduce carbon emissions and begin the hard work of climate change mitigation.']	['<urn:uuid:122a4733-090d-4d39-8c59-cc13819e0755>']	open-ended	direct	concise-and-natural	similar-to-document	expert	2025-04-14T18:34:38.591736	61	471	7258
62	I'm interested in mythology - which god was said to live in Eridu?	According to Sumerian mythology, Eridu was the home of the god Enki (later known by the Akkadians as Ea), who was believed to have founded the city and lived in the Abzu temple.	"['|Location||Tell Abu Shahrain, Dhi Qar Governorate, Iraq|\n|Area||At most 10 ha (25 acres)|\n|Founded||Approximately 5400 BC|\n|Abandoned||Approximately 600 BC|\n|Official name||Tell Eridu Archaeological Site|\n|Part of||Ahwar of Southern Iraq|\n|Inscription||2016 (40th Session)|\n|Area||33 ha (0.13 sq mi)|\n|Buffer zone||1,069 ha (4.13 sq mi)|\nEridu (Sumerian: 𒉣𒆠, NUN.KI/eridugki; Akkadian: irîtu; modern Arabic: Tell Abu Shahrain) is an archaeological site in southern Mesopotamia (modern Dhi Qar Governorate, Iraq). Eridu was long considered the earliest city in southern Mesopotamia. Located 12 km southwest of Ur, Eridu was the southernmost of a conglomeration of Sumerian cities that grew around temples, almost in sight of one another. These buildings were made of mud brick and built on top of one another. With the temples growing upward and the village growing outward, a larger city was built. In Sumerian mythology, Eridu was originally the home of Enki, later known by the Akkadians as Ea, who was considered to have founded the city. His temple was called E-Abzu, as Enki was believed to live in Abzu, an aquifer from which all life was believed to stem.\nHistory of research\nThe site at Tel Abu Shahrain, near Basra, has been excavated four times. It was initially excavated by John George Taylor in 1855, R. Campbell Thompson in 1918, and H. R. Hall in 1919. Excavation there resumed from 1946 to 1949 under Fuad Safar and Seton Lloyd of the Iraqi Directorate General of Antiquities and Heritage. These archaeological investigations showed that, according to A. Leo Oppenheim, ""eventually the entire south lapsed into stagnation, abandoning the political initiative to the rulers of the northern cities"", probably as a result of increasing salinity produced by continuous irrigation, and the city was abandoned in 600 BC. In 2019, excavations at Eridu were resumed by a joint Italian, French, and Iraqi effort.\nMyth and legend\nIn some, but not all, versions of the Sumerian King List, Eridu is the first of five cities where kingship was received before a flood came over the land. The Sumerian King List mentions two kings of Eridu: Alulim, who ruled for 28,800 years, and Alalngar, who ruled for 36,000 years. Adapa, a man of Eridu, is depicted as an early culture hero. He was considered to have brought civilization to the city during the time of King Alulim.\nIn Sumerian mythology, Eridu was the home of the Abzu temple of the god Enki, the Sumerian counterpart of the Akkadian god Ea, god of deep waters, wisdom and magic. Like all the Sumerian and Babylonian gods, Enki/Ea began as a local god who, according to the later cosmology, came to share the rule of the cosmos with Anu and Enlil. His kingdom was the sweet waters that lay below earth (Sumerian ab=water; zu=far).\nThe stories of Inanna, goddess of Uruk, describe how she had to go to Eridu in order to receive the gifts of civilization. At first Enki, the god of Eridu, attempted to retrieve these sources of his power but later willingly accepted that Uruk now was the centre of the land. This seems to be a mythical reference to the transfer of power northward.\nBabylonian texts talk of the foundation of Eridu by the god Marduk as the first city, ""the holy city, the dwelling of their [the other gods] delight"".\nIn the court of Assyria, special physicians trained in the ancient lore of Eridu, far to the south, foretold the course of sickness from signs and portents on the patient\'s body and offered the appropriate incantations and magical resources as cures.\nEridu appears to be one of the earliest settlement in the region, founded c. 5400 BC, close to the Persian Gulf near the mouth of the Euphrates River. Because of accumulation of silt at the shoreline over the millennia, the remains of Eridu are now some distance from the gulf at Abu Shahrain in Iraq. Excavation has shown that the city was founded on a virgin sand-dune site with no previous occupation. Piotr Steinkeller has hypothesised that the earliest divinity at Eridu was a Goddess, who later emerged as the Earth Goddess Ninhursag (Nin = Lady, Hur = Mountain, Sag = Sacred), with the later growth in Enki as a male divinity the result of a hieros gamos, with a male divinity or functionary of the temple.\nAccording to Gwendolyn Leick, Eridu was formed at the confluence of three separate ecosystems, supporting three distinct lifestyles, that came to an agreement about access to fresh water in a desert environment. The oldest agrarian settlement seems to have been based upon intensive subsistence irrigation agriculture derived from the Samarra culture to the north, characterised by the building of canals, and mud-brick buildings. The fisher-hunter cultures of the Arabian littoral were responsible for the extensive middens along the Arabian shoreline, and may have been the original Sumerians. They seem to have dwelt in reed huts. The third culture that contributed to the building of Eridu were the Semitic-speaking nomadic herders of herds of sheep and goats living in tents in semi-desert areas. All three cultures seem implicated in the earliest levels of the city. The urban settlement was centered on a large temple complex built of mudbrick, within a small depression that allowed water to accumulate.\nKate Fielden reports ""The earliest village settlement (c. 5000 BC) had grown into a substantial city of mudbrick and reed houses by c. 2900 BC, covering 8–10 ha (20–25 acres)"". Mallowan writes that by the Ubaid period, it was as an ""unusually large city"" of an area of approx. 20–25 acres, with a population of ""not less than 4000 souls"". Jacobsen describes that ""Eridu was for all practical purposes abandoned after the Ubaid period"", although it had recovered by Early Dynastic II as there was a Massive Early Dynastic II palace (100 m in each direction) partially excavated there. Ruth Whitehouse called it ""a Major Early Dynastic City"". By c. 2050 BC the city had declined; there is little evidence of occupation after that date. Eighteen superimposed mudbrick temples at the site underlie the unfinished Ziggurat of Amar-Sin (c. 2047–2039 BC). The finding of extensive deposits of fishbones associated with the earliest levels also shows a continuity of the Abzu cult associated later with Enki and Ea.\nEridu was abandoned for long periods, before it was finally deserted and allowed to fall into ruin in the 6th century BC. The encroachment of neighbouring sand dunes, and the rise of a saline water table, set early limits to its agricultural base so in its later Neo-Babylonian development, Eridu was rebuilt as a purely temple site, in honour of its earliest history.\nThe urban nucleus of Eridu was Enki\'s temple, called House of the Aquifer (Cuneiform: 𒂍𒍪 𒀊, E2.ZU.AB; Sumerian: e2-abzu; Akkadian: bītu apsû), which in later history was called House of the Waters (Cuneiform: 𒂍𒇉, E2.LAGAB×HAL; Sumerian: e2-engur; Akkadian: bītu engurru). The name refers to Enki\'s realm. His consort Ninhursag had a nearby temple at Ubaid.\nDuring the Ur III period Ur-Nammu had a ziggurat built over the remains of previous temples.\nAside from Enmerkar of Uruk (as mentioned in the Aratta epics), several later historical Sumerian kings are said in inscriptions found here to have worked on or renewed the e-abzu temple, including Elili of Ur; Ur-Nammu, Shulgi and Amar-Sin of Ur-III, and Nur-Adad of Larsa.\nHouse of the Aquifer (E-Abzu)\n|Level||Date (BC)||Period||Size (m)||Note|\n|XIV||5000–4500||Early Ubaid||-||No structure found|\n|XIII||5000–4500||Early Ubaid||-||No structure found|\n|XII||5000–4500||Early Ubaid||-||No structure found|\n|V||3800–3500||Early Uruk||-||Only platform remains|\n|IV||3800–3500||Early Uruk||-||Only platform remains|\n|III||3800–3500||Early Uruk||-||Only platform remains|\n|II||3500–3200||Early Uruk||-||Only platform remains|\n|I||3200||Early Uruk||-||Only platform remains|\n- Leick, Gwendolyn (2002), ""Mesopotamia: The Invention of the City"" (Penguin). Google Books.\n- Pollard, Elizabeth (2015). Worlds Together, Worlds Apart concise edition. New York: W.W. Norton & Company, Inc. p. 51. ISBN 9780393250930.\n- Taylor, JE (1855), ""Notes on Abu Shahrein and Tell el Lahm"", Journal of the Royal Asiatic Society, 15, pp. 404–15\n- Campbell Thompson, R (1920), ""The British Museum excavations at Abu Shahrain in Mesopotamia in 1918"", Archaeologia, 70, pp. 101–44\n- Hall, HR (1925), ""The Excavations of 1919 at Ur, el-\'Obeid, and Eridu, and the History of Early Babylonia"", Man, 25, Royal Anthropological Institute of Great Britain and Ireland, pp. 1–7\n- Hall, HR (1923), ""Ur and Eridu: The British Museum Excavations of 1919"", Journal of Egyptian Archaeology, 9, pp. 177–95\n- Lloyd, Seton; Shahrein, Abu (1974), ""A Memorandum"", Iraq, 36, pp. 129–38\n- Safar, Fuad; Mustafa, MA; Lloyd, Seton (1981), Eridu, Iraq: Ministry of Culture and Information, State Organization of Antiquites and Heritage\n-  A. Leo Oppenheim, Ancient Mesopotamia: Portrait of a Dead Civilization, Chicago: University of Chicago Press, 1964, revised in 1977\n- Franco D\'Agostino, Anne-Caroline Rendu Loisel, and Philippe Quénet, The first campaign at Eridu, April 2019 (Project AMEr), pp. 65-90, Rivista degli studi orientali : XCIII, 1/2, 2020\n- Marchesi, Gianni. ""The Sumerian King List and the Early History of Mesopotamia"". in M. G. Biga - M. Liverani (eds.), ana turri gimilli: Studi dedicati al Padre Werner R. Mayer, S. J., da amici e allievi (Vicino Oriente - Quaderno 5; Roma), pp. 231-248.\n- ""The Sumerian king list: translation"". etcsl.orinst.ox.ac.uk. Retrieved 2021-07-04.\n- Steinkeller, P., ""On Rulers, Priests, and Sacred Marriage: Tracing the Evolution of Early Sumerian Kingship. In Priests and Officials in the Ancient Near East."" Papers of the Second Colloquium on the Ancient Near East—The City and its Life held at the Middle Eastern Culture Center in Japan (Mitaka, Tokyo), ed. K. Watanabe, pp. 103–137. Heidelberg: Universitätsverlag C. Winter, 1999\n- Leick, Gwendolyn (2001), Mesopotamia: The Invention of the City (Allen Lane)\n- Mallowan, Max (1970), ""The Development of Cities from Al-U\'baid to the end of Uruk 5"" (Cambridge Ancient History)\n- Modelski, ""Archived copy"". Archived from the original on 2014-05-19. Retrieved 2008-07-17.CS1 maint: archived copy as title (link) accessed 17/12/2013\n- Adams, Robert McCormick (1966), The Evolution of Urban Society, (Chicago: University of Chicago Press)\n- Whitehouse, Ruth (1977),The First Cities, (Oxford: Phaidon)\n- Green (1975), pages 180–182\n- P. Delougaz, A Short Investigation of the Temple at Al-\'Ubaid, Iraq, vol. 5, pp. 1-11, 1938\n- AR George, House most high: the temples of ancient Mesopotamia, p. 65, Eisenbrauns, ISBN 0-931464-80-3\n- Green, Margaret Whitney (1975). Eridu in Sumerian Literature. Chicago: University of Chicago.\n- Leick, Gwendolyn (2001). Mesopotamia: The invention of the city. London: Allen Lane. ISBN 0-7139-9198-4.\n- Seton Lloyd, Ur-al \'Ubaid, \'Uqair and Eridu. An Interpretation of Some Evidence from the Flood-Pit, Iraq, British Institute for the Study of Iraq, vol. 22, Ur in Retrospect. In Memory of Sir C. Leonard Woolley, pp. 23–31, (Spring - Autumn, 1960)\n- Oates, Joan, ""Ur and Eridu: the Prehistory"", Iraq, 22\n- Oates, Joan (1960), Ur in Retrospect: In Memory of Sir C. Leonard Woolley, pp. 32–50\n- The Sumerian king list: translation, UK: Oxford\n- Archaeological Photographs from Eridu, IL, USA: Oriental Institute, University of Chicago\n- Reconstruction of Temple at Eridu, Massart, archived from the original on 2007-09-28, retrieved 2008-09-30\n- Recent photos of Eridu, The British Museum\n- Inana and Enki, The Electronic Text Corpus of Sumerian Literature, Oxford UK, JAB, editor: translation\n- Sumerian Eridu City-State and Ubaid Phase 1 (Tell Abu Shahrain) in Iraq The History of the Ancient Near East\n- Iraq launches campaign to secure archaeological sites in Dhi Qar Al-Mashareq 2021-09-24']"	['<urn:uuid:e07baa13-cb88-4fff-8d89-1e912bbfc11a>']	factoid	with-premise	concise-and-natural	similar-to-document	novice	2025-04-14T18:34:38.591736	66	177	11868
63	What shooting modes does the PowerShot G7 X Mark III have?	The PowerShot G7 X Mark III offers multiple shooting modes including Program AE, Shutter priority AE, Aperture priority AE, Manual, Custom, and Hybrid Auto. It also features Single, Continuous, Servo AF/AE, Touch (that can be turned on and off) and Manual Focus modes, with over 30 shooting modes and 15 photo effects in total.	['Canon PowerShot G7 X Mark III Digital Camera\nThe Canon PowerShot G7 X Mark III is the first-class upgrade from the Mark II featuring 20.1 megapixel 1.0 type stacked CMOS Image Sensor and DIGIC 8 processor. With the ISO sensitivity, up to 12800 (Expanded: 25600), and built-in EVF (0.39 type) at 2,360,000 dots and OLED type, creativity is up for a great deal of freedom with both shooting and recording now also in low light conditions.\nBoost the quality of your work with the 8.8 – 36.8 mm (35 mm equivalent: 24 – 120 mm), 4.2x Optical Zoom, ZoomPlus 10x and with Digital Tele-Converter (1.6x or 2.0x) which combined reaches approximately 17x.\nFocus on vlogging\nStand out from the smartphone crowd with the exciting movies modes: 4K (3840 x 2160, 29,97 / 25 fps) up to 9m 59s. Full HD (1920 x 1080, 119.9 / 100 / 59.94 / 50 / 29.97 / 25 fps), HD (1280 x 720, 50 fps – HD) up to 4 GB or 29m 59s and enjoy the vertical movie aspect feature.\nVersatile sizes and formats selection\nBe ready to choose between the best options for image sizes: 3:2 – (RAW, L) 5472 x 3648, (M) 3648 x 2432, (S1) 2736 x 1824, (S2) 2400 x 1600 4:3 – (RAW, L) 4864 x 3648, (M) 3248 x 2432, (S1) 2432 x 1824, (S2) 2112 x 1600 16:9 – (RAW, L) 5472 x 3072, (M) 3648 x 2048, (S1) 2736 x 1536, (S2) 2400 x 1344 1:1 – (RAW, L) 3648 x 3648, (M) 2432 x 2432, (S1) 1824 x 1824, (S2) 1600 x 1600\nCreate content your own way\nExperiment new technics with Single, Continuous, Servo AF/AE, Touch (that can be turned on and off) and Manual Focus modes, the AiAF – 31 points system and the RAW shooting option. Discover the Program AE, Shutter priority AE, Aperture priority AE, Manual, Custom, Hybrid Auto shooting modes among many more.\nCapture your point of view\nOwn the timing of the recording motion 1/8 – 1/2000 s (Movie Mode) 1 – 1/2000 s (Factory default) 30 – 1/25600 s (Electronic shutter) BULB, 30 – 1/2000 s (Total range – varies by shooting mode)\nRely on the Intelligent Image Stabilisation with 5-axis Advanced Dynamic IS & Auto Level to sensibly reduce vibrations even under low light capability on both indoor and outdoor shooting.\nSimple is more\nDiscover over 30 shooting modes and 15 photo effects perfected to enhance your originality and unique inspiration.\nEmbrace the fantastic built-in Bluetooth® integration along with WI-FI system and share your content with your audience wherever you are.\nYou are in control\nFeel free to work comfortably with the tilt 7.5 cm (3.0”) Touchscreen LCD (TFT). 3:2 aspect ratio and 1,040,000 dots that allows you to navigate clearly into the settings.\n- 4К mоvіеѕ & 3.5mm mіс іnрut – Теll уоur ѕtоrу wіth vіdео\n- 120fрѕ Full НD mоvіеѕ – Ѕmооthеr, mоrе dеtаіlеd ѕlоw-mоtіоn\n- 24mm 4.2х zооm – f/1.8-2.8 wіdе-аnglе zооm іn а соmрасt bоdу\n- 20.1 mеgаріхеl 1.0-tуре ѕtасkеd СМОЅ ѕеnѕоr – Fоr ѕuреrb іmаgе quаlіtу\n- Тіlt-uр ѕсrееn – Fоr ѕеlfіеѕ аnd lоw-аnglе ѕhооtіng\n- Вluеtооth аnd Wі-Fі – Аlwауѕ-оn lіnk wіth уоur ѕmаrt dеvісе\n- UЅВ Сhаrgіng – Соnvеnіеnt сhаrgіng whеrеvеr уоu аrе']	['<urn:uuid:0b75ccea-558e-4253-808f-9dc0e8a6ffa5>']	open-ended	direct	concise-and-natural	similar-to-document	novice	2025-04-14T18:34:38.591736	58	327	2991
64	How long have people been studying why fewer women choose engineering?	Research has investigated the gender gap in STEM fields for forty years, examining how explanations for this disparity have evolved over time.	"['Expanding engineering limits - culture, diversity, and gender\nThis guide supports the Stanford University course, ""Expanding Engineering Limits: Culture, Diversity, and Gender"" (ENGR/FEMGEN 311). It serves as a starting point for research on how engineers use theories of intersectionality, design thinking, and workflows to cultivate inclusive, flexible work cultures that produce sustainable and ethical solutions to engineering problems.\nTable of Contents\nIn this guide, the term workforce is used to categorize resources that relate to the education, the management, and the cultures of the people who are training for and work in engineering and technology fields. Resources under this heading include a broad array of topics that expand perspectives on diversity, intersectionality and gender dynamics in engineering workplaces, ecucational settings and institutions. Subtopics include: human relations, management sectors, race, gender, gender identity and institutional change that fosters cultures of inclusion and workplace atmospheres that attend to human dignity.\nKeywords for searching\nbias, belonging, ethnicity, gender, gender identity, LGBTQ, opportunities, organizational culture, personhood, preparation/education, race, stereotype/ depiction of..,socioeconomic class, workforce dynamics.\nSelect journal articles\nAyre, Mary, Julie Mills, and Judith Gill. 2013. ""\'Yes, I Do Belong’: The Women Who Stay in Engineering."" Engineering Studies 5 (3): 216–32. doi:10.1080/19378629.2013.855781.\nBuzzanell, Patrice M., Ziyu Long, Lindsey B. Anderson, Klod Kokini, and Jennifer C. Batra. 2015. ""Mentoring in Academe A Feminist Poststructural Lens on Stories of Women Engineering Faculty of Color."" Management Communication Quarterly 29 (3): 440–57. doi:10.1177/0893318915574311.\nCorrell, Shelley J., Stephen Benard, and In Paik. 2007. ""Getting a Job: Is There a Motherhood Penalty?"" American Journal of Sociology 112 (5): 1297–1338. doi:10.1086/511799.\nDobbin, Frank, and Alexandra Kalev. 2016. ""Why Diversity Programs Fail."" Harvard Business Review. July 1. https://hbr.org/2016/07/why-diversity-programs-fail.\nFaulkner, Wendy. 2009. ""Doing Gender in Engineering Workplace Cultures. I. Observations from the Field."" Engineering Studies 1 (1): 3–18. doi:10.1080/19378620902721322.\nHughes, Roxanne M. 2012. ""The Process of Choosing Science, Technology, Engineering, and Mathematics Careers by Undergraduate Women: A Narrative Life History Analysis."" ProQuest Information & Learning (US). http://search.proquest.com/docview/941026627/abstract/744D0C3C7A1B4553PQ/28.\nKanny, M. A., L. J. Sax, and T. A. Riggers-Pieh. 2014. ""Investigating Forty Years of Stem Research: How Explanations for the Gender Gap Have Evolved over Time."" Journal of Women and Minorities in Science and Engineering 20 (2). doi:10.1615/JWomenMinorScienEng.2014007246.\nMihaljević-Brandt, Helena, Lucía Santamaría, and Marco Tullney. 2016. ""The Effect of Gender in the Publication Patterns in Mathematics."" PLOS ONE 11 (10): e0165367. doi:10.1371/journal.pone.0165367.\nNathan, Max, and Neil Lee. 2013. ""Cultural Diversity, Innovation, and Entrepreneurship: Firm-Level Evidence from London."" Economic Geography 89 (4): 367–94. doi:10.1111/ecge.12016.\nNewcomer, Jason M., Patti J. Clark, Dixie K. Button, and Linda V. Weiland. 2016. “Gender Diversity in Aircraft Maintenance: A Cross-Sectional Triangulation of Male Perspectives.” Journal of Gender Studies 0 (0): 1–13. doi:10.1080/09589236.2016.1243046.\nRivera, Lauren A., and András Tilcsik. 2016. “Class Advantage, Commitment Penalty: The Gendered Effect of Social Class Signals in an Elite Labor Market.” American Sociological Review 81 (6): 1097–1131. doi:10.1177/0003122416668154.\nSax, Linda J., M. Allison Kanny, Jerry A. Jacobs, Hannah Whang, Dayna S. Weintraub, and Amber Hroch. 2016. “Understanding the Changing Dynamics of the Gender Gap in Undergraduate Engineering Majors: 1971-2011.” Research in Higher Education 57 (5): 570–600. doi:http://dx.doi.org/10.1007/s11162-015-9396-5.\nStachowiak, Dana M. 2016. “Queering It Up, Strutting Our Threads, and Baring Our Souls: Genderqueer Individuals Negotiating Social and Felt Sense of Gender.” Journal of Gender Studies 0 (0): 1–12. doi:10.1080/09589236.2016.1150817.\nWilliams, Joan C. 2014. “Hacking Tech’s Diversity Problem.” Harvard Business Review. October 1. https://hbr.org/2014/10/hacking-techs-diversity-problem.\n“Women in Science: Quarterly Thematic Publication, Issue I, March 2015; 2015.” 2016. Accessed December 9. http://unesdoc.unesco.org/Ulis/cgi-bin/ulis.pl?catno=235155&set=005633CC2A_1_89&gp=0&lin=1&ll=3.\nThis section includes resources that point to concepts of inclusion within design theory that unpack conscious considerations for diverse poplutions, including race, ethnicity, gender, gender identity, cultures, and abilities as an integral part of the design process. These sources point to the methodologies that intend to solve engineering problems through the utilitzation of desgin thinking and social science methods. It includes resources on ethnographic field work for understanding the communities of creators and users of an engineering solution as an integrated part of the innovation and production process.\nKeywords for searching\ncritical thinking, design process, design thinking, engineering design- socal aspects, iterative process, interactive process, problem solving, teams-teamwork, usability - use, usefulness, versioning, visualization.\nBairaktarova, Diana, William Z. Bernstein, Tahira Reid, and Karthik Ramani. 2016. “Beyond Surface Knowledge: An Exploration of How Empathic Design Techniques Enhances Engineers Understanding of Users’ Needs.” International Journal of Engineering Education 32 (1): 111–22.\nde, Andrade Romualdo. 2016. ""Team Igniter an Adaptive Toolkit to Guide and Leverage Collaboration in Teams Seeking to Problem-Solve and Innovate."" Order No. 10117700, Rochester Institute of Technology. https://search.proquest.com/docview/1802850242?accountid=14026.\nKaygan, Pınar. 2016. “Gender, Technology, and the Designer’s Work: A Feminist Review.” Design and Culture 8 (2): 235–52. doi:10.1080/17547075.2016.1172862.\nOwen, Charles. 2007. “Design Thinking: Notes on ins nature and Use”. Design Research Quarterly, Vol. 2, No. 1, pp. 16-27.\nRazzouk, Rim, and Valerie Shute. 2012. “What Is Design Thinking and Why Is It Important?” Review of Educational Research 82 (3): 330–48. doi:10.3102/0034654312457429.\nWalton, Gregory M., Christine Logel, Jennifer M. Peach, Steven J. Spencer, and Mark P. Zanna. 2015. “Two Brief Interventions to Mitigate a ‘chilly Climate’ Transform Women’s Experience, Relationships, and Achievement in Engineering.” Journal of Educational Psychology 107 (2): 468–85. doi:10.1037/a0037461.\nMethods & Theory\nThis section has resources on social science methods and theory. This section is intended to be of use to engineers and engineering students who want to include social science methodologies and theories into their research projects for problem solving.\nKeywords for searching\ndiversity (race & gender identities, socioeconomic status),empathy, ethnography, evaluation, feminism, inclusion, intersectionality, observation, participant observation, quality assessments, survey(s).\nGerdes, Anne. 2014. “What and Whose Values in Design?: The Challenge of Incorporating Ethics into Collaborative Prototyping.” Journal of Information, Communication and Ethics in Society 12 (1): 18–20. doi:10.1108/JICES-11-2013-0054.\nHeilbroner, Robert L. 1967. “Do Machines Make History?” Technology and Culture 8 (3): 335–45. doi:10.2307/3101719.\nJoerges, Bernward. 1999. “Do Politics Have Artefacts?” Social Studies of Science 29 (3): 411–31. doi:10.1177/030631299029003004.\nRosser, Sue Vilhauer. 2005. “Through the Lenses of Feminist Theory: Focus on Women and Information Technology.” Frontiers: A Journal of Women Studies 26 (1): 1–23. doi:10.1353/fro.2005.0015.\nVaughan, Diane. 1990. “Autonomy, Interdependence, and Social Control: NASA and the Space Shuttle Challenger.” Administrative Science Quarterly 35 (2): 225–57. doi:10.2307/2393390.\nWestbrook, Laurel, and Aliya Saperstein. 2015. “New Categories Are Not Enough Rethinking the Measurement of Sex and Gender in Social Surveys.” Gender & Society 29 (4): 534–60. doi:10.1177/0891243215584758.\nWinner, Langdon. 1980. “Do Artifacts Have Politics?” Daedalus 109 (1): 121–36. Accessed at: http://www.jstor.org/stable/20024652\nWoolgar, Steve, and Geoff Cooper. 1999. “Do Artefacts Have Ambivalence? Moses’ Bridges, Winner’s Bridges and Other Urban Legends in S&TS.” Social Studies of Science 29 (3): 433–49. Accessed at http://www.jstor.org/stable/285412\nHuman subject research & IRB\nHuman subject research and IRB (Institutional Review Board)\nIn the webpage you will find information about the guidelines for using human subjects for non-medical research.\nThis is a mixture of selected research databases that cover social science and science & engineering related disciplines with content on gender and diversity topics.\n- Volume 1. Women in science and technology\n- Volume 2. Body Politics : science and medicine define sex and gender\n- Volume 3. Gender bias in science and technology\n- Volume 4. Gendered innovations : creating science and technology']"	['<urn:uuid:78cd5cde-c47d-4a1f-b9bf-bdb9066c0d7e>']	factoid	direct	concise-and-natural	distant-from-document	novice	2025-04-14T18:34:38.591736	70	142	9258
65	What is the purpose of the Genetic and Rare Diseases Information Center?	GARD employs experienced information specialists to answer questions from the general public, including patients and their families, health care professionals, and biomedical researchers.	"['ASHG, along with partner organizations, develops educational materials for members and other human genetics specialists. These include webinars, online courses, infographics, and other resources. By sharing the latest information and advances, the Society hopes to provide the genetics community with opportunities to connect with and learn from one another, as well as build upon findings to spur continued progress in the field.\nTake Initiative: DNA Day Engagement and You (February 21, 2019)\nFrom Gene to Therapy (December 4, 2018)\nMatchmaker Exchange: Improving Diagnosis to Identify Novel Genes (July 24, 2018)\nTechnical and Validity Measures for NGS Data - Community Tools and Standards (February 24, 2016)\nStandardization for NGS Testing - Issues of Clinical Validity and Utility (September 1, 2015)\nPrenatal Cell-free DNA Screening Program (August 7, 2017):\nSeries: DNA Day \'15 for 15\' (April 25, 2018)\nSeries: Prenatal cfDNA Screening (August 7, 2017)\nSeries: Pediatric Genetic Testing (March 24, 2015)\nASHG has partnered with ReachMD to develop the interview series Genetically Speaking. These 15-minute interviews feature leading experts in clinical genetics care and research.\nASHG thanks its corporate sponsors for supporting its educational programming for health professionals.\nGuide to Interpreting Genomic Reports: A Genomics Toolkit (CSER Consortium; February 2017)\nA guide to genomic test results for non-genetics providers.\nClick here for a downloadable pdf.\nThe American College of Medical Genetics and Genomics (ACMG)\nACMG is responsible for providing education, resources and a voice for the medical genetics profession. ACMG also strives to make genetic services available to the public to improve health.\nNational Society of Genetic Counselors (NSGC)\nThe NSGC has patient-focused information about genetic counseling, genetic testing, and genetic conditions.\nAssociation of Professors of Human and Medical Genetics (APHMG)\nAPHMG is a group of organizations that promotes health and medical genetics educational programs in North American medical and graduate schools. More than ninety institutions are now official members of the Association.\nCenters for Disease Control and Prevention (CDC)- Public Health Genomics\nThe CDC website provides a variety of materials and resources on topics in genomics for health professionals and the general public, including information on genomics and disease, family history tools, and a public health genomics knowledge base.\nThe American Medical Association (AMA)- Genetics and Personalized Medicine\nThe AMA website provides a wealth of information and resources related to genetics and personalized medicine. These include CME programs, information about the precision medicine initiative, and tools for gathering family history information.\nThe American Academy of Family Physicians (AAFP)- AFP by topic, Genetics\nThe American Family Physicians Genetics page has a collection of the best content from AFP, as identified by AFP editors on genetics and related issues.\nGenetic Alliance: ""Understanding Genetics: A Guide for Patients and Professionals""\nThe Genetic Alliance (a non-profit organization that supports genetics policy and advocacy efforts) created an online guide to understanding genetics that covers basic scientific concepts and provides information about diagnoses of genetic conditions, family-history gathering, newborn screening, genetic counseling and genetic testing types and applications.\nThe Jackson Laboratory Clinical and Continuing Education in Genomics\nA collection of educational programs addressing specific knowledge gaps or barriers to integrating genomics into clinical practice, including courses, education on specific clinical topics and educational materials from the National Coalition of Health Professional Education in Genetics (NCHPEG).\nGenetics/Genomics Competency Center (G2C2)\nThis website contains genomics educational materials for clinical practice or the classroom. The resources are organized by discipline and are curated. The mission of G2C2 is to provide high quality educational resources for group instruction or self-directed learning in genetics/genomics by health care educators and practitioners.\nGlobal Genetics and Genomics Community (G3C)\nA collection of interactive cases that demonstrate how genetics and genomics link to health and illness. CME and CNE credits are available.\nGenetic and Rare Diseases Information Center (GARD)\nEstablished by the National Human Genome Research Institute (NHGRI) and the Office of Rare Diseases (ORD), GARD employs experienced information specialists to answer questions from the general public – including patients and their families, health care professionals, and biomedical researchers.\nGenetics Home Reference at the National Library of Medicine\nAn online guide to understanding genetic conditions that provides consumer-friendly information about the effects of genetic variations on human health.\nAn international point-of-care resource for clinicians providing clinically relevant and medically actionable information for inherited conditions covering diagnosis, management, and genetic counseling information.\nMarch of Dimes: Medical resources\nThe resources on this website provide information about preconception health, prematurity, newborn screening, and other topics related to newborn health, as well as information about prenatal testing and specific birth defects.\nPharmacogenetics: Clinical Pharmacogenetics Implementation Consortium Guidelines\nPractice Guidelines and Policy Statements from the American College of Medical Genetics and Genomics (ACMG)\nNational Comprehensive Cancer Network (NCCN) - Hereditary cancer syndromes\nGenetics in Healthcare Practice - Policy Statements\nAmerican Society of Human Genetics (ASHG)\nGenome Statute and Legislation Database at the National Human Genome Research Institute\nUpdated monthly and searchable by year, topic and/or state\nNational Society of Genetic Counselors (NSGC)\nLink to locating a genetic counselor\nAmerican College of Medical Genetics and Genomics (ACMG)\nA search engine for finding medical genetics clinical services\nPersonalized Medicine Coalition (PMC)\nThis independent, non-profit group works to advance the understanding and adoption of personalized medicine for the ultimate benefit of patients.\nFDA: Table of Pharmacogenomic Biomarkers in Drug Labeling\nA list of FDA-approved drugs with pharmacogenomic information in their labeling\nThe Pharmacogenomics Knowledgebase (PharmGKB)\nA comprehensive resource the curates knowledge about the impact of genetic variation on drug response for clinicians and researchers.\nOnline Mendelian Inheritance in Man (OMIM)\nA comprehensive compendium of human genes and genetic phenotypes.\nGTR: Genetic test registry\nHoused at NCBI, a central location for voluntary submission of genetic test information by providers. Includes test\'s purpose, methodology, validity, evidence of test\'s usefulness and laboratory contacts and credentials.\nU.S. Surgeon General\'s Family History Initiative\nThis site features disease-specific health risk fact sheets written specifically for primary care physicians, which encourage physicians to discuss family history information with their patients, as well as the Surgeon General\'s Family Health Portrait Tool to collect information from patients.\nGenetic Alliance: ""Does it Run in the Family?"" Toolkit\nThis toolkit allows users to create tailored family health history tools, with the goal of promoting conversations about health within the family and translating knowledge of family health history into healthy choices.\nGenetics and Health Impact Blog (CDC)\nComments from a public health perspective on genomic research and programs conducted by the CDC and other institutions around the world\nDNA Science Blog: Genetics in Context (PLOS)\nGeneticist blogs from the cutting edge in genomics, including genetic testing, stem cells, gene therapy and genetic disorders\nNational Society of Genetic Counselors Blog\nThis blog provides easy-to-understand information about current topics in genetic testing\nGenetics in Medicine GENEPOD\nPodcast from the official journal of the American College of Medical Genetics and Genomics\nPodcasts from the Naked Scientists (University of Cambridge)\nDNA Today Podcast\nA genetics radio show and podcast to educate the public on genetic and public health topics']"	['<urn:uuid:2d0eb103-4e1c-4ec7-85e3-23b1a9be1380>']	factoid	direct	concise-and-natural	similar-to-document	expert	2025-04-14T18:34:38.591736	72	187	8396
66	child development and learning phases ages transition from birth until adulthood need comprehensive explanation	Development from birth to adulthood occurs across four distinct planes: First plane (0-6 years) where children have an 'Absorbent Mind' - unconscious from 0-3 and conscious from 3-6. Second plane (6-12 years) is a calm period of intellectual growth and reasoning. Third plane (12-18 years) brings dramatic physical and psychological changes as social identity forms. Fourth plane (18-24 years) consolidates previous learning as young adults transition to independence. The first and third planes show evident changes while second and fourth are calmer periods of refinement. These stages each have unique characteristics and needs that must be supported differently for optimal development.	"['by Cornelia Tosch, February 2018\nWhat is a child about? How can a helpless baby become a full member of our society? Maria Montessori’s understanding was that “education” becomes an “aid to life” - a natural process all children undertake spontaneously, guided through ever evolving stages of development. Among others, she followed with her thoughts, former theories & thoughts of Jean-Jacques Rousseau (1712-1778) who was convinced that the development of a child is following an inner program of four different steps. The program is universal as it can be seen as part of the nature of the human being.\nMaria Montessori had a profound respect for the child. She considered children as fellow human beings, not as inferiors. She wrote,\nI have found that in his development, the child passes through certain phases, each of which has its own particular needs. The characteristics of each are so different that the passages from one phase to another have been described by certain psychologists as ‘rebirths’.\n(Montessori, “The Four Planes of Education”, p.1, reprinted in 2004)\nMaria Montessori termed the stages from birth to adulthood “The Four Planes of Development” and follows a holistic view as she considers the cognitive, biological, social and moral changes of the individual from birth to the age of 24.\n""The child\'s development follows a path of successive stages of independence, and our knowledge of this must guide us in our behavior towards him. We have to help the child to act, will and think for himself. This is the art of serving the spirit, an art which can be practiced to perfection only when working among children."" (The Absorbent Mind, p. 257)\nThese stages address the ways that the child in his personality, cognitive ability, and behavior changes during each distinct phase. Understanding the characteristics and needs of the child at each stage allows the adult to support the natural unfolding of life. The child becomes the active agent, and the adult is there to support him. The Montessori approach is the same at all ages, but the applications of the approach must change, according to the child’s needs.\nThe overall, ""Development is a series of rebirths"" (The Absorbent Mind, p. 17) and life follows a constructive rhythm. As indicated in figure 1, especially the first and the third stage are characterized by evident changes in the child and therefore marked in red triangles. The blue triangle is showing calmer periods with more latent development, less dynamic, more about a refinement. The first and second planes form the years of childhood, and the third and fourth planes form the passage into adulthood. The first plane is for the formation, or creation, of the individual and the second plane is for the development of the individual. The third plane brings another creation, the adult in society and the fourth plane develops that creation. In addition, the first and third plane contain of two sub-phases, each lasting about three years. In general, the first three years of each plane are for creation, and the last three years for development, or “crystallization.”\nWhile Maria Montessori puts the strongest emphasis on the first 3 years of life, these are periods that are normally less valued and educationally emphasized in the current school systems. They are putting highest educational input and effort on the time of university which is completely the opposite of Maria Montessori’s interpretation.\nThe First Plane of Development (0-6 years)\nThe first plane of development occurs from birth to around six years of age. It is divided into two sub-planes, (0-3) and (3-6).\nAt birth, a human baby is virtually helpless. At this stage, “it is not a question of development, but of creation from nothing”\n(Montessori, M., Education for a New World, “Periods and the Nature of the Absorbent Mind,”).\nThe First Sub-Plane (birth to three years), the child as the “Spiritual embryo / Unconscious creator”\nThis is the most critical period of development and the greatest changes take place during the first 3 years of life. In these three years, the child has what is called the unconscious Absorbent Mind. This means that he is not conscious of his actions and reactions. He does not act on a willed choice, nor has a conscious memory.\nBrain research classifies the brain in three parts: 1) a rational brain, 2) the limbic system and 3) the reptile brain. The discovery of the child in the first years is massively influencing how many and where synapsis is built. And the interaction with the parents are clearly influencing the emotional development of a child as well. In the didactic triangle of child, environment and parents/adults, we must not neglect that only a child which has secure feelings, can learn peacefully and discover the world to its full extent.\nThe Second Sub-Plane (three to six years) the child as “Conscious Creator/Worker.”\nAt this stage, the child still has the Absorbent Mind, but he is now in the stage of the conscious Absorbent Mind. The child can interact with his environment consciously and deliberately and realizes that he is learning. His work is joyful, purposeful activity; he is always busy. He is conscious of his thoughts, and the fact that he can think for himself. He begins to develop self-mastery and self-control. He can only learn through his own experiences, by doing this with his own hands, the instrument of his mind. This is the period of crystallization in the first plane.\nThe Second Plane of Development (6-12 years)\nCompared to the first plane of development, the years from ages six to twelve are a relatively calm and stable period. There is a great deal of intellectual work that takes place due to an unlimited curiosity but not the intense amount of physical and mental growth that took place in the first plane. The child has mastered the basic human skills. He is socially adapted to his culture. He now learns through reasoning, using his imagination and logic to explore areas of study. He wants to come to his own conclusion, wants to know how things work. The child at this age has an insatiably inquisitive mind. The question they asked in the first plane was, “What is it?” but now they ask, “Why is it?” “Why do these things happen?”, “What if they don’t happen?”, “How does the world work?”. They understand past, present and future. The child is intensely interested in learning about everything and he is able to feel the invisible. Just as the first plane of development was for absorbing the environment through the senses, the second plane is for understanding how all these components in the world work together. It is a so-called “cosmic education” = discover the world and universe!\nTo do so, children love to work in projects and discover deeply each topic through research. I discovered this myself with my 6 years old daughter. We were reading as a bed-time story “Asterix et Cleopatre” and consequently we did a whole week of research on the old Roman imperium, old Egypt, hieroglyphs, Cleopatra and Cesar, why did she die, which snake poisoned her and why, how many snakes are there nowadays, where to find Old Egypt, what happened to both imperia etc…. We are still in the process of researching more and more!\nDuring this plane, the child is in the sensitive period for peer identity and it is crucial for him to be accepted as a member of a group. They seek this partly as well outside of the family in the so-called peer-group. This plane also brings up a sensitive period for developing ethics and morals. The child is intensely interested in the concepts of justice, fairness and has a keen awareness of injustice. He is much more interested in heroes, famous people etc.\nThe Third Plane of Development (12-18 years)\nThis plane of development brings another “rebirth”, but this time it is not the birth of an individual, but the birth of social man. This is the time, says Montessori, “When the social man is created but has not yet reached full development.” . We can compare the third plane of development to the first, because it is a time of great transformation, both physically and psychologically and the first sub-plane (12-15 years) is a time of greater change than the second sub-plane (15-18 years).\nThe physical changes that accompany puberty are rapid and dramatic. The body is getting weaker, clumsier, less dynamic because of all the changes and children are not inclined to great energy. They like to sleep late.\nEmotionally the child finds himself in a period full of questioning. He finds himself in an in-between status: neither child anymore, nor adult. He is emotionally inconsistent, full of contradiction in its feelings, full of doubts and might show consequently very strong reactions. He is very vulnerable and criticism is hard to accept. It is a period of confusion, in search of their identity and including many questions about love, work, life, peace.\nAnd there is also a psychological change from the child in the family to the adult in society. They need to be responsible for the group they live in. Maria Montessori used the term Erdkinder to describe adolescents who are preparing to enter the larger, global community. Mentally, they have developed logical thinking, but they do not want to be pressured into learning facts. Consequently, academic learning should be connected to real life skills: gardening, cooking, sewing, fixing/repairing, etc. Maria Montessori was never able to create such a school, but she communicated her ideas about it. In this protected environment, under adult supervision, adolescents would be prepared for living on their own as part of the society. This would satisfy their strong need for independence while helping them learn that a harmonious society consists of inter-dependent people.\n""Therefore, work on the land is an introduction both to nature and to civilization and gives a limitless field for scientific and historic studies. If the produce can be used commercially this brings in the fundamental mechanism of society, that of production and exchange, on which economic life is based. This means that there is an opportunity to learn both academically and through actual experience what are the elements of social life. We have called these children the ""Erdkinder"" because they are learning about civilization through its origin in agriculture. They are the ""land-children.""\n(Montessori, M., Childhood to Adolescence, p. 68)\nMaria Montessori was ahead of her time. When using MRI technology, researchers are able to prove that certain areas of the brain grow and change during adolescence. The prefrontal cortex, the “rational brain” is not fully developed until well into early adulthood. In fact, what we are seeing is an “exuberance” of synaptic growth, second only to the amazing synaptic growth taking place in the first sub-plane between birth and the age of three. It is this exuberance, or over production of synapses that causes the adolescent brain to function so inefficiently. As experience shapes the growth and lowers less efficient pathways, the prefrontal cortex matures, and adolescents are able to reason better, use better judgment, and exhibit better impulse control.\nThe Forth Plane of Development (18-24 years)\nFollowing the bigger changes and creations in the third plane, the fourth plane is another stable period of development and consolidation of the creations formed in adolescence. A consolidation of all that have been acquired before This is the transition to adulthood. The physical development is over. The adolescent becomes an individual, part of the society, social person and it will emerge after all into a secure, responsible, independent, moral and social adult\nFrom 18-21 years, they are in a period of questioning, a career search. From 21-24, they are settling in with what they want to take on. If we have given the youth enough exposure to many branches of learning and practical skills, s/he can now choose a profession that is deeply satisfying. The quest for independence can now be achieved\nAs in the passage through all previous planes, his success now depends on how he has developed his potential until this point. If the preceding levels of independence have been realized, he will be able to make her own “choice of action,” aware of his own “possibilities and responsibilities.” Montessori writes,\n“Culture and education have no bounds or limits; now man is in a phase in which he must decide for himself how far he can proceed in the culture that belongs to the whole of humanity.” (Montessori, “Four Planes of Education,” p.14, reprinted in 2004).\nPersonality will blossom in each stage. He knows who he is, has his place in society, knows what he can and his limits, can take a decision, accepts consequences of his action and decision and someone aware of the part he is supposed to have in society.\n Oerter, Montada (1982). Entwicklungspsychologie. Beltz, Psychologische Verlagsunion\n Sunderland, M., Die neue Elternschule / The Science of Parenting, 2006, DK.\n Montessori, M. From Childhood to Adolescence / Von der Kindheit zur Jugend, 2015, Herder\nGiedd, J., Inside the Teenage Brain, 2002, Frontline.\nQuelle ne fut pas ma surprise en tombant sur cet article lors de mes recherches sur internet.\nEt pourtant cela faisait écho à ce que j\'ai pû observer durant mes nombreuses années de travail avec les enfants. Trop souvent j’ai entendu que les élèves devaient rester en position bien droite et statique sur leur chaise et trop souvent j’ai vu ces enfants se tordre, gigoter et prendre des positions improbables sur leurs chaises alors qu’ils étaient pourtant parfaitement concentrés sur leur activité. Une école primaire d\'Ottawa a trouvé une solution originale pour maintenir l\'attention des élèves.\nAs a Montessori Teacher and a mother of five, I’ve had many people ask me my advice on various misbehaviours. So I was very excited and happy when I was introduced to Jane Nelsen’s book, Positive Discipline. In this article, I will share with you some insights into how positive discipline works in the Montessori classroom as well as Jane Nelsen’s approach to positive discipline.\nLa mère parfaite n’existe pas !\nEtant maman de deux enfants et ayant étudié l’éducation sous toutes ses coutures, j’ai appris une chose en devenant maman : avec ses enfants, on fait ce qu’on PEUT et non ce qu’on VEUT !\nDeux Mille Feuilles\nÉcole Montessori Bilingue\nBilingual Montessori School\nRoute des Bois 2, 1278 La Rippe, Vaud (near Nyon)\nPhone: +41 76 580 61 78']"	['<urn:uuid:aa8ae9cc-1dbf-44a5-8ba7-30e017279d36>']	open-ended	with-premise	long-search-query	distant-from-document	expert	2025-04-14T18:34:38.591736	111	690	14535
67	Could you explain how the cornea gets its nourishment since it doesn't have any blood vessels like other parts of our body?	Unlike most tissues in the body, the cornea has no blood vessels to protect or nourish it against infection. Instead, the cornea receives its nutrients from two sources: the aqueous humor and tears, which fill the chamber behind it. The cornea is a transparent, dome-shaped layer that consists of five basic layers: endothelium, stroma, epithelium, Descemet's membrane and Bowman's layer, and despite being clear, it has a highly structured group of proteins and cells.	"[""About Cornea and External Diseases\nThis information is intended for general information only and should not be considered as medical advice on the part of Health-Tourism.com. Any decision on medical treatments, after-care or recovery should be done solely upon proper consultation and advice of a qualified physician.\nCornea and external diseases\nCorneal and External diseases involve the cornea, anterior chamber of the eye, eyelids, lens, conjunctiva and iris, which include cataracts; infections, irregularities and corneal allergies; refractive errors (astigmatism, nearsightedness and farsightedness); conjunctivitis (pink eye); tear disorders; dry eye; endophthalmitis; keratoconus; ptergium; Fuch's Dystrophy and many more.What is the cornea?\nThe cornea is the outermost, transparent, dome-shaped layer, which covers the pupil and iris in the front of the eye. The corneal tissue has five basic layers: endothelium, stroma, epithelium, Descemet's membrane and Bowman's layer. Though the cornea is clear, it has a highly structured group of proteins and cells. Contrary to most tissues in the body, the cornea has no blood vessels to protect or nourish against infection. Rather, the cornea gets its nutrients from the aqueous humor and tears, which fill the chamber behind it.The cornea, one of the defensive layers of the eye, serves two purposes:\n- First, together with the tear film, sclera (white section of the eye), eyelid, and the eye socket, the cornea protects the eye from germs, dirt, along with other hazardous matter.\n- Second, as the outermost lens of the eye, it is the point of entry for light into the eye. When light hits the cornea, it refracts or bends the incoming light onto the lens. The lens additionally refocuses the light onto the retina, a light-sensing layer of cells of light-sensing lining the back of the eye.\nTo see well, the lens and cornea have to focus the light rays accurately on the retina. This refractive procedure resembles the way a camera captures an image. The lens and cornea in the eye function as would the lens of a camera. The retina estimates the film. If the cornea fails to focus the light correctly, then the retina gets a blurry image.What irregularities and injuries affect the cornea?\nSome trauma, which includes blunt trauma, projectile foreign bodies, and lacerations, may result in scarring, which clouds the cornea. Hereditary problems, which include dystrophies and degenerations, might as well cloud the cornea. The commonest hereditary ailment seen in young people is keratoconus, an ailment where the cornea assumes a cone shape. This is popular in kids with Down’s syndrome as well as in people with allergic conjunctivitis. These patients might be able to use glasses or contact lenses for some time; however, might ultimately develop high astigmatism and scarring, which can't be rectified without corneal transplantation.\nSometimes, it might be essential to carry out a corneal transplant following cataract operation, if bullous keratopathy takes place. Bullous keratopathy is a disorder where the endothelial cells on the back of the cornea decline in number after a cataract operation. But this is less popular nowadays due to improved lens designs and new techniques.How can the cornea be damaged?\nThe surface of the eye may be seriously damaged by several problems, which include:\n- Thermal and chemical injuries\n- Pathological illnesses like pemphigoid and Stevens-Johnson syndrome\n- Inflammations and chronic infections\n- New tissue growths like tumors and ptergium (believed to be linked to sun damage)\n- Neurotrophic problems (because of damage to the eye’s sensory nerves)\n- Uncommon hereditary situations like aniridia (congenital lack of the iris)\nThese problems may lead to extensive damage on the eye surface, resulting in scarring and new blood vessel formation; damage that leads to loss of vision.\nBascom Palmer scientists are assessing the potential of common tears for modulating promoting and modulating the healing of these conditions. A complete understanding of the correct role of tears in the healing process must result in strategies, which would quicken visual recovery and boost the percentage of patients totally pleased after the operation.\nLearn more about Cornea and External Diseases""]"	['<urn:uuid:3737d2e1-2134-4a5b-a43f-c09e6e886b38>']	open-ended	direct	verbose-and-natural	similar-to-document	novice	2025-04-14T18:34:38.591736	123	469	4292
68	I've been studying early scientific illustrators, and I'm wondering what was Maria Sibylla Merian's specific focus in her watercolor observations during the 17th century?	Maria Sibylla Merian focused on systematically recording the transformations undergone by caterpillars. She raised every kind of caterpillar she could find and documented her observations in watercolor. She later expanded her work by traveling to Surinam to study and record the diverse insects there.	['March is Women’s History Month, when we celebrate the contributions women have made in all areas of society. My own observation of this month always includes finding an inspiring biography of a woman to read. Making an intentional choice to spend time with these stories is a way for me to honor these remarkable women, as well as all those whose stories continue to go untold. In no particular order, here are a few favorites to get you started in finding your own Women’s History Month read.\nHidden Figures by Margot Lee Shetterly\nThis book became an instant classic, its deserved fame bolstered by the excellent 2016 film of the same name. It tells the story of a number of Black female mathematicians whose largely behind-the-scenes work fueled the success of NASA in the latter half of the 20th century. These women struggled against both racism and sexism in a field dominated by white men, and their work enabled incredible achievements in the field of aerospace engineering and broke down seemingly insurmountable barriers.\nEven if you’ve read this one, this might be a good time for a re-read. The year 2020 saw both the death of NASA computer engineer Katherine Johnson and the launch of the Mars rover Perseverance; reading her story again might just give us a deeper appreciation for how we got here.\nMaria Sibylla Merian (1647-1717) lived in a time when emerging scientific methods were calling into question the very worldview of Christian Europe. Merian received artistic training from her stepfather and developed a keen interest in the transformations undergone by caterpillars. She lived out her fascination with diligence, systematically raising every kind of caterpillar she could find and recording her observations in watercolor. She eventually traveled to Surinam, which was then a Dutch colony, to study and record the abundantly diverse insects there.\nThis book traces not only the life of Merian, but also the bounds made in scientific thought within her lifetime. It is a fascinating story of a woman who pushed the boundaries enforced by her time.\nThe Fossil Hunter: Dinosaurs, Evolution and the Woman Whose Discoveries Changed the World by Shelley Emling\nMary Anning lived from 1799 to 1847 in Lyme Regis, England, where the soft rock of the cliffsides was revealing fossils that would again force the scientific community to drastically reconsider its understanding of the world. Like Merian, Anning was a woman working outside the male-dominated scientific establishment, engaging in field work at a time when such work was considered secondary to the discussions “real” scientists had in university settings.\nAnning’s lifetime is not terribly well-documented, so at times this book engages in speculation about her emotional reactions to the events we do know about. She persisted through her discoveries being credited to better-known men (and discredited by some of them), and ultimately ended up with several fossil species named after her.\nDorothy Day; The World Will Be Saved By Beauty: An Intimate Portrait of My Grandmother by Kate Hennessy\nDorothy Day (1897-1980) was a journalist and activist who converted to Catholicism as an adult and is best remembered for co-founding the Catholic Worker movement, which invites people into communities dedicated to hospitality for the poor. She is on the Catholic church’s path to sainthood, and there are numerous biographies of her (including an autobiography that you should add to this list, or catch the recent documentary about her life).\nBut this biography is unique. Written by her granddaughter, it gives primary importance to Day’s role as a mother, tracing her relationship with her daughter Tamar with all its joys and complications. The book certainly details the contours of Day’s public life and work but also gives an up-close look from within the family at the real woman behind the legendary work. It is a lovely reminder that heroic virtue and holiness takes shape within real human lives, with relationships at their heart.\nThea’s Song: The Life of Thea Bowman by Charlene Smith and John Feister\nThea Bowman was another 20th century woman who is now on the path to sainthood. Born in Mississippi in 1937, Bowman sadly died from breast cancer at the age of 52. Her too-short life saw a world rocked by social changes, and Bowman made choices to become a force of joy and healing within that world. She boldly claimed her identity as a Black Catholic woman even as she navigated life within a religious order that was primarily white.\nHer life’s work included teaching and preaching and bringing the American Black community into conversation with the Catholic Church. One of the authors of this book knew Bowman personally and gives a captivating look at the life of this inspiring woman.\nWomen’s History Month is a great opportunity to dive into the ways that women have contributed to your chosen field of interest. Women have often worked quietly and in supporting roles, but they have always been present throughout history, full of unique dreams and capabilities. These are only five books to spark your interest, but there are countless others out there. What would you add to this list?']	['<urn:uuid:3684f6c4-80eb-4811-9dbd-e6181c7619e8>']	factoid	with-premise	verbose-and-natural	similar-to-document	expert	2025-04-14T18:34:38.591736	170	301	5192
69	starting electrical company business need know who created general electric corporation history	Henry Villard founded the Edison General Electric Company in 1889 and later changed its name to the General Electric Company in 1893.	['April 10, 1727\nBirth of Samuel Heinicke (1727-1790) in Nautschütz, Germany. Heinicke became interested in the problems of the deaf after reading Surdus loquens, a book in which it was described how a Swiss physician had succeed in teaching deaf persons to speak. In 1778 he opened a school for the deaf. He developed the system of teaching lip-reading as opposed to signs, since he felt it was best for the deaf to understand language as normally used in society at large.\nApril 10, 1755\nBirth of Samuel Hahnemann (1755-1843) in Meissen, Germany. Hahnemann completed his M.D. at the University of Erlangen in 1779. Hahnemann was a German physician best known for creating a system of alternative medicine called homeopathy. It is based on the idea that small doses of toxins have curative powers. This principle, like cures like, became the basis for an approach to medicine which he gave the name homeopathy. His theory is contained in the book Organon der rationellen Heilkunst (1810). He was driven out of Leipzig by the medical establishment and moved to Paris where he was quite popular.\nApril 10, 1829\nBirth of Johannes Janssen in Xanten, Germany. Janssen is noted for his eight volume Geschichte des deutschen Volkes seit dem Ausgang des Mittelalters (1876-1894). The history made important contributions to German cultural history and especially 15th century studies, but since the viewpoint was quite blatantly pro-Catholic and anti-Protestant, the book became quite a controversial matter.\nApril 10, 1835\nBirth of Henry Villard in Speyer, Germany. Villard changed his name from Ferdinand Hilgard when he immigrated to the United States in 1853. In the U.S. he wrote first for German-American newspapers and later by larger mainstream papers including The New York Herald and The New York Tribune. He bought The New York Evening Post in 1881. He then turned to investments in railroads and became the president of the Oregon and California Railroad and the Oregon Steamship Company. In 1881 he gained control of the Northern Pacific. In 1889 he founded the Edison General Electric Company and later changed its name to the General Electric Company in 1893.\nApril 10, 1883\nBirth of Robert Faesi in Zürich, Switzerland. Faesi wrote plays, poetry, short stories and literary criticism. He was a professor of German literature at the University of Zürich. Among his literary works are, Aus der Brandung (1917), Füsilier Wipf (1917), Züricher Idylle (1908) and Die Stadt der Väter, Die Stadt der Freiheit, Die Stadt des Friedens (3 vols. 1941-52). He wrote the libretto for Willy Burkhard’s opera, Die schwarze Spinne. His published correspondence with Thomas Mann appeared in 1962.\nApril 10, 1920\nDeath of Moritz Benedikt Cantor in Heidelberg, Germany. Cantor was a professor of mathematics at the University of Heidelberg. He is considered one of the greatest historians of mathematics.\nApril 10, 1924\nDeath of Hugo Stinnes (1870-1924) in Berlin, Germany. An Industrialist, Stinnes beginning with a modest operation established by his grandfather (Stinnes Konzern) in coal mining, expanded the business to include steel mills, banks, and electrical companies as well as transportation (Hugo Stinnes GmbH). During World War I he profited greatly from the supply of war materials. A member of the Deutschnationale Volkspartei, he was elected to parliament in the early years of the Weimar Republic.\nApril 10, 1951\nThe West German parliament passes the Montanmitbestimmungsgesetz, a law giving German workers a voice in the decision making processes of companies in the iron, steel and coal industries.\nBack to Today in German History Calendar']	['<urn:uuid:0ade1584-fa92-4fe5-94bd-7254b3a98910>']	factoid	with-premise	long-search-query	distant-from-document	novice	2025-04-14T18:34:38.591736	95	133	3646
70	Do property inheritance documents need court approval?	In some cases, the affidavit of heirship must be approved by a probate court. This is required in certain states for any affidavit of heirship. Additionally, if there was a will being probated, the affidavit needs court approval to conclude the probate process.	['When a person dies without leaving a will, an affidavit of heirship may be needed to establish facts about the deceased person?s heirs and the transfer of property. While laws regarding an affidavit of heirship vary from state to state, the basic principles are the same across the nation. This guide will provide the information needed to understand the affidavit and why it is needed.\nWho Needs an Affidavit of Heirship\nWhen disposing of a deceased person?s property or transferring the deeds to the heirs, it is necessary to document the legal right of title has passed from the decedent, the person who has passed away, to the heirs. A probated will may establish the legal ownership, though even when a will has been probated the affidavit may occasionally be required before a deed transfer is affected.\nHowever, if the deceased passed away without leaving a will, the affidavit of heirship can establish legally who the heirs are, the property the deceased left behind, and how that property is to be disbursed. Additionally, the affidavit can usually be executed without involving probate court. This can speed up the process of transferring ownership of the property. For this reason, even in cases where a will has been left behind, some people may choose to execute an affidavit. In cases where there is no dispute over the heirs or how to disburse the property in question, an affidavit of heirship may be used. In that case, the heirs may agree to use the affidavit to execute the decedent?s wishes instead of taking the will to probate court.\nWhat the Affidavit Accomplishes\nThe affidavit spells out who the legal heirs to the property are, what the property is, and who gains ownership of the property. Legally specifying who the decedent?s heirs are establishes the rights and responsibilities of those heirs to dispose of the decedent?s property and wrap up the affairs on behalf of the deceased. A full list of the property owned by the decedent is included in the affidavit and establishes exactly what property is being transferred to the heirs. For land, this includes a legal description of the property, of the sort found on the title deed.\nThe affidavit also serves as an instrument for transferring ownership to the heirs. An affidavit of heirship may be used in lieu of a deed transfer and, in the case of land, the affidavit must be filed with the county recorder to establish the ownership of the land in the same way a deed would.\nWho Is Party to the Affidavit of Heirship\nThe primary parties to the affidavit are the heirs themselves. This is usually the spouse or registered domestic partner and any living children or blood relatives of the decedent. This may also involve friends of the decedent or even a former spouse, but it is less common to have others involved in an affidavit of heirship because everyone involved must be in agreement on the distribution of the decedent?s property. The affidavit may also include information about heirs of the decedent who have passed away and who their heirs were.\nThe affidavit must be signed by witnesses under oath before a notary public. The laws regarding who may attest to the affidavit vary from state to state. In most states, the witnesses must be one or more disinterested parties ? that is, the witnesses must not be heirs or family members of the deceased. This prevents any conflict of interest where the witness would have an incentive to lie on the affidavit. Some states may require only one witness, or witnesses who are family members, or a mix of family members and disinterested parties. It is important to know the state laws regarding who may attest to the affidavit.\nThe witnesses are usually required to know the decedent, the date they passed away, that names and birthdates of the family members and heirs, and whether the decedent had any outstanding debts at the time of their death. The witnesses will also usually be required to swear that they will not benefit financially from the estate themselves and can be held for perjury if their statements are false.\nExecuting the Affidavit\nOnce the witnesses have signed the affidavit in front of the notary, the document may be accepted as legal proof of heirship and transfer of ownership. In some cases, the document must be approved by a probate court. This is true in certain states that require this for any affidavit of heirship. Additionally, if the decedent left a will and it was in the process of being probated, the affidavit will need to be presented to the probate court for approval and to conclude the probate process.\nIf real estate was being transferred in the affidavit, it must also be filed with the county recorder?s office in the county where the land is located.\nHow to Create an Affidavit of Heirship\nWhile the affidavit of heirship is a simplified way of disbursing the property of a deceased person, it is nevertheless a legal document that must be properly created and executed. As such, it may be beneficial to have a lawyer familiar with estate law help create the affidavit of heirship on your behalf and help walk you through the process of getting the appropriate witnesses and executing and filing the affidavit.\nHowever, if you and the other heirs do not wish to engage an attorney for this process, it is possible to proceed to create the affidavit. Most states have an outline of what is required in the affidavit and that may be followed to ensure that all the requirements of the document are met. Consult your state?s website for information on the laws and requirements for an affidavit of heirship in your state.\nAnother option is to consult a legal forms website online. These sites have ready-made forms that require you to fill in your specific information to create a legal document. These are usually tailored to the requirements of your state. You will still need to familiarize yourself with your state?s requirements for witnesses and the requirements for filing the document with a county recorder or probate court.\nWhile leaving a will is the best way to ensure the decedent?s wishes are carried out after their death, in many cases due to the absence of a will or in order to conclude the matter speedily an affidavit of heirship may provide a simple and speedy option for heirs to legally establish ownership of the decedent?s property.']	['<urn:uuid:6967ca36-0f70-4223-8961-6cf8d345b2f5>']	factoid	direct	concise-and-natural	distant-from-document	expert	2025-04-14T18:34:38.591736	54	261	6340
71	studying the relationship between divinity and fulfillment can transcendental beings experience joy	According to philosophical understanding, God is His own happiness, as His happiness is His intellectual operation and His substance. Being the sovereign good and supreme good, God experiences the most perfect form of happiness. He rejoices more in His own happiness than other blessed beings do in theirs, since every being naturally loves itself more than another. God has supreme pleasure in Himself and universal joy in all good things, without any mixture of the contrary.	['We finish Book One today. Hooray! This is a good time to remind you to review, review, review. All of Book One was one, long sustained argument, an unbroken thread. You can’t just start at the end and hope to understand what the words and terms mean. Review!\n IT follows from this that God is His own happiness.\n For His happiness is His intellectual operation, as we have shown: and it was proved above that God’s act of intelligence is His substance. Therefore He is His own happiness.\n Again. Happiness, since it is the last end, is that which everyone wills principally, whether he has a natural inclination for it, or possesses it already. Now it has been proved that God principally wills His essence. Therefore His essence is His happiness.\nNotes Case in point about reviewing. “Substance” means essence or nature here. Don’t forget that we proved God’s existence and essence (substance, nature) are one and the same. This is how Aquinas can speak of willing toward an end.\n Further. Whatever a person wills he directs to his happiness: for happiness is what is not desired on account of something else, and is the term of the movement of desire in one who desires one thing for the sake of another, else that movement will be indefinite. Since then God wills all other things for the sake of His goodness which is His essence, it follows that He is His own happiness, even as He is His own essence and His own goodness.\n Moreover. There cannot be two sovereign goods: for if either lacked what the other has, neither would be sovereign and perfect. Now it has been shown above that God is the sovereign good. And it will be proved that happiness is the supreme good since it is the last end. Therefore happiness and God are one and the same. Therefore God is His own happiness.\nNotes We also proved, again starting at Chapter 13, that God has to be one essence. (That He is three persons we haven’t come to yet.) Since He is one, there can only be one sovereign good.\n FURTHERMORE, from what has been said we are able to consider the excellence of the divine happiness.\n For the nearer a thing is to happiness, the more perfectly is it happy. Hence, although a person be called happy on account of his hope of obtaining happiness, his happiness can nowise be compared to the happiness of one who has already actually obtained it. Now that which is happiness itself is nearest of all to happiness: and this has been proved to be true of God. Therefore He is singularly and perfectly happy.\nNotes A simpler argument you’d be hard-pressed to find.\n Again. Since joy is caused by love, as was proved, where there is greater love there is greater joy in possessing the thing loved. Now, other things being equal, every being loves itself more than another: a sign of which is, that the nearer a thing is to one, the more is it naturally loved. Therefore God rejoices more in His own happiness, which is Himself, than the other blessed in their happiness, which is not themselves. Therefore His happiness sets His desire more at rest, and is more perfect…\n Again. Weariness, and the various occupations which in this life must needs interrupt our contemplation wherein especially consists human happiness, if there be any in this life; errors, doubts, and the various misfortunes to which the present life is subject–all these show that human happiness, especially in this life, cannot bear comparison with the happiness of God…\nNotes To which we can only say: Amen.\n False and earthly happiness is but a shadow of that most perfect happiness. For it consists of five things, according to Boethius, namely pleasure, wealth, power, honour and renown. But God has the most supreme pleasure in Himself, and universal joy in all good things, without any admixture of the contrary. For wealth He possesses in Himself an all-sufficiency of all good things, as we have proved above. For power He has infinite might. For honour He has supremacy and governance over all things. For renown He has the admiration of every intellect which knows Him in any degree whatever.\nNotes Whatever else you think of the arguments above, and because we all agree there is such a thing called “happiness”, is Boethius right? How much can we distinguish between wealth and power? If you have power, do you need wealth? If you have honor do you need renown? Our culture obviously treasures renown and disparages honor. Anyway, as The Philosopher said, all these point towards one happiness, which is, of course, God.\nFinally, here are the words St Thomas uses to close Book One:\nTO HIM THEREFORE WHO IS SINGULARLY HAPPY, BE HONOUR AND GLORY FOR EVER AND EVER.']	['<urn:uuid:4a6705b3-bf73-41e1-8f59-daa059a96931>']	open-ended	with-premise	long-search-query	distant-from-document	expert	2025-04-14T18:34:38.591736	99	477	4635
72	what happens if police doesnt give miranda warnings	If police fail to give Miranda warnings before questioning someone in custody, any incriminating statements or confessions made can be suppressed and cannot be used as evidence in court. Additionally, if police try to continue questioning after someone invokes their Miranda rights, this would also violate their rights.	['If you are charged with committing a crime, you need the assistance of an experienced criminal defense attorney to help you fight the charges. An attorney should have strategies based on experience handling similar criminal cases—which may help in getting your charges dismissed or reduced to a crime with a less severe punishment. One important way that an attorney may be able to weaken or destroy the prosecution’s case against you is to file a motion to suppress evidence against you.\nHow Can a Motion to Suppress Evidence Help in Your Criminal Case?\nIt is critical to keep in mind that you are innocent until proven guilty in a criminal case—even if you know that you are guilty of committing the crime. The prosecution has the burden of proving your guilt beyond a reasonable doubt. This is a very high standard that the prosecutor cannot always meet, including cases where the accused is obviously guilty.\nA motion to suppress evidence is filed to prevent certain pieces of evidence from being used against you in your criminal case. It can help your case in the following ways:\n- If the suppressed evidence is essential to prove the criminal charges against you, its suppression could result in the criminal charges being dismissed.\n- If the prosecutor has other sufficient evidence against you, the charges against you will not be dismissed. However, the suppressed evidence can severely weaken the prosecutor’s case against you and create reasonable doubt—sufficient for you being found not guilty. It could also result in the prosecutor offering you a more favorable plea agreement due to the lack of evidence against you.\nCommon Grounds to File a Motion to Suppress Evidence\nIn order to successfully file a motion to suppress evidence, you need to have legal grounds to argue that the evidence should be suppressed. These grounds are generally based on police misconduct and violation of your constitutional rights. An experienced criminal defense attorney will be able to identify grounds to file a motion to suppress evidence in your case. Common reasons to file this motion include:\n- Unlawful searches and seizures. Under the Fourth Amendment, you are protected against illegal searches of your home, your property, and yourself. In general, police must have a valid search or arrest warrant or probable cause to believe that a crime was committed to stop, search, or arrest you. If the police did not have a proper warrant or probable cause, the evidence collected that supports the charges against you could be thrown out of court.\n- Failure to give Miranda warnings. Once the police take you into custody, they must inform you of your Miranda rights under the constitution before questioning you further. This includes being advised of the right to remain silent, that any statements can be used against you, and your right to an attorney. If they fail to give you these warnings and you make an incriminating statement or confess to the crime, these statements can be suppressed. In addition, the police could violate your rights by trying to continue to question you after you invoke your Miranda rights.\n- Coerced statements. If the police coerce you into confessing or making a statement, this may be grounds for suppression of your statement.\n- Chain of custody errors. The police are required to follow strict procedures for the handling of and storage of evidence against you. This is to ensure that it is not tampered with or mixed up with evidence in another criminal case. When the police mishandle the evidence or otherwise violate the rules, the evidence can be suppressed.\n- Witness identification. If you were identified in a police lineup, the police may have violated your rights by improperly suggesting that the witness identify you, failing to allow your attorney to attend the lineup, or failing to include other potential suspects in the lineup with a similar appearance. This can be grounds to suppress the witness’ identification—which could be crucial to proving you committed the crime.\n- DUI testing results. There can be many challenges to the tests performed by the police as part of a DUI arrest. This can include violation of the many rules regarding field sobriety tests, improper administration of breathalyzer tests, or calibration problems with testing machines. In addition, many successful challenges to evidence are based on the lack of probable cause to stop the accused in the first place.\nTalk to a Charlotte Criminal Defense Atttorney\nIf you are facing criminal charges, our experienced criminal defense attorneys are here to guide you through the criminal process. To learn more about how we can assist you, call our office today to schedule your free consultation.']	['<urn:uuid:80e6e23d-1b14-4a03-a9bb-e15fbdc836b6>']	factoid	with-premise	short-search-query	similar-to-document	novice	2025-04-14T18:34:38.591736	51	320	4726
73	why relaxation breathing helps reduce stress and anxiety	Abdominal breathing favors the parasympathetic nervous system. It increases blood oxygenation, slows heart rate, and decreases blood pressure. This turns down the sympathetic nervous system ('fight or flight'), leading to decreased stress and anxiety, improved cognition and brain function, better blood flow to vital organs, and relaxed muscle groups with reduced pain and inflammation.	['The pelvic floor muscles are a very important piece in this system as well, functioning alongside the diaphragm for breathing. During inhale they relax and descend down. With exhale, they recoil back up, mirroring the movement of the diaphragm. This piston-like movement provides massage and movement to the abdominal and pelvic organs, helps to pump blood and lymphatic fluid through the trunk, it helps to aid in pelvic floor muscle healthy function and keeps a balance through the trunk musculature.\nTaking even 5 minutes every day to practice abdominal breathing is beneficial to us as humans! The more you do the longer the carry-over becomes greater and greater. Abdominal breathing teaches self-soothing to allow for better stress management, less anxiety and better sleep. It is calming to our bodies in a world of stimulation from phones, tablets, computers, and TV. Abdominal breathing turns down our sympathetic nervous system. Wait! it sounds too technical but keep reading…The sympathetic nervous system is responsible for “fight or flight”, meaning if you walk up on a bear in the woods this system is what tells you to RUN! Of course like all things in nature, there has to be a balance, and the sympathetics’s partner is the parasympathetic system. We need a good balance between the two. When one is overstimulated (ahem, put down your cell phone for a minute and focus here!), we tend to live in that mode of heightened awareness. This increases heart rate, blood pressure and decreases blood flow away from our organs to divert it to where it needs to be to get away from that bear. Helpful when we really need to get away, but not helpful in our daily lives when really all we need to do is calm down.\nAbdominal breathing favors the parasympathetic nervous system. It brings increased oxygenation to our blood stream, it slows heart rate and decreases blood pressure. It turns the sympathetic nervous system down a notch, putting that bear into hibernation mode. This allows for decreased stress, decreased anxiety and improved cognition and brain function. This improves blood flow to vital organs, keeping systems mo ving happily. It relaxes muscle groups, reducing pain and inflammation. In a world where we are going every minute it would greatly benefit everyone to take just 5 minutes daily and JUST BREATHE.\nHere is how to do it:\nFind a comfortable position, the one shown below should work for most people. Do this at a time of day when you can follow it with a calm activity or are ready for sleep, as this is designed to soothe you and put you in a sedated state. Place one hand on your belly and one on your chest. Inhale through your nostrils, allow your abdomen to expand and blow up like a balloon.\nExhale, and the abdomen falls down. Repeat this for 5 to 10 minutes, feeling and allowing your entire body to relax.\nTry working this into your day, every day and see and feel the benefit!']	['<urn:uuid:76386f5f-a651-4f90-b377-c09648c1b7bb>']	factoid	direct	long-search-query	distant-from-document	novice	2025-04-14T18:34:38.591736	56	387	2921
74	mackenzie river length rank north america	The Mackenzie River is the second longest river in North America, after the Mississippi River. It flows from south to north into the Arctic Ocean.	"[""Indigenous Leader Bridges Environmental Divides in ‘Spectacular’ Canadian Wilderness\nIn the Northwest Territories, Stephen Kakfwi pursues solutions that respect the Earth and its original stewards\nYELLOWKNIFE, Northwest Territories—Stephen Kakfwi was born to the land, in a wilderness region so grand in scale that the adjective most frequently used to describe it, spectacular, can seem insufficient.\nConsider just a few of the natural wonders within Canada’s rugged Northwest Territories: the deepest lake in North America, the continent’s second-longest river, the largest unpolluted lake on the planet, a jagged-edge mountain range with 9,000-foot peaks and 3,000-foot-deep river canyons, a hidden waterfall twice the height of Niagara, a boreal forest three times the size of Britain.\n“There is incredible land everywhere you go,” says Kakfwi.\nA former territorial premier, past president of the Dene Nation, and lifelong Indigenous rights activist, Kakfwi has spent decades working to balance protection of the mostly pristine areas of this landscape with sustainable economic development for First Nations communities.\nAsk Kakfwi what he holds special and he says it’s not just iconic sites such as Great Bear Lake or Nahanni National Park Reserve, a World Heritage site in the Mackenzie Mountains. It’s the intimate places that live in his memory, places infused with personal importance.\n“What is closest to me is the land where I was born,” Kakfwi says. “Everywhere I look, there are stories. There are spiritual, sacred sites. And those are beautiful to me—in my heart and my mind, not just visually.”\nKakfwi was born in 1950 on the northern edge of the Territories’ vast boreal forest region near the Arctic Circle, at a hunting camp on the shores of Lake Yelta. He spent his early childhood in tiny Fort Good Hope, a Dene village along the 1,080-mile-long, north-flowing Mackenzie River, known in the Slavey language as the Deh Cho.\n“There was literally nothing there. There was a little grocery store, a church, and that was it,” he says.\n“For the first five years of my life, I lived off of moose and caribou, rabbits, ducks, and the berries that we find in the woods, in the bush. … We depended 100 percent on the bounties of the land, on the wildlife, the birds, the fish.”\nUnder a government assimilation policy, Kakfwi was removed from his home at the age of 9 and placed in Canada’s residential school system. “I was, like many others, sexually abused. I was physically beaten. I went through some horrible suffering,” Kakfwi says. When he returned to Fort Good Hope, he relearned the Slavey language and reconnected with a Dene culture that drew its sustenance and spiritual health from the land.\nDene leaders in his community taught him the traditional methods of hunting and trapping. He learned how to skin a moose in the frigid cold of deep winter, and the importance of using every part of the animal for food and clothing. Kakfwi says the land became his place of solace and recovery.\n“On days when I was heartbroken and lonely, and feeling sad and sorry for myself, I used to go to this hill. And on the hill, there was a stand of birch trees. Not very big birch trees, just small, little birch trees. I used to go there and lay underneath those trees, looking up at” the sky, Kakfwi says.\n“There’s a way to get comfort from the earth and from the land, just by laying on it, by connecting.”\nKakfwi’s connection to the land has shaped a career spent fighting for recognition of the rights of Indigenous people, including the right to make decisions about development and conservation on traditional territories where they have lived for thousands of years.\nAs a young man who came of age when the civil rights movement was at its height in the United States, Kakfwi thought protest could secure greater rights for his people and protection for the land from unchecked development. Over time, and following the guidance of Dene elders, Kakfwi came to understand that change could come through collaboration with governments and industry.\nKakfwi was elected in 1983 to lead the Dene Nation, an organization representing five Dene tribal groups, and worked to build a framework for Indigenous land claims. The Dene Nation’s traditional territories are known as Denedeh—Land of the People.\nKakfwi became a member of the territorial legislature in 1987 and held several cabinet portfolios, including minister of resources, wildlife, and economic development. He was premier from 2000 to 2003.\n“I worked in the legislature for 16 years because the job was so meaningful to me. I never took a holiday,” he says.\nWorking for change from within governments has paid dividends, Kakfwi says.\nThe Dene Nation represents five Indigenous tribal groups in the Northwest Territories: Gwich’In, Deh Cho, Sahtu, Tlicho, and Akaitcho.\nIt has taken decades, but there is a growing understanding of the value that First Nations bring to land use planning in the boreal region, and the historical knowledge they have about striking a balance between industrial activities and protection of the Earth, he says.\nAs Indigenous people, “we believe inherently that we are part of the Earth. We’re not separate. We don’t have dominion over it,” Kakfwi says.\n“Keeping our culture alive is all about being part of the land, being part of the wildlife, the fish, and the birds.”\nJust as Indigenous people, like all people, have rights that must be respected, those who seek to extract the region’s resources should acknowledge that the Earth also has rights, he says.\n“The Earth is a living being. It’s a living creature. And you have to respect the Earth,” he says.\n“Elders say the Earth loves us … because it’s been taking care of us for thousands and thousands and thousands of years,” he adds. “It takes care of us, no matter how much we misbehave.”\nEverywhere I look, there are stories. There are spiritual, sacred sites. And those are beautiful to me—in my heart and my mind, not just visually.Stephen Kakfwi\nBut people have not always taken care of the Earth in return. The boreal forest region in the Northwest Territories is a place rich with natural resources, including diamonds, gold, rare earth, silver, uranium, and oil and gas reserves. The capital city, Yellowknife, markets itself as the Diamond Capital of North America.\nNatural resources fuel the northern economy, but Kakfwi says development has proceeded without full and proper consideration of its impact on the land or on the well-being of its original inhabitants.\n“Our history is marked with people that have come, opened up mines, and then left behind a land that has been devastated. Water has been polluted. And for what? Just for very specific interests,” Kakfwi says.\n“We have to get to the day when every one of us—every government in this country, every government in the world—will respect the rights of the Earth. To keep the water clean, to protect the wildlife, to protect the land, and make sure things are done in balance, and all things are sustainable.”\nCanada's boreal forest\nYellowknife is the capital of Canada’s Northwest Territories, which has 235 million acres of boreal forest.\nThose beliefs formed the underpinning of Kakfwi’s work in the territorial legislature and during his professional life since leaving public office.\nKakfwi played central roles in developing and implementing a protected areas strategy for the Northwest Territories. He helped settle long-standing land claims by several Indigenous communities, including his own tribal group, the Sahtu Dene, and backed First Nations-led land protection measures that include Indigenous co-management of conservation areas.\nKakfwi was a strong supporter of a transfer of powers, called devolution, which in 2014 resulted in the government of the Northwest Territories assuming control over land use and resources from Canada’s federal government. The role of Indigenous governments is formally recognized within the process, laying a foundation for more locally accountable and responsible decision-making on lands and resources.\nGovernments are learning to “accept that we have to do business with the First Nations people … because they have rights,” he says.\nIn addition to his political and conservation leadership, Kakfwi is a musician whose recordings include a CD of traditional Dene songs.\nKakfwi stresses that traditional Indigenous knowledge, which is sensitive to the needs of people and the environment, is essential. No one knows the land better than those who have lived on it for millenniums, he says.\n“The Dene have always had a plan for our land. Every year, for thousands of years, we have decided who’s going to go to the fish lakes, which families are going to go to the mountains, which ones are going to go on the river, which ones are going to go to the [river] delta areas. We decide that amongst ourselves,” he says.\n“Where are the places where the moose are plentiful? Where is the best fish? Which areas should we leave [alone] for a few years until they become plentiful again? That’s land use planning, and that’s what we’ve done.”\nGood land use planning is about striking a balance, he adds, and about recognizing that the land is worth something more than just the minerals or resources that lie within it.\n“When you come into my country, I expect you to live as a good citizen, to respect the rights of everybody else, to take care of the land and the water and the wildlife, and leave it the way you find it,” he says.\n“I’ve always carried that with me. And I think all our people carry that with us. And it’s no different from a person that lives in a city that has a yard. Nobody is going to go into that yard and contaminate it. I mean, it’s just totally unacceptable. So why should we be any different from that?”\nWhen you come into my country, I expect you to live as a good citizen, to respect the rights of everybody else, to take care of the land and the water and the wildlife, and leave it the way you find it.Stephen Kakfwi\nAs global demand for natural resources increases, Kakfwi worries that neglect of the environment’s health could push the Earth out of balance. Dene prophecies have foreseen a future, he says, where the great Deh Cho River becomes just a little creek on its way to the Arctic Ocean.\n“The Earth maintains the balance. The air is good to us. The wildlife is good to us. The water’s bountiful. The temperatures and the climate are all made in order to sustain humans,” Kakfwi says, but it “can only handle so much.”\nWithin a few hundred years of the Industrial Revolution, “we are literally pushing the Earth to a point where it cannot rebound and take care of itself anymore. We have caused it perhaps irreparable damage,” he says.\n“The weather is changing. And our people have known that for many, many years. They see the changes. The permafrost is melting. The grounds are shifting. The Arctic ice is starting to melt. The people can read the signs. It’s caused us concern for many years, because we know, inherently, you have to keep a balance.”\nStill, Kakfwi is an optimist. In recent years, the Northwest Territories has protected millions of acres of the boreal that have ecological and cultural significance to indigenous people. These include the Horn Plateau National Wildlife Area (Edéhzhíe), Ramparts River National Wildlife Area (Ts’ude niline Tu’eyeta), and the proposed Thaidene Nene National Park Reserve. Nahanni National Park Reserve, created in 1972, was recently expanded and is now six times its original size.\nKakfwi believes that ongoing collaboration among governments, First Nations people, and industry in the boreal region is creating a path to economic prosperity without compromising the health of the environment.\n“We have more and more people now that come to the North, make it their home, want to keep the land and water [healthy], but also have some responsible, sustainable development occur,” he says.\nIndigenous people “have slowly won the battle, where now we are—by law and by policy and practice—now in the room when mines are being planned, oil and gas projects are being developed. We have a say. We are consulted. Our consent has to be often sought. … Now, in the Northwest Territories, they talk to us.”\nKakfwi has never forgotten what he learned as a boy, when he sought out a small stand of birch for healing. The way to connect with the land, to learn to respect and use it sustainably, is to pay attention to the details, the small things and quiet places that make up the greater whole.\nWhenever Kakfwi travels, he likes to take a walk to get oriented and to understand the new environment he is in. He studies the trees, the landscape, the birds overhead, and even the ants.\n“Anywhere you go, if you haven’t been there before, you will feel a little bit lost, because you cannot see past the next tree. And so we encourage people to go for walks, to pay attention,” he says.\n“And once you have that, your comfort zone is very, very different. You establish a relationship with yourself, with everything around you. And then you become a part of it.”\nPhotos by The Pew Charitable Trusts.\n235 million acresThe boreal forest region in the Northwest Territories covers 235 million acres, three times the size of Britain. About 88 percent is intact.\n2,014 feet deepGreat Slave Lake is the deepest lake in North America; its greatest depth is 2,014 feet.\n8th largestGreat Bear Lake is the largest unpolluted freshwater lake in the world, and the eighth largest in area.\n200 speciesThe boreal forest in the Northwest Territories is the breeding ground for 150 million to 500 million birds, representing more than 200 species.\n2nd longestThe Mackenzie River is the second longest in North America, behind the Mississippi River. It flows from south to north, into the Arctic Ocean.\n2xVirginia Falls, in Nahanni National Park Reserve, is twice the height of Niagara Falls.\n1stNahanni National Park Reserve, in the Mackenzie Mountains, was the first site designated by UNESCO as a World Heritage site.""]"	['<urn:uuid:e755847f-68ee-4590-9eea-6bbdfb716b1c>']	factoid	with-premise	short-search-query	similar-to-document	expert	2025-04-14T18:34:38.591736	41	146	14060
75	climate change impact rabbit range shifts	Climate change will force two-thirds of all lagomorph species to relocate. Rabbits, hares, and jackrabbits are likely to shift their ranges towards the poles, though the total size of their ranges may not change significantly. By 2100, almost a third of Earth's land area (31.5 million km²) will lose at least one species. Northern China could lose up to ten species, while areas like Montana and North Dakota might gain up to five species as they migrate from warming southern regions.	['Climate change will have major effects on the ecology and distribution of many animal species. Now new research suggests that rabbits will be particularly hard hit as climatic changes alter their habitats over the coming decades.\nRabbits, hares, and pikas could become this century’s new climate migrants—with up to two-thirds of species forced to relocate. There are almost certainly going to be extinctions among some of the more sensitive and less adaptable species.\nRabbits and their relatives, hares (referred to in North America as jackrabbits), and the lesser known pikas, belong to a group of mammals known as lagomorphs—of which there are 87 species worldwide.\nLagomorphs are particularly interesting to ecologists—and those of my colleagues who work in Global Food Security—as they are a major human food resource, a valued game species, agricultural pests, model lab animals, and key elements in food webs.\nYou can find rabbits, hares, and pikas almost everywhere, across a huge range of environmental conditions. They’re native to all continents except Antarctica, found from the equator to the Arctic, and from sea level to the very top of the Himalayas.\nA quarter of lagomorphs are already listed as threatened, and 13 species are endangered or critically endangered. We were particularly interested in how predicted changes in climate would affect this already highly vulnerable group.\nIn our study, colleagues from Queen’s University Belfast and I collated all known records of lagomorph species worldwide. Environmental conditions such as temperature or rainfall were correlated with the sites where each species occurred to establish the suitable habitat within which each can persist. Widely accepted climate models of projected future conditions were then used to extrapolate how suitable habitat would change.\nThe results, published in the open access scientific journal PLOS ONE suggest that two-thirds of all lagomorph species will be affected. Rabbits, hares, and jackrabbits are likely to shift towards the poles with little change in the total size of their range—the geographical area in which the species can be found.\nPikas meanwhile, are likely to shift to ever higher altitudes as the lower slopes warm up leading to huge range declines. This is likely to lead to the extinction of some such as Kozlov’s Pika Ochotona koslowi, a mysterious species unique to China.\nOf course the animals won’t just remain still while the climate changes around them—moving towards the poles or to higher ground is a standard strategy to track shifts in suitable habitat. Rabbits, hares, and jackrabbits can move long distances and can potentially move to cooler conditions without losing too much of their range; the effects of such shifts on ecosystems are largely unknown but likely to cause significant disruption.\nThe smaller and less bouncy pikas won’t be so lucky. Pikas inhabit generally cooler conditions in the high mountains of the Himalayas or Rockies and will be driven further upwards until no suitable habitat remains. My colleague Neil Reid, a conservation biologist and lagomorph expert at Queen’s, points out that “they will likely be pushed off the top of the mountains, literally, with total extinction the most probable outcome.”\nSpecies traits can be useful indicators of potential responses to climate change, yet have rarely been linked to changes in distributions. Smaller-bodied species were more likely to exhibit range contractions and shifts to higher ground, but species capable of having large numbers of offspring were more likely to shift towards the poles.\nThe effect of climate change on lagomorphs is predicted to be so substantial that almost a third of the Earth’s land area (31.5 million km2) will lose at least one species by 2100. It is predicted that northern China will lose up to ten species, whereas Montana and North Dakota in North America are likely to gain up to five species—climate rabbit refugees perhaps, fleeing the ever-warming southern states and Mexico. Generally, species on islands and mountains will be the hardest hit by changing temperatures.\nHowever predictive models are simplified versions of reality and as such are rough approximations of what seems likely to happen. Those we used did not account for the complexity of ecological systems, such as how species—like plants or predators—interact with lagomorphs.\nMoreover, small burrowing species such as the Pygmy rabbit Brachylagus idahoensis may be able to shelter from the effects of climate change, while larger species like the European hare Lepus europaeus may have to adapt to mitigate the effects of warming temperatures—for example in the way that the Antelope jackrabbit Lepus alleni uses its long ears to shed excess heat.\nSo we have to be careful in the interpretation of our models—but the consistency of the results across all lagomorph species does not paint a good picture of the future for the group.\nConservation strategies, such as assisted migration—where humans deliberately move species to areas of more suitable conditions, pre-empting future changes—may be one of the few options to save highly range-restricted species, even if it is highly controversial.\nCollection of more species records, particularly for already rare species, as well as targeting data-deficient geographic regions (such as Russia) will be vital in increasing our knowledge of the most threatened lagomorphs and informing future conservation management.']	['<urn:uuid:a1673367-10e3-4556-84c8-ebf7bc3a51a1>']	open-ended	with-premise	short-search-query	similar-to-document	expert	2025-04-14T18:34:38.591736	41	486	5478
76	How long can the ekranoplan operate without external support?	The ekranoplan can stay self-sufficient for 5 days.	['You can look at it from the outside in this post.\nLet’s climb inside this ship/plane.The entrance.\nThere is a massive bitt right near the door for various flotation devices.\nNear the same door there are fire-extinguishing appliances (have you seen anything like this on a plane?)\nThe view of the dock from the porthole of the opposite side entrance.\nWe’re moving towards the bow. On our way we pass a latrine located on the right.\nThere is a crew’s cabin on the same side (reminder: the ekranoplan can stay self-sufficient for 5 days)\nThe caboose is on the portside (take a look at the coverings)\nMoving on: there is a 500 liter (132 gallon) potable water tank on the right.\nA room with equipment.\nNow we are almost inside the bow. There should be a small anchor somewhere.\nInside the very tip of the bow there is a marine radar system.\nAll of the equipment is compliant with naval standards by size and weight.\nGoing back a little and takings the stairs up we enter the crew cabin. I assume this is the panel operator’s workplace.\nThe general view. The screen in the center is that of the marine radar located in the nose of the ekranoplan.\nThe co-pilot’s seat.\nI assume this is the engine control panel, but I don’t understand what the long handles are for. Maybe they are for the engine nozzles angle control or managing flaps during take-off.\nThe pilot’s side switches.\nThe pilot’s pedals.\nAir dehumidification devices are installed along all of the windows.\nThe co-pilot’s side panel.\nEquipment along the starboard.\nThe upper central control panel.\nThe view from the window at the pilot’s seat.\nLet’s take a closer look at the instruments on the central panel. These are located on the co-pilot’s side.\nEngine control instruments.\nThe upper panel.\nThe panel operator’s workplace.\nThere is another seat beside it, also has to do with something technical.\nThe view from the entrance to the cockpit.\nThe hatch for climbing up to the roof of the ekranoplan.\nThe way down from the cockpit, take a look at the naval type alarms.\nLet’s move on to the tail. Here are some instruments whose purpose is unclear to me.\nThere is a fire prevention system installed because there are TA-6a auxiliary propulsion units nearby.\nThere are two of them, one for each four of the engines, and if I’m not mistaken, they launch the engines of the ekranoplan. They are located on both sides.\nIf I am correct, this is test recording equipment.\nOn our way back we see the water tank again.\nI don’t know what this is, but there are numerous valves, typical of a ship.\nLet’s climb into the fire position located above\nThe cockpit under the first pair of the missile launchers.\nA few bottom views.\nAnd the view from the inside. Take a look at the rope with knots designed for evacuation in case there is an accident.\nThis is a BA-ZDP tower unit. This is some king of an electric alternating current engine located at a one of the sides. What is it for, I wonder?\nWe keep moving towards the tail.\nImpressive wiring system, but not as neatly installed as it is usually done on planes.\nA fire hose?\nOne of the hatches leading up to the tail unit horizontal stabilizers and radars that are installed there.\nThe bottom view of a shaft inside the tail unit. It is possible to climb to the second firing position and to the highest point of the ekranoplan by it.\nHalfway there (looking down).\nThe view downwards halfway.\nOn our way to the top we see the second firing position, there’s also some equipment.\nThe rear firing position.\nThe hatch, view from above.\nImages @ igor113']	['<urn:uuid:1e573f7d-32f4-47ca-bac6-5aebfbb47380>']	open-ended	with-premise	concise-and-natural	similar-to-document	expert	2025-04-14T18:34:38.591736	61	51	3546
77	problems combining computer software systems after business acquisition	When integrating two companies, they often have different enterprise systems with different workflows and integrations. The integration team must either migrate to common systems across the new enterprise or figure out how to integrate the different legacy systems. Usually, companies end up doing both - picking the best common system where migration is feasible and integrating systems where it is not.	['Think about the integration of lives when two people get married. Two existences, residences, and belongings become one, and often, the overlap in possessions means some decisions have to be made if it is to progress into a fully embedded union. Whose home? Which couch? Which set of dinnerware? Do we really need this beer sign collection?\nBusiness mergers and acquisitions reflect a similar commitment toward positive growth, but the amount of things to integrate once the ink dries becomes a whole different ballgame. In fact, integrating two companies into one, and all that entails, is extremely cumbersome and complex, to put it lightly.\nThere are people, buildings, supply chains, financial and legal constraints, sales and fulfillment processes, and a whole host of other considerations to manage, not the least of which is technology. These systems and applications running each company prior to the acquisition presumably were crucial to processes in their previous lives, which leaves some tough decisions to be made.\nIf you’re lucky, you’ll have some of the same enterprise systems in place at both organisations. Even if you do, there’s a good chance you’ve got different workflows and integrations around them, and it’s unlikely they’ll share common patterns.\nYour integration team, then, will be faced with two options:\n- Attempt to migrate to a common set of systems across the new enterprise.\n- Figure out how to integrate the different, disparate legacy systems\nThe reality is, you’re likely end up doing both – picking the best common system where migration is feasible and figuring out how to integrate systems where it is not. Even if you do select the best-in-class workflows for each enterprise system, the likelihood of doing new integrations to bring the systems together to form the optimal business process for the unified enterprise is very high.\nSimilar decisions need to be made around the choice of integration solutions in the new enterprise. Many companies have grown up with a range of integration tools and techniques fit for specific purposes and end up maintaining a disparate set of tools and skill sets to maintain this legacy architecture. Of course, this complexity only gets worse when two companies come together.\nSo how do stakeholders decide the path to proceed in a post-transaction world, where the systems and processes of two companies become one well-oiled machine to continue growing the business? What key considerations support a strategy for deciding which of the dozens of incoming products and services from a variety of vendors – many undoubtedly interspersed throughout the world – will make the cut?\nThe process begins with business and IT units finding common ground and getting on the same page. It’s not uncommon for the business side of organisations to assume the IT folks will just take care of the integration of systems into the post-transaction enterprise. However, an M&A transaction presents a great opportunity for business and IT to come together and determine how technology can best serve the needs of the new organisation.\nThe driving factor for coming to this agreement centres on the importance of data to any organisation, merging or otherwise. Data is the lifeblood of many modern enterprises, and it drives business decisions throughout organisations. In fact, the decision to merge/acquire was, without question, based on data-driven insights that led to decision-makers to pull the trigger on this particular business expansion.\nThe recommended course of action, then, is a single unified integration platform to consolidate and displace old integration technologies, integrate the business systems staying on post-merger, deliver high visibility to IT and business users alike, and support the seamless movement of data inside and outside the organisation. This solution should solve internal and B2B integration, and also accommodate one-off data migrations (for consolidation of existing data) and the ongoing integrations where it wasn’t feasible to choose a single system.\nHere are three considerations for enterprises experiencing technology overload in a post-acquisition organisation and how single-platform integration technology protects the critical exchange of information that will power the business and supply chain going forward.\nConsolidate Systems Applications\nPost-transaction, data quality is at risk in a disconnected environment of disparate, siloed solutions. Duplicate and obsolete records, combined with data inconsistency, lead to poor data quality, and your single greatest asset – data – is in jeopardy if there’s no formalisation of best practices or unified integration.\nThe goals of a smooth transition, then, include harmonised business and data management processes. And that’s where a standardised platform supporting application and B2B integration comes in to help the organisation:\n- Identify common data standards and approaches to sharing data, reducing exposure to costly data loss.\n- Replace outdated applications and systems with more efficient ones with expanded capabilities, including high-speed and large-file transfer.\n- Get rid of workflows that don’t work well in today’s hyper-digital ecosystem.\n- Reduce overlapping business partners and vendors performing services.\n- Adopt or adapt to fit current needs but also the organisation’s future needs.\nA flexible yet unified integration platform streamlines the architecture via expanded managed file transfer, secure file sharing and B2B/EDI processes, while also delivering such modern connectivity needs as application, cloud, and big data integration.\nCleaning out the proverbial IT closet positions the post-transaction enterprise for leaner, more refined communications processes as it faces its brave new world.\nAccelerate the Transaction’s Benefits\nWithout question, merging two companies is a strategic growth move for a greater good, and these two businesses look to gain multiple competitive benefits in the process. Integration is essential to the success of the acquisition, and if critical information flowing among applications, people, and systems is not consistent, realising any business value from the acquisition may be compromised.\nSome of the targeted benefits of a merger or acquisition include:\n- Uniting complementary products or services under one proverbial roof.\n- Increasing market share, customer base, and lines of business.\n- Combining resources to reduce costs, eliminate duplicated facilities or departments, and increase revenue.\n- Accessing new funds, assets, and infrastructure for further product development.\n- Gaining better production, development, or data facilities, which often can be more expensive to build than buy.\nDeploying a robust technology supporting strategic integration allows post-acquisition organisations to:\n- Streamline and automate processes for current customers, exceeding service-level agreements and reducing customer churn.\n- Offer flexible integration patterns, including support for multiple advanced protocols, to say “yes” to new business.\n- Connect to emerging cloud, hybrid, and API technologies to support agile operations and expanded development capabilities.\n- Support enterprise-wide visibility into past, present, and future workflows for data-driven analysis.\n- Increase data quality – from customers, partners, suppliers, etc. – for more accurate returns on big data initiatives.\nAn unintegrated, disjointed business ecosystem can bring processes to a slow crawl, but a fully integrated environment accelerates delivery of the competitive advantages to be gained when two companies join forces.\nDrive Organisational Flexibility\nThe power of a unified integration platform lies not only in the ability to integrate technical systems but also to integrate and empower human resources. Because of the complexity of the transaction, many companies designate integration liaisons to manage specific projects and delegate workloads.\nWhen the heavy lifting is done, however, these project managers fail to hand off some of the IT integration duties. Arming business users with capabilities to deploy easy-to-use, IT-approved systems frees up the IT department down the road from addressing single, tactical workflow problems.\nHowever, this citizen integration functionality, as it is known, only works if the solutions are actually easy to deploy and use. The need for security and governance shouldn’t outweigh the need for ease of use among end users, and an advanced integration platform, with an intuitive interface and strong admin controls, delivers both. The organisation then can move its more capable IT resources onto advanced projects that add business value and drive new revenue.\nThe strategy of enabling citizen integrators helps keep the business from regressing into a pre-transaction scenario where multiple rogue solutions – consumer-grade cloud applications, FTP servers, and more – were introduced only because employees needed something to complete their daily responsibilities. It also inspires confidence in team members when they see a tight data communications workflow and corporate security policy that also charges business users with helping them help themselves.\nWith the increases in technologies, endpoints, employees, vendors, and more coming into an enterprise after a merger, competitive enterprises cannot waste the opportunity to consolidate systems and align resources to build a better business future. A stronger business will emerge when an organisation deploys a single-platform integration solution for all of its application, B2B, cloud, big data, ad-hoc, and citizen integration needs.\nMergers and acquisitions are already massive, complex transactions, but organisations choosing reliable integration technology can protect the integrity of their mission-critical information and enable the efficient flow of business throughout this tumultuous time.\nA single-platform communications solution delivers the migration and integration balance necessary for thriving in a post-acquisition business ecosystem.\nDave Brunswick, vice president of solutions at Cleo (opens in new tab)\nImage source: Shutterstock/Kritchanut']	['<urn:uuid:b05bbb1a-8e7c-433d-ab7a-74dec2ad4ef7>']	factoid	direct	long-search-query	distant-from-document	novice	2025-04-14T18:34:38.591736	71	404	10211
78	What was the first time computers managed to beat humans at games like chess, and how did that happen?	A significant milestone was reached when IBM's Deep Blue defeated world chess champion Gary Kasparov. Later, in an even more complex challenge, Google DeepMind's AlphaGo defeated world champion Go player Lee Sedol. The victory over Lee Sedol was particularly significant because the ancient Chinese game of Go was considered a major hurdle in AI development due to its complexity.	"['What is Artificial Intelligence? How Does AI Work?\nArtificial intelligence (AI) is wide-ranging branch of computer science concerned with building smart machines capable of performing tasks the typically require human intelligence. AI is an interdisciplinary science with multiple approaches, but advancements in machine learning and deep learning are creating a paradigm shift in virtually every sector of the tech industry.\nCan machines think? — Alan Turing, 1950\nLess than a decade after breaking the Nazi encryption machine Enigma and helping the Allied Forces win World War II, mathematician Alan Turing changed history a second time with a simple question: ""Can machines think?""\nTuring\'s paper ""Computing Machinery and Intelligence"" (1950), and it\'s subsequent Turing Test, established the fundamental goal and vision of artificial intelligence.\nAt it\'s core, AI is the branch of computer science that aims to answer Turing\'s question in the affirmative. It is the endeavor to replicate or simulate human intelligence in machines.\nThe expansive goal of artificial intelligence has given rise to many questions and debates. So much so, that no singular definition of the field is universally accepted.\nThe major limitation in defining AI as simply ""building machines that are intelligent"" is that it doesn\'t actually explain what artificial intelligence is? What makes a machine intelligent?\nIn their groundbreaking textbook Artificial Intelligence: A Modern Approach, authors Stuart Russell and Peter Norvig approach the question by unifying their work around the theme of intelligent agents in machines. With this in mind, AI is ""the study of agents that receive percepts from the environment and perform actions."" (Russel and Norvig viii)\nNorvig and Russell go on to explore four different approaches that have historically defined the field of AI:\n- Thinking humanly\n- Thinking rationally\n- Acting humanly\n- Acting rationally\nThe first two ideas concern thought processes and reasoning, while the others deal with behavior. Norvig and Russell focus particularly on rational agents that act to achieve the best outcome, noting ""all the skills needed for the Turing Test also allow an agent to act rationally."" (Russel and Norvig 4).\nPatrick Winston, the Ford professor of artificial intelligence and computer science at MIT, defines AI as ""algorithms enabled by constraints, exposed by representations that support models targeted at loops that tie thinking, perception and action together.""\nWhile these definitions may seem abstract to the average person, they help focus the field as an area of computer science and provide a blueprint for infusing machines and programs with machine learning and other subsets of artificial intelligence.\nWhile addressing a crowd at the Japan AI Experience in 2017, DataRobot CEO Jeremy Achin began his speech by offering the following definition of how AI is used today:\n""AI is a computer system able to perform tasks that ordinarily require human intelligence... Many of these artificial intelligence systems are powered by machine learning, some of them are powered by deep learning and some of them are powered by very boring things like rules.""\nArtificial intelligence generally false under two broad categories:\n- Narrow AI: Sometimes referred to as ""Weak AI,"" this kind of artificial intelligence operates within a limited context and is a simulation of human intelligence. Narrow AI is often focused on performing a single task extremely well and while these machines may seem intelligent, they are operating under far more constraints and limitations than even the most basic human intelligence.\n- Artificial General Intelligence (AGI): AGI, sometimes referred to as ""Strong AI,"" is the kind of artificial intelligence we see in the movies, like the robots from Westworld or Data from Star Trek: The Next Generation. AGI is a machine with general intelligence and, much like a human being, it can apply that intelligence to solve any problem.\nNarrow Artificial Intelligence\nNarrow AI is all around us and is easily the most successful realization of artificial intelligence to date. With its focus on performing specific tasks, Narrow AI has experienced numerous breakthroughs in the last decade that have had ""significant societal benefits and have contributed to the economic vitality of the nation,"" according to ""Preparing for the Future of Artificial Intelligence,"" a 2016 report released by the Obama Administration.\nA few examples of Narrow AI include:\n- Google search\n- Image recognition software\n- Siri, Alexa and other personal assistants\n- Self-driving cars\n- IBM\'s Watson\nMachine Learning & Deep Learning\nMuch of Narrow AI is powered by breakthroughs in machine learning and deep learning. Understanding the difference between artificial intelligence, machine learning and deep learning can be confusing. Venture capitalist Frank Chen provides a good overview of how to distinguish between them, noting:\n""Artificial intelligence is a set of algorithms and intelligence to try to mimic human intelligence. Machine learning is one of them, and deep learning is one of those machine learning techniques.""\nSimply put, machine learning feeds a computer data and uses statistical techniques to help it ""learn"" how to get progressively better at a task, without having been specifically programmed for that task, eliminating the need for millions of lines of written code. Machine learning consists of both supervised learning (using labeled data sets) and unsupervised learning (using unlabeled data sets).\nDeep learning is a type of machine learning that runs inputs through a biologically-inspired neural network architecture. The neural networks contain a number of hidden layers through which the data is processed, allowing the machine to go ""deep"" in its learning, making connections and weighting input for the best results.\nArtificial General Intelligence\nThe creation of a machine with human-level intelligence that can be applied to any task is the Holy Grail for many AI researchers, but the quest for AGI has been fraught with difficulty.\nThe search for a ""universal algorithm for learning and acting in any environment,"" (Russel and Norvig 27) isn\'t new, but time hasn\'t eased the difficulty of essentially creating a machine with a full set of cognitive abilities.\nAGI has long been the muse of dystopian science fiction, in which super-intelligent robots overrun humanity, but experts agree it\'s not something we need to worry about anytime soon.\nIntelligent robots and artificial beings first appeared in the ancient Greek myths of Antiquity. Aristotle\'s development of the syllogism and it\'s use of deductive reasoning was a key moment in mankind\'s quest to understand its own intelligence. While the roots are long and deep, the history of artificial intelligence as we think of it today spans less than a century. The following is a quick look at some of the most important events in AI.\n- Warren McCullough and Walter Pitts publish ""A Logical Calculus of Ideas Immanent in Nervous Activity."" The paper proposed the first mathematic model for building a neural network.\n- In his book The Organization of Behavior: A Neuropsychological Theory, Donald Hebb proposes the theory that neural pathways are created from experiences and that connections between neurons become stronger the more frequently they\'re used. Hebbian learning continues to be an important model in AI.\n- Alan Turing publishes ""Computing Machinery and Intelligence, proposing what is now known as the Turing Test, a method for determining if a machine is intelligent.\n- Harvard undergraduates Marvin Minsky and Dean Edmonds build SNARC, the first neural network computer.\n- Claude Shannon publishes the paper ""Programming a Computer for Playing Chess.""\n- Isaac Asimov publishes the ""Three Laws of Robotics.""\n- Arthur Samuel develops a self-learning program to play checkers.\n- The Georgetown-IBM machine translation experiment automatically translates 60 carefully selected Russian sentences into English.\n- The phrase artificial intelligence is coined at the ""Dartmouth Summer Research Project on Artificial Intelligence."" Led by John McCarthy, the conference, which defined the scope and goals of AI, is widely considered to be the birth of artificial intelligence as we know it today.\n- Allen Newell and Herbert Simon demonstrate Logic Theorist (LT), the first reasoning program.\n- John McCarthy develops the AI programming language Lisp and publishes the paper ""Programs with Common Sense."" The paper proposed the hypothetical Advice Taker, a complete AI system with the ability to learn from experience as effectively as humans do.\n- Allen Newell, Herbert Simon and J.C. Shaw develop the General Problem Solver (GPS), a program designed to imitate human problem-solving.\n- Herbert Gelernter develops the Geometry Theorem Prover program.\n- Arthur Samuel coins the term machine learning while at IBM.\n- John McCarthy and Marvin Minsky found the MIT Artificial Intelligence Project.\n- John McCarthy starts the AI Lab at Stanford.\n- The Automatic Language Processing Advisory Committee (ALPAC) report by the U.S. government details the lack of progress in machine translations research, a major Cold War initiative with the promise of automatic and instantaneous translation of Russian. The ALPAC report leads to the cancellation of all government-funded MT projects.\n- The first successful expert systems are developed in DENDRAL, a XX program, and MYCIN, designed to diagnose blood infections, are created at Stanford.\n- The logic programming language PROLOG is created.\n- The ""Lighthill Report,"" detailing the disappointments in AI research, is released by the British government and leads to severe cuts in funding for artificial intelligence projects.\n- Frustration with the progress of AI development leads to major DARPA cutbacks in academic grants. Combined with the earlier ALPAC report and the previous year\'s ""Lighthill Report,"" artificial intelligence funding dries up and research stalls. This period is known as the ""First AI Winter.""\n- Digital Equipment Corporations develops R1 (also known as XCON), the first successful commercial expert system. Designed to configure orders for new computer systems, R1 kicks off an investment boom in expert systems that will last for much of the decade, effectively ending the first ""AI Winter.""\n- Japan\'s Ministry of International Trade and Industry launches the ambitious Fifth Generation Computer Systems project. The goal of FGCS is to develop supercomputer-like performance and a platform for AI development.\n- In response to Japan\'s FGCS, the U.S. government launches the Strategic Computing Initiative to provide DARPA funded research in advanced computing and artificial intelligence.\n- Companies are spending more than a billion dollars a year on expert systems and an entire industry known as the Lisp machine market springs up to support them. Companies like Symbolics and Lisp Machines Inc. build specialized computers to run on the AI programming language Lisp.\n- As computing technology improved, cheaper alternatives emerged and the Lisp machine market collapsed in 1987, ushering in the ""Second AI Winter."" During this period, expert systems proved too expensive to maintain and update, eventually falling out of favor.\n- Japan terminates the FGCS project in 1992, citing failure in meeting the ambitious goals outlined a decade earlier.\n- DARPA ends the Strategic Computing Initiative in 1993 after spending nearly $1 billion and falling far short of expectations.\n- U.S. forces deploy DART, an automated logistics planning and scheduling tool, during the Gulf War.\n- IBM\'s Deep Blue beats world chess champion Gary Kasparov\n- STANLEY, a self-driving car, wins the DARPA Grand Challenge.\n- The U.S. military begins investing in autonomous robots like Boston Dynamic\'s ""Big Dog"" and iRobot\'s ""PackBot.""\n- Google makes breakthroughs in speech recognition and introduces the feature in its iPhone app.\n- IBM\'s Watson trounces the competition on Jeopardy!.\n- Andrew Ng, founder of the Google Brain Deep Learning project, feeds a neural network using deep learning algorithms 10 million YouTube videos as a training set. The neural network learned to recognize a cat without being told what a cat is, ushering in breakthrough era for neural networks and deep learning funding.\n- Google makes first self-driving car to pass a state driving test.\n- Google DeepMind\'s AlphaGo defeats world champion Go player Lee Sedol. The complexity of the ancient Chinese game was seen as a major hurdle to clear in AI.']"	['<urn:uuid:a3d34fbd-2e26-4147-b4d2-50ee6b54084d>']	open-ended	direct	verbose-and-natural	distant-from-document	novice	2025-04-14T18:34:38.591736	102	380	12614
79	As a caregiver helping patients recover from injuries, I'm wondering what sorts of daily activities occupational therapists help people relearn to do independently?	Occupational therapy treatments focus on activities of daily living such as feeding, dressing, and grooming. They also address home safety and management, upper extremity and fine motor exercises, and caring for splints.	"[""Rehabilitation Services - Physical, Occupational, & Speech Therapy\nPhysical TherapyWith many illnesses or injuries, a variety of movement and functionality deficits may occur. Physical therapists are the hands-on specialists who evaluate, treat, and help prevent these problems. They work directly with patients on an inpatient or outpatient basis to help increase movement potential and decrease pain, while aiding recovery and achieving therapy goals.\nWho can benefit from physical therapy?Physical therapy helps many types of people, from someone learning to walk again after a stroke to a person regaining use of an injured hand. Among people who most commonly benefit from physical therapy are those who have experienced:\nBack and neck problems\nArthritis and joint pain\nHow physical therapy can helpOnce the patient's specific needs are determined, physical therapy can help restore function and develop skills that will increase mobility and safety. Specifically, physical therapy most often involves the following treatments to help achieve the highest possible level of health:\nRange of motion exercises\nTherapeutic exercise to improve strength and endurance\nFunction mobility skills\nJoint and soft tissue mobilization\nOccupational TherapyThe goal of occupational therapy is to help people re-establish some or all of their independence as they recover from injury or illness. Therapy programs include individualized assessment of home and job situations, as well as treatment and training to rebuild skills and strength. For some patients, that could be simply learning to use a spoon or tie shoes again or it may include finding alternative methods to help accomplish daily tasks and activities.\nWho can benefit from occupational therapy?Occupational therapy helps people whose ability to remain self-sufficient has been disrupted by illness, injury, developmental disability or the normal aging process. While it can help an injured worker recover and return to work, occupational therapy can help other people as well, from children with disabilities to adults who may have trouble properly grooming or feeding themselves for a variety of reasons. The therapist not only helps the patients, but also assists caregivers and family members. Some of the patients who benefit from occupational therapy include those who have experienced:\nCumulative trauma disorder\nHow occupational therapy can helpOccupational therapy programs are designed to identify specific needs and provide treatment that can bring back health, confidence and self-sufficiency in a work-related, as well as home environment.\nTreatments often focus on:\nActivities of daily living (feeding, dressing, grooming)\nHome safety and management\nUpper extremity, fine motor and therapeutic exercise programs\nCaring for splints\nSpeech-Language TherapySpeech-language therapy involves a variety of treatments to help improve communication and swallowing.\nWho can benefit from speech-language therapy?Speech-language therapy can help people of all ages with many types of communication disorders, whether from ongoing difficulties or a sudden injury. Patients who benefit from speech-language therapy include those who have experienced:\nHead or neck injury\nHow speech-language therapy can helpSpeech-language therapy can enhance the patient's quality of life by improving many aspects of communication.\nTreatments often focus on:\nVerbal and written expression\nAlternative communication systems\nStuttering or stammering\nThe inability to form words\nHearing impairment rehabilitation\nDevelopmental speech-language delay/disorder\nImproving memory skills\nContact UsKim Roberts""]"	['<urn:uuid:82e90923-0465-4b27-86ad-c9e6a292fde9>']	factoid	with-premise	verbose-and-natural	distant-from-document	expert	2025-04-14T18:34:38.591736	164	220	3639
80	what does latin word abante mean origin of word advance	The word advance originates from the Latin word abante, which literally means 'in front of.' In Latin, ab means 'away from,' and ante means 'before.' Therefore, the word advance means moving away from what was before.	['The icon that appears in your internet browser for this web site is a symbol for Tai, or Advance. Advance is the eleventh hexagram of the I Ching, one of the oldest of the Chinese classic texts. Our word advance originates from the Latin word abante, which literally means “in front of.” In Latin, ab means “away from,” and ante means “before.” The word advance means moving away from what was before. In modern English, advance can have many meanings:\n- To move or bring forward.\n- To improve.\n- To bring forward in time.\n- To move or to go forward; proceed.\n- To increase in quantity, value, price, etc.\n- To improve or make progress.\n- Forward movement; progress in space.\n- Having gone beyond others or beyond the average.\nEach hexagram of the I Ching is formed from the combination of two trigrams, or gua, which are comprised of three separate lines, each line representing either yin or yang. The interplay of yin and yang is represented by the Taijitu (the “diagram of the supreme ultimate”), which graphically displays the dynamic interplay of complementary opposites interacting within a greater whole. The diagram illustrates that the interaction between yin and yang is fluid, and the smaller circles indicate that yin and yang each contain the seed of the other.\nIn the I Ching, yin is traditionally represented by broken lines and yang is represented by solid lines. A hexagram is created beginning from the bottom and working upwards, the first three lines forming the lower trigram, and the second three lines forming the upper trigram. Each trigram represents certain qualities, which may be properly or improperly balanced. The lower trigram represents the inner state of the person; the upper trigram represents the outer state of the person, or their current situation.\nTai is formed with the lower, or inner, trigram of Heaven (Qian) and the upper, or outer, trigram of Earth (Kun). Heaven, represented by three solid lines, is pure yang. When properly balanced, Heaven represents strength, firmness, and creativity; when imbalanced, Heaven indicates force, aggression, and arbitrariness. Earth, represented by three broken lines, is pure yin. When properly balanced, Earth represents submission, receptivity, and flexibility; imbalanced, Earth indicates weakness, a lack of autonomy, and dependency.\nThe Chinese designation for this hexagram, Tai, is one of the most favorable words in the Chinese language. Tai indicates a condition of being more than great, and originally meant “more than” or “most.” Tai has been translated as tranquility or peace, and suggests progressing, proceeding, advancing. The last suggestion is the inspiration for our theme – advance.\nStrength and flexibility are the essence of yang and yin. Heaven epitomizes pure strength, and earth pure flexibility. When imbalanced, too much strength is applied outwardly, and there is aggression and rigidity; and too much flexibility occurs within, producing weakness and dependency. External strength combined with internal weakness results in many obstructions in our lives. Tranquility, or advance, arises from mastering strong energy within while applying a flexible nature outwardly, strictly governing oneself while being generous when dealing with others. This leads to an open mind and a more noble character, making advance possible.\nEach hexagram of the I Ching can also be represented by an ideograph – a picture that conveys the essence of its message. The ideograph for Tai consists of two parts. The upper portion, representing greatness, depicts a human being standing with arms and legs wide open. Underneath is running water, proceeding forward unimpeded. The message we can derive from this hexagram, which is also an inspiration for our theme, is:\nthe small goes, the great comes\nThe small is yin. The great is yang. “The small goes” because yin submits. “The great comes” because yang is strong. Thus, it is possible to accomplish great things when the flexibility of yin and the strength of yang are harmoniously combined and kept balanced. With the strength of yang on the inside and the flexibility of yin on the outside, staying true to our core principles yet keeping an open mind, we learn to govern ourselves and make great progress toward our goals. Tai represents heaven and earth moving together in harmony. Successful activity needs strength and flexibility to be unified. This is tranquility. This is peace.\nOf course, this goal is a process — a process of advancing and withdrawing, acquiring and consolidating, taking two steps forward and one step backward, all with proper timing. The I Ching provides some advice for successfully traversing this path.\n- All the activities of our lives are interconnected. What happens in one sphere of our life affects all of the others. While each distinct activity of our lives needs to be separated from the rest in order to be examined and improved, we need to keep in mind that all of our activities are related and form the totality of who we are. We can maintain a harmonious life balance by being mindful of how each individual part relates to the whole.\n- To be truly effective we need to cultivate the ability to be totally involved in the activity we are carrying on at any one moment. Concentrate completely on one activity at a time, and do not allow concerns in other areas of life to distract from the task at hand. The psychologist Mihaly Csikszentmihalyi calls this ability “flow.” This is the essence of mindfulness.\n- Do not seek to impress others, or yourself. Passages from the great wisdom literature, including the Tao Te Ching and the Bhagavad Gita, teach us to do our work for the sake of the work, and not for any anticipated benefit. And once our work is done, we should simply let it go. “Pride goes before destruction.” Modesty brings longer-lasting inner rewards.\n- Do not seek to influence others, or to change them for what you may think is “the better.” We have a natural propensity to “fix” people, to correct what we perceive to be their faults, to control them, to make them more like we want them to be. Rather than make both others and ourselves unhappy, we need to recognize and appreciate our differences, and accept everyone for who and what they are, just as we would like them to accept us for who and what we are.\n- Realizing the interconnection and oneness of nature, our natural social interactions should embody kindness and generosity. In an age of increasing globalization that fosters the interactions of diverse cultures, there is a genuine need for open and mutual communication and, as we face the unique challenges that confront civilization as a whole, cooperation must triumph over selfishness.\n- Nature proceeds in cycles. Attempting to alter what is inevitable impedes progress and causes inner strife. Life is change. Extremes gravitate toward their opposites. Resolve first to adapt, then to embrace, change rather than resist it. The easiest way to embrace change is to adopt an attitude of equanimity, to be flexible on the outside — by keeping an open mind.\n- As we learn from Ecclesiastes, there is a time for everything. There is a time to strive and advance; there is a time to ease up and consolidate. If we act at the proper time, and in the proper measure, being strong when it is appropriate, and being flexible when it is appropriate, then advance and tranquility are possible.\nSo we move forward according to the principles of Tai — balancing our inner core principles with the flexibility of an open mind — to examine life and advance from what was before.\n© 2012 WS Nadolny\nThere are many fine translations and commentaries on the I Ching. For the reader interested in exploring the subtle messages of the text, The Taoist I Ching (Shambhala Classics) by Thomas Cleary is highly recommended. To explore this ancient oracle in more depth, The Complete I Ching by Taoist Master Alfred Huang is an excellent reference.']	['<urn:uuid:fed902ec-5ca5-4f89-bb00-58de1cd2590b>']	factoid	with-premise	long-search-query	similar-to-document	novice	2025-04-14T18:34:38.591736	55	217	7953
81	curious about water in space is water on moons same as earth water chemical composition	Yes, the water found in space is exactly the same as Earth's water - it's the same H2O molecule. This is because the cloud from which the sun and planets formed contained the same basic materials. There's abundant water throughout our solar system, especially in the outer regions where it can freeze into moons.	['Planetary scientist Dr. Linda Spilker and NASA Jet Propulsion Laboratory (JPL) scientist Dr. Tom Spilker both worked on the Voyager space exploration project, sending two unmanned spacecraft beyond the boundaries of our solar system, which is examined in PBS NOVA’s THE FARTHEST – VOYAGER IN SPACE. Linda Spilker continues to work on the Cassini mission, exploring Saturn, its moons and its rings, documented in PBS NOVA’s DEATH DIVE TO SATURN. Both programs air on PBS Wednesday, September 13 (check local listings for times).\nAs their surnames suggest, the Doctors Spilker are married to one another. Linda Spilker wears a ring, designed by Tom Spilker in honor of their twentieth wedding anniversary that depicts Saturn surrounded by its rings.\nASSIGNMENT X: Did you meet while working on the Voyager project, or did you already know each other?\nTOM SPILKER: No. We met after the Voyager project.\nLINDA SPILKER: We were both working at JPL, and we met after Voyager had ended for both of us. We both worked on Voyager through the Neptune fly-by in 1989, and then we didn’t meet and get married until 1997. We have a lot in common.\nAX: What was your role on Voyager?\nTOM SPILKER: I was with the radio science support team as a graduate student, and we did things to make sure that the radio science experiments that we did at each one of the planets worked out properly.\nLINDA SPILKER: I worked on Voyager as the experiment rep for the Infrared Instrument. That just meant I was the interface between JPL and the science team. I really had a lot of fun with that. I had just gotten out of college when I started at JPL, and I learned so much about what Voyager was doing, and really, really enjoyed that opportunity, and actually used Voyager data to write my PhD thesis. I went back to UCLA, used Voyager data of the occultations of the rings, watching as a star goes behind the rings. So I feel really a connection, since I used that data for the next step in my career.\nAX: What did the data illustrate?\nLINDA SPILKER: For the rings [of the planets], it showed an incredible amount of detail. We [originally[ thought the rings were just these bland sheets of material, and instead, there were all of these different structures, these features in the rings, down to a hundred-feet resolution. It was tremendously exciting to go from one idea that was wrong to seeing something up close in such detail.\nAX: Are there elements in the rings that we would find here on Earth, or are they particular to Saturn?\nLINDA SPILKER: It turns out that all four of the outer planets have rings – Jupiter, Saturn, Uranus and Neptune all have ring systems, and the rings are made of individual particles, millions of them, that orbit around Saturn. They bump into each other, and they create these beautiful waves and beautiful structures. The Earth doesn’t have a ring, and the moon doesn’t have a ring, so it’s really something unique. But what’s neat about the rings is, they’re an analogy for the disc from which the solar system may have formed. So understanding how the rings work might tell us how planets formed around our sun.\nThe particles in Saturn’s rings are mostly made of water ice, ninety-nine percent water ice. So you’d find that in your freezer as ice cubes. There’s only a little tiny bit of material [in the rings] that’s not ice. We don’t know for sure if it’s silicates, or maybe organics, or iron, or exactly what it’s made of, but the rings are everyday material, water ice.\nAX: Do you know why some planets have rings and some don’t?\nLINDA SPILKER: It looks like all the giant planets have ring systems, but the inner planets are probably too small to hold onto the particles you’d need to make a ring. In the case of the Earth, since we have a great big moon, it would kick out any ring particles that tried to circle around the Earth.\nAX: So it has to do with the gravitational pull of the planet, and the smaller planets don’t have enough gravity to make rings?\nLINDA SPILKER: Right. Although what’s interesting is, we found an asteroid, or actually it’s what we’d call a trans-Neptunian object, a TNO, it’s like an asteroid, but further out in the solar system, and here’s this body, Chariklo, a little tiny thing, that has two rings around it. So you never know what can get a ring or not.\nAX: Both Voyager I and Voyager II launched in 1977. What years did the Voyager project encompass for you?\nLINDA SPILKER: I think the mission first got started, building the spacecraft in 1972. Voyager flew by Jupiter, both of the Voyagers, in 1979, two fly-bys of Saturn 1980, 1981, a Uranus fly-by in 1986, and a Neptune fly-by in 1989, and I have two daughters and I tell them that their births are based on the alignment of the planets, because there was a five-year window between the Saturn fly-by and the Uranus fly-by, and so I, like a lot of moms on Voyager, started their families and had their families in that five-year window.\nAX: What sorts of things are we learning from Voyager?\nLINDA SPILKER: Well, some of it, by understanding the weather on these other planets – they’re simpler, they don’t have a solid surface, they’re mostly just gas, hydrogen and helium. And then, like I said, for the ring systems, like at Saturn, studying and understanding how the ring particles interact, how they form bigger particles, can tell us something about how the planets formed in the early solar system. So that’s going back a ways, but that’s something. And also, two of the moons that Voyager observed up close, Enceladus at Saturn and Europa at Jupiter, have liquid water oceans underneath their icy crusts. And we wonder if there might be life in those oceans. So Voyager started this incredible progression of our search for knowledge, to try and answer questions that led to missions like Galileo and Juno and Jupiter, Cassini in its orbit of Saturn. I now work on the Cassini mission – I’ve been with Cassini for almost thirty years.\nThe giant planets are just mostly a giant atmosphere, and then as you get deeper into the planet, the atmosphere compresses, until the hydrogen becomes a liquid. It gets so much pressure, like a million atmospheres’ pressure, the hydrogen becomes a liquid. So there’s no solid surface, like the Earth has. There might be a little rocky core right in the middle, but no surface we could stand on if we went to a giant planet.\nAX: So when we look at the ringed planets, what we’re seeing when we look at the planet at is essentially a giant ball of atmosphere, with a little core, but we don’t see the core?\nLINDA SPILKER: Right. We don’t see the core, it’s too far down and it would be too hot, and you’d be crushed before you ever got to it.\nAX: And this is why Cassini is going to crash into Saturn, because the spacecraft can’t actually land on it …?\nLINDA SPILKER: But you could land on the moons. The moons are made mostly of water ice, just like the rings, and you could go back and land on one of the moons in these places.\nAX: And is the water actually like water here on Earth?\nLINDA SPILKER: It’s exactly like the water [on Earth]. It’s water ice, the same thing that we think of that we have here on Earth. H2O. Exactly the same.\nAX: Does that fact tell us anything about either the nature of water or the nature of things in the universe?\nLINDA SPILKER: I think that tells us that the cloud from which the sun and the planets formed was basically the same material, and so whether you’re at the Earth or further out, all the way out to Pluto, it’s the same water, the same H2O. There’s a lot of water in our solar system, and as you go further from the sun, that water doesn’t have a chance to evaporate, so it can freeze into the moons in the outer solar system. In the outer solar system, water ice is kind of the rock. It gets frozen solid, hard as a rock, and then you start to get interesting things like, one of the moons around Saturn, Titan, has liquid methane in lakes and seas at its north pole. So pretty soon, it gets really cold, water ice is a rock, but then these other things can condense and become a liquid, so that’s really intriguing, too.\nAX: Are any of the Voyager project workers actually STAR TREK fans who call it “V’ger,” as it is referred to in STAR TREK: THE MOTION PICTURE?\nLINDA SPILKER: [laughs] I don’t call it V’ger, but I love that movie, and in fact, I have a fun story. Some friends of mine went to see it and I had no idea what it was about. So they told me, “You have to go see this movie.” So I went to see it, and we got really good seats toward the front, and I hadn’t figured out it was Voyager until the very end, and as they’re walking down the steps and there’s the Voyager spacecraft, all my friends are looking at me to see what I would say when it turned out, here’s Voyager. “Oh, my gosh.” I was totally shocked. I love this movie.\nWe don’t call it V’ger, we call it Voyager. But we think of Voyager very fondly, and there’s a special place in my heart, since Voyager is the first mission I worked on right out of college. To think that it’s still going strong, still out there – Voyager II is in interstellar space, and it’s amazing to think about that. It is actually out in the material between the stars and going strong. I’m very proud to have worked on Voyager.\nAX: What is Voyager’s measurement technology based on? That is, computers are based on the Enigma Machine – what is that technology based on?\nTOM SPILKER: It’s a thing called an ultra-stable oscillator. They generated these as part of the post-World War II technology development. You can make something that will make the frequency of a radio transmitter, using one of these ultra-stable oscillators as the reference, make that frequency exquisitely stable, and so that you know this is exactly what frequency this spacecraft is transmitting. And so you can measure things like Doppler shift, which tells you about the gravity of the thing that it’s flying by. You can also, when the signal is going through the atmosphere, measure how much that atmosphere changes the frequency.\nAX: What do the signals tell you?\nTOM SPILKER: We use the radio waves – the way they propagate, the way they travel through things like atmospheres – we send the spacecraft such that it goes behind the planet as seen from Earth, and just as it’s going behind the planet, those radio waves are traveling through the planet’s atmosphere to get to Earth. We look at how that atmosphere changes those radio waves, and we can tell all kinds of things about that atmosphere – how much atmosphere is there, what its pressure is, what its temperature is, what chemical species in some cases are in that atmosphere, like ammonia. We’re very sensitive to ammonia, so we can make those measurements that none of the other instruments can make. And then there are other experiments that we can do with a radio signal that will tell us, how much does this planet weigh, how much does this moon weigh, how much mass is there? And it gives us a very, very accurate measurement of how much stuff is in that planet, or how much stuff is in that moon. It goes back to figuring out where the solar system came from. Knowing those things is very important for that.\nThis interview was conducted during PBS Nova’s party for the summer 2017 Television Critics Association (TCA) press tour at the Beverly Hilton Hotel.\nFollow us on Twitter at ASSIGNMENT X\nLike us on Facebook at ASSIGNMENT X\nArticle Source: Assignment X\nArticle: Exclusive Interview with scientists Linda Spilker and Tom Spilker on the new PBS NOVA series THE FARTHEST – VOYAGER IN SPACE']	['<urn:uuid:625376df-7583-45d7-8287-3fba39ab985f>']	factoid	with-premise	long-search-query	similar-to-document	novice	2025-04-14T18:34:38.591736	87	312	11594
82	what killed traditional newspaper business	Internet platforms have destroyed the business model that sustained newspapers for most of the last century. The emergence of search and social media, combined with the rise of talk radio and opinion broadcasts presented as news, plus twenty years of layoffs and shutdowns at news outlets, have radically altered the news landscape.	['Knight Media Forum 2019: Welcome remarks by Knight Foundation President Alberto Ibargüen\nInformed and engaged communities are the bedrock of a healthy democracy.\nBut that bedrock has begun to quake. We meet today not just to describe the problem, but to find solutions.\nWe’re here today because the way we inform ourselves is insufficient to meet the demands of our democratic republic. If you do not have a reliably informed citizenry, you will not have a functioning democracy. It’s that simple.\nAnd we’re here because we think one of the three ways forward is reinvigorating local news.\nThe emergence and quick dominance of search and social media; the rise of talk radio and opinion broadcasts presented as news; twenty years of layoffs and shutdowns at national and local news outlets—these changes have radically altered our news landscape. Internet platforms have decimated the business model that sustained newspapers for most of the last century, while concentrating data, and therefore power, in a few, global hands.\nThe result is that the local news ecosystem that was so uniquely and valuably American is in disarray. Communities are left without a broadly shared, common baseline of facts about what just happened, about where we are, about who we are.\nBut it’s in those local communities where we have the greatest chance to rebuild the trust we’ve lost in our institutions, in our news, in our leaders, and in ourselves.\nThe shorter the distance between our neighbors and our news, the stronger our community. There is strength in local, and local leads to trust.\nWhen there is distance, however, between news and reader, event and citizen, trust erodes and our social fabric frays.\nHaving spent most of my adult life supporting local journalism and its essential role in our democracy, I find this deeply troubling.\nBut I am also buoyed by a different emotion: hope.\nI confess to being a prisoner of hope. I believe, no matter the obstacles, in our ability to do better.\nThat belief is anchored by history and knowledge of what we’ve overcome. As an example, before Gutenberg mechanized the printing press, there was order. Publications were few and often carried the Church’s imprimatur. Everyone knew what to believe. After Gutenberg, there was a flood of information, and for a hundred years, the public had trouble figuring out what was true.\nNot long ago, most Americans got their news from print newspapers with trusted brands — including 98 percent of the registrants when we started this conference 12 years ago.\nInternet allowed social media to break the dam, multiplied the sources and concentrated the distribution systems. While more people than ever before in history now have access to more information than ever, that sea change came with a flood of misinformation, disinformation, harassment and invective. The distribution of information has been consolidated into the hands of a few, enormous and generally unregulated companies.\nI mentioned that local news was one of three avenues we need to pursue. I believe the other two are technology and legal. As a practical matter, our focus at Knight is on local news, but we should be aware of the need and possibilities of using artificial intelligence agents to solve the problems of misinformation and disinformation – the same problems that artificial intelligence agents were used to create.\nAnd we need to reconsider whether global internet companies should continue to be governed by rules written for American print and broadcast outlets in the 20th century. We need fresh concepts of economic competition. Should we return to an earlier notion of trust busting, where the public determines a corporate behemoth is just too big? Should the laws of libel and product liability apply to digital services and products?\nCongress, if I understand recent hearings, has taken note of the awesome power of the platforms and intends to act. I think the rolling thunder is just over the horizon. And it might be one of those rare moments when right and left come together.\nBut both technological advances and debates about law and regulation are largely outside the scope of this conference.\nOur focus is on finding new ways to fulfill the underlying purpose of local news: sustaining an informed citizenry. Or, as Jack Knight put it, illuminating the minds of readers “so that the people may determine their true interests.”\nFor a decade, with the help of many of you in this room, we’ve explored different approaches to helping local news adapt and we’re ready to apply what we’ve learned.\nThree weeks ago, the Aspen Institute’s Knight Commission on Trust, Media and Democracy issued its report, which I would recommend as a primer on these issues and an index of possible ways forward.\nLast week, Knight Foundation announced a new initiative to revitalize local news, doubling our previous plan and raising the ante to $300 million.\nBut I stress that it’s only the ante and that’s why I’m delighted to see you here and was so glad to read NYU professor Michael Posner’s call in Forbes for a “Marshall Plan for journalism.” That vision evokes the scale of the effort — not by government, but by we, the people.\nWe start with local news because some of the most remarkable journalistic feats in our nation’s history have been local.\nThe Watergate stories that took down a president and changed the course of history were written by two metro reporters.\nThe Catholic Church’s suppression of sexual abuse was uncovered by The Boston Globe’s local investigative unit.\nTwo weeks ago, Southern Baptist Convention’s sex scandals were uncovered by the Houston Chronicle.\nAnd last year, when a gunman murdered five journalists at the Capital Gazette in Annapolis, Maryland, their colleagues responded by putting out a paper the next day at a newspaper that was not liberal or conservative but just told the story of what had happened in their city and what might be next.\nThe two key parts of the Knight initiative in Trust, Journalism and Democracy that I want you to remember are: 1) it’s about local news, because that’s where we think trust can be regained, and 2) it’s structured so that anyone can participate.\nNo matter what you’re passionate about, or where you live, there’s something for you.\nIf you want to support investigative journalism, donate to ProPublica’s local investigative journalism program.\nIf you want to help local outlets pursue sustainable digital business models, donate to the American Journalism Project.\nIf you want to place reporters in a range of newspapers around the country, donate to Report for America.\nIf you understand the need for legal support for journalists, support the Reporters Committee for Freedom of the Press and legal clinics at various law schools supported by the Stanton Foundation and others.\nChoose one – or several – that catch your attention, and you can be part of the solution. Because the solution is local.\nThe fundamental goal of this $300 million push is to reinvigorate local news as the staging ground for the middle, a place where common facts are the common prize. And in doing so, to help rebuild trust in media and strengthen our democracy.\nA few days ago, I read a piece about social change that quoted two great writers who were also social activists, James Baldwin and Simone de Beauvoir, each calling for engagement in society. Baldwin wrote about race, power and pride and said, “We made the world we’re living in and we have to make it over.” And de Beauvoir noted, “It is ours to do because the present is not a potential past; it is the moment of choice and action.”\nIf you leave this week’s Media Forum with one thought, let it be this, from their combined message: We have to make this world over, and it is ours to do.\nJoin us is renewing trust in news, and in each other.']	['<urn:uuid:f38d55d3-ea4e-4b44-a44c-df041ea806be>']	factoid	direct	short-search-query	distant-from-document	novice	2025-04-14T18:34:38.591736	42	332	7825
83	main challenges remote petroleum operations garraf field	The main challenges included unavailability of approved waste disposal facilities, incompetent wastes management contractor, and land acquisition issues.	['Iraq Operations’ Five 5 Years Waste Management Blue Print to Manage Waste in Remote Environment of Garraf, Iraq Operations\n- Afzariah Binti Che Arshad (PETRONAS Carigali) | M. Akmal B. Yaacob (PETRONAS Carigali) | Shairizal B. Badzri (PETRONAS Carigali)\n- Document ID\n- Society of Petroleum Engineers\n- SPE International Conference and Exhibition on Health, Safety, Security, Environment, and Social Responsibility, 16-18 April, Abu Dhabi, UAE\n- Publication Date\n- Document Type\n- Conference Paper\n- 2018. Society of Petroleum Engineers\n- 7 Management and Information, 4.1 Processing Systems and Design, 4 Facilities Design, Construction and Operation, 7.7.1 New Technology Deployment, 7.1.6 Field Development Optimization and Planning, 4.1.2 Separation and Treating, 7.1 Asset and Portfolio Management, 6.5.3 Waste Management\n- Waste Management Blue Print\n- 0 in the last 30 days\n- 84 since 2007\n- Show more detail\n- View rights & permissions\n|SPE Member Price:||USD 9.50|\n|SPE Non-Member Price:||USD 28.00|\nManaging wastes in remote oil and gas operations for PETRONAS Carigali Iraq Holding B.V. in the field of Garraf, Iraq presents a multitude of challenges including unavailability of an approved waste disposal facilities, incompetent wastes management contractor and land acquisition issues requires PETRONAS to establish a Five Years Waste Management Blue Print.\nThe objective of the blueprint is to provide strategic guidance for approaches in resolving the waste management issues in Garraf. Primary problem statement from the forecast of the cumulative impact of waste generation since day one of operation until the completion of Garraf Field Development Plan warrants the need of significant change in day to day waste management practices in Garraf.\nThe method used in establishing this blue prints includes, eliminating and/or minimization of waste generation from source, optimizing each waste stream including reuse & recycle towards zero discharge and disposal through deployment of fit for purpose and green technologies which are environmental friendly. Additionally, PCIHBV also takes into consideration challenges of medium-skilled operator, support by stakeholder and authorities and environmental health impact.\nSince it has been launched in 2016, significant value creation identified as follows:\nSupport from host authority to invest on technology deployment.\nHost authority accepted and acknowledge the blue print and support by approving significant environment budget despite low oil price based on long term value gain.\nHazardous Waste Treatment and Disposal\nPrior to establishment of this blue print, all hazardous waste are stored in our HWS which will poses us to long-term liability due to mismanagement and integrity of the HW container. Various engagement and discussion were conducted between PCIHBV and host government to resolve this issue since 2016. PCIHBV established the contract with Ministry of Science and Technology (MoST) on hazardous waste treatment and disposal and being the first International Oil Company (IOC) in Iraq to establish as such contract.\nThe implementation of the blue print produced some significant value creation which includes cost saving by reusing some of the waste and optimization of the raw material and resources e.g. Land acquisition for landfill area, disposal cost etc.\nDevelopment of local nationality on environmental technology and inculcate mindset change on the environmental awareness\n|File Size||2 MB||Number of Pages||13|']	['<urn:uuid:21d3cfb4-db18-4753-bdee-e67a5f6601e5>']	factoid	direct	long-search-query	distant-from-document	expert	2025-04-14T18:34:38.591736	56	153	3507
84	What's the first step for contractor employees facing whistleblower retaliation?	The first required step is to report the retaliation to the Inspector General of the relevant government agency, who must then investigate the complaint.	"['November 22, 2021 | Military & Defense Contracting Fraud\nProtections for Employees Who Blow the Whistle on Defense Contractors\nSome defense contractors’ fraud, waste, and mismanagement cost the government an estimated tens of billions of dollars each year. It is one of the largest categories of fraud in contracting with the federal government. Employees of federal contractors in any industry are in a position to discover and report this fraud and often do so.\nTo protect such employees from retaliation, the federal False Claims Act (“FCA”) allows an employee to sue for back pay and other compensation. Provisions of the National Defense Authorization Act of 2013 added further protections which extend to whistleblowers outside the defense industry in many situations.\nWhat is the National Defense Authorization act?\nThe National Defense Authorization Act is an annual budget-related bill that sets conditions for defense spending. In many years, the NDAA makes permanent changes to federal law. The bill Congress passed on January 2, 2013 amended two federal statutes to protect whistleblowers who report mismanagement and a wide range of other misconduct by federal contractors and grant recipients. In contrast to the FCA, the NDAA does not allow such a whistleblower to sue the contractor for its mismanagement, so long as no fraud is involved. The NDAA aims to help contracting government agencies make fully informed contracting decisions and get better value from their contracts.\nWhat laws prohibit defense contractors from retaliating against whistleblowers?\nThe FCA already prohibits termination and other job-related retaliation against a whistleblower who files an FCA lawsuit based on fraud or other false claims that a contractor makes to 1) win a contract or 2) be paid on a contract.\nThe 2013 NDAA expanded protections for employees of contractors and subcontractors who report misconduct. One section of the NDAA applies to all industries. A second applies only to companies that contract with the Department of Defense (DOD) and/or the National Aeronautics and Space Administration (NASA).\nThe NDAA protections go beyond the FCA’s in two important ways. First, the NDAA protects a broader range of misconduct, including “gross mismanagement” of a federal contract or grant, a “gross waste” of federal funds, “a substantial and specific danger to public health or safety,” or a contract-related violation of a law or regulation. Second, multiple modes of reporting the misconduct are protected. A report to any of the following is protected:\n- A member of Congress or a representative of a committee of Congress\n- An Inspector General of a federal agency\n- The Government Accountability Office, Congress’s research and investigatory arm\n- A federal employee responsible for contract or grant oversight or management at the relevant agency\n- An authorized official of the Department of Justice or other law enforcement agency\n- A court or grand jury\n- A management official or other employee of the contractor or grantee who has the responsibility to investigate or address misconduct5\nReporting misconduct to one of these entities may entail less involvement than being a plaintiff (also known as a “relator”) in a federal lawsuit under the False Claims Act. And a plaintiff who faces retaliation for one of the protected categories of reporting under the NDAA can bring an NDAA lawsuit without having any FCA claims.\nWhat should I do if I suspect my employer of misconduct that violates the NDAA?\nIf you suspect waste, fraud, or abuse by a government contractor, you should contact an attorney experienced in FCA litigation and familiar with NDAA’s protections. There may be fraud or overcharging that goes beyond the NDAA’s waste, abuse, and mismanagement reporting provisions.\nIf you have faced retaliation for reporting waste, abuse, and mismanagement, you should definitely contact an attorney to discuss your situation. As a first step, the NDAA requires that the employee report the retaliation to the Inspector General of the relevant government agency.\nThe Inspector General is then required to investigate the complaint of retaliation. If the Inspector General determines that there was unlawful retaliation, the agency must order the contractor to reverse the retaliatory action and/or provide back pay. The agency must file its own lawsuit against the contractor in the event of noncompliance. If the agency fails to comply, the employee can file a lawsuit against the contractor. The employee can also join a lawsuit that the government agency has filed.\nMilitary & Defense Contracting Fraud | December 02, 2021\nThe defense and security contractor Academi is a collection of companies previously known as Blackwater....\nMilitary & Defense Contracting Fraud | September 23, 2019\nImagine this scenario: You go work for a military defense contractor, but soon learn...\nConstruction Fraud | April 04, 2019\nYou’ve probably heard the phrase, “If you see something, say something.” While it’s often used...\nContact the Carolina\nIf you’re wondering if it’s a good idea to speak with a whistleblower lawyer about what you know, let us set the record straight.\n- Corporate ethics hotlines can be risky and may lead to termination. If you’ve already done this, call us immediately.\n- Your coworkers could be aware of the fraud – or complicit in it – and you should not talk to them about it.\n- The first claim to be filed under the False Claims Act can proceed – if you’re not first, you’re at a serious disadvantage and may get nothing (another reason not to speak to your coworkers about it).\n- A confidential discussion costs you a few minutes, but could save you time, stress, and money.\n""*"" indicates required fields']"	['<urn:uuid:351af0a8-fc19-4305-859c-33c90c6a870e>']	factoid	with-premise	concise-and-natural	similar-to-document	expert	2025-04-14T18:34:38.591736	80	153	5746
85	Why can't the principle of nationality work in Eastern Europe?	The principle of nationality cannot work in Eastern Europe because there are large territories where linguistic groups are inextricably mixed, making it impossible to draw boundaries that clearly segregate linguistic groups. Any territorial division necessarily leaves minorities under foreign rule.	"['Editor’s Note: We stand on the shoulders of giants. So we have decided to revisit some of these giants for you to experience anew in our new “Vintage” feature. Some of these writings are great little gems from lesser-known classical liberals, while others are digestible pieces from the greats in our tradition. Think of it as akin to finding a great record in the attic, dusting it off, and letting it spin.\n[In a liberal world] the working of the market is not hampered by government interference. There are no trade barriers; men can live and work where they want. Frontiers are drawn on the maps but they do not hinder the migrations of men and shipping of commodities. Natives do not enjoy rights that are denied to aliens. Governments and their servants restrict their activities to the protection of life, health, and property against fraudulent or violent aggression. They do not discriminate against foreigners. The courts are independent and effectively protect everybody against the encroachments of officialdom. Everyone is permitted to say, to write, and to print what he likes. Education is not subject to government interference. Governments are like night-watchmen whom the citizens have entrusted with the task of handling the police power. The men in office are regarded as mortal men, not as superhuman beings or as paternal authorities who have the right and duty to hold the people in tutelage. Governments do not have the power to dictate to the citizens what language they must use in their daily speech or in what language they must bring up and educate their children. Administrative organs and tribunals are bound to use each man\'s language in dealing with him, provided this language is spoken in the district by a reasonable number of residents.\nIn such a world it makes no difference where the frontiers of a country are drawn. Nobody has a special material interest in enlarging the territory of the state in which he lives; nobody suffers loss if a part of this area is separated from the state. It is also immaterial whether all parts of the state\'s territory are in direct geographical connection, or whether they are separated by a piece of land belonging to another state. It is of no economic importance whether the country has a frontage on the ocean or not. In such a world the people of every village or district could decide by plebiscite to which state they wanted to belong.\n(from Omnipotent Government, 1944)\nThe liberals of an earlier age thought that the peoples of the world were peaceable by nature and that only monarchs desire war in order to increase their power and wealth by the conquest of provinces. They believed, therefore, that to assure lasting peace it was sufficient to replace the rule of dynastic princes by governments dependent on the people. If a democratic republic finds that its existing boundaries, as shaped by the course of history before the transition to liberalism, no longer correspond to the political wishes of the people, they must be peacefully changed to conform to the results of a plebiscite expressing the people\'s will. It must always be possible to shift the boundaries of the state if the will of the inhabitants of an area to attach themselves to a state other than the one to which they presently belong has made itself clearly known. In the seventeenth and eighteenth centuries, the Russian Czars incorporated into their empire large areas whose population had never felt the desire to belong to the Russian state. Even if the Russian Empire had adopted a completely democratic constitution, the wishes of the inhabitants of these territories would not have been satisfied, because they simply did not desire to associate themselves in any bond of political union with the Russians. Their democratic demand was: freedom from the Russian Empire; the formation of an independent Poland, Finland, Latvia, Lithuania, etc. The fact that these demands and similar ones on the part of other peoples (e.g., the Italians, the Germans in Schleswig-Holstein, the Slavs in the Hapsburg Empire) could be satisfied only by recourse to arms was the most important cause of all the wars that have been fought in Europe since the Congress of Vienna.\nThe right of self-determination in regard to the question of membership in a state thus means: whenever the inhabitants of a particular territory, whether it be a single village, a whole district, or a series of adjacent districts, make it known, by a freely conducted plebiscite, that they no longer wish to remain united to the state to which they belong at the time, but wish either to form an independent state or to attach themselves to some other state, their wishes are to be respected and complied with. This is the only feasible and effective way of preventing revolutions and civil and international wars.\nTo call this right of self-determination the ""right of self-determination of nations"" is to misunderstand it. It is not the right of self-determination of a delimited national unit, but the right of the inhabitants of every territory to decide on the state to which they wish to belong. This misunderstanding is even more grievous when the expression ""self-determination of nations"" is taken to mean that a national state has the right to detach and incorporate into itself against the will of the inhabitants parts of the nation that belong to the territory of another state. It is in terms of the right of self-determination of nations understood in this sense that the Italian Fascists seek to justify their demand that the canton Tessin and parts of other cantons be detached from Switzerland and united to Italy, even though the inhabitants of these cantons have no such desire. A similar position is taken by some of the advocates of Pan-Germanism in regard to German Switzerland and the Netherlands. However, the right of self-determination of which we speak is not the right of self-determination of nations, but rather the right of self-determination of the inhabitants of every territory large enough to form an independent administrative unit. If it were in any way possible to grant this right of self-determination to every individual person, it would have to be done. This is impracticable only because of compelling technical considerations, which make it necessary that a region be governed as a single administrative unit and that the right of self-determination be restricted to the will of the majority of the inhabitants of areas large enough to count as territorial units in the administration of the country.\nSo far as the right of self-determination was given effect at all, and wherever it would have been permitted to take effect, in the nineteenth and twentieth centuries, it led or would have led to the formation of states composed of a single nationality (i.e., people speaking the same language) and to the dissolution of states composed of several nationalities, but only as a consequence of the free choice of those entitled to participate in the plebiscite. The formation of states comprising all the members of a national group was the result of the exercise of the right of self-determination, not its purpose. If some members of a nation feel happier politically independent than as a part of a state composed of all the members of the same linguistic group, one may, of course, attempt to change their political ideas by persuasion in order to win them over to the principle of nationality, according to which all members of the same linguistic group should form a single, independent state. If, however, one seeks to determine their political fate against their will by appealing to an alleged higher right of the nation, one violates the right of self-determination no less effectively than by practicing any other form of oppression. A partition of Switzerland among Germany, France, and Italy, even if it were performed exactly according to linguistic boundaries, would be just as gross a violation of the right of self-determination as was the partition of Poland.\n(from Liberalism, 1927)\nThe principle of nationality was derived from the liberal principle of self-determination. But the Poles, the Czechs, and the Magyars substituted for this democratic principle an aggressive nationalism aiming at the domination of people speaking other languages. Very soon German and Italian nationalists and many other linguistic groups adopted the same attitude.\nIt would be a mistake to ascribe the ascendancy of modern nationalism to human wickedness. The nationalists are not innately aggressive men; they become aggressive through their conception of nationalism. They are confronted with conditions which were unknown to the champions of the old principle of self-determination. And their etatist prejudices prevent them from finding a solution for the problems they have to face other than that provided by aggressive nationalism.\nWhat the Western liberals have failed to recognize is that there are large territories inhabited by people of different idioms. This important fact could once be neglected in Western Europe but it could not be overlooked in Eastern Europe. The principle of nationality cannot work in a country where linguistic groups are inextricably mixed. Here you cannot draw boundaries which clearly segregate linguistic groups. Every territorial division necessarily leaves minorities under foreign rule.\nThe problem becomes especially fateful because of the changeability of linguistic structures. Men do not necessarily stay in the place of their birth. They have always migrated from comparatively overpopulated into comparatively underpopulated areas. In our age of rapid economic change brought about by capitalism, the propensity to migrate has increased to an unprecedented extent. Millions move from the agricultural districts into the centers of mining, trade, and industry. Millions move from countries where the soil is poor to those offering more favorable conditions for agriculture. These migrations transform minorities into majorities and vice versa. They bring alien minorities into countries formerly linguistically homogeneous.\nThe principle of nationality was based on the assumption that every individual clings throughout his life to the language of his parents, which he has learned in early childhood. This too is an error. Men can change their language in the course of their life; they can daily and habitually speak a language other than that of their parents. Linguistic assimilation is not always the spontaneous outcome of the conditions under which the individual lives. It is caused not only by environment and cultural factors; governments can encourage it or even achieve it by compulsion. It is an illusion to believe that language is a nonarbitrary criterion for an impartial delimitation of boundaries. The state can, under certain conditions, influence the linguistic character of its citizens.\n(from Omnipotent Government, 1944)']"	['<urn:uuid:25d31171-d617-4e34-8dc9-0405699820cc>']	factoid	direct	concise-and-natural	similar-to-document	novice	2025-04-14T18:34:38.591736	62	299	10964
86	How did scoring a single championship point make such a big difference for racing teams back in the early 1980s?	In the early 1980s, scoring even one championship point was extremely significant because points were only awarded down to sixth place, making them much harder to achieve. For teams, scoring points had substantial practical benefits - as demonstrated in the case of ATS team, earning a single point meant they received free transportation for their cars to races for the entire following year, which otherwise would have been a significant cost for the team. As driver Slim Borgudd described it, scoring a point 'was like winning the Nobel Prize.'	"['Slim Borgudd: The story behind Avon’s first ever point in Formula 1\nAvon Tyres has a rich and storied history in motorsport. Over the years, the British manufacturer has stamped its authority in rallying, hill climb and many more categories. But lesser known is the time it spent in the top echelon of single-seater competition: Formula 1.\nFirst called into the sport in the 1950s, and again in the 1980s, a range of talented drivers raced on Avon rubber during their careers, including World Champions Jack Brabham and Keke Rosberg, and Grand Prix winners Michele Alboreto and Jochen Mass.\nWhile there were no podiums, victories or titles achieved with Avon products, the Melksham-based operation still made its mark during 29 sporadic appearances split across six seasons.\nIt all started with Scuderia Ambrosiana driver Reg Parnell at Silverstone midway through the 1954 season, and ended with the March 821s of Rupert Keegan and Raul Boesel at Dijon-Prenois in 1982.\nBut Avon’s 11th outing, at the 1981 British Grand Prix, was undoubtedly its most memorable.\nAhead of the 1981 season, tyre supply in Formula 1 was up in the air.\nGoodyear\'s departure from the sport left a void and rival manufacturer Michelin stepped in to provide all teams with a standard tyre. At the fourth round of the campaign in San Marino, Avon and Pirelli entered the fray to spread supply, only for Goodyear to reverse its decision and return mid-season with Williams, Brabham and Lotus.\nAvon initially supplied Fittipaldi Automotive drivers Chico Serra and Rosberg, before also appearing on Derek Daly’s March, Eliseo Salazar\'s Ensign and Marc Surer’s Theodore.\nBut a breakthrough result would come when it joined forces with ATS and Slim Borgudd.\nATS – named after German alloy wheel brand Auto Technisches Spezialzubehör – parted ways with Michelin following the tyre manufacturer\'s home event in France and, from the British Grand Prix onwards, used Avon tyres.\n“There was a lot of politics going on,” Borgudd says of the tyre situation in Formula 1 at the time.\n""We were competing at the French Grand Prix in Dijon and I didn’t qualify. Afterwards, [we were told that] there were no tyres… I said, ‘We’ve got to have a tyre!’, so we moved to the Avon tyre from the British Grand Prix onwards.\n“The Avon was a really good tyre. I had used the Michelin tyre in the races before that, so it was a good comparison for me to have.”\nWhat followed next would go down in Avon’s motorsport history.\nHaving failed to make the grid at the previous four events, Borgudd jumped the notoriously difficult pre-qualifying hurdle at Silverstone to qualify 21st – ahead of talents such as Elio de Angelis, Eddie Cheever and Nigel Mansell.\nIn a race of attrition, the Swedish driver – whose side job as a session drummer with ABBA led to him running the band\'s logo on his car – took full advantage. Staying out of trouble, he steadily worked his way up the order and, after rivals fell by the wayside, reached the chequered flag in sixth position.\nWhile on the surface it was a solitary point, it meant so much more to ATS and Avon.\n“Back then, you only got points down to sixth place, so it was very hard to get a point,” Borgudd looks back with a wry smile, remembering the magnitude of what he, the team and Avon achieved.\n“It meant a lot to everybody. It was like winning the Nobel Prize.\n“It also gave the team free transportation… normally when cars were flown out or shipped out to races, it was all costs for the team, but that point meant that for the whole next year they had free transportation.\n“I still have a picture of a trophy with Avon on it, because somebody came over and clocked that we were the only ones who had actually scored a point [on Avon tyres].”\nJohn Watson, who won the Grand Prix for McLaren, has fond memories of sharing the paddock with Borgudd, and emphasised his contemporary\'s achievement.\n“Slim had a relaxed personality and character, and was easy to get on with,"" says Watson.\n“Some drivers are closed and introverted, but Slim was very open. With his background in music, he understood what a stage was.\n“I remember when I scored my first point in Formula 1 and it’s a pretty important moment, especially in that time, when points only went down to sixth place.\n“For any driver, scoring your first point is a landmark day.”\nIt would prove to be Borgudd’s only point in Formula 1, as he saw out the 1981 season with ATS before embarking on a brief, three-race spell with Tyrrell in 1982.\nAvon as a manufacturer went on to score seven more points with Cheever, Salazar and Manfred Winkelhock. Borgudd’s spirited run at Silverstone, though, will always stand out from the rest.']"	['<urn:uuid:5b0cb06b-056e-4e64-b666-01e9db24991e>']	open-ended	direct	verbose-and-natural	distant-from-document	novice	2025-04-14T18:34:38.591736	112	547	4681
87	please explain what partial derivative means in basic terms im new to calculus	A partial derivative is an operation that is defined for functions where we want to find how the function changes with respect to one variable while holding the other variables constant. For example, if you have a function F(x,y)=x+y, the partial derivative with respect to x (written as ∂F/∂x) would be 1, and the partial derivative with respect to y (∂F/∂y) would also be 1. It's important to understand that partial derivatives are only meaningful when we're working with functions, not just values.	"[""Taking the Partial Derivative of a Function\nDate: 09/06/2002 at 04:16:02 From: Robert Curl Subject: Partial derivative Dear Dr. Math, Given an equation, for example, x+y=0, is it true that if we perform the same operation on both sides we could have the same results on both sides? For example, if we add 6 to both sides, and on the left side we have (x+y)+6, and on the right side we have 0+6=6, we know that the results on the left and the right sides are equal, that is, (x+y)+6=0+6. But how about the operation of partial derivative? For the equation x+y=0, if we perform the operation of partial derivative on the left side and let F(x,y)=x+y, we have (partial F)/(partial x)=1 and (partial F)/(partial y)=1, but on the right, we know the result is zero, which is not equal to the result on the left. One must insert the solution, y=y(x)=-x, into the equation; that is, on the left side, F(x,y)=x+y, x=x, y=-x, then dF/dx=0, which is equal to the result obtained on the right. From the above, I think the left and right sides of an equation are not always symmetric. For the operations plus, minus, etc., we could always have same results on both sides after performing the same operation on both sides, but for the operation of derivative, the results on both sides are not always equal. Am I right? I need your help. Thank you very much. Sincerely, Robert Curl\nDate: 09/06/2002 at 08:46:01 From: Doctor Jerry Subject: Re: Partial derivative Hi Robert, The partial derivative with respect to a variable is an operation that is defined in terms of functions. That is, one takes the partial derivative of a function. If you apply the partial derivative operation to an equation, you are assuming that the equation defines one of the variables in terms of the others. Using x+y=0 as an example and assuming that we want to differentiate with respect to x, we think of y as the function of x that we would find if we could solve this equation for y in terms of x. Differentiating with respect to x then gives (y_x means the derivative of y with respect to x) 1 + y_x = 0 and so y_x = -1 This is correct becasue y = -x and we may differentiate the function f(x,y) = -x with respect to x and find f_x = -1. In this case we are using y as a symbol for f. - Doctor Jerry, The Math Forum http://mathforum.org/dr.math/\nDate: 09/06/2002 at 08:47:31 From: Doctor Mitteldorf Subject: Re: Partial derivative Dear Robert, It is easy to get into trouble manipulating symbols according to preset rules, without thinking about what those symbols mean. In this case, the equation x+y=0 might mean that we are considering a function F(x,y) defined on the (x,y) plane to be the sum of the x and y coordinates. Perhaps we are interested in the locus of points on that plane where the function F has the value 0. In this case, it might be a meaningful operation to take the partial derivative of the function F with respect to x; but it wouldn't make sense to differentiate the right side, because the 0 on the right represents a value, not a function. Of course, we might have a different situation, and the same equation x+y=0 might mean something different: perhaps y is tied to x in such a way as to guarantee that x+y is always 0. Then the partial differentiation with respect to x would itself not be a meaningful operation, since y is inextricably tied to x, and x can never be varied while y is held constant. My perspective is that you start with an understanding of a situation in the world, and model that in relations among numbers. Ultimately it is our understanding of the world and of the way that our model represents it that determines what mathematical manipulations are permitted and appropriate. I am a scientist and statistician - not a pure mathematician. Mathematicians might have a different perspective; it is their discipline to define precisely the circumstances under which certain purely formal symbolic manipulations can be performed, with internal consistency in the results. I would be interested to see what a pure mathematician might say about the paradox which you have uncovered. - Doctor Mitteldorf, The Math Forum http://mathforum.org/dr.math/\nSearch the Dr. Math Library:\nAsk Dr. MathTM\n© 1994- The Math Forum at NCTM. All rights reserved.""]"	['<urn:uuid:cf6c2c8d-066e-47e3-a849-669c50810ac8>']	open-ended	with-premise	long-search-query	distant-from-document	novice	2025-04-14T18:34:38.591736	78	502	4261
88	solutions prevent greenhouse methane crisis	Four main solution areas have been identified: 1) Transitioning to a zero net emissions economy quickly, which can benefit society economically, 2) Implementing climate-adaptive ecological restoration and researching carbon sequestration, 3) Having realistic discussions about geoengineering possibilities and limitations, and 4) Building psychological resilience to handle alarming planetary changes. Studies show that investing $520 billion in efficiency could reduce U.S. energy demand by 23% and save $1.2 trillion by 2020.	"['We\'ve written before about the danger that climate change will lead to the thawing and release of methane frozen on the ocean floor, and indeed the worrisome news that some scientists were observing patches of Arctic sea foaming with gas bubbles from ""methane chimneys"" rising from the sea floor.\nNow, researchers in Alaska have found a similar process underway:\nNatalia Shakhova, a scientist at the university and a leader of the study, said it was too soon to say whether the findings suggest that a dangerous release of methane looms. In a telephone news conference, she said researchers were only beginning to track the movement of this methane into the atmosphere as the undersea permafrost that traps it degrades.\nBut climate experts familiar with the new research reported in Friday’s issue of the journal Science that even though it does not suggest imminent climate catastrophe, it is important because of methane’s role as a greenhouse gas. Although carbon dioxide is far more abundant and persistent in the atmosphere, ton for ton atmospheric methane traps at least 25 times as much heat.\nUntil recently, undersea permafrost has been little studied, but work so far shows it is already sending surprising amounts of methane into the atmosphere, Dr. Shakhova and other researchers are finding.\nLast year, scientists from Britain and Germany reported that they had detected plumes of methane rising from the Arctic seabed in the West Spitsbergen area, north of Scandinavia. At the time, they said they had begun their work hoping to gain data to predict future emissions and had not expected to find evidence that the process was under way.\nIt is “indispensable” to keep track of methane in the region, Martin Heimann of the Max Planck Institute in Germany said in a commentary accompanying the Science report. So far, Dr. Heimann wrote, methane contributions from Arctic permafrost have been “negligible.” He added: “But will this persist into the future under sustained warming trends? We do not know.”\nIn an e-mail message, Euan G. Nisbet of the University of London, an expert on atmospheric methane, said the situation “needs to be watched carefully.”\nAtmospheric concentrations of methane have more than doubled since pre-industrial times, Dr. Heimann wrote. Most of it comes from human activities including energy production, cattle raising and the cultivation of rice. But about 40 percent is natural, including the decomposition of organic materials in wetlands and frozen wetlands like permafrost.\nDr. Shakhova said that permafrost in the East Siberian Arctic Shelf, peat land that flooded as sea levels rose after the last ice age, is degrading in part because runoff from rivers that feed the Arctic Ocean is warmer than in the past.\nShe estimated that annual methane emissions from the East Siberian Arctic Shelf total about seven teragrams. (A teragram is 1.1 million tons.) By some estimates, global methane emissions total about 500 teragrams a year.\nDr. Shakhova said that undersea methane ordinarily undergoes oxidation as it rises to the surface, where it is released as carbon dioxide. But because water over the shelf is at most about 50 meters deep, she said, the gas bubbles to the surface there as methane. As a result, she said, atmospheric levels of methane over the Arctic are 1.85 parts per million, almost three times as high as the global average of 0.6 or 0.7 parts per million. Concentrations over the shelf are 2 parts per million or higher.\nA huge release of one-frozen methane is (potentially) almost the definition of a feedback loop, perhaps even a tipping point into runaway climate change.\nLuckily, we\'re not there yet. Scientists have been very clear to say that while these field observations are surprising and disturbing, they do not yet indicate a catastrophe. We need to wait for more data to figure out if it\'s time to panic yet.\nIn the meantime, we need to focus even more strongly one four solution spaces:\n1) Getting to a zero net emissions economy as quickly as possible. It\'s very clear now that we can do this at a net gain for society, with more (and more wide-spread) prosperity for most people, and that it\'s largely the opposition of entrenched interests that\'s preventing us from making huge strides forward: this needs to change.\n2) Implementing climate-adaptive ecological restoration, safeguarding ecosystem services and researching soil carbon sequestration and other practices that have multiple benefits while pulling greenhouse gasses from the air. We need to be helping the planet\'s natural systems heal towards resilience as much as we possibly can.\n3) Engaging in a stronger and more realistic debate about geoengineering, its limits and its politics, especially since news of potential tipping points always accelerates calls for geoengineering research, even deployment. Geoengineering\'s main use in the climate debate at the moment is as a propaganda tool by those seeking to stall action on emissions reduction; that doesn\'t mean that we don\'t need to discuss what mega-scale answers might be possible should we find tipping points sliding past more quickly than we feared.\n4) Building psychological resilience in the face of huge and alarming planetary changes. News like this is disturbing. We need to find ways, as a culture, as communities and as individuals, to understand disturbing changes without losing our balance. Psychologically wrecked people are no good to themselves, others or the planet. We need to promote the capacity to be healthy and happy despite monumental challenges.\nWe\'ll be returning to all these ideas again in the near future.\nFirst, I so appreciate your work, and esp. the even tone of your writing. My suspicion is that this is really a problem that we should consider seriously now. Scientists can probably predict what if scenarios, and if the methane comes up, we in trouble.\nI heard Heinberg speak last week in Portland, Oregon. Post-Carbon Institute. I appreciate their work, too. OK, back to The Long Emergency, and happy while the ship goes down week.\nThe same issue of Science also has an interesting study on the behavioural reasons why people don\'t pursue energy efficiency -- even when it is clearly in their economic self interest to do so. This is another facet of your ""solution space 1""\nPast studies show that $520 billion invested in efficiency could cut U.S. energy demand by 23% and save $1.2 trillion by 2020.\nBeginning with those numbers,it\'s hard to understand why we haven\'t done more. The authors point out that we clearly aren\'t the ""rational actors"" that we like to think we are.\nI think their argument that we need more attention to the behavioural side of how we use energy really persuasive. We need to get better at simultaneously dealing with both the cultural and technological.\nIf the greenhouse shit hits the fan in an undeniable fashion, the skeptic crowd will claim that it was the methane all along. And suggest the solution is to conquer Canada.\nJudging from the sheer mass of vitriolic anti-science and anti-environmental sentiment I find on many sites, it seems that there is a concerted and orchestrated effort out there to derail action on any of the four solutions mentioned. Hopefully, all I am seeing is just the last gasp of a very vocal minority. Unfortunately this is also what I thought about Rush Limbaugh several years ago. I greatly fear that this planet is in for a serious world of hurt. This pessimistic environmental scientist believes that it is entirely possible that the tipping point has already been reached.\nNot to pile on the bad news, but a recent news article revealed a higher degree of methane gas being released in areas where permafrost was disappearing. That would be Siberia and Alaska.\n""But about 40 percent is natural, including the decomposition of organic materials in wetlands and frozen wetlands like permafrost.""\n40 percent of what?\nI suggest interested readers check www.climateprogress.org and search for methane. Joe Romm is a Ph.D. in physics from a journalism family and provides the most comprehensive website for climate change science that non-techies can understand. His focus is purely on the politics, science and anti-polemics, thereby complementing Alex\'s site quite nicely. Methane is a serious long term issue that is hiding under the covers. The more everyone understands what is possible to understand, the better. Even googling methane and climate change will yield some interesting results. One of the earlier comments by a climate scientist said we may have already reached the tipping point. James Hansen of NASA thinks this could be the case, as reported by Alex on several occasions. One answer at a practical level is to focus on ""resilient sustainability"" for cities and communities.\nSee google docs folder link for 3 articles and 3 strategy maps that include an introductory discussion of how to strategically move toward reslient sustainabilty [RS]. RS is the combination of 3 content domains: strategy execution, sustainability & resilience.\nirv beiman, ph.d. firstname.lastname@example.org\nwhoops, that previous link only included the most recent article. the article set can be reached at:\nI was thinking about some local uses for Carbon Offsetting funds, namely bikes... time to redouble the effort! http://unpollute.ning.com -emett']"	['<urn:uuid:e67bb67f-c948-4fbf-914a-3855aaf359ef>']	open-ended	with-premise	short-search-query	distant-from-document	expert	2025-04-14T18:34:38.591736	43	527	9351
89	contemporary southeast asian art media types interactive installation performance photography public engagement	The exhibition presents various types of media including interactive installations, text-based works, painting, performance, photography, and sound art. Some works are in game form or are durational pieces designed to be experienced by the public over multiple visits. This diverse range of media demonstrates how Southeast Asian contemporary art integrates with daily life, offering different experiences with each visit.	['Concept Context Contestation: art and the collective in Southeast Asia\nA new exhibition about conceptual approaches to making art for, with, and about the collective and collective issues in Southeast Asia.\nFor the last two decades Southeast Asian contemporary art has travelled the world generating universal excitement. Though diverse in its forms and in the cultural and political environments that have spawned it, Southeast Asian art of today is testament to Southeast Asia’s shared regional history and 21st century solidarity. In a spirit of unity, and anticipating ASEAN integration commencing in 2015, Bangkok Art and Culture Centre is proudly presenting a new and specially BACC-commissioned exhibition of regional contemporary visual art, the biggest ever Southeast Asian contemporary art show produced in Thailand Concept Context Contestation: art and the collective in Southeast Asia\nCurated in close collaboration with BACC by three respected Southeast Asia specialists and curators from Singapore, Indonesia and Thailand, Concept Context Contestation: art and the collective in Southeast Asia, will be visually stimulating for the public at large, while also offering art historical insights into our own regional visual culture of today. Through the works of artists from Thailand, Indonesia, Philippines, Vietnam, Malaysia, Singapore, Burma and Cambodia, the show will chart one of regional contemporary art’s most important threads, locally-rooted conceptual thinking used to engage in ideas about and for the collective. It will investigate the close connection between conceptual approaches and social ideologies in Southeast Asian contemporary art of the last four decades.\nWith nearly 50 artworks by over 40 celebrated and internationally-known Southeast Asian artists of three generations, the exhibition will defend the idea that conceptual approaches used in contemporary art of Southeast Asia are not necessarily imported but rather can find their source in home culture.\nNumerous internationally-celebrated artists are featured in: seminal Singaporean practitioners, well-known for their sharp social commentary, include Amanda Heng and Lee Wen; from Philippines canon-shaping practitioners include Imelda Cajipe Endaya and Alwin Reamillo; Indonesian heavyweights with potent political voices are FX Harsono, Eko Nugroho and Daging Tumbuh and Popok Tri Wahyudi, among others. The large Thai section will not only present Thai pioneers of the contemporary such as Sutee Kunavichayanont, Vasan Sitthiket, and Manit Sriwanichpoom but also emerging talents of the second generation of contemporaries still keen to engage audiences in issues of the collective.\nWorks of all media, all countries, and three generations will be displayed in dynamic visual dialogue, so showing how forms and approaches may evolve over time, and differ over geography, but how layered thinking and metaphoric tactics remain constants -and are in fact hallmarks- of the contemporary in Southeast Asian visual art.\nThe show will present media of all types - interactive installation, text-based works, painting, performance, photography, sound art etc...- and will include several works that are in game form, or are durational pieces designed to be used and experienced by the public over the course of days and weeks. Thus repeated visits can yield a different experience each time, so fully illustrating the way in which Southeast Asian contemporary art and life mesh.\nThe exhibition will be documented by a fully-illustrated ten-essay research catalogue by the curators and other specialists, available after the opening. In addition, BACC will be hosting a weekend of free public educational talks and panel discussions by experts in the field. Concept Context Contestation: art and the collective in Southeast Asia will reveal the connection between conceptual approaches in art, and an active art of social change, the two components’ linkage a key characteristic of Southeast Asian contemporary practice.\nCurated by :\nIola Lenzi (Singapore)\nAgung Hujatnikajennong (Indonesia)\nVipash Purichanont (Thailand)\nBui Cong Khanh\nEko Nugroho and Daging Tumbuh\nImelda Cajipe Endaya\nNguyen Van Cuong\nPopok Tri Wahyudi\nTang Mun Kit\nTay Wei Leng\nThao Nguyen Phan\nVu Dan Tan\nWong Hoy Cheong\nSpecial Performance Documentation:\nFor information, please contact:\nCommunication and Public Relation Dept.\nBangkok Art and Culture Centre\n939 Rama 1 Rd. Wang Mai\nPathumwan Bangkok 10330\nPhone: 02 214 6630 Fax: 02 214 6639']	['<urn:uuid:6475f8ab-c85b-4c9f-ab0e-146bed777282>']	open-ended	direct	long-search-query	similar-to-document	expert	2025-04-14T18:34:38.591736	111	422	4507
90	I'm trying to understand how omega-3 affects brain function. How exactly do polyunsaturated fatty acids like omega-3 help with the transmission of nerve messages in the brain?	Polyunsaturated fatty acids make cell membranes more malleable and sensitive to deformation. This helps with a process called endocytosis, where synaptic vesicles form, transport, and transmit neurotransmitters at the synapse. These vesicles can reform in less than a tenth of a second, enabling rapid synaptic recycling. The abundance of these lipids in the brain could therefore be a major advantage for cognitive function.	['Here’s a little more good news in understanding the mechanism of getting omega3 fatty acids into brain cells. Eat smart, think smart.\nConsuming oils with high polyunsaturated fatty acid content, in particular those containing omega-3s, is beneficial for the health. But the mechanisms underlying this phenomenon are poorly known.\nResearchers at the Institut de Pharmacologie Moléculaire et Cellulaire (CNRS/Université Nice Sophia Antipolis), the Unité Compartimentation et Dynamique Cellulaires (CNRS/Institut Curie/UPMC), the INSERM and the Université de Poitiers1 investigated the effect of lipids bearing polyunsaturated chains when they are integrated into cell membranes.\nTheir work shows that the presence of these lipids makes the membranes more malleable and therefore more sensitive to deformation and fission by proteins. These results, published on August 8, 2014 in Science, could help explain the extraordinary efficacy of endocytosis2 in neuron cells.\nConsuming polyunsaturated fatty acids (such as omega-3 fatty acids) is good for the health. The effects range from neuronal differentiation to protection against cerebral ischemia3. However the molecular mechanisms underlying these effects are poorly understood, prompting researchers to focus on the role of these fatty acids in cell membrane function.\nFor a cell to function properly, the membrane must be able to deform and divide into small vesicles. This phenomenon is called endocytosis. Generally, these vesicles allow the cells to encapsulate molecules and transport them.\nIn neurons, these synaptic vesicles will act as a transmission pathway to the synapse for nerve messages. They are formed inside the cell, then they move to its exterior and fuse with its membrane, to transmit the neurotransmitters that they contain. Then they reform in less than a tenth of a second: this is synaptic recycling.\nIn the work published in Science, the researchers show that cell- or artificial membranes rich in polyunsaturated lipids are much more sensitive to the action of two proteins, dynamin and endophilin, which facilitate membrane deformation and fission. Other measurements in the study and in simulations suggest that these lipids also make the membranes more malleable.\nBy facilitating the deformation and scission necessary for endocytosis, the presence of polyunsaturated lipids could explain rapid synaptic vesicle recycling.. The abundance of these lipids in the brain could then represent a major advantage for cognitive function.\nThis work partially sheds light on the mode of action of omega-3. Considering that the body cannot synthesize them and that they can only be supplied by a suitable diet (rich in oily fish, etc.), it seems important to continue this work to understand the link between the functions performed by these lipids in the neuronal membrane and their health benefits.\n(1) This study was conducted in collaboration with teams from the Centre Commun de Microscopie Appliquée (Université Nice Sophia Antipolis) and the Laboratoire Signalisation et Transports Ioniques Membranaires (CNRS/Université de Poitiers/Université François Rabelais de Tours).\n(2) Endocytosis is the name of the process by which the cells absorb various substances present in the surrounding medium by encapsulating them in a lipoprotein membrane. Endocytosis is involved in several physiological functions.\n(3) See for example prior work by the Institut de Pharmacologie Moléculaire et Cellulaire on this type of stroke: Polyunsaturated fatty acids are potent neuroprotectors; Lauritzen I, Blondeau N, Heurteaux C, Widmann C, Romey G, Lazdunski M; EMBO J. (2000) 19:1784-93.']	['<urn:uuid:534ba4f5-dff0-4f20-8f85-a969fcf1aa9c>']	factoid	with-premise	verbose-and-natural	similar-to-document	novice	2025-04-14T18:34:38.591736	175	425	3648
91	My 3-year-old son keeps getting urinary infections which is unusual for boys his age. Could this be a sign of vesicoureteral reflux and how would doctors check for it?	Yes, urinary tract infections are uncommon in boys at any age unless vesicoureteral reflux (VUR) is present. To diagnose VUR, doctors typically perform tests including a voiding cystourethrogram (VCUG), which is an X-ray test where the bladder is filled with liquid dye to see if urine flows back into the ureters and kidneys. They may also do a renal ultrasound to check kidney size and shape, and blood tests to measure kidney function.	"[""What is vesicoureteral reflux?\nVesicoureteral reflux (VUR) occurs when urine in the bladder flows back into the ureters and kidneys. This condition is most frequently diagnosed in infancy and childhood. A child who has vesicoureteral reflux is at risk for developing recurrent kidney infections, which, over time, can cause damage and scarring to the kidneys.\nWhat causes vesicoureteral reflux?\nThere are many different reasons why a child may develop vesicoureteral reflux. Some of the more common causes include:\n- Having parents or siblings with VUR\n- Severe abnormal urinating patters such as excessive holding of urine\n- Being born with neural tube defects such as spina bifida\n- Having other urinary tract abnormalities, such as posterior urethral valves, ureterocele, or ureter duplication\nDuring infancy, the disease is more common among boys because as they urinate, there is more pressure in their entire urinary tract. In early childhood, the irregularity is more common in girls. VUR is more common in Caucasian children than in African-American children.\nWhat are the symptoms of vesicoureteral reflux?\nThe following are the most common symptoms of vesicoureteral reflux,however, each child may experience symptoms differently. Symptoms may include:\n- Urinary tract infection (urinary tract infections are uncommon in children younger than age 5 and unlikely in boys at any age, unless VUR is present)\n- Trouble with urination including:\n- Wetting pants\n- An abdominal mass from a swollen kidney\n- Poor weight gain\n- High blood pressure\nThe symptoms of VUR may resemble other conditions or medical problems. Always consult your child's physician for a diagnosis.\nHow is vesicoureteral reflux diagnosed?\nVUR can often be suspected by ultrasound before a child is born if there is stretching of the kidney (hydronephrosis), but this does not prove that reflux is present. If there is a family history of VUR, but your child has no symptoms, your child's physician may elect to perform a diagnostic test to rule out VUR.\nDiagnostic procedures for VUR include:\n- Voiding cystourethrogram (VCUG): A VCUG is an X-ray test that examines the urinary tract. A small catheter (hollow tube) is placed in the urethra (tube that drains urine from the bladder to the outside of the body) and the bladder is filled with a liquid dye. X-ray images will be taken as the bladder fills and empties. The images will show if there is any reverse flow of urine into the ureters and kidneys.\nRenal ultrasound: This is a noninvasive test in which a probe is passed over the kidney producing sound waves which bounce off the kidney, transmitting a picture of the organ on a video screen. The test is used to determine the size and shape of the kidney, and to detect a mass, kidney stone, cyst, or other obstruction or abnormalities. This cannot prove that reflux is present, but can see the stretch of the kidneys that reflux can produce, or scarring caused by reflux.\n- Blood tests to measure kidney function.\nWhat is the treatment for vesicoureteral reflux?\nVUR can occur in varying degrees of severity. It can be very mild, when urine backs up only a short distance in the ureters. Or, it can be severe and lead to kidney infections and permanent kidney damage (scarring). A Children’s National, specific treatment for VUR will be determined by your child's doctor based on:\n- Your child's age, overall health, and medical history\n- The severity or grade of reflux\n- Your child's ability to take specific medications, procedures, or therapies\n- Possibility of the reflux going away on its own\n- Your opinion or preference\nYour child's doctor may assign a grading system (ranging from 1-5) to indicate the degree of reflux. The higher the grade, the more severe the reflux.\nVUR Grade 1-3\nMost children who have grade 1 through 3 VUR do not need any type of intense therapy. The reflux resolves on its own over time, usually within five years. Children who develop frequent fevers or infections may require ongoing preventive antibiotic therapy and periodic urine tests.\nPreventive antibiotics have been shown to stop urinary infection s in some cases and pose little risk of problems. They do not make your child less immune to disease or infection. The doses used are very low, just enough to prevent a urinary infection from starting. While you are waiting for the reflux to go away, it is sometime best to keep your child on a preventive antibiotic so that they do not have more infections.\nSurgical treatment is also available.\nVUR Grade 4-5\nChildren who have grade 4 and 5 reflux may require surgery. During the procedure, the surgeon will create a flap-valve apparatus for the ureter that will the urine from flowing into the kidney. In more severe cases, the scarred kidney and ureter may need to be surgically removed.\nThe procedure can be performed through open surgery, laparoscopic surgery, and robotic surgery.\nOpen surgery is done through a lower abdominal incision (bikini incision), the bladder is opened and the ureters are repaired in such a way to prevent more reflux. The success rate is very high (95 – 97 percent)Laparoscopic surgery with robotic assistance (DaVinci) can now be performed at selected hospitals and offers a generally shorter hospital stay and more rapid recovery with three small incisions. Children’s National Medical Center offers all surgical options for correcting VUR, including robotic surgery and minimally invasive procedures. The pediatric specialists will develop a care plan that best meets the needs of each individual child.""]"	['<urn:uuid:c0c1475a-3659-43c0-8f73-6d75b0ae9c6e>']	open-ended	with-premise	verbose-and-natural	similar-to-document	novice	2025-04-14T18:34:38.591736	167	438	5565
92	When is it safe to fly after getting veins treated?	You should avoid flying for 48 hours after the treatment.	['Laser Vein Treatment\n• Remove all make-up & moisturizers from the treatment site.\n• Avoid Flying for 48 hours after the treatment.\n• You should not be pregnant or breastfeeding to receive treatment.\n• When treating facial telangiectasia near the mouth, rolled-up gauze can be placed between the lips and teeth to protect the teeth from discomfort.\n• Do not treat over areas with tattoos.\n• Apply ice or chilled gel-packs prior to treatment to increase comfort and provide increased epidermal protection.\n• Avoid aspirin or other blood thinning drugs (vitamin E, Ginkgo Biloba), ibuprofen, or arthritis medication for two days before and after treatment.\n• Avoid alcohol and do not smoke for two days before and after treatment.\n• To avoid discomfort do not shave legs the day of your appointment.\n• Do not apply lotions to legs before appointment.\n• Eat a light meal/snack one hour before treatment.\n• Bring loose fitting shorts/pants to wear after treatment.\n• If you develop a fever or other illness before your appointment you must reschedule.\n• Ice, chilled gel-packs or chilled hydro-gel pads may be applied post treatment to increase patient comfort and provide increased epidermal protection.\n• The veins may have an urticarial reaction after treatment. This is common and will diminish more quickly with the application of a mild-potency topical corticosteroid.\n• Portions of larger vessels will sometimes darken and become stiff and painful for an extended period after a treatment. The dark coagulum can be removed at 1 to 2 weeks following the treatment by nicking the vessel with a needle or blade and applying pressure to force out the coagulum.\n• For larger leg veins, some physicians say the use of compression stockings (30-40 mm Hg Pressure) for up to five days after treatment may increase patient comfort and help reduce bruising.\n• If crusting occurs, an antibiotic ointment or lubricating cream may be recommended.\n• Post-inflammatory hyper-pigmentation is a common response (especially with darker skin types) to vascular laser treatments and tends to resolve over time.\n• Immediately upon leaving the office go for a brisk 15-minute walk.\n• You may resume your normal daily activities immediately, but avoid strenuous aerobic exercise until further advised.\n• You are encouraged to wear your compression stockings as much as you can following your Sclerotherapy treatment. The more you wear your stockings, the better your Sclerotherapy results. You may remove your stockings to wash them, but while they are being washed, keep your legs elevated as much as possible.\n• Take Extra Strength Tylenol for pain. AVOID ASPIRIN, ADVIL, OR IBUPROFEN. These tend to thin blood and can lead to excessive bruising.\n• Call if you experience any severe reactions such as profound pain, intense itching, redness or fever.\n• Avoid hot conditions such as saunas, Jacuzzis, or hot showers for 7-10 days after treatment.\n• The recommended time interval between treatments is 4-6 weeks, depending on the rate of clearance following each treatment. Larger reticular vessels can take three months to resolve and should not be re-treated before then.\n• Consistent repeated treatments will produce best resolution of leg veins.']	['<urn:uuid:85d0a634-d262-4264-bb76-f413b045c2fc>']	open-ended	direct	concise-and-natural	distant-from-document	novice	2025-04-14T18:34:38.591736	51	57	3228
93	animal expert wondering nocturnal predator scorpion hunting	Scorpions are nocturnal predators that feed on small spiders and soft-bodied insects, including other scorpions and even smaller members of their own species. They have poor eyesight and don't actively stalk their prey. Instead, they lie in wait to grab prey with their pincers. Small insects are eaten immediately, while larger prey are stung and consumed after they stop struggling.	['The term occasional invaders includes those pests that often occur in buildings at some stage of their life cycle but that do not usually complete their entire life cycle within the building. Most of these pests live outside of buildings and come indoors only on occasion. Although they may enter in large numbers, they usually do little damage and are considered a nuisance simply because of their presence. Many of these pests enter buildings while they are flying around at night, because they are attracted to lights in or on the home or building (thus the term light-attracted pests).\nMany of these pests are most effectively and efficiently managed by eliminating conditions that allow them to harbor and build up to great numbers near the structure. This generally involves some sort of sanitation procedure or basic change in landscaping. Techniques such as caulking cracks and crevices around doors and windows or inserting screening into weep holes of a brick facade on the structure’s exterior can be very useful for long-term relief from these pests. There may be some factor or stimulus that is causing the pests to enter the structure. Other factors that may stimulate pest movement into structures include environmental extremes (such as unusual dry spells), excessive rainfall (and poor drainage adjacent to the foundation), onset of winter (some pests seek to overwinter in structures), or presence of some unusual food source.\nOutdoor Centipedes usually live outdoors in damp area, such as under leaves, stones, boards, or tree bark or in mulch around outdoor plantings. When these centipede habitats are near a home’s foundation, centipedes will wander inside, where they may be found at floor level almost anywhere. If provoked, larger centipedes may bite, causing some pain and slight swelling. Actually, their “bites” are not caused by their jaws or mouthparts, but by their front legs, which are modified to look and and function like jaws and contain venom glands. Smaller species are not large enough to penetrate human skin. Centipede bites are usually not serious, but an antiseptic should be used on the wound and a physician consulted in all cases where the skin is punctured.\nCentipedes are usually brownish, flattened, and elongate animals that have many body segments. One pair of legs is attached to most of these body segments. They differ from millipedes in that millipedes have two pairs of legs on most segments and bodies that are not flattened. Centipedes range in length from 1 to 6 inches and can run very rapidly.\nCentipedes do not eat food supplies or household furnishings. Since they eat insects, spiders and other arthropods, they are beneficial; however, most people consider them a nuisance when they wander indoors and want them controlled.\nHouse Centipedes are the most common pest in many parts of the United States. Unlike most other centipedes, this species generally lives its entire life inside a building.\nThe body of this centipede is usually only 1 to 1-1/2 inches long at the most, but its fifteen pairs of very long legs makes it seem much larger. The body is grayish yellow with three dark stripes extending along the full length of the back. The legs are quite long in proportion to the body, and have alternate light and dark bands running around them.\nIn the home, the house centipede prefers to live in damp areas, such as cellars, closets, bathrooms, attics (during warmer months), and unexcavated areas under the house. Eggs are laid in these same damp places, as well as behind baseboards or beneath bark on firewood.\nThe house centipede forages at night for small insects and their larvae, and for spiders. Although this centipede can bite, its jaws are quite weak. There usually is not more than a slight swelling if bite occurs.\nMillipedes normally live outdoors in damp places, such as under decaying leaves and in mulch around outdoor plantings. They feed on damp and decaying vegetable matter as well as on new roots and green leaves. In wooded areas millipedes live in piles of leaf litter. In dry weather they will migrate out of the litter piles as the leaves dry, and they may cross roads and enter buildings in large numbers. This behavior may also occur in lawns that contain thick thatch layers, or yards where large piles of leaves and compost piles are present.\nMillipedes, or thousand-leggers, as they are commonly known, are elongate brownish animals that re oval in cross section and appear to have two pairs attached to most body segments. Actually, each apparent body segment consist of two segments that are fused together and appear as one. Millipedes that commonly invade homes are 1/2 to 1-1/2 inches long and tend to coil up when resting.\nThe house cricket and the field cricket commonly invade homes. When present in large numbers, crickets are considerable annoyance and can cause damage to some fabrics, such as linens, rayon, and furs. They will attack paper, all kinds of foods, and even rubber. However, unless large numbers occur, such damage is usually minor.\nCrickets are rather closely related to cockroaches, so they have similar omnivorous feeding habits. The young, or nymphs, look like adults, except that their wings and genitalia are not developed fully.\nEarwigs are primarily scavengers on dead animal and plant material, but some species are predators. Other species may actually feed on living plants. They are active at night, and some species are attracted in large numbers to lights. During the day they usually find shelter beneath stones, boards, and debris. Only a few of the winged species are good fliers. Earwigs are often transported great distances in potted plants, nursery stock, or other plant material.\nConsiderable difficulty has been experienced in the southern United States the striped earwig. It is about 1-inch long, and readily attracted to lights. It produces a strong odor when disturbed or crushed. The striped earwig is lighter in color than the other earwigs, and the pronotum and front wings are usually marked with pale stripes along the edges and in the middle. This insect has the ability to develop large populations within a single season, and it can be a severe pest in new subdivisions or where land is being cleared for new buildings.\nScorpions are quite common in much of the southern and southwestern United States. Most species that enter houses are not very poisonous, their stings being comparable to those of bees or wasps. However, certain species in the desert Southwest can be dangerous, especially to sensitive or allergic people. Most scorpions are active at night. During the day they hide under bark, boards, and rocks or in rubbish. In houses, they are most often found in undisturbed area, such as closets, seldom-used shoes, or folded clothing.\nScorpions feed on small spiders and soft-bodied insects. They will eat other species of scorpions and even small individuals of their own species. They have poor eyesight, so they do not stalk or chase prey but lie in waiting to grab it with their pincers. Small insects are eaten immediately, but larger prey are stung and eaten after they cease to struggle.']	['<urn:uuid:6581a5ba-3010-4251-8d95-7f28e4962c90>']	open-ended	with-premise	short-search-query	distant-from-document	expert	2025-04-14T18:34:38.591736	59	384	7168
94	comparing music grade difficulty systems ameb rcm	In comparing AMEB (Australian) and RCM (Canadian) grade systems, AMEB grade levels are generally more advanced. Grade 5 AMEB is approximately equivalent to grade 7 RCM, grade 6 AMEB corresponds to grades 8-9 RCM, and grades 7-8 AMEB are comparable to grades 9-10 RCM. AMEB's piano grades are divided into three levels: Level 1 (preliminary to 4th grade), Level 2 (5th to 8th grade with certificate of performance), and Level 3 (associate and licentiate diplomas - AMusA and LMusA).	['I’ve had quite a few viewers and course participants from Australia ask me about AMEB, the Australian Music Examinations Board, which is the standard piano school in Australia. On this channel we’ve also discussed RCM (Canada/USA), ABRSM (UK/worldwide) and Trinity (UK/worldwide), so I figured it was time to have a look at Australia’s school.\nThis is an Australian-specific school, so if you’re in Canada or the US, you won’t be able to take these examinations. However, if you’re interested in the curriculum, you can follow along no matter where you live or what level you’re at.\nThat’s my favorite part of the syllabi created by the music schools. It gives piano teachers and students a sense of direction. A sense of what level you’re at, what music you’re capable of playing, and where you’re going. It’s an immensely helpful resource, whether you’re self-taught or not. I use the RCM syllabus in my studio near-daily.\nIn this episode we’re going to take a birds’-eye view of AMEB for anyone interested in checking it out. Let’s get started!\nThe Australian Music Examinations Board: Details\nAMEB came into existence way back in 1887, though back then it was simply a programme of music examinations from the universities of Adelaide and Melbourne. It was officially pronounced AMEB in 1918 as a national body.\nWith AMEB, you can study and take exams in a large variety of disciplines, some non-musical. In addition to the myriad instruments you can learn, they also include speech and drama.\nFor piano players, they have a few different options. Their “piano” option is much like the RCM and ABRSM in that it’s mainly Classical-focused. But there’s also the “piano for leisure” category, which focuses on contemporary music, as does “rockschool keyboard”. These are great alternatives for kids (and adults!) who want an alternative to classical.\nHowever, we’ll be discussing the standard piano exam and syllabus in this video. I do encourage you to explore their website if you want further information on their programs.\nAMEB’s Level System\nABRSM’s piano grades are divided into three levels:\n- Level 1 is preliminary to 4th grade\n- Level 2 is 5th to 8th grade (certificate of performance)\n- Level 3 is associate and licentiate diplomas (AMusA and LMusA)\nAMEB Piano Syllabus\nAMEB just recently updated their syllabi – the piano syllabus was fully revised in November 2018. The newest publications are called “Series 18” grade books, handbooks and recordings and there’s a book for each of the nine levels (preliminary to grade 8).\nIn addition to these Series 18 repertoire books (which we’ll talk more about soon), they also have a Technical work book in two volumes (level 1 and level 2), and a single sight reading book that covers the preliminary level all the way up to grade 8.\nThe new syllabus contains “Series 17” and “Series 18”, both of which you can play from for exams. The old syllabus (Series 15-17) is being phased out and starting in 2021 you’ll only be able to use the new syllabus.\nI could stand to be corrected here, but it looks like they come up with a new Series every 5 years. Series 17 came out in 2014, and Series 18 came out in 2019. But it looks like the syllabi are completely rehauled every decade or so, which is much like the RCM. I’m making a guess here, though, so please let me know if I’m off for those of you who are more familiar with AMEB.\nTheir syllabus isn’t freely available (like the RCM/ABRSM), but a digital download is inexpensive at $10.\nI love the AMEB syllabus because it’s so detailed and comprehensive. They have huge repertoire lists with dozens of selections of each list per grade. This is awesome for both teachers and students who are interested in exploring a wide variety of repertoire.\nAnother benefit of the syllabus, and this is something the RCM does as well, is it has a comprehensive book list. So if you’re looking for general reference music books, books on repertoire, performance or technique, it gives you a ton of fantastic options to study.\nThe RCM syllabus has always been my go-to because I’m Canadian, and because the syllabus is so robust. However, it looks like AMEB has an even more robust syllabus, so if you’re interested I encourage you to explore it.\nAMEB Repertoire books\nEach of the 9 repertoire books contain a small selection of pieces at each grade level. They include music that spans the Baroque era all the way to modern pieces. There’s an emphasis on Australian composers (just like how the RCM emphasizes Canadian composers).\nThe graded repertoire books are divided into lists A, B and C for level 1, and lists A, B, C and D for level 2. This is in line with what the RCM and ABRSM does.\nThe comparison of difficulty is quite similar to ABRSM. At a grade 5 level you’re given a Bach invention, for example, which is a late-intermediate assignment. In grade 7 you’re given a Bach sinfonia, which is early-advanced. This means the Level 2 grades are quite difficult – again, much like ABRSM.\nTo compare this to the RCM, it seems like grade 5 AMEB is about equivalent to grade 7 RCM. Grade 6 AMEB is around grade 8-9 RCM, and Grade 7-8 AMEB is comparable to grade 9-10 RCM.\nTo go along with the repertoire books, you can also purchase digital recordings of each of the pieces. The recordings are done by top Australian performers.\nLike the other major music schools around the world, an AMEB study path doesn’t just involve learning repertoire. It also involves learning technique, aural skills and sight reading. These are all skills you’d be demonstrating in an examination setting.\nInterestingly, students at Level 2 (Grade 5 to 8) can now do what’s called a “collaborative piano exam”. This involves working with other musicians and is a really great way to fill the gap of piano players always being solitary creatures.\nAnother cool thing you can do with AMEB is a repertoire-only exam, which is available for both Level 1 and Level 2. You prepare 4 pieces at level 1, and 5 pieces at level 2, just like you would with a regular test. You don’t have to prepare any technical exercises for the exam.\nThe point of this is that you’re still developing technique, just through your pieces. And the assumption is that you’re still practicing technical exercises, you’re just not asked to perform them in an exam.\nA standard exam is thus called a “comprehensive exam”.\nAs the AMEB says,\n“Although Technical work is not examined separately in an AMEB Repertoire examination, the gradual accrual of technical skills still forms part of the examination criteria at each grade. Examiners will be assessing candidates on their technical ability as demonstrated in the performance of the repertoire requirements, so candidates will still need to work on technical skills in the practice room – they just won’t be examined separately through Technical work in examination. Similarly, the development of sight-reading ability, aural skills and general knowledge is essential for a well-rounded musician, and students will need to continue to work on these areas to reach their full potential, even if they are only being formally assessed on the performance of repertoire.”\nA note that the repertoire-only exams are only available for regular piano students, not rockschool or piano leisure students.\nSomething unique about AMEB exams is that they’ll actually ask you questions (the syllabus details what kind of questions these are). For example, in level 2 exams, they might ask you about the composer, the style period, and so on.\nExam length is similar to the other schools. A preliminary exam will run 12 minutes (it goes by in a flash!) whereas a grade 8 exam is about 50 minutes.\nAMEB offers theory exams from a Grade 1 to 6 level. These grades, however, don’t correspond with the repertoire grades.\nUp until a grade 5 level, you’re not required to do a theory exam to get your certificate when you do a practical exam. But starting from Grade 6, you need to pass a music theory test in order to pass your practical test.\nHere’s a table of what theory test you need to pass at each grade level (starting at grade 6). It’s helpful to know that Trinity and ABRSM exams also count for this.\nThere are three types of theory exams with AMEB: Theory of Music, Musicianship, and Music Craft. Here’s a quick explanation of the three different types of theory:\n“Theory and Musicianship are quite closely linked up to Grade Four level. Both syllabi focus on Keys and Scales, Intervals, Chords, Time and Rhythm, Transposition, Terms and Signs and Rhythmic Invention but Musicianship tends to introduce Keys, Scales and Intervals at a faster rate than Theory. As Theory progresses there is a greater focus on the creative aspects: Harmonisation, Melody Writing and questions about General Knowledge. At Grade Four level Musicianship introduces an Aural component. This features recognition of scale forms, intervals, triad positions, motion and cadence recognition questions. There are also Time and Rhythm, Expression and Mood and Form questions. From Grade Four to Associate, Musicianship exams comprise a Written and an Aural component. Theory has no Aural component.\nMusic Craft was developed to teach the theoretical and aural aspects of music in different ways to Theory and Musicianship. Thus, pitches are described using the Helmholtz system, harmonies are described using a mixture of Roman Numerals and figured bass, cadences are described differently to the Theory and Musicianship syllabi (e.g. Authentic rather than Perfect). Also, terms include German and French words.”\nSome other neat facts about AMEB’s theory exams is that you can do the written exams online. They even have online courses from grade 1-3, and I suspect there will be more where that came from in the future.\nIf you’re Australian, I highly recommend looking into the various programs AMEB offers – especially since you don’t need to be into Classical music to benefit from it.\nFor non-Australians, it still might be worth a look if you’re looking for a great curriculum and would like an alternative to the RCM, ABRSM and Trinity schools.\nAs a non-Australian, and as someone who’s never taught via AMEB, I’m sure there are significant gaps in my knowledge. Still, I hope this enough to get you going, and I hope it helps you compare some of the options that are out there.']	['<urn:uuid:a16e3d4d-77fb-4a22-b9ca-b2b9aeb9f54b>']	open-ended	with-premise	short-search-query	distant-from-document	expert	2025-04-14T18:34:38.591736	49	481	10335
95	What are the key environmental factors that impact the spread and severity of plant infections transmitted by leafhoppers, particularly during summer months?	Hot, dry summers reduce both the numbers of aster leafhoppers and the amount of yellows infection. Drought conditions reduce the sources of suitable host plants for the leafhopper. However, it's important to note that disease severity is related to multiple factors including the size of leafhopper populations, the percent of the population that is infectious, and the ability of those individuals to transmit the pathogen. Temperature and moisture greatly influence the severity of aster yellows, and even low infestations of leafhoppers with a high proportion of infected individuals (greater than 2%) could cause serious outbreaks.	['The Aster Leafhopper and Aster Yellows\nTable of Contents\nAster yellows disease causes periodic and occasionally extensive damage to lettuce, celery and carrot crops grown in Ontario. Aster yellows is caused by a mycoplasma-like organism and is disseminated by the aster leafhopper, Macrosteles phytoplasma (AYP).\nThe aster leafhopper completes 3-5 generations in southern Ontario after overwintering in the egg stage. In some years, populations originating in the southern USA may spread into Ontario and contribute to leafhopper infestations. Some of these migrants may be infected with AYP. Both the leafhopper and the AYP have a large number of plant hosts, a fact which contributes greatly to annual fluctuations in the incidence of aster yellows in crops.\nFigure 1. Adult leafhopper with 4th instar nymph.\nIn Ontario, the aster leafhopper overwinters as an egg in the leaf tissue of winter grains such as wheat and rye (Figure 2). Snowcover promotes survival of leafhopper eggs. Adult leafhoppers cannot survive winters in southern Ontario. Once enough degree days for development have accumulated in the spring, the eggs will hatch into nymphs. Approximately 130 degree days above 9°C are required for egg maturation based on temperatures at grass levels and an additional 270 degree days are required for development to adulthood. As the winter grains mature in late May and early June, local first-generation leafhoppers disperse to more favourable hosts such as weeds, grasses, and vegetable crops. During the growing season significant populations may be produced on spring grains.\nFigure 2. Winter wheat.\nMigrant leafhoppers from the USA may arrive following persistent southerly flow of warm air with rainshowers associated with cold fronts. As grain matures from south to north in the USA, a corresponding movement of adult leafhoppers into Ontario may take place. Because leafhopper movements are regulated by wind and weather patterns, migrations into Ontario are not consistent from year to year. Both local dispersal and long distance migration influences the incidence and severity of the aster leafhopper infestations and aster yellows.\nAster leafhoppers that carry AYP feed on susceptible crops by inserting their mouthparts into the phloem tissue. The AYP cannot be immediately transmitted by a leafhopper because the AYP must multiply inside the insect. This time period is called the incubation period and takes 2-3 weeks and the insect is infected for life.\nFigure 3. AY on lettuce.\nFigure 4. AY on celery.\nInfected leafhoppers must feed on a susceptible plant for a sufficient time to inoculate it with the AYP and the pathogen must multiply in the plant before symptoms appear. For young lettuce or celery plants, aster yellows symptoms may appear in 7 days while in older plants symptoms occur after 14-21 days (Figure 3 and Figure 4). Disease severity in a crop is related to the size of the leafhopper populations, the percent of the population that is infectious and the ability of those individuals to transmit the pathogen. Large numbers of leafhoppers does not necessarily result in more severe infection of aster yellows. In some years the percent infectivity of the insects has been less than 2% and the subsequent aster yellows levels in vegetable crops has remained below 1%. If the percent infectivity were higher, possibly more aster yellows would result. In addition, other factors such as temperature and moisture greatly influence the severity of aster yellows. Hot, dry summers reduce the numbers of aster leafhoppers and the amount of yellows infection and drought conditions reduce the sources of suitable host plants of the leafhopper. However, low infestations of leafhoppers with a high proportion of infected individuals (greater than 2%) could cause serious outbreaks of aster yellows.\nBoth the aster leafhoppers and AYP have a wide range of host plants which include many broadleafs weeds, grains and grasses. Weeds that occur near crops may influence the densities of leafhoppers in the crop, by their effect on the survival of the insect and its natural enemies. In addition, certain weeds in cultivated crops may influence the incidence of the disease.\nFigure 5. Pineappleweed.\nFigure 6. Kentucky bluegrass.\nOver 300 species of plants have been identified as hosts of AYP . This includes grains such as wheat, oats, rye and barley as well as many common Ontario weeds such as quackgrass, plantain, chickory, knotweed, lambs-quarters, wild asters, sowthistle, ragweed, stinkweed, and wild carrot. Pineappleweed and Kentucky bluegrass are favoured hosts in the Holland Marsh region (Figure 5 and Figure 6).\nSerious outbreaks of aster yellows in the Holland Bradford-Marsh have resulted in 15-50% losses of lettuce. The proportion or percentage of leafhoppers carrying the disease cannot be easily determined, however technology is available to test leafhoppers for their infectivity level. Unfortunately this technology is not commercially available yet in Ontario.\nFigure 7. Yellow sticky traps.\nThe three parameters, leafhopper infectivity, leafhopper abundance and percent yellows infection, are the key elements of the epidemiology of aster yellows. Currently, only leafhopper abundance data and percent yellows infection can be collected economically. In years when leafhopper populations are high, the presence of alternate wild hosts helps to sustain and prolong the leafhopper population by providing feeding and breeding sites during periods of dispersal or when preferred crops are unavailable. When infectivity of leafhoppers is high, more wild hosts will be exposed to aster yellows and subsequently act as disease sources. Infected perennial weeds would act as reservoirs of aster yellows and would influence the level of disease in the next season.\nTwo methods can be used to monitor leafhoppers. Yellow or orange sticky traps can be used to monitor aster leafhoppers in lettuce, celery and carrots, however they must be checked daily so that rapid changes in the population can be detected (Figure 7). Timing is critical for this pest. The sticky trap method is effective at detecting major increases in the leafhopper population in a region.\nNylon/cloth sweep nets can be used to monitor leafhoppers in individual fields. The aster leafhopper will fly in short bursts between plants depending on wind conditions. Sweep nets work best when wind speeds are low and the foliage is dry. For carrots, celery and lettuce record the number of aster leafhoppers per 100 sweeps. One sweep equals one pass over the foliage and it is recommended to do 20 sweeps in 5 locations per field.\nThe treatment threshold varies not only between lettuce, celery and carrots but also from year to year, depending on the forecasted potential for aster yellows. The infectivity of the leafhoppers can be determined early in the season by tracking US populations and testing local leafhoppers. The infectivity combined with the numbers caught in the sweep net determines the annual potential for disease.\nAn aster yellows index (AYI) determines the need to treat a crop. The AYI is calculated by multiplying the infectivity level by the average number of leafhoppers collected per 100 sweeps (Infectivity Rate) x (100 Sweeps) = (Aster Yellows Index).\nTable 1. Treatment thresholds for carrots, celery and lettuce with respect to their AYI.\nIn Ontario, many of the carrot varieties have not been evaluated for aster yellows tolerance. Spartan Bonus is considered susceptible whereas Six Pak II is considered resistant. Growers should use an AYI of 70 as a guideline for other varieties.\nFor more information:\nToll Free: 1-877-424-1300']	['<urn:uuid:240588e3-3b70-4319-8258-9143b6f4b287>']	open-ended	direct	verbose-and-natural	distant-from-document	expert	2025-04-14T18:34:38.591736	157	635	7676
96	What type of fishing float works best in deep water?	The bodied waggler float is best for deep water fishing. It's especially good when swims are deeper than rod length, and its buoyant body allows plenty of shot to be used to sink bait quickly to the bottom.	['There are several simple methods of fishing with a float. Indeed this style of fishing will, no doubt, be the way most of us started out on our angling adventures.\nThis time we will cover a method of presenting bait on the bottom especially good for the margins of the stillwaters we find in our area.\nThe sliding float method is especially good when the swims are deeper than the length of your rod. A sliding rig has only a short length of line at the hook end of the float when casting. This is useful when there is either bankside vegetation or you need a little more weight to cast beyond a certain feature and wish to avoid tangles. The best float to use is a bodied waggler preferably with some loading to make sure the float does not slide up the line during the cast. The body of these floats are generally very buoyant which allows plenty of shot to be used to sink the bait quickly to the bottom. If you are fishing a swim amongst reeds or other such vegetation then this method holds the bait firm on the bottom avoiding it drifting and getting fast.\nThe waggler floats are usually marked with their weight and have an embedded swivel eye at the base for the line to pass through.\nUsing your main line you need to use either a sliding stop knot or a float stop and tie or thread one of these up your line to somewhere around the depth you think the swim is that you are to fish. For newcomers to the sport who may not be familiar with some of the terms used, the swim is the stretch of river, or the part of a lake, that is being fished at that particular time. If you are experienced with knots then you can tie your own sliding stop knot around the line in the desired position. To ensure it does not catch on the rod rings when casting leave the ends around ten millimetres long. The knot or stop does need to be adjustable until the depth of the swim is attained. The stop knot or float stop is fed up the line followed by a bead. The bead needs to be small enough not pass over the knot or stop and large enough not to pass through the eye on the bottom of the float. Now pass the line end through the eye on the bottom of the float and select a hook to tie on the end of the line. The size of the hook is determined by what bait you wish to use. I use between sizes six and twelve in the lakes here for bait such as sweetcorn and luncheon meat. Barbless hooks are best as they cause less damage to the fish and means they can be released easily letting them return to their environment in the best condition possible after being caught. Now you need to attach some soft split shot below the float to make the line sink quickly. It is important to use a very soft shot as you will maybe need to adjust the position of the shot to get the float rig working correctly.\nTake three AAA size shots to start with and position them together not too far below the float followed by a No8 shot maybe twenty centimetres above the hook. The shot you use should be easily pressed over the line with your fingers and thumbs not pliers. You will damage the line if you squeeze the shot tight on the line with pliers then try and slide it up or down. The shot should be soft enough to open with your nails and reposition when necessary.\nSo now load the bait and reel the line up the rod. The stop knot should pass through the rod rings easily and you will have your bead, float, shot and hook just below the rod tip ring. Overhead cast to where you want to be in your swim and wait for the float to run freely up the line until it reaches the stop. If the float lays down the knot is too high. If the float goes under the water the knot is too low. The float ideally needs to have the top section above the water so the bottom split shot is just on the bottom and the hook bait presented naturally. Wait for the float to dip and strike to set the hook. This method of float fishing is also good in harbours or marinas, if fishing is allowed in these areas of course.\nCopyright © Gary Smith']	['<urn:uuid:3031ad6d-be7d-42d1-a731-d66258ff0654>']	factoid	with-premise	concise-and-natural	distant-from-document	novice	2025-04-14T18:34:38.591736	52	206	3991
97	I'm working on wildfire risk assessment and would like to know about the makeup of the Woodmoor settlement. What's the residential layout and natural environment of this Colorado community?	Woodmoor is a large community with approximately 2,500 single dwelling homes on ½ acre lots. Two-thirds of the lots are situated in ponderosa pine forest, while the remaining lots have mixed brush and grasses.	['Individual homeowners learn about and implement Firewise improvements to their home and landscape differently. The facilitators of the Woodmoor Firewise Community at Monument, Colorado have recognized and embraced their differences. By learning to accept and work with homeowners where they are, they are able to help them take little steps that over time can create resiliency. Recognizing that everyone learns and takes action diversly has helped this community learn how to encourage their neighbors to make Firewise changes to their properties that collectively will make a difference. This concept identified in a scientific theory by distinguished communications professor, Everett M. Rogers Phd Diffusion of Innovations explains similarly how people adopt new ideas. Read about how they learned about each other, accepted each other and have taken action to reduce their risk to wildfire.\nWoodmoor at Monument, Colorado\nWoodmoor is a large community with about 2,500 owners of single dwelling homes on ½ acre lots. Two-thirds of the lots are in ponderosa pine forest. Others have mixed brush and grasses. The recent Waldo Canyon and Black Forest wildfires, which destroyed over 1,000 homes were a big motivator for Woodmoor homeowners to reduce fuels on their properties. Part of Woodmoor was on pre-evacuation alert during the Black Forest Fire in 2013.\nWoodmoor held an outdoor fair with booths on Firewise Day. Experts manned booths and talked to homeowners about Firewise landscaping and mitigation based on defensible space guidelines, hardening homes against burning embers, and emergency preparedness. Homeowners took the information and plan to work on mitigating their properties over the next few years.\nAlso in 2014, the community held two classes on “Do it Yourself Mitigation”, with 80 homeowners in attendance. The Woodmoor Firewise group did property evaluations for 89 homeowners, and sponsored 6 reduced-fee disposal Saturdays during the summer months, when residents in neighboring communities could drop off their slash for only $5 per load.\nWoodmoor says “As you and other leaders of Firewise communities know, education of homeowners must be continuous and “using all means”. I would guess that at least 40% of our owners have done some mitigation, and 40% have done none. The other 20% group are cautious deciders, and often need personal home and lot mitigation evaluations, “how to” mitigation classes, and other ways to be motivated. So our Firewise activities are focused on education and motivating the “open but undecided” owners, and owners who have done some mitigations, but need to do more. One of my personal surprises is how many residents have done “baby steps” of mitigation over the last 10 years. Maybe the first step was removal of flammable trees and shrubs within 15 feet of their homes. Next, they thinned their pine trees growing within 30 feet of their houses. A few thinned their pines to 10 foot crown spacing out to the property lines. Consequently, the lots of owners in our 40% mitigators group exhibit various degrees of mitigation. I do not know of any homeowner who did all the recommended mitigation at one time.”\nImage of damage from Black Forest Fire on the Inci Website']	['<urn:uuid:ae48091c-dc52-42f8-8ca7-b7f1ad883e34>']	factoid	with-premise	verbose-and-natural	distant-from-document	expert	2025-04-14T18:34:38.591736	189	209	3232
98	What makes 2DLT unique compared to traditional HIV drugs?	Unlike traditional drugs that inhibit viral processes at the cell surface or in infected cells, 2DLT can cripple and inactivate free virus even in the absence of cells.	"[""|HIV takes double hit before entry.|\n|Jump to Full Text|\n|PMID: 23216951 Owner: NLM Status: MEDLINE|\n|In the absence of a vaccine or a cure, identification of novel HIV-1 inhibitors remains important. A paper in Retrovirology describes a rationally designed bi-specific protein that irreversibly damages the viral envelope glycoprotein complex via a two-punch mechanism. In contrast to traditional drugs that inhibit essential steps in the viral life cycle at the cell surface or in the infected cells, this inhibitor cripples free virus in the absence of cells.|\n|Rogier W Sanders|\n|Type: Journal Article; Research Support, Non-U.S. Gov't Date: 2013-12-07|\n|Title: BMC biology Volume: 10 ISSN: 1741-7007 ISO Abbreviation: BMC Biol. Publication Date: 2013|\n|Created Date: 2012-12-10 Completed Date: 2013-05-24 Revised Date: 2013-07-11|\nMedline Journal Info:\n|Nlm Unique ID: 101190720 Medline TA: BMC Biol Country: England|\n|Languages: eng Pagination: 99 Citation Subset: IM|\n|Laboratory of Experimental Virology, Department of Medical Microbiology, Center for Infection and Immunity Amsterdam (CINIMA), Academic Medical Center, University of Amsterdam, 1105 AZ Amsterdam, The Netherlands. email@example.com|\n|APA/MLA Format Download EndNote Download BibTex|\nGene Expression Regulation, Viral\nHIV / physiology*\nHIV Infections / prevention & control\nenv Gene Products, Human Immunodeficiency Virus / genetics, metabolism\n|0/Anti-HIV Agents; 0/Recombinant Proteins; 0/env Gene Products, Human Immunodeficiency Virus|\nJournal ID (nlm-ta): BMC Biol\nJournal ID (iso-abbrev): BMC Biol\nPublisher: BioMed Central\nCopyright ©2012 Sanders; licensee BioMed Central Ltd.\nReceived Day: 29 Month: 11 Year: 2012\nAccepted Day: 5 Month: 12 Year: 2012\ncollection publication date: Year: 2012\nElectronic publication date: Day: 7 Month: 12 Year: 2012\nVolume: 10First Page: 99 Last Page: 99\nPubMed Id: 23216951\nPublisher Id: 1741-7007-10-99\n|HIV takes double hit before entry|\n|Rogier W Sanders12||Email: firstname.lastname@example.org|\n1Laboratory of Experimental Virology, Department of Medical Microbiology, Center for Infection and Immunity Amsterdam (CINIMA), Academic Medical Center, University of Amsterdam, 1105 AZ Amsterdam, The Netherlands\n2Department of Microbiology and Immunology, Weill Medical College of Cornell University, New York, NY 10065, USA\nAntiviral drugs are potent at suppressing viral replication in HIV infected individuals. Unfortunately, current drug regimens cannot cure infected persons because of the establishment very early in infection of latent viral reservoirs that cannot be eliminated by conventional drugs. The continuous virus evolution and the danger of viral escape from the available drugs necessitate the search for novel inhibitors. HIV drugs can target a variety of processes of the viral life cycle that include viral entry into host cells, viral reverse transcription, integration into the host genome, and virus maturation.\nThe HIV envelope glycoprotein complex (Env) is an intricate molecular machine that mediates the viral attachment to target cells and the fusion of viral with cellular membranes (Figure 1), and does so in a highly coordinated manner. Env has been compared with a mousetrap : it is spring-loaded and snaps shut when touching an infectable target cell. This process occurs roughly in three phases, each of which can be inhibited by a class of entry inhibitors. The first event is the binding of the surface subunit gp120 to the primary receptor on the target cell, CD4, and this step can be inhibited by CD4 mimetics, though these have not proved effective enough for clinical use . CD4 binding creates and exposes the binding site on gp120 for the second receptor, one of the chemokine receptors CCR5 or CXCR4. The binding to the co-receptor can be inhibited by co-receptor antagonists , an example of which is the CCR5-antagonist maraviroc, which has been in clinical use since 2007. CD4 binding also induces conformational changes in the transmembrane subunit gp41 that result in exposure of the hydrophobic fusion peptides. Insertion of the fusion peptides into the target membrane, followed by further conformational changes induced by binding to the co-receptor, culminates in membrane fusion and release of the viral genetic material into the cytoplasm. The CD4-induced, activated state of gp41 can be targeted by fusion inhibitors  and one such fusion inhibitor, enfuvirtide (T20), has been used to treat HIV-1 infected individuals since 2003. Owing to its poor bioavailability (it requires intravenous injection twice daily), and relatively expensive manufacturing (it is a peptide), the use of enfuvirtide is declining as it is superceded by cheaper alternative drugs that are orally bioavailable. Nevertheless, enfuvirtide saved many lives when it came on the market at a time when no alternative new drugs were available. Second and third generation enfuvirtide-like fusion inhibitors have been designed that are more potent, have better pharmacokinetic properties, and are less prone to viral escape . One such fusion inhibitor is the peptide T1144.\nNow a team led by Shibo Jiang has designed a novel bi-specific inhibitor, 2DLT, which essentially is a fusion protein of a soluble version of CD4 and the third generation fusion inhibitor T1144 . As such it can inhibit the interaction of gp120 with CD4 as well as the conformational changes in gp41 that result in membrane fusion, resulting in HIV-1 inhibition at low nanomolar concentrations. While bi-specific and multispecific proteins are widely studied for use in cancer therapy, only a few bi-specific molecules have been designed for HIV-1. One such molecule, termed sCD4-17b, shares a CD4 mimetic component with 2DLT, but further contains an antibody fragment directed to an epitope on gp120 that is induced by CD4 binding and that overlaps with the co-receptor binding site . A second approach uses bi-specific antibody molecules targeting two different epitopes, one on gp120 and one on gp41 . Both sCD4-17 and the bi-specific antibodies result in low nanomolar inhibition of HIV similar to 2DLT.\nThe beauty of the 2DLT inhibitor is not its dual activity per se, but its potential to inactivate the virus in the absence of cells. For viral entry into cells it is essential that the mousetrap shuts when a mouse is eating from the cheese, in other words when the virus is attaching to a target cell. This timing is coordinated by the activation of Env's spring-loaded fusion machinery only when the virus is attaching to an infectable cell via CD4. It has been previously recognized that CD4 mimetics induce a short-lived activated Env state that deteriorates into an inactive Env form ; however, CD4 mimetics are usually not very potent in doing so. The CD4 component of 2DLT also induces the short-lived activated Env form, but then the fusion inhibitor component, T1144, delivers a second blow by binding and blocking the activated fusion machinery in gp41. Thus, 2DLT induces a premature and irreversible collapse of the viral mousetrap, thereby preventing viral entry into target cells. As a consequence, in regular infection inhibition experiments in the presence of target cells, 2DLT is as potent as its most potent constituent, T1144. However, in sharp contrast, 2DLT is able to disable the virus in the absence of target cells, while T1144 is not. What Lu et al.  did not study is whether the two separate components of 2DLT, T1144 and soluble CD4, can disable the virus when simply mixed. This would reveal whether the combined action of the two components of 2DLT require a physical linkage.\nDespite the promise of its novel mechanism of action, some formidable challenges lie ahead before 2DLT or 2DLT-derivates will be suitable for wide clinical use. Poor bioavailability and expensive manufacturing put 2DLT at a disadvantage compared to currently available small molecule inhibitors. However, its mechanism of viral deactivation away from cells puts it at a unique advantage, and as such it warrants further research. 2DLT may be considered for use as a microbicide - for example, in vaginal gels that are aimed at preventing HIV-1 transmission at the vaginal mucosal surface. Another interesting application of 2DLT could be in viral immune prophylaxis (VIP). VIP is a gene therapy vaccination approach in which the constitutive expression by host cells of a neutralizing antibody or an inhibitory protein provides vaccine-like protection against viral infection . In summary, the bi-specific and dual active 2DLT inhibitor described by Lu et al., with its one-two punch that inactivates free virus, represents a novel drug approach that warrants further evaluation.\nThe author is a recipient of a Vidi fellowship from the Netherlands Organization for Scientific Research (NWO), and a Starting Investigator grant from the European Research Council (ERC-StG-2011-280829-SHEV).\n|Binley J,Moore JP,HIV-cell fusion. The viral mousetrapNatureYear: 199738734634810.1038/387346a09163413|\n|Haim H,Si Z,Madani N,Wang L,Courter JR,Princiotto A,Kassa A,DeGrace M,McGee-Estrada K,Mefford M,Gabuzda D,Smith AB,Sodroski J,Soluble CD4 and CD4-mimetic compounds inhibit HIV-1 infection by induction of a short-lived activated statePLoS PathogYear: 20095e100036010.1371/journal.ppat.100036019343205|\n|Ray N,Doms RW,HIV-1 coreceptors and their inhibitorsCurr Top Microbiol ImmunolYear: 20063039712010.1007/978-3-540-33397-5_516570858|\n|Eggink D,Berkhout B,Sanders RW,Inhibition of HIV-1 by fusion inhibitorsCurr Pharm DesYear: 2010163716372810.2174/13816121079407921821128887|\n|Eggink D,Bontjer I,Langedijk JP,Berkhout B,Sanders RW,Resistance of human immunodeficiency virus type 1 to a third-generation fusion inhibitor requires multiple mutations in gp41 and is accompanied by a dramatic loss of gp41 functionJ VirolYear: 201185107851079710.1128/JVI.05331-1121835789|\n|Lu L,Pan C,Li Y,Lu H,He H,Jiang S,A bivalent recombinant protein inactivates HIV-1 by targeting the gp41 prehairpin fusion intermediate induced by CD4 D1D2 domainsRetrovirologyYear: 2012910410.1186/1742-4690-9-10423217195|\n|Lagenaur LA,Villarroel VA,Bundoc V,Dey B,Berger EA,sCD4-17b bifunctional protein: extremely broad and potent neutralization of HIV-1 Env pseudotyped viruses from genetically diverse primary isolatesRetrovirologyYear: 201071110.1186/1742-4690-7-1120158904|\n|Mouquet H,Warncke M,Scheid JF,Seaman MS,Nussenzweig MC,Enhanced HIV-1 neutralization by antibody heteroligationProc Natl Acad Sci USAYear: 2012|\n|Berkhout B,Sanders RW,Gene therapy as a vaccine for HIV-1Expert Opin Biol TherYear: 2012121315132110.1517/14712598.2012.70717722803517|\nPrevious Document: Transient neuromyopathy after bromide intoxication in a dog with idiopathic epilepsy.\nNext Document: Compensability index for compensation radiotherapy after treatment interruptions.""]"	['<urn:uuid:c795dbd6-f91d-4a36-9ac6-fda6f69ea1d4>']	factoid	with-premise	concise-and-natural	similar-to-document	expert	2025-04-14T18:34:38.591736	57	168	10855
99	hemodialysis center treatment frequency sessions per week	People who get regular dialysis usually have it 3 times each week in a hemodialysis center.	"[""This material must not be used for commercial purposes, or in any hospital or medical facility. Failure to comply may result in legal action.\nHemodialysis For Acute Kidney Failure\nWhat do I need to know about hemodialysis for acute kidney failure:\nHemodialysis is a procedure that uses a machine to do the job of your kidneys. The machine pumps your blood through a dialyzer, or artificial kidney. The dialyzer filters fluid, salts, and waste from your blood. Once they are removed, clean blood from the dialyzer returns to your body through a vein. Acute kidney failure happens when your kidneys suddenly stop working. Failure happens quickly, within hours or days.\nWhat needs to be done before hemodialysis:\nYour weight, temperature, pulse, and blood pressure will be checked. A central venous catheter will be placed into a vein in your arm, chest, or groin. The catheter will be hooked up to the machine and dialysis will start.\nWhat happens after hemodialysis:\nYou may need to rest in the hospital so your healthcare provider can monitor your condition. You may need more hemodialysis sessions. People who get regular dialysis usually have it 3 times each week in a hemodialysis center. If you need long-term dialysis, you may need surgery to make an arteriovenous fistula or arteriovenous graft. Ask your healthcare provider for more information about an arteriovenous fistula or graft.\nCall 911 for any of the following:\n- You have sudden chest pain or trouble breathing.\n- You are breathing fast or have a fast heartbeat.\n- You feel confused, dizzy, or lightheaded.\nSeek care immediately if:\n- Blood soaks through your bandage.\n- The skin around your fistula or graft is painful, hot, red, or swollen.\n- You cannot eat or drink because you are vomiting.\n- Your fingers are blue or pale, or they feel cool to the touch.\nContact your healthcare provider if:\n- You have a fever.\n- You do not feel a buzzing sensation in your fistula or graft.\n- You have chills, cough, or feel weak and achy.\n- Your skin itches or you have a rash.\n- You cannot make it to your follow-up or dialysis visit.\n- You have questions or concerns about your condition or care.\nmay help prevent anemia (low level of red blood cells). Take vitamins as directed by your healthcare provider.\nYour healthcare provider will tell you what changes you need to make to the foods you eat. A dietitian can help you plan meals.\n- Eat foods as directed. You may need extra calories or protein. Limit potassium, phosphorus, and sodium (salt). Talk to your healthcare provider or dietitian for help or more information about nutrition.\n- Drink liquids as directed. Ask how much liquid to drink each day and which liquids are best for you. Keep a record of how much liquid you drink each day. Count ice cubes, soup, gravy, gelatin, and popsicles. Limit caffeine.\n- Keep your mouth moist. Suck on hard candy or lemon wedges, or chew gum.\nFollow up with your healthcare provider as directed:\nWrite down your questions so you remember to ask them during your visits.\n© 2016 Truven Health Analytics Inc. Information is for End User's use only and may not be sold, redistributed or otherwise used for commercial purposes. All illustrations and images included in CareNotes® are the copyrighted property of A.D.A.M., Inc. or Truven Health Analytics.\nThe above information is an educational aid only. It is not intended as medical advice for individual conditions or treatments. Talk to your doctor, nurse or pharmacist before following any medical regimen to see if it is safe and effective for you.""]"	['<urn:uuid:901b7997-db90-4742-abde-c4962e1d904b>']	open-ended	with-premise	short-search-query	similar-to-document	expert	2025-04-14T18:34:38.591736	57	91	3557
100	los angeles yearly sunshine proportion	Los Angeles receives sunlight approximately 44% of the time throughout the year.	['Sunlight Damage on Roof\nSimilar to the effects of the Sun on your skin, it is important to note that commercial roofing systems are also very sensitive to the Sun. Particularly, in certain areas of the United States like the Western parts. The geographical locations are heavily impacted by the amount of Sun they get. For instance, if you reside in the Los Angeles area, the amount of Sun that you will get is about 44% of the time throughout the year. Unfortunately, even though some people may enjoy this warm weather, this amount can wreak havoc on roofing systems, especially because the Sun is composed of both UV radiation and infrared radiation.\nSunlight Damage on Roof\nNegative Domino Effects of Sunlight on Your Roofing\nThere are a number of different negative domino effects that you will most likely see on your roof today. Some of the more commonly known include bumps, cracks as well as other kinds of damages. In fact, as the beams of the Sun break down the roofing material, it causes the chemicals in the seam sealants to split and open prematurely over time. In addition to the seams of the roofing opening up, the damages from the Sun can also adversely affect the fasteners on the roof because of the pulls, tears and the twist that negatively compromises the fasteners. Further damages may be caused by the integrity of the roof, even after the Sun has set in the afternoon. This is because dark coloring on your roofing system will inherently hold onto the heat and will not allow it to cool down. Thereby, causing the heat that remains to continue to compromise the integrity of your roof.\nHidden Damages Exposed When Ultraviolet Radiation Compromises the Roofing\nEven more insidious, you will also discover that the ultraviolet radiation, another part of the sunlight, is not easily detected but imposes a lot of extra damage to the roofing’s overall structure. Therefore, it does not matter what kinds of materials used in your roofing system, the degradation continues in pretty much any type that is installed (i.e. single-ply-membrane, bitumen and others). Also, because this is hidden damage that is being done behind the scene in a more subtle fashion, no one will notice that these problems have been occurring until the rain clouds move in as the Sun goes away. Hence, when you inspect the roofing system on your building, the after effects usually show split seams, cracking, and blistering that will ultimately allow water flows to work their way down inside the building causing all kinds of interior damage.\nDealing with the damages that occur to your roofing system can be a huge issue for you when you are impacted by its effects. Due to the severe damages that the Sun can cause on the roofing materials and the seams in the design, the damages can completely destroy the tops of your building. To avoid these issues and damages, it is important to note that many property owners are turning to a newer innovative solution that is called cool roof systems.']	['<urn:uuid:291efccb-36a3-4d9e-8aa6-d189339caf38>']	factoid	direct	short-search-query	distant-from-document	expert	2025-04-14T18:34:38.591736	38	80	2995
