qid	question	answer	context	document_ids	question_factuality	question_premise	question_phrasing	question_linguistic_variation	question_multi-doc	user_expertise-categorization	generation_timestamp	question_length	answer_length	context_length
1	How does the artwork 'For Proctor Silex' create an interesting visual illusion for viewers as they approach it?	Willie Cole's 'For Proctor Silex (Evidence and Presence)' creates an optical illusion where from a distance, it appears to be an African figurine displayed against patterned cloth, but upon closer inspection, it transforms into an industrial iron, which was the very tool used to burn the pattern into the canvas.	['OBERLIN, Ohio—The “Black Atlantic” is a cultural and geographic concept coined in 1993 by Paul Gilroy, and proposes a theory of the African diaspora that addresses points of origin obscured by slave trade and forced displacement, drawing identity from the bonds formed in the course of transport across the Atlantic Ocean. As Gilroy would have it, this produced “a culture that is not specifically African, American, Caribbean, or British, but all of these at once, a black Atlantic culture whose themes and techniques transcend ethnicity and nationality to produce something new …” In a show that opened earlier this year at the Allen Memorial Art Museum at Oberlin College, Afterlives of the Black Atlantic teases out aesthetics and individual visions that arise from this context. The show was co-curated by Andrea Gyorody, Ellen Johnson ’33 Assistant Curator of Modern and Contemporary Art, and Matthew Francis Rarey, Assistant Professor of the Arts of Africa and the Black Atlantic, and Oberlin’s first African Diasporic specialist.\n“We met shortly after I was hired in 2017,” said Gyorody, during a gallery tour with Hyperallergic. “And [Rarey] mentioned, sort of offhandedly, that 2019 would be the 400-year anniversary of the arrival of [the first] slave ships in the United States.” This sparked a discussion that evolved into the mounting of Afterlives, which brings together works from the United States, Europe, Latin America, the Caribbean, and Africa, drawn mostly from the AMAM collection, and supplemented by several loans and a site-specific commission by José Rodríguez. This work, titled “\\sə-kər\\” presents as an 12-foot Virgin of Regla — patron saint of the city of Havana and an adaptive form of the Orisha Yemayá, who protects the seas — identifiable by her regalia though the garments have been hollowed out to create a kind of open teepee, and the face has been replaced by a mirror that captures the visage of the viewer as she approaches.\n“I chose the phonetic spelling because it alludes to that space between and can be read as both ‘sucker’ and ‘succor,’” Rodríguez told The Oberlin Review. At the feet of the structure’s opening, an elaborate arrangement of pennies forms the threshold to the interior space, bedecked with the spiritual tools of Santería practitioners. Santería is a prime example of the African diasporic evolution of culture, as it represents an adaptation of Yoruba spiritual practice to enable its survival under Catholic colonialism in places like Cuba, Puerto Rico, the Dominican Republic, and other places in the mainland Americas. Rodríguez’s virgin seems to watch over the exhibition, while a twentieth-century Bocio figure by an unrecorded artist or workshop, attributed to the Republic of Benin or Togo, stands as a kind of sentinel as one enters the main gallery space. The figure is reminiscent of the Congolese/Central African nkisi, but where nkisi are objects inhabited by spirits, Bocio translates as “empowered cadaver” and exists in connection with Vodun spirituality, a set of practices that fused with outside influences under colonial rule and the slave trade, making its way from its West African origins to its expression as Vodou in Haiti.\nIn addition to works like these, that find roots in specific African traditions, there are a number of contemporary works that grapple with the more ambiguous aspects of identity wrought by slavery and involuntary relocation. “Untitled” (1999) by Leonardo Drew presents an abstracted geography, with one half of the large-scale wall hanging constructed of hundreds of cell-like openings stuffed with cotton, and the other half comprised of mirroring cells made primarily of discarded wood and rusty industrial materials of indeterminate origin. It is not a stretch to imagine this landscape as an abstracted picture of the labor history of Black people in the United States, where North-South divisions have merely offered enslaved and freed Black people different flavors of limiting and exploitative conditions.\nThis hangs adjacent to a bright blue candy spill by the late Félix González-Torres, which likewise transforms the floor space into a sort of proxy-ocean — one which directly implicates the sugar industry responsible for stoking demand in the slave trade, as well as any viewer who responds to the explicit invitation to take a piece of candy. In his lifetime, González-Torres oftentimes dealt with his own HIV-positive status and the condition of others living with AIDS, represented here by the slow disappearance of a sculptural work via audience participation; that the meaning of the work can be so readily adapted to the erosive and gutting power of disappearance and dislocation enacted by the slave trade acts as a powerful illustration of the way contemporary artists of color have responded either literally or thematically to this violent history. This point is echoed in a number of other works, including those by Alison Saar, Fred Wilson, Wangechi Mutu, and Dawoud Bey.\nAfterlives of the Black Atlantic hits all the right notes, bringing a stunning variety of media, sources, and perspectives into dynamic conversation. Each work carries its own weight, but moving through the gallery creates a number of arresting tableaus. In perhaps the most startling reveal of the show, “For Proctor Silex (Evidence and Presence)” by Willie Cole appears, from afar, to be an African figurine displayed against a backdrop of thick patterned cloth. Drawing closer, the figure transforms into an industrial iron, the very implement used to burn the motif into the canvas. This optical shifting between sacred figure and labor implement, decorative adornment and burnt offing, encapsulates the polarities of diasporic experience, and provides one of many moments that will linger in this space of examination.\nAfterlives of the Black Atlantic remains on display at the Allen Memorial Art Museum through May 24, 2020.']	['<urn:uuid:b8ccd355-214a-424c-9d1c-5102ef4e8ce1>']	factoid	direct	verbose-and-natural	distant-from-document	single-doc	expert	2025-04-22T14:43:01.858830	18	50	943
2	I read somewhere about planets without stars and it got me curious - is it really possible for a planet to exist without being attached to any star? Can someone explain how this works?	Yes, it is possible. These are called free-floating or rogue planets, and they have no gravitational ties to any star or stellar object, so they wander alone in space. They are believed to have been ejected from their original solar systems sometime in the distant past.	['An international team of astronomers using the European Southern Observatory’s Very Large Telescope (VLT) and the Canada-France-Hawaii Telescope has identified what they’re calling the most exciting free-floating, or rogue planet ever found.\nWriting in the journal Astronomy & Astrophysics, the scientists say this rogue planet, located relatively close to our own Solar System – about only 100 light years from Earth — may help explain how planets and stars form.\nA free-floating planet is one that has no gravitational ties to any particular star or other stellar object so it wanders alone in space. Also called orphan or nomad planets, these objects are believed to have been ejected from their original home solar system at some time in the distant past.\nWhile objects such as this have been found before, scientists haven’t been too clear on whether or not they were true planets or if they were, perhaps, brown dwarfs, which are stars that failed to fully form and are unable to generate or sustain the needed nuclear fusion to become true stars.\nSince this newly identified rogue planet doesn’t have a very bright star close to it, the astronomers say that they were able to study it and its atmosphere in great detail.\n“Looking for planets around their stars is akin to studying a firefly sitting one centimeter away from a distant, powerful car headlight,” says Philippe Delorme from the Institut de planétologie et d’astrophysique de Grenoble, lead author of the study that identified the new planetary object. “This nearby free-floating object offered the opportunity to study the firefly in detail without the dazzling lights of the car messing everything up.”\nAstronomers say that this newly discovered object, called CFBDSIR2149, seems to be traveling along with a group of young stars that may have all formed at the same time called the AB Doradus Moving Group. This group of about 30 or so stars is also moving through space with the star AB Doradus, the primary star within a three star system found in the constellation Dorado.\nThe scientists say that this planetary object is the first that was ever identified within a moving group of stars. And, if they find that it’s actually linked with the AB Doradus Moving Group, it, like the stars in the group, would be a relatively young object.\nA real association between this nomad planet with the moving star group, according to the astronomers, could make it easier for them to figure out the object’s age, temperature, mass and atmospheric composition.\nBut the scientists say there’s also a small chance that the planet’s relationship with the moving star group might be by chance.\nIf it’s found that the planetary object isn’t actually associated with the group, the astronomers say that it would be trickier to track down its nature and physical properties.\n“Further work should confirm CFBDSIR2149 as a free-floating planet,” says Delorme. “This object could be used as a benchmark for understanding the physics of any similar exoplanets that are discovered by future special high-contrast imaging systems, including the SPHERE instrument that will be installed on the VLT.”\nIf you go, bring a good warm jacket, it’s gonna be a little chilly. 🙂\nA great discovery\nThis is theoretically IMPOSSIBLE!!!\nThis “Planet” (??)… Must have a parent Star\nAll “non-sun” Objects in space (meteorites, comets, asteroids)\nAre ‘slave’ of a Star…! Find the star!\nThis might be an interesting read for you – http://science.nasa.gov/science-news/science-at-nasa/2011/18may_orphanplanets/\nVery certainly, “Science World”, I will read the article.\nThank you very much!\nBut…”All “non-sun” Objects in space (meteorites, comets, asteroids)\nAre ‘slave’ of a Star…! ”\nSincerely yours! Guy D.\nHot Jupiters disrupt the orbit of fellow planets in a particular solar system, forcing these now orphan planets into empty space.\nComments are closed.']	['<urn:uuid:11a1ed44-6b7e-4df2-8c4b-10fe5e71d9f6>']	factoid	with-premise	verbose-and-natural	distant-from-document	single-doc	novice	2025-04-22T14:43:01.858830	34	46	627
3	cybersecurity attack types human technical defenses	Cyber attacks target both human vulnerabilities through social engineering (exploiting trust and helpfulness) and technical systems through active attacks (like DDoS and man-in-the-middle attacks). Defense requires both human vigilance (maintaining suspicion, limiting personal information shared) and technical measures (firewalls, software updates, access restrictions) for comprehensive protection.	['To obtain confidential information, criminals often abuse the good faith, helpfulness or insecurity of their victims. Whether this involves fictitious telephone calls, fake policemen or phishing - the target of social engineering attacks is always a human being. The best protection is a “healthy dose of suspicion”.\nProtect yourself against social engineering attacks by...\n- disclosing as little information about yourself as possible. On social networks in particular, you should only ever divulge information very sparingly.\n- never letting anybody else know your passwords or TAN codes - not even system administrators or your boss. A password belongs to you, and you alone!\n- being wary when receiving requests by e-mail or telephone. Even e-mails from known senders and telephone calls received from familiar telephone numbers can be fake!\nSocial engineering attacks aim at eliciting personal or confidential information (for instance access data, passwords, etc.) from you, to then use them illicitly.\nAs a first step, criminals try to collect as much information about their victim as possible. That’s because with this information, it is easier to mislead them. This for instance allows fraudsters to then pretend to be someone you know.\nAnd the ideal means to obtain information is the Internet. Social networks in particular, such as Facebook, Xing, Instagram etc., contain very many personal details. Based on such data, attackers can then specifically address someone. Thanks to the information collected, they then seem trustworthy.\nHow can you effectively protect yourself?\nUnfortunately, there are no technical measures protecting against any social engineering attacks. Since attackers specifically exploit human characteristics such as helpfulness, insecurity, good faith and basic trust in others, it is very difficult to discover and fend off a social engineering attack.\nGenerally, the only protection is a “healthy dose of suspicion” towards strangers - but also towards people you (seemingly) know. It is also often helpful to scrutinise the information you disclose about yourself, and who you disclose this to.\nIn case of suspicion, advise your financial institution\nIf anything seems suspicious with regard to your e-banking, don’t divulge anything, and advise your financial institution as soon as you can. The coordinates can be found here.\nSocial engineering examples\n- Someone pretends to be an engineer (for instance working for a communication company, an electricity provider, etc.) and tries to gain access to your house or company this way.\n- You receive an e-mail asking you to click on a link and then log in, or to disclose some personal details.\n- Someone calls you on the telephone and would like to ask you certain questions for a survey (for instance as to how much you earn, about security measures on your computer, etc.).\n- An attacker fakes the e-mail sender address and this way pretends to be someone else you know (potentially with an attachment containing malware).\n- At work, you are approached by someone purporting to be an IT employee who pretends having to undertake some maintenance tasks on your computer.\n- Some social engineering attacks even involve people specifically applying for a vacancy in a company to then proceed to steal specific information.', 'What is an Active Attack in Cybersecurity?\nCybersecurity is an ever-growing concern in today’s digital world. An active attack is a type of cyber attack that involves an attacker actively manipulating a system or network in order to gain access to sensitive information. Active attacks are more dangerous than passive attacks, as they require more technical knowledge and skill to execute. Cyberattacks can be targeted or un-targeted, with the former being more damaging due to its tailored nature. Common types of active attacks include session hijacking, man-in-the-middle attacks, credential reuse and malware. These malicious attempts seek to unlawfully access data, disrupt digital operations or damage information systems and networks. As such, it is important for individuals and organizations alike to take steps towards protecting themselves from these threats by implementing strong cybersecurity measures such as antivirus software and two factor authentication protocols.\nTypes of Active Attacks\nDenial of Service (DoS) attacks are a type of active attack that involves flooding a network or system with an overwhelming amount of requests, preventing legitimate users from accessing the system or network. Distributed Denial of Service (DDoS) attacks are similar to DoS attacks, except they involve multiple computers sending requests to the target system or network in order to overwhelm it. Man-in-the-Middle (MitM) Attacks involve an attacker intercepting communications between two parties and viewing, modifying, or stealing the data being exchanged. Spoofing Attacks involve an attacker impersonating another user or system in order to gain access to sensitive information or resources. Password Attacks involve an attacker attempting to guess a user’s password in order to gain access to the system or network.\nProtocol attacks, also known as state-exhaustion attacks, cause service disruption by overconsuming server resources and/or the resources of networks connected with them. In a DoS attack, malicious actors flood a target with large amounts of traffic which can make it inaccessible for its intended users. This type of attack is distinct from other denial-of-service (DoS) attacks because it uses only one Internet connected device and one network connection for flooding its target with traffic until it becomes overwhelmed and shuts down temporarily. It is important for organizations and individuals alike to understand how these types of active cyberattacks work so that they can take steps towards defending against them effectively.\nPassword attacks are another form of active attack which involves attackers attempting to guess passwords in order gain access into systems and networks without authorization. These types of cyberattacks have become increasingly common as hackers become more sophisticated in their methods for obtaining confidential information from unsuspecting victims online through brute force techniques such as dictionary attacks where they use lists containing thousands upon thousands words trying each one until they find the correct combination that unlocks access into accounts protected by passwords alone without any additional security measures like two factor authentication enabled on them making them vulnerable targets for these kinds of cyberattacks if not properly secured against them beforehand by implementing stronger password policies across all systems within their organization’s infrastructure .\nMan-in-the Middle (MitM) Attacks are also considered active cyberattacks where attackers intercept communications between two parties allowing them view , modify , steal data being exchanged between both sides . MitM attackers usually use malicious software installed on devices connected within same local area networks such as public WiFis allowing anyone who has physical access into those networks eavesdrop on all communications taking place within those networks without anyone noticing what’s happening until after damage has already been done . Organizations should always be aware about potential risks associated with using public WiFis when transmitting sensitive data over unsecured connections since this could potentially lead MitM attackers gaining unauthorized access into their systems if proper security measures aren’t taken beforehand like using Virtual Private Networks (VPNs).\nPreventing Active Cyber Attacks\nPreventing active cyber attacks is an important part of maintaining a secure system or network. One of the most effective ways to do this is by using strong, unique passwords for each account. This makes it difficult for attackers to guess passwords and gain access to the system or network. Additionally, restricting access to sensitive information or resources can help prevent attackers from gaining access. Firewalls can also be used to restrict access by only allowing traffic from trusted sources. Monitoring the traffic on the network can help identify malicious activity and allow administrators to take action before an attack occurs. Regularly updating software is also important as it helps prevent attackers from exploiting known security vulnerabilities. Furthermore, two-factor authentication and changing default router credentials can make it harder for hackers to gain unauthorized access. Finally, using complex passwords that are unique for each account makes them harder for scammers to crack and prevents them from gaining access multiple accounts at once. By taking these steps, organizations can better protect themselves against active cyber attacks and maintain a secure system or network environment.\nIn conclusion, active attacks are a serious threat to any system or network. To protect against these threats, organizations and individuals should take the necessary steps to secure their networks and systems. This includes using strong passwords, restricting access, using firewalls, monitoring network traffic, regularly updating software and encrypting data. Additionally, understanding the fundamentals of network security such as access control and ransomware prevention is essential for keeping your IT systems safe from malicious actors. Finally, being aware of different types of cyberattacks can help you identify potential threats before they become a problem. By taking these precautions and staying vigilant about cybersecurity threats, organizations can reduce the risk of an active attack on their networks or systems.\nBe First to Comment']	['<urn:uuid:8c4407ff-50b2-48e7-9c41-2fac4866470b>', '<urn:uuid:e53087f8-afcf-4656-91b0-b8f0fc28a81f>']	factoid	direct	short-search-query	similar-to-document	multi-aspect	novice	2025-04-22T14:43:01.858830	6	46	1458
4	how many genetic crossovers barley cell	Researchers found 12-19 crossovers per cell (spread across all chromosomes) and that there were 1-3 crossovers per chromosome. But crossovers were 25 times more likely at the ends compared to the middle of chromosomes.	"['Sex is not just an activity confined to higher animals such as mammals and not just for reproduction. The changes in offspring that result during reproduction are integral to the continuing adaptation and evolution of millions of species of plants, fungi and other organisms throughout the food chain.\nWithout such genetic shuffling, life wouldn\'t have adapted to the planet\'s frequently changing climates, or to new pests, parasites and predators. Our environment is always changing and emerging pressures, such as predicted rises in temperature for example, could threaten food production systems by affecting overall yield – it has been estimated that a 1°C rise in night-time temperature could reduce rice yields by about 10%. Don\'t panic about your coffee, despite the doomsday scenarios you read in mainstream media, other species of coffee will be suited for whatever conditions arise, but what about other plants that have not been artificially selected by mankind for agriculture?\nControlling genetic shuffling could be our friend. And so researchers are taking a detailed look at the fundamental molecular processes by which organisms shuffle and exchange their genes. Looking at barley as a model for cereal food crops, their findings show how genetic exchange during sex can be affected by more subtle increases in temperature than previously recognized. It\'s a discovery that could impact fields ranging from agricultural development to advanced plant breeding techniques to community-scale climate and ecosystem modeling.\nPlant breeders in particular have been interested in how genetic variation is controlled ever since Gregor Mendel\'s classic experiments with peas in the late 1860s. If you don\'t understand how characteristics such as seed size or growth are inherited you can\'t breed varieties to exhibit certain traits or not to express them at all.\nFortunately, genetic variation through generations is a natural part of sexual reproduction in higher organisms (eukaryotes) such as fungi, plants and animals. At conception each individual receives a set of maternal and paternal chromosomes. Together this set of chromosome pairs makes up their genetic blueprint. During sexual reproduction DNA replicates on structures called chromosomes, which typically appear in pairs, and a process called meiosis ensures only one half of each chromosome pair is passed to their eggs or sperm cells. Subsequent fusion of a sperm and egg during fertilization restores a full set of chromosome pairs; this halving and re-pairing during meiosis prevents the number of chromosomes doubling each generation which would cause chaos in a cell.\nOne of the consequences of this system is that just as a deck of cards is split in two before being shuffled back together before a round of poker, each new generation – be it from a seed, spore or a fetus – receives a slightly different set of genes from their parents. This genetic variation arises because during meiosis, segments of the parental chromosome pairs reciprocally recombine to swap places in a process called crossing-over.\nAkin to the top half of the king of spades appearing with the bottom half of the queen of hearts, the creation of new stretches of DNA in this way adds a whole new dimension of variation into the mix – as it would in a game of poker if cards recombined in such a way.\nProfessor Chris Franklin and colleagues from the University of Birmingham, funded by the UK\'s Biotechnology and Biological Sciences Research Council, have been looking at the molecular processes behind this variation for over 15 years. Franklin says that although meiosis has been studied for more than a century, it was not until the late 1980s onwards when scientists started looking at yeast that we really began to understand the underlying biochemical processes.""\nBy then, it had been long-noted by plant breeders working that some plants didn\'t engage in the gene-swapping meiotic processes as others did. In the cereals in particular, certain chromosomal regions didn\'t crossover very much, it was assumed that these regions didn\'t have much in the way of genes and just contained \'junk DNA\'. Dr. Robbie Waugh\'s work at the Scottish Crop Research Institute showed in grass chromosomes that 30-50% of genes are not in these regions.\n""So then it became more of an issue because there is a very low frequency of recombination there compared to distal ends of chromosomes, potentially limiting genetic variability,"" says Franklin.\nNew genetic combinations are formed during meiosis. But regions where this happens are restricted in barley. Image: NIH\nTeaming up with colleagues at the James Hutton Institute (JHI), University of Dundee and at Aberystwyth University, who had a strong background in the genetics of the grass family, Franklin says the idea was to try to determine why crossing-over is mainly confined to the ends of chromosomes.\n""We thought it would be a good idea to apply the tools for we have developed to study meiosis in Arabidopsis to cereals,"" says Franklin, adding that barley was a good system as like most organisms their chromosomes are in pairs (diploid) rather than the complex hexaploid (three pairs) seen in bread-making wheat. As a food crop, findings in barley would likely be applicable to a host of other cereal grasses, including oat and rye for example.\nDetecting meiotic proteins with fluorescent antibodies that can be seen with light microscopes, coupled with techniques that allow the isolation of chromosomes during meiosis, Franklin and colleagues, James Higgins, Sue Armstrong and Ruth Perry were able to assemble, image by image, a complete picture of chromosomal recombination and crossing over during meiosis in barley in exquisite detail.\nOverall, researchers found 12-19 crossovers per cell (spread across all chromosomes) and that there were 1-3 crossovers per chromosome. But crossovers were 25 times more likely at the ends compared to the middle of chromosomes, which is very different to observations of recombination in brassica crops, such as cabbage and broccoli, and Arabidopsis.\nThe story that began to emerge was that the initiation of recombination does occur throughout the length of the chromosome – but it\'s all a matter of timing. The process begins two-to-three hours earlier at the end of chromosome (distal) than the interstitial (middle) and proximal regions found towards the centre of the chromosomes. ""Basically, it starts at the end and works to the middle.""\nMeiotic proteins ASY1 (red) and Zyp1 (green) in a barley nucleus. The skewed distribution of the short stretches of ZYP1 is due to recombination initiating earlier in the distal regions compared to interstitial and proximal chromosomal regions. Image: James Higgins\nWhat might be the origins of such a system? It could be due to the nature of the physical structures at play. Chromosomes are dynamic assemblages, and cluster near the membrane of the nucleus within which they are contained. Because the ends of chromosomes (telomeres) are tethered there, that could be where recombination initiates first.\nBut Franklin doesn\'t think that\'s the whole reason. Another aspect of recombination timing could be related to cycles in chromatin, which are the DNA-plus-protein structures on chromosomes that control gene expression and DNA replication, as well as storage and repair. Hence, these chromatin cycles are physical changes in mechanical forces along the chromosomes that are in some way related to the biochemical transitions taking place.\n""The problem is that you can\'t directly measure these mechanical forces,"" Franklin explains. ""The evidence in yeast is that you get cycles of [chromatin] expansion and contraction, and that the maximum expansion is coincident with some of the most important biochemical transitions in the recombination process."" Among these important biochemical transitions are the way that double-strand breaks in DNA can be repaired, so they end up as non-crossover regions.\nHaving found that the chromatin cycles are important in crossing over, the researchers then asked themselves if they could perturb them and affect the distribution of recombination events to get genetic mixing along more of the chromosome. To jolt the system, Franklin and colleagues used temperature because it\'s been known for a long time that extra heat during meiosis can affect the process but most studies used large increases in temperature, which completely disrupt meiosis. Only one late-1950\'s study in Tradescantia, a non-crop species, had shown that a subtle increase in temperature has an effect on crossover distribution.\nThis proved to be the case in barley. But to the team\'s surprise a subtle increase in temperature of around 5°C had no effect on the cycles and length of meiotic process. ""What did change was the timing of DNA replication before meiosis,"" says Franklin. Typically, gene-rich regions in the end regions are replicated earlier and DNA in the middle and centre of chromosomes is replicated later. ""We found with increased temperature that the differentiation of when DNA was replicated was blurred. In a sense we\'ve found the underlying basis to the earlier observation in Tradescantia, and as a consequence we see more crossovers in interstitial regions,"" he says.\nHow much more is hard to quantify as there is natural variation in crossing over frequency from chromosome to chromosome. On chromosome 5 for example, there is a 3.5-fold increase in cross over number in the interstitial areas. It\'s also noteworthy that increased temperature caused a significant drop in the average number of crossover events per cell, from 14.8 to 13.5. So higher temperatures could cause fewer recombination events to occur, which could slow reactions to environmental change, balanced by more crossovers in areas where it does not usually occur.\nAnd the subtlety of the temperature increase is pivotal. When meiosis fails, plants become infertile. Even an increase from 25°C to 30°C causes a significant reduction in fertility (ref 5). ""There\'s a potential environmental and food issue here in that you have climate change and with median temperature increases you could see a reduction in fertility because of a failure in meiosis,"" says Franklin.\nIn the short term, the work could help plant breeders to adapt new varieties. For instance, a technique called map-based cloning uses recombination events to move genetic markers to where a target gene might be. But if there are no (or few) crossing over events you can\'t infiltrate that part of the chromosome to pin it down.\nTemperature effect on chiasma (crossover) position in barley meiosis chromosomes (blue rings) at 22°C (left) and 30°C (right). In the diagram maternal and paternal chromosomes are shown in different shades of blue; crossovers are the black crosses and the centromeres are represented by triangles. At 30°C more crossovers are observed in the interstitial regions of the chromosomes making it possible to separate desirable from undesirable traits during plant breeding. Image: James Higgins\nSimilarly, when breeders make new varieties they often bring in desirable genes along with \'junk DNA\', so subsequent cross-breeds are needed to filter out all but the wanted traits. But if there are no crossovers in that region you can\'t breed them out. ""This is called linkage drag – where you drag in a whole lot of stuff you don\'t want,"" says Franklin. ""With more recombination you could break up this unwanted material. And the beauty of using heat-temperature is that it\'s dead simple and requires no major facilities.""\nThe next phase of the research is to use a range of temperatures rather than just the 30°C employed and to see how the different chromosomes respond. ""We don\'t know what the rules are and that\'s what we really want to find out,"" says Franklin. His team\'s microscopy work will also be complemented by collaboration with the JHI, and their expertise in genetic studies may yet pin down the rules of sex, reproduction and chromosomal recombination in many important food plants.\nPlant Sex Gets Hot']"	['<urn:uuid:5b16474a-c64a-4830-9b02-1bbc414cd98e>']	factoid	direct	short-search-query	distant-from-document	single-doc	novice	2025-04-22T14:43:01.858830	6	34	1925
5	I notice my colleagues always rush to reply to messages instantly, but I wonder about the real impact of these quick responses. What's the typical response time to communications and how does this affect work efficiency?	Studies have found that 70% of all emails are opened within 6 seconds of receipt. After checking an email, it takes an average of 64 seconds to resume the original task. When an email requires action outside the inbox, it takes over 9 minutes to return to the original task. Research shows that this constant task switching can cost as much as 40 percent of someone's productive time, with workers averaging only 3 minutes on any given task before switching.	['It’s probably safe to say we all spend longer than we’d like on email and instant messengers like Slack. They’re always there in the background, compelling us to click over for a “quick” check in. But these switches add up and fracture blocks of time where we’d otherwise be more focused.\nWhich made us ask: How much of your day is spent multitasking with communication tools? And just how bad is it for your productivity?\nBy looking at the anonymized data of close to 50,000 RescueTime users, we discovered a pattern of communication multitasking that was more severe than we had imagined.\n40% of your productive time at work is spent multitasking\nThere’s a ton of research showing that multitasking is bad for us. In fact, studies show it’s pretty much impossible. When we try to “multitask” our brains are actually just switching quickly back and forth between tasks. Instead of making us more efficient, multitasking ends up sapping our productivity, killing our focus, and adding to our stress level.\nHowever, most of us don’t consider the way we use email or instant messenger as multitasking.\nWe happily keep our inbox or Slack open all day while working on other projects and think nothing of it. But this is multitasking.\nAnd worse, it’s multitasking in a way that leaves us constantly open to interruptions and disruption. (I mean, you wouldn’t want your bus driver reading the newspaper while taking you to work, right?)\nWhen we looked at the time people spending ‘checking in’ on emails or instant messenger, we found that the average knowledge worker spends 40.1% of their productive time a day multitasking with communication tools.\nThis means, nearly half the time you spend on productive tasks (whether that’s writing or software development or design) is also spent multitasking with email and instant messengers.\nNow, why is this an issue? For one, we’ve become addicted to answering emails and notifications as quickly as possible, at the expense of other work.\nIn one study, Thomas W. Jackson of Loughborough University found 70% of all emails received were opened within 6 seconds of their receipt.\nOnce you check an email, it takes you an average of 64 seconds to resume your original task. Even worse, another study found that when an email involves doing something outside our inbox, it takes over 9 minutes to return to the original task.\nThe average knowledge worker only has 1h 12m of productive time a day without being interrupted by email and IM\nLooking at total time spent without communication multitasking, our research found the average knowledge worker only has 1 hour and 12 minutes a day for completely focused work.\nThat’s barely one hour a day (or 6 total hours a week) without the negative effects of multitasking with communication tools.\nWhile email and IM are necessities for doing most modern jobs, this number shows just why it can feel so hard to get meaningful work done every day.\nWant to know where your time is actually going? Sign up for RescueTime for free and get an accurate picture of how you spend time on your digital devices.\nHow communication multitasking is affecting our productivity\nWe can probably all agree that spending too long on email during the day isn’t great. In fact, when Gloria Mark of UC, Irvine, studied the effects of email on stress and productivity, she found:\n“The longer daily duration spent on email, the lower the assessed productivity and the higher the stress.”\nHowever, if just spending too long on communication tasks is making us more stressed and less productive, what about the compounding effects of multitasking and ‘checking in’ constantly?\nConstant switching fragments our workday and trains our mind out of focus mode\nWe might be used to bouncing between email and Google docs, Slack, and other tools. But every time you switch contexts like this to do a single task, you’re fragmenting your workday. And it’s taking a toll on your focus.\nIn one study, Gloria Mark found that, in general, workers average only 3 minutes on any given task before switching and about 2 minutes using any digital tool before switching.\nBy not working for long, uninterrupted periods, we’re effectively training our minds out of focus mode and making them more vulnerable to distraction.\nWe’re exposing ourselves to an unnecessary risk of interruption\nWhen you think of what gets in the way of focused work, you probably assume it’s some external interruption. However, Mark explains that we’re just as likely to interrupt ourselves as to be interrupted by an external source.\nWhen we constantly leave our communication tools open, we’re exposing ourselves to both internal and external interruptions.\nEven worse, Mark found that once interrupted, we rarely go right back to the task at hand. Instead, she found that not only do we engage in an average of two or more intervening activities, but it takes 23 minutes and 15 seconds before we go back to our original task.\nNow, think about what happens when you casually glance at your email.\nIf you’re lucky, all you’ll see are a few tasks you have to deal with at some point. However, you might also get a message that completely takes up your day and pushes you into a bad mood.\nAnd while it might feel like you need to urgently respond to every email that comes in, do you? (There’s a reason you can’t email 911.)\nConstantly multitasking with communication tools tricks us into thinking they’re the most important part of our day.\nJuggling tasks makes everything take longer\nOne of the ways psychologists measure the effects of multitasking is to compare how long it takes us to do a task uninterrupted versus when we try to “juggle” multiple ones at once.\nIt’s probably no surprise that multitasking slows us down. But the research around task switching shows something even more interesting: The more complex a task, the more time is lost.\nIn other words, when you multitask with communication tools—which often require time to understand context, a certain level of deep thought, and enough willpower and focus to get through it—you’re essentially hitting the brakes on your productivity.\nAccording to Dr. David Meyer, “even brief mental blocks created by shifting between tasks can cost as much as 40 percent of someone’s productive time.”\nCommunication tools are necessary. But it’s up to you to use them well.\nIt’s easy to downplay how much of an impact leaving communication tools open can have on your productivity.\nBut, as author and CEO Dorie Clark writes in Harvard Business Review, it’s the very nature of email and IM, not how time-consuming they are, that makes them so stressful:\n“Recognizing the downstream consequences and impact on one’s time is essential when evaluating your decision [to deal with an email].”\nOur research shows that the majority of us have a problem with multitasking communication tools. And while there’s no one-size-fits-all answer to when to use communication tools, it is your choice how you use them.\nThe next time you feel like jumping into email for “just a second,” take a moment to think about the true cost of that action.\nDo you think multitasking with email and communication tools is disrupting your workday? Let us know in the comments or on Twitter.']	['<urn:uuid:38e8bebd-175e-4002-890e-f414eb6004b6>']	open-ended	with-premise	verbose-and-natural	distant-from-document	single-doc	expert	2025-04-22T14:43:01.858830	36	80	1219
6	Where can you find natural habitats of SPS corals?	SPS corals' preferred habitats exist in areas like the islands of the Indian and Pacific Oceans, including places like Fiji, the Maldives, Hawaii or Papua New Guinea.	['Small-polyp stony (SPS) corals are among the most diverse and well-known types of corals found in the world’s oceans – they include many specific kinds of corals from the genera Pocillopora, Montipora and Stylophora. Their skeleton is slowly secreted by the epidermis found at the base of each polyp, and as in the case of LPS (large polyp stony) corals, some varieties can grow much faster than others. The beauty and diversity of SPS corals is maintained in stable habitats, where factors such as temperature, alkalinity and water PH are kept between specific values.\nLarge-Polyp Stony Corals vs. Small-Polyp Corals – The “Great” Divide\nAlthough there is a difference between small-polyp stony corals and LPS corals, there isn’t an actual, concrete distinction between the two. Rather, the transition is smooth, as there are many SPS coral species that have larger polyps, and can often be compared with LPS species. The general difference, however, has to do with the greater fragility and more specific habitat conditions that small-polyp corals may require in order to survive and propagate. In contrast with large-polyp stony corals, they cannot be found at depths where sunlight can’t reach them too easily, nor in waters that are too polluted, murky or cold.\nThe Beauty and Fragility of Small-Polyp Stony Corals\nThere are many beautiful SPS species that are commonly sought out by divers in tropical regions. These include anything from stunning “flowery” patterns like the ones found on Sunset Montipora, and the vibrant purple stem-shaped polyps of the Booberry Acropora, to incredibly exotic, intricate shapes and multicolored textures, such as in the case of the Bisma Worm Rock. Despite their beauty, SPS corals are quite fragile, being commonly attacked by creatures such as nudibranchs and various insects or flatworms that mainly attack Acropora corals. Also, they require specific habitat conditions, such as a temperature between 72 and 78 F, and moderate to intense sunlight.\nThe Preferred Habitats of these Stony Corals\nAside from stable temperature and lighting conditions that can usually only be found in the tropics, small-polyp corals also require several other important factors to be just right, before they can survive. The water alkalinity, for example, has to be between 8-12 dKH, while PH fluctuations of only 0.3 (ideal ranges are somewhere between 8.1 and 8.4) can be tolerated, before the corals begin to find it difficult to maintain their life cycles in the long run. Preferred habitats exist in areas like the islands of the Indian and Pacific Oceans, including places like Fiji, the Maldives, Hawaii or Papua New Guinea.\nPopular Small-Polyp Stony Coral Varieties\nThe most well-known varieties of SPS corals include primarily members of Acroporidae, including the Montipora and Acropora genera. These corals are most fascinating, not only due to their remarkable natural beauty and diversity, but also because they are the dominant reef building hard coral species of the world’s oceans. Although their diversity makes them extremely prevalent, like most small-polyp stony corals, these species still require specific conditions in order to thrive and survive, which is part of the reason why they are rarely able to live in deeper waters.']	['<urn:uuid:0188cd88-89b3-4ca7-996c-d2270ed15854>']	factoid	direct	concise-and-natural	distant-from-document	single-doc	expert	2025-04-22T14:43:01.858830	9	27	520
7	I live in Europe and never traveled abroad. Can I still get hepatitis?	Yes, you can get hepatitis E even without traveling. Sporadic and autochthonous cases of HEV infection in developed countries, including Europe, were reported among patients who had no history of travelling to hepatitis E endemic countries. HEV is currently considered as an emerging pathogen in these developed areas.	['Hepatitis E virus (HEV) represents one of the major cause of acute viral hepatitis worldwide (1), according to the WHO, one-third of the world population has been exposed to HEV (2). In Europe, there is extensive evidence indicating that HEV is prevalently transmitted by the ingestion of pork and wild boar raw meat (3), however, other sources of infection such as blood transfusion and organ transplantations have also been reported (4,5).\nSporadic and autochthonous cases of HEV infection in developed countries, including Europe, were reported among the patients who had no history of travelling to hepatitis E endemic countries, therefore HEV is currently considered as an emerging pathogen in these developed areas (6). HEV seroprevalence shows significant variability in different European regions, ranging from 0,03% to 52% among the general and blood donors (BDs) populations (7,8). In Italy, data about HEV prevalence in general population and BDs are scattered and variably available (9-14). A study published in 2016 reported a seroprevalence of 49% among BDs in an area close to the one included in this study (8). The anti HEV antibody seroprevalence varies greatly depending on the area studied and the tests used. This study aimed to assess the HEV infection prevalence among BDs living in the Grater Romagna area, in North-Eastern Italy. The Greater Romagna area includes the provinces of Forlì-Cesena, Ravenna and Rimini with a total of 32,000 BDs. Moreover, using different serological test systems significantly influences the estimates of seroprevalence (15). A secondary goal of this study was to evaluate the variability of seroprevalence results obtained on the same group of patients when different serological methods are used.\nMaterials and methods\nAll serum samples were at first screened for anti-HEV immunoglobulin G (IgG) antibodies by a commercial enzyme-linked immunosorbent assay (ELISA) (DIA.PRO Diagnostic Bioprobes, Sesto San Giovanni (MI), Italy), used in our diagnostic routine and so considered as the reference test. Then, all serum samples were re-tested for anti-HEV IgG antibodies by the other two commercial ELISA (Wantai, Biologic Pharmacy Enterprise, Beijing, China; Euroimmun, Lübeck, Germany). The obtained results were used to calculate the agreement between the different tests. All the samples that resulted positive and borderline by anyone of the three ELISA methods used were confirmed by an IgG immunoblotting (IB) assay (recomLine HEV IgG/IgM Mikrogen Diagnostik, Germany), which is currently used as a confirmatory test in our diagnostic routine. The seroprevalence rate was calculated based on the IB confirmed test results.\nThe sample size is representative of the BDs population in this area, and the sample sizes were calculated considering the expected prevalence of HEV infection. Samples were randomly daily collected among BDs screened by the Blood Transfusion Unit and tested for the aim of this study by the Unit of Microbiology of the Greater Romagna Hub Laboratory, Pievesestina, Italy. The prevalence of anti-HEV-IgG antibodies was calculated for each one of the screening tests used, as well as the percentages of sensitivity and specificity were calculated (Table 1). The proportion of agreement between the different ELISA tests was detected by using Cohen’s Kappa coefficient that assumes values between -1 (no agreement) and 1 (complete agreement). Statistical analysis was performed using Stata software version 14.2 (Stata Corp., College Station, TX, USA). Statistical significance was set as p<0.05.\nFrom May 1st to July 31st 2016, 90 serum samples were collected from Forlì-Cesena BDs, 300 from Ravenna BDs and 110 from Rimini BDs, for a total of 500 BDs (78% male, age: 18-82 years, median 43 years). In 22 out of 500 (4%) serum samples tested, anti-HEV IgG was found to be positive by DIA.PRO, and 19 of these 22 positive samples were further confirmed by IB test. When the 500 serum samples were tested by the Euroimmun assay, in 11 Anti-HEV IgG was found to be positive (2%), and Eight out of these 11 IgG positive specimens were confirmed by IB. By using Wantai method, in 17 samples out of 500 (3.4%), Anti-HEV IgG was found to be positive, and 14 IgG positive samples were confirmed by IB.\nThe eight positive samples obtained by the Euroimmune method and confirmed with IB, also the 14 samples obtained with Wantai and confirmed with IB were all included in the 19 specimens initially selected by DIA PRO (with IB confirmation) (Figure 1). The overall anti-HEV IgG prevalence was 3.8% (CI: 2.303 – 5.871), 1.6% (CI: 0.693 – 3.128) and 2.8% (CI: 1.539 – 4.653) when samples were tested with DIA.PRO, Euroimmun and Wantai methods, respectively. Table 1 summarizes the different values of seroprevalence calculated for individual serologic tests before and after the IB confirmatory testing.\nFurthermore, the comparison between the DIA.PRO method (arbitrarily selected as the reference being the one in routine use in our laboratory) versus the Euroimmun, and the DIA.PRO versus the Wantai assays showed a concordance of 98% and 99%, respectively.\nThis study aimed to describe the IgG seroprevalence against HEV infection in a BDs population living in the Greater Romagna area: the overall percentage of seroprevalence was 3.8%. This value has been obtained with the diagnostic algorithm routinely used in our laboratory (based on DIA.PRO EIA test followed by IB confirmation). However, the rates of seroprevalence in this BDs population calculated as above reported for each one of the methods used in this study are quite close, ranging from 3.8% and 1.6% (Table 1).\nThe seroprevalence values found in this study were similar or even lower than those already published among other different Italian BDs population [9,11,13]. A recent study performed among Italian BDs reported an overall IgG-HEV prevalence of 8.7% showing this as the lowest seroprevalence rate reported among BDs population in Europe. In particular, in the Emilia Romagna region, it has been reported an overall IgG seroprevalence of 3.6%, similarly to that values found in our study (that included only the Romagna area of the Emilia-Romagna region) by using the Wantai method . Nevertheless, a work published in 2016 describing the HEV seroprevalence in a quite geographically close area to the one included in this report (the Abruzzo region, some 250 km South of the studied zone) showed anti-HEV antibodies’ overall prevalence of 49% among an unselected population of BDs . As far as the detection of anti-HEV IgG is concerned, the results obtained in this study by the different commercially available ELISA tests are similar. As previously reported, the use of a confirmatory test based on the IB method does not substantially modify the seroprevalence values . The results obtained are in strong disagreement with those reported in other studies that considered Wantai assay as the reference method for the investigation of the HEV human seroprevalence. Most of the studies using the Wantai test are compared and analysed in the publication from Hartl and co-workers: this work showed higher seroprevalence values obtained by using this Chinese test when compared to other serological methods .\nEpidemiological studies in the last few years have demonstrated that HEV infection should be considered as an emerging zoonosis. However, data about the human seroprevalence are extremely variable and largely depending on many issues, including the geographical area, population studied, and, in particular, on the serological assay used (17). Moreover, the public health implications of the HEV infection in Europe have changed because of the increasing numbers of cases and the recent report of chronic, persistent HEV infection associated with progression to cirrhosis in immune-suppressed patients (18).\nIn conclusion, the data obtained in this study and the overall trend of HEV infection in Europe and Italy underline the need for achieving a harmonised testing algorithm that is necessary in order to be able to compare data from different studies. We suggest that the identification of potential HEV human infection should be added to the standard laboratory work-up for viral hepatitis.']	['<urn:uuid:74397257-bd55-4e22-9a82-b8fa5267f31a>']	open-ended	with-premise	concise-and-natural	distant-from-document	single-doc	novice	2025-04-22T14:43:01.858830	13	48	1291
8	total funding amount digital health startups ryse	The digital health startups received a total funding of £4.5 million through investments from RYSE Asset Management and other investors.	['RYSE provides major funding boost for digital health start-ups\nPioneering digital health start-ups receive major funding boost to transform COVID-19 patient care\nAt a time when investment and funding prospects for the health tech and life science industry have dried up, and some investors have reneged on deals due to COVID-19: LiveSmart, Knok Healthcare, Log My Care, Firza, and MediShout are among the few start-ups to receive funding.\nPatients will benefit from enhanced care delivery thanks to innovations made possible by investments from RYSE Asset Management and other investors. These companies have raised a total of £4.5 million.\nBusy clinical working environments are faced with logistical issues that can create delays and increase the cost of care delivery. This has been particularly apparent during the pandemic where efficient redeployment of resources is paramount.\nVirtual consultations, workforce optimisation solutions for primary healthcare providers, platforms for digital virus tracking and care home digitalisation, and reporting solutions for logistical and medical supply issues for helpdesk prioritisation are just some of the innovations to receive investment.\nThe innovations and some of the jobs created include:\n- Firza is on a mission to reduce the overbearing workload on primary healthcare by using a centralised workforce with Robotic Process Automation (RPA) and Artificial Intelligence (AI) technology to emulate the actions of a human interacting with healthcare systems to optimise medication management. The company plans to employ an additional 45 people over the next year.\n- Knok Healthcare, a global telehealth and SaaS start-up, has an integrated solution for remote consultations through a combination of AI triage, scheduling, video-consultation, health records, and integration with hospitals and clinics. The company plans to employ an additional 19 people over the next year.\n- Log My Care has developed care management software and offers a free COVID-19 symptom tracking tool to support care homes in overcoming challenges in early symptom detection in this high risk population. The company plans to employ an additional 6 people over the next year.\n- MediShout has developed communication software that allows clinical staff to flag non clinical issues such as out of service facilities and reduced levels of PPE. This has allowed key healthcare workers to focus on delivering patient care. The company plans to employ an additional 15 people over the next year.\n- LiveSmart provides health assessments and reacted quickly to the new situation. The team sourced and supplied antibody tests via their well-established platform; supporting both small businesses and large corporate clients. The company plans to employ an additional 36 people over the next year.\nAll of the companies have been awarded seed funding as a result of their successful applications to the RYSE digital health funding programme. The programme invited early stage digital health and medtech companies to apply for funding with the opportunity to work alongside RYSE, DigitalHealth.London (DH.L), and MedCity.\nAshish Kalraiya, CEO at MediShout, said: “We are proud of our ability to innovate in direct response to the needs on the frontline, and during the COVID-19 pandemic it has been critical to ensure that the needs of patients affected by conditions other than COVID-19 continue to be met. The connections we have forged through working with RYSE, MedCity, and DigitalHealth.London mean we can devise creative solutions we know will best meet the needs of the NHS staff and patients. RYSE’s funding has transformed our innovation into a reality that benefits patients and our crucial NHS staff.”\nClaudio D’Angelo, Managing Partner at RYSE Asset Management LLP, said: “We are delighted to have seeded much-needed investment in these health tech start-ups who are transforming patient care through innovation. We are well aware of the funding gap for early-stage digital health innovations, even outside of COVID-19, and are passionate about plugging the gap to support the growth of this industry – and, critically, through public-private collaborations as we know this will have the greatest impact. The innovations developed with our funding demonstrate the huge return on investment investors stand to gain when they fund health tech start-ups.”\nSarah Haywood, Executive Director of MedCity, said: “Securing funding for early-stage health tech companies has always been a challenge; thrown into the unprecedented times of COVID-19 the challenge is even greater, particularly for companies not eligible for the Government’s Future Fund and for companies raising money for the first time. Now is innovation’s time to shine – demonstrating how it can meet patients’ needs as we unite across the globe to tackle this deadly virus.\n“We are immensely grateful to RYSE for their generous investment and for their recognition of the critical role these companies play. The innovations coming out of our partnership with RYSE and DigitalHealth.London bear testament to the profound contribution health tech start-ups make to our health systems and patients, and to what we stand to lose if the UK’s digital health sector is not supported to thrive. We also recognise the important role that early stage investors, like RYSE, play in supporting companies beyond investment. Mentoring, advice, guidance, and connections are equally important to the success of our next generation of innovators.”\nAnna King, Commercial Director, Health Innovation Network, a founder partner of DigitalHealth.London said: “We set up DigitalHealth.London to help encourage and support the development of digital health businesses working with the NHS. Investment from RYSE in these high potential companies has been vital to enable them to develop high quality products that meet patient needs in the NHS and internationally. We are delighted to see investors like RYSE supporting more companies in this exciting sector.”\nRYSE Asset Management LLP (RYSE), is a London based FCA authorised and regulated investment manager, tailored for high net worth individuals, family offices, and institutions. In partnership with DigitalHealth.London (DH.L) and MedCity to identify, support, and invest in early-stage digital health and MedTech companies, to commercialise and scale within the NHS and other healthcare delivery systems.\n- LiveSmart (health assessments): A PHE approved #COVID19 antibody test (Abbott).\n- Knok healthcare (virtual consultations): A check-up from the comfort of home provides social impact and safety.\n- Log my Care (care management): A free symptom tracking tool to help care homes detect early symptoms of #COVID19.\n- Firza health (medication management): Delivering technology and digital services for modern day primary care.\n- MediShout (healthcare reporting): Improving healthcare reporting logistics and supply chain issues during #COVID19 – improving the quality of care and reducing error.\nIn 20018, we also invested in:\nSkin Analytics (AI cancer screening): A new skin cancer community assessment service to reduce delays during #COVID19.\nDigitalHealth.London is helping health tech entrepreneurs and healthcare professionals turn the idea of digital innovation into tangible improvements for staff and patients.\nTheir iniatives bring together health and care stakeholders across London to collaborate and solve some of the biggest challenges in the NHS:\nDigital Pioneer Fellowship: The NHS Digital Pioneer Fellowship supports up to 30 digital change makers employed by NHS organisations in London to design and lead transformation projects underpinned by digital innovation.\nDigitalHealth.London accelerator: Each year, the Accelerator programme works with up to 20 high potential SMEs, giving bespoke support and advice, a programme of expert-led workshops and events, and brokering meaningful connections between innovators and NHS organisations with specific challenges']	['<urn:uuid:5d0272f0-6be7-481e-b33e-b9bf6c4fbd8f>']	open-ended	direct	short-search-query	similar-to-document	single-doc	expert	2025-04-22T14:43:01.858830	7	20	1197
9	I've been wondering about bike maintenance since my tires keep going flat even though I barely use my bicycle. Why does this happen?	Even bicycles that don't get used frequently end up with flat tires because pressurized air naturally tries to escape. The air moves through tiny microscopic pores in the rubber tires and through the valve stem used to fill them. Since the pressurized air always moves to areas with lower pressure, it gradually escapes even without wear-and-tear from riding.	['Comets are solar system bodies that orbit the Sun, just as planets do, except a comet usually has a very elongated orbit. Part of its orbit is very, very far from the Sun and part is quite close to the Sun. They are sometimes nicknamed dirty “cosmic snowballs,” because they are small, irregularly shaped chunks of rock, various ices, and dust.\nAs the comet gets closer to the Sun, some of the ice starts to melt and boil off, along with particles of dust. These particles and gases make a cloud around the nucleus, called a coma. The coma is lit by the Sun. The sunlight also pushes this material into the brightly lit “tail” of the comet.\nEarly bicycle tires were made out of solid rubber. (Before that, iron covered the edges of wooden bicycle wheels.) Solid rubber tires made bicycling a bumpy experience because they were unable to provide any cushioning on rough roads. When the airfilled rubber bicycle tire was invented, it made riding a lot more comfortable. But along with the comfort of air-filled tires came the frequent task of filling them up. The rubber that is used to make bicycle tires is thin and porous, which means that it has tiny microscopic pores, or holes, through which air can escape over time.\nAir that is pumped into bicycle tires is pressurized, meaning it is compressed into a much smaller space than it would ordinarily occupy. Without pressurized air inside, a bicycle tire would not have its firm shape. Air under high pressure, like all gases, moves or migrates to surrounding areas that have lower pressure, traveling even through fairly solid materials. Air in a bicycle tire naturally tries to escape through the valve stem that is used to fill it and the inner tube that holds it. So even bicycles that don’t undergo the wear-and-tear of frequent use eventually end up with flat tires.\nToday’s computers contain millions of transistors placed in a tiny piece of sili- 162 con, some so tiny that they can fit in an ant’s mouth. The transistors (devices that control the flow of electric current) are packed and interconnected in layers beneath the surface of the chip, which is used to make electrical connections to other devices. There is a grid of thin metallic wires on the surface of the chip.\nThis silicon chip was independently co-invented by two American electrical engineers, Jack Kilby and Robert Noyce, in 1958–1959. The chip, along with the invention of the microprocessor, allowed computers to get smaller and more efficient. Silicon chips are also used in calculators, microwave ovens, automobile radios, and video cassette recorders (VCRs).\nThe surface of a liquid is the seat of a special force as a result of which molecules on the surface are bound together to form something like a stretched membrane. They tend to compress the molecules below to the smallest possible volume, which causes the drop to take a round shape as for a given mass he sphere has minimum volume.\nIn the early days of theatre, the players were lit by gas lamps hidden across the front of the stage. Early in the twentieth century, it was discovered that if a stick of lime was added to the gas, the light became more intense, and so they began to use the “limelight” to illuminate the spot on stage where the most important part of the play took place. Later called the “spotlight,” the “limelight” was where all actors fought to be.\nYes and no. Bones are hard connective tissue, made up of bone cells, fat cells, and blood vessels, as well as nonliving materials, including water and minerals. Some bones have a very hard, heavy outer layer made out of compact bone. Under this layer is a lighter layer called spongy bone, which is located inside the end, or head, of a long bone.\nSpongy bone is tough and hard, but light, because it has lots of irregularly-shaped sheets and spikes of bone (called trabeculae) that make it porous (full of tiny holes). The soft, jelly-like inner core of bone is called the bone marrow. It is where red blood cells, certain white blood cells, and blood platelets are formed. The jawbone is the hardest bone in your body. Although bones are hard, they are not the hardest substance in the human body: the enamel on your teeth is harder.\nChocolate contains a range of nutrients which include minerals such as potassium, calcium and iron. It also contains the B-vitamin riboflavin. It is true that most of chocolates’ calories do come from fat but the ingredient, known as cocoa butter, is the kind of fat that consists mostly of monounsaturated fatty acid also found in olive oil; the ‘healthy’ fat needed in all diets.\nAlthough studies are constantly being done with chocolate (and why not?), studies in the past have shown two significant additional positives. In these test studies, the people who consumed cocoa regularly had a lower blood pressure than those that did not, were less likely to die from cardiovascular disease and had better peripheral blood flow.\nDark chocolate has the potential to have the largest quantity of cocoa solids – at least to 70%. This means that 70% of the chocolate is from the cocoa bean and less from added sugars, oils and perhaps other fillers. Thus the antioxidants in the dark chocolate surpasses pecans (14% less) and red wine (25% less).\nBesides the wonderful benefits from our all natural chocolate, nuts are a great food. In general, nuts are loaded with protein. Peanuts have the most, followed by almonds, cashews and walnuts. Protein is essential for healthy brain and muscle function, and for vegetarians, are a great substitute for animal protein. Nuts also contain omega-3 fatty acids, antioxidants and fiber. Thus with all the added benefits of chocolate, chocolate dipped nuts are a great snack.\nGuion S. Bluford Jr. became the first African American to fly in space during the space shuttle Challenger mission STS-8, which took place from August 30 to September 5, 1983.\nHe returned to space again in 1985 aboard Challenger mission STA-61-A/Spacelab D1. Mae C. Jemison became the first African American woman in space on September 12, 1992, when she flew aboard the space 18 shuttle Endeavour mission Spacelab-J.\nGuglielmo Marconi, of Bologna, Italy, was the first to prove that radio signals could be sent over long distances. Radio is the radiation and detection of signals spread through space as electromagnetic waves to convey information.\nIt was first called wireless telegraphy because it duplicated the effect of telegraphy without using wires. On December 21, 1901, Marconi successfully sent Morse code signals from Newfoundland to England.\nIf there is “blackmail” then there must be “white mail.” Mail was a Scottish word for rent or tax, and during the reign of James I, taxes or mail were paid in silver, which, because of its colour, was called “white mail.” During the sixteenth and seventeenth centuries, bandits along the Scottish border demanded protection money from the farmers. Because black signified evil, this cruel extortion was called a black tax, or “blackmail.”']	['<urn:uuid:1caede63-7f07-44b2-be22-f147b8179d1a>']	factoid	with-premise	verbose-and-natural	similar-to-document	single-doc	novice	2025-04-22T14:43:01.858830	23	58	1190
10	How does Theatre 95's integration with urban space compare to modern sustainable design principles for community engagement and site planning?	Theatre 95's design emphasizes urban integration through a semi-public hall-atrium and the Fil D'Ariane footpath, creating a porous space for journeys, encounters, and everyday life within the town. This approach aligns with sustainable design principles that promote community engagement through features like bicycle storage areas, carpooling encouragement, and public transportation incentives. The design also considers site planning aspects by respecting existing terrain, incorporating indigenous landscaping, and creating balanced site contours, all while maintaining the building's role as a cultural centerpiece in the urban environment.	['Sumit Singhal loves modern architecture. He comes from a family of builders who have built more than 20 projects in the last ten years near Delhi in India. He has recently started writing about the architectural projects that catch his imagination.\nTheatre 95 in Cergy-Pontoise, France by gpaa\nJuly 29th, 2015 by Sumit Singhal\nArticle source: GPAA\nThe challenges of the project\nThe contractor’s definition of the project is articulated around a close link between reshaping the Théâtre 95, outlining an urban morphology, and finding free spaces in the urban island.The theatre’s extension is framed by a dialectic vision of interactions between town and theatre: the theatre stretches beyond its limits while the town must find in it a porous space, a space of journeys, echoes, dreams, encounters and everyday life, devoted to contemporary modes of expression and deliberately placed at the very heart of the town.\nA peculiar face-off:\nThe Théâtre 95, opposite the prefecture, is housed in the third building to be built in the new town which arose in the 1970s. This emblem of the town’s history was once the home of Cergy Pontoise school of architecture and urban planning before becoming an arts school and then being transformed into a theatre.\nThe extension is a complex project which is framed by more general considerations about emerging social and urban developments and cultural practices. The aim is to invite the wider public to discover new strategies to reinvent the town.\nThe building’s pleated roof is the first component which strikes the visitor’s eye: this is the outline which the extension has borrowed to link old and new. The connection consists in a “semi public” hall-atrium, which follows the “Fil D’Ariane” – a public footpath which winds its way without interruption from the South-East to the North-West of the town, and is thus “integrated” into the building.\nThe existing pleated outline of the roof is continued in the hall-atrium volume, where it transforms into juxtaposed strips which create shafts of light entering the hall.\nThe pleats are also echoed in the new auditorium, facing South, creating a new rhythm which emphasizes the choice of erecting the new structure out of line with Cergy’s traditional orthogonal grid. The pleated outline has become the “crown” which is found in the volume of the new auditorium.\nThe new volume rises in an almost baroque posture, as if in confrontation with what is already there: the existing building conserves its identity, the atrium linking it to the new, setting up a “face-off” relationship between two visions which mix, stand in opposition and join together in a boldly chaotic statement.\nThe new extension will house a “flexible” 400 seat auditorium: the volume, which includes stage and technical areas, is blind, and covered with golden scales which bring light to a fairly colourless urban environment.\nThe choice of materials is based on a certain number of considerations which include questions of maintenance, longevity, environment, energy efficiency, but also appearance and style. Copper offers a response to all these questions.\nA performance space is also a strong presence at the heart of a town: it is a playful space, designed for culture and leisure which, like a magic lantern, must shine and draw all eyes towards it. Thus the envelope is made up of smooth, diamond-shaped scales, made from a copper aluminium alloy lending them a golden shade which will fade very little over time, and also contributing to lighting the hall-atrium. Within the auditorium, the space becomes more technical: Shaped as a cube to which the lighting and sound box, storage areas and dressing rooms are attached, it is equipped with retractable seating on 4 sides, making any seating layout possible, and a technical grid tailored to the dimensions of the auditorium.\nThe audience enters via a footbridge on the first level. These technical characteristics are supplemented by details which create a welcoming and baroque atmosphere: the wooden acoustic panels are carved with motifs inspired by the orchard behind the theatre, the wrought iron balustrades hark back to theatres of old, like the iron-work of Parisian balconies, the deep purple hues declined over the walls and seating which are lightened with touches of red here and there.', 'Sustainability is the process applied to our quest to sustain economic growth while maintaining our long-term environmental health. Sustainability means designing structures that take advantage of technological advancements to create eco-friendly products. Inert-gas-filled insulated windows, engineered-wood products made from scrap wood shavings, sawdust and assorted wood fibers, and thermal break window frames that keep the cold and hot air out are all examples of sustainable products. These products provide the owner and occupants with the following benefits:\n- Reduced maintenance and replacement costs over the life of the building\n- Energy conservation\n- Improved occupant health\n- Productive working environment\nSustainable products incorporated into the building should follow these selection guidelines:\n- Recycled content\n- Natural, plentiful, or renewable materials\n- Products manufactured by a resource-efficient process\n- Locally available products\n- Salvaged, refurbished, or remanufactured products\n- Reusable or recyclable products\n- Durable products\nUsing sustainable materials can also improve the indoor working environment and save money. Consider these advantages:\n- Materials that emit few or no carcinogens or irritants, as demonstrated by the manufacturer’s long-term testing results\n- Minimal chemical emissions from volatile organic compounds (VOC) that out-gas (continue to emit chemical vapors after installation)\n- Moisture-resistant materials that are not easily susceptible to mold growth\n- Materials that are easily maintained and require simple nontoxic cleaners\n- Equipment systems that promote healthy indoor air quality (IAQ) by identifying indoor air pollutants\n- Products and systems that help reduce water consumption\nThe sustainable approach to design would include requirements to do the following:\n- Simplify construction details.\n- Utilize repetitive details and components.\n- Standardize design components.\n- Incorporate accurate dimensions in the design, as some product and material sizes many have been reduced.\n- Simplify building systems so future expansion projects can take advantage of simplified designs or components.\n- Consider occupant safety and worker productivity gains in the new design.\n- Investigate more efficient and environmentally sensitive ways to bring underground utilities into the site with the least disruption to the existing terrain.\n- Consider other ways of disposing of site drainage onsite rather than offsite.\n- Adjust new site contours to provide for a balanced site where no offsite fill or off-site disposal of surplus soils is required.\n- Optimize dimensions to utilize a standard product size.\n- Minimize plumbing pipe and HVAC ductwork bends to reduce liquid and air friction.\n- Select fittings and fasteners that permit quick assembly.\n- Select sealants with the least environmental impact and longest life.\n- Investigate ways to accumulate salvaged and waste materials for recycling.\n- Consider donating surplus materials to a nonprofit organization such as Habitat for Humanity.\n- Deconstruct all existing structures with substantial recoverable materials and dispose of them to recyclers.\nWhen designing a new green structure, a number of goals must be set. The site must meet or exceed standards for sedimentation control and erosion:\n- Prevent the loss of soil during excavation and construction due to surface water drainage; keep dust down and cover large stored piles of earth to prevent wind erosion.\n- Prevent the silting up of existing storm drains in the immediate area by constructing erosion and silt fence enclosures around areas to be excavated.\n- Prevent the siltation of existing nearby streams or waterways by installing erosion and silt fencing around those streams adjacent to areas to be excavated.\n- Protect topsoil piles for reuse. (Topsoil piles are generated early in the construction process as soil is stripped during rough grading operations; respreading is one of the last operations, commencing as landscaping is put in place.)\nThe site utilities should reduce soil erosion during excavation of trenches and contain storm water runoff:\n- Plan infiltration swales and basins during trenching operations to contain surface water.\n- Retain or recharge existing water tables by minimizing site disturbances; leave as many trees as possible; use existing vegetation and retain natural contours.\n- Consider a design to store roof runoff when the building has been completed; it can be used as gray water or reclaimed wastewater.\n- Investigate a small onsite, state-of-the-art treatment plant to recycle reclaimed water.\nAn open-space and landscaping program can accommodate the following:\n- Protect trees during construction; they enhance property values and lower cooling loads.\n- Consider indigenous landscaping; it supports natural wildlife and plantings and lowers the level of irrigation as well as the need to fertilize and apply chemical treatment.\n- Minimize pesticide use by installing weed cloth; use mulches and planting species that create dense planting beds.\nSite -circulation and transportation programs should meet these objectives:\n- Encourage carpooling.\n- Provide areas for people to store bicycles during working hours.\n- Encourage the use of public transportation by instituting a program of incentives.']	['<urn:uuid:ca709921-73b8-4c4b-9777-c55044ef781c>', '<urn:uuid:87dc776d-1577-43c5-924a-15b33db12340>']	factoid	direct	verbose-and-natural	similar-to-document	multi-aspect	expert	2025-04-22T14:43:01.858830	20	84	1492
11	How do scientists detect toxic materials in water, and what are the health effects?	Scientists now use fluorescence spectroscopy combined with machine learning to quickly detect toxic materials like naphthenic acids in water. This method processes data through a convolutional neural network to achieve highly accurate results, though it can only detect fluorescent toxins. Regarding health effects, when contaminants like copper exceed safe levels (above 1.3 parts per million), they can cause short-term gastrointestinal problems including nausea and vomiting, and long-term exposure could lead to liver or kidney damage. People with Wilson's Disease are particularly sensitive to copper contamination.	['Toxic Materials Detected in Water With Help of Machine Learning\nIdentifying toxic materials in water with machine learning\nNew UBCO research can quickly identify toxins from oil sands and tailing ponds\nUBCO researchers are using fluorescence spectroscopy to quickly detect key toxins in tailing ponds water.\nWaste materials from oil sands extraction, stored in tailings ponds, can pose a risk to the natural habitat and neighbouring communities when they leach into groundwater and surface ecosystems.\nUntil now, the challenge for the oil sands industry is that the proper analysis of toxic waste materials has been difficult to achieve without complex and lengthy testing. And there’s a backlog. For example, in Alberta alone, there are an estimated 1.4 billion cubic metres of fluid tailings, explains Nicolás Peleato, an Assistant Professor of Civil Engineering at UBC Okanagan\nHis team of researchers at UBCO’s School of Engineering has uncovered a new, faster and more reliable, method of analyzing these samples. It’s the first step, says Dr. Peleato, but the results look promising.\n“Current methods require the use of expensive equipment and it can take days or weeks to get results,” he adds. “There is a need for a low-cost method to monitor these waters more frequently as a way to protect public and aquatic ecosystems.”\nAlong with master’s student María Claudia Rincón Remolina, the researchers used fluorescence spectroscopy to quickly detect key toxins in the water. They also ran the results through a modelling program that accurately predicts the composition of the water.\nThe composition can be used as a benchmark for further testing of other samples, Rincón explains. The researchers are using a convolutional neural network that processes data in a grid-like topology, such as an image. It’s similar, she says, to the type of modelling used for classifying hard to identify fingerprints, facial recognition and even self-driving cars.\n“The modelling takes into account variability in the background of the water quality and can separate hard to detect signals, and as a result it can achieve highly accurate results,” says Rincón.\nThe research looked at a mixture of organic compounds that are toxic, including naphthenic acids—which can be found in many petroleum sources. By using high-dimensional fluorescence, the researchers can identify most types of organic matter.\n“The modelling method searches for key materials, and maps out the sample’s composition,” explains Peleato. “The results of the initial sample analysis are then processed through powerful image processing models to accurately determine comprehensive results.”\nWhile results to date are encouraging, both Rincón and Dr. Peleato caution the technique needs to be further evaluated at a larger scale—at which point there may be potential to incorporate screening of additional toxins.\nPeleato explains this potential screening tool is the first step, but it does have some limitations since not all toxins or naphthenic acids can be detected—only those that are fluorescent. And the technology will have to be scaled up for future, more in-depth testing.\nWhile it will not replace current analytical methods that are more accurate, Dr. Peleato says this approach will allow the oil sands industry to accurately screen and treat its waste materials. This is a necessary step to continue to meet the Canadian Council of Ministers of the Environment standards and guidelines.\nThe research appears in the Journal of Hazardous Materials , and is funded by the Natural Sciences and Engineering Research Council of Canada Discovery Grant program.', 'The third post in our series highlighting some of the contaminants that can be found in water wells. Northeast Water Wells is available to collect samples and test your well water for contaminants anytime.\nIf you have a private well, regular water quality testing is very important. Northeast Water Wells recommends testing your well at least every two years. Many contaminants cannot be identified by taste or odor, making it difficult for homeowners to know if the water quality of their well has changed.\nWhat is copper?\nCopper is a reddish metal that occurs naturally in rock, soil, plants, animals, water, and sediment. Since copper is easily shaped or molded, it is commonly used to make coins, electrical wiring, and household plumbing materials. Copper compounds are also used as agricultural pesticides and to control algae in lakes and reservoirs. All living organisms including humans need copper to survive; therefore a trace of copper in our diet is necessary for good health. However, some forms of copper or excess amounts can also cause health problems.\nHow does copper get into drinking water?\nThe level of copper in surface and groundwater is generally very low. High levels of copper may get into the environment through mining, farming, manufacturing operations, and municipal or industrial waste water releases into rivers and lakes. Copper can get into drinking water either by directly contaminating well water or through corrosion of copper pipes if your water is acidic. Corrosion of pipes is by far the greatest cause for concern.\nWhy is Copper regulated?\nIn 1974, Congress passed the Safe Drinking Water Act. This law requires EPA to determine safe levels of chemicals in drinking water which do or may cause health problems. These non-enforceable levels, based solely on possible health risks and exposure, are called Maximum Contaminant Level Goals. The MCLG for copper has been set at 1.3 parts per million (ppm) because EPA believes this level of protection would not cause any of the potential health problems described below.\nSince copper contamination generally occurs from corrosion of household copper pipes, it cannot be directly detected or removed by the water system. Instead, EPA is requiring water systems to control the corrosiveness of their water if the level of copper at home taps exceeds an Action Level. The Action Level for copper has also been set at 1.3 ppm because EPA believes, given present technology and resources, this is the lowest level to which water systems can reasonably be required to control this contaminant should it occur in drinking water at their customers home taps.\nWhat are the health effects of excess copper in drinking water?\nCopper is an essential nutrient, required by the body in very small amounts. However, the EPA has found copper to potentially cause the following health effects when people are exposed to it at levels above the Action Level. Short periods of exposure can cause gastrointestinal disturbance, including nausea and vomiting. Use of water that exceeds the Action Level over many years could cause liver or kidney damage. People with Wilson’s Disease (an inherited disorder that causes too much copper to accumulate in your liver, brain and other vital organs) may be more sensitive than others to the effect of copper contamination and should contact their health care provider.\nHow do I remove copper from my drinking water?\nHeating or boiling your water will not remove copper. Because some of the water evaporates during the boiling process, the copper concentrations can actually increase slightly as the water is boiled. Additionally, chlorine treatments will not remove copper.\nIf water tests indicate copper is present in drinking water, the first course of action is to try to identify the source. If possible and cost-effective, eliminate the source. If the source is the water system piping, this is generally not practical.\nIf the source of copper is the in-home plumbing system, flushing the water system before using the water for drinking or cooking is a practical option. Flushing the system means anytime the water from a particular faucet has not been used for several hours (approximately six or more), it should be run until it becomes as cold as it will get. Flush each faucet individually before using the water for drinking or cooking. Water run from the tap during the flushing can be used for non-consumption purposes such as watering plants, washing dishes or clothing or cleaning. Avoid cooking with or consuming water from hot water taps as hot water dissolves copper more readily than cold.\nIf flushing the water system does not reduce copper levels to an acceptable level or is not an alternative of choice, consider an alternative drinking water source such as bottled water or water treatment. If the water is corrosive because of low pH (acidic), a neutralizing filter can be used to raise the pH of the water. This will reduce or eliminate corrosion problems.\nReverse osmosis and distillation treatment can be used to remove copper from drinking water. Typically, copper removal by reverse osmosis or distillation is used to treat water at one faucet.\nNortheast Water Wells offers a variety of testing packages to take care of all of your water needs. Call today to set up a time for us to collect a sample of your water. All of our testing is done through a state certified analytical lab.\nIf you live in Massachusetts you can view the guidelines for Well Water Testing here\nIf you live in New Hampshire you can view the guidelines for Well Water Testing here\nArticle written by Karen Provencher, Northeast Water Wells']	['<urn:uuid:7f6b26cf-20c0-4179-9dbf-e222f612917f>', '<urn:uuid:63226b96-f370-4b85-8b9d-4c7973be37b8>']	open-ended	with-premise	concise-and-natural	similar-to-document	multi-aspect	novice	2025-04-22T14:43:01.858830	14	85	1496
12	Why are fractals considered infinitely deep and ghostly?	Fractals are considered infinitely deep because you can keep focusing on them and never reach the end of their repeating patterns. They are considered ghostly because even when a whole page is full of fractals, their total area is zero since they're just a collection of infinite lines. In nature, fractals can be found in snowflakes, river networks, flowers, trees, lightning strikes, and blood vessels, though natural fractals only replicate by several layers.	['Mathematics is visible everywhere in nature, even where we are not expecting it. It can help explain the way galaxies spiral, a seashell curves, patterns replicate, and rivers bend.\nEven subjective emotions, like what we find beautiful, can have mathematic explanations.\n“Maths is not only seen as beautiful – beauty is also mathematical,” says Dr Thomas Britz, a lecturer in UNSW Science’s School of Mathematics & Statistics. “The two are intertwined.”\nDr Britz works in combinatorics, a field focused on complex counting and puzzle solving. While combinatorics sits within pure mathematics, Dr Britz has always been drawn to the philosophical questions about mathematics.\nHe also finds beauty in the mathematical process.\n“From a personal point of view, maths is just really fun to do. I’ve loved it ever since I was a little kid.\n“Sometimes, the beauty and enjoyment of maths is in the concepts, or in the results, or in the explanations. Other times, it’s the thought processes that make your mind turn in nice ways, the emotions that you get, or just working in the flow – like getting lost in a good book.”\nHere, Dr Britz shares some of his favourite connections between maths and beauty.\n1. Symmetry – but with a touch of surprise\nIn 2018, Dr Britz gave a TEDx talk on the Mathematics of Emotion, where he used recent studies on maths and emotions to touch on how maths might help explain emotions, like beauty.\n“Our brains reward us when we recognise patterns, whether this is seeing symmetry, organising parts of a whole, or puzzle-solving,” he says.\n“When we spot something deviating from a pattern – when there’s a touch of the unexpected – our brains reward us once again. We feel delight and excitement.”\nFor example, humans perceive symmetrical faces as beautiful. However, a feature that breaks up the symmetry in a small, interesting or surprising way – such as a beauty spot – adds to the beauty.\n“This same idea can be seen in music,” says Dr Britz. “Patterned and ordered sounds with a touch of the unexpected can have added personality, charm and depth.”\nMany mathematical concepts exhibit a similar harmony between pattern and surprise, elegance and chaos, truth and mystery.\n“The interwovenness of maths and beauty is itself beautiful to me,” says Dr Britz.\n2. Fractals: infinite and ghostly\nFractals are self-referential patterns that repeat themselves, to some degree, on smaller scales. The closer you look, the more repetitions you will see – like the fronds and leaves of a fern.\n“These repeating patterns are everywhere in nature,” says Dr Britz. “In snowflakes, river networks, flowers, trees, lightning strikes – even in our blood vessels.”\nFractals in nature can often only replicate by several layers, but theoretic fractals can be infinite. Many computer-generated simulations have been created as models of infinite fractals.\n“You can keep focusing on a fractal, but you’ll never get to the end of it,” says Dr Britz.\n“Fractals are infinitely deep. They are also infinitely ghostly.\n“You might have a whole page full of fractals, but the total area that you’ve drawn is still zero, because it’s just a bunch of infinite lines.”\n3. Pi: an unknowable truth\nPi (or ‘π’) is a number often first learnt in high school geometry. In simplest terms, it is a number slightly more than 3.\nBut Pi is a lot more than this.\n“When you look into other aspects of nature, you will suddenly find Pi everywhere,” says Dr Britz. “Not only is it linked to every circle, but Pi sometimes pops up in formulas that have nothing to do with circles, like in probability and calculus.”\nDespite being the most famous number (International Pi Day is held annually on 14 March, 3.14 in American dating), there is a lot of mystery around it.\n“We know a lot about Pi, but we really don’t know anything about Pi,” says Dr Britz.\n“There’s a beauty about it – a beautiful dichotomy or tension.”\nPi is infinite and, by definition, unknowable. No pattern has yet been identified in its decimal points. It’s understood that any combination of numbers, like your phone number or birthday, will appear in Pi somewhere (you can search this via an online lookup tool of the first 200 million digits).\nWe currently know 50 trillion digits of Pi, a record broken earlier this year. But, as we cannot calculate the exact value of Pi, we can never completely calculate the circumference or area of a circle – although we can get close.\n“What’s going on here?” says Dr Britz. “What is it about this strange number that somehow ties all the circles of the world together?\n“There’s some underlying truth to Pi, but we don’t understand it. This mystique makes it all the more beautiful.”\n4. A golden and ancient ratio\nThe Golden Ratio (or ‘ϕ’) is perhaps the most popular mathematical theorem for beauty. It’s considered the most aesthetically pleasing way to proportion an object.\nThe ratio can be shortened, roughly, to 1.618. When presented geometrically, the ratio creates the Golden Rectangle or the Golden Spiral.\n“Throughout history, the ratio was treated as a benchmark for the ideal form, whether in architecture, artwork, or the human body,” says Dr Britz. “It was called the ‘Divine Proportion’.\n5. A paradox closer to magic\nA famous geometrical theorem called the Banach-Tarski paradox says that if you have a ball in 3D space and split it into a few specific pieces, there is a way to reassemble the parts so that you create two balls.\n“This is already interesting, but it gets even weirder,” says Dr Britz.\n“When the two new balls are created, they will both be the same size as the first ball.”\nMathematically speaking, this theorem works – it is possible to reassemble the pieces in a way that doubles the balls.\n“You can’t do this in real life,” says Dr Britz. “But you can do it mathematically.\nThat’s sort of magic. That is magic.\nFractals, the Banach-Tarski paradox and Pi are just the surface of the mathematical concepts he finds beauty in.\n“To experience many beautiful parts of maths, you need a lot of background knowledge,” says Dr Britz. “You need a lot of basic – and often very boring – training. It’s a bit like doing a million push ups before playing a sport.\n“But it is worth it. I hope that more people get to the fun bit of maths. There is so much more beauty to uncover.”']	['<urn:uuid:108e2e0b-4188-46aa-8039-eb8c6d94f7e8>']	open-ended	direct	concise-and-natural	similar-to-document	single-doc	expert	2025-04-22T14:43:01.858830	8	73	1085
13	How do lysosomes coordinate with other cellular components for waste processing, and what happens to materials that cannot be recycled?	Lysosomes coordinate with multiple cellular components in waste processing. They work with the plasma membrane during endocytosis, where vesicles containing extracellular fluid are pinched off and enter the cell. For materials that cannot be recycled, lysosomes employ exocytosis - they fuse with the cell membrane and dump the unrecyclable waste out of the cell. The process is selective and organized - lysosomes contain specific hydrolytic enzymes that digest biological substances in an acidic environment. They interact with food vacuoles and can digest entire organelles when necessary. When processing cellular debris, they form residual bodies containing undigested wastes, which can then be eliminated from the cell.	['Cell Structure and Genetic Control\n• Basic unit of structure and function of the body.\n▫ Highly organized molecular factory.\n• Great diversity of function.\n▫ Organ physiology derived from complex functions of\n• 3 principal parts:\n▫ Plasma membrane.\n▫ Cytoplasm and organelles.\n• Is selectively permeable.\n▫ Double layer of phospholipids due to hydrophobic/hydrophilic\nRestrict passage of H20 and H20 soluble ions.\n▫ Proteins span or partially span the membrane.\nProvide structural support, transport molecules, serve as receptors.\n▫ Negatively charged carbohydrates attach to the outer\nInvolved with regulatory molecules.\nCytoplasm, Organelles, Nucleoli\n▫ Aqueous content of the cell.\n▫ Sub-cellular structures within the cytoplasm.\n▫ Is a large spheroid body.\n▫ Largest of the organelles.\n▫ Contains the genetic material (DNA).\nCenters for production of ribosomes.\n▫ Phagocytic cells use pseudopods to surround and engulf particles.\n▫ Pseudopods join, fuse, and surround ingested particle (food\nLysosomes digest food vacuole.\n▫ Protects from invading organisms.\n▫ Removes debris.\nPlasma membrane invaginates, fuses, vesicle containing ECF pinches off,\nand vesicle enters cell.\nBulk Transport (continued)\n• Receptor-mediated endocytosis:\n▫ Interaction of molecules in ECF with specific membrane receptor\n▫ Membrane invaginates, fuses, pinches off and forms vesicle.\n▫ Vesicle enters cell.\n▫ Process by which cellular products are secreted into extracellular\n▫ Proteins and other molecules to be secreted are packaged in vesicles\nby Golgi complex.\n▫ Vesicles fuse with plasma membrane and release contents into\nCilia, Flagella, Microvilli\n▫ Tiny hair-like structures that project from the\nsurface of the cell.\nStroke in unison.\nRespiratory tract, uterine tube.\n▫ Simple whip-like structure that propels sperm\nthrough its environment.\n▫ Numerous folds (finger-like projections) increase\nCytoplasm and Cytoskeleton\n▫ Jelly-like matrix within\n▫ Includes organelles and\n▫ Highly organized\nfunction as cytoskeleton.\n▫ Actin and myosin\n▫ Spindle apparatus\nContain only digestive enzymes.\nPrimary lysosome fuses with food vacuole or organelle.\nContain partially digested remnants of other organelles and organic\n▫ Residual body:\nContain undigested wastes.\nProcess that destroys worn-out organelles, so that they can be\n▫ Apoptosis (programmed cell death):\nLysosomes release digestive enzymes into the cell.\n• Membrane-enclosed organelles.\n▫ Contain specific enzymes that promote\n▫ Oxidize molecules and form H202.\n• Catalase: converts H202 H20 + 02.\n• Oxidation of toxic molecules by peroxisomes\nis an important function of liver and kidney\n• Sites for energy\nproduction of all cells;\nbut mature RBCs.\n• Contain own DNA, can\n▫ Outer membrane: smooth.\n▫ Inner membrane: cristae.\n▫ Cristae and matrix\nHave different roles in\n• Protein factories:\n▫ Proteins produced according to genetic information\ncontained in mRNA.\n▫ Located in cytoplasm and on the surface of\n• rRNA molecules serve as enzymes (ribozymes)\nrequired for protein synthesis.\n▫ Contains 2 subunits composed of rRNA and\nEndoplasmic Reticulum (ER)\n• Granular (rough) ER:\n▫ Bears ribosomes on\nsurface, in cells active in\nProteins enter cisternae\nare modified for secretion.\n• Agranular (smooth) ER:\n▫ Provides site for enzyme\nreactions in steroid\nhormone production and\n▫ Storage of Ca2+\n• Stacks of hollow, flattened\nsacks with cisternae.\n▫ One side of sack faces site for\nentry of vesicles from ER that\ncontain cellular products.\n▫ Other site faces towards\nplasma membrane and\nreleases vesicles of chemically\n• Modifies proteins, separates\naccording to destination, and\npackages into vesicles.\nMost cells have single nucleus.\nEnclosed by inner and outer membrane\n◦ Outer membrane is continuous with ER.\nNuclear pore complexes fuse inner and outer\n◦ Selective active transport of proteins and RNA.\nRegulation of gene expression.\nTransport of mRNA out of nucleus to ribosomes.\n◦ DNA contains the genes that code for the production\n▫ Cells deprived of blood supply swell, the membrane\nruptures, and the cell bursts (necrosis).\n▫ Cells shrink, membranes become bubbled, nuclei condense.\n• Capsases (“executioner enzymes”):\n▫ Mitochondria membranes become permeable to proteins\nand other products.\n• Programmed cell death:\n▫ Physiological process responsible for remodeling of tissues\nduring embryonic development and tissue turnover in the', 'Lysosomes could be called cells’ recycling centres because they digest and recycle waste inside the cells.\nWhy are lysosomes often referred to as the recycling centers of the cell?\nWhy are lysosomes often referred to as the “recycling centers” of the cell? The enzymes within the lysosome break down failing organelles and other structures within the cell. What is the function of plant vacuoles?\nDoes lysosome recycle materials?\nAs most high schoolers learn, the lysosome carries out waste disposal and recycling. In a process known as autophagy (meaning “self-eating”), it takes in old cellular components and unneeded large molecules, such as proteins, nucleic acids and sugars, and digests them with the help of enzymes and acids.\nHow lysosomes function as a waste disposal system explain?\nAnswer: Lysosomes break down and enzymes are released freely in damaged cells, old cells, dead cells, or cell organelles that do not work to digest them. In these processes, they remove cellular debris. Therefore, they are also called Cellular Waste Disposal Systems.\nWhat organelles are the recycling center of the cell?\nCells also have to recycle compartments called organelles when they become old and worn out. For this task, they rely on an organelle called the lysosome, which works like a cellular stomach.\nWhich are recycling centers for the cell quizlet?\nThe Golgi apparatus packages molecules processed by the endoplasmic reticulum to be transported out of the cell. These organelles are the recycling center of the cell. They digest foreign bacteria that invade the cell, rid the cell of toxic substances, and recycle worn-out cell components.\nWhy is recycling important in cells?\nRecycling, the reuse of material, saves energy and resources. … FERARI distributes the recyclable molecules, mainly transport proteins and receptors, and reintroduces them into the cellular cycle. In this way, valuable cell components do not have to be constantly produced anew, which not only saves energy but also time.\nHow do lysosomes break down materials?\nLysosomes break down macromolecules into their constituent parts, which are then recycled. These membrane-bound organelles contain a variety of enzymes called hydrolases that can digest proteins, nucleic acids, lipids, and complex sugars. The lumen of a lysosome is more acidic than the cytoplasm.\nWhy do we need lysosomes?\nA lysosome is a membrane-bound cell organelle that contains digestive enzymes. … They break down excess or worn-out cell parts. They may be used to destroy invading viruses and bacteria. If the cell is damaged beyond repair, lysosomes can help it to self-destruct in a process called programmed cell death, or apoptosis.\nWhat do lysosomes look like and do?\nLysosomes appear initially as spherical bodies about 50-70nm in diameter and are bounded by a single membrane. Several hundred lysosomes may be present in a single animal cell. Recent work suggests that there are two types of lysosomes: secretory lysosomes and conventional ones.\nWhy are lysosomes known as a waste disposal system of the cell B suicide bags of the cell?\nLysosomes are known as suicide bags of the cell because they contain lytic enzymes capable of digesting cells and unwanted materials. … This causes the hydrolytic enzymes to be released. The released enzymes then digest their own cell, causing the cell to die.\nWhere does lysosome waste go?\n(Otherwise, if a lysosome were to leak or burst, the enzymes could kill everything in the cell.) When a lysosome comes across cellular debris it can’t reuse, it fuses with the cell membrane and dumps the waste out of the cell in a process called exocytosis.\nWhich is known as waste disposal of cell?\nLysosome. Lysosomes act as the waste disposal system of the cell. They have hydrolyzing enzymes which can digest biological substances.\nDo lysosomes destroy and recycle old organelles?\nLysosomes are organelles that digest and dispose of unwanted protein, DNA, RNA, carbohydrates, and lipids in the cell. … Aside from breaking down unwanted molecules, and even other organelles, its recycling function is at the center of a process called autophagy, in which the cell digests itself.\nWhat type of cells contain lysosomes?\nlysosome, subcellular organelle that is found in nearly all types of eukaryotic cells (cells with a clearly defined nucleus) and that is responsible for the digestion of macromolecules, old cell parts, and microorganisms.']	['<urn:uuid:3b56b51f-05b8-4bc9-aa95-fbaf5491fd62>', '<urn:uuid:f163f9b3-b051-4780-a57f-db3ab6e34db2>']	open-ended	direct	verbose-and-natural	distant-from-document	multi-aspect	expert	2025-04-22T14:43:01.858830	20	105	1365
14	How do regular fasts differ from Yom Kippur's fast?	Yom Kippur has a full fast lasting 25 hours from sunset until after sundown the next day, and includes abstaining from both food and water. In contrast, minor Jewish fasts only last from dawn to sunset. These minor fasts are similar to the Muslim month of Ramadan, where fasting occurs only during daylight hours each day and does not include nighttime hours.	['Aramaic Literature – Part 7 – The MishnahThe eighth treatise in Mo’ed is Rosh Hashanah. This literally means “the head of the year,” or “the first of the year,” thus it is the New Year celebration. In our modern calendar, Rosh Hashanah occurs in September. It corresponds to the first day of the seventh month in the Old Testament liturgical calendar (Leviticus 23). The celebration of Rosh Hashanah seems to be a post-biblical development. In the Old Testament period there seems to have been two calendars, a religious (summarized in Lev 23) that started in the spring, and a civil calendar, that started in the fall. According to Edmund Thiele, the use of these different calendars account for some of the apparent discrepancies between the chronologies of the Israelite and Judean kings between the time of Solomon, and the destruction of Solomon’s temple (see his major work The Mysterious Numbers of the Hebrew Kings). In the post-biblical period, this civil calendar has become the primary calendar for the reckoning of Jewish dates. As a result, Rosh Hashanah occurs at the beginning of the liturgical seventh month.\nThe treatise “Rosh Hashanah,” besides dealing with Rosh Hashanah proper, also deals with other calendar-related issues. It treats, for example, the shofar, which is the horn blown to announce the arrival of the day. It also deals with various other issues related to setting the calendar, since the Jewish calendar is lunar, and occasionally a thirteenth month must be added in order to keep the calendar in line with the seasons. This also explains why it is that Jewish holy days do not always fall on the same days in our calendar. In that way, the determination of the Jewish calendar is a little like the determination of Easter in the Christian calendar. In Western Christianity, the date of Easter is determined as the first Sunday after the first full moon after the vernal (spring) equinox (when the sun’s orbit recrosses the equator).\nFollowing the treatise on Rosh Hashanah, the Mishnah deals with issues relative to fasting in the treatise Ta’anith. This treatise on fasting not only defines the practices related to fasting, but also sets out certain prayers. There are two full fast days, defined as lasting from sunset until full darkness the following evening. These are Yom Kippur (the Day of Atonement, the only regular fast day mentioned in the Pentateuch) and the 9th of Av, which commemorates the destruction of the temple. Several other minor fasts are discussed. These minor fasts last only from dawn to sunset. In this feature, the Jewish minor fasts are like the fasting of Ramadan the Muslim month of fasting. In that month, the fasting lasts during the daylight hours of each day of the month, but does not include nighttime hours.\nThe tenth treatise in Mo’ed is Megillah (scroll). It deals with the practices related to Purim. The name comes from the fact that the Book of Esther, which explains the origin of Purim, is one of the Five Scrolls that is read at particular times during the Jewish year. Obviously, Esther is read at Purim. Song of Songs is read at Passover. Ruth is read at Pentecost (the Feast of Weeks). Lamentations is read on the 9th of Av. Ecclesiastes is read at the Feast of Tabernacles.\nAramaic Thoughts‘ Copyright 2016© Benjamin Shaw. ‘Aramaic Thoughts‘ articles may be reproduced in whole under the following provisions: 1) A proper credit must be given to the author at the end of each story, along with a link to http://www.studylight.org/ls/at/ 2)‘Aramaic Thoughts‘ content may not be arranged or “mirrored” as a competitive online service.\n- 1Share on Facebook (Opens in new window)1\n- Click to share on Twitter (Opens in new window)\n- Click to Press This! (Opens in new window)\n- Click to share on LinkedIn (Opens in new window)\n- Click to share on Google+ (Opens in new window)\n- Click to share on Pinterest (Opens in new window)\n- Click to share on Tumblr (Opens in new window)\n- Click to share on Reddit (Opens in new window)\n- Click to share on Pocket (Opens in new window)\n- Click to email (Opens in new window)\n- Click to print (Opens in new window)', 'What’s Yom Kippur about exactly?\nYom Kippur is the Day of Atonement, when we ask forgiveness for the wrongs we have committed over the past year. Jewish tradition believes that on this day God places a seal upon the Divine decrees affecting each person for the coming year. Traditionally, Jews fast on this somber day and also refrain from other bodily pleasures.\nHow long does Yom Kippur last?\nYom Kippur, which falls 10 days after Rosh Hashanah, lasts one day. This year it begins at sunset on Tuesday, Oct. 11 and lasts through Wednesday, Oct. 12. The fast itself lasts 25 hours, from just before sunset on Oct. 11 until just after sundown on Oct. 12.\nWhy do people fast on Yom Kippur?\nYom Kippur is the day on which we are instructed to divorce ourselves as completely as humanly possible from the mundane world in which we live, in order to devote ourselves with all our hearts and minds to our relationship with the Divine. Fasting is the most widespread manifestation of this devotion. Other examples include: refraining from washing, sexual relations, and the wearing of leather (a sign of luxury in earlier times).\nDo children have to fast? Is anyone exempt from fasting? Can I at least drink water?\nTraditionally, Jews are not required to fast until they reach bar/bat mitzvah age (12 or 13), and children under the age of 9 are not allowed to fast. People for whom fasting is a health risk, along with pregnant and nursing women, are also exempt. The fast includes abstaining from water, but, again, only if doing so does not pose health risk. For tips on fasting without jeopardizing your health, click here.\nI heard it’s OK to wear sneakers to services. Is that really true?\nYes, many Jews wear sneakers, or white athletic shoes, on Yom Kippur. That’s because of a desire to avoid leather (a sign of luxury in early times) and the tradition of wearing white, as a symbol of purity.\nWhat prayers are unique to Yom Kippur?\nThe evening of Yom Kippur begins with Kol Nidre, a prayer that is repeated three times and asks that all vows and oaths that we have made throughout the year will be forgiven, so we can start the new year with a clean slate. Another major prayer is the Vidui, or confession, which includes Ashamnu and Al-Chet, prayers which list all the sins individuals in the community have committed.\nWhat parts of the Torah are read on Yom Kippur?\nOn Yom Kippur, congregations traditionally read a passage from Leviticus about the sacrificing of a goat (the origin of the term scapegoat). The Reform movement has replaced that reading with one from Deuteronomy on human freedom to make moral choices. In addition to these readings from the Torah (the five books of Moses) on the afternoon of Yom Kippur it is customary to read the Book of Jonah, from the Prophets section of the Bible.\nWhat’s this I keep hearing about the Yom Kippur breakfast? I thought people skipped breakfast on Yom Kippur.\nA break-fast is an informal meal in the evening, after the Yom Kippur fast has ended. In the United States break-fasts tend to resemble morning breakfast (or at least brunch) in that they tend to be dairy (rather than meat) and to include bagels, cream cheese, smoked fish, salads and sandwich fillings like cheese, tuna salad and egg salad. Here are some recommended recipes.\nHow do I greet people on and before Yom Kippur? Since it’s such a somber holiday, saying “Have a good holiday,” or “Happy Yom Kippur” seems awkward. And are there any other special words or phrases I need to know?\nYou can say, “Have an easy fast” or “gmar hatima tova” (may you be inscribed for a good year.) It’s also acceptable to say “shana tova” (happy new year). As for other words and phrases for the holiday, check out our glossary for Yom Kippur. (We also have ones for Rosh Hashanah and Sukkot.)\nPronounced: MITZ-vuh or meetz-VAH, Origin: Hebrew, commandment, also used to mean good deed.\nPronunced: TORE-uh, Origin: Hebrew, the Five Books of Moses.\nPronounced: yohm KIPP-er, also yohm kee-PORE, Origin: Hebrew, The Day of Atonement, the holiest day on the Jewish calendar and, with Rosh Hashanah, one of the High Holidays.']	['<urn:uuid:25334aec-7ace-47ac-86ab-0368853e9956>', '<urn:uuid:81f21fef-b222-4f67-a740-c556bffd2c0a>']	open-ended	direct	concise-and-natural	distant-from-document	multi-aspect	novice	2025-04-22T14:43:01.858830	9	62	1431
15	utility easement termination possibility and compensation rates for permanent utility easements real estate expert	While utility easements are typically permanent and very difficult to terminate, in rare cases they can be abandoned if the utility no longer needs them. Regarding compensation rates, government appraisers typically calculate between 30% and 90% of the full land value for utility easement compensation. The exact amount depends on how much the easement diminishes the value and enjoyment of the land, considering that ownership is retained but control is partially lost.	['The term easement in real estate law refers to the right of one party who is not a bona fide owner of a landed property, to access a specified portion or the whole of another party’s landed property for his personal use. There are two parties to an easement, the owner of the land who holds a title to the property on which another person holds an easement, and the holder of the easement. Holding of an easement on a piece of land does not confer ownership of the property on the holder. An easement merely allows the easement beneficiary to access that part of the property which the easement covers, for his personal use. The ownership of the piece of land on which an easement exists, still resides with the one holding the title to the land. However, as long as someone apart from the title holder holds an easement to a portion of a real estate property, the bona fide owner of the property cannot erect a permanent structure on that that portion of his real estate.\nEasements come in different forms and can in most cases be conferred on the beneficiary only by authority and or express permission of the owner to title to the land on which the easement resides. There are two broad categories of easements: Public easement and private easement. An example of a private easement is the type granted to a person to use part or all of a piece of land adjoining to that of the holder of the easement. The easement holder can use the land for his personal purpose, but he cannot erect a permanent structure on his easement. For example, a property owner may not be able to gain access to his property without passing through the adjoining property owned by someone else. In that case, he will need to approach the owner of the adjoining property to grant him an easement. The easement may be in form of a strip of land through which the easement benefactor passes to get to his own property.\nTypes of Easements\nEasements can be created in a number of ways. Some of the more common types of easements are: Appurtenant easement, ‘In gross’ easement, easement ‘By necessity’, and easement ‘By Prescription’. These are the most common private and public easements. There are other easements which are created for the protection of our common historical heritage, like monuments, historical buildings and ecological sites. We shall limit our discussion to the more common easements which are daily encountered in real estate transactions. We shall discuss the creation of these easements, their limitations, as well as the responsibilities and privileges of holders of these easements.\nWhere two parcels of land or real estates are adjacent to each other, the land or estate on which an easement resides is called the servient tenement, while the other land whose owner benefits from the easement is called the dominant tenement. In the case of two adjacent landed properties or real estates, the dominant tenement owner may need to hold an easement on the servient tenement in order to get to his own land. Hence the dominant tenement is the beneficiary, while the servient tenement bears the burden of the easement. This type of easement is called an ‘Appurtenant’. In an appurtenant easement, the dominant tenement owner is allowed to pass over the servient tenement either on foot or by motorized vehicle. For example, in order to get to his property, the dominant tenement owner, his family, friends and visitors can use the driveway of the servient tenement.\nDominant tenement versus servient tenementAppurtenant easements are usually very difficult to revoke. The servient tenement is saddled with the inconveniences of the presence of an appurtenant easement on it. If the burden of the presence of an appurtenant easement becomes too great on the servient tenement, or the owner wishes to construct a permanent structure on the easement; or use it for some other purpose, he will have to provide an alternative access route to the dominant tenement. Where it is necessary for example to reroute a driveway on which an appurtenant easement exists, the owner of the servient tenement will have to pay all the cost of providing the alternate route to the dominant tenement, if the new route has to pass over a third party land which he does not own. An appurtenant easement cannot be extinguished or revoked when the servient tenement changes hands either through sale, or transfer. It must be included in the will of the owner, so that his inheritors are made aware of their obligations to the dominant tenement.\nEasement ‘In gross’\nAn easement in gross is a type of easement which benefits the holder of the easement personally without his having to own land within the vicinity of the easement. The grant of easement in gross could be to an individual person, or a corporate body. For example if you have trees on your land you can grant an easement to someone who wants to use them for timber. The person so favored is said to hold an easement in gross on your land, for the specific purpose of felling the trees, and converting them to his personal use. After such a holder of the easement in gross has completed his assignment of felling the trees on your land, you can terminate his easement in gross. This is a personal easement in gross. You can also grant an also grant an easement in gross to a corporate body or a municipality for example the construction and maintenance of a highway or a rail line.\nA more common example of easement in gross is the right of public utilities to use part of your land to for installation of their equipments. You must also grant them the right of access to service these equipments, and carry out maintenance on them. The right of public utilities to easement in gross on your land cannot be terminated because these utility companies hold their easement in gross for the public good, which overrides your personal convenience. Examples of public utilities which hold permanent easements in gross are public electricity companies, gas companies, and public water supply companies. These companies hold their easements in gross in perpetuity, and their right of way cannot be abridged. Their agents have free access to these facilities by the right of way granted to them by their easement in gross. You cannot legally deny workers of these utility companies free access to their easement in gross. Also you cannot legally remove electricity poles from your land or dig up gas and water pipes installed by the appropriate public utility companies on your land.\nEasement by Necessity\nEasement by necessity is a type of easement granted to a person or corporate body out of absolute necessity. As an example, a piece of real estate property may not have direct access to a highway or source of water. An easement by necessity may be granted to the landlocked property so that it can have access to the highway or source of water. All occupants of such a landlocked real estate will now be able to access the highway or source of water by the deed of easement by necessity. Visitors to the landlocked estate can also access the estate through the easement by necessity granted to it.\nHowever, the easement by necessity may be temporary and can be terminated or revoked if the necessity which gave rise to it in the first place no longer exists. If in the example given above, a new highway is constructed to which the land locked real estate now has direct access, the original ease by necessity can be revoked, since the necessity to have access to a highway no longer exists. Similarly, if an alternative source of water becomes available to the landlocked estate, the easement by necessity granted to access the first source of water is no longer absolutely necessary. Therefore the easement by necessity granted to the real estate to access a source of water can then be terminated.\nEasement by prescription\nEasement by prescription differs markedly from the other types of easements, because of how it is created. An easement by prescription can be claimed by someone who has been using a piece of real estate land for a particular purpose over a period of time, even though he does not have a title to the real estate property; nor had he obtained prior permission from the rightful owner before he commenced use of the land for his personal purpose. The mere fact that he had occupied the land for a considerable time, and used it for a purpose evident to the public; may be enough to grant him an easement by prescription. This claim to long-time usage is recognized in common law, and he can continue to use the land for the particular purpose which he had used it over time.\nEasements by prescription are implied easements and do not require rigorous proof in law like the other types of easements. All a holder of an easement by prescription is required to prove is that he has occupied the piece of land continuously for a period prescribed by law. The period of continuous uninterrupted occupation varies from five to thirty years, depending on local laws which vary from one state to another. Once this legal requirement is met, he can continue to use the land for that same purpose indefinitely. However, if before the prescribed period of occupation is reached, the legal owner of the land assets his rights, the easement by prescription can be terminated.\nTermination of Easements\nEasements can be terminated in a number of ways which include:\n* Unity of ownership or merger of both Dominant tenement and Servient tenements:\nAn easement on a real estate property can be terminated if the owner of the servient tenement acquires ownership of the dominant tenement. He can thus merge his two adjoining properties into one continuous whole. With the merger of both servient and dominant tenements, the real estate property owner now holds easement to his own property; and hence is not beholden to anyone else with respect to use of any part of his merged property. In both the letter and spirit of the law, the easement stands terminated.\n* Valid written release of the easement by the owner of the dominant tenement.\nThe holder of an easement on a piece of real estate which adjoins their own dominant tenement can terminate the easement by delivering a validly written release of easement to the owner of the servient easement. By this action the dominant tenement has terminated the easement it holds on the servient tenement.\nAn owner of a dominant tenement can cause the easement he holds on a servient tenement to lapse and thus have it terminated, if he fails to use his right of access to the servient tenement continuously for a long period of time as specified by law. He has thus abandoned his easement, and the easement is thus considered terminated.\nThe laws of easement are in two broad categories: Public easement and private easement. The public easements are governed by acts of state and federal legislatures and are irrevocable except by amendments to the legislative acts by parliament. Private easements on the other hand can be made and revoked by mutual agreement between the two parties to the easement. While negotiating purchase of real estate property, one needs to carefully consider the effect of subsisting easements on the real estate before committing oneself to the final purchase.\n1. Everhart, Marion E: Everhart on Easements, Todd Publications (June 1981)\n2. Jackson, Paul: The law of easements and profits, Butterworth (1978)\n3. West Virginia Limiting Liability of Landowners Act\nWest Virginia Code - Chapter 19, Article 25-1 through 25-5\nLaws of the Department of Agriculture\n4. Wilson, Donald A. Easements and Reversions, Landmark Enterprises (June 1, 1992)\nOrder your paper now!\nI asked Essay Lab to write an essay for me and received paper the next day after I ordered it! Thank you!\nAwesome WORK! If I ever need to write my essay – I will use only EssayLab!\nThese people are lifesavers! Just ask – “write me an essay” and they will start right away!', 'If your property is subject to a utility easement, it means that a utility – private or government – has a non-ownership right to use your property for the purposes of providing and maintaining the specified utility. That probably sounds a bit convoluted, so let’s break it down.\nUtility easements are the path through which things like power, natural gas, water, and wastewater travel. A utility taking an easement doesn’t technically convey any ownership of the land. It’s an “interest in real property,” which is a fancy way of saying that an easement takes rights to use the land without actually owning it. These can be temporary or permanent, though in some cases, the difference is minimal.\nCan the utility company come onto my property?\nYes, that’s an essential part of utility easements. If equipment or services are damaged, require maintenance, or need altering, utility easements give utilities the right to enter your land to do their work. If there’s trouble with gas, electric, or other utility lines that run through, over, or under your land, the easement gives the utility company the right to access and repair them. An easement may also enable them to erect utility poles or towers on your property.\nAn overview of easements\nThe easiest way to understand the different kinds of easements is to think of it as “the right of another to use your land for a specified purpose.”\nYou still own the land, but you can’t just do anything you want with it anymore. You’re not allowed to interfere with the easement holder’s use and enjoyment of the easement. On the other hand, the easement holder – the utility in this case – can only use the land for the stated purpose of the easement.\nThere are plenty of examples of non-utility easements, such as shared driveways or driveway easements across your land so a neighbor can access private property. You might grant someone an easement to come fish in your pond. Utility easements are another animal entirely.\nPermanent utility easements\nA permanent utility easement (PUE) is a permanent right to access and use your land for utility service purposes, including maintenance, upgrades, and so on. The easement will remain even if you sell the property; it “runs with the land.” When a government entity like GDOT takes a right of way, they take the property. A permanent utility easement means they’re taking rights to use the property for a specific purpose, permanently. You still own it. They just get to use it.\nExamples of PUEs include power line easements across your property, drainage easements, or even things like buried utilities such as gas, power, or data lines.\nNote: Utility easements are almost always permanent.\nWhen do permanent utility easements happen?\nUtility easements are often taken when there is a road widening project. Existing power lines must be moved off the new road and onto your property, for example. Road work can push other utilities like gas lines, cable, and telephone poles onto your property as well. Unlucky property owners may lose land to GDOT for a road and then lose even more to utility easement relocation!\nIf the county or city needs to install water or sewer service, they may also grab an easement. These installations can be very destructive to properties in their path, as they are often engineered without regard to things like fences, trees, sheds, and clear spaces. Another common example of a PUE is when a city or county is installing sewer or water lines. Sewer lines aren’t always even servicing the property they cross but may result in large, unsightly manhole covers for maintenance and repair access.\nWhat are Georgia’s utility easement laws?\nThe law allows GDOT, municipalities, and utilities to take easements on your land. The rules governing a utility easement are typically spelled out in a PUE deed, and can give the utility the ability to:\n- Enlarge the utility – install larger pipes, for example\n- Repair and maintain the utility\n- Allow new improvements to be made to the utility\n- Allow heavy trucks to park in the easement area\n- Disallow the property owner from parking in the easement area\n- Forbid the property owner from excluding trespassers\n- Prevent the property owner from improving or constructing anything that impedes access to the easement at any time\nGDOT, counties, cities, and utility companies may have different wording in their Permanent Utility Easement deed language, but they will be broadly similar.\nThese agreements are usually a mess of legalese and often take a great deal more than is necessary. We highly recommend hiring an experienced eminent domain attorney to review any easement agreement before you sign it. You may be signing away more than you think, and you may be entitled to compensation.\nDoes my property already have a utility easement?\nTo discover existing easements on your property, you’d want to do a title search, which can be very complex and time consuming. This should have been discovered when you purchased the property, as an attorney or title company would have done the necessary research. Easements typically transfer with the land, so if a prior owner agreed to an easement, you’re stuck with it.\nThere are, of course, smaller and more common easements that service almost every property. If you have cable and internet, you probably already have an easement for those services. But that does not mean utility companies can abuse their rights.\nHow might utility companies abuse an easement?\nEasements are, ideally, very clearly defined. Georgia law limits their scope in some cases, but the PUE itself should also specify use and area. Also note that easement agreements are typically very broadly worded, giving the utility more leeway in their activities.\nIf the utility exceeds its authorized area or activities (though unlikely), it’s basically trespassing on your private property. If the utility negligently damages or destroys unrelated property in the exercise of their easement, they may owe you compensation.\nNote: Developments that want to use an easement for additional traffic may exceed the easement.\nCan I remove a utility easement from my property?\nProbably not. In the case of a PUE, the language in the easement agreement is almost always permanent. Even a temporary easement may last an uncomfortably long time and will be difficult, if not impossible, to end before its allotted expiration.\nThat said, in rare cases, the utility may abandon an easement it no longer needs.\nWhat are my utility easement rights?\nYou are entitled to use and enjoyment of the easement area, but within the limitations defined by the easement. Many homeowners and property owners get frustrated because they’re not sure what they can and can’t do. Perhaps they have a large easement area and either don’t want to maintain the area or conversely worry that it might become desolate and useless.\nThe truth is, it’s still your property regardless of the easement, and you must maintain it just as any property owner. And it need not become a wasteland.\nDepending on the type of PUE and the language in the deed, you may be able to fence the area, plant a garden, install flower beds or decorative bushes, or do other things that don’t interfere with the easement. The risk, in these cases, is that if the easement holder needs to damage or tear up these improvements for some reason, they’re probably not required to compensate you.\nHow do I calculate the value of a utility easement?\nYou may be entitled to compensation for the easement taking. What you may be owed depends on how much the easement diminishes the value of, and your enjoyment of, the land. Remember that you retain ownership, so you’re not technically losing the land where they’re taking the easement. You’re simply losing some control over it.\nEvery property, and therefore every easement, is unique. It follows that the amount of compensation you should seek for an easement is likewise variable and depends on many unique circumstances.\nEffects of easements: Setbacks, acreage, and tax value\nA utility easement affects your property in some ways you might not think of. Here are a few examples.\nSetbacks – Most counties and municipalities have regulations that govern the construction of “improvements” – anything to be built on the land. One of these regulations may be a “setback” requirement; a specific distance an improvement must be from the property line. For example, the county may require that a fence be built at least 10 feet away from property lines.\nThe good news is that easements typically do not count against setback ordinances. The bad news is that the easement may not allow you to build what you want, regardless. Consult the PUE deed.\nAcreage – Sometimes, size matters. Some property owners, commercial and residential, are deeply vested in the acreage because it greatly affects the value of the property. Commercial owners may need a certain amount of acreage to account for runoffs and permeable surface requirements, while a residential property owner may want to add another building and need the acreage to meet zoning requirements.\nThe good news is that easements typically do not reduce acreage. If you have 0.65 acres with a 0.13 acre easement, you generally still have 0.65 acres. The bad news, again, is that your desired use may not be permitted by the PUE, making the acreage size moot.\nTax Value – While taxes are going to vary depending on location, an easement may allow you to reduce that tax burden. If your land is worth less due to the existence of a utility easement, you may not have to pay as much tax on it, though this won’t be true in every case. This can get really complicated, and is something you should discuss with your eminent domain attorney.\nAppraisals, value, and easements\nI have seen government appraisers calculate anywhere between 30% and 90% of the full land value for utility easement compensation.1\nHow can an eminent domain lawyer help me?\nWhen you’re faced with a legal process and the accompanying legal document that will fundamentally change your rights, it’s a good idea to have a lawyer on your side. For many people, property is the largest single investment they’ll make in their lives. For businesses, property rights can make or break multimillion-dollar projects.\nOur eminent domain team has over 85 years of combined experience and can help you:\n- Translate legalese into plain language. Easement agreements are packed with dense language. They may hide additional infringements in hard-to-understand jargon. They can bundle multiple easements into one, and you might not realize it. We’ve read hundreds and hundreds of easement documents. We know what they mean, and we can look for these hidden add-ons so you know exactly what you’re agreeing to.\n- Fight for maximum compensation. From the government to the local utility company, others have the right to take easements on your land. However, as the property owner, you should get fair compensation for what they take. We can help calculate the current and future damage to the value of your land and help you seek the highest possible compensation. And if the agreement is unfair or the compensation offer unreasonable, we can fight on your behalf – even in court if needed.\n- Leverage a network of professionals to build your case. We’ve cultivated a list of non-legal professionals with very specific expertise. We know who to call, for what, and when – and who can get things done for you. From eminent domain appraisals to engineers to surveyors and environmental professionals, we have the human resources to help us determine the true value of your land.\nLearn 2 More: About Our Formidable Team\nGet help grappling with utility easements\nThis article is not an all-encompassing list of the issues you may face if someone takes a utility easement on your property. If a utility company or government project burdens your private property with an easement, whether it’s for water pipes or power lines, we strongly encourage you to contact an attorney.\nOur team handles eminent domain cases, large and small. From a small residential plat to intricate, vast commercial property, we can evaluate your case and give you an experienced perspective on the taking. We have former state DOT attorneys on our team who are well-versed in the tactics utilities use, and how to try to ensure you’re treated fairly. Reach out to the Georgia Eminent Domain Law Firm by calling 1-888-391-1339 or filling out our contact form today.']	['<urn:uuid:eea5d37b-4580-4ca5-a28f-7ceb71d80e83>', '<urn:uuid:4a02e96f-831d-4a94-b6b6-59eba792374f>']	factoid	with-premise	long-search-query	similar-to-document	multi-aspect	expert	2025-04-22T14:43:01.858830	14	72	4152
16	animals used drug safety experiments	Toxicology testing must typically be conducted in at least two types of animals: small animals, such as mice or rats, and larger animals such as dogs or primates.	"[""This is Part 1. Part 2 is here.\nHow a New Drug is Approved - Part 1\nAccording to current estimates, developing and approving a new drug can take 10 or more years and the cost can be north of $2.5 billion. These statistics will vary depending on many factors including the chemical structure of the drug, how it is manufactured, its medical uses, the complexity of the clinical trials that will be required, and marketing considerations such as the availability of alternative treatments. However, there are certain scientific and regulatory requirements that any new drug candidate must satisfy to bring it from early development to market approval.\nThe US Food and Drug Administration, commonly known as the FDA, is responsible for ensuring the safety and effectiveness of all pharmaceuticals produced, sold, or used in the United States. This presentation will focus on the steps involved in the approval of new drugs, although the model is similar for medical devices and other entities that fall under the purview of the FDA.\nLegislation and FDA Guidances\nThere are two types of rules that govern the activities of the FDA: the laws that define the agency’s mandate, and “guidances” that describe the techniques FDA uses or recommends to others to successfully traverse the drug development process.\nSome of the key pieces of legislation that defined the role and authority of the FDA are:\n· The Federal Food, Drug, and Cosmetic Act of 1938 which was passed after a legally marketed elixir killed 107 people, including many children. The FD&C Act completely overhauled the public health system. Among other provisions, the law authorized the FDA to demand evidence of safety for new drugs, issue standards for food, and conduct factory inspections.\n· The Kefauver-Harris Amendments of 1962, which were inspired by the thalidomide tragedy in Europe, strengthened the rules for drug safety and required manufacturers to prove their drugs' effectiveness.\n· The Medical Device Amendments of 1976 followed a U.S. Senate finding that faulty medical devices had caused 10,000 injuries, including 731 deaths. The law applied safety and effectiveness safeguards to new devices.\nFDA issues regulations to implement its statutory authority. These create binding obligations and have the rule of law. Guidance documents are descriptions of FDA’s current thinking on a topic. They are not binding, but are a valuable resource for those wishing to understand the FDA perspective on the best ways to comply with relevant regulations. Alternative practices that adhere to the regulations are acceptable, but following the FDA guidances is the best way to improve your chances of successfully completing the required activities and securing FDA approval. According to the FDA website, there are currently over 4000 guidance documents that pertain to virtually every interaction one would have with the agency, including everything from Advertising to User Fees (that are paid by applicants to defray costs associated with FDA document reviews).\nDrug Development and Approval Process\nThe process for development and approval of a new drug consists of the following stages:\n· Pre-clinical testing and lead compound identification\n· Investigational New Drug (IND) application\no Phase 1\no Phase 2\no Phase 3\n· New Drug Application (NDA) filing and approval\n· Post-marketing surveillance (also referred to as Phase 4)\nWe will now take a closer look at each of these stages.\nIn the course of what is known as the “Discovery Phase” of Drug Development, laboratory or literature investigations may suggest to a researcher that a particular chemical compound could demonstrate biological activity in the treatment of a particular type of disease. This may be due to its structural similarity to other, proven drugs, and the researcher may propose modifications to make it more effective. Alternatively, the new compound may have a structure that could, theoretically, allow it to interact with an organ in the human body that has been affected by disease, or with a pathogen, such as a virus or bacterium, to prevent it from infecting a person. This compound, referred to as a New Chemical Entity, or NCE, will then go through a range of experiments that investigate this potential therapeutic effect, while evaluating whether it might cause harm to the patient. Using a variety of “Toxicology” (or, more optimistically, “safety”) tests, the investigator will monitor the reactions in animals to varying amounts of the NCE. The designs for these tests are first submitted to a review board (known as the Institutional Animal Care and Use Committee, or IACUC) for the laboratory at which they would be performed.\nThe IACUC will only allow the testing to proceed if it is deemed to be truly necessary to provide the information that the sponsor needs, and if it will be conducted in an ethical manner, under humane conditions and in the smallest reasonable number of animals. Toxicology testing must typically be conducted in at least two types of animals: small animals, such as mice or rats, and larger animals such as dogs or primates. The drug is given in an escalating range of doses until it reaches a dose at which some animals die. The dose just below this lethal dose is referred to as the Maximum Tolerated Dose, or MTD. This procedure enables the investigator to begin to determine how high a dose could be safely given to humans- typically a fraction of the MTD in animals. Additional safety studies will be performed to examine the potential for the NCE to cause cancer or genetic abnormalities in animals. There will also be some experiments that can indicate the amount of the drug that is absorbed by the animal and how rapidly it is cleared from the body. The Preclinical Phase will take several years depending on the types of studies that are needed. A large percentage of potential drug candidates are eliminated due to the findings in these tests.\nThe Maximum Tolerated Dose and its relationship to the doses that are expected to be therapeutic in humans will be investigated in Phase 1 clinical trials, but only after the sponsor convinces the FDA that the potential benefits of the drug outweigh the risks.\nInvestigational New Drug (IND) application\nAbout one in a thousand compounds that are tested in the laboratory progress to the point of submitting an IND, or Investigational New Drug application. This document contains all of the information gathered in preclinical tests, a description of the proposed tests in humans, and preliminary plans for how the NCE will be manufactured and tested for purity and potency. The sponsor waits for 30 days after the IND is submitted and, unless FDA responds to the contrary (in a classic instance of “no news is good news”), it may begin the “clinical” phase in which the drug will be tested for the first time in humans. If FDA has any concerns during the 30-day review period, it will notify the sponsor that the program is on “clinical hold” until these issues are resolved.\nClinical Phase I\nThe designs and procedures used in clinical trials must adhere to the principles in the Declaration of Helsinki which set the ethical guidelines for human experimentation. Some of the rules in this declaration include the requirement for pre-approval of research protocols by Institutional Review Boards, the need to base the design of human studies on the results of investigations in animals, and the concept of informed consent. Informed consent requires that subjects in a clinical study must be volunteers who understand the risks associated with receiving the drug and undergoing the testing.\nThis first stage of clinical testing consists of “dose finding” or “dose ranging” studies aimed at determining the drug doses that will be safe in humans. The test subjects are typically healthy adults. In certain instances, however, some Phase 1 subjects may actually suffer from the disease or condition for which the drug would be used, and the dose ranging studies may also provide insight into the amount of drug that would be needed to be therapeutically effective. These are small studies, usually using less than a few dozen subjects, and take only a few days. The sponsor will analyze the data and determine whether the drug candidate is ready to proceed to the next stage of testing, or if additional Phase 1 or even preclinical tests are needed. This phase typically takes about one year.\nClinical Phase II\nParticipants in Phase 2 trials are referred to as “patients”, rather than “subjects”, because they have the disease or condition targeted by the drug under investigation. Trials in this phase continue to optimize dosages and begin to examine the effectiveness of the drug. Here, again, the studies are small (usually 100-300 patients) and of relatively short duration. Measurements are taken, within hours or days of dosing, of the amount of the drug that is absorbed in the patient’s blood or of the concentration of some chemical that is indicative of the disease (such as glucose or the protein HbA1C in body fluids of a patient with Diabetes). The second phase of clinical testing and data analysis generally takes about 2 years.\nDuring the course of the development of a new drug, it may be necessary to perform additional preclinical testing to further explore the characteristics of the NCE. More Phase 2 studies could also be conducted to further examine dosages and other parameters in small populations. In the next presentation in this series, we will see what happens when the sponsor believes that enough safety and efficacy information is available to support administering the drug to much larger groups of patients and to justify the potential expenditure of millions of dollars and years of effort in Phase III clinical testing and beyond.""]"	['<urn:uuid:b6f08c79-5cde-4f4d-924f-c462becd1e85>']	factoid	direct	short-search-query	distant-from-document	single-doc	expert	2025-04-22T14:43:01.858830	5	28	1611
17	effect of others apathy vs conformity pressure academic performance motivation	When exposed to others' apathy in academic situations, people with low commitment to academic goals perform worse and give up more easily. In contrast, when exposed to conformity pressure from others, people tend to align with the group's responses even if they know they're wrong, with studies showing people agree with incorrect majority answers about 37% of the time. The key difference is that apathy specifically decreases motivation in uncommitted individuals, while conformity pressure leads people to change their responses regardless of their original motivation level.	['One of the themes in this blog over the years is goal contagion, which is the idea that we often adopt the goals of the people around us. See someone helping others, and you suddenly want to be helpful. See someone being aggressive, and it makes you more likely to engage aggressively with others.\nWhat about apathy?\nIf you see people being indifferent about a task, is that contagious as well?\nThat question was explored in a paper in the August, 2014 issue of the Journal of Personality and Social Psychology by Pontus Leander, James Shah, and Stacey Sanders.\nThey suggested that when people are wavering in their commitment to a goal, then being exposed to apathy decreases people’s motivation to pursue a task.\nIn one study, participants performed twelve GRE analogy problems. Prior to solving these problems, participants did a task in which they responded to words presented in the center of the screen. Prior to seeing the words, pictures were flashed quickly on the screen. These pictures either showed students looking bored or students looking engaged. A control group saw no pictures. The pictures were flashed quickly enough that they could be perceived subliminally. After solving the GRE problems, participants were asked for their undergraduate grade-point average, which is a broad measure of their commitment to academic work.\nParticipants with a high GPA were relatively unaffected by the prime. Those with a low GPA solved fewer analogy problems when they were primed by apathetic students than when they were primed by energetic students or received no priming at all.\nThis result suggests that exposure to apathy can decrease the motivation for people who are already unsure of their commitment to a goal. Another study used primes for apathy and primes for anger. Only primes for apathy led people to perform more poorly on a later test. This finding suggests that apathy is not just creating negative emotion that influences performance.\nAnother study used a more sensitive measure of commitment to academic achievement. Once again, participants were exposed to images of people being either apathetic or not. For half the participants, the images showed academic situations, and for half the participants, the images were of nonacademic situations. All participants then solved anagrams, which they were told were a measure of verbal fluency. For example, they might see the letter ECTAR and would have to form the word CRATE.\nAn interesting pattern of results was obtained. The prime that was not in an academic context had very little influence on people’s behavior.\nThe pictures in an academic context had an interesting influence on people’s behavior. Participants who were not strongly committed to academic achievement spent less time on the anagrams and solved fewer anagrams when they saw pictures priming apathy than when they saw pictures unrelated to apathy. Participants who were strongly committed to academic achievement actually spent more time on the anagrams and solved more of them when they saw pictures related to apathy than when they saw pictures unrelated to apathy.\nThis pattern suggests two conclusions. First, the influence of apathy is situation specific. Second, the influence of seeing apathy depends on a person’s commitment to the goal. People who are not committed to the goal interpret apathy as a signal that they should also give up. People who are strongly committed to the goal actually get even more committed by seeing apathy.\nThe researchers ran several other studies to rule out other interpretations of the study. For example, one study demonstrated that just thinking about the goal does not lead to these effects. The influence of apathy requires that people have either a low or high commitment to the goal.\nWhat does all of this mean?\nWe interpret the actions of the people around us. When we see people acting indifferently to a task, we know that they are expressing a lack of interest in that task. That lack of interest is then related to people’s existing commitment to a goal. When people are wavering in their commitment to a goal, then seeing others who are apathetic nudges them in the direction of giving up. When people are highly committed to the goal, then seeing others who are apathetic actually increases their commitment to the goal.', 'In the 1950’s Solomon Asch undertook a series of landmark experiments that revealed the extent to which conformity rules our lives\nThe original experiment\nIn the aftermath of World War II, a Polish social psychology researcher was trying to find the answer to a very simple question about human behavior –Would a person agree with their friends and colleagues, even if they knew that these people were wrong?\nThis scientist was Dr. Solomon E. Asch. He wanted to understand how people could go along with choices made by others that were obviously flawed. To examine our attitudes towards conformity, he designed a simple, yet powerful experiment using the most basic materials.\nAsch developed a set of cards with lines of different lengths on them. People were asked to compare a target line with a set of lines, and judge which line among them was the same length as the target. Each person responded to a number of such items; all similar in simplicity.\nThe card task was simple enough that when participants were tested individually, the success rate was was over 99%. Asch, however wanted to know what would happen when people took the test in a group setting.\nSo each real participant (called a critical participant) was surrounded by a number of individuals (called the majority). Asch instructed the majority to purposely give consistent but wrong answers to some items.\nAll members of the group were asked to give their response aloud, so that everyone knew each others’ responses. The experiment was designed so that the real participant was never the first to answer. He was also part of the same peer group; a male college student, and was recruited to the study by a member of the majority.\nFalling in line\nInitially, the critical participant gave the correct answer for each set of lines, even when others gave the wrong answer. But as the trials went on, Asch found that when the majority showed consistency in their responses – even if they were obviously wrong – the critical participants began to feel compelled to agree with them.\nSome people held out more than others; but most participants (75%) agreed at least once with others, even though they later admitted that they did not actually believe their responses. Overall, people agreed with the majority’s incorrect answer just over one third (37%) of the time.\nWhy did they agree?\nThe critical participants agreed with the group for different reasons. While some truly doubted their own eyes, others chose to agree with the group to reduce the chances of conflict with the group.\nThis need to conform in spite of disagreement is affected by a number of factors. When the participant was similar to the group (in terms of race, gender, etc), and when they are women; they are just a little more likely to conform.\nWhen the majority included less than 3 participants, people were less likely to conform; but they tended to conform more when they hailed from collectivist cultures. When other researchers used more ambiguous or complex material, they found that people conformed more – possibly because they were less able to trust the validity of their own judgments. People also conformed more if the participant believed that the majority was more powerful or knowledgeable then themselves.\nInterestingly, it can take very little to block this need to conform. When the critical participants had to write their responses instead of saying them aloud, they confirmed far lesser. Also, if even one member of the majority dissented (gave the correct answer), the participant was less likely to conform. Having support for one’s beliefs seems to empower us to share them confidently.\nHow we are influenced by others\nInformational Influence – When a lot of people emphatically give a consistent answer, people tend to second guess themselves. They think that they may be wrong (assuming that the majority is more likely to be right), or that they are missing some information that the others have. Agreeing with the majority also carries the comfort of being insulated from the potentially negative outcomes of a personal error.\nWithout consciously realizing it, most people trust the collective wisdom of the group to be greater than their own; particularly when there is no clear answer. Humans develop a tendency to conform quite early. Young children (3-4 year old) agree with the majority when both sides make sense; but are less likely to agree with implausible arguments even when given by a majority.\nNormative Influence – We like others to like us. Sometimes we agree with others just to keep the peace, or to make them accept us. The desire to be liked by our peers and the need to belong with a group is a basic human need. Falling in with the ideas and behaviors of others is one way to connect with them. Thus, we are more susceptible to conforming with others when we feel isolated and rejected, as we hope that this will allow us to win favor with them.\nSome people are more likely to feel the need to conform. People with more social power – the popular kids – have lots of connections (and thus social support), and thus are less pressured to conform to rules and norms. On the other hand, people with fewer connections may feel the need to maintain those relationships that they do have; and thus conform more frequently. Autistic children are the only ones that show immunity to such social pressure; perhaps since they are less tuned to the social cues that demand conforming behavior.\nShould we worry?\nIn general, conformity serves as a mechanism for survival, since we become a part of a larger (and hopefully more powerful/wise) community. But sometimes, conforming with peers who lead us astray could get us or others into trouble. Children and teens sometimes find themselves doing things with friends that they wouldn’t have otherwise done. Even adults sometimes conform to take the easy route even when they know it’s not a good idea. Researcher Dominic Abrams found that people will even endanger themselves and others when they feel the needs to conform.\nHow to avoid the conformity trap\nSo how can we reduce the pressure to conform when we know that we shouldn’t agree with the majority? There are a lot of little ways that can take the pressure off:\n- Be anonymous: Being unknown – like being on the internet or wearing a mask – can remove the pressure to conform, since no one knows who broke away from the crowd (this is partly why mask-wearing superheros can do things others feel threatened by!).\n- Have support: Gathering even a little support can help the minority voice become confident enough to be heard.\n- Have power (being an expert, being popular, etc): Anything that makes us valuable, popular, or lends weight to our choices can also empower us to challenge the majority.\n- Be different (uniqueness): People who see themselves as different from the group can explain their dissent using their ‘difference’. Someone who is different also feels disconnected from the majority in general; and are thus insulated from the pressure to conform.\nIn situations when we feel that we should break away from the norm and strike out on our own, it may help to focus on one or more of these points.']	['<urn:uuid:0d6cc5da-72f9-426d-a303-627f1184c17c>', '<urn:uuid:34dcfdd9-4d11-46eb-83c3-f86b540b92b4>']	factoid	with-premise	long-search-query	similar-to-document	comparison	novice	2025-04-22T14:43:01.858830	10	86	1936
18	What bacterial supplements help restore intestinal microflora balance?	Probiotics like Linex containing natural lactobacilli bacteria help restore microflora balance. Other options include prebiotics which provide nutrients for beneficial bacteria growth, and synbiotics like Normoflorin which combine both probiotics and prebiotics.	"['The concept of dysbacteriosis( dysboz) of the intestine implies a persistent violation in it of the quantitative composition of beneficial and pathogenic microorganisms. This digestive disorder is quite common, and it occurs equally in both male and female representatives of different age categories. Determine the presence of dysbacteriosis at an early stage is not possible, since it is characterized by an asymptomatic course.\nThe human intestine normally contains a certain number of ""useful"" microorganisms that participate in the act of digestion, maintaining the defenses of the body, and the synthesis of certain vitamins. Violation of such a balance in the body can cause serious diseases.\n- 1 Symptoms\n- 2 Reasons\n- 3 Diagnosis\n- 4 Treatment\n- 4.1 Drug therapy\n- 4.2 Diet\n- 4.3 Folk remedies\nThe nature of the main manifestations of imbalance of the intestinal microflora in an adult depends on the specific stage of the dysbiosis.\n- For the initial stage of imbalance of microflora is characterized by a subtle disturbance of the balance of intestinal microflora. Most often, such a violation is caused by a short-term intake of antibacterial drugs. No typical complaints at this stage a person does not show. In rare cases, rumbling in the abdomen may occur, which can disappear after the antibiotic is stopped.\n- The next stage of dysbiosis is not so inconspicuous for a person as compared to the first one. The second stage is characterized by a complete or partial disappearance of appetite, the appearance of nausea, an unpleasant aftertaste in the mouth, flatulence, as well as a violation of the stool to the side of constipation or vice versa diarrhea. Similar symptoms are also characteristic of some other diseases of the gastrointestinal tract, but most often it indicates exactly the imbalance of microflora in the large intestine.\n- The formation of characteristic for the 3rd stage of symptoms, speaks of the need to initiate appropriate treatment aimed at maintaining the balance of beneficial and pathogenic bacteria in the intestine. The key factor in the formation of symptoms in this case is the detrimental effect of the products of vital activity of pathogenic microorganisms on the wall of the large intestine. The formed inflammatory process manifests itself in the form of abdominal pain( around the navel), pronounced nausea, lack of appetite, presence in the stool of fragments of undigested food.\n- The development of a severe degree of dysbiosis indicates a complete replacement of the positive intestinal microflora by pathogens. Such a condition can cause a violation of absorption of many nutrients and vitamins. Often the true companions of dysbiosis are anemia, beriberi, and a persistent decrease in immunity. In addition to digestive disorders, the fourth stage of dysbiosis is characterized by the appearance of insomnia, chronic fatigue, apathy, down to a depressive state. Severe course of dysbiosis can cause serious pathological conditions in the human body.\nIt is important that the degree of expression of characteristic symptoms depends on the influence of various factors, among which one can single out the lifestyle of the person himself, his immune status, and the general state of the body. The same dose of antibacterial drug is able to cause dysbacteriosis of different severity in different people, depending on the individual characteristics of the organism.\nThe main causes of the imbalance of microflora in the large intestine are:\n- the most common is dysbiosis after antibiotics, namely their uncontrolled use;\n- frequent use of alcohol;\n- regular use in food and drink chlorinated water;\n- hormonal imbalance and violation of deceit substances( excess weight);\n- decrease in acidity of gastric juice;\n- previous gastric surgery( resection);\n- diverticula of skin and duodenum;\n- acute intestinal obstruction;\n- chronic pancreatitis, peptic ulcer of stomach and duodenum, gastritis;\n- weakened immunity;\n- previously transmitted intestinal infections( poisoning);\n- prevalence in the daily diet of carbohydrate food, and a lack of vegetable and sour-milk components;\n- regular stress;\n- inadequate hand and food hygiene;\n- is a chronic inflammatory disease in the body.\nThe doctor-gastroenterologist is engaged in the diagnosis of this diagnosis, as well as the subsequent treatment. Preliminary diagnosis is made on the basis of patient complaints. To confirm the diagnosis of dysbiosis, it is necessary to conduct a number of the following additional studies:\n- EGD - study, or gastroscopy, which allows to obtain a visual picture of the state of the walls of the stomach and duodenum;\n- Rectoromanoscopy , which allows to exclude organic pathology of the colon at a length of about 30 cm. The study is carried out using a special instrument( rectoscope), which is inserted into the rectum;\n- Irrigoscopy .This method implies the acquisition of an X-ray of the large intestine, in which a radiopaque substance was previously introduced.\n- Colonoscopy .This method is a kind of analogue of sigmoidoscopy, except that the site of the intestinal gut has an extent of up to 1 meter.\nIn order to treat dysbacteriosis with maximum efficiency, drug therapy must necessarily be carried out in combination with a special diet.""Formula of success"" in the treatment of imbalance intestinal microflora contains only two links:\n- Complete elimination of the negative impact factor( antibiotics).\n- Elimination of imbalance by artificial replenishment of a population of beneficial bacteria in the large intestine.\nThe first item of the list of drugs used are antibiotics, which are indicated for use in cases of excessive contamination of the pathogenic microflora in the large intestine. The best option would be broad-spectrum antibacterial drugs. In particularly severe cases, it is advisable to prescribe antibiotics of the tetracycline series, fluoroquinolones, and cephalosporins. If the imbalance of the intestinal microflora proceeds in a less severe form, then antibacterial drugs can be replaced by more sparing antimicrobial drugs( furazolidone).\nWith dysbacteriosis, the average duration of treatment with antibiotics is from 7 to 10 days. After passing the course of antibacterial therapy, an obligatory condition is the intake of enterosorbents, which will clear the lumen of the intestine from the products of the disintegration of pathogenic microorganisms.\nTo improve the digestive process, enzyme preparations such as Mezim or Creon are used, as well as hepatoprotective agents( Essentiale).To normalize the lost motor function of the intestine, special stimulants of peristalsis( Trimedat) are used.\nAnd, of course, the most important point of the plan for treating the imbalance of microflora is the restoration of the natural ratio of bacteria in the large intestine. In this issue, the following groups of drugs are helpful:\n- Probiotics .This group of medicines contains in its composition a natural culture of beneficial bacteria, which normally inhabit the lumen of the large intestine of a person, and participate in the act of digestion. The most striking representative of this series is the Linex preparation, which contains natural lactobacilli bacteria. The reception of this drug stimulates the colonization of the lumen of the large intestine with a useful microflora, and ensures its further growth and reproduction.\n- Prebiotics .This group of drugs is a kind of nutrient substrate, which creates favorable conditions for the growth of beneficial microorganisms in the intestine.\n- Syndiotics .This group of drugs is a combination of prebiotics and probiotics. The most striking representative is Normoflorin.\n- In addition to the main therapy, it is necessary to include the intake of immunostimulatory drugs , as well as of vitamin preparations .\nThere are several basic principles of dietotherapy for dysbiosis:\n- The daily diet should be observed.\n- The therapeutic diet for dysbacteriosis should be balanced in the protein-energy ratio. In the daily diet should be present a sufficient amount of dietary fiber.\n- An obligatory condition is the daily use of fermented milk products containing lacto- and bifidobacteria in their composition.\n- The composition of the diet should be based on the individual tolerability of those or other foods. If the human body does not absorb any product, then it should be completely excluded from the diet.\nIf all manifestations of fermental dyspepsia prevail in the symptoms of dysbacteriosis, then the diet should limit or completely eliminate the consumption of foods rich in carbohydrates( baking, potatoes, confectionery, whole milk, raw vegetables).To the use recommended boiled meat, steamed dishes, cottage cheese, cereals.\nIf in dysbacteriosis putrefactive processes prevail in the intestine, then in the diet should be sharply limited consumption of protein and fatty foods. Recommended for use are sour-milk products, cereals and boiled vegetables in a wipe form. A compote made from fresh fruit or dried fruit will also help to suppress the processes of decay in the lumen of the large intestine.\nIf the imbalance of microflora is accompanied by constipation, then it is necessary to ensure sufficient intake of plant fiber in the body. Boiled and fresh vegetables are good stimulants for food in the intestines. A good effect is wheat bran.\nIf diarrhea predominates in the symptoms of a dysbacteriosis, then the daily diet should consist of boiled and wiped food, moderate temperature. Compotes made of blueberries and cherry can help cope with the problem of liquid stool.\nAs a good supplement to the basic treatment and diet therapy, you can use some advice of traditional medicine that will help to cope with the main manifestations of imbalance of microflora in the large intestine.\nTool number 1 .A powerful bactericidal agent that can suppress the growth of pathogenic microflora in the intestines is ordinary garlic. The period of daily use should be at least 2 weeks. To suppress putrefactive processes in the intestine, it is sufficient to use 1 clove of garlic 1-2 times a day. You can consume garlic both during meals and before meals, washed down with water or kefir.\nTool number 2 .Such a popular and useful product of beekeeping as propolis, due to its bactericidal properties, can eliminate putrefactive and fermenting processes in the intestines as soon as possible, and also suppress the growth of pathogenic bacteria. It is enough to take 15-20 drops of alcohol tincture of propolis 2 times a day before eating. The tincture is used in diluted form. You can use drinking water or milk for this. The course of treatment is 3-4 weeks.\nTool number 3. Do not forget about medicinal herbs that have a pronounced antimicrobial and anti-inflammatory effect. Infusions of herbs, mother-and-stepmother, eucalyptus, St. John\'s wort, sage and chamomile are able to suppress the growth and multiplication of pathogens, viruses and fungi in the intestine. To achieve maximum effect, it is necessary to prepare a medicinal harvest from the above mentioned herbs. To do this, take a mix of 1 teaspoon of each herb.1 tbsp. The resulting mixture is poured with 2 cups of boiling water, and infused for 40 minutes. The resulting infusion should be filtered and used warmly on a quarter cup 3 times a day before eating.\nOf course, more detailed information on the methods of treating dysbacteriosis can be obtained from a gastroenterologist who will use an individual approach.']"	['<urn:uuid:7efce85a-c323-4218-ae3d-793901a40b81>']	factoid	with-premise	concise-and-natural	similar-to-document	single-doc	expert	2025-04-22T14:43:01.858830	8	32	1825
19	renaissance neoplatonic movement magical practices northern europe relationship agrippa nettesheim occulta philosophia	Agrippa of Nettesheim (1486-circa 1535) played an important role in the Renaissance by popularizing magical practices and Neoplatonic attitudes in Northern Europe. His De occulta philosophia became the best known manual of Renaissance magic, incorporating both Ficinian magic from the Hermetic revival and Cabalist magic developed by Pico, Reuchlin, and other Renaissance Cabalists.	['Agrippa and the Crisis of Renaissance Thought\nRenaissance and Revolution\nAgrippa of Nettesheim (1486-circa 1535), German occultist and mystic, played an important part in the Renaissance by popularizing in the North those magical practices and attitudes inherent in the Neoplatonic movement that was initiated in Florence by Marsilio Ficino and Pico della Mirandola. The Renaissance ideal of the Magus, the “divine” man with powers of operating on the cosmos and achieving universal knowledge and power—adumbrated in Pico’s famous Oration on the Dignity of Man—found its theorist in Agrippa who wrote a text book on how to become a Magus. His De occulta philosophia was the best known manual of Renaissance magic, incorporating both the Ficinian magic deriving from the Hermetic revival, and the Cabalist magic indicated by Pico and further developed by Reuchlin and the hosts of Renaissance Cabalists. A few years before the publication of the final version of the De occulta philosophia (1583), Agrippa published his De vanitate scientiarum in which he attacked all sciences as vain and useless, including the occult sciences which he was about to expound enthusiastically in his next book. Which of these two sensational works represents the true mind of Agrippa, the one which teaches the techniques of Renaissance magic and promises to lead the student to Pisgah heights of illumination, or the one which casts doubts on those techniques, and indeed on all human hope of valid knowledge of any kind?\nThe reversal of mood from visions of power and “knowledge infinite” to total doubt is believed by Nauert to represent a “crisis” in Renaissance thought. The great Faust figures of literature, both Marlowe’s and Goethe’s, repeat the Agrippan pattern of confidence alternating with despair. Nauert examines the argument that doubt is inherent in magic through its reliance on the irrational, and from his analysis of the De vanitate he concludes that Agrippa’s skepticism may derive, not so much from ancient skepticism and the contemporary revival of Sextus Empiricus, as from the mystical tradition, from the “negative theology” of Pseudo-Dionysius, Cusanus, and others. His use of the ass as the symbol of total “unknowing” has mystical implications, and his insistence that faith in Gospel truth is the only refuge from the uncertainty of human knowledge suggests that Agrippa’s spiritual oscillations might represent the hesitations of a Christian conscience disturbed about the legitimacy of the occult philosophy as much as a swing from credulity to skepticism. One undoubted fact in the confused situation is that Agrippa never abandoned his intensive study of the occult sciences either before, during, or after his attack on their vanity. Coupled with the fact that he published his attack on these sciences before he published his textbook on them suggests a simpler explanation of the two books. When accused as a magician on account of the occult philosophy he could usefully point to what he had said of the vanity of magic in the other book. The life of a Renaissance Magus was not a safe one. Ficino was always afraid; Pico got into bad trouble; and Giordano…\nThis article is available to online subscribers only.\nPlease choose from one of the options below to access this article:\nPurchase a print premium subscription (20 issues per year) and also receive online access to all content on nybooks.com.\nPurchase an Online Edition subscription and receive full access to all articles published by the Review since 1963.\nPurchase a trial Online Edition subscription and receive unlimited access for one week to all the content on nybooks.com.']	['<urn:uuid:533c1214-b9a6-419d-8ae3-bd2ca3882f03>']	open-ended	direct	long-search-query	similar-to-document	single-doc	expert	2025-04-22T14:43:01.858830	12	53	586
20	I'm analyzing brain tissue changes in sensory processing disorders - what specific anatomical alterations are seen in both ringing ears and long-term pain conditions?	Patients with both tinnitus and chronic pain show significant loss of gray matter in specific brain regions. They also exhibit compromised circuit function and neurological changes in these areas.	"['Have you ever wondered what that ringing ""in the ears"" sensation is all about? Well it appears that the brain region responsible for the phenomenon shares a gateway with chronic pain.\nHave you ever wondered what that ringing “in the ears” sensation is all about? Well it appears that the brain region responsible for the phenomenon shares a gateway with chronic pain.\nAbout one in five people have tinnitus, which causes them to hear sounds such as ringing or buzzing that aren’t there. Since previous research determined that not everyone with damaged cochlear nerves or cortical auditory circuits experience tinnitus, it was hypothesized that a higher-level cognitive system is involved. Researchers from Technical University of Munich (TUM) and Georgetown University (GU) took the investigation a step further and found evidence suggesting that tinnitus and chronic pain have a common central gatekeeping system. similar structural and functional systems.\n“The article is a review article which summarizes results from different studies with different methods, subjects, and patients,” one of the authors Markus Ploner, MD, Heisenberg professor of human pain research at TUM, told MD Magazine.\nThe team analyzed multiple studies using a variety of strategies, including functional magnetic resonance imaging, diffusion tensor imaging, and voxel-based morphometry, among others. Each review consisted of groups of patients “large enough to allow reliable statistical inferences,” according to author Josef P. Rauschecker, PhD, DSc, a professor of neuroscience at GU.\nChanges in the same brain circuit, like the ventromedial prefrontal cortex and nucleus accumbens, play a significant role in emotions and perceptual sensations. These regions also monitor the flow of information to the brain, which the authors say is controlled by dopamine and serotonin. The new evidence suggests that similar structural and functional characteristics in those brain regions could help explain both tinnitus and chronic pain. Which, to sum up, means that this region acts as a central gatekeeping system for both conditions, as described in Trends in Cognitive Sciences.\n“The most surprising and insightful part of the study was that tinnitus and chronic pain share the same brain structures and, thus, brain mechanisms,” Rauschecker said.\nOne of the common characteristics observed in both patients with tinnitus and chronic pain was the significant loss of gray matter in the specified brain regions. Furthermore, compromised circuit function and neurological changes were witnessed as well, according to a news release.\n“Both teams involved in this study will continue their research on tinnitus and chronic pain, respectively. The next steps could be to investigate the molecular mechanisms in the circuits involved, so we can ultimately develop more effective drug treatment,” Rauschecker continued.\nWhile the findings are promising, the authors stress that there are still unanswered questions to investigate before clinical interventions can be employed. For example, they need to better understand the structure, function, and metabolism of the brain circuits.\n“This could help for the early diagnosis and early and targets treatment of both disorders,” Ploner concluded. “This is an endeavor which cannot be accomplished by a single research group but needs concerted efforts of researchers from different background and disciplines.”']"	['<urn:uuid:ffa5e90c-8838-4b1c-9c51-71f2e80efdcd>']	factoid	with-premise	verbose-and-natural	distant-from-document	single-doc	expert	2025-04-22T14:43:01.858830	24	29	512
21	what counts as historic building national register	A building eligible for the National Register must include all of its basic structural elements and is created to shelter human activity. Examples include houses, barns, churches, and hotels. Parts of buildings alone (like interiors, facades, or wings) are not eligible independent of the rest of the existing building.	"['U.S. Department of the Interior, National Park Service\nIV. HOW TO DEFINE CATEGORIES OF HISTORIC PROPERTIES\nThe National Register of Historic Places includes significant properties, classified as buildings, sites, districts, structures, or objects. It is not used to list intangible values, except in so far as they are associated with or reflected by historic properties. The National Register does not list cultural events, or skilled or talented individuals, as is done in some countries. Rather, the National Register is oriented to recognizing physically concrete properties that are relatively fixed in location.\nFor purposes of National Register nominations, small groups of properties are listed under a single category, using the primary resource. For example, a city hall and fountain would be categorized by the city hall (building), a farmhouse with two outbuildings would be categorized by the farmhouse (building), and a city park with a gazebo would be categorized by the park (site). Properties with large acreage or a number of resources are usually considered districts. Common sense and reason should dictate the selection of categories.\nA building, such as a house, barn, church, hotel, or similar construction, is created principally to shelter any form of human activity. ""Building"" may also be used to refer to a historically and functionally related unit, such as a courthouse and jail or a house and barn.\nBuildings eligible for the National Register must include all of their basic structural elements. Parts of buildings, such as interiors, facades, or wings, are not eligible independent of the rest of the existing building. The whole building must be considered, and its significant features must be identified.\nIf a building has lost any of its basic structural elements, it is usually considered a ""ruin"" and is categorized as a site.\nExamples of buildings include:\nThe term ""structure"" is used to distinguish from buildings those functional constructions made usually for purposes other than creating human shelter.\nStructures nominated to the National Register must include all of the extant basic structural elements. Parts of structures can not be considered eligible if the whole structure remains. For example, a truss bridge is composed of the metal or wooden truss, the abutments, and supporting piers, all of which, if extant, must be included when considering the property for eligibility.\nIf a structure has lost its historic configuration or pattern of organization through deterioration or demolition, it is usually considered a ""ruin"" and is categorized as a site.\nExamples of structures include:\nThe term ""object"" is used to distinguish from buildings and structures those constructions that are primarily artistic in nature or are relatively small in scale and simply constructed. Although it may be, by nature or design, movable, an object is associated with a specific setting or environment.\nSmall objects not designed for a specific location are normally not eligible. Such works include transportable sculpture, furniture, and other decorative arts that, unlike a fixed outdoor sculpture, do not possess association with a specific place.\nObjects should be in a setting appropriate to their significant historic use, roles, or character. Objects relocated to a museum are inappropriate for listing in the National Register.\nExamples of objects include:\nA site is the location of a significant event, a prehistoric or historic occupation or activity, or a building or structure, whether standing, ruined, or vanished, where the location itself possesses historic, cultural, or archeological value regardless of the value of any existing structure.\nA site can possess associative significance or information potential or both, and can be significant under any or all of the four criteria. A site need not be marked by physical remains if it is the location of a prehistoric or historic event or pattern of events and if no buildings, structures, or objects marked it at the time of the events. However, when the location of a prehistoric or historic event cannot be conclusively determined because no other cultural materials were present or survive, documentation must be carefully evaluated to determine whether the traditionally recognized or identified site is accurate.\nA site may be a natural landmark strongly associated with significant prehistoric or historic events or patterns of events, if the significance of the natural feature is well documented through scholarly research. Generally, though, the National Register excludes from the definition of ""site"" natural waterways or bodies of water that served as determinants in the location of communities or were significant in the locality\'s subsequent economic development. While they may have been ""avenues of exploration,"" the features most appropriate to document this significance are the properties built in association with the waterways.\nExamples of sites include:\nA district possesses a significant concentration, linkage, or continuity of sites, buildings, structures, or objects united historically or aesthetically by plan or physical development.\nConcentration, Linkage, & Continuity of Features\nA district can contain buildings, structures, sites, objects, or open spaces that do not contribute to the significance of the district. The number of noncontributing properties a district can contain yet still convey its sense of time and place and historical development depends on how these properties affect the district\'s integrity. In archeological districts, the primary factor to be considered is the effect of any disturbances on the information potential of the district as a whole.\nIn addition, a canal can be treated as a discontiguous district when the system consists of man- made sections of canal interspersed with sections of river navigation. For scattered archeological properties, a discontiguous district is appropriate when the deposits are related to each other through cultural affiliation, period of use, or site type.\nIt is not appropriate to use the discontiguous district format to include an isolated resource or small group of resources which were once connected to the district, but have since been separated either through demolition or new construction. For example, do not use the discontiguous district format to nominate individual buildings of a downtown commerical district that have become isolated through demolition.\n|National Register Home | Publications Home | Previous Page | Next Page|']"	['<urn:uuid:c9cdb013-30a5-4370-a4fa-b5199038289b>']	factoid	with-premise	short-search-query	similar-to-document	single-doc	novice	2025-04-22T14:43:01.858830	7	49	998
22	My mom has been having some health issues, and I'm concerned about stroke and dementia - what's the connection between these conditions, and how has the risk changed over time?	Stroke is a major risk factor for dementia, but the good news is that the risk has significantly decreased over time. According to research, in the past, having a stroke meant a 90% greater risk of developing dementia. Today, that risk has dropped to 40%, largely due to better control of blood pressure and improved stroke prevention and care. When stroke does lead to dementia (vascular dementia), symptoms can include memory loss, confusion, depression, and problems with planning and organization.	"['Summary: Researchers reveal people may be developing Alzheimer’s later in life and living less time with the neurodegenerative disease.\nSource: UT San Antonio.\nThe diagnosis is one that a family never wants to hear: Your father has Alzheimer’s disease. Your mother has stroke-related dementia.\nA recently released study, included in a special supplement to the Journal of Gerontology, indicates that dementia’s impact might be compressing a bit. That is, people might be developing dementia later and living with it for a shorter period of time.\nSudha Seshadri, M.D., professor of neurology and founding director of the Glenn Biggs Institute for Alzheimer’s & Neurodegenerative Diseases at UT Health San Antonio, is the senior author on the study, which draws evidence from the Framingham Heart Study.\nIn data from four different time periods over a period of 30 years, the mean age at dementia onset increased, while the length of time living with dementia decreased. Is it because prevention and care of stroke today is superior compared to decades ago? Stroke is a major risk factor for dementia.\n“Prevention of stroke and reduced impact of stroke are great advances, but neither completely explains the trend we are seeing,” Dr. Seshadri said. “We are looking at other causes, such as lower burden of multiple infections because of vaccination, and possibly lower levels of lead or other pollutants in the atmosphere. Early education and nutrition might also play a role.”\nStroke risk has decreased because of greater control of blood pressure. Dr. Seshadri again cited Framingham data: “In the past, if you had a stroke you were at 90 percent greater risk to develop dementia. Today, you have a 40 percent greater risk,” she said.\nAbout this neuroscience research article\nSource: Will Sansom – UT San Antonio Publisher: Organized by NeuroscienceNews.com. Image Source: NeuroscienceNews.com image is in the public domain. Original Research: Abstract for “Are Trends in Dementia Incidence Associated With Compression in Morbidity? Evidence From The Framingham Heart Study” by Carole Dufouil, PhD Alexa Beiser, PhD Geneviève Chêne, MD, PhD Sudha Seshadri, MD in Journal of Gerontology. Published April 16 2018. doi:10.1093/geronb/gby001\nCite This NeuroscienceNews.com Article\n[cbtabs][cbtab title=”MLA”]UT San Antonio “Dementia Trend Shows Later Onset With Fewer Years of Disease.” NeuroscienceNews. NeuroscienceNews, 23 April 2018. <https://neurosciencenews.com/later-dementia-onset-8863/>.[/cbtab][cbtab title=”APA”]UT San Antonio (2018, April 23). Dementia Trend Shows Later Onset With Fewer Years of Disease. NeuroscienceNews. Retrieved April 23, 2018 from https://neurosciencenews.com/later-dementia-onset-8863/[/cbtab][cbtab title=”Chicago”]UT San Antonio “Dementia Trend Shows Later Onset With Fewer Years of Disease.” https://neurosciencenews.com/later-dementia-onset-8863/ (accessed April 23, 2018).[/cbtab][/cbtabs]\nAre Trends in Dementia Incidence Associated With Compression in Morbidity? Evidence From The Framingham Heart Study\nObjectives Several epidemiological studies suggest declining trends in dementia over the last three decades with both decreasing age-specific prevalence and incidence. There is limited data on whether this delayed clinical onset is accompanied by a shorter postdiagnosis survival.\nMethods A total of 5,205 participants from the Framingham Original and Offspring cohorts were studied. Four epochs were considered from 1977–1984 to 2004–2008. Gender and education adjusted 5-year mortality risks were estimated using delayed entry Cox models with the earliest epoch as reference category. Stratified analyses by sex, education, and age were undertaken. A nested case control study of 317 dementia cases and 317 controls matched on age, gender and epoch was initiated.\nResults In the whole sample, 5-year mortality risk has decreased with time, it was 33% lower in the last epoch compared to the earliest. In the 317 persons who developed dementia, age at onset increased (1.5 years/epoch), and years alive with dementia decreased (1 year/epoch) over time. We observed however, a decreased adjusted relative mortality risk (by 18%) in persons with dementia in 1986–1991 compared to 1977–1983 and no significant change from then to the latest epoch. The nested case control study suggested in matched controls that 5-year mortality relative risk had increased by 60% in the last epoch compared to Epoch 1.\nDiscussion In the FHS, in the last 30 years, disease duration in persons with dementia has decreased. However, age-adjusted mortality risk has slightly decreased after 1977–1983. Consequences of such trends on dementia prevalence should be investigated.', 'Signs of Dementia\nDoes my loved one have dementia?\nIt\'s a question that many caregivers simultaneously want and dread the answer to.\nThere\'s a common misconception that the primary indicator of dementia is memory loss. The reality is that different forms of dementia have different signs.\nHere are some of the indicators that signal each type of dementia:\n- Memory loss: Early-stage Alzheimer\'s is almost always hallmarked by some form of memory loss. A person may experience difficulty trying to remember a particular word, or the name of someone they just met. They may also be more prone to losing important objects. Often, a person\'s short-term memory is the first thing affected by Alzheimer\'s. Memory loss gradually gets worse until long-term recollections are impacted as well. A person in the later stages of Alzheimer\'s won\'t be able to remember their own name, how to dress themselves, or even how to smile.\n- Trouble performing familiar tasks: A super-organized individual may become scattered as a result of Alzheimer\'s. Not remembering the route to the grocery store, or forgetting something they just read are two additional early-to-mid-stage Alzheimer\'s indicators.\n- Bad judgment: Unsound financial decisions, inappropriate public outbursts and an inability to understand and abide by social norms of cleanliness and grooming are all signs of increasingly poor judgment that may signify Alzheimer\'s.\n- Social withdrawal: People suffering from memory loss may be reluctant to engage in social activities. They are easily overwhelmed by large groups of people, even close friends and family.\n- Sundowning: When the sun goes down, an Alzheimer\'s sufferer may become fearful, agitated and sad. They may pace, hallucinate, shadow their caregiver and wander. This collection of emotions and subsequent behaviors is referred to as ""Sundowner\'s syndrome"" or ""sundowning."" Sundowning can be a sign of virtually any type of dementia, but is most often seen in individuals with Alzheimer\'s disease.\nDiscover more information about the signs, symptoms and treatment of Alzheimer\'s disease.\n- Hallucinations and delusions: Seeing, hearing, and even tasting things that aren\'t real is widely considered one of the first signs of Lewy Body dementia. Also occurring early on in the disease are persistent fictitious beliefs about a particular person or circumstance. For caregivers this particular dementia behavior can seem like manipulation.\n- Sleep troubles: Another early symptom of Lewy Body dementia is known as REM Sleep Behavior Disorder (RBD). A person with RBD will talk and move while still asleep. Once awake, they may struggle to distinguish between their dreams and reality. A person with RBD won\'t always develop Lewy Body dementia, but research suggests that about 66 percent of people with RBD will eventually develop a degenerative brain disorder, according to the Lewy Body Dementia Association.\n- Varying degrees of alertness: A person with Lewy Body dementia may experience periods of lucidity, interspersed with episodes of profound disorientation that may last anywhere from minutes to days. These cycles of confusion and clarity are not tied to any particular time of day, unlike the sundowning behaviors seen in people with Alzheimer\'s disease.\n- Movement issues: A person with Lewy Body dementia may resemble a person suffering from Parkinson\'s because of the effect the disease has on their ability to control their body and perceive their environment. Stiff movements, a hunched over posture and shuffling can all be physical manifestations of cognitive degeneration. These mobility issues also up a person\'s risk for falling.\n- Medication sensitivity: About half of Lewy Body dementia sufferers develop an extreme sensitivity to antipsychotic medications that can result in worsening of existing dementia symptoms, and Neuroleptic Malignant Syndrome, which can lead to death.\n- Memory and cognitive issues: Memory loss is typically one of the last symptoms to show up in people with Lewy Body dementia.\nDiscover more information about the signs, symptoms and treatment of Lewy Body dementia.\n- The symptoms of vascular dementia are different depending on which part of the person\'s brain was impacted by reduced blood flow. Memory loss, confusion, depression, problems with planning and organization, mobility issues and urinary incontinence are all possible signs that a person is suffering from vascular dementia.\nDiscover more information about the signs, symptoms and treatment of Vascular dementia.\nFrontotemporal Dementia (Pick\'s disease)\n- There are several types of frontotemporal dementia (FTD), each with their own set of initial symptoms that gradually begin to intersect as the disease gets worse.\n- Loss of inhibition: Saying and doing inappropriate things is a common sign that a person has developed some form of FTD.\n- Problems with language: A person who consistently has trouble remembering words, or using the right words to describe what they\'re talking about may suffer from FTD.\n- Movement issues: Similar to Lewy Body dementia, FTD can also cause rigid movements and a lack of coordination.\nDiscover more information about the signs, symptoms and treatment of Frontal Lobe dementia.']"	['<urn:uuid:1b0d3d83-a900-4930-ad5a-df40f1b0fc4a>', '<urn:uuid:524fcbc9-1fbf-4cee-87fb-4d81d252d244>']	factoid	with-premise	verbose-and-natural	similar-to-document	multi-aspect	novice	2025-04-22T14:43:01.858830	30	80	1490
23	australian connection armenian genocide evidence archives exist	While Australian newspapers and Anzacs witnessed and documented the Armenian Genocide with many helping to rescue survivors, the Ottoman archives contain approximately 150 million documents spanning that period that remain largely untapped by historians. The Ottoman Ministry of Interior documents and Council of Ministers records contain extensive documentation about the Armenian relocations, and these archives are open to all scholars with helpful staff willing to assist researchers.	"['""Knowledge and information can be powerful tools in the struggle for human rights, especially when secrecy, silence, and denial of atrocities - whether historic or contemporary - continue to violate the rights of people living today"" Clint Curle, Canadian Museum of Human Rights, April 2013.\nThe Armenian Genocide Education Website Australia (AGEWA) is a project of the Armenian Historical Society of Australia (AHSA).\nAGEWA is designed to assist Australian history studies teachers and their students with the tools to respectively teach and learn about a major international human rights issue - the Armenian, Greek and Assyrian Genocides.\nFounded in 2008, the AHSA is a not for profit organisation established with the objective of gathering and recording the contributions of the Armenian-Australian community, researching the mutual history of the Armenian and Australian peoples and promoting and educating the Australian public about major episodes in the history of Armenia.\nAHSA is a member of the History Council of New South Wales.\nThe Genocide of the Armenians perpetrated by the Ottoman Turkish government between 1915 and 1923 is one of the greatest crimes against humanity and civilisation. The systematic campaign of mass deportations and killings adopted by the Ottoman authorities during this period resulted in the annihilation of up to 1,500,000 innocent Armenian men, women and children, and deprived survivors of their once historic homelands and possessions. Furthermore, this event transformed a multi-cultural Ottoman society into a largely mono-ethnic Turkish nation state. According to many Holocaust scholars, the Armenian Genocide provided a blueprint for the Jewish Holocaust and other genocides in the 20th and 21st centuries. Genocide is considered to be the ultimate form of intolerance.\nLittle known, however, is the strong Australian connection to this historic event. Many Anzacs serving in the region bore witness to the Armenian Genocide with some even helping to rescue survivors. Australian newspapers reported the massacres and deportations as they occurred which aroused deep sympathy from the Australian public. This led to a massive humanitarian campaign by Australians to provide aid, food and shelter to the victims and survivors of the Armenian Genocide as they escaped violence.\nThis website provides Australian middle and senior secondary students with Australian resources to learn about this historic event, its relevance to Australia and its present day implications, within the extent of the Australian Curriculum - History.\nThe Teachers Resources page catalogues those areas in the current NSW history syllabus, as well as the Australian Curriculum - History, where there is scope for teaching the Armenian Genocide.\nSourced primarily from Australian archives, background information regarding the Armenian Genocide and related topics such as the Greek Genocide, the Assyrian Genocide, Anzac Eyewitnesses, Australia’s Response to the Armenian Genocide, Genocide Denial and Recognition and Just Resolution can be found on this website.\nThis is supplemented with video, images, maps, reading lists, student exercises and research tools to assist Australian history teachers and students make an independent historical enquiry into one of the most well documented examples of extermination in modern history.\nThe United Nations General Assembly has recognised Genocide as a fundamental human rights issue. As the historical record has demonstrated, any society is capable of committing genocide given the right circumstances. It is therefore not the intention of this website to stereotype any particular ethnic group. The AHSA hopes that through education, these acts are never again repeated.\nThe AGEWA would like to thank Dr Panayiotis Diamadis, history lecturer at the University of Technology, and Vicken Babkenian, independent researcher for the Australian Institute for Holocaust and Genocide Studies, for their assistance in providing research material for the contents of this website.', 'The debate over what happened to Armenians in World War I-era Ottoman Anatolia continues to polarize historians and politicians. Armenian historians argue that Ottoman forces killed more than one million Armenians in a deliberate act of genocide. Other historians—most famously Bernard Lewis and Guenter Lewy—acknowledge that hundreds of thousands of Armenians died but question whether this was a deliberate act of genocide or rather an outgrowth of fighting and famine. In recent decades, the debate has shifted from academic to legislative grounds. In 2001, the French parliament voted to recognize an Armenian genocide. In 2007, U.S. political leaders narrowly averted an Armenian genocide resolution in the House of Representatives. While Armenian activists lobby politicians to recognize an Armenian genocide formally, which is likely to be a first step toward a demand for collective reparations, and genocide studies scholars seek to close the book on the Armenian narrative, it is ironic that many of the archives that contain documentation from the period remain untapped.\nThe Richness of Ottoman Archives\nThere are approximately 150 million documents that span every period and region of the Ottoman realm in the stacks and vaults of the Ottoman Archives. Each day, new collections in these Ottoman archives are opened to researchers. All these extensive records are well preserved and organized.\nThe first published catalog of Ottoman archival holdings appeared in 1955 and consisted of ninety pages of archival inventory and commentary. Archivist Attila Çetin followed in 1979 with a more extensive catalog, which is also available in Italian. As the classifying and organizing of the archives continued, the catalog grew. The 1992 edition is 634 pages long. The expanded 1995 compilation provides access to even more documents. Revised editions are to be forthcoming from time to time, as more detailed descriptions become available for the various fonds or individual record groups.\nOttoman archival documentation constitutes an unequaled trove of information about how people lived from the fifteenth through the early twentieth centuries in a territory now comprised of twenty-two nations. İlber Ortaylı, director of the Topkapı Palace Museum at Istanbul, argues that the history of the Ottoman Empire should not be written without Ottoman sources. He is not alone in this. His position is buttressed by a number of specialists in the study of the Ottoman state and society. Albert Hourani, for example, the late British scholar of Middle Eastern affairs, argued that his best advice to history students considering Middle East specialization would be to ""learn Ottoman Turkish well and learn also how to use Ottoman documents, since the exploitation of Ottoman archives, located in Istanbul and in smaller cities and towns, is perhaps the most important task of the next generation.""\nThe Archives and the Armenians\nThere are few comprehensive sources about Armenian life in Anatolia outside of Ottoman archival sources. Diplomatic records, such as those cited by Armenian historian Vahakn Dadrian, as the basis for discussions among genocide scholars are spotty and intertwined with wartime politics. The Ottoman Ministry of the Interior (Dahiliye Nezareti) was the government department directing and supervising the relocation and resettlement of the Armenian population. The collection of the ministry documents covers the period from 1866 to 1922 and consists of 4,598 registers or notebooks. It is classified according to twenty-one subcollections, according to office of origin. Among the available documents in the Ottoman archives are several dozen registers containing the records of the deliberations and actions of the Council of Ministers, which set policies, received reports, and discussed problems that arose regarding the relocations and other wartime events. The minutes of its meetings, deliberations, resolutions, and decisions are bound in 224 volumes covering the years 1885 through 1922. These registers include each and every decree pertaining to the decision to relocate the Ottoman Armenians away from the war zones during World War I. The Records Office of the Sublime Porte (Babıali Evrak Odası) also contains substantial documentation, including the correspondence between the grand vizier and the ministries, as well as the central government and the provinces that can illuminate the events of 1915.\nIt is ironic, therefore, as politicians seek to deliberate on questions of history, that few historians investigating Armenian issues have actually consulted the Ottoman archives. As Australian historian Jeremy Salt has explained,\nThere is little explanation as to why more historians do not consult the Ottoman archives. They are open to all scholars. Bernard Lewis, Cleveland Dodge Professor Emeritus of Near Eastern Studies at Princeton University, who has worked extensively in the Ottoman archives since 1949, has argued that ""the Ottoman archives are in the care of a competent and devoted staff who are always willing to place their time and knowledge at the disposal of the visiting scholar, with a personal helpfulness and courtesy that will surprise those with purely Western experience. [These records] are open to all who can read them."" The late Stanford Shaw, Professor Emeritus of Turkish and Judeo-Turkish History at the University of California, Los Angeles, also spoke highly of the helpfulness of the archivists. He argued that the sheer amount of new material available removed any excuse for any scholar investigating various nationalist revolts not to spend time examining the new sources.\nEven Taner Akçam of University of Minnesota, one of the most vocal proponents of Armenian genocide claims, noted the improvement in the working conditions of the archives. In a recent article, he thanked the staff and especially the deputy director-general of state archives for their help and openness during his last visit. The archivists are now helpful to all researchers, not only those pursuing research which supports the Turkish government\'s line.\nTurkish Military Archives\nThe archives of the Turkish General Staff Military History and Strategic Studies Directorate in Ankara (Türkiye Cumhuriyeti Genelkurmay Askeri Tarih ve Stratejik Etüt Başkanlığı Arşivleri) provide a military perspective. Indeed, more than the Ottoman Archives in the Prime Minister\'s Office, this repository provides a rich trove of information about internal conditions in the empire, operations of the Ottoman army, and the Special Organization (Teşkilat-ı Mahsusa), somewhat equivalent to the Ottoman special forces, for the period 1914-22.\nThe World War I and War of Independence archives alone number over five and a half million documents spread among Turkish General Staff Division reports and War Ministry files. Division 1 (Operations) contains military operations plans and orders, operations and situation reports, maps and overlays, general staff orders, mobilization instructions and orders, organizational orders, training and exercise instructions, spot combat reports. Division 2 (Intelligence) contains intelligence estimates and reports and orders of battle. Divisions 3 and 4 (Logistics) contain files concerning procurement, animals, munitions, transportation, rations, and accounting. The Ministry of War files contain the General Command\'s ciphered cables to military units as well as the papers of the infantry, fortress artillery, and other divisions. Vehip Pasha\'s Third Army (Erzurum), Jemal Pasha\'s Fourth Army (Damascus), and Ali İhsan Pasha\'s Sixth Army (Baghdad) are included among the staff files. These also include the Lightning Armies and Caucasian Armies groups.\nThe cataloging and microfilming of the military archives repository up to the end of 1922 is complete. Once-secret documents should provide new information on the Armenian issue. In addition to the microfilmed documents, the Turkish General Staff Military History and Strategic Studies Directorate publishes volumes of documents from its collection, including Latin alphabet transliteration of all documents.\nJustin McCarthy, professor of Middle Eastern history and demographer at the University of Louisville/Kentucky, one of the few Western scholars to have done systematic research in the Ottoman archives, has written that the ""reports of Ottoman soldiers and officials were not political documents or public relations exercises. They were secret internal reports in which responsible men relayed to their governments what they believed to be true."" Indeed, the military records have already called into question conventional wisdom about the Special Organization, namely, the organization\'s involvement in the Armenian relocations. \nOther Ankara Resources\nThe Turkish Historical Society (Türk Tarih Kurumu) at Ankara is also open to the public. The society houses private collections relating to strategy and political matters in the twentieth century, which include the papers of World War I-era war minister Enver Pasha together with those of his chief aide-de-camp and brother-in-law, Kazım Orbay. The Enver Pasha collection, donated in 1972 by his daughter Mahpeyker Enver, consists of 789 single, disparate items of handwritten notes, memoranda, reports, military records, cards and invitations, dispatches, letters of appreciation of colleagues and opponents, photographic albums, topographic maps, charts, private correspondence, diaries, and miscellany for the period 1914-22. There are no restrictions on access to these. Because in the early decades of the twentieth century it was customary for officials to keep their papers upon their departure, these remain a relatively rare resource. Orbay\'s papers add additional insight because they enable historians to gauge which issues most occupied the Ottoman Empire\'s highest ranking military official of the time. Few scholars have used this last collection perhaps because they remain unaware of it.\nThe National Library (Milli Kütüphane) at Ankara houses thousands of Muslim court records, most of which were transferred from local museums and offices scattered around Turkey. These records contain a vast array of information concerning imperial administration, city government, the affairs of townspeople and villagers and deal with almost every aspect of the lives of the subjects be it personal status, taxes, loans, sales, price regulations, complaints, flight, or theft. Any matter requiring official resolution, registration, verification, or adjudication was potentially the domain of the Muslim judge (kadı) even when the matters applied to non-Muslims, such as Armenian Christians. Many Turkish historians have employed Muslim court records extensively for Anatolian regional studies, but they remain relatively untapped by Armenian historians.\nSole reference to Ottoman archives will not and should not satisfy historians; a full study of the Armenians during World War I should consider material from all sides in a conflict. The Armenian community maintains a number of archives. The archives in Watertown, Massachusetts, contain repositories from the Dashnak Party (Dashnaksutiun, the Armenian Revolutionary Federation) and the First Republic of Armenia. Both of the above together with the archives of the Armenian patriarchate in Jerusalem and the Catholicosate, the seat of the supreme religious leader of the Armenian people, in Echmiadzin, Armenia, remain closed to non-Armenian researchers. Tatul Sonentz-Papazian, Dashnakist archivist, for example, denied İnönü University scholar Göknur Akçadağ access to the Watertown archives in a June 20, 2008 letter. Dashnaksutiun archives are also not available to those Armenians who do not tow the party line. Historian Ara Sarafian, director of the Gomidas Institute in London, complained that ""some Armenian archives in the diaspora are not open to researchers for a variety of reasons. The most important ones are the Jerusalem Patriarchate archives. I have tried to access them twice and [been] turned away. The other archives are the Zoryan Institute archives, composed of the private papers of Armenian survivors, whose families deposited their records with the Zoryan Institute in the 1980s. As far as I know, these materials are still not cataloged and accessible to scholars."" Beyond the closure of Armenian archives to non-Armenian and even to some Armenian scholars, few of these allow the public to access catalogs detailing their holdings.\nMany scholars writing on the Armenian question utilize Britain\'s National Archives (formerly the Public Record Office) in Kew Gardens. While the British government has made available many of their diplomats\' reports for study, much material from the British occupation of Istanbul (1919-22) and elsewhere in Anatolia following World War I remains closed to researchers under the Official Secrets Act and are only partially available in the archives of the government of India in Delhi. British authorities say they remain sealed for national security reasons. Their release should be important to historians as they will include evidence regarding returning Armenian refugees and other related matters. Files of the British Eastern Mediterranean Special Intelligence Bureau also remain closed, perhaps because the British government does not wish to expose those who may have committed espionage on behalf of Britain. These are important because they should enable historians to research British espionage and sabotage, demoralizing propaganda, and attempts to provoke treason and desertion from Ottoman ranks during and immediately after 1914-18. The documents of the Secret Office of War Propaganda, which under the direction of Lord James Bryce and Arnold Toynbee developed propaganda used against the Central Powers during World War I, also remain sealed. Their opening will allow historians to assess whether British officials in the heat of war created or exaggerated accounts of deliberate atrocities.\nAn International Historians\' Commission\nHistory cannot be decided by politicians weighing either constituent concerns or emotions more than evidence. Nor should the debate on history be closed while the existing narrative utilizes only a small portion of the source material. The same holds true not only for Armenian historians but also for their Turkish counterparts and others.\nRather, historians should work together to consider all source material, both in Armenian and Turkish archives. Each should be open fully. Cherry-picking documents to ""prove"" preconceived ideas and to ignore documents that undercut theses is poor history and, in a politicized atmosphere, can do far more harm than good.\nOn April 10, 2005, Turkish prime minister Recep Tayyip Erdoğan extended an invitation to Armenian president Robert Kocharian to establish a joint commission consisting of historians and other experts to study the developments and events of 1915, not only in the archives of Turkey and Armenia but also in those of relevant third countries such as Russia, Britain, France, Germany, Austria-Hungary, and the United States, and to share their findings with the public. Ninety-seven members of the Council of Europe\'s Parliamentary Assembly at Strasbourg signed a declaration calling on Armenia to accept the Turkish proposal. In his annual commemoration message to the Armenian-American community in 2005, President George W. Bush expressed support for Turkey\'s proposal, declaring, ""We look to a future of freedom, peace, and prosperity in Armenia and Turkey and hope that Prime Minister Erdoğan\'s recent proposal for a joint Turkish-Armenian commission can help advance these processes."" Secretary of State Condoleezza Rice reiterated the point two years later, telling Congress,\nIt is unfortunate that the Armenian government has failed to accept the joint commission, for without joint consideration of all evidence, the wounds of the past will not heal and, indeed, when an incomplete narrative enters the political realm, the consequences can be grave.\n See, for example, Vahakn N. Dadrian, The History of the Armenian Genocide (Providence: Berghahn Books, 1995), p. xviii.']"	['<urn:uuid:003ac89c-f822-4e96-9cda-ca35d05fea31>', '<urn:uuid:151d250a-98c3-48c2-813c-f72750e775ba>']	factoid	with-premise	long-search-query	distant-from-document	multi-aspect	expert	2025-04-22T14:43:01.858830	7	67	3011
24	How long does it usually take for someone to set up a windmill near their pond, and do they need any help with the installation?	Installation typically requires two people and takes about 1.5 days total. The first day is needed for two people to build the tower and head assembly, while an additional half day is required to physically set the windmill, anchor it, and run the airlines and diffusers. Most people complete this work over a 2-day weekend or across several evenings. It's important to ensure you have plenty of time and don't rush the process.	['Notice of Shipping Delays: Due to COVID-19, shipments may be delayed 5 – 7 days, but please rest assured your order will still be delivered to you. We appreciate your patience in these difficult times.\nFrequently Asked Questions\nHere are the answers to some of the questions that come up often. If you have other questions, feel free to contact us.\nUsually, this happens when you have low wind speeds or winds coming from multiple directions vs. a head on wind. The rocking back and forth is due to the backpressure on the system. If you have your diffuser at 12’, then it will take about 5 pounds of pressure to push air thru the airlines. To build up 5 lbs. of pressure it might take a 4, 5 or 6 mph wind to get the head to turn. This is a normal part of operation, especially at low wind speeds or if you have inconsistent winds. If you have tree’s or buildings within 200-300’ of your windmill, they might block the wind or cause the wind to “swirl”, which can sometimes prevent your windmill head from finding a straight on wind.\nAt low wind speeds, the windmill might only be producing .5 to 1.0 CFM of air. This is sufficient for one diffuser, but probably isn’t enough air to operate two diffusers. As your wind speed picks up, the second and even third (if you have one) diffusers should kick in if your valves are adjusted appropriately.\nAir will always go to the point of least resistance. If one diffuser is a little shallower or has a shorter airline, it will want to take most of the air. You’ll have to restrict the airflow to this diffuser with a valve system so that air is forced out of your other air stones.\nYes, you can use a hydraulic needle valve to adjust the airflow if you want. These can be purchased online at Northerntool.com or Grainger.com or in your local farm store. Most have a ¼”, 3/8” or ½” female NPT threads on both sides. You can purchase a brass fitting with male threads on one side and a hose barb on the other side and connect it in-line if you want a more precise adjustment.\nWe sell several different styles of diffusers. Some have their own bases and some do not. The air stone diffuser does not have a base so we put it inside of a bucket to keep it out of the mud and the muck on the bottom and to also weight it down into something so it sits on the bottom flatly and pushes air and water upwards. We recommend adding some rock, sand or concrete to the bottom of the bucket for weight and tying a marker onto the bucket so that you can raise and lower it or move it if needed.\nYour air stone diffuser will give you the best performance if you clean it every year or two. If you raise it up and it looks clean, then just lower it back down. If it has mud, algae or other things clogging the pours, then it should be cleaned. You can use a 10% solution of muriatic acid or give it a gentle cleaning with a steel brush. Putting it into a clean bucket of water and blowing air through it after cleaning is a good way to make sure the pores are open and that it’s operating correctly. If you have a rubber membrane diffuser, it may not need to be cleaned. You can check it by blowing air through it to check it’s performance.\nVery little. We use high quality sealed bearings so there is nothing to grease inside of the compressor. You might have to change out the diaphragm and check valves every 4-7 years. Other than that, you should periodically check your air stone diffuser and clean it if necessary every year or two.\nThis can vary greatly. If this is your first windmill, then figure on taking one day with two people to build the tower and head assembly and a second ½ day to physically set the windmill, anchor it and run your airlines and diffusers. Most people do this over a 2 day weekend or over several evenings. The key is to make sure you have plenty of time and don’t get in a big hurry.\nWe strongly recommend using a secondary anchor system. We supply 4’ ground stakes but encourage people to set these into concrete (either as a footing or as a pad). The second alternative is to use a screw in earth anchor and to set this in the middle of the windmill tower. You can then use plastic coated cable and tie into the three sides of the windmill to secure it.\nYou can set your windmill up to 1000’ feet away from the pond and still aerate it. We see minimal friction loss with air, especially with our larger ½” airline.\nYes, we do carry a full line of replacement parts for most aeration windmills. We manufacture these to our high quality performance specs and offer a guarantee that they’ll work or we’ll give you your money back. We have replacement blades, blade braces, hub assemblies, domes, tail arms, tail fins, tower pieces, stake clamps, diaphragms and check valves. All of our parts are offered at a lower price than the original equipment manufacturer. We have a parts warehouse in the U.S., Western Canada and in Eastern Canada.\nTower hinges are our #1 accessory and make it much easier to install a windmill or to service it. Weighted airline is #2 in that it lets your airline sink to the bottom of the pond vs. having it floating on the surface. This keeps it from getting caught by boats, swimmers, fishing hooks, etc.. Either a freeze control system or pressure relief valve is also very popular in that both protect the compressor in climates where you have may freezing. Lastly, we sell a lot of air stone housing buckets and air stone markers.\nIn Northern climates, we recommend either a freeze control tank or a pressure relief valve in most situations. The freeze control tank insures your airlines stay open in that they release a small amount of isopropyl alcohol into the airline to unthaw it if it freezes due to condensation. The pressure relief valve is an alternative to the freeze control tank and releases back pressure at about 20 psi. This protects the compressor should your airlines ever freeze but doesn’t unthaw them. If you’re running multiple diffusers, you may not need a freeze control tank , especially if you split the airlines coming down the windmill tower as you have a lower probability of multiple airlines freezing at once. Please call and we’ll evaluate your situation.\nTheoretically, we can pump water using an airlift pump. We do not currently sell any water pumps but airlift pumps are available online. The key is depth and submergence as bot are critical when using an airlift pump.\nWe have technicians that will be able to come by to identify and recommend products to help get unwanted growth under control. We can then apply the products ourselves, or guide you step by step on the application process.\nWe will generally be able to make it out within a week of the request.\nAeration has several beneficial effects on ponds and lakes. For one, it breaks up the thermal stratification in the water, allowing dissolved oxygen to make its way through the whole body of water, rather than just surface level water. This has a beneficial effect for the fish because they will be able to forage deeper than before do to an abundance of oxygen. Additionally, when dissolved oxygen is present at the bottom of the pond, it allows certain aerobic bacteria to decompose organic matter buildup. This will increase water health over time by reducing nutrient content in eutrophic ponds, making it harder for aquatic vegetation to thrive.\nAeration will not kill the algae already present in a pond. It will, however, lower the nutrient load in the pond, making it harder for future algae growth.\nIt depends. Since no two ponds are the same, there is a lot to consider when developing a treatment plan for a specific body of water. We offer comprehensive water quality testing to gain insight into what is going on at the chemical level in the water. This is the best way to ensure that the treatment plan is targeting the correct problems and imbalances in the water. Another thing we consider is what the client’s end goal for the body of water is. For example, if the client is wanting to raise a trophy bass lake, where vegetation is needed, applying chemical would be a last resort. On the other hand, if the end goal is a swimming hole, excess vegetation would be a nuisance and would likely require chemical application.\nYes! We are trained professionals and have extensive experience in repair of all manufacturers of floating lake fountains.\nWe do not do any kind of mechanical removal of vegetation or physical cleaning of koi ponds.\nYes, we sell and apply products meant to seal leaks in ponds.\nWe have licensing & are insured in Arkansas to commercially apply aquatic pesticides.\nCombined our service department has over 22 years of experience in this industry.\nWe are staffed an Aquatic Biologist with a Bachelor’s in Science in Biology from the University of Arkansas.\nOur services are below market average and affordable for the average pond owner.\nWe will travel anywhere within, roughly, a 60 mile radius from Springdale, AR.\nYes, we offer full installation for all our products locally. We build and test each system here in house before each installation.\nYes, we have 16 foot Skiff boat and a kayak for shallow waters.\nWe offer consultations for local ponds as well as comprehensive water quality testing. We offer one time visits, maintenance programs for floating lake fountains and lake aerations, as well as a contractual program. With a lake management contract, a technician would come out twice a month in the Spring and Summer months and once a month all other months. While there, we would assess the needs and wants of the client and proceed accordingly. If there is trash bordering the pond, we will pick it up.']	['<urn:uuid:b279a38f-9ffa-4594-a205-89e25c629e3b>']	open-ended	direct	verbose-and-natural	distant-from-document	single-doc	novice	2025-04-22T14:43:01.858830	25	73	1738
25	What drives wetland biogeochemistry and how does it impact water quality?	Wetland biogeochemistry is largely driven by carbon biogeochemistry, with iron and sulfur redox cycling being significant components affected by microbial activity. These processes impact water quality by controlling nutrient and contaminant mobility, greenhouse gas emissions, and carbon sequestration. Additionally, factors like acid sulfate soils can release acid and heavy metals into waterways when exposed to air, affecting pH levels and dissolved oxygen content.	['The long-term goal of the Argonne Wetland Hydrobiogeochemistry Scientific Focus Area is to develop a mechanistic understanding and ability to model the coupled hydrological, geochemical, and biological processes controlling water quality in wetlands.\nWetlands are landscape features where standing water is present all or part of the time. Within wetlands, the movement of water and the biogeochemically catalyzed transformations of its constituents determine the mobility of nutrients and contaminants, the emission of greenhouse gasses into the atmosphere, carbon (C ) cycling and sequestration in subsurface environments, and the quality of water itself. In terms of mass action as well as the extent of integration with other elemental cycles, biogeochemical processes within wetlands are largely driven by C biogeochemistry. Iron (Fe) and sulfur (S) are also abundant elements that are often found in wetlands. Their redox cycling is driven largely by microbial activity, and they are a significant component of major and minor elemental cycling and energy flux. In addition to greatly affecting elemental cycling and transport, wetlands provide many critical ecosystem services, such as water storage. While water availability is often a concern within arid regions of the western United States, higher population densities, continued residential and industrial development, and the importance of water in energy production also create strong concerns about water quality in the more humid parts of the eastern United States. In addition, extreme weather events and encroachment by humans can contribute to changes in water inputs and outputs, as well as the biogeochemical interactions within wetlands and the biota associated with them. Finally, although they are important, the explicit hydrologically driven wetland biogeochemical functions that control C, nutrient, and contaminant cycling and water quality are still not understood well enough to be adequately modeled. This is due in part to deficiencies in our current understanding of the complex and dynamic hydrologically driven biogeochemical processes that occur in wetlands.\nThe long-term, 10-yr objective of the Argonne Wetland Hydrobiogeochemistry Scientific Focus Area (SFA) is the development of a mechanistic understanding and ability to model the coupled hydrological, geochemical, and biological processes controlling water quality in wetlands and the implications of these processes for watersheds commonly found in humid regions of the United States. To accomplish this, the Argonne Wetland Hydrobiogeochemistry SFA is studying wetland hydrobiogeochemistry with a focus on a riparian wetland field site at the Savannah River Site (SRS). The wetlands lie within Tims Branch at the SRS, whose waters ultimately flow into the Savannah River approximately 20 km downstream. This site is representative of many riparian wetlands found in the humid regions of the Southeast that have C‑rich soils and high Fe content. However, it is unique in that it received large amounts of contaminant metals (Ni, Cr, Cu, Pb) and uranium (U) as a result of previous industrial-scale manufacturing of fuel and target assemblies at the M-Area Facility. Therefore, the function of the wetlands in relation to control of water quality, including the concentration of metals and U within the soluble and particulate components of groundwater and surface waters of Tims Branch, is a risk driver for DOE Environmental Management.\nMembers of the Argonne Wetland Hydrobiogeochemistry SFA, whose efforts are directly supported by the SFA or bring complementary expertise via synergistic collaboration, represent the critical components required to obtain our 10-yr objective. Members of the team from Argonne who are directly supported by the SFA include Kemner (PI, Biogeochemistry, Synchrotron Science), O’Loughlin (Co-PI, Biogeochemistry, Environmental Chemistry/Microbiology), and Weisenhorn (Wetland Ecology and Microbial Modeling). Members of the team from outside of Argonne who are directly supported by the SFA include Boyanov (Bulgarian Academy of Science, Physical Chemistry, Synchrotron Science), Kaplan (SRNL, Soil Science), Seaman (Savannah River Ecology Laboratory, University of Georgia, Study-site instrumentation and data collection), and Segre (Illinois Institute of Technology, Synchrotron Science, Assoc. Dir. of the Materials Research Collaborative Access Team/Environmental Collaborative Access Team [MRCAT/EnviroCAT] at the Advanced Photon Source [APS]). Members of the SFA who are not directly supported by SFA funds are supported through a variety of other means. Many have been collaborating with the SFA for many years, have a proven record of productivity, and have alternative sources of support to synergistically work with the SFA team. Through other collaborative projects supported by other means, some institutions provide additional support to the project by supplying graduate students and postdocs, and by supporting visiting researcher sabbaticals at Argonne. Members of the team from specific institutions and their scientific expertise are Clemson (University class field trips to map U concentrations in the field, EPSCoR project): Powell (Actinide Chemist, geochemical and reactive transport modeling), Martinez (Radiobiologist), and Moysey (Geophysics); Florida International University (DOE Environmental Management): Lawrence and Mahmoudi (Hydrological Modeling of Tims Branch watershed); Pacific Northwest National Laboratory (PNNL) (Environmental Molecular Science Laboratory [EMSL]): Hess (Environmental Chemistry, EMSL CESD Integrative Lead), Tfaily (Organic Chemist), Pasa-Tolic (EMSL Science lead for mass spectrometry); Korea University: Kwon (Environmental Engineering, Biogeochemistry); China University of Geosciences (111 Project for Visiting Researchers, Groundwater Contamination and Remediation): Yan (Geochemistry), Wang (Biogeochemistry and President of the China University of Geosciences), Dong (Microbiology, Biogeochemistry), Deng (Wetland Biogeochemistry); Universidade Federal de Alfenas: Rodriguez (Environmental Engineering, Biochemistry, wastewater treatment and floc formation); Argonne National Laboratory (KBase): Henry (Metabolic Modeling, KBase PI at Argonne); Lawrence Berkeley National Laboratory (LBNL) (ENIGMA, KBase): Adams (Microbiology, ENIGMA Co-PI), Arkin (Microbiology, ENIGMA PI, KBase PI at LBNL), and Chakraborty (Microbiology, ENIGMA-supported researcher); University of Leeds: Mishra (Synchrotron characterization of C and S speciation).\nThe overarching hypothesis of our work is that hydrologically driven biogeochemical processes that create redox dynamic conditions from the nanometer to meter scales are a major driver of groundwater and surface water quality within riparian wetland environments. A focus on major (e.g., C and Fe) biogeochemical cycles and their controls on U hydrobiogeochemistry and water quality within the Tims Branch wetlands builds on decades of synchrotron-based biogeochemistry and omics-based microbial community research expertise previously developed within the Argonne Subsurface Biogeochemical Research (SBR) Program SFA. In addition, the focus on hydrobiogeochemistry within a riparian wetland in the Southeastern United States complements and expands the portfolio of existing SBR SFA field site testbeds concerned with the Biogeochemistry of Actinides (LLNL SFA), mountainous watersheds (LBNL SFA), shallow alluvial groundwater systems (Stanford Linear Accelerator [SLAC] SFA), hydropower-impacted and highly managed rivers (PNNL SFA), and critical terrestrial-aquatic interfaces within streams (ORNL SFA) in both arid and humid regions of the United States that play major roles in controlling groundwater and surface water quality. As such, this Science Plan aligns with the SBR program within the Climate and Environmental Science (CESD) Division of the DOE Biological and Environmental Research (BER). In addition, because wetlands, by definition, encompass both terrestrial and aquatic environments, our investigation of the Tims Branch riparian wetlands and watershed complement burgeoning efforts within the Terrestrial Ecosystem Science Program of BER to better understand critical earth system terrestrial-aquatic interfaces.\nWe identified three major components (focus areas) of the Tims Branch riparian wetland that represent critical zones containing hydrologically driven biogeochemical drivers, which determine water quality: sediment, rhizosphere, and stream. These three focus areas are interdependent and must be considered as a whole for a systems-level understanding of our overarching hypothesis. Within these three focus areas, we identified two common thematic knowledge gaps that inhibit our ability to predict controls on water quality:\n- In-depth understanding of the molecular-scale biogeochemical processes that affect Fe, C, and contaminant speciation within the wetland sediment, rhizosphere, and stream environments; and\n- In-depth understanding of hydrologically driven biogeochemical controls on the mass transfer of Fe, C, and contaminants within wetland sediment, rhizosphere, and stream environments.\nHolistically addressing 18 specific hypotheses related to these two knowledge gaps within the sediment, rhizosphere, and stream environments organizes the SFA in its development of a hydrobiogeochemical conceptual model of the Tims Branch riparian wetland.\nFrom the beginning of this project, we strongly emphasize our integrated lab- and field-based experimental research and field monitoring. This enables us to derive key model parameters as we simultaneously develop the mechanistic understanding necessary for modeling approaches to predict wetland controls on water quality. Materials collected from sediment, rhizosphere, and stream environments will be analyzed to identify major hydrologically driven biogeochemical processes that affect surface and ground water within wetlands. Besides standard analysis approaches, we will use unique capabilities (synchrotron-based approaches at the APS at Argonne, omics-based approaches and collaborative research with KBase researchers at Argonne and LBNL, and mass spectrometry-based approaches to characterize C chemistry through collaboration with scientists at the EMSL at PNNL) to analyze field and lab materials.\nThe modeling approaches we are integrating with our work in the early years of this project are limited primarily to reactive transport models (RTMs) of lab-based mixed batch reactors, diffusive and advective flow experiments, and small-scale field observations. We supplement RTMs with metabolic modeling of organisms and interacting consortia to more accurately capture the role of microbial communities in mediating biogeochemical transformations. These models are used for hypothesis testing, and to analyze the sensitivity of hydrobiogeochemical processes in determining water quality. In addition, the iterations between lab- and field-based experiments in concert with modeling improve both aspects of the work. In later years of the SFA, we envision a stronger emphasis on field-scale observations and characterizations and their coupling with hydrological system model development of the Tims Branch riparian watershed to explain the particulate and soluble fractions of nutrient and contaminant element transport via surface water and groundwater flow.\nA preliminary hydrological model has already been developed for the Tims Branch. It is critical for ultimately modeling coupled hydrological and biogeochemical controls, and it provides key insights and data related to surface water and groundwater flow within the watershed. These are critical for identifying hydrological drivers of the biogeochemical processes to be studied and to be integrated into RTMs. Finally, the hydrological model of Tims Branch (~6 km in length) we will develop—with reactive transport models embedded within it—will enable us to explore the effect of these integrated models on the National Water Model, which makes use of 1-km grids.', 'Issues affecting water quality\nMaintaining good water quality is essential to human health, the environment, agricultural industries and the recreational value of waterways, wetlands and coastal waters.\nAwareness of the environmental conditions and human activities that influence water quality is an important part of effective water management.\nStrategies for managing these issues are developed by state and territory governments, local government councils and shires, or regional organisations such as natural resource management bodies, supported by national guidance contained in the National Water Quality Management Strategy (NWQMS).\nWater managers and jurisdictional agencies can:\n- use the nationally agreed guidelines for managing water quality to help develop water quality management strategies, plans and regulatory arrangements\n- apply the guidance in Characterising the Relationship between Water Quality and Water Quantity to understand issues affecting water quality and assist managers in making informed decisions about how water is managed in the landscape to maintain and improve its quality.\nAcid sulfate soils\nAcid sulfate soil is the common term for soil that contains naturally-occurring chemical compounds known as metal sulfides. When soil containing metal sulfides is exposed to air it can pose a risk to water quality by releasing acid and other contaminants, such as heavy metals, into waterways and wetlands once rewetted. This causes a reduction in pH and dissolved oxygen levels.\nGuidance is available to help jurisdictional water and land managers identify and manage acid sulfate soils. The guidance provides information to prevent, minimise, mitigate and remediate the harmful effects that disturbance of acid sulfate soils can have on water quality, aquatic ecosystems, farming and fishing, and built infrastructure.\nResources for managing acid sulfate soils\n- National Guidance for the Management of Acid Sulfate Soils in Inland Aquatic Ecosystems — for the identification and management of inland acid sulfate soils, to reduce or eliminate the risks they pose to the Australian environment and economy.\n- National Strategy for the Management of Coastal Acid Sulfate Soils — a holistic and comprehensive approach to defining the acid sulfate soil problem, prevent it from increasing and encourage remedial actions to reduce existing acid water runoff.\nBlackwater events are a natural feature of lowland river systems and occur when a build-up of leaf litter and debris on floodplains is washed into waterways during flooding. The high amount of organic material in the water is then consumed by bacteria, depleting dissolved oxygen in the water.\nManaging the effects of blackwater events is the responsibility of jurisdictional water authorities and land and water managers. Although backwater events can cause localised fish kills, they also serve to redistribute essential organic matter and nutrients through the landscape.\nBushfires can affect water quality particularly when there is heavy rain immediately after a fire has occurred. Altered soil structure and loss of vegetation cover in the aftermath of a fire can increase the risk that sediments and pollutants will run off into waterways.\nDrinking water, water for agricultural activities and local aquatic ecosystems can be negatively affected by rapid surface-water runoff after bushfires.\nPrimary responsibility for the management of these risks is at the state and territory, regional and local level, where it is important for authorities to minimise the threat of intense bushfires and respond quickly after a major fire to stabilise the soils and facilitate natural recovery of vegetation.\nIn late 2009, the Australian Government commissioned The University of Melbourne to conduct a review of existing literature on the impacts of bushfires on water quality in Australia. The review considers the water quality impacts of bushfires on various uses and values, including drinking water, aquatic ecosystems and agriculture, and recommends management actions for before, during, and after a fire to minimise impacts on water quality.\nCyanobacteria (blue-green algae)\nCyanobacteria (blue-green algae) are microscopic, algae-like bacteria that bloom in still or slow flowing water, where there is abundant sunlight and high nutrient levels.\nBlooms can occur in freshwater, coastal and marine waters.\nRisks to human and livestock health, agriculture and the environment posed by blue-green algae make it a priority issue for water quality managers, jurisdictional agencies and the Australian Government.\nSalinity refers to the level of salts in soil or water, and is a natural feature of the Australian landscape. Human activities including vegetation clearance, poor land management and irrigation practices can increase salinity levels in soils or waterways.\nManagement of salinity, and the human activities that contribute to it, are planned and enacted by jurisdictional water authorities and land and water managers.\nCities and urban areas produce large volumes of runoff that needs to be appropriately managed.\nThe quality of urban stormwater can be significantly impacted by point and diffuse sources of contamination from industry and transport, water treatment facilities and residential homes.\nUrban water catchments tend to have large areas of impervious surfaces such as roads, rooftops and pavement which:\n- increase the velocity of water flow\n- inhibit ponding and the infiltration of water into soil.\nUrban streams are an important feature of our cities and suburbs and play a major role in stormwater management and flood control. While these streams tend to be modified or engineered, they are an important part of the urban landscape providing valuable recreation, aesthetics and biodiversity conservation areas.\nDuring rainfall events, stormwater flows can rapidly enter urban streams carrying high levels of nutrients, sediment and heavy metals. This produces water with high biological oxygen demand (BOD) and low dissolved oxygen levels. Rapid changes in water quality can affect receiving aquatic environments, such as coastal waters, estuaries, rivers and wetlands.\nEffects on aquatic environments include:\n- outbreak or establishment of invasive aquatic species\n- smothering of aquatic plants\n- toxicity to aquatic organisms\n- closure of recreational access due to the introduction of pathogens, the promotion of algal blooms or other contamination issues such as odours, harmful skin reactions and visual amenity.\nAs urban development in Australia grows, along with increasing demand for fit for purpose water for a wide variety of uses, managing urban water quality will present an increasingly complex challenge for governments and the community.\nThe NWQMS has 3 guidelines relevant to the management and use of urban stormwater:\n- Urban stormwater management provides a nationally consistent approach for managing stormwater in an ecologically sustainable manner. Water managers can use the guidelines to:\n- identify objectives for stormwater management\n- integrate management activities at the catchment, waterway and local development level.\n- Water recycling — augmentation of drinking water supplies covers the use of recycled water (including stormwater) to boost drinking water supplies.\n- Water recycling — stormwater harvesting and reuse provides guidance on managing potential public health and environment risks associated with the reuse of:\n- roof water collected from non-residential buildings\n- urban stormwater from sealed areas, including drains waterways and wetlands.']	['<urn:uuid:80c9ac2d-2df6-4419-85c6-3643a02145ee>', '<urn:uuid:eaf8e8f5-7ae3-4a33-8586-c85d45814b2b>']	factoid	direct	concise-and-natural	distant-from-document	multi-aspect	expert	2025-04-22T14:43:01.858830	11	63	2790
26	beer recipe methods flavor retention how long alcohol remains cooking time different dishes	Beer can enhance recipes in various ways - it acts as a meat tenderizer, yeast enhancer for breads and pancakes, and deglazing agent. The type of beer should complement the dish - dark beer for robust dishes and light beer for lighter fare. However, it's important to note that alcohol doesn't completely evaporate during cooking. After 15 minutes of cooking, 40% of alcohol remains; after one hour, 25% remains; and even after 2.5 hours, 5% still remains. The alcohol retention depends on heat and surface area - hotter temperatures and larger pans result in more alcohol evaporation. As a general rule, alcohol content decreases by 10% every 30 minutes of cooking, up to 2 hours.	"[""Cooking with beer is as old as drinking it, a practice enjoyed by many over the thousands of years of beer-making. In ancient Egypt, Sumerians believed that cooking with beer was a way to bring “healthy food” to the people. Today, beer lovers continue to infuse recipes with this intoxicating concoction of hops and barley in an effort to enhance and enrich everyday food.\nMethod 1 of 4: Cooking with Beer\n1Know your beer. There are three main types: these are ales, stouts and lagers. Ales and lagers tend to be best for using in cooking, although stout has its place too, such as in some versions of Christmas pudding.Ad\n2Select a beer that complements and enhances the food. The right type of beer should be considered before you cook. One rule of thumb to follow is like wine––use dark beer for robust dishes and light beer for lighter fare.\n- Generally, pale ale beer complements nearly all recipes. Beer intensifies during cooking, so a lighter tasting beer may lend more of a blended flavor than a darker beer.\n- Nut brown beer is ideal for rich dishes such as stews or cheese dishes.\n- Strong Belgian ales can complement meat dishes. Most meat, especially red meat, dishes will require the use of dark brown ale rather than the lighter version.\n- Fruity beers work well with desserts.\n- Wheat ales can enhance seafood and poultry dishes.\n- Lager beer is works well for baking breads because it adds levity to the dough. Beer can be used instead of yeast in pancakes and some breads.\n- Adding beer to batters produces a light and crispy texture.\n3Evaluate the malt and hop levels. Malt and hops are the flavoring agents in beer. If the levels are higher, more beer flavor will come out in your dish.\n4Determine the role that the beer will play in your cooking. Enhancing the flavor of the food is not the only role beer can have in cooking. It is also a natural meat tenderizer, yeast enhancer that complements bread and pancakes. It is also a deglazing agent that can re-infuse a dish with cooked ingredients.\n5Choose a reputable brand. Never cook or use beer that you wouldn’t enjoy drinking. Remember that price doesn’t always dictate flavor, so if you're not already sure, sample the beer before adding it to your dish.\n- Don't be afraid of using stale beer though. Last night's unfinished beer, provided it was refrigerated, can be used to cook with. After all, it's probably flat and no good for drinking now!\n6Follow the directions in the recipe. Beer is meant to enhance and enliven the flavors of your food, so add this ingredient specifically according to directions. Overdoing the addition of beer could overwhelm the flavor of the food and cause it to be unpalatable.\n7Bring the beer to room temperature before cooking. Beer that is too cold or hot may compromise other ingredients in your dish. If your brew has been in the refrigerator, make sure you take it out and give it time to get to room temperature before adding it to your dish. Only do otherwise if a recipe overrides this specifically.\n8Remember to use a measuring device created for liquids. Some measuring cups are actually made for dry ingredients such as flour or sugar. Purchase a measuring cup system from the local home store that is designed specifically for liquids to obtain an accurate reading.\n9Start cooking. Below are some typical dishes made using beer. Experiment with a few to see what your favorites will be.Ad\nMethod 2 of 4: Beer soup\n1Assemble the following ingredients:\n- 2 liters (0.5 US gal), 3½ pints, 9 cups chicken or vegetable stock\n- 300ml, ½ pint, 1 1/4 cups German beer\n- 250g, 9 oz stale bread (crusts removed)\n- Salt and pepper to season\n- Freshly grated nutmeg to season\n- 100ml, 4 fl oz, 7 tablespoons single (light) cream.\n2Pour the stock into a saucepan.\n3Add the German beer and stale bread.\n4Season to taste with salt and pepper.\n5Cover the saucepan. Cook over a very gentle heat for half an hour.\n6Remove from the heat. Allow to cool enough to blend.\n7Puree in the blender. Add a little grated nutmeg and the cream. Check the flavor.\n8Reheat. Serve scalding hot.Ad\nMethod 3 of 4: Beer pancakes\nThe beer will provide the rising action needed to make the pancakes work.\n1Assemble the following ingredients:\n- 2 cups all-purpose/ plain flour\n- 2 cups beer\n- 2 eggs, slightly beaten\n- 2 tbsp honey or maple syrup\n- Some butter.\n2Pour the beer, eggs and syrup or honey into a mixing bowl. Mix well.\n3Sift the flour into the wet ingredients. Mix well with a whisk. Aim for a thin and lumpy mix.\n4Heat a little butter in a frying pan. Spoon in the batter and cook the pancake. Use medium heat.\n- Flip the pancake over when the bubbles appear on the surface and the pancake edge is firm.\n- For fluffier, thicker pancakes, use a pancake ring.\n- Add things such as chocolate chips, berries or chopped bananas as wished.\n- A half cup of wholemeal flour can be substituted for a half cup of plain flour if desired.\nMethod 4 of 4: Rosemary beer bread\n1Preheat oven to 375ºF/190ºC.\n2Gather the ingredients. You will need:\n- 3 cups self-rising flour (self-raising flour)\n- ½ cup sugar\n- 12 ounces of pale or light beer\n- 1/3 cup fresh chopped rosemary\n- 2 tablespoons of melted butter\n3Coat a loaf pan with butter and set aside.\n4Combine the flour, sugar, rosemary and beer in a large bowl and mix well.\n5Pour the mixture into a loaf pan.\n6Bake for about 55 minutes, or until the bread is risen and a knife inserted into the center comes out clean.\n7About three minutes before the bread is finished, brush melted butter over the top.\nGive us 3 minutes of knowledge!\nSome other delicious beer recipes\n- Tell diners or guests beforehand that you included beer as an ingredient to the dish. Some people have allergies to wheat or hops.\n- Consider serving a different type of beer with your beer-cooked dish to complement different flavors.\n- Try different beers to create an appreciation of different flavors and tastes\n- Although the alcohol level cooks down in most recipes, tell anyone who is avoiding alcohol that the dish contains beer.\n- Avoid using old or outdated beer in your recipes––like any ingredient, throw it away once it has expired.\n- Not all beer is vegetarian or vegan; be aware if you're following such a diet and using beer for cooking.\nSources and Citations\n- http://www.npr.org/templates/story/story.php?storyId=113747902 – research source\n- http://www.globalgourmet.com/food/egg/egg0397 – research source/beertips.html#axzz1ZkTfVTC3\n- http://www.foodnetwork.com/recipes/calling-all-cooks/beer-bread-recipe/index.html – research source\n- Beer soup recipe adapted from Larousse Gastronomique, Beer soup, p. 89, (2009), ISBN 978-0-600-62042-6.\n- Beer pancake recipe adapted from http://captainjaxestreasureisland.blogspot.com/2008/08/beer-pancakes.html.\nIn other languages:\nThanks to all authors for creating a page that has been read 54,972 times."", 'Does Alcohol Evaporate from Cooking Wine?\nHow much alcohol remains after a dish is cooked? You might be surprised...\nThere\'s nothing like hanging out with friends and family at a summer picnic and grabbing a hot, beer-boiled bratwurst right off the grill. The alcohol-saturated meat is tender and moist, and yes, thanks, you\'ll have seconds.\nCooking food in alcohol or adding it to food is, of course, nothing new. Wine, spirits and beer are commonly used to add a burst of flavor and aroma. Think coq au vin, lager-spiked turkey chili, or pork brined in rum before cooking. Then there are specializes wines often thought of more for cooking than drinking — marsalas and the like.\nAnd just about everyone, including many professional chefs and backyard grillers, believes that all the alcohol added to a meal during the cooking process evaporates (or dissipates), leaving behind only a faint aroma and subtle taste.\nAre they right? Is your Bud-soaked brat ""innocent"" when it comes off the grill, or will you get a buzz from eating five of them? (Actually, after that many brats, a buzz might be the least of your worries.)\nSorry to spoil the party, but here\'s the real deal: Simply heating alcohol, or any other cooking liquid, does not make it evaporate as quickly as a child\'s allowance in a candy store. The longer you cook, the more alcohol cooks out, but you have to cook food for about 3 hours to fully erase all traces of alcohol. A study from the U.S. Department of Agriculture\'s Nutrient Data lab confirmed this and added that food baked or simmered in alcohol for 15 minutes still retains 40 percent of the alcohol. After an hour of cooking, 25 percent of the alcohol remains, and even after two and a half hours there\'s still 5 percent of it. In fact, some cooking methods are less effective at removing alcohol than simply letting food stand out overnight uncovered.\nConsider a Brandy Alexander pie made with 3 tablespoons of brandy and 1/4 cup of creme de cacao. According to data from the Washington Post, the pie retains 85 percent of the alcohol in these ingredients. Main dishes follow the same scenario. In scalloped oysters, for example, with 1/4 cup dry sherry poured over the works and then baked for 25 minutes, 45 percent of the alcohol remains.\nHow about a chicken dish prepared and simmered with 1/2 cup of Burgundy for 15 minutes? Forty percent of the alcohol in the wine remains. A pot roast made with a cup of Burgundy and roasted for more than 2 hours, however, retains only 5 percent.\nThe extent to which alcohol evaporates during cooking depends on two main things: heat and surface area. Hotter temps will burn off more alcohol, and a bigger pan with more surface area will produce the same result.\nAs a reference, here\'s a helpful rule of thumb: After 30 minutes of cooking, alcohol content decreases by 10 percent with each successive half-hour of cooking, up to 2 hours. That means it takes 30 minutes to boil alcohol down to 35 percent and you can lower that to 25 percent with an hour of cooking. Two hours gets you down to 10 percent.\nAnother tip: It\'s always a very good habit to cook with the same kind of high-quality wine that you\'d choose to pour into a glass. A wine\'s flavor intensifies during the cooking process, so if you\'re making a sauce spiked with an old bottle of Thunderbird, the result will reflect it. Incorporate a quality wine instead and enjoy its flavor all the way through the meal.\nReady to decant?\nInterested in cooking with wine? This Classic Chicken in Red Wine uses 2 1/2 cups of wine, simmering the chicken in a wine-stock sauce for 40 minutes before cooking it down to thicken for an additional 10 minutes. These garlicky White Wine Mussels steam in a broth made with a cup of something nice and dry. Bottle-of-Red Wine Sauce is no misnomer: the meaty chuck-laced sauce calls for an entire bottle of robust red, simmered for 90 minutes, then cooked down for another hour.\nRemember, too, that any remaining alcohol in a dish can be a big deal — or even dangerous — for anyone who doesn\'t drink. Plan and cook accordingly.']"	['<urn:uuid:8e7dbc46-dcc2-4f06-a882-efc9e16cbe9e>', '<urn:uuid:0b8ae0a9-ccdb-4cad-a8c3-6e9ac188a11e>']	open-ended	with-premise	long-search-query	similar-to-document	multi-aspect	expert	2025-04-22T14:43:01.858830	13	115	1900
27	I'm interested in Latin American literary movements - can you tell me what distinguishes the post-Roberto Bolaño generation of writers from previous ones?	The post-Roberto Bolaño generation is distinguished by two main characteristics: their global reach of themes and their refusal to embrace Magical Realism as a signature style.	"['Traveler of the Century\nBy Andrés Neuman, translated by Nick Caistor and Lorenza García\nFarrar, Straus and Giroux, 576 pages, $30\nThere was a time when Latin American writers felt compelled — maybe the word is “constrained” — to focus their work on a single topic: Latin America. Jorge Luis Borges sought to change this provincial attitude. In a 1951 lecture titled “The Argentine Writer and Tradition,” he called on his colleagues to see beyond the ends of their noses. “What ought to be the themes of Argentine letters?” he asked. His answer was blunt: Western Civilization, by which he meant everything. If Shakespeare, who never left England, is celebrated for plays set in Italy and Denmark, why shouldn’t an Argentine write about a small German city in the 19th century?\nAndrés Neuman, a young writer who was born in 1977 in Buenos Aires and grew up in Spain, takes Borges at his word. “Traveler of the Century,” published in Spanish in 2009 and awarded the prestigious Alfaguara Novel Prize the same year, is magisterial in the vein of Thomas Mann’s “The Magic Mountain,” or Hermann Broch’s “The Sleepwalkers.” That is, it is large and philosophical and deliciously slow — an old-fashioned kind of narrative, less interested in pleasing the tyrannical literary market with fast, easy satisfactions than in bamboozling it through sustained ruminations on politics, God and the nature of things. Most important, it is a novel that doesn’t mention Borges at all. Nor is it concerned with the fate of the Argentine soul.\nNamed by the British magazine Granta as one of the most promising writers from Latin America, Neuman is an essayist, poet and short story writer as well as a novelist. Despite his tender age, he has published almost 20 books, including a volume of aphorisms. (What is it about Latin Americans that causes us to redefine the adjective “prolific”? Might it be the sense that we have to work twice as hard in order to validate our talents?) Neuman belongs to what could be labeled the post-Roberto Bolaño generation. What distinguishes it from its predecessors is the global reach of its themes and the refusal to embrace Magical Realism as a signature style.\n“Traveler of the Century” is easily summarized. The time is around the 1820s. A strange traveler by the name of Hans arrives at the appropriately named city of Wandernburg, on the border between Saxony and Prussia. He has no past and no future. He is passionate about Voltaire. He has no urgent task, which means he can stay as long as he pleases. Only when he’s exhausted the place will he depart. For Hans, that means exploring the city’s inner life. He spends his days meeting bizarre characters, like an organ grinder with whom he engages in probing philosophical conversations.\nThis encounter leads to others. Soon Hans is in touch with a vast array of human types, among them Professor Mietter, a sharp conservative thinker. He also meets a beautiful, provocative, insightful and erudite young woman named Sophie Gottlieb. In the tradition of the epistemological novel, nothing much happens. Or a lot does, depending on your point of view.\nUsing the typology of Isaiah Berlin, Neuman is not a hedgehog but a fox — that is, he knows many things, rather than one big one. “Traveler of the Century” is astonishingly complex in its theological, metaphysical and scientific interests. One has the impression that Immanuel Kant’s “Critique of Pure Reason” has been streamed into a fictional meditation. Reading the book, I was mesmerized by Neuman’s attention to historical detail and his patience with the circumvolutions of the human mind. Sophie, for instance, who is intended to marry local gentry dullard Rudi Wilderhous, keeps Hans on edge. Their affair is soft and intermittent. When Sophie finally kisses Hans after a twisted courtship, the encounter makes the reader shiver.\nI love this book, and not only because of its melodic cadence, superbly rendered by translators Nick Caistor and Lorenza García. I’m also entranced by its reactionary attitude. I took it with me on a lecture trip to Reno, Nev., where I stayed in a casino hotel. The book’s philosophical patience was the perfect antidote to the superficiality of our anodyne, consumerist culture. Its pages seem to scream: Enough with gambling, with the forgetfulness of gambling, with the insatiable drive to accumulate material goods. Neuman, an Argentine writer, has given us a lesson on how to transcend what Borges called “our provincialism” — the obsession with looking at our immediate environment as the only explanation of who we are: He has written a book about the world-less world of ideas.\nIlan Stavans is the Lewis-Sebring Professor in Latin American and Latino Culture at Amherst College. His translation of Juan Rulfo’s “The Plain in Flames” is due out from University of Texas Press in September.\nThis story ""Stopping To Think About the Roses"" was written by Ilan Stavans.']"	['<urn:uuid:7ef20c29-a21a-4b35-b6c6-08ff9e70f798>']	factoid	with-premise	verbose-and-natural	similar-to-document	single-doc	expert	2025-04-22T14:43:01.858830	23	26	822
28	what check venue acoustics power outlets during walkthrough event space	During a venue walkthrough, you should check the number of electrical outlets, ask about building circuits to prevent power failures, and listen to room acoustics since large hollow spaces can make music echo and low-ceilinged spaces can become uncomfortably loud with large groups.	"[""From the humblest backyard wedding to the grandest networking gala, every event depends on its location to set the tone. Finding a suitable venue that matches the mood and theme of your event, as well as your budget and logistical needs, can feel like trying to solve a complicated logic puzzle. If advanced mathematics isn't your strong suit, don't despair; there are easier ways of finding the location that's perfect for you.\nWhat Type of Venue?\nDifferent types of events have different needs. A wedding venue reflects the couple's personality, whereas a corporate meeting requires a venue with fewer distractions. Take care to match the ambiance to the theme; a Victorian ballroom for an elegant gala, or a sleek, skyline-view loft for a modern art auction. Decide what kind of experience you want your guests to have, and keep that vision at the forefront of your mind as you visit possible locations.\nCommon types of event venues include: convention facilities, which will be equipped for large numbers of people, easily accessible from airports, and up-to-date with the latest technology. Conference and banquet halls deal with events on an ongoing basis and will be set up to receive caterers, bands, and more with ease. Hotels usually have large event rooms, and these will offer overnight accommodations as well. Retreats offer a more private setting for meetings, usually with workshops or courses offered. Resorts can handle a large number of guests with luxurious accommodations. Many restaurants have private rooms for large parties, or can be rented for the evening.\nNot every event requires a grand setting. If you're planning an intimate gathering with a small number of guests, you can use local parks or beaches for free or a very low fee. Be sure to check with the area's administration before holding an event somewhere public, though. It's not the height of romance for a security guard to interrupt your wedding vows.\nIf you're planning a destination wedding, keep in mind that your choice of venue may limit which of your guests can attend. Be sure that the location can be easily reached without prohibitive travel costs if you want to be accessible to more family and friends.\nKeep an open mind. Even if you have a vivid mental picture of your location already, you might be surprised by how much an unexpected venue can stand out in guests' memories. Investigate local nightclubs, art galleries, museums, theaters, yachts, and even zoos and nature preserves for their hosting possibilities. Many non-traditional venues are now offering perks specifically to attract special events.\nOn Your Walkthrough\nTry to visit when conditions best match the time and day of your event. An event venue that looks great during the night may be dingy by day, or a sleepy weekday resort may be swamped on the weekends.\nWhom are you inviting? Your choice of venue can limit the number of guests, and most buildings have hard limits to comply with fire codes. Likewise, a too-large venue will make a small number of guests feel uncomfortable. Not only should there be enough room for the number of guests you invite, but there should be enough bathrooms and facilities to avoid uncomfortable lines. Visualize your party navigating the space. Are the hallways wide enough for wheelchairs? Could uneven ground trip older guests? Will there be a good flow, or are certain areas prone to congestion? The venue's management should be able to provide you with some layout suggestions.\nCheck the number of electrical outlets and ask about the building's circuits. You don't want the power to fail if your DJ plugs in while your caterer is blending the soup. Also, listen to the room's acoustics. Large, hollow spaces can make music echo, and low-ceilinged spaces can become uncomfortably loud when large groups are talking. Also, check that traffic noise isn't audible from the street. Furthermore, there should be ample parking for the number of guests attending, or you can inquire about hiring a valet. Ask if neighborhood safety is a concern.\nQuestions to Ask\nNo matter what type of venue you choose, make sure the venue staff is easy to work with. Even the most pristine beach isn't worth the hassle of an owner who nickels and dimes you to death. In your initial tour, ask questions about the venue's policies and get a feel for how flexible they are. You want to find someone who will go out of his or her way to accommodate you.\nFind out about all regulations ahead of time, including opening and closing time and amplified sound curfews. Will you need to put down a deposit? What happens if you have to cancel your event? Are you responsible for damages? Ask about safety regulations and fire codes, especially if you plan to involve candles, pets, or large numbers of people. Does the venue provide overnight accommodations for out-of-town attendees? If any portion of your event will be held outside, are there contingency plans for weather? Is there a smokers' area? Is there WiFi? Make sure the venue doesn't book multiple parties in a day, or that your location will not be affected by other parties if they do.\nWho will be there on the day of your event to assist you? Ask ahead of time if you'll be obligated to use the venue's staff instead of hiring your own. Many locations come with an in-house DJ, caterer, security staff, and more. If you use their caterer, are you allowed to bring in outside food? You don't want your mother to have to leave her special blueberry cobbler at the door.\nAre you allowed to decorate, or to remove existing decorations? Some venues have strict rules, and you’ll want to find those out before you try to tape or nail anything to a wall. Will you be responsible for renting things like tables and chairs, or does the venue provide enough for your invitees? If you need to rent materials, factor it into the venue's price.\nDon't be afraid to negotiate. If the venue is at the high end of your budget, many places will give you a better price if you book them on off-season times of year or days of the week. Be sure to book your date early, though; most venues do fill quickly, especially during peak season.\nRemember, choosing your venue doesn't have to be a stressful process. Before you even begin walkthroughs, take a moment to visualize your event from start to finish. Think of your guests arriving, mingling, and leaving. The more clearly you can picture your event ahead of time, the easier it will be to pinpoint exactly which type of venue will meet your needs.""]"	['<urn:uuid:dc16b357-ec5f-4e77-aba2-2480bf8f28ef>']	factoid	direct	long-search-query	similar-to-document	single-doc	expert	2025-04-22T14:43:01.858830	10	43	1119
29	compare voluntary carbon offsets mandatory carbon offsets definition types differences	Voluntary carbon offsets and mandatory (compliance) offsets differ in several key aspects. Compliance offsets are those that companies purchase to meet legal obligations such as cap-and-trade schemes like the ETS in Europe. In contrast, voluntary offsets are purchased at companies' discretion to balance out their residual emissions. The voluntary carbon market is currently opaque and fragmented, with only $300m worth of trading in 2019, while needing to grow 15-fold by 2030 to support climate goals. Both types of offsets involve financing projects that deliver verified emissions reductions, typically in developing countries, through activities like renewable energy, energy efficiency, waste management, tree-planting or carbon removal technologies.	['Net-zero emissions companies is one of the fastest-growing business trends. According to scientists achieving net-zero before 2050 is critical to keeping us safe from the catastrophic consequences of climate change.\nThe number of net-zero emissions commitments has doubled this year, as many prioritize climate action in their recovery from Covid-19 ( Data-Driven EnviroLab report).\nStill, many organizations struggle to make their first steps to become Net-Zero companies.\nIn this article, you will learn what net-zero companies are, why embark in such endeavour and how to make your net-zero targets credible.\n- 1 What is a Net-Zero company?\n- 2 Why should a company be net-zero?\n- 3 How to start a net-zero roadmap?\n- 4 Using Offsets in net-zero targets\n- 5 Conclusions\nWhat is a Net-Zero company?\nNet-zero company refers to an organization who reaches a balance between emissions produced due to its activities and those emissions it removes from the atmosphere.\nAchieving a net-zero emissions state is similar to maintaining the water level of a bathtub with the tap fully on and its drain open.\nTo achieve the water balance, we could turn down the faucet (reduce the emissions released) until there’s no more water coming out. Besides, we could also drain an equal amount down the plughole (removing emissions from the atmosphere).\nTherefore, a company achieves a net-zero state by first reducing to the minimum its carbon sources. Then, the organization can balance out the remaining emissions by investing in projects that remove emissions (carbon sinks).\nWhy should a company be net-zero?\nReason 1 – Is the right thing to do to save the planet\nAfter the signature of the Paris Agreement in 2015, science has become widely accepted. The cause for climate change is due to human activities (read my article about Milankovitch Earth’s Cycles). Therefore, companies have increasingly focused on reducing carbon emissions.\nNevertheless, emissions kept growing. As a consequence, we have reached a global warming of 1 degree Celsius above pre-industrial levels (UN IPCC SR15).\nTo limit global warming to the safe threshold that science has set at 1.5°C, reducing carbon emissions alone is not sufficient. We need to go further and reach a Net-Zero future where we stop emissions growth.\nCOVID-19 has reminded us that human and, planetary health are interlinked and, this is one of the reasons that Net-Zero emissions companies has become one of the fastest-growing business trends.\nDecreasing GHG emissions also reduce air pollution and, prevent millions of premature deaths. Also, shifting to energy efficiency and renewable energy aligns with efforts to improve energy security and reduce poverty (IPCC, 2018).\nReason 2 – It makes business sense\nBesides, companies have understood that achieving net-zero emissions is not only the right thing to do to save the planet. Net-zero also reduces climate risks (read my TCFD article), reduce costs and attracts ESG investors and talent.\nDespite these two reasons, some companies don’t understand the benefits of the low carbon economy yet. Ignoring those benefits means ignoring the risk that new regulation, customers or investors could put your business out of the market.\nHow to start a net-zero roadmap?\nAccording to Race to Zero, a campaign led by the UNFCCC Champions for Climate Action, 25% of global CO2 emissions are covered by net-zero commitments. Moreover, a recent report by the Energy and Climate Intelligence Unit (ECIU) and Oxford Net Zero affirms that 21% of the world’s 2,000 largest public companies, representing sales of nearly $14 trillion, now have net-zero commitments.\nHowever, there is a great degree of inconsistency in the scope, timeline and actions necessary to reach net-zero. This inconsistency creates confusion and puts off many companies who want to start their net-zero journey.\nIn a Linkedin poll I recently launched, +50% of the answers were to understand how to start the net-zero journey.\nSo how should you create a roadmap to make your company net-zero?\nIt requires you to follow a 4-step process:\n1- Understand your carbon footprint\nYou cannot improve what you don’t measure. Therefore, developing a basic map of your emissions in both your operations and in your supply chain should be the first step.\nHere, the GHG protocol is the most widely used international accounting tool for greenhouse gas emissions accounting.\nCalculating the emissions from the companies operations such as factories, offices or, fleet is quite a straight forward process. It requires collecting fuel and electricity consumption and applying some international emissions coefficients.\nBeyond the company’s operations, there are other emissions produced in the supply chain. Moreover, according to CDP, supply chain emissions are on average 11.4 times greater than those generated from companies operations. Therefore, the potential for reductions is vast in the supply chain.\nBusinesses must close the ‘Say : Do’ gap; the greenwashing space between their environmental pledges and (lack of) actions to meet themPaul Polman, former Unilever CEO\nYour company should build a first simple supply chain emissions model focusing on company spending. Above all, the purchasing of products and services is the category that usually drives most of the emissions. Then using some tables called EEIO, the company can translate spending into carbon emissions.\nOver time, you can improve the model by incorporating data from suppliers and the company’s products life cycle assessments (LCA).\nStill, if the company doesn’t have the carbon footprint expertise, it may be wise to use external help to develop the first model. Here my article about supply chain carbon footprinting and here a list of experts in carbon footprinting or LCAs.\n2 – Set credible and ambitious net-zero targets\nCredibility and ambition are vital aspects when setting a net-zero target.\nThe level of ambition of a target depends on its scope, timeline aligned with science and the strategy behind:\n- Scope: your company should consider including 100% of the emissions produced in your factories, offices, warehouses, fleet and other assets that you own or operate. Besides, since most companies have the majority of their emissions from their supply chain, your targets should also cover those emissions. 52% of the 160 largest GHG emitters committed to net-zero before 2050 and half of them cover the full scope of emissions according to a benchmark from The Climate 100+, a 575 investor initiative with $54 trillion assets under management.\n- Timeline: net-zero targets should be no later than 2050. Your stakeholders also will appreciate that you set interim targets 2025, 2030 to review your progress. Of course, companies that can afford to go faster should do, although without compromising the level of abatement in their targets.\n- Strategy: validating your target against the Science-Based Target Initiative (SBTi) criteria will provide confidence to your stakeholders that your targets are aligned with the latest climate science. The other option is to disclose transparently the methodology your company has used to calculate its targets.\nAccording to Dexter Galvin, Global Director of CDP Supply Chain, there are six benefits of setting a science-based target. Brand reputation, investor confidence, Resilience against regulation, Increased innovation, Bottom line savings and Competitive edge,\n3 – Create programs and internal capabilities\nAchieving a net-zero target will require building a carbon emissions program and the capabilities to implement them.\nCarbon Emission Programs\nAs explained in the bathtub analogy, your company will need to reduce emissions and increase carbon removals.\nTo avoid or reduce emissions, the company can invest in energy efficiency or power the business with renewable energy. Besides, companies can contribute to fighting climate change by developing low-carbon products, services and low-carbon technologies that reduce their customers’ carbon emissions.\nThe company can also decide to invest in low-carbon projects inside of its value chain, also called insetting or outside of its value chain by using offsets.\nTo increase carbon removals, the company must promote carbon removals projects, also called carbon sinks, within its operations or in its value chain. Besides, companies can finance carbon sequestration projects outside its value chain. Typical projects are those related to nature-based solutions such as protecting or restoring forests, or soil carbon sequestration.\nBesides, projects such as Carbon Capture Sequestration and Utilization (CCSU), Direct Air Carbon Capture and Storage (DACCS) or Bioenergy with Carbon Capture and Storage (BECCS) technology allow to capture and store carbon.\n|Avoid or Reduce Emissions||Increase Carbon Removals|\n|Energy Efficiency, Renewables in company’s operations||Nature-Based Solutions, CCSU, DACCS in company’s operations|\n|Low-carbon Services or Products|\n|Finance Low-carbon projects (insetting-offsetting)||Finance carbon absorption projects|\nCarbon Emission Capabilities\nFor both reducing carbon emissions and increase carbon removals, you will need to build internal capabilities.\nYour company should create a robust central sustainability team to set the foundations and coordinate the full carbon emission program.\nOver time, this central team will train experts embedded in other functions of the company. With this knowledge transfer, functions such as procurement, product design, communications, or investor relations will develop their function sustainability programs aligned with the overarching strategy.\nAs important as having the right capabilities in the organization is having robust incentives. Incentives for senior leaders who will cascade down emission targets into their organizations.\nAlso, your company can use instruments such as an internal carbon price to influence investment decisions and internalize externalities. 23% of companies use an internal carbon charge of $27 per metric ton in the EU, while in Asia, it’s $18, according to research done by McKinsey & Company. These prices are still below the $50 to $100 per ton needed by 2030 to achieve the Paris Agreement’s reductions.\n4 – Report progress\nThe last step is Voluntary sharing progress against net-zero targets.\nYour company should use existing formal reporting by using an integrated report. The report should be aligned with broader climate-related reporting such as the Taskforce on Climate-Related Financial Disclosures (read TCFD article).\nBesides, it could use sustainability reporting platforms such as CDP. These platforms allow your company to gain a competitive advantage by getting ahead of regulatory changes and identifying growing risks. Moreover, it will enable your company to find new opportunities for action that your investors, employees and customers are demanding.\nThis is the decade of climate action. We have got to get it done. And when we are busy in the office, I remind my colleagues that this is the market that we hoped forBill Goldie, director of offsetting at Redshaw Advisors\nThese four steps are required to be revised continuously and, perfect is the enemy of the good. Start with simple objectives, simplify as much as possible and aim at solving the most material issues first. Aim at improving year after year using feedback from the company’s stakeholders.\nUsing Offsets in net-zero targets\nMandatory vs Voluntary Carbon Offsets\nCompanies should reduce their emissions according to the latest science.\nThe organization should balance out residual emissions that cannot be eliminated with activities that reduce or remove GHG from the atmosphere by using carbon offsets.\nThere are two types of carbon offsets: compliance offsets and voluntary offsets.\nCompliance offsets are those offsets that companies purchase to meet legal obligations such as cap-and-trade schemes (e.g. ETS in Europe)\nVoluntary offsets are those offsets that companies purchase at their discretion. They have become a widespread and controversial instrument to fight climate change.\nThe main concerns are the project’s Additionality, Permanence Double-counting, Leakage and global availability of offsets.\nThe solution is first to choose offsets that are validated and verified under well-known standards such as Gold Standard or Voluntary Carbon Standard (VCS).\nSecond, the company should transparent and publicly communicate the use of offsets.\nFinally, choose offsets that invest in programs that empower local communities, improve health and tackle poverty while reducing carbon emissions.\nThe Voluntary Carbon Offset Market needs to grow by 15 times by 2030 and 100x by 2050 according to Taskforce on Scaling Voluntary Carbon Markets’ (TSVCM) latest report. Besides, prices in 2030 could go from current y $2-10 per ton to $15-90 depending on the scenario. This could create a market of $30-50 billion in 2030.\nSBTi and Carbon Offsets\nNote that the SBTi currently doesn’t accept the use of offsets as reductions toward the progress of companies’ science-based targets.\nStill, they consider offsets as an option for companies wanting to contribute to financing additional emission reductions beyond their science-based target.\nThe paper Foundations for net-zero target setting in the corporate sector published in September 2020 by SBti, provided the initial conceptual foundations for future guidance clarifying net-zero targets and in particular the role of offsets.\nCurrently, the SBTi is working on detailed criteria and guidance using a public consultation in early 2021. A final net-zero framework, including target-setting guidance and target validation criteria, will be released in 2021 before COP26 in November 2021.\n- Net-Zero emissions companies is one of the fastest-growing business trends.\n- The lack of standard approaches puts off organizations who struggle to make their first steps.\n- Following a 4-step process, your company can start early the net-zero journey to improve over time.\n- Offsetting will play a role in decarbonizing unavoidable emissions.\n- The SBTi framework to be published before COP26 will enable companies to set credible net-zero targets and offsets.\nWe are all responsible for doing what is in our hand to address this climate crisis. I hope this article will bring you some clarity into net-zero companies.\nThe lack of standards should not stop companies from going from bold ambition to action.\nWhether as an individual or as a company, starting a net-zero emissions journey is the best way to help make this world a better place.', 'As demand for carbon offsetting grows to help companies meet their climate goals, Mark Carney is driving efforts to scale up the voluntary carbon market, while improving its environmental integrity.\nWith a recent surge of carbon neutrality pledges, many businesses around the world are looking to offset the emissions they cannot cut.\nThis means financing projects, usually in developing countries, that deliver verified emissions reductions. These might come from in renewable energy, energy efficiency, waste management, tree-planting or carbon removal technologies.\nDemand for carbon credits has risen rapidly in the past two years, with the volume of emissions reduction claimed doubling between 2017 and 2020. It is expected to continue to soar in the near future as companies look for ways to meet their newly set goals.\n“It’s incredibly important that the ‘net’ in net zero, the offset is credible, verifiable, transparent and is preserving that precious and very limited carbon budget,” Carney, the UN special envoy for climate action and finance, told the Green Horizon finance summit on Tuesday.\nToday, the voluntary carbon market is “opaque, cumbersome and fragmentated” and “struggle[s] with low liquidity and scarce financing”, Carney said.\nIn 2019, just over $300m worth of trading took place on the voluntary market “when these projects should be measured in the tens of billions dollars per year,” he added.\nUK climate champion: Oil majors can join the ‘race to zero’ – if they align with 1.5C\nCarney convened as taskforce in September to design a blueprint for scaling up the market into a transparent, verifiable and resilient emissions trading mechanism, which is expected in January 2021.\nOn Tuesday, the initiative chaired by Bill Winters, group chief executive of Standard Chartered, and sponsored by the Institute of International Finance (IIF), published its first set of 17 recommendations, which it opened to consultation until 10 December.\nThe taskforce found efforts to upscale the market will need to be “significant”, estimating voluntary offsetting would need to grow at least 15-fold between 2019 and 2030 to support levels of investments consistent with limiting global heating to 1.5C – the upper goal of the Paris Agreement.\nDemand for voluntary carbon credits is expected to exceed 88 million tonnes of CO2 equivalent in 2020. That “is still notably short of what is needed to support net zero [emissions targets], estimated to be at least 2 gigatonnes of CO2 per year by 2030,” the taskforce found.\nOn the supply side, “there is no shortage,” said Annette Nazareth, Partner at Davis Polk and the taskforce’s operating lead. And while demand has fluctuated, there is a groundswell of interest among companies, including small and medium-size businesses, added Tim Adams, president and CEO of the Institute of International Finance.\nAt scale, the voluntary carbon market could “create an enormous green investment opportunity much of which, if not most of which, will flow to emerging and developing economies bringing vital capital flows and investments at the time when the transition is imperative,” Carney said. “In order for any of that to happen, we need a functioning and professional market.”\nComment: As young people, we urge financial institutions to stop financing fossil fuels\nMany climate campaigners are sceptical of carbon offsetting, warning it could give polluters a free pass without driving genuinely additional emissions cuts.\nFor example, oil majors have made big investments in forestry offsets in recent years, to burnish their green credentials.\nOf forestry and land use credits issued on the voluntary carbon market in 2019, nearly 90% were projects aiming to protect existing forests at risk of land-use change or deforestation, known as Redd+, according to data from Ecosystem Marketplace.\nMargaret Kim, CEO of Gold Standard, one of the main certifiers of voluntary carbon market projects, told Climate Home News these forestry credits – which account for around a third of the market — lacked integrity. It is too easy to game the baseline and ignore deforestation that is displaced outside the project boundary, she explained.\n“Companies should reduce within and finance beyond — particularly those major polluters like the oil and gas industry who seem to be very keen to enter into this forestry offset market,” she said. “Many [oil and gas companies] are setting targets that are not in line with what science says is needed and are seeking to profit from markets that are aiming to solve a problem that they created.”\nUK looks to Cop26 climate summit to fix awkward relationship with Biden\nThe Carney taskforce emphasised buying credits was no substitute for a company cutting its own emissions and called for stronger quality control. It stopped short of adjudicating which types of project were valid.\nWhile she supported scaling up the market, Kim cautioned against “force-fitting all the private sector finance into the voluntary carbon market”.\n“The voluntary carbon market should be used as a catalytic tool, not as a means to deliver private finance. It is important to set that expectation. We can’t… just hope the market is going to solve the problem.”']	['<urn:uuid:2c7dbf75-d14f-4e6c-9f9d-18de0b5975c8>', '<urn:uuid:699f75b3-f5ee-4658-8e3a-d515f33ec44d>']	open-ended	direct	long-search-query	similar-to-document	comparison	novice	2025-04-22T14:43:01.858830	10	105	3065
30	worried about not being able to breastfeed what options and alternatives exist for feeding baby	If you're unable to breastfeed or don't produce enough milk, there are several alternatives available. Formula is an option, and scientists are working on improving it by adding specific nutritional components and prebiotics to help newborns produce immunoglobulins faster than with traditional formulas. Formula can also be supplemented with DHA to support better cognitive development. Another option is using breast milk banks, which provide access to breast milk for families that would like to breastfeed but cannot. You can learn more about milk banks at mothersmilk.org.	['The Museum’s Health Sciences Department is partnering with the University of Colorado Anschutz Medical Campus to publish a monthly series on the Museum blog called “Know Health”. The articles focus on current health topics selected by CU’s medical and graduate students in order to provide both English and Spanish speaking communities with current, accurate information. The posts in the “Know Health” series are edited versions of articles that first appeared in Contrapoder magazine. Thank you to the students at the University of Colorado Anschutz Medical Campus for bringing these stories to life.\n(aka Dr. Nicole Garneau, chair and curator, Health Sciences Department)\nGuest Author, Veronica Searles Quick is a student in the School of Medicine at the University of Colorado Anschutz Medical Campus.\nAugust is the number one month of the year for births, making August both a month of joy and a time for making decisions, particularly for new moms.\nOne of the biggest decisions she will make is whether to breastfeed, use formula, or use a combination of both. While medical experts recommend breastfeeding exclusively for at least the first six months of life, new moms are rarely told why this is so important. As a result, many may be confused by the options and unable to feel confident in making the right decision for their family.\nScientific studies for many years have confirmed and reconfirmed what many moms know by intuition, that breastfeeding is good for both baby and mom. It improves baby’s immune and gut function, helps bond mom and baby, and lowers a mother’s risk for multiple diseases.\nBenefit 1: Improved Immune System\nBreastfeeding helps kickstart the baby’s immune system and the developing gut. The benefit of the immune system starts with an antibody that all of us have, but newborns are born without, called Immunoglobulin A. This molecule helps defend the body against infections. When a newborn drinks breast milk, it acquires the immunoglobulin from his mom, protecting him against any bacteria and viruses in which his mom is already immune. This process is called “passive immunity,” and it protects the baby until its own immune system is fully developed.\nTied to immunity is the baby’s microbiome, the community of healthy bacteria that helps the baby’s body work. Breastfed babies are less likely to have gut problems, including dangerous conditions like enterocolitis, an inflammatory condition that can cause permanent damage to an infant’s intestines. Researchers in Pittsburgh recently discovered a compound in breast milk that may explain this protective effect: sodium nitrate. After this enters the baby’s gut it gets converted to nitric oxide, which protects the intestine and promotes healthy blood flow, possibly helping foster a healthy gut microbiome in the process.\nFor moms that are unable to breastfeed, or don’t produce enough milk, these data can feel suffocating. While it’s true that formula does not contain antibodies, there is hope. Scientists are looking at how adding specific nutritional components and prebiotics can help newborns make more immunoglobulin and faster, helping to protect them sooner than was originally possible with traditional formulas.\nBenefit 2: Mom/Baby Bonding\nBreastfeeding also helps strengthen the bond between mother and child during the critical first few months of life. One recent study found that mothers who fed their babies breast milk exclusively were more attentive and nurturing than mothers who fed their babies formula. While it’s unclear the science behind this particular result, a strong hypothesis supporting bonding for the mother is that breastfeeding releases the hormone oxytocin. Oxytocin is often referred to as the “bonding hormone” as it induces generosity, intimacy and trust. What about women that can’t breastfeed and all the dads out there in the world bonding with their children? A study in 2010 found that high oxytocin levels in fathers were triggered by active play with their child, while levels of the hormone in moms could be obtained by cuddling.\nBenefit 3: Mom’s Long-term Health\nBreastfeeding is that it’s good for mom’s long--term health. A new study published in Obstetrics and Gynecology found that women who breastfeed have lower rates of hypertension, heart disease and breast cancer than those who don’t. Because this result is due to the biological process of breastfeeding itself, there is no equivalent yet in the formula world. However, non-breastfeeding moms, or women that do not have children, can engage in many other healthy techniques to obtain these benefits, including meditation and yoga, healthy eating and regular exercise to name only a few.\nBenefit 4: Baby’s Brain Development\nFinally, breastfeeding is known to positively affect the child’s developing brain. Researchers at Brown University have discovered that infants fed only breast milk have more advanced brain development in regions affecting intelligence, language, motor and emotional function than those fed formula. This difference is due to changes in brain white matter, which affects how well cells in the brain and body communicate.\nThe difference was surprising: breastfed kids had up to 30 percent more white matter growth at two years of age than non-breastfed kids. This may be due to the chemical DHA, which is in breast milk and affects brain development. Supporting this idea, babies fed formula supplemented with DHA have better cognitive development than babies fed standard formula.\nThe verdict? When it comes to providing the most benefits to mother and baby, breastfeeding beats formula in every category. However, there is hope for non-breastfeeding moms out there. In addition to the things listed above pertaining to formula families, there are now breast milk banks that can help families that would like to breastfeed. Visit http://www.mothersmilk.org to learn more, and check out this opinion piece on the big world of milk banks from mom and professor of public policy, Elizabeth Currid-Halkett: http://www.nytimes.com/2015/03/27/opinion/give-breast-milk.html']	['<urn:uuid:fe3fe2d7-e4c8-4437-a0f1-65b0fc7ac398>']	open-ended	with-premise	long-search-query	similar-to-document	single-doc	novice	2025-04-22T14:43:01.858830	15	86	951
31	How many people participated in the fear and sleep study?	15 healthy participants were gathered and trained to become afraid of a pair of faces.	"[""September 23, 2013\nSome Fears May Be Eliminated Just By Going To Sleep\n[ Watch the Video: Never Fear! Just Sleep On It! ]\nMichael Harper for redOrbit.com - Your Universe OnlineNew research has shown some fears could be unlearned while a person sleeps.\nUsing specific odors, such as woody or floral scents, scientists were able to reduce the reactions of subjects who were trained to fear a pair of faces prior to the study. Going forward, the researchers say this new technique could be used to treat people who suffer from post-traumatic stress disorder (PTSD) or other crippling bouts of terror. The research also dovetails previous research, which shows people can be alleviated of their fears if they’re simply subjected to them directly, such as handling spiders or climbing to the top of a tall building.\n“Sleep sort of stamps memories in more strongly,” said Gottfried in an interview with the Washington Post's Meeri Kim. “That’s when a lot of memory formation can take place.”\nGottfried and colleagues gathered 15 healthy participants and trained them to become afraid of a pair of faces. These subjects were shown a series of photos with people’s faces in them. When the subjects saw two specific faces, they were given a mild electric shock to their feet to create a fear reaction. The pair of faces were also accompanied by a specific odor, such as lemon or rose. After several successions of being shocked and smelling a certain odor, the participants showed fear reactions to the faces, evidenced by beads of sweat and confirmed with an fMRI.\nOnce a fear of the faces was established, the participants were asked to fall asleep. Once they reached a deep phase of sleep, known as slow-wave sleep, the researchers sprayed the associated scent in the room to trigger the memory or fear in their minds. After they awoke, the researchers subjected them to another fMRI scan as they showed the pair of frightful faces. Those who had been subjected to the odors as they slept the longest saw the greatest reduction of fear reactions. To avoid any preconditioning, the subjects did not know the researchers would be spraying the scents as they slept.\n“While this particular odorant was being presented during sleep, it was reactivating the memory of that face over and over again which is similar to the process of fear extinction during exposure therapy,” explained Katherina Hauner, a postdoctoral fellow in neurology at Northwestern University Feinberg School of Medicine, who helped with the study.\nExplaining the research, Gottfried said: “From a clinical perspective, this can be a new approach to try and treat stressful or traumatic memories.”\nFears can be learned in a couple of ways, either from direct experience or observing something traumatic. For instance, being bitten by a dog as a child can create a lifelong fear of canines. Additionally, people who observe a close friend or family member being bitten by a dog can also develop a similar fear. While these fears can be created rather quickly, Gottfried and Hauner say unlearning these fears through a process they call “fear extinction” may take much longer.\n“This is a very novel area. I think the process has to be refined,” said Hauner in closing, noting that more research is needed.""]"	['<urn:uuid:927d0098-6a35-4e5c-8df2-4c1ce8720eed>']	factoid	direct	concise-and-natural	similar-to-document	single-doc	novice	2025-04-22T14:43:01.858830	10	15	548
32	how new contact lens coating works	The coating technology works by decoupling the surface properties from the core lens material. The surface has a very high water content that allows the lens to integrate into the ocular environment without disrupting the tear film, while the lens core maintains a low water content for consistent manufacturability, oxygen transmission, and easy handling. The biocompatible polymer coating can be applied to any contact lens material during the final manufacturing stage.	"['Tangible Redefining the Contact Lens Experience:\nAn Interview with Brandon Felkins of Tangible Science\nWhat is the need Tangible Science seeks to address?\nInitially, we started thinking about a way to help patients who suffer from contact lens-induced dry eyes, a condition that affects more than half of all contact lens wearers.But, over time, our focus has become broader. Our mission today is to create the most comfortable contact lens-wearing experience possible.\n""Our mission today is to create the most comfortable contact lens-wearing experience possible.""\nWhat key insight was most important to guiding the design of your solution?\nThe key insight for the product came from taking a step back and really looking at the design of contact lenses, specifically the material properties, with a fresh pair of eyes. Up until this point in time, contact lens design had been driven by the big contact lens manufacturers, whose focus had typically been on trying to develop a single lens material that would be easy to manufacture and also great for the patient. But innovating within the constraints of existing manufacturing often led to incremental changes that were subject to tradeoffs between comfort and manufacturability.\nOur approach was to look at the problem first and foremost from a more biological perspective to really understand what a ‘perfect lens’ for the patient would look like. Once we understood that, we then focused on figuring out how to make it manufacturable. By freeing ourselves from typical constraints early on, we were able to develop a novel solution.\nUltimately, we determined that a ‘perfect’ lens would have surface properties that were decoupled from the properties of the core lens material. Specifically, the lens surface would have a very high water content that would allow the lens to integrate into the ocular environment without disrupting the ocular tear film. Conversely, the lens core would have a low water content that would enable consistent manufacturability, oxygen transmission to the eye, and easy lens handling by the patient, which is how the majority of existing lenses were designed. By decoupling the surface from the core of the lens, we were able to develop a surface coating technology that could be used to ‘shield’ the eye and the tear film from the sub-optimal surface properties of the existing lenses on the market.\nHow does your solution work?\nOur core product is a biocompatible polymer coating that enhances the surface properties of the contact lens. The coating increases comfort and performance by making the lens more wettable, slippery, and resistant to deposits and proteins that can stick to its surface and cause fogging. The coating can be applied to any contact lens material at the final stage of manufacturing.\nAt what stage of development is the solution?\nThe Tangible Hydra-PEG coating, which is our first commercial product, is initially being used on hard and hybrid contact lenses. We have partnership agreements in place with every major custom contact lens manufacturer in the US, as well as multiple international agreements. So the technology is widely available in the custom contact lens space.\nWe’re also expanding into soft lenses. We’ve developed relationships with disposable contact lens manufacturers that are producing a variety of disposable lenses enhanced with our coating technology. Our initial strategy is to partner with existing lens companies in the US to sell and distribute these lenses. The initial commercial product in this space is a daily disposable hydrogel lens that is enhanced by our polymers and is currently being sold to patients via one of our custom lens partners. And we have several others that will be launching soon.\nWhat are your plans for the future?\nIn parallel with expanding into soft lenses, we have other new products coming out that will serve as companion products to our custom lens coating. The first is Tangible Clean, a daily cleaning and conditioning solution that will be on the market before the end of 2018. The second is Tangible Boost, a monthly refreshing solution to help rebuild the coating on the lens that will become available in 2019.\nTell us about a major obstacle you encountered and how you overcame it.\nThe coating we’ve developed is visibly clear, and it\'s similar in its properties to the contact lens material itself. So we faced a big challenge developing testing methods to detect the coating and discern which formulations worked best. We had to develop our own assays to help guide product development because available methods just didn\'t work for us. And this took a really long time. But the process of refining our measurement techniques actually helped us improve the product. It was a virtuous cycle.\nCommercialization of the Tangible Hydra-PEG coating was also tricky. The coating is considered by the FDA to be a “component” rather than a “finished medical device.” Because of this, we technically couldn’t seek regulatory clearance or launch the technology without partnering with the manufacturers of the finished lenses, who would ultimately need to obtain the 510(k) clearances to use the coating in conjunction with their products. This limited the speed at which we could commercialize. The custom contact lens market is highly fragmented with dozens of custom lens labs across the US, and even more around the world that manufacture the custom lenses. Currently, we have partnerships with four contact lens material suppliers and over twenty contact lens manufacturers across the globe, including the US, Europe, Brazil, and New Zealand. And we’re bringing on new lab partners every month. But putting these arrangements into place has been logistically challenging as a small company. And it’s proven to be a lot more work than originally anticipated to maintain quality control and support each of these customers.\n""When you\'re doing your project planning, multiply everything by π.""\nReflecting on your experience, what advice do you have for other health technology innovators?\nWhen you’re doing your project planning, multiply everything by π. If you think something’s going to take three months to complete, multiply it by 3.14 and that\'s the actual time you’ll need to get it done. The same goes for budget planning. It seems to work almost every time!\nBrandon Felkins co-founded Tangible Science out of the Biodesign Innovation Fellowship in 2011 with teammate Victor McCray. Karen Havenstrite joined the team as co-founder and CTO shortly thereafter. To learn more, visit the Tangible Science website.\nDisclaimer of Endorsement: All references to specific products, companies, or services, including links to external sites, are for educational purposes only and do not constitute or imply an endorsement by the Byers Center for Biodesign or Stanford University.']"	['<urn:uuid:a25ebbf1-97d9-4397-97d4-cdc2a749eba2>']	open-ended	direct	short-search-query	distant-from-document	single-doc	novice	2025-04-22T14:43:01.858830	6	71	1097
33	wheat genome research food security benefits	Wheat genome research directly contributes to global food security in two ways: First, as revealed by the IWGSC project, having a high-quality reference sequence allows breeders to drastically reduce the time between gene discovery and developing commercially available varieties. Second, according to the Earlham Institute's work, the ability to sequence multiple wheat varieties enables faster breeding programs and helps develop improved wheat varieties that can withstand environmental stresses while maintaining high yields - critical for feeding the growing global population expected to reach 9.8 billion by 2050.	['The Wheat Genome Sequence Odyssey\nPosted 9th August 2017 by Fabio Caligaris\nAs the world population is expected to reach 9.8 billion by 2050, it is crucial to have innovative genomics tools to address global food security in a sustainable way. At the 5th Plant Genomics and Gene Editing Congress: Europe, Kellye Eversole, Executive Director of the International Wheat Genome Sequencing Consortium (IWGSC), announced that a high-quality reference sequence of the wheat genome is now available for the research community. The reference sequence is an essential tool to accelerate crop improvement programmes and wheat genomics research.\nThis achievement is the result of 12 years of collaborative research, which began in December 2004 when Kansas Wheat and Kansas State University, under the leadership of Forrest Chumley, hired Kellye Eversole to establish an international consortium that would lay a foundation for a paradigm shift in wheat breeding. The IWGSC odyssey began in the spring of 2005 as four individuals – Kellye Eversole, Rudi Appels, Bikram Gil, and Catherine Feuillet – launched the International Consortium with a vision of completing a useful sequence of bread wheat for the breeding community.\nOne of the first steps was to determine what should be sequenced: progenitors of bread wheat, or one of the diploid, tetraploid, or hexaploidy wheats. Industry and growers made the choice simple. They unanimously supported sequencing what is growing on 95% of wheat fields, the hexaploid bread wheat genome, Triticum aestivum, and preferably the variety for which the most genetic stocks exist and which could be translated quickly into breeding programmes.\nBut, bread wheat is huge (~16 Gigabases – i.e. 5 times the size of the human genome), complex (three ancestral genomes – one of which is unknown), and contains a high percentage of repetitive elements that complicates assembly of genome sequences. At the time, only rice had a high-quality reference genome sequence available and the rice genome is equivalent to one of the smaller bread wheat chromosomes.\nIn the landscape of rapidly changing sequencing technologies, it was critical to select an approach that would be “technology neutral”, i.e. one that would allow them to build resources that could be used regardless of the sequencing technology available at the time. Then, the only technology neutral foundation was a BAC-based physical map. As most breeders rely on the information contained within individual chromosomes and as the Consortium was able to reduce the complexity of the genome by focusing on manageable pieces, chromosome-based physical maps were selected as the foundation that would underpin any sequencing technology.\nThus, the goal of the IWGSC was to produce a high-quality, physical map-based, ordered, and annotated genome sequence comparable in quality to the rice genome sequence. Having a complete, ordered sequence was considered key, as it would allow breeders to drastically reduce the time between gene discovery and commercially available variety.\n41 Rice Genomes\nLuckily, a laboratory in the Czech Republic led by Jaroslav Dolezel had developed a technology to flow sort chromosome arms, breaking down the huge bread wheat genome into 41 smaller, much manageable, pieces (40 chromosome arms and chromosome 3B). The challenge seemed more approachable and comparable to sequencing 41 rice genomes! The first BAC library was ready (3B) and the lab started working on the production of 40 chromosome arm specific BAC libraries.\nThe concept of building physical maps for each chromosome/chromosome arm of bread wheat was a daunting challenge and was viewed with much scepticism by most in the scientific community. However, scepticism turned to interest in 2008 with the completion of the high-quality physical map of the largest wheat chromosome – 3B, equivalent in size to the entire soybean genome. With this success, projects were launched in many countries to develop physical maps of chromosomes and, with financial support from Bayer CropScience, all physical maps were completed by 2015. Since they were working with chromosome-based physical maps, it was not necessary to wait until all the maps were finished before sequencing could begin. Work began in 2009 to sequence chromosome 3B and others followed as the maps were completed. This method also facilitated map-based cloning projects on these individual chromosomes.\nAs sequencing technologies became more efficient and affordable, the IWGSC established a side project in 2010 that would generate draft survey sequences of individual chromosomes. This would provide at least some information on each chromosome and would allow breeders to start isolating or refining regions of interest. The work on chromosome specific BAC-based physical maps and pseudo-molecule sequencing continued in parallel.\nThe real breakthrough came with the software DeNovoMAGICTM which was developed by the firm NRGene to be used to assemble Illumina whole genome sequence. With this, the IWGSC could produce a whole genome assembly of the 16Gb genome in 7 months and validate its quality against other sequence-based and chromosome-based resources developed by the IWGSC over the previous years. They released the whole genome assembly with Hi-C and POPSeq to the scientific community in June 2016. Although this was an impressive assembly, it did not completely achieve the high-quality standard that was the target for the IWGSC.\nSince June 2016, the IWGSC has integrated all chromosome-based resources (physical maps, genetic maps, whole-genome-profiling-WGPTM sequence tags, optical maps, and markers) and released in January 2017 IWGSC RefSeq v1.0, the first version of the high-quality reference sequence of bread wheat.\nHaving at their disposal all the chromosome-based resources and the WGPTM tags generated over the previous years proved invaluable as the quality of the assembly tripled with the addition of these resources. Most of these were used to refine the order of the sequence and to decrease the number of pieces per chromosome (to an average of 75 scaffolds per chromosome). The automated annotation process, using two different annotation pipelines to develop a high confidence set of genes, was completed and released to the community in June 2017. Final analysis is underway and the goal is to submit the manuscript by late summer 2017.\nWhat started with 4 people in 2005, has now grown to 1800 members working in more than 530 institutes or companies in 62 countries. The IWGSC reached its goal of generating a high-quality reference sequence of bread wheat, a significant milestone for agriculture and the scientific community and now work will focus on manual and functional annotation of the reference sequence as well as sequence improvement. All the IWGSC data are available under the Toronto agreement at the IWGSC data repository hosted by URGI-INRA (France).\nSo, what are the lessons learned from this odyssey?\n- Every crop of importance for food, feed and fibre should have at least one high-quality manually and functionally annotated reference sequence, preferably more.\n- BAC libraries are essential for generating high-quality references and are critical for map-based cloning.\n- Maintaining flexibility is crucial so one can adopt new technologies as they are developed without losing sight of the need for quality.\n- The key guiding principle is to never lose sight of the original vision even when the rest of the scientific community may not be supportive.\nView the agenda here.\nLeave a Reply', 'Can we produce a better wheat crop to feed the world? Single to multiple wheat genomics\nPosted: 13 January 2017 | Earlham Institute (EI) | No comments yet\nThe Earlham Institute (EI) aims to diversify one of the world’s most complex genomes to improve yield quality and increase wider production of this critical food crop…\nEntering a ‘wheat pan-genomics’ era from single to multiple wheat DNA references, the Earlham Institute (EI) aims to diversify one of the world’s most complex genomes to improve yield quality and increase wider production of this critical food crop.\nBread wheat (Triticum aestivum L.) is the UK’s most economically important crop and the world’s most widely cultivated cereal. Wheat production is vital in both emerging and growing economies.\nTherefore, understanding the genomics of wheat is essential to sustain increased yields for the growing global population, while protecting the crop from common disease epidemics and adaptation to extreme climate change conditions.\nThe wheat genome has been an enigma to scientists due to its exceptionally large and complex genome. Previously leading a first complete analysis of the bread wheat genome with other BBSRC-funded institutes, EI have now leveraged their genomics advances to work with both industry and research partners to enable the production of a better and more sustainable crop to aid global food security.\nFrom the most sourced plant data reference released in November 2015, EI have developed a new method that is influencing industry and academia to further advance the field of wheat genomics into a multi-reference era. Unveiling new genome assemblies for five further wheat varieties presented publicly at PAG, the largest plant and animal genomics global conference, in San Diego, 14 January 2017.\nThe new wheat analyses came to fruition after delivery of the first wheat reference using the bioinformatics ‘w2rap’ tool for genome assembly developed by Bernardo Clavijo and his Algorithm Development Team at EI. Life science industry leaders Bayer Crop Sciences approached the team to run a pilot project sequencing a commercially significant wheat variety.\nNow, the industry-academia partnership has resulted in a further four varieties being commissioned. A relationship that gives the industry access to unprecedented genome analysis power, while supporting the development of EI’s wheat genomics methods.\nW2rap is a bioinformatics pipeline that can decipher complex genomes and produce robust assemblies to guide precision breeding. This genome assembly method will be applied to EI’s further wheat analysis, using tailored data generation based on work of the Institute’s Platforms and Pipelines Group at EI.\nThe unique tool will not only be able to assemble the wheat varieties sequence data but allow wheat genomes to be produced effectively, robustly and efficiently with next-generation sequencing technologies.\nIn parallel to this, a £2m grant funded by BBSRC to EI in collaboration with the John Innes Centre (JIC) and NIAB, led by Dr Matt Clark, will aim to understand the genetic makeup of 14 different varieties of wheat cultivars important for global agriculture and analyse this data to help map the genetic variation in the crop.\nThis project includes the further sequencing and assembly of eight more wheat lines with the w2rap method, and will build wheat genetic resources for the purpose of crop breeding, preservation, and research.\nProf Neil Hall, Director of EI, said: “These new projects are extremely timely as technical advances we have made in assembly and analysis of the wheat genome have overcome the hurdles in complexity and now enable us to approach wheat genetics in a whole new way.\n“Now we should see the rapid advances in breeding programs that have previously been possible in other crops.”\nFred Van Ex, Head of the Genomics group at Bayer Crop Science, said: “More than two billion people worldwide rely on wheat as a staple food. In order to meet demands of the growing world population, Bayer is dedicated to develop high-yielding wheat plants that are able to withstand increasing environmental stresses and provide high-quality end products.\nAdditional wheat genome sequences will allow us to accelerate the identification of genes underlying yield and resistance to biotic and abiotic stress. We are, therefore, very excited to work with EI as the wheat genome sequences generated by the w2rap pipeline will enable us to develop markers for breeding and isolate genes to support the development of improved wheat varieties that meet the farmers and society needs.”\nBernardo Clavijo, Bioinformatics Algorithms Project Leader, said: “w2rap will be of great value for public wheat initiatives; helping us to advance into wheat pan-genomics faster, and from a completely open and transparent public-research focus.\n“From a breeder’s perspective, this is also a tool that allows them to peek inside their germplasm with unprecedented detail and robustness – allowing cross-analysis with our high-quality public wheat reference. We are thrilled to be working with Bayer Crop Sciences; making the impact of powerful whole-genome sequencing and assembly approaches a reality.\n“In terms of global food security, the gains are twofold. The immediate incorporation of new technology into the breeding programme allows breeders to produce better varieties faster and cheaper. As well as the methods we develop enabling a fast transition from a single-reference to a many-genomes scenario that will effectively start to tap in the enormous variation of the world’s wheat genomic resources.”\nGrant lead Dr Matt Clark, Head of Technology Development at EI, said: “These new wheat genomes sequences have the potential to radically alter both wheat breeding and crop improvement. Facilitating new areas of research in understanding the contributions of genetic variation and gene function to the crop’s many different traits.\n“Working within an international strategic framework, the project will maximise the benefit to UK research and industry wheat breeding practices. Delivering world-class knowledge, resources and training to help maintain the UK’s global position in wheat genomics and contributing to the priority of global food security.”']	['<urn:uuid:891e7b67-3e2f-4465-bd20-1261c31f5d6b>', '<urn:uuid:cee56391-0270-49a0-9bdd-4157e87bfefd>']	open-ended	direct	short-search-query	similar-to-document	multi-aspect	expert	2025-04-22T14:43:01.858830	6	87	2145
34	What limits do pie charts have and how should they be visually improved?	Pie charts should not have more than 7 categories because our brains struggle with comparing angles. For visual improvement, instead of using different hues, a monochromatic color scheme should be used, which gives clarity and looks good in grayscale.	['Visualizations are great for getting a grasp of data sets, but sometimes the data is too big for a certain visualization technique. Every type of visualization has limits for how much it can display while still being useful. These limits are not due to the visualization alone, they also come from the capabilities of our brains to perceive and interpret what our eyes see. Different visualization types rely on different capabilities of our visual system, so they each have different limits. Let’s look at a few examples.\nPie charts are among the most popular visualizations for percentage data, but they aren’t appropriate for more than seven categories. This is because our brains are not particularly good at telling different angles apart, and even worse at telling how far apart different angles are from each other. If you have a pie chart with more than seven categories, consider turning it into a bar chart. The part to whole relationship is no longer apparent, but typically, it is more important to see the difference between different categories. The data for the pie chart below was taken from a pie chart in an infographic recently submitted to Visual.ly. It has far too many categories to be useful, although being sorted certainly helps it a lot. The bar chart shows the same data, but the part to whole relationship is not visible. In this case, that relationship is not critical, though. What is more important is seeing not just which categories are ahead of other categories, but how much they are ahead (in this case, the only significant differences seem to be in the first two categories). Keeping the axis from 0-100% also shows the context of the values.\nYou’ll notice in the charts above, that the colors repeat. The same repetition was there in the original pie chart, only with different colors. Colors are another limiting factor in many visualizations of categorical data. The maximum number of colors (with similar luminance values) that we can distinguish and remember easily is around 12. The 12 colors below have been taken from Colorbrewer2.org, a great resource for categorical and continuous color scales.\nBar and Column Charts\nBar and column charts of categorical data also have an upper bound on the appropriate number of bars. That number is limited by several things. Hypothetically, screen space is important. After all, if you can’t see all of the chart at once, you rely on memory instead of visualization for the parts off the screen. Today, screens resolutions are typically high enough that pixel counts aren’t the limiting factor anymore. So the limiting factors are a bit more complex and depend on several issues. First, what matters more in the data? Is there an overall trend that is important, or is the difference between individual categories the focus? If the overall trend is the important factor, you might be able to get away with 50 bars or more. If individual differences are important, you probably want to keep the total number of bars under 12. Every bar you add increases the number of comparison possibilities exponentially. That is not to say that people actually make all of these comparisons when they look at a bar chart, they likely spot important ones or big differences and only make those. The chart below illustrates how individual differences are hard to compare using too many bars, while overall trends are still visible.\nLine charts are another visualization type with limits on the number of elements. The number of points on the lines are only limited by screen space, however the number of lines is limited by perceptual issues. Too many lines will cause people to have a hard time seeing and tracing each individual line, depending on line crossings. The chart below was taken from a graphic recently submitted to Visual.ly. It only has seven lines, but the bottom ones cross a lot and cross at low angles to each other (try tracing the purple line). This isn’t a deal-breaker for this particular chart because the red line with the huge spike is where the story is, but not all data has the same story.\nScatterplots can easily show more elements than any other visualization for tabular data (the runner-up being parallel coordinates). There are several reasons for the high numbers. The first reason is incredibly simple: dots don’t take up much space. Second, since a scatterplot is good at showing correlation between dimensions, the information really comes from the aggregate group, not from the individual. For this reason, the space consuming labeling necessary on other charts isn’t needed for many scatterplots. In the example below it is easy to see two groupings, and some slight correlation within each group. With some color changes, axis labels, and explanatory text, this visual could take hundreds of elements and turn them into a small set of insights. One critical thing to remember is that with almost all visualizations, the more elements there are, the longer people will need to spend examining the visualization before they can interpret the information. For an infographic intended to make a point, quick and easy is probably the goal. Bottom line: if your data is too big, you may need more analysis to distill it down to the essential parts. Drew Skau is a PhD Computer Science Visualization student at UNCC, with an undergraduate degree in Architecture.', 'Whether you are creating a dashboard or static report, regardless of the tool, these visual tips will help you improve its presentation and create a more intuitive and professionally-looking user interface.\nAvoid Borders. Use space to separate different elements.\nIt is a common habit to put a box around all the visualization. These solid borders can make your design look rigid as well as cluttered (like the canvas is sliced into different pieces). While the intention, which is to separate or group elements together, should be retained, the means should be slightly tweaked. Let’s look at a simple example here:\nWhile the interface looks clean and organized, the black borders are too bold on white background, therefore stealing steal users’ attention from the first glance. Remember that user’s visual attention is limited, and it is your responsibility to guide their eyes to the right visual element. Because the borders are not the focus of this report, they should not be that noticeable. So instead of using ‘lines’ to separate and group elements, we can use space and subtle color contrast.\nBy removing the borders and creating a consistent distance around each box, you can separate each element more efficiently. The contrast between the chart’s white background and the report’s light gray is enough to draw this distinction. Note that if the background is darker, the contrast might be too much and can again steal the attention from other visualizations – subtlety is the key. For example:\nI have also removed the borders around navigation labels on the top left corners because they don’t serve any purpose. The distance between the words is enough to tell them apart.\nUse only a few contrasting elements but signify the contrast.\nContrast or difference between elements not only makes your dashboard look more interesting, but also plays an important role in creating a visual hierarchy for your interface. A visual hierarchy is how we organize and prioritize the information presented in an interface. So before applying any visual property, you need to think about what information is more important than others. For example, in the context of a chart, chart title is the most important, then go data label, legend, axis title, axis label and gridline. These texts should have different property that let user easily read chart title first, then focus on data label without the noise of axis label or gridlines.\nThe first rule is the same type of elements should have the same properties (For example: all chart titles and KPI title should have the same font size and font-weight) and there should not be too many groups with different properties. The second rule is to make the contrast prominent: exaggerate the different between groups to help users see the difference more clearly. To establish a visual hierarchy, for example, we use bigger, bolder font, brighter colors and for less important elements, we use much more subtle colors.\nHere the dashboard looks more interesting and is easier to read because I have increased the contrast by bolding the important text and using light colors for less important elements such as the unselected tabs, axis labels and gridline on the charts. On the navigation tab, I also change the selected tab to a brighter hue so that the difference is much more prominent.\nConsistently Align Text Left\nThis rule is very simple but often overlooked. Intuitively we read from left to right, therefore it is easier for users to receive the information from your text elements if they are left-aligned. Another reason to consistently align text left is that we can have all text starts from the imaginary vertical line, which makes the content much more compact and organized.\nAn additional tip is to never align text center (except for dashboard title)\nUsing colors: only use different color (hue) when it has a unique purpose\nA hue is a specific shade of color that we usually assign a name to it such as green, red, blue and orange. The general rule is to avoid using more than 5 different hues on an interface and to use as few hues as possible. As a good example, each pie chart below uses the color turquoise with variations in lightness. This is monochromatic color scheme. This scheme is highly recommended for data visualization because it gives clarity without using different hues. Furthermore, it will also look good when printed in grayscale because the difference in lightness can be easily accommodated by black ink.\nWhile each of the three pie charts uses monochromatic scheme, the three are based on different hues. Let’s think about what these colors do for the chart – they represent different values of Store Name, Age Group and Class Name. Because we already have the chart titles and data labels, it is redundant to use a different hue for each chart. Therefore, we can use the same color scheme for all pie chart to reduce the number of different hues used in the dashboard.\nThink about the purpose of every single visual element on your canvas.\nWhen it comes to the final review, think about the role every single element plays in communicating information. Whether it’s text, lines, border, shadow or color, it needs to help user focus on the right material or access information quicker. No element should be there for decoration. A ‘good-looking’ report or dashboard is the one that uses visual elements efficiently to deliver the business insights in the most clear and concise manner. Any element that does not help you achieve this is noise and should be removed. Happy Dahsboarding!']	['<urn:uuid:76db4f61-1ae4-4e50-a62b-cfd868cc590c>', '<urn:uuid:3cab5982-7155-42f8-a715-4bcf44af4c27>']	factoid	with-premise	concise-and-natural	similar-to-document	multi-aspect	expert	2025-04-22T14:43:01.858830	13	39	1831
35	I've been told by investigators they need to talk to me about something urgent, and they promised to help me out if I speak with them right away. Should I trust them and go?	No, you should not speak with the police without an attorney present. Police are legally allowed to lie during interrogations to get confessions. Their promises to 'help you out' are just tactics to get evidence from you. The only person who has authority over your charges is the district attorney, not the police. Nothing good ever comes from speaking with police without an attorney present.	['Frequently Asked Questions\nThe most important time in your criminal case happens before you even reach a courtroom. Understanding the criminal process, your rights, and what to do when, where, and how can make or break your case. If you or someone you love has been contacted by the police regarding an investigation, arrested, or charged, it always helps to speak with an attorney.\nBelow are a list of frequently asked questions that I received through email or the phone, this is my response:\nThe police have called and said that they just want to ask a few questions and that I am not a suspect, what do I do?\nCall a lawyer. Always have an attorney present during any interaction with the police, even those that appear harmless. Police are legally allowed to lie to you during an interrogation in order to get a confession. You can be 100% innocent and accidentally place yourself at a crime scene or surrender information that makes you look guilty just by how they ask questions. Your memory is not perfect and neither are the police. I cannot emphasize this enough to people: nothing good ever comes from speaking with police without your attorney.\nThe police have said that if I cooperate with them and refuse a lawyer, they’ll go easy on me. Is this true?\nNo. I am a former attorney at the Clark County Public Defender’s Office and can tell you right now that the only person who has any say in your charges is the district attorney. That is a tactic often used by the police to get a confession out of you. It is their job to appear nice, friendly, and on your side so that they can get the most evidence out of you without the benefit of an attorney protecting your rights.\nDo I need a criminal attorney?\nIt is during the period when the police are investigating you that you get the most out of your lawyer. Not only can they prevent you from accidentally saying something that will incriminate you, but also they ensure that your rights are being protected. The lawyer will force them to also investigate evidence that favors your innocence and can be the one steering the conversation to protect you.\nIs it ever okay to speak to the police alone?\nNo. See above.\nIt’s “just pot” do I really need a lawyer anymore?\nAny charge, especially for an activity that may raise its head again, requires a lawyer to assist in getting the charges reduced or dismissed so that you have no criminal record going forward. Judges don’t appreciate seeing the same individuals in their courtroom repeatedly, and employers may not feel the same about “just pot” as you do.\nMy _________has been arrested. What is the bail process and how much will I need to bring?\nBail is an insurance policy, of sorts, that the defendant will show back up if released. How much bail depends on the charges you are facing. Not all cases require bail. Call a Bail Bonds Company to find out their fees and what fees (if any) are returned on completion of your case.\nWhat is the difference between a misdemeanor and a felony?\nA misdemeanor is by definition “a lesser criminal act,” that is punished through fines and jail time of less than a year. Examples are possession of pot, solicitation, and shoplifting. A felony is a serious criminal offense that is punished through jail or prison time of over a year to life, and fines upwards to several thousand dollars. Most violent crimes are felonies, as well as, major theft crimes, drug trafficking crimes, and sex crimes.\nDon’t be stupid, keep your damn mouth shut and Call Kuzemka Law Group NOW!']	['<urn:uuid:376f98b5-771a-4e54-bde8-022ca4a671b2>']	open-ended	with-premise	verbose-and-natural	distant-from-document	single-doc	novice	2025-04-22T14:43:01.858830	34	65	628
36	What are the different duration requirements for completing each level of EMT certification, from basic to paramedic training?	EMT certification has three levels with varying durations. The EMT-Basic course is the shortest, taking 3-11 weeks with 120-150 credit hours. The EMT-Intermediate program requires an additional 300-350 credit hours of coursework and training. The EMT Paramedic, which is the highest level, typically takes two years to complete and requires 1,200 to 1,800 hours of training, leading to an associate or bachelor's degree.	['EMTs are the first responders to provide medical care to the sick and injured during an emergency crisis. If you wish to have a career as an Emergency Medical Technician (EMT), you should take into account the commitment, dedication and time factor needed to be an EMT. It can be a full-time job that can take up to several months of rigorous and intense training. So, when determining how long does it take to become an EMT, you must take into account the EMT course duration, the length of different EMT programs, and the time taken for obtaining an EMT license.\nHow Long does it Take to Get EMT Certified?\nThere is no universal time duration to get certified as an EMT. An EMT course length varies with country, state, community colleges, and schools. Some states have their own regional Emergency Medical Services (EMS) agencies which differ from one to another. In the United States, obtaining an EMT Certification takes a minimum of three weeks to a maximum of two to four years depending upon your professional goals, national standards, and state requirements.\nIn order to become an EMT certified, the candidate must meet specific education and other relevant criteria. Further, the candidate must complete the necessary coursework and different level of the training program within the defined time. The time that each level takes depends on several factors like student dedication, class schedule and time taken for completing the licensing examinations. On successful completion of the training program, the candidate can appear for the certification tests and apply for the certification documents through an online portal to become an EMT certified.\nEMT Course Duration\nThe training and classes duration also depends upon the colleges, schools, and universities offering EMT courses. Some institutes may provide a certificate course of 3-6 months while some may offer a two or four-year associate or bachelor’s degree program. The number of hours is divided among class, lab, and field training that must be necessarily obtained. It depends upon the school or institute on how they structure the course to achieve the defined hours.\nMoreover, it also depends upon the course level you select. Generally, there are three levels of EMT certification course: EMT-Basic, EMT-Intermediate and EMT Paramedic. Each program requires its own set of training, lectures, and duration. For example, an EMT-Basic course requires 2 to 3 weeks to complete, having a class of minimum 8 to 10 hours a day, while an EMT Paramedic course may take two years to earn an associate degree.\nThe EMT-Basic has the shortest course duration as compared to the Intermediate and Paramedic. You need to cover all the theory and practical learning part in such a short time. On the other hand, a Paramedic course can take thousands of hours for completion. You will be attending two years of classes and clinical training to finally become an EMT Paramedic certified.\nDuration of Different EMT Programs\nEach EMT program level has its own pace. Generally, the basic level takes a shorter period to complete as compared to the advanced level of EMT. In many instances, the students are also required to complete the basic level to qualify for the advanced level of the EMT program.\nThe course duration for an EMT-Basic generally takes up to 3-11 weeks consisting of 120-150 credit hours. The basic level EMT program is not as challenging as the Intermediate and Paramedic programs. The program includes coursework and clinical training essential to become a professional EMT.\nEMT Intermediate program requires the student to attend another 300-350 credit hours of coursework and training. An EMT Intermediate takes on more responsibilities in comparison to the Basic level training. The course includes advanced learning in performing medical procedures on patients like conducting IV treatment, endotracheal intubations, etc.\n3. EMT Paramedic\nEMT Paramedic is the highest level of EMT certification program. It generally takes two years of excessive training, both classroom and clinical training at hospitals and fire departments. The coursework includes a variety of subjects like anatomy, physiology, cardiology, pharmacology, and other advanced medical treatments. The program requires 1,200 to 1,800 hours of training which leads to an associate or bachelor’s degree upon completion.\nGetting EMT License\nAfter completing different levels of EMT training course, the candidate may apply for the state’s licensing examination or the National Registry examination. It must be noted that the candidate must be an EMT certified before applying for an EMT licensure. Many states prefer the EMT certification to be obtained through the National Registry of Emergency Medical Technicians (NREMT) to qualify for the license. Further, the examination conducted by the NREMT requires the candidate to pass both cognitive and psychomotor exam.\nThe Cognitive exam is a computerized adaptive test which covers entire coursework taught in class during the training program. The candidate is given six opportunities to pass the exam. If not successful in the first attempt, the candidate may reapply within 15 days for the same. On the passing of the cognitive exam, the candidate may appear for the psychomotor exam. The exam tests the practical leaning of the candidate regarding medical procedures before getting an EMT certification.\nThe licensing procedure also varies with the state. Many states conduct background checks and may not give license to a candidate with a criminal record. Moreover, every two or three years EMTs are required to renew their licensure through continuing education.\nYou need to understand the amount of time and dedication required before applying for an EMT program. Ask yourself how long will it take for you to become an EMT professional and whether you can devote the amount of time required to complete the courses. Based on your career goals and priorities, you can opt for shorter or longer duration courses available.\nMore reading :- If you are looking for other career alternatives like nursing, medical assistant in healthcare sector. These fields offer diverse working environment and career advance opportunities. Know how long does it take to get your CMA?']	['<urn:uuid:8cda8b19-0cb0-4bde-9022-d8017b092c46>']	open-ended	direct	verbose-and-natural	similar-to-document	single-doc	novice	2025-04-22T14:43:01.858830	18	63	999
37	Could you explain how the modulus of resilience can be determined from a stress-strain diagram, and what specific area should be considered?	The modulus of resilience can be found by looking at the area under the linear portion of a stress-strain curve. Specifically, it is determined by integrating the stress-strain curve from zero to the elastic limit.	"['If you want to be a good structural engineer, the idea of the modulus of resilience is the must for you. Resilience is actually is a term of materials science. The capability of a body to imbibe energy, when the body deformed in its elastic limit.\nIn this article, the term modulus of resilience will discuss broadly. Stay with us to enrich your civil engineering knowledge. Cheers!!\nModulus of Resilience\nNow, can you sense what the modulus of resilience is? In the language of the book, it is the maximum quantity of energy that can be imbibed per unit volume without creating any permanent perversion.\nIn other words, if you provide force in a body, the quantity of absorbed energy within a certain limit is known as modulus of resilience. Lastly, the limit is the elasticity limit and It is normally denoted as ‘μ’. Sometimes we denote this as ‘Ur’ too.\nThe modulus of resilience varies from material to material. Do you know why? Because the elasticity limit is not constant for varying materials.\nHow to Calculate the Modulus of Resilience\nOkay, at this moment we have some ideas about resilience and modulus of resilience. Let\'s see how to calculate the modulus of resilience.\nFrom the definition we know, ""Resilience"" is an engineering term that refers to the amount of energy a material can absorb and still return to its original state. The modulus of resilience ‘μ’ for a given compound represents the area under the elastic portion of the stress-strain curve for that compound, and is written as:\nμ = σ12 ÷ 2E\nμ is the modulus of resilience,\nσ1 is the yield strain and\nE is Young\'s modulus.\nThere is also a way to find out the modulus of resilience from the stress-strain diagram.\nFigure 1: stress-strain diagram\nAccordingly, In the figure, there is a stress-strain diagram. The area under the linear portion of a stress-strain curve is the resilience of the material. If you integrate the stress-strain curve from zero to elastic limit, you will find the modulus of resilience.\nUnit of the Modulus of Resilience\nAs we know, Unit of the Modulus of Resilience is nothing but the quantity of energy absorbed by per unit volume of a body. So, in the SI system, the unit will clearly be Joule per cubic meter (J.m-3).\nIt is also provable by the stress-strain diagram, which is in below.\nFrom figure 1,\nμ = Area underneath the stress-strain (σ–ε) curve up to yield = σ × ε\nμ [=] Pa × % = (N·m−2)·(unitless)\nμ [=] N·m·m−3\nμ [=] J·m−3\nFinally, a civil engineer or structural engineer, you will always have to work with materials. The parameter modulus of resilience expresses the elasticity limit of a body in a passive way. It also symbolizes the load bearing capability of a body.\nMoreover, if you are a student of civil and structural engineering, you will find many more applications of the modulus of resilience in your future academic activities.\nSo, hopefully, this article will help you a lot!']"	['<urn:uuid:58ae042b-cd4f-4b28-9d82-90422eb18eb7>']	factoid	direct	verbose-and-natural	similar-to-document	single-doc	expert	2025-04-22T14:43:01.858830	22	35	508
38	camera setup monitoring vs cheese quality checking which requires more visual inspection	The camera setup requires more visual inspection as it needs constant monitoring through a mounted display that allows the hand model to see the frame, proper focus settings at f/5.6 or deeper, and clip playback capabilities. In contrast, blue cheese quality checking is more straightforward, primarily focusing on ensuring the cheese is creamy and moist, with the level of pungency being a key indicator of its quality.	"['It’s right in the middle of a filming day when the client springs it on you…“Can we film this from overhead? You know, like one of those Tasty recipe videos on Facebook?” The sort of question that would have been good to hear about two days ago when you were finalizing your grip order.\nNow, if you’ve got a studio dolly with an offset and riser (or a really solid short jib), you might be good to go. But if not, here’s a quick and dirty recipe for an overhead filming rig that a reasonably well-equipped grip truck likely has the parts for. And you can hack in replacement components if you’re missing anything.\nNote that this setup only works for lockoff shots. It’s a really good fit for simple top-down tabletop work, but would also work for narrative or anything else as long as the frames don’t need to be very wide, and you don’t need to operate the camera. With the right gear on hand, you can build this rig in 15 minutes or less. Here’s a rough list of the parts that you need:\n- 2x 8ft speedrail\n1x 2-4ft speedrail crossbar\n2x truss crossbars w/ junior pins (Dana Dolly end blocks also work well)\n1x cheese plate w/ locking ball head (add a QRP for simpler mounting)\n2x combo stands (add wheels for simpler adjustments and fine-tuning)\n2x 90-degree grid clamps\n2x flat-top grid clamps\n2x 3/8″ bolts\nTake a look at these rough overhead sketches. If your table is short, you could certainly span it on the x-axis. But after doing a few of these videos, I find that spanning with 8ft speedrail on the y-axis tends to work better. For one, that allows you to frame up a fairly wide shot, taking full advantage of a 16:9 aspect ratio without showing the legs of the stands. Combo stands allow you to quickly raise and lower the rig to make adjustments or change the framing, and they are rock-solid once locked down. And if you put the combos on wheels, you will find that this simplifies fine tuning the final frame. Here are a few rig pictures with a very simple camera setup, using an FS5 camera tethered to an Odyssey 7Q recorder.\nI like to hang the monitor on an arm close to the hand model so that they can see the frame for themselves. Most of the time, you can get away with locked-off focus on these types of shoots, as long as you can get an f/5.6 stop or deeper. The camera shown here is super lightweight, but you can easily support 30-40lbs without any issue on this rig. Just ensure you have a good solid cheeseplate w/ locking ballhead, the type you’ll often find in a basic car rigging kit. You’ll also want to make sure you have a remote trigger for the camera within reach, and some way of playing clips back without having to climb up on a ladder.\nOne last tip for these type of shoots: have a few white and black v-flats on hand. If you aren’t familiar, these are simple 4×8 foamcore panels taped together on the long edge to create a flexible hinge, so that you can stand them up in a v-shape. It’s a quick and easy way to fly in some bounce or negative as needed. Easy to make, and ridiculously handy on studio shoots! They’re big enough to affect lighting even when you position them away from the table (to allow crew access), and they’re easy to adjust simply by walking the vee around. You can see several of them in the background of these bts photos.\nIf you film enough of these overhead setups to buy the parts, here are the bits that I would purchase:\n2x 8ft Schedule 40 Speedrail Pipe\n1x 2ft Schedule 40 Speedrail Pipe\n2x Ladder Truss w/ Jr Pin for 1-1/4 pipe\n1x Ball Camera Leveling Mount w/ cheese base\n2x 90-degree Fixed Grid Clamps\n2x Grid Clamp w/ 3/8″ female thread\n2x 3/8 bolts from any hardware store\n2x combo stands with Jr receiver\n6x wheels for the combos\nSeason and adjust to taste. Hope this helps, happy shooting!', ""Where do our favorite dishes come from? In our ongoing series ‘Food History’ we take a look at classic dishes and their roots, this time with a focus on the world of blue cheese.\nSomeone once told me a joke about the French and cheese:\n“Put a plate of smelly cheese in the middle of the table and everyone will pull back, scrunching up their noses and saying, ‘eww.’ Except for the Frenchmen. He will lean in and say ‘ah….'”\nAppreciating blue cheeses takes time, and it certainly isn’t everyone’s favorite. But some of us just can’t get enough of the mold. How did this obsession start?\nLet’s start by breaking down the term: “blue cheese.”\nBlue cheese is in fact a general classification of cheeses–from cow, sheep or goat milk–that have cultures of the mold Penicillium in them. Yup, the same stuff that’s in the antibiotic Penicillin. Because it’s a general term for a variety of individual cheeses, we can’t talk about the specific history of blue cheese, but one of the most well known blue cheeses is Roquefort, and because of its story, it is an excellent place to start.\nRoquefort is actually one of the oldest known cheeses, being praised as far back as 79 A.D. It is said that it was the favorite cheese of Charlemagne, and that he himself called it le fromage des rois et des papes – the cheese of kings and popes.\nBut how did people start eating this pungent cheese decorated with green mold?\nLegend has it that a young sheepherder eating a lunch of ewe’s milk curds and bread left his lunch in a cave while he left for more interesting pursuits; in this case pursuing a lovely maiden. When he returned to the cave months later, he found his cheese moldy, yet delicious.\nWhether that’s true or not, we can only imagine the first person that looked at a molding cheese and thought the themselves, “sure, I’ll try that.” But good thing they did.\nIt is still produced in caves, and in France you can even visit those caves. To highlight it’s importance to French cheese culture, Roquefort was the first cheese to receive a Appellation d’Origine Controlée, a French certification that protects various regional products and their production. Champagne for example is regulated under the Appellation d’Origine Controlée as well, any sparkling wine that isn’t from the Champagne region isn’t champagne, and on the off chance that you’re ever eating a cheese labeled Roquefort that isn’t from the region of Aveyron it’s not actually real Roquefort.\nBut not everyone is a Roquefort fan. For other blue cheese lovers there’s Gorgonzola, Cambazola, Bleu d’Auvergne, Stilton, and several others. In the U.S., however, many of us have grown accustomed the the generic, industrialized form of blue cheese, but if you’re a real cheese connoisseur you’ll know that it’s important to choose the good stuff.\nA good blue cheese variety should be creamy and moist, the more pungent the better. Crumblier varieties will be stronger – hello Roquefort – with that distinctive “bite.” If you’re a novice to the blue cheese family, this might not be the place to start. Kick things off with Gorgonzola or a Danish Blue instead to get yourself initiated.\nFind a plate, serve up a few varieties and have a tasting to find your favorite.\nAnd don’t let the mold scare you.\nCheck out more of our Food History series.""]"	['<urn:uuid:c5ff52f4-d34a-4f47-bb40-f00b6f9913a1>', '<urn:uuid:52d8df0d-1f20-479a-8f1a-693df0aeaee9>']	factoid	with-premise	long-search-query	similar-to-document	comparison	novice	2025-04-22T14:43:01.858830	12	67	1278
39	When was the article about fatigue cracks published?	The article was published online on May 25, 2019.	['The Development of Fatigue Cracks in Metals\nDariusz Rozumekdownload PDF\nAbstract. The work presents the development of fatigue cracks in metals (review article). In particular, the reasons for the growth of fatigue cracks, models of the development of cracks, cracks initiation and propagation for various modes as well as effects of the cracks growth were presented. Fatigue of materials, especially the formation of fatigue cracks and their growth, belong to the important problems of solid mechanics. Designers and constructors of machines and industrial devices are focusing their attention on problems concerning durability and reliability of these devices. Therefore, the engineering materials that are used to construct devices should have the best properties, because choosing the right material affects, to a large extent, the durability of construction.\nFatigue Crack Growth, Microstructure, Load, Mixed Mode\nPublished online 5/25/2019, 7 pages\nCopyright © 2019 by the author(s)\nPublished under license by Materials Research Forum LLC., Millersville PA, USA\nCitation: Dariusz Rozumek, The Development of Fatigue Cracks in Metals, Materials Research Proceedings, Vol. 12, pp 124-130, 2019\nThe article was published as article 18 of the book Experimental Mechanics of Solids\nContent from this work may be used under the terms of the Creative Commons Attribution 3.0 licence. Any further distribution of this work must maintain attribution to the author(s) and the title of the work, journal citation and DOI.\n S. Suresh, Fatigue of Materials, Cambridge University Press, Cambridge, UK, 1991.\n J. Gadomski, P. Pyrzanowski, Experimental investigation of fatigue destruction of CFRP using the electrical resistance change method, Measurement. 87 (2016) 236-245. https://doi.org/10.1016/j.measurement.2016.03.036\n L. Sniezek, T. Slezak, K. Grzelak, V. Hutsaylyuk, An experimental investigation of propagation the semi-elliptical surface cracks in an austenitic steel, Int. J. Pressure Vessels and Piping. 144 (2016) 35–44. https://doi.org/10.1016/j.ijpvp.2016.05.006\n S. Kocańda, Fatigue Failure of Metals, WNT, Warsaw, 1985.\n D. Rozumek, E. Macha, A survey of failure criteria and parameters in mixed-mode fatigue crack growth, Materials Science. 45 (2009) 190-210. https://doi.org/10.1007/s11003-009-9179-2\n U. Zerbst, M. Madia, C. Klinger, D. Bettge, Y. Murakam, Defects as a root cause of fatigue failure of metallic components. I: Basic aspects, Eng. Failure Analysis. 97 (2019) 777-792. https://doi.org/10.1016/j.engfailanal.2019.01.055\n P. Lukas, L. Kunz, Notch size effect in fatigue, Fatigue Fract. Eng. Mater. Struct. 12 (1989), 175-186. https://doi.org/10.1016/0142-1123(89)90261-2\n C. Verdu, J. Adrien, J.Y. Buffiere, Three-dimensional shape of the early stages of fatigue cracks nucleated in nodular cast iron, Mater. Sci. Eng. A. 483–484 (2008), 402-405.\n K. Przybyłowicz, J. Przybyłowicz, Repetytorium z materiałoznawstwa, cz. II, Fizyczne podstawy materiałoznawstwa, Politechnika Świętokrzyska, Skrypt nr 279.\n D. Rozumek, Z. Marciniak, Fatigue properties of notched specimens made of FeP04 steel, Materials Science. 47 (2012) 462-469. https://doi.org/10.1007/s11003-012-9417-x\n G.R. Irwin, Analysis of stresses and strains near the end of a crack traversing a plate, Journal of Applied Mechanics. 24 (1957) 361-364.\n P.C. Paris, F. Erdogan, A critical analysis of crack propagation laws, J. of Basic Eng., Trans, American Society of Mechanical Engineers. 85 (1960) 528-534.\n R.G. Forman, V.E. Kearney, R.M. Engle, Numerical analysis of crack propagation in cyclic-loaded structures, Journal of Basic Eng., ASME. 89 (1967) 459-464. https://doi.org/10.1115/1.3609637\n D. Rozumek, Survey of formulas used to describe the fatigue crack growth rate, Materials Science. 49(6) (2014) 723–733. https://doi.org/10.1007/s11003-014-9667-x\n D. Rozumek, C.T. Lachowicz, E. Macha, Analytical and numerical evaluation of stress intensity factor along crack paths in the cruciform specimens under out-of-phase cyclic loading, Engineering Fracture Mechanics. 77 (2010) 1808-1821. https://doi.org/10.1016/j.engfracmech.2010.02.027\n J. Lewandowski, D. Rozumek, Cracks growth in S355 steel under cyclic bending with fillet welded joint, Theoretical and Applied Fracture Mechanics. 86 (2016) 342-350. https://doi.org/10.1016/j.tafmec.2016.09.003\n D. Rozumek, Z. Marciniak, Control system of the fatigue stand for material tests under combined bending with torsion loading and experimental results, Mechanical Systems and Signal Processing. 22 (2008) 1289-1296. https://doi.org/10.1016/j.ymssp.2007.09.009\n G. Robak, D. Krzyzak, A. Cichanski, Determining effective length for 40 HM-T steel by use of non-local line method concept, Polish Maritime Research. 25 (2018) 128-136. https://doi.org/10.2478/pomr-2018-0015']	['<urn:uuid:4d7dab07-f19b-432d-a254-4daf2de81fbe>']	factoid	direct	concise-and-natural	similar-to-document	single-doc	novice	2025-04-22T14:43:01.858830	8	9	637
40	How do the traditional customs and language practices on Amrum reflect its unique cultural heritage?	Amrum has several distinctive cultural practices and linguistic traditions. The local dialect called Öömrang, a variety of North Frisian language, is still spoken by about 800 people (one-third of the population), though High German is now the main language. The island maintains unique customs such as the Biakin celebration on February 21, where huge bonfires are lit to bid farewell to winter and people blacken each other's faces with soot. Another notable tradition is the Hulken on New Year's Eve, where young Amrum natives dress up imaginatively and run from house to house while others try to guess their identity. The island also has a traditional costume in black and white, decorated with valuable silver jewelry, which is worn by girls and women during confirmations and tourist events.	['Frisian. Pure. Culture on Amrum\nFrom the Stone Age tombs of pre- and early history, through the burial mounds of the Bronze Age, the Iron Age house, the history of seafaring and whaling, right up to modern times – there is plenty to discover here. Visit our museums and history-steeped sites, and learn some interesting facts about the lives of the islanders. It’ll be worth it.\nSt. Clemens Church with a baptismal font from the 13th century, Gothic apostolic row, and other artistic treasures. The “speaking” gravestones in the cemetery reveal the lives lived by Amrum natives when they were whalers, seafarers, and millers.\nExperience the beautiful traditional Frisian costume worn by Amrum islanders during entertainment evenings showcasing local history at the midsummer festival and village fairs.\nIn earlier times, islanders used to bid farewell to sailors during the Biakefest. Every year on February 21, the Biakehaufen is ignited on the islands. People warm themselves by the fire and then savor some kale. Truly delicious.\nVisit the Iron Age plot at the Vogelkoje with a Stone Age tomb and an Iron Age house.\nThings really heat up on Amrum in the winter! On New Year’s Eve, disguised characters – so-called Hulken – move from house to house. At midnight, everyone congregates in Norddorf in the “Hüttmannwiese” or in front of St. Clemens Church.\nArt markets and exhibitions bring you closer to authentic Frisian craftwork.\nAmrum’s lighthouse is the highest accessible tower on the North Sea coast.\nAmrum on Wikipedia\nPre- and early history\nMegalithic tombs such as the dolmen of Nebel mark the earliest traces of human settlement. They originate from the Early Stone Age. There are numerous burial mounds from the Bronze and Iron Age, for example the “Esenhugh” in Steenodde. The remnants of an Iron Age village can be found in the dune area west of the Vogelkoje. It remains disputed as to whether the Ambrones, who together with the Cimbri and the Teutons fought the Roman Republic around the year 100 BC, originally came from the area of the island that was still connected to the mainland back then.\nRelics from Viking times such as dwellings and fireplaces have been discovered at several sites. The “Borag” hill situated to the east of Norddorf is home to what is believed to be a fortified tower castle from this period (German: “Burg” or English: “fortress”). The“Krümwal”, an earthwork stretching approximately 1.5 km between Nebel and Steenodde, is also believed to originate from this time.\nMedieval and modern times up to around 1890\nIn the early Middle Ages, the island was colonized by the Frisians who arrived from the mouth of the Rhine. During the Middle Ages, Amrum belonged to the so-called Uthlande, or outer lands, which only gradually fell under the dominion of the Danish King or the Duke of Schleswig. The Danish Census Book of King Waldemar from 1231 lists two county divisions of Föhr, namely Føør voestæ (western division) and Føør østær (eastern division), with Amrum belonging to the western division. Amrum is listed in the census book as ambrum, hus, ha, co. with the presence of houses, hares, and rabbits also mentioned in the text. After conflicts between the Danish kings and the counts of Schauenburg and Holstein over the rule of Schleswig, the western division and Listland became enclaves of the Danish Kingdom and – unlike neighboring regions – did not belong to the Duchy of Schleswig. This state of affairs endured until 1864, with no change between 1460 and 1484 with the pledging of the county division to Bishop Nicholas IV of Schleswig, and 1661 to 1677 or 1683 with the sale of the division to Count Hans von Schack.\nAmrum was represented by several councilors in the county division. They were replaced by “Gangfersmänner” in 1697 whose duties included collecting taxes among other things. At the same time, the county division was turned into a Birk, which would be led by a Birkvogt who lived on the island of Föhr. The state power was usually represented by just a few people or by nobody at all in the county division, meaning that the inhabitants remained largely independent save having to pay taxes. They enjoyed additional privileges; for example they were not required to serve in the military “indefinitely” from 1735.\nIn addition to salt works, agriculture, and fishing, seafaring was one of the island’s main sources of income. Sailors from Amrum, including many captains, were active in whaling and merchant shipping between the 17th and 19th centuries in particular.\nHark Olufs, a sailor from Amrum who had been enslaved by Algerians in 1724, advanced to the rank of a General until he was allowed to return to his native island in 1736. In the 18th and 19th centuries, the recovery of stranded ships became an important source of income for the island. The number of shipwrecks on Amrum’s western coast only started to decrease significantly with the construction of lighthouses from 1875 and the application of modern navigational technologies. Tourism began to flourish in the late 19th century and this would change the island’s economic structure forever more.\nAfter the war in 1864, Amrum, just like the whole of Schleswig, was jointly ruled by Austria and Prussia. Amrum then fell to Prussia and in 1867 became part of the Prussian province of Schleswig-Holstein. The island initially formed a municipality within the district of Tønder.\nEspecially after 1864, more than a quarter of Amrum’s population emigrated, with the vast majority heading for the USA. Links between Amrum and the USA remain strong to this very day.\nFrom the start of bathing to today\nOn September 1, 1885, the architect, Ludolf Schulze of Waldhausen bei Hannover submitted a request to the island’s community representatives in the hope of securing permission to start building work on a seaside resort in Wittdün on the southern tip. Even though his request was rejected, the seed of the idea for a coastal resort had been planted. Amrum local Volkert Quedens and Heligoland native Paul Jansen Köhn seized the initiative and started building the island’s first hotels in 1889. Heinrich Andresen came to the island in 1891. He founded a joint-stock company, bought the hotels and permit from Quedens and Köhn, and built a large spa hotel on the southern tip of Wittdün and the “Kaiserhof” which were opened in 1892. Unlike in many other seaside resorts, Amrum also welcomed large ships to dock. From 1893, a railway service was operated on Amrum.\nIn Norddorf, it was Pastor Friedrich von Bodelschwingh who assumed the role of building contractor. In 1890, he was granted approval to build a facility that would later consist of several sea hospices. These were run by the deaconess house Serepta. While Bodelschwingh had a Christian vision for his sea resort, secular endeavors to enter the tourism and recreational market were also afoot in Norddorf, where the hotelier Heinrich Hüttmann, among others, was a prominent player.\nOn October 13, 1912, the municipality of Wittdün was formed from the southern part of the island; on July 25, 1925, the municipality of Norddorf was formed from the northern part. On February 23, 1926, the remaining communities were merged and renamed Nebel.\nIn the 1920 referendum on nationality, there was a clear majority in favor of Germany, whereas a large portion of the district of Tønder voted for Denmark.\nFrom the 1950s, there was an upsurge in construction activity, especially in the western part of Nebel and Süddorf. The dikes in Norddorf and Steenodde broke with the storm surge of 1962, resulting in the flooding of Amrum’s two marshy areas. Today, Amrum residents live solely on tourism. Initially, this meant guest houses with simple rooms or hotels in typical resort architecture. However, since the 1970s, holiday homes have become especially popular. The “sea hospices” have long been a thing of the past and are run nowadays by the AOK-Nordseeklinik for mother-child recuperation. Property on the island is gradually being sold off due to the high prices that sellers can demand, just like on Sylt.\nLanguage and culture\nNowadays, High German is the main language spoken on Amrum. Around one third of the population can still speak the local island dialect of the North Frisian language called Öömrang. These 800 or so Amrum natives are generally multilingual. The North Frisian dialects vary quite considerably. Öömrang is very similar to the variety of Frisian spoken on Föhr but is quite difficult for speakers of the Sylt dialect to understand even though both belong to the same dialectal branch. Many Amrum natives also speak Low German because this was the language of the coastal seafarers. Danish is spoken by just a small number of Amrum residents.\nAmrum’s traditional costume bears the colors black and white, and is richly decorated with valuable silver jewelry. It is worn by girls and women, especially at confirmations and during tourist events. See also: Traditional costume of the islands Föhr, Amrum, and the Halligen\nThere are many peculiar customs on Amrum. Biakin is celebrated on February 21 (Öömrang: Piadersinj, German similar to: Petersabend). Huge bonfires are lit in a ritual to bid farewell to winter. People blacken each other’s faces with soot. This custom dates back to the old feast day of Petri Stuhlfeier (Öömrang: Piadersdai) which was originally marked on February 22, and is also celebrated in other North Frisian communities.\nThe Hulken takes place on New Year’s Eve and involves groups of mainly young, imaginatively dressed Amrum natives running from house to house as people try to guess their identity.']	['<urn:uuid:76cb6ae7-ced7-4c16-ae33-35ace50ca540>']	open-ended	direct	verbose-and-natural	similar-to-document	single-doc	expert	2025-04-22T14:43:01.858830	15	128	1594
41	I'm doing research about women artists in history and wanted to know more about Mary Cassatt's work. What subjects did she usually paint and what influenced her style?	Mary Cassatt was best known for her paintings and pastel drawings of families and children. Her work was deeply influenced by Japanese wood block prints, which is evident in her use of strong foreground figurative placement, pattern, and color.	['Emerging in Paris in the second half of the 19th century, Impressionism’s beginnings were founded in Realism, a movement dedicated to capturing the struggles and beauty of everyday life. Impressionist painters like Edgar Degas, Alfred Sisley, Mary Cassatt, and Pierre-Auguste Renoir desired to express a quick moment, a passing glance, and the movement of light across a landscape on canvas. The creation of tubed paints allowed artists to work outdoors, and plein air painting became a catalyst for the Impressionists’ work, moving the artists away from the studio and into the environment. Technically, Impressionists were dedicated to short brush strokes and paint colors taken directly from the tube. Using moving paint to depict the world, vibrant color and light move across the Impressionist’s canvas capturing landscapes, modern scenes, and the sun’s radiant light.\nThe following collections of Impressionist painting and sculpture were selected for the scope of their contents as well as the breadth of subject matter and technique that they represent. They were also chosen for their significance within the canon of art history, and their influence on the cultural landscape both past and present.\n25. Norton Simon Museum\nHome to one of the most significant collections of Impressionist art in California, the Norton Simon Museum, once known as the Pasadena Art Museum and the Pasadena Art Institute, is home to the works of Edgar Degas, Pierre-August Renoir, and Claude Monet. The Degas collection alone comprises over 100 pieces of work by the French Impressionist, and includes many of his images of dancers, a subject for which he perhaps best known. World-renowned architect Frank O. Gehry, who is also a museum trustee, has recently renovated the museum.\n24. The Fogg Museum\nHarvard University, Cambridge, MA\nThe Fogg Museum, opened in 1895, is Harvard University’s oldest museum. Some of the most interesting paintings from 19th century France call the museum home. Known for his light, airy brushstrokes, and his ability to capture reflections of light on moving objects, Claude Monet has a number of pieces in the collection including The Gare Saint-Lazare, Arrival of a Train from 1877. The Fogg Museum’s Impressionist collection is part of the university’s museum system that includes the Arthur M. Sackler Museum and the Busch-Reisinger Museum, which are reopening this fall in a new building designed by Renzo Piano.\n23. Musee d’Orsay\nDedicated primarily to French art and artists, and originally built as a railway station, the Musee d’Orsay in Paris, France is the home to some of Impressionism’s biggest names. Renoir’s love of dance, movement, and light, is evident in the Country Dance, a large-scale piece with two figures enjoying some outdoor amusements. The painter, whose wife posed for the female figure in the painting, captures the movement of light in a sunny day in the country.\n22. The Sterling and Francine Clark Institute\nWell-known philanthropist, explorer, and horse breeder Sterling Clark and his wife Francine amassed a sizeable collection of art during the early 20th century. Collectors of Dutch and Italian Old Masters paintings, the Clarks slowly acquired a number of primarily French Impressionist works in the early 20th century. The collection includes works by Berthe Morisot, Pierre-Auguste Renoir, Edgar Degas, and Alfred Sisley. Sisley, known for his plain air work, was one of the movement’s most dedicated landscape painters. His painting, Tames at Hampton Court from 1874, captures the warmth of a summer afternoon on the river, and is one of the collection’s excellent examples of Impressionism.\n21. Musee Marmottan Monet\nThe Musee Marmottan Monet houses what is perhaps Impressionism’s most valuable painting, Impressionism, Sunrise created by Claude Monet in 1872. In the piece, a depiction of the harbor of Le Havre, France Monet uses short expressive brush strokes to simply suggest the watery landscape. The painting’s title provided the Impressionist movement with its name, and the style, meant to imply the subject rather than directly represent it set off controversy among critics worldwide. The painting was stolen in 1985 and was recovered five years later, and is currently part of the world’s largest collection of Monet’s work.\n20. The National Museum of Western Art\nClaude Monet painted numerous images of water lilies, including one from 1916 that is part of the of The National Museum of Western Art in Tokyo. The collection, compiled by Japanese businessman Matsukata Kojiro, became established as a museum in 1959, nine years after Kojiro’s death. Kojiro also purchased Auguste Rodin’s Gates of Hell sculpture, which is currently located at the Musee Rodin in Paris.\n19. The Phillips Collection\nKnown as the first museum of modern art, Washington D.C.’s The Phillips Collection houses some of the 20th century’s most seminal works. Founded in 1921, Duncan and Marjorie Acker Phillips built a collection that rivals those of many of the world’s most famous museums. Pierre-Auguste Renoir’s most important work, Luncheon of the Boating Party from 1875, is perhaps the collection’s most famous piece, telling the story of an outdoor party rich with light, airy brushstrokes to mimic the ambiance of the painting’s festive setting.\n18. National Gallery\nThe heart of Impressionism is in the movement’s desire to capture light and to render an “impression” of a place. Claude Monet embodied this desire in his numerous series that incorporate images of cathedrals, landscapes, and the lily pond at his home in Giverny. The Water Lily Pond from 1899 is one in a series of nearly 250 pieces of the subject and it currently resides in the National Gallery in London. Home to a number of important pieces from the period the gallery’s collection include Degas’ Beach Scene from 1869, and Renoir’s At the Theater from 1876, Monet’s water lilies sit alongside scenes of middle class life and landscapes that radiate with life.\n17. Kimbell Art Museum\nOne of the greatest collections of art in the Southwest, Fort Worth’s Kimbell Museum houses an Impressionist collection that includes works by Monet and Degas. Texas collectors Kay and Velma Kimbell assembled the collection, housed in a building by architect Louis Kahn, over a number of decades.\n16. The Metropolitan Museum of Art\nNew York City, NY\nFounded in 1870, The Metropolitan Museum of Art in New York City is America’s largest museum. The collection includes nearly 50,000 pieces of European painting, sculpture and decorative art, including a number of Impressionist paintings. Edouard Manet, known for his paintings that depicted working class people and modern life, is known as one of the pivotal figures in the transition from Realism to Impressionism. His piece, Mlle. Victorine Meurent in the Costume of an Espada reveals the artist’s pre-Impressionist work. Unlike an Impressionist scene, which is often taken from life, the piece is staged and flat and somewhat stiff. The brush strokes are refined except for the figures in the background whose Impressionist movements peek from behind their more polished counterparts.\n15. Museum of Fine Arts Boston\nSchool of the Museum of Fine Arts Boston, Boston, MA\n14. Shelburne Museum\nManet’s In the Garden, is part of the Shelburne Museum in Shelburne, VT. Painted in 1870, one can see Manet transitioning between Realism and Impressionism. The soft colors blend together as if the viewer has taken just a quick glimpse of his subject. The museum’s collection, which was assembled by Electra Havemeyer Webb in the early 20th century, contains work by Monet and Degas as well as a pastel drawing of a mother and child titled Louisine Havemeyer and Her Daughter, Electra.\n13. J. Paul Getty Museum\nLos Angeles, CA\nPerhaps one of the world’s most interesting collections in the United States, the J. Paul Getty Museum is home to the work of Monet, Renoir, and Sisley. The collection includes Manet’s The Rue Monsier With Flags from 1878, and again marks a moment in time, a quick glimpse of the 1878 Exposition Universelle, painted from Manet’s studio window above the street.\n12. Musee de l’Orangerie\nTo house eight large panels of Monet’s water lilies, the Musee de l’Orangerie in Paris constructed two oval galleries. The galleries, whose walls are covered in water lilies, transport visitors directly to Monet’s garden at Giverny. Most famous for this particular cycle of paintings by Monet, the museum, once the orangery of the Tuileries Palace, has a number of other Impressionist works by Sisley and Renoir.\n11. Corcoran Gallery of Art\nCorcoran College of Art and Design and George Washington University, Washington D.C.\nOriginally founded in 1869 by William Corcoran, the gallery’s primary focus was to collect important works of American art. As time passed, the gallery grew to include a number of important works by European painters including a significant number of French Impressionist works. Like many Impressionist painters, Degas dedicated himself to the study of movement and light, and his work is best showcased in his numerous paintings, drawing, and sculptures of ballet dancers at the Paris Opera House.\n10. Musee Rodin\nWhile Impressionist artists primarily focused on color and paint on canvas, Auguste Rodin’s sculptural works are often cited for their Impressionistic qualities. Dedicated to capturing movement and feeling, his large figurative works looks as though they have been pulled directly from an Impressionist’s canvas. The Musee Rodin’s collection is dedicated to the work of this vital artist and his dogged attempts to capture movement in marble and bronze.\n9. The Art Institute of Chicago\nThe School of the Art Institute of Chicago, Chicago, IL\nMary Cassatt, one of Impressionism’s few female artists, is best known for her paintings and pastel drawings of families and children. The museum’s collection houses one of her most famous works, The Child’s Bath from 1893, as well as nearly 30 works by Monet alone. Like many of the Impressionists, Cassatt’s work was deeply influenced by Japanese wood block prints, and is evident in her use of strong foreground figurative placement, pattern, and color.\n8. Courtauld Gallery\nUniversity of London, London, England\nKnown primarily for its Impressionist collection, London’s Courtauld Gallery showcases the work of Monet and Degas as well as those of post-Impressionist artists Cezanne and van Gogh. One of the gallery’s most noteworthy pieces is Manet’s A Bar at the Folies-Bergere from 1882. Known as a piece that embodies the atmosphere of late 19th century Paris, the painting captures the light and chaos of one of the city’s most famous nightclubs.\n7. Frederic C. Hamilton Collection\nDenver Art Museum, Denver, CO\nThe Denver Art Museum’s Frederic C. Hamilton Collection showcases a considerable number of works of art from around the world and has a particularly significant collection of Impressionist paintings. Along with works by Morisot and Monet, the museum includes work by Pissarro including his plein air painting, Autumn Poplars from 1893. Pissarro, Impressionism’s oldest member, was a pivotal figure in the education of a number of his successors. Mentor to post-Impressionist painter Paul Gauguin, and an inspiration to Renoir, Pissarro’s work was essential to the establishment of Impressionism’s initial theories.\n6. Milwaukee Art Museum\nWhile only a few Impressionists worked in a medium other than painting, Degas often used bronze in an attempt to better understand his subject. Many of his smaller sculptures of dancers and figures were used in his studio to help him flesh out the more formal aspects of a painting. The Milwaukee Art Museum’s extensive Impressionist collection showcases a number of Impressionist works including Degas’ Dancer Holding Her Right Foot in Her Right Hand. Not intended for gallery display by Degas, the bronze figure seems to imply the passing movement of a ballerina as she prepares herself for the evening’s performance.\n5. Tate Britain\nKnown as one of Impressionism’s great inspirations, the work of British artist JMW Turner is considered by many to be an early catalyst of Impressionism. His airy brush strokes, and commitment to light are evident in his washy oil painted landscapes and watercolors. One of Britain’s largest collections, Tate Britain is home to a number of works by the artist including Snow Storm-Steam-Boat off a Harbour’s Mouth from 1842. Turner’s work exemplifies much of what the Impressionists were after: light across a landscape, a passing storm, and flickering, watery light.\n4. Museum of Modern Art\nNew York, NY\nHome to one of the world’s best collections of Modern art, the Museum of Modern Art in New York also has a significant number of Impressionist pieces. Considered an early tenet to Modernism, Impressionist artists like Monet and Degas heavily influenced Modernist successors Cezanne, Picasso, and Matisse. In Monet’s Poplars at Giverny, Sunrise, one can almost envision a future Jackson Pollack spilling movement and light across his outstretched canvas.\n3. The Chester Dale Collection\nNational Gallery of Art, Washington D.C.\nThe National Gallery of Art in Washington D.C. houses a number of significant collections including American banker Chester Dale’s group of 19th century French paintings. Mary Cassatt’s painting The Loge from 1882 is one of the collection’s seminal works, depicting two young women in their box seat at the theater. Using short brush strokes to evoke the excitement and interest of the subject, Cassatt successfully brings the two figures to life.\n2. The Davies Sisters Collection\nNational Museum Cardiff, Cardiff, UK\nWelsh sisters Gwendoline and Margaret Davies assembled one of Britian’s most important collections of Impressionist art that contains work by Monet, Rodin, and Pissarro. The sisters donated 260 works of art to the museum in the early 20th century including Monet’s Rouen Cathedral Setting Sun (Symphony in Grey and Pink). The painting, another in a series by the artist, evokes the movement of warm sunlight as it cascades across the cathedral.\n1. Barnes Foundation\nKnown across the globe as the home of what is perhaps the most significant collection of painting, sculpture, and objects from ancient cultures as well as modern masters, the Barnes Foundation is steeped in controversy and beauty. Recently relocated from its original location in Merion, PA, the collection, amassed by chemist Albert C. Barnes contains one of the most complete collections of Impressionist and post-Impressionist art in the world. With 181 paintings by Renoir alone, the Barnes Foundation has work by Monet and Degas as well as their successors Picasso and Matisse. The foundation was established in 1922, and was carefully curated by Dr. Barnes with the intention of showcasing a variety of objects including those of regional significance beside the works of modern masters. The grounds were carefully landscaped to align with the works inside, and all aspects of the collection’s layout were thoughtfully considered.']	['<urn:uuid:a85c5994-5b9a-40d8-8aa3-743bd0b17f4b>']	factoid	with-premise	verbose-and-natural	distant-from-document	single-doc	novice	2025-04-22T14:43:01.858830	28	39	2397
42	working for nonprofit need get crime stats san diego what problems might encounter	Getting crime data in San Diego is particularly challenging. Unless you are a journalist or student, SANDAG (which controls crime data) often restricts access by charging high access fees ranging from $500 to $3000, limiting the number of records provided, publishing data in difficult-to-use formats, or outright refusing to release the data. For most nonprofits, crime data adequate for long-term trend analysis is effectively impossible to acquire without involving lawyers.	['Municipal governments control many datasets that are very useful to social services, community planning groups, health advocates, and many other civic and social organizations. This document assesses data needs for these organizations and proposes that municipal governments should commit to publishing these datasets rather than requiring data users to request them.\nProperly managing any modern organization requires data, no less so for the social and civic organizations that form the backbone of a healthy community. Unfortunately, these organizations face roadblocks acquiring and using data they need. The problem is not the existence of data—we have never had more data than we do today—but rather the difficulty associated with getting the correct data in usable forms. Correcting this situation must start by studying who in a community uses data and what needs they have.\nThis document presents the results from a series of interviews with various types of data consumers. This analysis does not aim to be comprehensive; rather, it is representative of a sample of types of data users. The goal is to identify some of the high-value San Diego municipal government datasets that would benefit civic development if they were released to the public.\nUsers from a range of civic and social organizations were interviewed about their data use, their data needs, and the problems they have with using data. About 30 users from 21 groups and organizations were interviewed, including:\n- 8 community planning groups\n- 3 affordable housing advocates and developers\n- 3 city council members\n- 7 other nonprofits\nThere are many more users and user types to consider in a complete analysis, but because a small number of datasets were mentioned by most of the interviewees, we believe that this set is sufficiently representative for an initial assessment.\nThe interviewees reported using or wanting a range of data that fell into these categories:\n- Housing. Types and locations of housing, housing costs, market rents, and ownership.\n- Crime & Public Safety. Incident-level data about arrests. Calls for service.\n- Infrastructure. 311 calls, construction permits, capital improvement projects, and defects.\n- Transportation. Street segment traffic counts and collision locations.\n- Business & Economy. Business locations and aggregated tax payments.\n- Homelessness. Locations of homeless people, encampments, and related arrests.\n- Health. Disease prevalence.\nThe datasets in these categories primarily originate in government organizations, although some are available from private organizations, both commercial and nonprofit, as summarized in the table below.\n|Municipal Govt||State Govt||National Govt||Private Commercial||Private Nonprofit|\nTable 1. Data categories and primary sources.\nAmong the datasets and sources identified, those that were most inaccessible and had the highest marginal value were those held by municipal governments. The following datasets held by municipal governments were mentioned most frequently as being valuable to civic and social organizations:\n- Summaries of arrests\n- Summaries of service calls for fire and police\n- Street segment traffic counts\n- 311 reports\n- Construction permits\n- Capital improvement projects\n- Business tax payments, aggregated to census block by NAICS code\nThese are datasets that municipal open government efforts should strive to make available.\nLimits to Data Access\nInterviewees reported that many valuable datasets were difficult or impossible to acquire, usually for one of the following reasons:\n- The dataset is owned by a private corporation, and the price is too high.\n- The data is available, but because of privacy issues it is aggregated to a high-level geography. For instance, the data reports values per county, which cannot be used for analyzing neighborhoods.\n- The data is available, but privacy rules like HIPAA mandate controls that the organization can’t implement.\n- The data is held by an agency incapable of releasing the data.\n- The data is held by an agency that refuses to release the data.\nThe first three problems are matters of cost or negotiation and fall within the realm of normal business issues. While these access issues are obstacles, they can be overcome with time, money, or creativity. The final two are more problematic and will be considered in more detail.\nItem (4), limited access because of agency capability, often appears to be the result of an inflexible, uncooperative bureaucracy, but is really a lack of technology and process. Few government agencies, particularly at the municipal��������������������������������������level, have the software and processes to release data. Data has not been vetted to ensure that releasing it will not pose risks to the organization, and the agency staff don’t have software support for releasing it easily to the web, so data releases can be ad hoc and time-consuming, a task for which there is no additional budget. These issues can be solved by tackling the issue directly, most effectively with a high-level mandate for increased openness and transparency, followed by software and services to limit the cost of the mandate.\nProblem (5), limited access because of agency intransigence, is much more difficult. The agency that was mentioned most frequently (and with one exception, solely) as being unwilling to release data is SANDAG. Interviewees noted difficulty in getting crime data, traffic counts, and traffic model inputs and outputs from SANDAG. SANDAG restricts access to these datasets by charging access fees, limiting the number of records provided, publishing the data only in difficult-to-use formats, or by an outright refusal to release the data. Several interviewees said that the only way to get SANDAG to cooperate is to have a lawyer send the request, sometimes with the threat of a lawsuit.��Because the datasets that SANDAG controls—crime and traffic data—have a high social value, and because the time, cost, and effort to force SANDAG to comply with Public Records Requests is so onerous, their disinclination to release data imposes a significant social cost.\nIn a detailed analysis of 18 Public Records Requests to SANDAG for crime and traffic data over the last three years, requests made by journalists and county employees were handled quickly and without cost. Requests made by citizens who were not journalists or students were not fulfilled—largely denied indirectly by offering to fill these requests at costs ranging from $500 to $3000. Although this limited analysis cannot make a definitive assessment of SANDAG policy, it appears SANDAG management is restricting access to data by setting unreasonably high fees.\nRecommendations for Better Civic Use of Data\nTo ensure that cities realize the full value of the data they produce and manage, we recommend that municipal governments:\n- Require release. Identify datasets of high public value, and mandate their release.\n- Make release easy. Provide departments with simple tools for publishing data.\n- Make use easy. Work with external organizations to serve the needs of the community.\nThese recommendations will increase the pace of civic development in the San Diego region, and, through public-private partnership, can be implemented with minimal cost.\nRequire Release: Identify Datasets and Mandate Release\nMunicipalities should identify the datasets that have the highest social value and formally mandate that departments release that data.\nWhen data has a social value, not releasing the data has a social cost, a cost which impedes community development. We recommend that municipal governments identify the data that has the highest value to citizens, nonprofits, planning groups, and businesses and formally mandate its release, requiring agencies to publish data to the Internet, rather than requiring citizens to use the PRA process.\nDeveloping a list of the datasets to release should involve an extension of this analysis, followed by a community process to ensure that the datasets are valuable to users. The community process should involve government departments from all of the regional governments, because the most common users of data shared by a governmental department are staff at other government departments.\nThis initial analysis suggests that the most important datasets for municipal governments to release are in the categories of crime and public safety, infrastructure, and transportation—datasets that should be considered in more detail.\nCrime and Public Safety\nAs the largest single category in a city’s budget, it should be expected that data about public safety is important to communities. Crime and public safety data are valuable for a wide range of uses but are also the hardest to acquire. This data should be considered commensurate with the demographic data collected by the US Census and formally released to the public. Instead, the data is restricted and limited, with a few notable exceptions, such as the San Diego Fire Dispatch online application.\nMunicipalities should mandate the release of:\n- Summary arrest records, including time, date, type, location, and description of crime.\n- Calls for service, for both police and fire.\n- CAPPed property lists.\nIn California, summary arrest records are considered public data and include the time, date, category, location, and brief description of arrests. However, despite being explicitly referenced as public data in the California Public Records Act, San Diego Police departments and SANDAG are very reluctant to release crime incident data, and will only release small numbers of records for a fee unless the Public Records Act request is issued by a lawyer. For most nonprofits, crime data adequate for long-term trend analysis is effectively impossible to acquire.\nBecause of the value to the community and the difficulty in obtaining these datasets, crime and public safety data should be a top priority for transparency efforts to secure and ensure continued public access.\nSecond only to public safety, the maintenance of streets, sidewalks, utilities, and other infrastructure is a major category attracting citizens��� complaints, concern, and comments. By releasing infrastructure information, citizens are better able to understand the city’s budget constraints and can become participants in the process of setting priorities, rather than being adversaries.\nSome of the infrastructure datasets identified in this analysis as valuable are:\n- Construction permits\n- Capital improvement projects\n- 311 calls\n- Asset maps, locations of street lights, and traffic lights\n- Other contracted work\nOur initial analysis indicates that, at least in the City Of San Diego, the departments that control these datasets are very interested in making them available to the public, with the primary impediment being the effort required to release them. Although the political will exists to release more data, a mandate to release these datasets may still be valuable, as it would allow the departments to increase the priority of data efforts when creating their budgets.\nWhile citizens are concerned about traffic congestion and road safety, it is mainly nonprofits who use traffic data. Transportation data is instrumental to the operation of transportation advocates like Move San Diego and Walk San Diego, policy centers like the Equinox Center, affordable housing advocates who study jobs/housing fit, health advocates, social service organizations, and many others. Unfortunately, good, usable traffic counts and traffic model inputs and outputs are very difficult to get from SANDAG.\nWe recommend that cities either require SANDAG to publish the following datasets as spreadsheets rather than PDFs, or request the data from SANDAG and publish it themselves. Some of the most valuable traffic datasets are:\n- Traffic counts\n- Traffic surveys\n- Transportation model outputs\nBecause these datasets have been restricted and difficult to obtain, mandates to publish them would be particularly valuable.\nMake Release Easy: Provide Simple Software and Dedicated Help\nTo ensure that municipal staff participate in data release efforts, it is important to make releasing data easy and to provide dedicated support, preferably without impact to departments’ budgets.\nFew municipal departments already have the processes, staff, and software for properly releasing data, and the experience of many data management efforts is that without high-level mandates, support, and training, programs that add extra work for staff are likely to fail.\nA solution to this problem is to allow staff to submit data in whatever form is easiest for them and centralize the collection, cleaning, and publication of data outside the departments that provide the data. Then, as the data release programs mature, departments can add specialized software and processes to make data release more formal and automatic.\nWe recommend that municipalities make data release easy for staff by providing:\n- Simple software for staff to submit data, such as one of the Open Source data repositories, CKAN or DKAN.\n- A dedicated person who can work across departments to help staff find and upload data.\n- Data cleaning and publication services that are outside of the departments that produce the data.\nData is most valuable when it can be combined with other data, which requires that datasets adhere to standards that establish common fields where links can be made. Data use is most efficient when data users are consulted about their needs so the data can be structured to make analysis easy. The easiest way to accommodate both of these requirements is to provide a single organization that works with both data users and data producers to broker information exchange.\nBecause the goal of the data release effort is to make data public, and most of the data users are outside of municipal government, the role of the data broker can be played by a non-governmental organization; this is the intent behind the San Diego Regional Data Library. Using an external organization has many benefits, including a broader connection to users, more flexible cost structure, and immunity from changes in administration. However, cities that want more control to distribute data that cannot be made public can set up internal data departments using the same software and processes as those employed by an external organization.\nMake Use Easy: Use Partnerships to Serve Data Users\nData is only valuable if it is usable, and making data usable requires studying users, a task better suited to private organizations who work closely with users.\nWhile it is natural for a municipal government pursing a transparency program to publish its own data, many cross-community efforts have concluded that data distribution has the highest value when data is stored and distributed outside of government. This was the conclusion of an effort that began in San Diego in 2011, led by Planning Director Bill Andersen and the National Neighborhood Indicators Partnership. Nationwide, many of the most successful data intermediaries, such as the Connecticut Data Collaborative, are public-private partnerships.\nGovernment���������s internal efforts can begin with good intentions, but because they are subject to the changing priorities of each transition in administration, a stable, long-term effort should be a partnership with government, not a program of government.\nThere are a variety of models that can be employed, ranging from having a nonprofit run centralized repositories for both government and private data users, to distributed models with multiple data repositories. Regardless of the model chosen, it is important to recognize the importance of working closely with end users to understand their needs and ensure that the data products produced are useful. Working with organizations that have close connections to data users is an excellent way to satisfy this requirement.\nOpen Data Accelerates Community Building\nMunicipalities exist to organize, protect, and develop communities, and they can better serve this mission, at a lower cost, by allowing the community to more fully participate in data collection, analysis, and use. Making the best use of data will require both government and private organizations to work together, and we suggest that the best arrangement for the San Diego region will involve:\n- Municipal governments identifying the high-value datasets they produce and mandating their release.\n- Community representatives, such as Regional Taskforce for the Homeless, the San Diego Housing Alliance, the Malin Burnham Center for Civic Engagement, and many other social and civic organizations, woking together to identify data needs.\n- Data producers, such as universities, hospitals, social service organizations, and governments collaborating to share and distribute data.\n- Data intermediaries, such as the San Diego Regional Data Library, providing technology and management support to the collaborations.\nThe cities of the San Diego region have an unprecedented opportunity to accelerate community development efforts region-wide by creating the type of data collaboratives that other cities have benefitted from for decades. Building the technology and relationships will take time and dedication from many players, so we should start now. We at the San Diego Regional Data Library look forward to contributing to this effort. If you are or your organization is building a piece of this new civic infrastructure, we’d like to meet you; please contact the Director of the Library, Eric Busboom, at firstname.lastname@example.org or 858-386-4134.']	['<urn:uuid:0a0e9bc5-fa14-48c5-a856-fe73334a5a6c>']	open-ended	with-premise	long-search-query	distant-from-document	single-doc	expert	2025-04-22T14:43:01.858830	13	70	2718
43	treatment options melanoma kidney success	Both melanoma and kidney cancer have various treatment paths with positive outcomes when caught early. For melanoma, treatment options include surgery to remove the cancer with a margin of normal skin, chemotherapy to slow cancer cell growth, immunotherapy to help the body's immune system fight cancer, and targeted therapy to block cancer cell signals. For kidney cancer (renal cell carcinoma), the survival rate is high if caught in early stages, with modern surgical procedures and promising treatment options having improved the outlook for many patients. Treatment decisions for both cancers are guided by staging processes that help determine the most effective approach.	"['Does Alcohol Lead to Liver Cancer? Many people know that abuse of alcohol can lead to such damage to the liver that it puts the organ at risk for cancer. But why does that happen? The liver is an organ that removes toxins from the body, and ethanol can be thought of as a type of poison the liver has to break down and remove. This is why alcohol coming into contact with the cells of the liver is inevitable after a person’s had a drink. Role of Acetaldehyde Medical\nLearning About Liver Cancer Many people do not experience any signs of liver cancer in its earliest stages, which can make it difficult to diagnose until it’s in the advanced stages. Fortunately, there are many treatment options that can help you fight liver cancer once it has been diagnosed. Liver cancer occurs when the cells in the liver develop mutations (changes) to their DNA. DNA provides directions for every chemical process that occurs in your body. Mutations to DNA change the instructions it provides. As a result, cells may begin\n5 Ways to Prevent Liver Cancer Liver cancer is a frightening disease, but a little bit of prevention can go a long way in ensuring you don’t develop this condition. While you can’t always control whether or not you get cancer – for instance, sometimes people develop cancer because of their genetics – a healthy lifestyle can help you live a long and cancer-free life. Here are five ways you can cut down your odds of developing liver cancer during your lifetime. 1. Protect yourself from hepatitis B and C.\nResponsibly Identifying Kidney Cancer Symptoms Kidney cancer, medically speaking renal cell carcinoma, has various manifestations leading to extreme levels of fatigue, weight loss, and back pain in more than 50,000 people diagnosed with it in the United States last year. While some kidney cancer patients are not diagnosed until their cancer has metastasized to other areas of the body, for many, the diagnosis is not necessary the end-of-the-road. Modern advancements in surgical procedures, promising treatment options and diagnostic protocol have changed the outlook for many with cancer of the kidneys.\nCommon Risk Factors for Kidney Cancer Kidney cancer becomes more common with age, typically affecting adults in their 60s and 70s. Also called ‘renal cancer’, it is rare in individuals under the age of 50. When caught early, the prognosis can be quite positive. Early detection can be a challenge, though, as there are no obvious symptoms from the outset. When symptoms are exhibited, they can include blood in the urine, persistent pain in the sides and/or lower back, or a swelling in the side. Often times, kidney cancer may\nThe Stages of Kidney Cancer If you’ve received a kidney cancer diagnosis, one of the first steps toward treatment is to determine the stage of your cancer. This process, known as staging, gives your doctor a way to categorize your condition in terms of location as well as the extent to which it has spread. This process also helps your doctor determine effective treatments. In addition to determine the trajectory for treatment, staging also enables your doctor to give you a more accurate determination of recovery or outlook. Outlook is\nTreating Renal Cell Carcinoma Kidney Cancer Renal cell carcinoma is the most common type of kidney cancer. It causes cells to divide at an aggressive rate, so a tumor forms on one of the kidneys. It is rare that a person will develop tumors on both of their kidneys simultaneously. More often than not, patients don’t realize that they have cancer until the tumor has grown to be fairly large. Luckily, this type of cancer has a high rate of survival if it is caught in its early stages though.\nThe Symptoms, Treatment and Prospects for Kidney Cancer Kidney cancer is one of the 10 most common cancers in the United States. There will be nearly 64,000 cases of kidney cancer diagnosed in 2017, and the majority of them will be older men. Most kidney cancers are diagnosed in people over 60. It’s very rare in younger people. About 14,400 people will die of kidney cancer. The risk of getting this type of cancer throughout a person’s lifetime is about 1 in 63. Kidney Cancer Symptoms Symptoms of kidney cancer\nAll About Melanoma and Treating the Disease One of the most common types of cancer that affects millions of Americans each year is melanoma. Melanoma is the most serious kind of skin cancer. It develops in cells called melanocytes that produce melanin, which is responsible for giving the skin its color. Melanoma can develop not only on your skin but in the eyes and even internal organs in rare instances. Taking a look at melanoma pictures is important if you have any kind of outstanding mole or moles anywhere on\nThe Reality Of Melanoma Individuals with melanoma have cancer of the skin that begins as melanocytes. These are cells on the body that create moles. It’s important to remember not all moles develop melanoma. Different melanoma types can occur on various parts of the body. There are melanoma signs a person should look for on their body Types Of Melanoma There are three main melanoma types. Mucosal: This occurs in the body’s mucous membranes. This includes throat, mouth, nasal passages and more. Cutaneous: This is melanoma of the skin. It', ""|Article Views: 127|\n|Skin cancer melanoma: Symptoms, diagnosis and treatment|\n|Posted on Oct 15, 2015|\n|Melanoma is a form of skin cancer that develops in the cells that produce the pigment that gives your skin its color- melanin. It is by far the most aggressive forms of skin cancer and can also form in your eyes and, uncommonly, in internal organs, such as your intestines. |\nThe precise reason behind all melanomas is unclear, but persistent exposure to ultraviolet radiation from sunlight or tanning lamps and beds increases your risk of developing melanoma. Restricting your exposure to UV rays can help\xa0reduce your risk of melanoma.\nMelanoma skin cancer occurs in people over 40 years of age and especially in women.\nSymptoms related to Melanoma skin cancer\nMelanomas can appear anywhere on your body, mostly on the back, legs, arms and face which are directly exposed to harmful sun rays. They can also develop on less exposed areas such as soles of your feet, palms of your hands and fingernail beds. Signs and\xa0symptoms of melanoma skin cancer\xa0include:\n- Any change in size, color, shape, or texture of a mole or other skin growth\n- An open or inflamed skin wound that won't heal\n- A change in an existing mole\n- A small, dark, multicolored spot with irregular borders that may bleed\n- A cluster of shiny, firm, dark bumps\n- A mole larger than a pencil eraser\n- A flesh-colored oval bump with a rolled border, which may develop into a bleeding ulcer\n- A reddish, brown, or\xa0bluish black patch of skin\xa0on the chest or back\n- A firm, reddish, wart-like bump that grows gradually\n- A flat spot that becomes a bleeding sore that won't heal\nTests and diagnosis for Melanoma skin cancer\nIn order to check for melanomas, your skin specialist may:\n- Perform a\xa0physical examination of your skin.\n- Do a skin biopsy which involves taking a sample of your skin and have it tested for melanomas.\n- Examine your lymph nodes to see if they are unusual in any way. This may be followed by an examination to check whether melanomas have spread to the lymph system.\n- Use imaging tests like PET scan, CT scan, and MRI to see whether cancer has spread to other parts of your body, such as the lungs, brain, or liver.\n- Full body photography to look for changes in any mole and to watch for new\xa0moles appearing in normal skin.\nTreatment for Melanoma skin cancer\nMelanoma skin cancer can be effectively cured if it's found and treated in its early stages when its impact is limited to skin only. Several treatment options are there to cure melanoma skin cancer including:\n- A surgery where entire melanoma is removed along with a margin of normal-appearing skin.\n- Chemotherapy in which medicines are used to stop or slow the growth of cancer cells.\n- Immunotherapy in which medicines are used to help your body's immune system fight the cancer.\n- Targeted therapy in which medicines are used to prevent cancer by blocking signals in the cell. The therapy is given only when a patient shows changed cell mutation.\nRegular follow-up consultations are important after you have been diagnosed with melanoma. Follow guidelines from your\xa0skin doctor.\n|Written by : Lazoi Team |""]"	['<urn:uuid:2e87eaca-ab88-40e5-8a24-eba40a543f3e>', '<urn:uuid:5567a842-75b8-43bb-919e-666a6f1d879b>']	open-ended	with-premise	short-search-query	distant-from-document	multi-aspect	novice	2025-04-22T14:43:01.858830	5	102	1458
44	what causes nmo vs pituitary tumors	NMO is an autoimmune disorder where the immune system attacks healthy cells in the optic nerve and spinal cord, with some cases linked to tuberculosis. In contrast, most pituitary tumors arise spontaneously and sporadically, are not genetically inherited, and their exact cause remains unknown.	"['Pituitary Tumor: Endoscopic Procedure Revolutionizes Pituitary Surgery\nOne of the most extraordinary advances pioneered at the Skull Base Institute is the minimally invasive, fully endoscopic approach to treating pituitary tumors (pituitary adenomas) and other skull base disorders. This innovative procedure utilizes a tiny endoscope – 2.7 mm wide and 20 cm long – with angled tips that is inserted through the nostril and into the skull base. This approach offers numerous advantages in terms of the surgery and recovery period.\nFirst, since the camera is “placed” at the tip of the endoscope, surgeons have a vivid panoramic view of the brain. They can look around corners and make a full visual assessment. This panoramic view also provides surgeons with the ability to remove the entire tumor in most cases. This process is in sharp contrast to the traditional approach, which requires viewing the tumor site through a microscope outside the skull at a focal distance that limits visibility.\nSince the point of entry is through a nostril, no incision is required. Consequently, there is no scarring, no nasal packing, and the brain is undisturbed. The time required for the actual surgical procedure, the length of hospital stay and overall recovery time are dramatically reduced. Patients return home within 24-48 hours of surgery and enjoy a rapid overall recovery, and return to work and normal activities.\nThe Fully Endoscopic technique was simultaneously and independently pioneered at the Skull Base Institute and another major university medical center in Pittsburgh in 1996. Since then more than one thousand patients with pituitary tumors from all fifty states of our nation and international patients from countries such as Australia, Belgium, Canada, Egypt, France, Germany, Israel, Italy, Japan, Mexico, Poland, South Korea, Switzerland have sought surgical care at the Skull Base Institute.\nToday, our research and development is focused on yet further innovating the field of Minimally Invasive Brain and Skull Base surgery in general and pituitary surgery in particular. High definition intraoperative digital imaging, 3D endoscopy, custom designed microinstruments, headsup displays and even nanotechnology are all being tapped in order to provide our patients with the most cutting edge and yet least invasive approaches to rid them of their tumors.\nMost pituitary tumors are benign overgrowths (adenomas), while malignant overgrowths (pituitary carcinomas) are very rare. Pituitary tumors arise primarily from the anterior pituitary gland (adenohypophysis) whereas the posterior pituitary gland (neurohypohysis) rarely gives rise to tumors. The pituitary gland is multifunctional in nature and it secretes a variety of “key hormones” that regulate and control the activity of virtually all other glands of the human body. There are many different kinds of pituitary tumors.\nPituitary tumors include:\nClassically, pituitary tumors are divided into two groups: functional (secretory) and non-functional (non-secretory), the latter usually do not present until reaching a sufficient size to cause mass effect or compression on the surrounding neurovascular structures, mainly the optic nerves, whereas the former frequently present at an earlier stage with symptoms caused by the physiological effects of the excess hormones they secrete. This distinction is not always adhered to as “secretory” tumors may grow large enough to cause symptomatic mass effect and “non-secretory” tumors, can grow and destroy the normal pituitary gland leading to a decrease in the secretion of some or all of the pituitary hormones, a condition known as panhypopituitarism.\nPituitary tumors represent approximately 10% of all intracranial tumors. They are most common in the third and fourth decades of life, and equally affect both sexes. Approximately 70% of pituitary adenomas produce hormones (secretory) while 30% do not produce any hormones (non-secretory).\nThe key hormones secreted by the anterior pituitary gland are:\n- Growth hormone (GH) which, as the name indicates, regulates growth\n- Adrenocorticotropic hormone (ACTH) which stimulates the adrenal glands to produce cortisol\n- Thyroid-stimulating hormone (TSH), which stimulates and regulates the activity of the thyroid gland; also known as Thyrotropin\n- Luteinizing hormone (LH) and follicle-stimulating hormone (FSH), the effect of these hormones depends on one’s sex, in women they regulate ovulation, estrogen and progesterone production, and in men they regulate sperm formation and testosterone production\n- Prolactin (PRL), during pregnancy this hormone helps prepare the breasts for future milk production and after birth it promotes the synthesis of milk\n- Melanocyte-Stimulating Hormone (MSH), which regulates the production of melanin, a dark pigment, by melanocytes in the skin\nThe key hormones secreted by the posterior pituitary gland are:\n- Oxytocin, which stimulates contractions of the uterus during labor and the ejection of milk during breast-feeding\n- Antidiuretic hormone (ADH), which increases reabsorption of water into the blood by the kidneys and therefore decreases urine production; also known as Vasopressin\nMost pituitary tumors arise spontaneously and sporadically and are not genetically inherited. The events within the cell that lead to disordered pituitary cellular growth and/or hormonal over-secretion are not yet fully understood. Like most other tumors, the actual cause of pituitary tumors remains unknown.\nOther causes such as aneurysms, tumors, chronic meningeal inflammation, or other lesions may irritate trigeminal nerve roots at the pons and cause symptoms of TN to occur, these can be identified and ruled out by MRI scan of the brain.\nIn general, pituitary tumors become symptomatic either due to an endocrinologic disturbance or due to mass effect on the surrounding neurovascular structures. Other symptoms are specific to the specific type of pituitary tumor, such as Cushing’s disease, acromegaly, hyperprolactinemia, etc.\nSymptoms of pituitary tumors vary depending on the size (microadenomas are smaller than one centimeter and macroadenomas are larger than one centimeter) and location of the tumor and whether the tumor affects the secretion of hormones by overproduction (hypersecretion) or under production (hyposecretion) of one or more of the pituitary hormones, and whether it exerts pressure on nearby neural or vascular structures (the optic nerves are particularly vulnerable due to their close proximity to the pituitary gland).\nSome patients with large tumors may have acute hemorrhage into the tumor, a condition known as “pituitary apoplexy” which causes sudden onset of headache, visual loss, double vision, and/or pituitary failure. This condition is a medical emergency and immediate decompressive surgery is indicated.\nIn addition to complete medical history, physical examination and hormonal studies; magnetic resonance imaging (MRI) is the investigation of choice to diagnose pituitary tumors as it is able to detect these tumors at a very early stage. Other investigations such as computed tomography (CT) may be helpful occasionally. Visual studies are important for evaluating any possible or existing visual field defects caused by large tumors that compress the optic pathways; bitemporal hemianopsia (loss of peripheral visual field) is the classic finding when a pituitary tumor compresses the optic chiasm.\nThe three treatment options for symptomatic pituitary tumors are medical, surgical and radiotherapy (Gamma Knife (GK) or conventional radiotherapy). In some cases, such as in incomplete surgical removal these treatments may be combined. The goal of treatment, and the best measure for its success, is to reduce pressure on nearby structures (generally the optic nerves or the pituitary gland itself) and/or to normalize the levels of hormone production.\nThe goals of therapy are improved quality of life and survival, relief of mass effect and reversal of its associated signs and symptoms, normalization of hormonal hypersecretion, preservation or recovery of normal pituitary function, and prevention of recurrence.\nPituitary tumors are treatable and in many cases curable. The optimal treatment modality is determined by the type and size of the tumor, and how far it has grown into the surrounding brain at the time of diagnosis. Early detection of pituitary tumors is often “key” to successful treatment.\nFuture Prospects such as gene therapy, tissue-specific promoters and stem cell therapy may play a role in the future treatment of pituitary adenomas.\nIf you or someone you love has questions about Pituitary Tumors, please Contact Us and we will be happy to help you.', ""Due to the part of the nerve affected by the condition, NMO is classified as a demyelinating disease. The protective covering of a nerve is formed from a substance called myelin. Demyelinating diseases target this covering.\nThe condition is extremely rare, affecting between 0.052 and 0.44 in every 100,000 people worldwide.\nThe disease is more common in people over 40 years of age.\nThis MNT Knowledge Center article will explain the types, causes, symptoms, and treatments for NMO, as well as explaining how the condition is diagnosed and ways it can complicate.\n- The two types of Devic's disease, or neuromyelitis optica (NMO) are relapsing NMO and monophasic NMO, and the type depends on the frequency of attacks.\n- Symptoms can include a temporary loss of eyesight, eye pain, and altered sensations in the limbs.\n- The cause is unknown, and there is no cure.\n- Treatment aims to prevent relapses and control symptoms.\nNMO damages the protective covering of the optic nerve and nerves in the spinal cord.\nThere are two types of NMO: Relapsing, which is the most common, and monophasic, which is rare.\nWith this type, there is an initial attack of inflammation in the optic nerve and spinal cord followed by attacks over a period of several years. Further research is required to confirm what triggers these attacks.\nThe individual may not be able to fully recover from the nerve damage caused by these attacks cause, and the damage is often permanent. This can cause disability and affects females more frequently than males.\nA limited number of attacks is experienced over a period of days or weeks. There are no subsequent attacks. This form of NMO affects both sexes equally.\nIt is easier for an individual to recover from monophasic NMO.\nThe type of NMO will affect how severe symptoms are as well as the likelihood of complications and disability. Relapsing NMO is by far the most common, with 90 percent of people experiencing more than one attack.\nSigns and symptoms of NMO vary. A person with NMO will experience at least one bout of inflammation of the optic nerve and spinal cord.\nSymptoms of optic nerve inflammation, termed optic neuritis (ON), include:\n- temporary loss of eyesight affecting at least one eye, with a risk of permanent vision loss\n- swelling of the optic disc\n- pain in the eye that generally gets worse with movement, and tends to become more severe after a week then resolve in a few days\n- reduced sensitivity to color\nPeople who develop optic neuritis should not drive a vehicle due to reduced vision. They may also develop symptoms in the spinal cord from a process called transverse myelitis (TM) including:\n- altered sensations, with sensitivity to temperature, numbness, tingling, and a sensation of coldness or burning\n- weak, heavy limbs, sometimes leading to total paralysis\n- changes in urination patterns, including urinary incontinence, difficulty urinating, and more frequent urination\n- fecal incontinence or constipation\nA person with NMO may have just one mild attack of ON and one episode of TM, recover completely, or almost completely, and have no more relapses ever. Others may have several attacks throughout their life and experience lifelong disability.\nNMO spectrum disorder is a term used for individuals who experience inflammation of either the optic nerve or spinal cord but not both.\nIn most cases, just the optic nerve and spinal cord are affected. In very rare cases, parts of the brain may also be affected. When the brain stem is affected, an individual may experience uncontrollable hiccups or vomiting.\nThe exact causes of NMO have not yet been confirmed. NMO is an autoimmune disorder, meaning that the immune system mistakenly attacks healthy cells in the optic nerve and spinal cord.\nThe condition does not run in families. In some rare cases, tuberculosis (TB) and some environmental organisms have been linked to the development of NMO. However, scientists have not been able to isolate specific organisms so far.\nMany people with NMO have an antibody called Neuromyelitis Optica Immunoglobin G (NMO IgG) in the blood. Research suggests that NMO IgG may damage aquaporin-4, the water channel that surrounds optic nerve and spinal cord cells, causing the inflammatory effect of the condition.\nHowever, more research is required to confirm the exact triggers of both the condition and its attacks.\nNMO can be difficult to diagnose, as it presents signs and symptoms which are often similar to those found in other diseases, such as:\n- multiple sclerosis (MS)\n- acute demyelinating encephalomyelitis (ADEM)\n- systemic lupus erythematosus (SLE)\n- mixed connective tissue disorder (MCTD)\n- some viral inflammations\n- an inflammation linked to cancer called paraneoplastic optic neuropathy\nThe following tests can help rule out other conditions.\n- Blood tests: These test for the antibody NMO IgG.\n- Lumbar puncture test, or spinal tap: These collect a small amount of fluid from the brain and spinal cord. Results will indicate high levels of white blood cells and the presence of specific proteins linked to NMO or other conditions.\n- MRI scan: This scan can show damage and lesions on or around the nerves. MRI scans of people with NMO show changes associated with optic neuritis, and will show up lesions consisting of three or more segments of the spinal cord. This type of spinal cord lesion makes it easier for the doctor to rule out MS.\nPhysical therapy can help to increase mobility and ease discomfort.\nThere is no cure for NMO. However, there are treatments that can help some of the symptoms as well as the frequency and intensity of relapses.\nSteroids: A doctor may prescribe steroids, such as injected methylprednisone. The patient would take a course of oral steroids following the injection.\nPlasma exchange therapy, or plasmapheresis: This treatment is usually given to people with NMO who have not responded to steroid treatment. Plasma exchange removes from the blood the antibodies that cause inflammation. The blood is removed, and the blood cells are separated from plasma. The blood cells, diluted with fresh plasma or a substitute, are returned to the bloodstream.\nPreventing relapses: If the patient's immune system can be suppressed, the chances of relapses are significantly reduced. Azathioprine (AZT), a medication that suppresses immune activity, is sometimes prescribed. The doctor may prescribe a combination of AZT and steroids.\nAZT can cause the following side effects:\nThe anticonvulsant drug carbamazepine may be prescribed for pain, urinary problems, muscle spasms, and stiffness. Carbamazepine is often used with people who have other demyelinating diseases.\nPhysical therapy, rehabilitation, mobility, and visual aids may also be provided to help return function and ease discomfort if the damage is permanent and the person with NMO experiences ongoing disabilities.\nNMO can lead to a range of effects in various systems, including:\n- Breathing problems: This occurs as a result of muscle weakness. For people with severe NMO, muscles may become so weak that they need artificial ventilation.\n- Depression: The mental strain of living with NMO, especially if symptoms are severe, can lead to clinical depression.\n- Erectile and sexual dysfunction: Some men may experience problems achieving or maintaining an erection. Both men and women may experience difficulties achieving an orgasm.\n- Fragile bones: Long-term steroid therapy can lead to osteoporosis. Tiny pores occur in the bones, making them weak and prone to fracture.\n- Paralysis: People with NMO that severely damages the spinal cord can cause limb paralysis.\n- Vision loss: Permanent loss of vision can occur, due to severe damage to the optic nerve.\nThe likelihood of complications and the outlook of a person with the condition depend on the severity of the initial attack and the number of relapses. The age a person is when they first experience the condition also contributes.\nRelapsing NMO normally leads to permanent vision loss, paralysis, or muscle weakness within 5 years. Respiratory failure as a result of the condition can be fatal in 25 to 50 percent of people with NMO.""]"	['<urn:uuid:3b61d248-d756-4aeb-ae67-646e573f12dd>', '<urn:uuid:c62a8f23-98c5-44b0-8627-c879e47813ed>']	factoid	direct	short-search-query	similar-to-document	comparison	novice	2025-04-22T14:43:01.858830	6	44	2639
45	experienced fisherman looking for when to use spinnerbaits vs crankbaits in public ponds current areas	In public ponds with current areas (like stream entries or spillways), spinnerbaits are more versatile and can be used in a wide range of conditions with low casts to reduce splash. Crankbaits, on the other hand, are better suited for deeper waters and work well with composite (cranking) rods. Since fish congregate in current areas during hot summer months due to higher oxygen levels, spinnerbaits would be more effective in these locations.	"[""Whether Alexandria is your atmosphere, Baltimore is your backyard, or Chesterfield is your corner of the world, if you enjoy chasing fish like bass, crappie, and bluegill, somewhere close by there’s a public pond that's worth a peek. You probably drive past it on a regular basis with little more than a glance, because that little watering hole couldn’t possibly represent a good fishing opportunity… or could it? As anyone who’s taken the time to cast a line into public ponds can tell you, these minor league bodies of water sometimes hold major league fish. But you can’t approach them as you would a lake or a river. Adjust your fishing techniques to match the body of water, however, and you can discover some fast-action fishing. Fail to do so, and those little ponds can drive you crazy in a big way.\nA TIME LIMIT applies, when you’re hitting a small pond. These bodies of water get fished regularly, and since they’re so contained something as simple as walking the perimeter can be enough to spook every educated fish in attendance. Cast for an hour or two and you’ll likely have spooked most of the fish that you haven’t already caught. And when you do make a big catch you may want to head home, because a single fish thrashing on the surface is often enough to shut off the bites for 30 minutes or more. Don’t think of small public ponds as places you’ll spend an entire day, but instead, consider them for short angling diversions lasting just an hour or so on your way home from work, after school, or during a break in the weather.\nWEEDBEDS can be key features and in some cases the weeds will occupy a giant area of the water. Fishing the edges of weeds is always a good bet, but in ponds that get really choked be sure to bring a weedless topwater offering like a frog or mouse. They’ll drive fish like bass and snakeheads nuts, and allow you to retrieve right through the thick stuff.\nREED-COVERED shorelines can be magic, if you come prepared. Not because they hold so many fish, but because they deny access to 99 percent of the people who attempt to fish public ponds. As a general rule of thumb at any public access area the tough-to-reach spots are the best spots. So, bring a pair of waders with you and quietly stalk your way to inaccessible areas — you’ll likely discover where the biggest fish in the pond hang out.\nCURRENT is a rare feature in ponds and moving water holds more oxygen than stagnant water, so fish will congregate where it does exist (especially during the hot summer months when oxygen levels drop). Many ponds, however, will have a bit of current either where a stream enters or a spillway drains. Either can hold the lion’s share of the fish.\nDAMS generally mark the deepest spots in ponds, and particularly during the heat of summer the fish will often move as deep as possible to cool off. Always try probing the depths along a dam and if you have catfish in your sights, sink your baits here.\nSPECIES diversity is not normally great, and in many ponds you’ll find just sunfish, catfish, and bass. Use this to your advantage by catching some small sunnies (or using a lure that closely mimics them) to target the bigger predators.\nTERRESTRIAL baits and lures tent to shine in these waterways, because they’re often short on minnow and fry. As a result the fish are used to hunting insects, worms, and other non-aquatic critters. When fishing for bluegill, tiny poppers that mimic grasshoppers, plastic or hair ants, and dry flies will often out-catch lures like spinners or jigs. And when bass are the target most of the time you’ll be better off casting plastic worms than crankbaits. Similarly, if you’re fishing bait this is one of those times where ground worms may well produce more bites than minnow.\nFEED THE DUCKS at urban ponds which families often visit. Like most creatures on planet Earth, fish are lazy at heart and will be happy to accept a free meal — including bread. At ponds where people regularly toss morsels out for the ducks, fish sometimes become acclimated to following the ducks and grabbing some free handouts for themselves. So bring along a heel or two, feed the ducks, and then start casting. Note: this is particularly effective for carp, which enjoy eating bread more than most other species; when you apply this tactic bait up with dough-balls. Bonus Tip: if there's a mulberry tree overhanging the water, expect to find carp there, too - they love mulberries.\nWhile those local ponds may not represent a destination for day-long angling, as a temporary diversion they can excel. Don’t sell that little mud-hole short, because when you need a quick break from the “real” world, nothing does the trick like bending a rod from the shoreline of a pond.\nFishing Hotspot, or Not?\nNaturally, you can’t count on each and every drainage ditch to hold 10-pound bass. Some small public ponds are winners, and others aren’t. Geographic location and fishing pressure are often characteristics we might use to rate a body of water, but they really don’t provide many clues when it comes to ponds, especially in urban areas. Instead, pay attention to:\n- The shape of the fish you catch. When all the fish have huge heads and small bodies, you know the pond is overpopulated and underfed so you’re not likely to catch many decent fish in it.\n- Whether or not the pond is on the trout stocking schedules. If it is, once the stocked trout are all caught out it may not offer the best action in the world. Stocking locations get hit a lot harder than other areas, and people often return to the same place out of habit. If a pond doesn’t appear on the schedule, however, chances are far fewer people fish it at any given time.\n- Whether or not there’s any inaccessible shoreline. Areas where people can walk 360-degrees around the pond and hit all or virtually all locations don’t provide many places for the fish to hide.\n- Depth is a critical feature, since it gives fish sanctuary in the heat of summer and the cold of winter. You can’t really judge depth until you try a pond out and take a few casts, but if you find there are large areas of deep water, there’s a better shot at finding good numbers and/or sizes of fish in that body of water.\n- How well the area around the pond is manicured. If the grass is mowed down to the shoreline every week, weed killer gets sprayed around the parking lot, and flower beds line the paths, it may look nice… but there’s probably less food and lower water quality in that pond. Regular maintenance doesn’t necessarily guarantee a pond will be a bust but if you look around and see these conditions when you’re not getting any bites, it’s time to move on."", 'If you\'re searching for tips on how to catch a bass, there\'s really no easy place to begin. There is a lot to consider, especially with the range and variety of choices to make and factors to determine. But whether it\'s smallmouth, largemouth or striped bass you\'re going after, many of the same supplies and techniques apply.\nBass are an aggressive fish, and can quickly become an angler\'s favorite species to go after. They can be found in plenty of places, can be caught with minimal equipment, and are one of the most prevalent species in America.\nBass are found in many American lakes and rivers, and can be found elsewhere as well, including far off places like Japan where the species is popular with anglers. From Washington to South Carolina, and Minnesota to Arizona, bass are found in just about any corner of the US.\nThe biggest bass, and therefore the most popular places to fish for them, are Texas, Florida and California. Mexico happens to have a huge population of bass in their lakes and rivers, making it a great activity for American vacationers.\nRod and reel\nCasting rods are the most common tools in the bass fisherman\'s arsenal, and are able to handle the heavy lines and big lures used in most cases. Ease and comfort take precedence with casting rods, and many of the techniques used to catch bass require one. Power fishing, which bass fishing often becomes, is best done with a casting rod.\nSpecific circumstances may call for a spinning rod, where lighter line and smaller baits are handled easier and with more finesse. Open water is ideal, as spinning rods can cause trouble when mixed with heavy cover and shallows.\nTo learn how to fish bass at night effectively, go here.\nMost rods are made of graphite, creating a lightweight and sensitive feel great for long hours on the water. Graphite can be combined with fiberglass to form a composite rod, adding durability to the previously mentioned attributes. Composites are also known as cranking rods, and perform well when used with crankbaits.\nRod action refers to flexibility, and a fast action rod is best for bass. The faster the action, the more pressure gets put on the bass.\nReels operate under similar categories, and combining a casting rod with a casting reel (or spinning rod with spinning reel) will create ideal conditions. Many brands and styles are available, with new innovations appearing regularly.\nUltimately, bass fishing line should be matched with the bait or lure you are using, so as to get the most out of the combination. The three common types are monofilament, fluorocarbon and braided line.\nMonofilament lines are buoyant, and are ideal for topwater or spinnerbaits. They are best when used with spinnerbaits and buzzbaits.\nFluorocarbon line is more sensitive, has lower stretch and much more invisibility. However, fluorocarbon line sinks, which can have an effect on the action and drama you\'re trying to exhibit with your technique. It\'s too difficult to twitch or jerk a line that\'s completely underwater. Use fluorocarbon line when casting worms, crankbaits or when using the drop shotting technique see below).\nBraided fishing line has a smaller diameter, increasing the amount of line that can be put in a reel and extending the range of casts. Frogs, heavy swimming lures and flipping jigs work well with braided line.\nRead more about fishing lines here.\nLures and bait\nThe variety of lures and bait available for bass fishing can be intimidating. Many different styles will work when combined with the right rod, reel and line. These are some of the most popular choices:\nSoft plastics, or worms, are likely the oldest baits used in bass fishing. They are usually taken in by bass quickly and not spit out like hard-bodied lures often are. Worms can be retrieved slowly or bounced around the bottom with the drop shotting technique (see below).\nSpoon lures are typically made of metal and are heavier than others. The concave shape resembles a shoehorn, and creates action while retrieved through the water that drives bass crazy. They are best used in deep water, and can navigate through thick underwater vegetation easier than just about any other bait.\nSpinnerbaits are versatile and despite their odd shape and look, can be extremely effective. One end includes a jig head and hook, while the other features one or two shiny spinning blades that attract bass nicely.\nThirty years ago, jerkbaits were touted as the secret baits of the pros. Clear and cold water is ideal for using jerkbaits, which come in a variety of sizes and colors. The idea is to match the appearance and action of a baitfish, especially with jerks and tugs on the line while retrieving.\nSee other top bass fishing lure choices here.\nCrankbaits, plugs, minnows and wobblers are all terms used to describe virtually the same thing. Again, they resemble a small fish (one a bass would enjoy eating), and most include a lip on the front that creates a diving action when jerked. Sizes, colors and lip styles are in abundance, but choosing the right one depends on water conditions. Clear water makes natural colors, like those actually found on fish in that region, work best. Use brightly colored, flashy jerkbaits in murky waters.\nSpinnerbaits feature metal blades that spin like a propeller when retrieved. Pros like Jimmy Houston swear by them, and for good reason; they\'re great because they can be used in a wide range of conditions. Ideally, low casts will help reduce splash on the lure\'s presentation, and avoid scaring fish away.\nPoppers and other topwater baits stay on the surface and are operated with a tug (or pop) of the line, followed by a rest, and repeated as it\'s retrieved. Topwater action can entice bass from as deep as 30 feet to rise to the surface and hit a lure, one of the funnest parts of any style of fishing.\nBass are ambush feeders, meaning they will snatch at a lure with lightning quickness as it passses through the fish\'s area. Even if they aren\'t feeding, bass have been known to gulp lures out of sheer aggressiveness. This acts in the angler\'s favor, naturally.\nWhile experts and amateurs alike will constantly debate over the best technique for catching bass, here are a few of our favorites.\nJerkbait: The jerkbait fishing technique is probably the easiest to learn, and one of the first additions to a beginning angler\'s trick bag. A stiff rod and a supple line will help make it effective.\nA simple overhand or sidearm cast will send your jerkbait a good distance from your boat or the shore, giving enough room and opportunity for reeling back in while occasionally jerking the rod to attract bass and entice their attention.\nExecute strong, downward thrusts of the rod while reeling in, which should mimic a less-than-healthy meal that bass will jump all over.\nThe jerkbait approach is best suited for clear water, since sight is the biggest part of the technique.\nFind out how to follow autumn bass patterns here.\nPitching and Flipping: Though they are similar, pitching and flipping are two different ways to get a lure into shallow water with thick cover, a favorite hiding spot for big bass. A long rod (between 6.5 and 7.5 feet) and the right soft bait are essential in both techniques.\nPitching is the easier of the two; using a soft bait (ideally worm or tube jigs), let out as much line as the length of your rod. Keeping the reel open, grab the lure with your off hand, and lower the rod tip towards the water. Pull on the lure to bend the rod and add tension, aiming the handle towards your targeted area. In one motion, let go of the lure, release the reel and flick the rod up slightly. The slingshot effect, with practice, can be one of the best ways to get a bass lure where you want it to go.\nFlipping involves letting out 10-15 feet of line and closing the reel. The line should be grasped between the reel and the first line guide, with a sideways arm extension. Raise the rod to swing the bait towards you, and the momentum should push it back out while feeding the line through your hand. The slack created by the movements will need to be tightened, and the technique will need to be practiced thoroughly to get it down.\nFor the only fishing knots you\'ll ever need to know, go here.\nDrop Shotting: Invented by a saltwater fisherman, then popularized on the West Coast among professionals, the approach is great for highly-pressured lakes. Technically it\'s classified as a ""finesse"" technique. The drop shot technique uses a suspended lure above a weight, tied with the drop shot knot. The distance between the weight and the lure can vary, depending on the muddiness and vegetation of the body of water\'s bottom.\nThe line is either cast out or dropped straight down, and the weight is meant to settle on the bottom. A light wiggle of the line should attract bass without much work, with the occasional stop to let it rest. The line should be kept taught so any bites can be detected easily. Once a bite is felt, a strong tug upwards on the line should hook the fish, and the reeling in can begin.\nTopwater: Probably the most exciting technique for catching bass, topwater fishing brings the action to the surface, allowing you to actually see, hear and anticipate the strike. There\'s nothing like watching a bass follow your bait and seeing the splash as it attacks its potential meal.\nPoppers, jitterbugs and frogs are popular topwater lures, and most involve a long cast and a consistent retrieval of the line. Extravagant movements, especially with poppers and frogs, will do an especially good job of attracting lunkers.\nEnjoy the outdoors?\nDon\'t miss a story! Sign up for daily stories delivered to your inbox.']"	['<urn:uuid:cba3c6dd-b64e-48e4-9501-5140ad488511>', '<urn:uuid:97b8a44d-982f-4b80-a5e1-e869649c30fc>']	factoid	with-premise	long-search-query	similar-to-document	comparison	expert	2025-04-22T14:43:01.858830	15	72	2863
46	what happens zinc common cold symptoms	When zinc is administered within 24 hours of symptom onset, it reduces the duration of common cold symptoms in healthy people.	['Post by Emily White\nWith winter well and truly here it seems people are getting struck down with the flu left right and centre. The best way to avoid getting sick this winter is by supporting your immune system and there are many ways this can be done through a healthy diet/lifestyle.\nNot only is sugar bad for your waistline, but it has also been shown to negatively affect immunity. One study in the American Journal of Clinical Nutrition found that oral 100-g portions of carbohydrate from glucose, fructose, sucrose, honey, or orange juice all significantly decreased the capacity of neutrophils to engulf bacteria meaning the ability to fight infection was reduced. Starch ingestion did not have this effect. The decrease in phagocytic index was rapid following the ingestion of simple carbohydrates (1). Therefore it is important to be mindful of your sugar intake in order to maximise your immunity.\nThe nutritional importance of Zinc has been known for a long time however recently it has been discovered that Zinc could play a central role in the immune system, with zinc-deficient persons experiencing increased susceptibility to a variety of pathogens (2). One study in particular found that zinc administered within 24 hours of onset of symptoms reduces the duration of common cold symptoms in healthy people (3). So dosing up on Zinc when you feel a cold coming on could be a good way to reduce your downtime!\nThe old famous immune boosting vitamin! Vitamin C functions as an antioxidant to neutralise free radicals as well as supporting a healthy immune system. Studies have suggested that a Vitamin C deficiency can result in reduced resistance against certain pathogens whilst a higher supply enhances several immune system parameters (4). It should be noted however that these results have been found when Vitamin C intake was consistently adequate- not just dosing up when you feel a cold coming on (5). So in the case of Vitamin C- consistency is key!\nGet enough micronutrients\nMany people nowadays are just not getting enough micronutrients. This is usually from a diet high in processed food and not enough vegetables. There is some evidence that suggests that deficiencies in certain micronutrients (such as zinc, selenium and iron) can alter immune response in animals. Although the effect on human immune response is yet to be assessed the research at this stage is promising and there is no harm in ensuring that you get enough micronutrients!! This is where ensuring you are getting at least 6 servings of vegetables per day and even supplementing with a good quality greens powder can be awesome.\nVitamin D is an important factor in immune status. Some studies have shown that there is a link between vitamin D status and the risk of developing influenza. People who have low vitamin D levels may have an increased risk of developing influenza (6). The best way to get Vitamin D is from sun exposure so even sitting out in the sun for 15 minutes during your lunch break can work wonders for your health and everyday performance. However in winter this can be difficult so if you think that you may not be getting enough, it is definitely important to consider a supplement.\n1. Sanchez A, Reeser JL, Lau HS, Yahiku PY, Willard RE, McMillan PJ, et al. Role of sugars in human neutrophilic phagocytosis. The American Journal of Clinical Nutrition. 1973;26(11):1180-4.\n2. Shankar AH, Prasad AS. Zinc and immune function: the biological basis of altered resistance to infection. The American Journal of Clinical Nutrition. 1998;68(2):447S-63S.\n3. Singh M, Das RR. Zinc for the common cold. Cochrane Database Syst Rev. 2013(6):CD001364.\n4. Strohle A, Hahn A. [Vitamin C and immune function]. Med Monatsschr Pharm. 2009;32(2):49-54; quiz 5-6.\n5. Wintergerst ES, Maggini S, Hornig DH. Immune-enhancing role of vitamin C and zinc and effect on clinical conditions. Ann Nutr Metab. 2006;50(2):85-94.\n6. Laaksi I, Ruohola JP, Tuohimaa P, Auvinen A, Haataja R, Pihlajamaki H, et al. An association of serum vitamin D concentrations < 40 nmol/L with acute respiratory tract infection in young Finnish men. Am J Clin Nutr. 2007;86(3):714-7.\nResearch and popular science articles by the members and faculty of the Holistic Performance Institute.']	['<urn:uuid:608a7291-2673-48ef-96a9-6e4ffba0b379>']	factoid	direct	short-search-query	similar-to-document	single-doc	novice	2025-04-22T14:43:01.858830	6	21	700
47	My friend is planning to visit Ghana in November and mentioned something about a big festival - what's the main celebration that happens and where does it take place?	The Hogbetsotso festival is one of Ghana's biggest festivals, celebrated on the first Saturday of November in Anloga, the capital of the Anlo state. The highlight of the festival is the grand Durbar of chiefs and people of Anlo, which is a colorful celebration where chiefs dress in Kente cloth while their subjects pay homage to them with singing, drumming, and dancing. The festival includes various activities leading up to the main durbar, including conflict resolution between families, ceremonial purification of stools, and village cleaning from the Volta River to the Mono River in Togo. It's such a significant event that notable figures like John Dramani Mahama and the Late JJ Rawlings have attended it.	['Ghana has several festivals celebrated by the various tribes for various reasons and at different times of the year. And one of such festivals is the popular Hogbetsotso festival which is a big festival in Ghana.\nIn this article, I’m going to talk about this historical festival and detailing which people celebrate this festival, when it is celebrated, why it is celebrated and how it is celebrated.\nWhich People Celebrate Hogbetsotso?\nThe festival is celebrated by chiefs and people of Anlo in the Volta region of Ghana. For a better understanding, these are the main towns that make up Anlo; Keta, Dzita, Anloga, Anyako, Kedzi, Afiadenyigba, Alakple, Tito, Whuti, etc.\nWhen Is It Celebrated\nThe Hogbetsotso Festival is celebrated yearly in November specifically on the first Saturday of the month.\nWhere Are The Celebrations Held?\nHogbetsotso is held at the capital of the Anlo state which is Anloga.\nWhy It Is Celebrated\nThe festival is celebrated to mark their escape from King Togbe Agorkoli, who was believed to be a wicked ruler at the time. They got the name (Hogbetsotso) which means “the festival of exodus” in the Ewe Language.\nAccording to history, the Anlo’s migrated from South Sudan to Notsie in Togo and then to their present day land. So it was in Notsie where they had to escape from a wicked king at the time called Togbe Agorkoli.\nTo make their escape possible, the women had to deliberately pour all their waste water on a particular portion of the mud wall that was around the town. The wall gradually became soft allowing them to create a hole in it and that was how they escaped from Togbe Agorkoli.\nIt is also believed that they had to walk backwards in order for it to make it look like they were rather going into the town and not leaving instead. And this is how the “Agbadza” dance came about.\nHow It Is Celebrated\nThe festival is not a one day event as several activities are held in the run up to the grand Durba it self. It offers them the opportunity to resolve conflicts or misunderstandings between families or people and to promote togetherness.\nCeremonial stools are also deliberately purified by pouring libations because they believe that is where their ancestors dwell. After the purification ceremony is massive general cleaning of the villages from the Volta River and ends at the Mono River in Togo.\nThe icing on the cake is the grand Durbar of the chiefs and the people of Anlo. It is a colourful durbar like non other as the chiefs dress nicely in their Kente whiles their subjects pay homage to them amidst singing, drumming and dancing.\nThe main dance for the day is the “Agbadza” dance. It is a thing of beauty that requires the use of some technique to perfectly execute. However, when you watch how the natives and those who have mastered this art dance, you would think it is that simple.\nAgbadza is a popular ewe dance that can be performed at all kinds of events such as funerals, naming ceremonies, weddings etc. Although it is an Ewe dance, it can be performed by persons from other tribes as well.\nHogbetsotso Festival is indeed one of the biggest festivals in Ghana is not surprising that it is often attended by some notable persons in the country such as John Dramani Mahama and the Late JJ Rawlings in 2019.']	['<urn:uuid:2c22c940-d24f-46d8-a321-bf9a947c3586>']	open-ended	with-premise	verbose-and-natural	distant-from-document	single-doc	novice	2025-04-22T14:43:01.858830	29	115	575
48	I'm studying how cells communicate in the body. How do adhesion molecules help cells stick together, and what happens when these molecules get disrupted after severe injuries?	Adhesion molecules like E-cadherin help cells stick together through cell-cell adhesion mechanisms. When severe trauma occurs, these adhesion processes get disrupted as damage-associated molecular patterns (DAMPs) are massively released from injured cells. These DAMPs then trigger widespread inflammation by binding to neutrophils, leading to increased endothelial permeability and vascular problems. This disruption can ultimately contribute to organ failure and make patients more susceptible to infections.	"[""By Victor R. Preedy\nThis ebook covers the constitution and category of adhesion molecules when it comes to signaling pathways and gene expression. It discusses immunohistochemical localization, neutrophil migration, and junctional, practical, and inflammatory adhesion molecules in pathologies reminiscent of leukocyte decompression affliction and ischemia reperfusion harm. Highlighting the clinical purposes of present learn, chapters conceal diabetes, weight problems, and metabolic syndrome; hypoxia; kidney disorder; smoking, atrial traumatic inflammation, and middle ailment, the mind and dementia; and tumor proliferation. eventually, it appears to be like at molecular imaging and bioinformatics, high-throughput applied sciences, and chemotherapy.\nRead or Download Adhesion Molecules (Modern Insights Into Disease from Molecules to Man) PDF\nSimilar endocrinology & metabolism books\nIt is a 3-in-1 reference e-book. It offers a whole clinical dictionary protecting countless numbers of phrases and expressions with regards to Cushing's affliction. It additionally supplies large lists of bibliographic citations. eventually, it presents info to clients on easy methods to replace their wisdom utilizing a variety of web assets.\nThis crucial reference, edited via Ronald Ross Watson and Betsy Dokken, collects the learn had to make the special connection among pre-diabetes, diabetes, and heart problems. Glucose consumption and usage in Pre-Diabetes and Diabetes: Implications for heart problems explains the mechanisms of development from pre-diabetes to diabetes to heart problems.\nAn intuitive, creative and strong process, sentinel lymph node biopsy has entered medical perform with surprising rapidity and now represents a brand new usual of take care of cancer and breast melanoma sufferers, whereas displaying nice promise for the therapy of urologic, colorectal, gynecologic, and head and neck cancers.\nIodine is a necessary micronutrient and an necessary section of the thyroid hormones, that are required for regular development and improvement. The iodine deficiency issues (IDD) surround a spectrum of inauspicious future health results together with goiter, cretinism, hypothyroidism, development retardation, and elevated being pregnant loss and boy or girl mortality.\n- The Evidence Base for Diabetes Care\n- The Official Patient's Sourcebook on Multiple Endocrine Neoplasia Type 1: A Revised and Updated Directory for the Internet Age\n- The Clinician’s Guide to the Treatment of Obesity\n- Molecular Basis of Thyroid Cancer\n- Insulin Resistance: A Clinical Handbook\n- Energy and protein metabolism and nutrition\nAdditional info for Adhesion Molecules (Modern Insights Into Disease from Molecules to Man)\nAnnu. Rev. Immunol. 25: 619-647. Morishita, H. and T. Yagi. 2007. Protocadherin family: diversity, structure, and function. Curr. Opin. Cell. Biol. 19: 584-592. Parisini, E. G. H. B. H. Wang. 2007. The Crystal Structure of Human E-cadherin Domains 1 and 2, and Comparison with other Cadherins in the Context of Adhesion Mechanism. J. Mol. Biol. 373: 401-411. D. P. Chen, F. Bahna, B. Honig, and L. Shapiro. 2003. Cadherin-mediated cell-cell adhesion: sticking together as a family. Curr. Opin. Struct.\n2007. Phospho-regulation of beta-catenin adhesion and signaling functions. Physiology 22: 303-309. Furuse, M. and K. Furuse, H. Sasaki, and S. Tsukita. 2001. Conversion of zonulae occludentes from tight to leaky strand type by introducing claudin-2 into Madin-Darby canine kidney I cells. J. Cell Biol. 153: 263-272. Gonzalez-Mariscal, L. C. Namorado, D. Martin, J. Luna, L. Alarcon, S. Valencia, P. Muriel, L. L. Reyes. 2000. Tight junction proteins ZO-1, ZO-2, and occludin along isolated renal tubules.\n2008. Clustering endothelial E-selectin in clathrin-coated pits and lipid rafts enhances leukocyte adhesion under flow. Blood 111: 1989-1998. Shimaoka, M. and J. A. Springer. 2002. Conformational regulation of integrin structure and function. Annu. Rev. Biophys. Biomol. Struct. 31: 485-516. Shimaoka, M. and T. H. Liu, Y. Yang, Y. D. Jun, A. McCormack, R. Zhang, A. Joachimiak, J. H. A. Springer. 2003. Structures of the aL I domain and its complex with ICAM-1 reveal a shape-shifting pathway for integrin regulation."", 'SDRP Journal of Anesthesia & Surgery(SDRP-JAS)\nTrauma-Induced, DAMP-Mediated Remote Organ Injury and Immunosuppression in the Acutely Ill PatientSubmit Manuscript no this topic Topic Articles: 0\nTrauma is the third lead cause of mortality worldwide and is the first cause of fatality and invalidity in the 16-45 age group. While early mortality is mainly due to overwhelming hemorrhage and catastrophic central nervous system injuries, later deaths are triggered by multi-organ failure and healthcare-acquired infections. Even if early deaths were reduced with road safety and pre-hospital care improvements, multi-organ failure and healthcare-acquired infections remain a serious burden for severe trauma patients. Indeed, 45% of patients admitted to a Level 1 Trauma Center develop multi-organ failure and infection remains the leading cause of death after trauma.\nIt is now thought that the innate immune system plays a key role in both trauma-induced remote organ failure and in trauma-induced immunosuppression. Indeed, according to the danger theory, damage-associated molecular patterns (DAMPs) are massively released following severe musculoskeletal injury, which then bind to various receptors on the surface of neutrophils and elicit widespread systemic inflammation. For instance, mitochondrial DNA and formyl peptide are released after cellular breakdown, bind Toll-like receptor 9 and formyl peptide receptors on the surface of neutrophils and trigger multiple inflammatory processes, such as transcription of pro-inflammatory genes, leukocyte chemotaxis and cytokine production. DAMPs are numerous and emerge from multiple intracellular compartments: cytoplasm (S100 proteins, eosinophil-derived neurotoxin, uric acid), nucleus (free nuclear DNA, high mobility group box 1, histones), and mitochondria (mitochondrial transcription factor A, mitochondrial DNA and formyl peptides). Mitochondrial DAMPs are particularly relevant since they show evolutionarily conserved similarities to bacterial pathogen-associated molecular patterns (PAMPs).\nMany indices converge on a close association between injury severity and the amount of DAMPs released. Upon DAMPs binding, activated neutrophils propagate inflammation and mediate cell injury essentially in the lungs by releasing neutrophil extracellular traps, which contain elastases, histones and proteases. In the systemic circulation, mitochondrial DAMPs also bind formyl peptide receptors on smooth muscle cells and subsequently induce vascular hyporesponsiveness. Endothelial permeability is also increased through both neutrophil-dependent and neutrophil-independent pathways. In the lungs, increased permeability, lung congestion and direct epithelial damage promote lung injury and acute respiratory distress syndrome. At the same time, DAMPs profoundly reduce innate and acquired immune responses. For instance, leukocyte HLA-DR gene expression and ex-vivo stimulated cytokine production negatively correlate with plasma levels of nuclear DAMPs released in human plasma after severe trauma.\nAccurate characterization of DAMPs release and their consequences after trauma has broad clinical applications since it could entail an individualized approach for both preventive and curative therapeutic strategies. For instance, the determination of the DAMP load and the inflammatory status of the patient could tailor the timing of definitive surgical treatment. Moreover, synthetic formyl peptide antagonists and TLR9 inhibitors are already available for clinical use and could dampen both the pro-inflammatory and the anti-inflammatory responses after trauma.\nIn this Research Topic, we aim to shed light on DAMPs and the role they play in the interactive crosstalk between musculoskeletal trauma, remote organ injury and immunosuppression. Original Research, Review, Protocols, Hypothesis & Theory and Methods articles are all welcome. Manuscripts can include basic research and corresponding animal and human research. We will highlight key mechanisms of inflammation research to influence further discussion on the development of therapeutic attempts to limit both excess pro- and anti-inflammatory actions.']"	['<urn:uuid:750f07f7-c4d7-4d9c-b614-9adc61396dac>', '<urn:uuid:f1cbf428-7942-4a9a-a8b3-190eb54b6f7e>']	factoid	with-premise	verbose-and-natural	similar-to-document	multi-aspect	novice	2025-04-22T14:43:01.858830	27	65	1175
49	typical office culture components and required time phases implementing new work methods	The key components of organizational culture are values (what's important to the organization), beliefs (ways to achieve goals), and norms (accepted behaviors and practices). The implementation of new work methods involves four distinct stages: exploration (assessing and creating readiness), installation (preparing resources and infrastructure), initial implementation (first attempts at new practices), and full implementation. The process typically takes 2-4 years to reach full implementation, when 50% of practitioners meet fidelity criteria.	"[""| Organizational Culture\nOrganizational culture can be viewed as an important concept in organizational psychology and social psychology. It is important to define organizational culture.\nOrganizational Culture Definition:\nWhat is organizational culture? There are many possible definitions of organizational culture. Below is one organizational culture definition:\nOrganizational culture reflects the values, beliefs, and norms that characterize an organization as a whole.\nThis definition suggests that organizational culture reflects what is common, typical, and general for the organization. Values, beliefs, and behaviors that are uncommon in the organization, or specific to a particular subgroup within an organization, would not be considered to be part of the culture of the organization.\nElements of Organizational Culture:\nThere are many possible elements of organizational culture. The above definition includes three of the elements of organizational culture.\nOrganizational Values. Values reflect what we feel is important. Organizations may have core values that reflect what is important in the organization. These values may be guiding principles of behavior for all members in the organization. The core values may be stated on the organization's website. For example, an organization could state that their core values are creativity, humor, integrity, dedication, mutual respect, kindness, and contribution to society.\nOrganizational Beliefs. Beliefs that are part of an organization's culture may include beliefs about the best ways to achieve certain goals such as increasing productivity and job motivation. For example, an organization may convey the belief that the expression of humor in the workplace is an effective way to increase productivity and job motivation.\nOrganizational Norms. Norms reflect the typical and accepted behaviors in an organization. They may reflect the values and beliefs of the organization. They may reflect how certain tasks are generally expected to be acomplished, the attributes of the work environment, the typical ways that people communicate in the organization, and the typical leadership styles in the organization. For example, the work environment of a company may be described as relaxed, cheerful, and pleasant. Moreover, the organization may have a participative decision making process in which many people in the organization are able to express their views concerning important decisions. Also, an organization may have many meetings to discuss ideas.\nThe Importance of the Organizational Culture Concept\nOrganizational culture may be an important concept for a few reasons. First, understanding the culture of an organization may be helpful for applicants. They may have a better idea about whether they would like to work for a company. Second, understanding the culture of an organization may help in training new employees. Third, understanding organizational culture may help leaders to identify possible sources of problems in the organization.\nOrganizational Culture and Leadership\nThere may be at least three ways in which leadership is important with respect to organizational culture. First, a leader of an organization may play an important role in identifying the elements of the organization's culture. The leader could make a list of the organization's current values, beliefs, and norms. Second, after identifying the current elements of the organization's culture, the leader can make evaluations of the elements of organizational culture that may be negative. The leader could make a list of the specific values, beliefs, and norms that may contribute to major problems in the organization (e.g., a lack of job motivation). Third, after identifying the possible negative elements, the leader could develop strategies to foster a positive organizational culture change. The leader could make a list of the elements of a more ideal culture, develop specific ways to communicate the changes, and develop techniques to motivate people to adopt the new culture.\nOrganizational Culture Change\nThere may be many reasons why the culture of an organization needs to be changed. These reasons may include lack of morale, lack of job motivation, lack of job meaning, and changes in the business (e.g., the development of a new product) that would require a change in the way things are done in the organization.\nFor example, there may be too much micromanagment in a company. It may be better if employees had more autonomy. This may increase morale. Sherman (1989) found that unit morale was positively correlated with automony. Because this finding is correlational, we cannot make causal conclusions.\nThis process of culture change should involve all members of the organization. This process of culture change could involve surveys in which members describe specific elements of the organizational culture that members view as negative.\nCulture vs. Organizational Culture\nAlthough the concept of organizational culture is similar to the concept of culture(e.g., the elements of culture may be similar to the elements of organizational culture), it is important to make a distinction between the two concepts. There may be a few ways in which these concepts may be different. First, organizational culture may be more formal than culture. Some organizations may have a significant part of their culture in written form. For example, they may have the core values stated on the website, and the values, beliefs, and norms of the organization may be indicated in employee manuals. In contrast, much of the values, beliefs, and norms that are a reflection of a culture may be unwritten. Second, there may be less consistency between elements of organizational culture than elements of culture. Some of the elements of organizational culture that are in written form may be inconsistent with certain norms observed in the organization. In contrast, many of the norms of a culture may simply reflect the values of the culture.\nSherman, J. D. (1989). Technical supervision and turnover among\nengineers and technicans. Group & Organization Studies, 14, 411-"", 'Implementation is a process, not an event. The process is marked by implementation stages that have been identified in practice. To use innovations and Active Implementation in practice takes time and effort. Exploration, Installation, Initial Implementation, and Full Implementation Stages guide organization and system investments in innovations supported by implementation best practices.\nThe Stages of Implementation are Exploration, Installation, Initial Implementation, and Full Implementation. The availability of a skilled Implementation Team facilitates the expeditious movement from Exploration to Full Implementation. Without an Implementation Team, organizations and individuals will struggle with what should be predictable issues as well as the many unpredictable issues that always arise as attempts are made to use an innovation in practice. The issues will be there with or without an Implementation Team. The question is, who will resolve stage-based issues effectively and efficiently?\nThe stages of implementation are not linear. They are additive and interactive with movement back and forth with changes in environments, people, and implementation supports. The stages are specific to an innovation. A given organization might be in the Full Implementation Stage with one innovation and in the Exploration Stage with another. With experience, skilled Implementation Teams can anticipate and prevent issues and help move an innovation to Full Implementation more quickly across innovations within an organization and across organizations. And, organizations can learn to learn (develop “absorptive capacity”) and improve their ability to identify, assimilate, and apply innovations more quickly (Cohen & Levinthal, 1990; Jiménez-Barrionuevo, García-Morales, & Molina, 2011). For example, in the Teaching-Family Model the time for whole organizations to reach site certification (organization fidelity) criteria was reduced substantially while success improved. For Teaching-Family group homes, 23% sustained for 5 years or more without the support of an Implementation Team and 84% sustained for 6 years or more with the support of a site-based Implementation Team. And, the time to develop site based Implementation Teams was reduced from 6.4 to 3.7 years as site development processes were operationalized (Fixsen & Blase, 2018; Fixsen, Blase, Timbers, & Wolf, 2007). Learning to learn produces substantial increases in benefits to a recipient population. As the site development Implementation Teams gained experience, they were able to anticipate problems and do better Exploration and Installation work with organizations to help them be successful more quickly and with more certainty.\nThe stages of implementation outlined here are descriptive rather than prescriptive. By describing identifiable stages, Implementation Teams and others can adjust their inputs to match the stage for achieving full, effective, and sustained use of an innovation. Too often, precious time and resources are wasted on trying to insist that organizations and practitioners use an innovation when they have not yet decided it is a good idea (Romney, Israel, & Zlatevski, 2014). Poorly-informed or half-hearted attempts to appear to do what is mandated consume enormous resources and yield few socially significant outcomes.\nExploration Stage activities include shared communication about the strengths and needs of recipients, identification of possible effective innovations that might help fill the gaps in current approaches, assessment of the Implementation Drivers needed to support practitioners and others, discussion of resources required and their sources, and so on. Exploration activities include executive leadership and stakeholders who consider need, risks and risk management, and contextual factors. The result of Exploration is a common understanding and acceptance of the innovation and the required implementation supports, and a collective decision to proceed (i.e. mutually informed agreement).\nThe Exploration Stage is not just to assess readiness; it also involves creating readiness. Creating readiness for change in individuals and organizations is an important part of the work and effectiveness of Implementation Teams. Prochaska, Prochaska, and Levesque (2001) found that at a given point in time about 20% of individuals and organizations in their studies were “ready for change.” Thus, creating readiness is an essential function for uses of effective innovations and a core activity in the Exploration Stage. Clarifying goals, establishing collaborations, locating and developing resources, securing agreements, and so on are good uses of Exploration Stage discussions.\nThe functions of the Exploration Stage are a critical starting place for work with organizations, systems, and others. Taking the time for Exploration saves time and money (Romney, 2011) and improves the chances for success (Saldana, Chamberlain, Wang, & Brown, 2011; Slavin, Madden, Chamberlain, & Cheung, 2010).\nThe function of the Installation Stage is to acquire or repurpose the resources needed to fully and effectively engage in the new ways of work. Resources and activities during Installation are focused on creating new job descriptions, establishing interview methods and preparing interviewers to select practitioners and staff to do the new work, employing people to do the work, developing data collection sources and protocols, establishing access to timely training, preparing a coaching service delivery plan, and so on. While these topics are discussed and debated during the Exploration Stage (promises made), the resources must materialize during the Installation Stage (promises kept).\nBecause of the changes required, the Installation Stage is replete with adaptive leadership challenges. The attention given to the innovation and implementation supports may lead to dissatisfaction among existing staff members who feel devalued and ignored in the rush to do the new things. Some stakeholders who were enthusiastic advocates during Exploration do not follow through on promises they made. Funders get anxious when funds are being spent and no “real work” with recipients has started. And so on.\nImplementation Team members and leaders need to be ready to exercise adaptive leadership to resolve these issues as supports for the new ways of work are being developed. Knowing that adaptive issues abound during Installation and Initial Implementation, Implementation Teams use this as an opportunity to develop the adaptive leadership capacity in an organization and establish the practice-policy communication cycles from the beginning of the process (see Systemic Change). The issues give the Implementation Team and leaders many opportunities to teach, learn, and practice the new ways to provide adaptive and effective leadership in the organization.\nA challenge during the Exploration and Installation stages is finding or developing the expertise to use the Implementation Drivers to select, prepare, and support the first group of practitioners. Very few organizations or systems already have a functioning Implementation Team in place. As a result, accessing outside expertise is a good option to get started (Nord & Tucker, 1987). This is the approach Ogden and colleagues used to initiate the use of effective innovations in Norway. Once a program was underway, they recruited “home grown” staff who had been high fidelity practitioners to be the first members of an implementation team (e.g. trainers, coaches, fidelity assessors) to support the next generations of practitioners (Tommeraas & Ogden, 2016). However, very few evidence-based practices or innovations have purveyors (program-specific Implementation Teams) that support the effective use of the innovation. Thus, most organizations and systems are left to “do it yourself” and that approach has not proved to be very effective (C. H. Brown et al., 2014; Durlak & DuPre, 2008; Vernez et al., 2006).\nInitial Implementation Stage\nThe Initial Implementation Stage begins when the first practitioners attempt to use an innovation with recipients for the first time. Active Implementation staff refer to this as the awkward stage where the new ways of work are not comfortable for practitioners, managers, and leaders. It “doesn’t feel right” to interact with others in the new ways and “it is confusing” when the units within an organization are not yet fully functional and integrated. Very few attempts to use innovations are able to successfully negotiate the difficulties encountered during Initial Implementation where the challenges are many, the supports for change are weak, and the inertia of the status quo is strong.\nDuring the Initial Implementation Stage, newly selected practitioners and staff are attempting to use newly learned skills (e.g. the innovation) in the context of an organization that is just learning how to change to accommodate and support the new ways of work. This is a fragile stage where the awkwardness associated with trying new things and the difficulties associated with changing old ways of work provide strong motivations for giving up and going back to the familiar, comfortable, and well supported status quo. The status quo is powerful and resilient and readily bounces back from efforts to change it (Jalava, 2006; Oser, 2000; Zimmerman et al., 1998; Zucker, 1987).\nImplementation Teams using the Implementation Drivers are essential to success (80% vs. 14%; Fixsen, Blase, Timbers, & Wolf, 2001; Balas & Boren, 2000) during the Initial Implementation Stage. Implementation Teams help to develop the staff competencies required by the evidence-based program, help administrators adjust organization roles and functions to align with the program, and help leaders in the provider organization fully support the process of using the program and incorporating the necessary implementation supports.\nComparison of Intervention Use With and Without an Implementation Team\nFull Implementation Stage\nThe Full Implementation Stage is reached when at least 50% of the practitioners in an organization meet fidelity criteria on a given day. The 50% criterion is a benchmark established by the Active Implementation Research Network as an indicator of Full Implementation. That is, if the goal is to have 20 practitioners use an innovation in an organization, Full Implementation is reached on the day when 10 (50%) of those practitioners are at or above fidelity criteria. It often takes 2 to 4 years to reach Full Implementation when it is achieved at all (C. H. Brown et al., 2014; Brunk et al., 2014; Fixsen et al., 2007; Sabatier, 1986; Swales et al., 2012)\nThe first time the 50% benchmark is reached is cause for celebration. However, it likely will not be sustained for very long. Once a current high fidelity practitioner leaves it will take a few months for the replacement to be selected, trained, coached, and finally meet fidelity criteria. In the meantime, if that one practitioner was the tipping point for reaching the 50% benchmark then the organization will fall short of the 50% benchmark. If key people on the Implementation Team leave, the Competency Drivers may suffer and it may take longer for practitioners to meet and sustain high fidelity performance. Eventually, for the few organizations that reach and sustain Full Implementation, the new ways of providing innovation-based services become the standard ways of work and the implementation supports become the standard way the organization functions (Fixsen & Blase, 2018). The use of the innovation and the Implementation Drivers becomes the new status quo.\nNote that sustaining Full Implementation requires two things: a) it requires using the Implementation Drivers effectively to generate new high fidelity practitioners routinely and b) it requires keeping current high fidelity practitioners employed. If high fidelity practitioners leave faster than new ones can be developed to meet fidelity standards, then the 50% criterion for Full Implementation will never be reached or sustained. The pool is draining faster than it is being filled. The Leadership and Organization Drivers are designed to keep improving supports for practitioners so they can continue to provide high fidelity results for many years, even when doing demanding work (Aarons et al., 2009; Glisson, Schoenwald, et al., 2008; Strouse, Carroll-Hernandez, Sherman, & Sheldon, 2004).\nImplementation Teams remain essential contributors to the ongoing success of using the evidence-based program. Practitioners, staff, administrators, and leaders come and go and each new person needs to develop the competencies to effectively carry out the innovation and its implementation supports. Managers and administrators come and go and need to continually adjust organizational supports to facilitate the work of practitioners. Systems continue to change and impact organizations and practitioners. Evidence-based programs continue to be developed and programs already in place continue to be improved. The number of variables and complexity of issues probably qualify as “wicked problems” as described by Rittel and Webber (1973). The work of Implementation Teams is to ensure that the gains in the use of effective practices are maintained and improved over time and through transitions of leaders and staff.']"	['<urn:uuid:8a08adc3-69bd-400c-a773-e36efb6b08a2>', '<urn:uuid:e753d969-eb11-4507-8366-e0278e50b83b>']	factoid	direct	long-search-query	distant-from-document	multi-aspect	novice	2025-04-22T14:43:01.858830	12	71	2921
50	how many rock art sites found along pecos river and surrounding texas area	There are about 300 rock-art sites in the Lower Pecos region, which is estimated to represent only 25 percent of everything that was ever painted. Many sites have been lost to erosion, and there are likely still undocumented sites.	['Texan cultures painted or carved art on the rocks at thousands\nof sites across southwestern portions of Texas. Much of the\nart is concentrated in the canyons of the lower Pecos River,\nbut much more is found from Big Bend to the mountains near\nEl Paso, at the western tip of the state. Dr. Solveig Turpin,\nwho has retired from The University of Texas at Austin, is\na leader in the study of Texas rock art, and has visited\nand catalogued scores of sites. She also is a founder of\nthe Rock Art Foundation, which is preserving Texas rock art.\nWhat are some of the leading Texas rock-art sites with\npossible astronomical connections?\nOne recently studied site is called Lewis Canyon. It’s\non the Pecos River, about 30 miles above its intersection with\nthe Rio Grande, near the town of Comstock. It started with Forrest\nKirkland, who recorded a number of glyphs at the site [in the\n1930s and ’40s]. But additional work started in 1990, and\nwe found a number of glyphs that Kirkland hadn’t recorded.\nWe’ve gone twice a year since then, and we’re still\nAstronomers have looked at the site, and they see possible “star\nmaps” in some of the petroglyphs. The glyphs are all pecked\ninto the flat bedrock surface, so the viewer was looking at them\nfrom above. These could be native constellations. I think the\nconnection is a little weak. I’m not saying that these\ncouldn’t be drawings of the stars. But there’s a\ndifference between just observing what’s around you, which\nwas probably what happened here, and actually being an astronomer\n-- using the stars in some way to track the seasons or guide\nyou on your travels.\nA site called Myers Springs has pictures of Suns, and maybe\ncomets or falling stars. The Suns are typical Plains Indian Sun\nsymbols. They might be related to the practice of astronomy,\nbut we have no ethnography on the site -- no record of their\nviews of the cosmos. The only records we have are from the early\nSpaniards, who were trying to make the Indians look as bad as\nHunters and gatherers probably didn’t stay in the same\nplace long enough to make a record of solstice or equinox markers.\nExcept for noting the cardinal directions -- north, south, east,\nwest -- it was probably too difficult for them to have accurate\ncalendar markers. They probably had constellations, but they\ndidn’t have math.\nBut there are some pretty convincing images of seasonal markers\nat Paint Rock. These were probably displaced Plains Indians coming\nin with a complete system of beliefs about the sky.\nWhat about the rock art of the Lower Pecos?\nThere are probably 300 rock-art sites in the Lower Pecos, and\nthat probably represents about 25 percent of everything that\nwas ever painted. There’s a lot of erosion, so we’ve\nlost a lot. There probably are still a lot of undocumented sites,\nbut the real treasure trove is probably in the mountains of Mexico.\nThe first art we see in the Lower Pecos are small painted pebbles.\nThe oldest are probably 7,000 years old.\nAs far as we can tell, the great paintings started around 4,000\nyears ago, although there’s no means of getting absolute\ndates of these sites.\nThe paintings are highly structured religious art. To me, they’re\nthe oldest ritual religious artworks in the world. They are elaborate\nfigures done in multiple colors. The biggest is 12 feet tall.\nThe population density in this area rose because upland water\nsources dried up, and the bulk of the people stayed along the\nrivers. With more and more people crammed into the area, you\nneeded a system that offered some structure to the society. And\nyou needed an authority figure. The cardinal rule is that if\nyou get more than six or seven people together, you can’t\nreach consensus, so you need social structure and ranking. So\nthere was probably a rise of some kind of officialdom. There\nwere shamans who supposedly had the ability to contact the spirit\nBill Newcomb [former director of the Texas Memorial Museum and\nauthor of The Indians of Texas] explained it in terms of “medicine” societies.\nGroups got together, and rock art was part of their ritual. They\nportrayed a shamanistic universe that confirmed the power of\nthe leaders. There was a communal, organized world view. People\nmet every year, so they might have needed a rough calendar, which\nmeans they used a rough astronomy. There could have been something\nin the sky that told them, “It’s time for the big\nmeeting at Rattlesnake Canyon.”\nTom Campbell, in 1958, suggested that the paintings were the\nproduct of a mescal bean cult. The beans aren’t hallucinogenic,\nbut they put you in a stupor. And today, Carolyn Boyd thinks\nthey were peyote visions. It’s immaterial how they got\ninto a trance; we just believe that they had some vehicle for\ngetting into a trance state and commune with the spirits. This\nwas a sort of supernatural astronomy -- a layered universe. The\nshamans had to go through a “hole” in the universe,\nwhich is depicted in the paintings as a circle with matter streaming\nout of it and the shaman falling into it. It’s a picture\nof their cosmos, their view of the universe.\nWhen were the last works of rock art in this region\nWe don’t know how long people painted these big, early\nartworks. It could have been a couple of hundred years. About\n3,000 years ago, the climate changed. It probably disrupted the\nlocal society, and the people may have retreated into New Mexico.\nThen a different kind of painting came in. Instead of the grandiose, “I’m\nthe guy who can talk to the spirits,” the pictures are\nall only about two or three inches tall. They’re little\nguys with staffs, pregnant women, little dogs. They’re\nfunny little things. But it looks like they went to great lengths\nto use the old paintings without defacing them. It’s as\nthough they recognized the power of the ancients.\nAround A.D. 1000-1200, the Lower Pecos starts sharing the rock-art\nstyle with the Big Bend. We see pictures of people, about four\nfeet tall, done in red monochrome. They’re carrying bows\nand arrows, and there are naturalistic animals -- turkeys, deer,\nDr. Solveig Turpin is a Texas rock art researcher in Austin.']	['<urn:uuid:107affad-7dc8-4f48-ae19-0636c3611509>']	factoid	direct	long-search-query	similar-to-document	single-doc	novice	2025-04-22T14:43:01.858830	13	39	1063
