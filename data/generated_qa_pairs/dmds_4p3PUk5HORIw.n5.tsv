qid	question	answer	context	document_ids	question_factuality	question_premise	question_phrasing	question_linguistic_variation	question_multi-doc	user_expertise-categorization	generation_timestamp	question_length	answer_length	context_length
1	different types teeth functions describe	There are four different types of teeth with specific functions: 1) Incisors are the front teeth (four on upper and lower jaw) that take the first bite of food and are visible when smiling; 2) Canines are two pairs of sharp teeth that primarily function to tear difficult-to-chew food; 3) Premolars are found at each side of the mouth and help further masticate food; 4) Molars act as reserves if a premolar is lost and are usually hard to reach when brushing.	['Did you know that the first process of food digestion occurs in the mouth? The first mechanical digestion stage occurs as food is chewed down by teeth, coupled with chemical digestion by salivary enzymes, and the masticated food then proceeds to the gastrointestinal tract for further digestion. Herein, we will be talking about dentistry basics, from tooth types down to their respective functions. Before that, however, it would be helpful if we first talk about the underlying tooth anatomy.\nWhat Is a Tooth Made Of?\nA tooth has three primary structures. These are:\n(1)the enamel – hard, mineralized covering of the tooth\n(2)the cementum – covers the root of the tooth and helps stabilize tooth anchorage\n(3)the dentine – consists majority of the tooth and it has a protective function; susceptible to cavities and decay\n(4)the pulp – core of the tooth, containing nerves and blood vessels\nAltogether, these structures make up the specific anatomical regions of a tooth. Specifically, the crown and the root are the two basic anatomical parts of the tooth which serve specific functions. The root is the part embedded under the gums which serve as an anchor to the tooth socket of the jaw. Conversely, the crown is the visible part which is covered with enamel and is subject to wear and tear. The crown and the root usually varies in shape and number respectively, depending on the type of tooth it comprises.\nDifferent Types of Teeth According to Functions\n(1) the Incisors – these are the front teeth (four on the upper jaw and four on the lower jaw) which usually takes the first bite of food and these are also the ones we see whenever we smile.\n(2) the Canines – these are two pairs of sharp teeth and it primarily functions to tear food (especially the ones that are difficult to chew)\n(3) the Premolars – found at the each side of the mouth and helps further masticate food\n(4) the Molars – act as reserves in any case that a premolar is lost; usually hard to reach when brushing.\nThese types of teeth appear at different stages of human development. Some of these teeth appear during infancy and slowly but surely, the others follow as a person grows and develops and this explains why numbers vary in a person’s lifetime.\nYour Number of Teeth Vary as You Develop\nCommonly, adult humans have more in number than children, having 32 permanent ones in a set. This set consists of 12 molars (along with four wisdom teeth), eight premolars, four canines and eight incisors. In contrast, children have a set of teeth consisting of 20 baby teeth, which eventually fall out and gets replaced as they grow. However, it should be noted that proper dental care should is needed regardless if the set of teeth is permanent or deciduous.\nDental Care In a Nutshell\nProper dental care is essential to maximize the functionality of our teeth. Besides its aesthetic value, these also serve as the primary mechanical digesters which help us break down food for our body to process. A tooth has several structures which altogether, forms the two major parts of a tooth, which vary in shape and size. These variations all lead to a person having different types of teeth which serve different roles in chewing. While precursors of these teeth types are present during childhood, completion of a set of teeth usually does not occur until puberty, where a full set of 32 teeth from each type are present.']	['<urn:uuid:e1ab1145-7b17-4313-911c-aa064b9e5cd5>']	open-ended	with-premise	short-search-query	similar-to-document	single-doc	expert	2025-04-20T22:06:50.645746	40	460	3395
2	planning visit to chicago history oak park frank lloyd wright home studio what to expect inside	The Frank Lloyd Wright Home and Studio in Oak Park, Illinois is a National Historic Landmark where the architect spent his first 20 years of career and perfected the Prairie Style. You can take a guided tour of the building's interiors, examine exterior sculptures designed by Wright's collaborator Richard Bock, and explore the surrounding historic district through a self-guided audio tour. There's also a gift shop offering Prairie and Arts & Crafts merchandise.	['The bustling metropolis of Chicago has long attracted visitors in search of unique and diverse cultural experiences.\nIn particular, the city’s architecture is recognized as both an inspiration and reflection of the USA’s architectural history, and no Chicago architecture tour would be complete without a look into Frank Lloyd Wright’s genius. To immerse yourself in his designs, look no further than Chicago and nearby Oak Park, where you’ll find the world’s largest collection of Wright-designed buildings. These are just a few of gorgeous examples of the architect’s desire to create structures “in harmony with humanity and its environment.”\nFrank Lloyd Wright Home and Studio, Oak Park\nFirst stop: Oak Park, Illinois, where the architect spent the first 20 years of his prolific career. This charming and historic village has long welcomed the LGBTQ+ community (it was one of the first to introduce a domestic partnership registry for same-sex couples and enact an ordinance barring employment and housing discrimination based on sexual orientation). Here, you can take a guided tour of the interiors of his National Historic Landmark home and studio. It was here that he perfected the Prairie Style, working diligently in the site’s studio, which was completed in 1898. Outside, spend some time examining the sculptures on the building’s exterior (designed by Wright’s friend and collaborator Richard Bock), then explore the surrounding historic district via self-guided audio tour. Before you leave, stop by the gift shop that carries Prairie and Arts & Crafts merchandise for a take-home memento.\nTouring the Frank Lloyd Wright Home and Studio\nUnity Temple, Oak Park\nWright, who grew up a Unitarian, called Unity Temple his “contribution to Modern Architecture,” and it’s not hard to see why. The only remaining public building produced during his Prairie period, the Unity Temple was also one of the first houses of worship created with materials traditionally reserved for factories or warehouses; in particular, reinforced concrete. The choice proved to be enormously influential to the modern architects and designers who came after him. Also a National Historic Landmark, the building is still in use for a weekly service open to everyone, and it can also be toured. Inside and out, visitors will marvel at the temple’s simple yet bold design, which Wright intended to be reminiscent of an ancient temple.\nA peek inside Unity Temple, built in the early 1900s and just a three-block walk from The Frank Lloyd Wright Home and Studio\nFrederick C. Robie House, Chicago\nThis Prairie-style building is located on the scenic campus of University of Chicago, where it provides another kind of education altogether. Guided interior tours take guests through Wright’s vision of a family home, with the children’s playroom, entryway, living room, dining room and master bedroom, among others, on display. The house continues to evolve as it undergoes renovations, an effort to bring what is often considered the most architecturally-significant structure in the USA back to its original splendor. Catch the Private Spaces Tour for a behind-the-scenes look at areas not normally accessible to the public, including the servant’s wing and the third floor.\nThe Robie House, a U.S. National Landmark designed by Frank Lloyd Wright\nChicago serves as the Midwest’s major transportation hub, with two international airports offering worldwide service. Fly in to O’Hare (ORD) or Midway (MDW); both airports offer multiple transportation options into the city, including trains, shuttles and rental cars. Oak Park is located about 18 minutes outside of Chicago, accessible via Interstate 290 West or on the Chicago Transit Authority Green line train.']	['<urn:uuid:d2030477-e955-4a55-9976-690ae5cf2478>']	open-ended	with-premise	long-search-query	similar-to-document	single-doc	novice	2025-04-20T22:06:50.645746	95	465	3743
3	dna mismatch repair cancer disease connection	DNA mismatch repair (MMR) defects are directly linked to cancer development and disease. Mutations in human MMR genes (hMSH1, hMLH1, and hMSH2) lead to cancer due to defective mismatch repair. Specifically, MMR defects are associated with Lynch syndrome, an inherited predisposition to cancer of the colon and other organs. Additionally, MMR-defective tumor cells show resistance to several chemotherapeutic agents, highlighting the importance of MMR proteins in both cancer development and treatment response.	['- PDF (204 kb)\n- Full Text with Thumbnail Figures\n- Full Text with Large Figures\n- Cited by in Scopus (36)\n- Request permission\n- ATR Kinase Activation Mediated by MutSα and MutLα in Response to Cytotoxic O-Methylguanine Adducts\nMolecular Cell, Volume 22, Issue 4, 19 May 2006, Pages 501-510\nKen-ichi Yoshioka, Yoshiko Yoshioka and Peggy Hsieh\nSummarySN1-type alkylating agents that produce cytotoxic O6-methyl-G (O6-meG) DNA adducts induce cell cycle arrest and apoptosis in a manner requiring the DNA mismatch repair (MMR) proteins MutSα and MutLα. Here, we show that checkpoint signaling in response to DNA methylation occurs during S phase and requires DNA replication that gives rise to O6-meG/T mispairs. DNA binding studies reveal that MutSα specifically recognizes O6-meG/T mispairs, but not O6-meG/C. In an in vitro assay, ATR-ATRIP, but not RPA, is preferentially recruited to O6-meG/T mismatches in a MutSα- and MutLα-dependent manner. Furthermore, ATR kinase is activated to phosphorylate Chk1 in the presence of O6-meG/T mispairs and MMR proteins. These results suggest that MMR proteins can act as direct sensors of methylation damage and help recruit ATR-ATRIP to sites of cytotoxic O6-meG adducts to initiate ATR checkpoint signaling.\nSummary | Full Text | PDF (493 kb)\n- Reconstitution of 5′-Directed Human Mismatch Repair in a Purified System\nCell, Volume 122, Issue 5, 9 September 2005, Pages 693-705\nYanbin Zhang, Fenghua Yuan, Steven R. Presnell, Keli Tian, Yin Gao, Alan E. Tomkinson, Liya Gu and Guo-Min Li\nSummaryThis paper reports reconstitution of 5′-nick-directed mismatch repair using purified human proteins. The reconstituted system includes MutSα or MutSβ, MutLα, RPA, EXO1, HMGB1, PCNA, RFC, polymerase δ, and ligase I. In this system, MutSβ plays a limited role in repair of base-base mismatches, but it processes insertion/deletion mispairs much more efficiently than MutSα, which efficiently corrects both types of heteroduplexes. MutLα reduces the processivity of EXO1 and terminates EXO1-catalyzed excision upon mismatch removal. In the absence of MutLα, mismatch-provoked excision by EXO1 occurs extensively. RPA and HMGB1 play similar but complementary roles in stimulating MutSα-activated, EXO1-catalyzed excision in the presence of a mismatch, but RPA has a distinct role in facilitating MutLα-mediated excision termination past mismatch. Evidence is provided that efficient repair of a single mismatch requires multiple molecules of MutSα-MutLα complex. These data suggest a model for human mismatch repair involving coordinated initiation and termination of mismatch-provoked excision.\nSummary | Full Text | PDF (722 kb)\n- Mammalian mismatch repair: error-free or error-prone?\nTrends in Biochemical Sciences, Volume 37, Issue 5, 1 May 2012, Pages 206-214\nJavier Peña-Diaz and Josef Jiricny\nAbstractA considerable surge of interest in the mismatch repair (MMR) system has been brought about by the discovery of a link between Lynch syndrome, an inherited predisposition to cancer of the colon and other organs, and malfunction of this key DNA metabolic pathway. This review focuses on recent advances in our understanding of the molecular mechanisms of canonical MMR, which improves replication fidelity by removing misincorporated nucleotides from the nascent DNA strand. We also discuss the involvement of MMR proteins in two other processes: trinucleotide repeat expansion and antibody maturation, in which MMR proteins are required for mutagenesis rather than for its prevention.\nAbstract | Full Text | PDF (400 kb)\nCopyright © 2006 Elsevier Inc. All rights reserved.\nCancer Cell, Volume 9, Issue 6, 417-418, 13 June 2006\nPreviewAdd/View Comments (0)\nMismatch repair proteins as sensors of alkylation DNA damage\n1 Division of Hematology/Oncology, Department of Medicine and Moores Cancer Center, University of California, San Diego, School of Medicine, 3855 Health Sciences Drive, La Jolla, California 92093\n2 Department of Cell Biology, Albert Einstein College of Medicine, 1300 Morris Park Avenue, Bronx, New York 10461\n- The DNA mismatch repair (MMR) system maintains genome integrity by correcting replication errors. MMR also stimulates checkpoint and cell death responses to DNA damage suggested by the resistance of MMR-defective tumor cells to several chemotherapeutic agents. MMR-dependent cytotoxic response may result from futile repair; however, MMR-mediated apoptosis has been genetically separated from its repair function. In a recent issue of Molecular Cell, Yoshioka and coworkers show that MMR complexes (MutSα and MutLα) are required for the recruitment of ATR-ATRIP to sites of alkylation damage, demonstrating that MMR complexes can function as sensors in DNA damage signal transduction.', 'Life would not be possible without the ability to repair damaged DNA. Since replication errors, including mismatch, and harmful exogenous factors are everyday problems for a living organism, a broad repertoire of repair genes has evolved in prokaryotes and eukaryotes. The following types of DNA repair can be distinguished by their basic mechanisms: (1) excision repair to remove a damaged DNA site, such as a strand with a thymine dimer; (2) mismatch repair to correct errors of replication by excising a stretch of single-stranded DNA containing the wrong base; (3) repair of UV-damaged DNA during replication; and (4) transcription coupled repair in active genes.\nA. Excision repair\nThe damaged strand of DNA is distorted and can be recognized by a set of three proteins, the UvrA, UvrB, and UvrC endonucleases in prokaryotes and XPA, XPB, and XPC in human cells. This DNA strand is cleaved on both sides of the damage by an exonuclease protein complex, and a stretch of about 12 or 13 nucleotides in prokaryotes and 27 to 29 nucleotides in eukaryotes is removed. DNA repair synthesis restores the missing stretch and a DNA ligase closes the gap.\nB. Mismatch repair\nMismatch repair corrects errors of replication. However, the newly synthesized DNA strand containing the wrong base must be distinguished from the parent strand, and the site of a mismatch identified. The former is based on a difference in methylation in prokaryotes. The daughter strand is undermethylated at this stage. E.coli has three mismatch repair systems: long patch, short patch, and very short patch. The long patch system can replace 1kb DNA and more. It requires three repair proteins, MutH, MutL, and MutS, which have the human homologues hMSH1, hMLH1, and hMSH2. Mutations in their respective genes lead to cancer due to defective mismatch repair. C. Replication repair of UV-damaged DNA DNA damage interferes with replication, especially in the leading strand. Large stretches remain unreplicated beyond the damaged site (in the 3? direction of the new strand) unless swiftly repaired. The lagging strand is not affected as much because Okazaki fragments (about 100 nucleotides in length) of newly synthesized DNA are also formed beyond the damaged site. This leads to an asymmetric replication fork and single-stranded regions of the leading strand. Aside from repair by recombination, the damaged site can be bypassed.\nD. Double-strand repair by homologous recombination\nDouble-strand damage is a common consequence of γ radiation. An important human pathway for mediating repair requires three proteins, encoded by the genes ATM, BRCA1, and BRCA2. Their names are derived from important diseases that result from mutations in these genes: ataxia telangiectasia and hereditary predisposition to breast cancer (BRCA1 and BRCA2). ATM, a member of a protein kinase family, is activated in response to DNA damage (1). Its active form phosphorylates BRCA1at specific sites (2). Phosphorylated BRCA1 induces homologous recombination in cooperation with BRCA2 and mRAD5, the mammalian homologue of E. coli RecA repair protein (3). This is required for efficient DNA double-break repair. Phosphorylated BRCA1 may also be involved in transcription and transcription-coupled DNA repair (4). (Figure redrawn from Ventikaraman, 1999).']	['<urn:uuid:8c853c21-ab8d-4825-bb17-d2ff6d625e2b>', '<urn:uuid:87d99b80-1445-4815-8251-d851e628d72b>']	open-ended	direct	short-search-query	similar-to-document	multi-aspect	expert	2025-04-20T22:06:50.645746	45	510	8063
4	How were horses and alpacas first used by ancient civilizations?	Both horses and alpacas were initially used as food sources by ancient civilizations. Stone Age cave paintings show horses as objects of human prey, while alpacas were later selectively bred by the Incas around 6,000 years ago. After domestication, horses evolved to serve multiple purposes including agriculture, transportation, and warfare. The Incas, on the other hand, primarily valued alpacas for their precious fiber, which became the basis of their economy and was even used as currency. The Incas held alpacas in such high esteem that they were worshipped as a gift from 'Pachmana' (Earth Mother).	"['Alpacas are a domesticated member of the camel (camelid) family. The camelid family also includes llamas, guanacos, and vicunas from South America, and the Bactrian and Dromedary camels from Asia and Africa. This family of animals originated on the plains of North America about 10 million years ago. A common ancestor to the South American camelids migrated to South America about 2.5 million years ago. Two wild species, vicunas and guanacos, emerged. They still live in the Andes. It is believed that about 6,000 years ago alpacas were created through selective breeding which was heavily influenced by the vicuna. There are similarities in size, fiber, and dentition (teeth) between the alpaca and the wild vicuna.\nThe Incas, a highly sophisticated civilization of its time, are the first purveyors of these very docile creatures. They developed the fleece that at the time was some of the most prized fabric in the world.\nThe Incas of pre-historic South America would separate alpacas according to their color and characteristics. Archaeological evidence suggests alpacas were held in high esteem and even worshipped in Inca society. They in fact identified these creatures as a gift from “Pachmana,” or the Earth Mother. Incas viewed these animals as a gift loaned to humans provided they were well taken care of.\nAlpaca fibers and other textiles in fact formed the basis of this early agrarian economy. In fact, fibers were used as a medium of exchange (like money today) in this ancient society…they were so valuable that Inca fighters set fire to warehouses containing the fiber when they retreated from battle.\nSpanish Explorers Brought the Alpaca to Near Extinction\nWhen Spanish explorers discovered South America in the early 14th century, they grossly underestimated the value of alpaca fibers due to the abundance of gold, silver and other precious metals.\nIn its effort to conquer the native peoples of Peru, Bolivia and northern Chile, the Spaniards endorsed and actively participated in the wholesale slaughter of these animals…as much as 90% of the alpaca herd in those days were slaughtered and left out in the fields to rot.\nFortunately, a few of the native Incas were able to stave off extinction of alpacas by hiding them in the barren and remote altiplano.\nFading into the background for two or three centuries, the alpacas once again gained prominence during the 19th century and Industrial Revolution. Fabric and finished goods like hats, scarves and other clothing were held in high esteem across Europe during this time.\nBut with the advent of synthetic fibers, alpacas once again faded into obscurity in the early parts of the 20th century…albeit not as violently as before. However, in recent years, interest in alpaca fiber clothing has surged, perhaps partly because alpaca ranching has a reasonably low impact on the environment, but arguably because of its exquisite characteristics of softness and warmth. Outdoor sports enthusiasts recognize that its lighter weight and better warmth provides them more comfort in colder weather, so outfitters such as R.E.I. and others are beginning to stock more alpaca products. Using an alpaca and wool blend such as merino is common to the alpaca fiber industry in order to improve processing and the qualities of the final product.\nIn December 2006 the General Assembly of the United Nations proclaimed 2009 to be the International Year of Alpaca Fibers, so as to raise the profile of alpaca and other natural fibers. Today, alpaca fiber continues to be prized for its special and unique characteristics by manufacturers, hand spinners and weavers all over the globe.\nThe Alpaca Hacienda', 'The Domestication of the Horse\nThe Domestication of the Horse\nThe horse is best described as a single species (Equus) that consists of various breeds. It is a hoofed, herbivorous mammal that has had a significant impact throughout the history of man. In fact, the horse has been referred to as ""the proudest conquest of man,"" by the French zoologist Georges Buffon (1707-1788). Its importance to the course of human history cannot be understated.\nThe relationship between horse and human is quite unique. Unlike most other large animals, the horse has been likened to a partner and friend. From its earliest history, the horse has served as a food source, aided agriculture by plowing fields and bringing in the harvest, helped to track and hunt down game, served as an important transport vehicle for goods and people, as well as an integral part of war and conquest, and has even provided many forms of recreational activities.\nThe earliest form of the horse, known as Hyrocotherium, or eohippus (dawn horse) is believed to have first appeared on Earth approximately 50 million years ago. It is characterized as a being a timid creature about the size of a dog. It lived in swampy areas and its range extended over much of the world. As the habitat of the early horse changed from swamp to forest to grasslands, it went through many evolutionary changes until it reached its present form, Equus, that is seen today. It should be noted that although the range of the horse initially spread throughout the world, it became extinct in the Western world some 8,000 years ago and was brought into Western hemisphere by the Spanish about 500 years ago.\nInitially, the wild horse was used as an important food source. This use is depicted in Stone Age cave paintings, such as those seen at Lascaux, France. These are believed to be nearly 20,000 years old and show the horse as an object of human prey. In order to make better use of the horse, it was domesticated. There is little agreement as to when this actually happened, but it is likely that other animals such as dogs and cattle were domesticated first. Some authorities argue that early Cro-Magnon farmers were nomadic and domesticated the horse for use as a pack animal. Others make the case that domestication took place 6,000 years ago by native tribes living on the steppes adjacent to the Black Sea. After this time, horses quickly became indispensable to the people who used them.\nAfter domestication, horses were still used as a ready food source for their meat and mare\'s milk. But there is recent evidence that humans began to ride horses soon after domestication. The minimum amount of equipment needed in order to control the horse would be some sort of bit or mouthpiece attached to reins. Archeological evidence shows horse teeth with bit wear dated as early as 4000 b.c. The early forms of bits were probably just ropes around the jaw, but more sophisticated bridle setups using antlers coupled with some other pliable material have been uncovered. This type of system would give the rider superior control over the horse and indicates that these early nomads were probably accomplished horsemen.\nEarly humans had seen the power of draft animals and the yoke was believed to be invented around 5000 b.c. to harness this source of power. In the Near East, cattle were used to drive sledges, but wheels were added in the next millennium. As horses were brought from Asia between 2000 and 3000 b.c., it became apparent that these animals were much faster than cattle and they quickly became the draft animal of choice. The yoke collar needed to be modified for use with horses because it restricted their breathing. Breast straps and other types of collars were invented in order to circumvent this problem. By the fifteenth century b.c., Egyptians developed a wishbone-shaped yoke, joined by straps, that enabled the horse to breath without restriction while being used as a draft animal.\nThe use of the horse for war and hunting was not overlooked at this time either. An early form of a chariot pulled by horses was developed for these purposes. It was a light, responsive vehicle that gave the driver much control. All metal bits were introduced for use on chariots around 1500 b.c. These made the use of the chariot in warfare more effective because they were stronger and gave greater control over the team of horses. At that time, warfare was still restricted to large chariot forces requiring disciplined horses that were in good condition. It would be another 500 years before men got directly on the horse and began forming cavalry units.\nSpeculation exists as to the reason why early humans generally preferred driving horses rather than riding them. One reason commonly given was the small stature of early breeds. However, given the fact that small horses are used today as effective mounts, this assumption is questionable. It may have been sociological factors that influenced this, as it seems the upper class had a great aversion to horse sweat, so it may have been undignified to come into direct contact with the horse.\nThe first group to make wide use of the horse for successful warfare was a united group of nomadic tribes from the Russian steppe, collectively called the Scythians, in 800 b.c. They mastered the technique of horseback archery and measured their wealth in terms of horses. As feared warriors, they influenced future generations with their demonstration of the horse\'s great power and advantage. By 700 b.c. riding horseback was much preferred over a chariot for both hunting and war.\nNative tribes of the steppes made most of the technological advances involving the horse, and these advances were brought west as they tribes invaded neighboring lands. The first horseshoes are no exception. Horseshoes of various types were used by these tribes, but it was the Romans who made wide use of them. Horseshoes were used by the Romans to reduce wear of the hooves, especially on paved roads. These iron shoes were known as ""hipposandals"" and were tied to hooves. The modern iron horseshoe, which is nailed to the hoof, first appeared around the fifth century a.d.\nThe advent of the saddle, a seat for the rider, was an important development. The leather saddle, developed in around the third century b.c., greatly improved the utility of the horse. In conjunction with this, the stirrup—used to support the rider\'s feet—was used to help in the control of the horse as well. Attila the Hun (406?-453) is credited with introducing the stirrup to Europe, giving rider\'s in the West much more maneuverability and control. Subsequent advances in both the saddle and stirrup occurred throughout early history, with modern stirrups and saddles bearing a striking resemblance to those of the Middle Ages.\nThroughout human history the horse has been extremely beneficial and has helped to shape society into what it is today. The horse has been an important component in recreation, travel, labor, and especially war. First and foremost, its domestication was absolutely essential for society to realize how useful and valuable the animal could be. Secondly, the invention of important technological advances, such as the invention of the collar, bit, bridle, stirrup, saddle, and horseshoe, allowed humans to make the horse much more functional, which was important in its rise to significance.\nThe first domesticated horses were used as food and their hide, but subsequent generations began to use the animal for other things. It seems likely that the next major step was to use the horse to lessen the work done by man. As the horse was harnessed as a power source, the need for human power was reduced. This had the effect of reducing human slavery, since slaves were used primarily as a major source of power.\nThe legacy of the importance of the horse in antiquity would not be complete unless its significance in waging war was discussed. Armies that used horses for conquest had distinct advantages over those that did not. During antiquity the world was overrun by various ethnic groups that relied heavily upon horses during battle. The initial preference was for two-man chariots, that consisted of a driver and an archer positioned to shoot arrows at the enemy. Later, men directly mounted horses for combat purposes. This gave well-trained armies using the horse a significant advantage and proved to be a key component of conquest for thousands of years. The horse figured significantly in the course of human history and those that could best harness its natural power played more dominant roles until the technological advances of machines made the horse relatively obsolete in modern countries.\nJAMES J. HOFFMANN\nCaras, Roger A. A Perfect Harmony: The Intertwining Lives of Animals and Humans Throughout History. New York: Simon & Schuster, 1996.\nFacklam, Margery. Who Harnessed the Horse? The Story of Animal Domestication. New York: Little Brown & Co., 1992.']"	['<urn:uuid:7889f3ca-2c70-4f76-b9af-6e8d2c58b7b2>', '<urn:uuid:d647530f-9e9d-40b9-aff5-c0e089c1a69d>']	open-ended	direct	concise-and-natural	similar-to-document	comparison	novice	2025-04-20T22:06:50.645746	64	605	12628
5	password hack future problems	While some data like regularly changed passwords might not be a major concern, there are serious long-term risks. Data vital to national security that is being transmitted now with quantum-crackable encryption could jeopardize security even 50 or 100 years later if released. Additionally, permanent personal data like social security numbers or biometric identities (such as iris scans) that have been transmitted using current encryption methods could be compromised. Bitcoin, which relies on elliptic curve encryption, is also vulnerable and particularly difficult to modify due to its distributed nature.	"['Written by: Anastasia Marchenkova\nImagine having a super-fast computer that could solve problems and crunch data faster than anything we\'ve seen before. It can develop new medicine more quickly and cheaply, solve energy efficiency problems in batteries and solar panels, and create better materials to withstand heat and strain.\nSounds great, right? But, there\'s a catch. These super-fast computers, or quantum computers, could crack the security measures we use to keep our online information safe. This is the “quantum threat” – the potential for quantum computers to crack many of today\'s encryption schemes, which could result in a collapse of our modern-day cybersecurity infrastructure.\nOur current encryption methods, which keep our digital communications safe, heavily rely on the assumption that certain mathematical problems are hard to solve with classical computers. RSA encryption is based on factoring large numbers, while elliptic curve cryptography (ECC) is based on finding the discrete logarithm of a random elliptic curve element. Both of these problems are computationally difficult for classical computers.\nThe concept of a \'quantum threat\' gained significant attention after the proposal of Shor\'s algorithm by mathematician Peter Shor in 1994. Shor\'s algorithm was quantum’s killer app, being the first to demonstrate the potential power of a large-scale, fully functional quantum computer in the real world. This algorithm could unlock the digital \'locks\' we currently use to secure our online world.\nShor’s algorithm can factor large numbers exponentially faster than the best-known algorithms running on classical computers. The implication for encryption is profound. If a large and error-corrected quantum computer could run Shor\'s algorithm, it could crack these encryption methods, posing a significant threat to digital security worldwide.\nAnd how fast? Exponentially faster is very fast. Breaking a 2048-bit RSA key would take 1 billion years with a classical computer. A quantum computer could do it in 100 seconds.\nThe immediate focus on examining post-quantum security solutions is no longer optional. Ironically, ECC was recommended over RSA because it offered the same level of security but used shorter keys, resulting in faster computation and less demand for resources such as storage and bandwidth. This made ECC a more efficient and appealing choice for a broad range of applications, from securing web traffic to protecting sensitive government data. However, ECC is even more vulnerable to quantum computers than RSA, requiring even less qubits for the calculation.\nSome data are just valuable for a day, or less. Your job requires you to reset your password every 3 months. So, in the worst-case scenario, a large-scale coherent quantum computer will be released and every website will force us to reset our passwords, encrypted with a new cryptosystem. No big deal.\nBut what about data vital to national security that is being sent now with quantum-crackable encryption, and can jeopardize national security even in 50 or 100 years if released? What about Bitcoin, built on elliptic curve encryption as the foundation of all its transactions - and is very hard to change because of its trustless, distributed nature? Or was your social security number sent over the web using ECC? Your WorldCoin biometric identity? You can’t replace your iris.\nThese data breaches can devastate businesses, industries, and individuals. They can lead to a loss of consumer trust, damage to reputation, and significant financial loss. The Equifax breach in 2017 exposed the personal data of 143 million people, resulting in a settlement of $425 million just to help people affected by the breach. And that was not a fundamental encryption issue. What we could be facing is a data breach armageddon.\nThe impacts of a large quantum computer are profound on fundamental science. Trillions of dollars are being invested into quantum computing because of applications in materials, healthcare, and financial modeling. But with worldwide security at risk, the race to build a large-scale quantum computer is also highly funded by governments. China, Russia, the US, the UK, and many others are putting hundreds of billions into quantum technology. For instance, the US CHIPS Act provides roughly $280 billion in new funding to boost domestic research and manufacturing of semiconductors in the United States.\nThere is some disagreement on when the threat will arrive. As of mid-2023, we are in the era of Noisy Intermediate-Scale Quantum (NISQ) computers. NISQ devices are characterized by having just tens to a few hundred qubits (quantum bits, similar to bits, but are two-level quantum mechanical systems that can be prepared in a probability of a 0 or 1) and noise in quantum operations that introduces calculation errors. These machines, while needing to scale up to break modern encryption, are growing rapidly. While there are scalability challenges to address in quantum hardware, quantum computers have been accelerating in qubit counts, while gate infidelity, or the error rates applying quantum circuits, have been decreasing.\nWhile we still have time before the quantum threat, the scaling and error issue is being attacked from multiple sides. Researchers are focused on manufacturing techniques for superconducting hardware to scale on qubit count and fidelity, new modalities are emerging, like photonic-based quantum computers, and theorists are working on new error correction algorithms to reduce the number of supporting qubits. In the last few years, co-designed application-specific processors became the most viable path to quantum advantage to reduce qubit count and error correction needed to scale.\nMicrosoft Research has calculated around 2500 qubits are needed to compute elliptic curve discrete logarithms to crack a standard 256-bit key. Around 4000 qubits are needed for 2048-bit RSA. While these are logical qubits, not physical (logical qubits will require additional physical qubits for error correction), the resource estimates keep decreasing.\n10 years ago, John Preskills’s lecture noted you would need 10 million physical qubits to break RSA cryptography. In 2023, a PRL paper, ""Performance Analysis of a Repetition Cat Code Architecture: Computing 256-bit Elliptic Curve Logarithm in 9 Hours with 126 133 Cat Qubits” was released showing two orders of magnitude reduction in qubit counts. Assuming a similar pace of two orders of magnitude reduction of qubit counts in the next 10 years, we may only need ~2000 qubits to break RSA encryption. BTQ’s QByte Quantum Risk Calculator tracks qubit counts and quantum infidelity. Even pessimistic estimates place that threshold by 2030, without any advances in error correction or fabrication.\nGiven the potential vulnerabilities, there is a growing consensus on the need for preemptive action. This involves adopting post-quantum cryptography algorithms that are resistant to quantum attacks. While there\'s uncertainty about when a quantum computer capable of breaking current cryptography will exist, the risk is significant enough to warrant immediate action.\nIndustry and academia are closely collaborating to advance post-quantum security. For instance, NIST\'s Post-Quantum Cryptography Standardization project involves both academic and industry players in the development and evaluation of new cryptographic systems. BTQ’s post-quantum cryptography scheme, Preon, was recently selected by NIST as a candidate for the Post-Quantum Cryptography Standard.\nPost-quantum cryptographic algorithms are designed to resist attacks by classical and quantum computers. These include lattice-based, hash-based, code-based, multivariate polynomial, and isogeny-based cryptographic algorithms. Each approach has pros and cons regarding security, performance, and key and ciphertext sizes.\nWhile post-quantum cryptographic algorithms are still being studied, evaluations show promise in their resistance against known quantum attacks. However, more research is needed to identify unknown vulnerabilities and thoroughly assess their security.\nEven though the standard has not yet been released, large tech companies, including Google, Microsoft, and IBM, are testing post-quantum algorithms, and some have already started implementing post-quantum security in their products and services. Blockchains, like QRL and Nexus, are being built “quantum-first” to protect against the risks of large-scale quantum computers.\nAs post-quantum cryptographic standards emerge, organizations may need to comply with new regulatory requirements that take adequate measures to secure data against quantum threats.\nWhile the new standards and cryptosystems are open source, cryptography isn’t enough for this new, unknown world where crypto agility is critical. Vendors approved by government players as post-quantum solutions will have a head start - especially since national security is a crucial priority and holder of a lot of long-term data. The transition to post-quantum cryptography will be more complex for companies in regulated industries like finance or healthcare. These companies will need to balance the adoption of new technologies with compliance with existing and emerging regulations.\nThe advent of quantum computing is a double-edged sword, bringing immense promise, but significant threats to digital security. It\'s crucial for governments, industries, and academia to work together to develop, implement, and scale the adoption of robust post-quantum cryptographic algorithms. Although the active quantum threat is still a future concern, the time to act is now.']"	['<urn:uuid:61f051f1-2534-4ed7-95d4-ff759f2c6751>']	open-ended	direct	short-search-query	distant-from-document	single-doc	novice	2025-04-20T22:06:50.645746	29	608	9663
