qid	question	answer	context	document_ids	question_factuality	question_premise	question_phrasing	question_linguistic_variation	question_multi-doc	user_expertise-categorization	generation_timestamp	question_length	answer_length	context_length
1	I manage HR for an accounting firm - what's the connection between CPA exam transcript requirements and the changing demographics in the workforce, particularly looking ahead to 2044?	For CPA exam eligibility, candidates must submit official transcripts to their state board of accountancy, with strict requirements on transcript hours and course content. This process intersects with significant demographic shifts, as by 2044, minority groups will reach majority status in the U.S. This demographic change has implications for the accounting profession, as businesses need to adapt their practices to reflect these changing demographics.	['To become a CPA, you have to follow a specific roadmap, and it starts well before you submit your CPA exam application.\nThe goal is to pass the first time you take the test.\nYour personal timeline for studying should be a key consideration. You’ll need to be well prepared by the time you actually take the test.\nHere are all of the important steps, timelines, documents, and details you need to know about when you apply to take the CPA exam.\nHow to schedule for the CPA exam\nThere are six important steps you need to take as you apply for the CPA exam.\nYour CPA exam application process must align with state and national requirements. You need to understand all of the essential documents, submissions and due dates before you begin.\nHere’s how to schedule the CPA exam so that everything goes smoothly.\nStep 1: Make sure you are eligible\nFirst, you have to be sure you have met all of the requirements for the CPA exam.\nYour eligibility is determined by education and state-specific CPA requirements. For example, in some states, there are age and residency requirements.\nIn almost all states, the educational institution you attend must be approved by the state board of accountancy. There are also strict requirements on the number of transcript hours you have in college and graduate courses as well as what those courses covered.\nIn the event that you have a unique circumstance or don’t have official university documents, you can use NASBA’s Education Verification Services. International students will find this especially helpful.\nStep 2: Send in all official transcripts to your state board\nOnce you have completed all of the education requirements, you will have your official school transcripts sent to your state board of accountancy.\nWhen you are determining where to send transcripts for the CPA exam, you should go to the NASBA website and select your state or jurisdiction.\nThe state board will provide clear instructions for how they will accept your transcripts and any additional steps to ensure they are reviewed and approved.\nStep 3: Submit CPA exam application and fees\nOnce your official transcripts are submitted and approved, you will be able to apply for the CPA exam.\nTo do this, you will submit an online application and pay the application fee — one of several CPA exam costs you will incur. The fee varies by state but is usually around $100 to $200. The fee only covers administrative costs to apply, and won’t apply to anything else.\nWhile you submit your transcripts to your state board of accountancy, the application stage of your process is done through the CPA Examination Services. That’s the organization you pay the fee to, and which receives your application.\nStep 4: Obtain your Authorization to Test (ATT)\nOnce your transcripts and application are received and approved, you will receive an Authorization to Test (ATT).\nThe ATT represents your eligibility, and allows you to register for the individual CPA exam sections. For most states, you’ll have 90 days to complete this process and pay.\nWhile the cost varies by state, the average you’ll pay for the four sections is $175-$250 each.\nStep 5: Obtain your Notice to Schedule (NTS)\nOnce you have paid, you can schedule your exam. The CPA exam NTS (Notice to Schedule) is an official document that authorizes you to take the exam in the U.S. at any of the testing locations.\nHow long does it take to get the NTS for the CPA exam? The timeline for this step varies.\nSome states are completely digital and get your NTS to you the same day. In other states, it could take as many as six weeks.\nStep 6: Schedule your exam with Prometric\nLast step: here’s how to schedule the CPA exam.\nThe CPA exam is only available to be taken at Prometric testing facilities. There are four times a year when you take the CPA exam.\n- January 1-March 10\n- April 1-June 10\n- July 1-September 10\n- October 1-December 10\nWondering where to take the CPA exam? There may be a Prometric location near you. If you will have to travel beyond your area, those arrangements need to be part of your test prep.\nYou can learn more about what the test experience will be like and register online through Prometric.\nCPA exam application: partner organizations\nAs you see from the steps, there are many organizations along the way that you will become familiar with as you apply for the CPA exam.\nHere is more information on each one of those and where they will intersect in your CPA exam application process.\nThe State Boards of Accountancy\nEach US state has its own board of accountancy. These serve the function of licensing and regulating public accountants.\nState-run boards are appointed by the governor, but also have input from state senators. The governance structure varies.\nYour state’s board will evaluate your education to see if you qualify to take the CPA exam. Some state boards of accountancy also directly oversee a public accountant’s license renewal and continuing education requirements.\nNASBA maintains a directory of state boards with links to all of their websites.\nThe National Association of State Boards of Accountancy (NASBA)\nThe National Association of State Boards of Accountancy (NASBA) is the way all individual state boards are connected.\nThis network provides guidance for CPA exam candidates. NASBA provides CPA exams, CPA licensure, ongoing CPA education and more. These will be resources you may use throughout your career as a CPA.\nThe American Institute of Certified Public Accountants (AICPA)\nThe American Institute of Certified Public Accountants (AICPA) is a professional network for accounting professionals. They have numerous resources for CPA exam candidates and CPA professionals.\nYou can become a member of AICPA to benefit from their advocacy, guidance, continuing education, certifications, and more.\nPrometric is the only testing facility in which you can take the CPA exam. They have online resources that can help you understand the format, process and test-taking environment you will experience during the uniform CPA exam.\nYou can check out their website for a seat availability tool and to find a Prometric location near you.\nThere are several nuances and state-specific requirements to your CPA exam application. Here are the answers to some additional questions you may have.\nHow long does it take to get the NTS for the CPA exam?\nThe Notice to Schedule (NTS) is an official document. You must have this before you are authorized to register for the CPA exam.\nSome states have digitized their process entirely, which means that you could get your NTS the same day you apply for it. More commonly, it will take 3-6 weeks.\nWhere to take the CPA exam?\nYou can only take the CPA exam at a Prometric testing site. There are many locations around the country. Use Prometric’s test location finder and learn more about scheduling and registration.\nWhere should I send transcripts for the CPA exam?\nYour official university transcripts should be sent to your state board of accountancy. They will review these transcripts. If they comply with your state standards for education requirements, you will be approved to register for the CPA exam.\nWhen should I apply for the CPA exam?\nThe CPA exam application process could take a couple of months. You will have a certain window of time in which to complete certain steps.\nYou should plan to start the application process once you know when you want to take the test. This will mean determining your study plan and study schedule.\nIt takes most people 300-400 hours total to study for the CPA exam. Keeping that in mind will help you schedule correctly, to optimize your chances of passing.\nDo you have to reapply for the CPA exam?\nIf you didn’t pass a particular section of the CPA exam (less than the 75 threshold), you will have to reapply and retake that section. To do this, you’ll have to wait 24 hours after getting a failing score. Also, you cannot sit for that section of the exam again until the next testing window.', 'Diversity in the workplace can offer a plethora of benefits to your company, including adding the diversity of thought and general cultural intelligence to your workplace. However, there are other tangible, impactful benefits of diversity in the workplace, backed up by statistics.\nWe’ve compiled a list of 13 diversity in the workplace statistics that will provide you with unique insights of trends and benefits of diversity. Within each section, we’ll discuss the stat impacts business results and how you can truly value people in your organization.\nLet’s get started.\nWhy is diversity important in the workplace?\nDiversity in the workplace is essential for several reasons.\nFirst, it helps to ensure that all voices and different perspectives are represented and heard. This can be especially important in decision-making situations. In addition, a diverse workforce can help to foster a climate of creativity and innovation. When people with different backgrounds and experiences come together, they can share ideas and find new solutions to problems.\nDiversity can also help businesses better reflect the communities they serve. By hiring a diverse workforce, companies can send a message that they value inclusion and respect for all. Diversity is an essential component of any thriving business.\nFinally, as we’ll discuss in upcoming sections, diversity can have a positive impact on your business’ profits.\n13 Useful diversity in the workplace statistics\nAll of this belief is backed up by raw statistics that prove the benefits of diversity in the workplace. We’ll break these statistics down into categories (benefits, trends, gender, etc.), and discuss how they can be of service to your business.\nBenefits of diversity in the workplace statistics\n1. Companies with diverse executive leadership and board teams earn more in profits.\nAccording to a 2021 McKinsey study, companies with diverse boards and executive leadership earn more in profits.\nSpecifically, companies in the top 25 percent in gender diversity were likely to out-earn companies in the bottom 25 percent in gender diversity by… 25%. Also, the top ethnically and culturally diverse companies out-profited the least ethnically and culturally diverse companies by 36%.\nCompanies need to make an effort to recruit board members and executive leadership from a range of backgrounds. While contributing to the diversity of thought in the most important space, it also clearly leads to better results financially.\n2. Diverse businesses can capture new markets better\nAccording to a 2013 study, more diverse companies are 70% more likely to enter and capture new markets.\nOne the reasons is because diverse management teams can better identify opportunities and solve problems. The diversity of thought leads to a diversity of ideas on how to capture new markets. To take advantage of this, companies need to ensure that they are recruiting and promoting from various backgrounds.\nAdditionally, they must create an inclusive environment where all employees feel comfortable sharing their ideas. By prioritizing diversity, companies can position themselves to succeed in today’s global marketplace.\n3. Diverse companies produce higher innovation revenues\nA 2020 study found that companies with more diversity had 19% higher innovation revenues. This showcases the connection between diversity and innovation.\nThe explanation for this? The study said that “increasing the diversity of leadership teams leads to more and better innovation and improved financial performance.” Highly innovative teams bring new ideas into being faster through brainstorming sessions and exchanges of differing perspectives on problems or projects. However, these insights can only occur when there’s full representation across racial, gender, ethnic, and cultural lines.\nDiversity trends in the workplace statistics\n4. By 2044, minority groups will reach majority status\nThis shift has far-reaching implications for businesses, which must adapt their diversity practices to reflect the country’s changing demographics.\nWhile 2044 may seem a long way off, businesses must start making these changes (if they haven’t already) to stay ahead of the curve. By preparing for the future and making their operations more inclusive, companies can position themselves to succeed in an increasingly diverse America.\nGeneral population info\n5. Racial and ethnic minorities make up 48% of Gen Z\nAccording to a report from Pew Research Center, nearly half of Gen Zers (people born between the mid-1990s to mid-2010s) are non-white.\nAs the workforce grows increasingly diverse by the generation, companies must do more to ensure that their teams reflect the society they operate in. Gen Z is already pushing companies to do this, as we’ve discussed in a previous article. More than 80% of Gen Zers surveyed by Monster said a company’s commitment to diversity practices was “important when choosing an employer.” Another poll found 75% of people in Gen Z would reconsider applying at a company if they weren’t satisfied with their diversity and inclusion efforts.\nThis statistic is a powerful reminder that the needs of minority groups must be considered when making decisions about company policies and practices. By prioritizing diversity, equity, and inclusion, organizations can stay ahead of the curve and position themselves for success in the 21st century.\nReceive a call in as little as 10 minutes.\n6. According to the 2020 Census, whites and non-Hispanics are shrinking\nIn 2020 in the United States, individuals who self-identify as white or non-Hispanic dropped below 58% of the population for the first time in history. It fell from 63.7% in 2010.\nThis shift is driven by several factors, including immigration, fertility rates, and population aging. Consequently, we can expect the workforce to become increasingly diverse in the years to come.\nThis shift has profound implications for businesses and organizations across the country. As the workforce becomes more diverse, companies must ensure that their policies and practices are inclusive of all employees from all backgrounds. This includes everything from ensuring job descriptions are free of bias to offering training on unconscious bias and cross-cultural communication.\nThe benefits of such diversity are well-documented. A more diverse workforce can lead to improved creativity and innovation and a better understanding of and sensitivity to different cultures. In an increasingly global economy, these are invaluable skills.\nGender diversity in the workplace statistics\n7. Organizations with an equal number of men and women generate up to 41% more revenue\nWe’ve discussed how general diversity is good for business, but this stat hammers down how gender diversity and equality can impact profits.\nCompanies should invest in gender diversity initiatives and promoting women to leadership positions across their company.\n8. Women are underrepresented at every level of the corporate ladder\nAccording to a 2021 study by McKinsey, “women remain underrepresented across the corporate level.” The worst levels of underrepresentation come at the C-suite and senior vice president level, where women make up between 25% and 30% of all positions despite making up just over half of the workforce. Gains have been made from 2016 to 2021, but nevertheless, underrepresentation is present across the board.\nBy understanding that there is a general underrepresentation of women at many levels of an organization, companies can take steps to address the issue. This will include implementing policies that promote equal pay for work, better parental accommodations, and more.\nRacial/Ethnic diversity in the workplace statistics\n9. White job seekers are 2.1% likelier to receive an interview callback than black applicants\nThis statistic is a harsh reality of today’s job market. (The study surveyed over 83,000 job applicants–a large pool.) Studies have consistently shown that white job seekers are more likely to receive interview callbacks than their black counterparts.\nWhile many factors contribute to this disparity, one key reason is that employers often unconsciously favor applicants who share their race or ethnicity. As a result, white job seekers often enjoy a slight but significant advantage in the hiring process, especially when contacted by white hiring managers.\nSo what can companies do to address this issue?\nOne solution is to adopt blind application processes, where personal information like names and addresses are removed from applications before they are reviewed. This helps to level the playing field and ensures that all applicants are evaluated purely on their qualifications.\nIn addition, companies can also make an effort to diversify their hiring committees to better reflect the population as a whole.\nBy taking these steps, companies can help to ensure that all job seekers have an equal chance of success.\n10. White and Asian employees typically make more money than Hispanic and Black individuals\nWhite and Asian employees’ median salaries are around 35% to 65% more than Hispanic and Black employees’ median salaries. But what can companies do to use this stat to make better decisions and improve racial and ethnic diversity in the workplace?\nOne way is to ensure that all employees have access to the same opportunities for advancement. This includes training and development programs, mentorship initiatives, and leadership roles. They also need to make sure everyone makes the same amount of money when starting in a position, especially when all candidates have the same experience.\nBy leveling the playing field, companies can give everyone a chance to succeed professionally and financially, regardless of their background.\nAdditionally, companies should work to increase the overall representation of Hispanic and Black employees at all levels within their workforce. This can be done through targeted recruiting efforts and creating an inclusive culture that values diversity.\nAge diversity in the workplace statistics\n11. The number of workers over age 75 is expected to rise between 2019 to 2029, while the number of employees aged 16 to 24 is expected to decline in the same period\nThe aging workforce is one of the most pressing issues facing employers today. This trend has major implications for businesses, which will need to adapt their policies and practices to meet the needs of an older workforce.\nOne way to do this is by implementing flexible work arrangements, such as telecommuting or part-time work, which can help older workers stay employed.\nEmployers should also consider investing in training and development programs catering to older workers and adopting recruiting and retention strategies focusing on age diversity. By taking these steps, businesses can ensure that they are prepared for the aging workforce and positioned to take advantage of the valuable skills and experience that older workers offer.\nLGBTQIA+ diversity in the workplace statistics\n12. LGBTQIA+ persons confronted a higher degree of unemployment during the COVID-19 pandemic’s peak than the general public\nAccording to a poll, “17% of LGBTQ people had lost their jobs because of COVID-19, compared to 13% of the general population.”\nAs the COVID-19 pandemic forced businesses across the country to make difficult decisions about layoffs and furloughs, data suggests LGBTQA+ Americans were disproportionately impacted by job loss.\nWhile the reasons for this disparity are not entirely clear, experts suggest that LGBTQIA+ workers are more likely to work in sectors that have been hard hit by the pandemic, such as hospitality and retail. These findings underscore the need for businesses to ensure that their workplace is inclusive and supportive of all employees, regardless of sexual orientation or gender identity. This includes implementing nondiscrimination policies, offering inclusive benefits to LGBTQIA+ families, and providing training on sensitivity and inclusivity.\n13. LGBTQIA+ individuals are concerned that their gender identity or sexual orientation would harm their professional prospects\nThree in 20 LGBTQIA+ women and six in 20 LGBTQIA+ men believe their sexual orientation will harm their professional prospects.\nCompanies should focus on creating policies and programs that support LGBTQIA+ employees, such as comprehensive nondiscrimination protections, domestic partner benefits, and fully inclusive health care coverage. It should also make investments in hiring and advancing LGBTQIA+ employees.\nCompanies should make these investments and show LGBTQIA+ employees they are valued team members.\nThe general population is becoming more diverse, and the workplace needs to follow suit. Our shared diversity in the workplace statistics provide insights into how diversity practices affect profitability, inclusion (or lack there of), and why your company needs to invest in diversity now rather than down the road.\nFor a company’s culture (and business) to thrive, it needs the employees who feel they can authentically be themselves. Your company needs to make an investment in diversity in order to achieve this, though.\nIf you need DEI solutions focused on talent and training, contact us today!']	['<urn:uuid:9c69f32e-a114-4265-bcf9-edcc702f5d87>', '<urn:uuid:797c05f4-472c-46ba-98bd-82716bafd075>']	factoid	with-premise	verbose-and-natural	distant-from-document	multi-aspect	expert	2025-05-12T20:11:36.665104	28	64	3380
2	materials used build atacama desert dome houses	The structures were built using local materials called 'Brea' and 'Tortora', which are native to a nearby village 40 minutes from the site. The basic structure used wooden pillars buried in sand, and seawater was used instead of cement for the foundation. The Brea material provides protection from harsh sunlight.	['You can explore the beauty of Chile’s Atacama Desert through eco-friendly domes\nThe latest Piedras Bayas Beachcamp introduced by MOREAS is an architectural beauty with both its ultra-creative structure and its eco-friendly design. Located in the Atacama Desert, North of Chile, the cabin covers an area of 7534.74 square feet and serves as a prime spot for tourists to stay! Surrounded by the natural life of the desert, the structure takes complete advantage of light structures, uses local materials, has a non-contaminant sanitary system and includes ample amount of green-facilities to aid to the conservation of the environment.\nThe arid Atacama Desert is a comparatively dry zone which sees a very low amount of rain per year. The coastal zone remains at a very high temperate all year round and is rich in archeology and geology. The major attraction of the Piedras Bayas Beachcamp is the limited intervention of human life which provides optimal privacy and serenity by allowing the visitors to intake all the natural beauty that surrounds them.\nThe complete structure consists of a service station and three separate bedrooms. Inside the service station, two bathrooms, one office, one community room, a room for the ranger and a family room has been built in. Each of these rooms, feature a separate bathroom and is essentially a wood platform with a dome. The entire process of building these cabins involved three stages along with three carpenters, two local artisans, and one on-site architect. Each of these domes is built well separated (a distance of 50 meters) from the other so as to maintain visitors’ privacy and to allow maximum viewing of the surrounding natural landscapes.\nA skeleton was first formed using the local materials and the rest of the structure was built through it. Instead of forming a large building which would occupy a bigger volume, multiple smaller buildings were made which were then all interconnected through exterior pathways.\nThe basic structure was built out of wooden pillars, each of which were buried one meter into the sand. The sea water was used to conjugate the pillars and create a stable foundation which helped omit the use of cement. Local materials that were mainly used during the construction were “Brea” and “Tortora”, both of which are native to a small village that is 40 minutes away from the site.\nA local bush was used which can easily grow in the wetlands around the area and validate for the use of the material for construction purposes. The Brea material being the top best with its ability to protect the building from harsh sunlight.\nCurrently a mandatory of at least 2 nights stay can be booked at the Piedras Bayas Beachcamp for USD 120 where the guests are offered kayaks, electricity and hot water all day long.\nFunctionally, such structures can prove to be beneficial as they can easily be rebuilt or removed without compromising on the aesthetic values of the surrounding natural landscape.\nAll Images: © Alejandro Gálvez, Cristina Ananias, Eduardo Montesinos via Arch Daily\nProject: Piedras Bayas BeachCamp\nh/t: Arch Daily']	['<urn:uuid:122a4444-f7db-480a-afa7-d5267867e9c8>']	factoid	with-premise	short-search-query	distant-from-document	single-doc	novice	2025-05-12T20:11:36.665104	7	50	512
3	What are the benefits of supplier segmentation for risk management and how can software help monitor risks?	Supplier segmentation helps identify exposure to risk by revealing dependencies on single sources for critical supplies. Software risk management systems then help by providing automated monitoring, early detection of red flags, and collaboration tools for managing risks. Organizations that assign an executive to oversee supply chain risk management typically achieve over 100% ROI on their risk management efforts.	"[""How To Perform Supplier Segmentation And Increase The Value Of Your Supply Chain\nTable of contents\n1. What is supplier segmentation?\n2. Why is supplier segmentation important?\n3. Types of supplier segmentation\n4. How to perform supplier segmentation\n5. How to perform supplier segmentation using the Kraljix Matrix\nHere's a quick video summary to go along with the article if you don't have time to read it all:\nCorrect supplier segmentation can help drive and deliver increased value for your supply chain. Some organizations segment their suppliers by how much they spend with them, the general thought being the more we spend with them, the more important they are for our organization.\nHowever, this is not always the case. How much you spend with a supplier is no clear indication of whether that supplier is a strategic partner for your organization. And this is why you should perform a proper supplier segmentation - to determine the role in helping to deliver value to the end customer a supplier provides.\nWhat is supplier segmentation?\nNo two suppliers are alike, as they can impact your business in different ways. Because of this, suppliers must be divided into different groups.\nSupplier segmentation is the process of dividing suppliers into distinct groups based on needs, characteristics, or behavior. This process incorporates:\npreparing supplier segmentation teams;\nreviewing supplier segments;\nidentifying opportunities with suppliers;\ndeveloping product/service agreements;\ngenerating supplier/cost profitability reports.\nWhy is supplier segmentation important?\nSuppliers who are vital to your organization need a higher level of engagement. By classifying each of your suppliers using pre-agreed criteria, you can decide upon the appropriate level of attention needed to ensure that they deliver superior service/products.\nSupplier segmentation can also provide insights into your supply base regarding the extent to which each vendor is important to your business operations. This enables you to develop a closer working relationship with key suppliers at all levels (executive, operational, and transactional).\nLast but not least, by categorizing your suppliers, you can also identify your level of exposure to risk. For example, many organizations depend on a single source of supply for critical goods and services. If that source is unable to fulfill its offering, you’ll also be unable to satisfy your customers.\nTypes of supplier segmentation\nBased on the product/service supplied, suppliers can be classified into one of four quadrants (also known as the Kraljic Matrix.):\nHowever, the classification can also depend on:\nthe breadth of supply base;\nvolume of supplied goods and/or services.\nTaking into account these attributes, you could segment your suppliers by:\n1. Spend: annual spend with a supplier is essential. You should also take a look at the YOY spend growth as there could be a supplier that doesn’t have substantial spend, but you plan on increasing their scope and consider them as a strategic partner in the future.\n2. Innovation/collaboration: this refers to the:\nspecificity of the offer: whether the supplier is offering a unique or customized product/service or an off-the-shelf one.\nbreakthrough offering: together with your supplier, you’re creating a new market segment, entering a new market, capture market share, etc.\n3. Supplier risk: this not only influences the supplier segmentation but also your sourcing strategies and supply plans. There are two types of risk you should consider:\npotential failures: this helps you assess the magnitude of the impact on your organization caused by a failure of supplier products/services.\nactual failures: past incidents/events are analyzed to determine the effect they had on business continuity.\n4. Customer impact: suppliers whose products/services enable you to enhance your customers’ experience or increase your customer base significantly should be considered strategic partners.\nHow to perform supplier segmentation\nSmall and medium-sized organizations have successfully implemented supplier segmentation based on business criticality, like in the example below:\nPriority 1 should include a small number of strategic suppliers as they often supply high-value and low-volume goods or services vital to your business operations. They are helping you grow your business by making investments in technology and new product development, and therefore they are high-risk and should be monitored closely.\nPriority 2 should cover a larger number of important suppliers, companies that enable you to run your business on an everyday basis. You could rely on alternative sources of supply if they fail, but it would be inconvenient or stressful to replace them. Risks associated with this category are mainly related to quality, service, and reputation, so you should focus on supplier performance and contractual obligations.\nLast but not least, Priority 3 will include the rest of your suppliers, mostly low-value but high-volume vendors. They supply easily replaceable commodities or services and the risk level is low, so you should only monitor their conformance to price and service levels.\nLarger organizations with big supply bases and mature supplier relationship management (SRM) programs will see better results by implementing the Kraljic Matrix.\nHow to perform supplier segmentation using the Kraljix Matrix\nThe Kraljic Matrix is one of the most effective ways to deliver accurate supplier segmentation and has been proposed by Peter Kraljic in 1983. In the HBR article where it was published, he argued that supply item should be mapped against two key dimensions: risk and profitability.\nRisk relates to the likelihood of an unexpected event in the supply chains to disrupt operations, while profitability describes the impact of a supply item upon the bottom line. By putting these two dimensions together, you get a two by two matrix that looks like this:\nEach of these categories represents a different buyer-supplier relationship type and requires a distinct set of sourcing strategies.\n1. Non-critical items\nThese items are low-risk and have a low impact on your organization’s profitability. The most common example of non-critical items is office stationery. They are important for employees to perform their duties, but they don’t have a significant impact on your business, nor does their absence represent a serious threat.\nFor this type of items, e-auctioning and catalogs are the best way to redirect responsibilities either directly to suppliers or to internal customers that are requisitioning the goods.\n2. Leverage items\nFor items with high profitability and a low-risk factor, buyers possess the balance of power in the relationship with suppliers. Procurement professionals take advantage of this factor to drive lower prices, and suppliers can be easily substituted as their offerings are almost the same.\n3. Bottleneck items\nOn the flip side, there are items with high risk but low profitability. Here, the strength is in the hands of suppliers as they can drive prices upwards.\nThe supplier relationship is demanding, even though they have a limited impact on an organization’s profitability, and procurement professionals found that these suppliers absorb more of buyers’ time compared to any other segment.\nTo solve this, organizations can employ innovative internal activities to redevelop product requirements so that it can be replaced with another and preferably sourced from a leverage supplier.\n4. Strategic items\nLastly, high supplier risk and high-profit impact items cover strategic suppliers, the most critical to your business.\nManaging strategic suppliers requires a diverse array of skills and can subsume a significant proportion of executive time in sponsoring and directing the relationship. In the relationship with strategic partners, you can expect long-term commitment as well as proactive development.\nWhichever method you choose, the result should allow you to select the appropriate level of engagement with suppliers. However, keep in mind that supplier segmentation should be performed at least once a year to ensure the best outcome. Should one of your key suppliers fail to fulfill its offering, this could cause you to reputational damage and even force you to shut down business operations."", 'Supply Chain Risk Management\nWhat is Supply Chain Risk Management?\nSupply Chain Risk Management Definition: Supply chain risk management (SCRM) involves monitoring for and identifying potential threats to the supply chain. Although there is no way to completely eliminate risk, many issues can be avoided and contingency plans can be put into place to minimize or mitigate the impact on operations and/or profitability.\nSupply Chain Risk Management Examples: Generally speaking, there are two types of risk associated with supply chains. There are things that are impossible to predict and plan for, such as natural disasters, epidemics, and acts of terrorism, and there are other things that can be planned for, provided an organization has the data necessary to observe and act on early indicators. This may include:\nThe Role of Software Risk Management in Supply Chain Reliability\nBecause there are many potential issues that can impact the supply chain, executives have the difficult duty of assessing which problems are likely to cause the most harm to operations and profitability. Although cyber security is a core focus, there are numerous ways software can impact the reliability and profitability of a supply chain and not all of them are apparent, even to those in IT. While there is some risk involved in the hacking or insertion of malicious code in software, most issues surround unintentional coding errors, which routinely go undetected. As the infrastructure and architecture grow, these errors can create vulnerabilities and cause unexpected conflicts. By identifying these issues in advance and taking steps to reduce their impact, the risk is effectively managed prior to breakdown of the chain. Bear in mind, these errors can occur at a local level as well as at any stage in the supply chain, which is why it’s important for organizations to work with vendors and companies that have systems and transparency in place.\nSupply Chain Risk Management Best Practices\nAutomation: Supplier risk management (SRM) processes, including the collection, management, and analysis of data, should be automated. Human eyes cannot always detect errors, let alone conflicting codes within a system.\nSupplier Transparency: Performance information from suppliers should be readily available and included in analysis.\nEarly Detection: The supply chain risk management process should make use of automation software detect potential red flags before they become a problem.\nClassification: All potential risks should be identified, classified, and prioritized in order to determine which corrective actions offer the greatest ROI.\nPlans & Contingency Plans: A supply chain risk management plan should be developed and include methods for managing imminent risks as well as for minimizing the impact of issues that cannot be prevented or would be cost-prohibitive to prevent.\nMonitoring: Supply chain risk management tools should be used to identify new triggers, even after an assessment and corrections have been carried out.\nCollaboration: SCRM metrics and information should be available to various departments and heads at the same time for easy collaboration regarding supply chain risk management strategies.\nLeadership: SCRM should be overseen by one leader, even if duties are delegated to a team. Historically, organizations that assign a lead, usually an executive or VP, to handle SCRM see the greatest ROI. These organizations are also the only ones to see in excess of 100% ROI on their SCRM efforts.']"	['<urn:uuid:f3119bdc-9c42-48ef-a0e9-837bfd6bb5bb>', '<urn:uuid:08a27c68-0dd9-4293-992b-f4e82c905629>']	factoid	with-premise	concise-and-natural	similar-to-document	multi-aspect	expert	2025-05-12T20:11:36.665104	17	58	1817
4	What are the main advantages of having a construction manager at risk compared to the traditional design-bid-build method for construction projects?	The construction manager at risk method has two main advantages: it allows the construction manager to provide expertise early in the design phase and maintain primary accountability throughout the building process. This early involvement enables informed decisions about project design related to budget, helps avoid over-budget issues that could cause delays, and minimizes design rework since the construction manager provides input early and frequently.	['For most business owners, interviewing contractors for new facility construction conjures up images of a fast-talking auctioneer, in this case angling for the lowest bid.\nLegend has it that as the much anticipated Mercury 7 awaited launch on Feb. 20, 1962, just before delivering the famed line “Godspeed, John Glenn,” a NASA engineer reminded the iconic astronaut, “Remember, John, this was built by the low bidder.”\nChoosing a contractor for your commercial project is much more than selecting the low bid, or what is commonly referred to as the traditional design-bid-build method. Recent trends involve a focus on “alternative delivery methods,” a blending of tactics owners may employ to gain efficiencies throughout the project.\nFor years alternative methods of project delivery have been reserved for private industry. Recent legislation has been enacted to allow for the use of alternative delivery methods by public entities.\nThe most common alternative delivery methods are:\n• Construction manager at risk, where the construction manager holds all of the subcontracts and risk while providing the owner with a guaranteed maximum price.\n• Construction management agency, when the construction management agency acts as the agent for the owner with no risk or contractual obligation other than the construction manager contract.\n• Design/build, when the contractor also serves as the designer on the project.\nOf these three methods, the construction manager at risk method has become very popular, its primary advantages being that it allows the construction manager to provide expertise early in the design phase and hold primary accountability throughout the entire building process. When interviewing construction managers, a key attribute to consider is the company’s ability to problem-solve with a diplomatic touch. A worthy construction manager is a bridge-builder who works well with a variety of personalities. Serving as the team lead on the project, the construction manager must be a strong communicator with excellent negotiating and organization skills.\nConstruction managers should provide valuable knowledge concerning market conditions, and construction means and methods to the project team. Having this information early in the design phase enables the team to make informed decisions about the project design as it relates to the budget. Furthermore, as the project proceeds through design, the construction manager will keep the team abreast of the overall construction cost. This is an advantage construction management has over the traditional design-bid-build method. A construction manager avoids over-budget issues that could cause delays. In addition, rework of design is minimized because the construction manager is giving input early and often.\nUpon completion of the design, the construction manager develops tailored work scopes for each subcontractor phase of the job to ensure apples-to-apples bidding for the subcontractors. The clearly defined bid instruction enhances subcontractor participation by leveling the playing field, thus increasing competition and providing the most competitive bid environment for the owner. The construction manager markets the project to the construction community, gathers the bids and hires the best subcontractors for the job.\nA construction manager provides business owners with service beyond what a low-bid contract provides. Throughout the entire process, the construction manager serves as the key point of contact and is charged with keeping the project on time and on budget. The construction manager builds relationships focused on collaboration among all parties of a project team – owner, architect and contractor – turning the owners’ building experience from dreadful to enjoyable.']	['<urn:uuid:f9eb954b-c01a-450d-8275-31146b2c33e5>']	factoid	direct	verbose-and-natural	similar-to-document	single-doc	novice	2025-05-12T20:11:36.665104	21	64	559
5	thrombophilia testing infertility routine evaluation benefit	Thrombophilia testing is not recommended for routine infertility evaluation. Studies have shown no association between thrombophilias (like Factor V Leiden or Prothrombin gene mutations) and assisted reproduction failure or infertility. Additionally, thrombophilia testing doesn't predict who will benefit from Low Molecular Weight Heparin treatment.	['SUMMARY: CHOOSING WISELY® is a quality improvement initiative led by the American Board of Internal Medicine Foundation in collaboration with leading medical societies in the United States such as the American Society of Hematology (ASH). This organization was established to improve quality of medical care, after it was noted that about 25% of the tests ordered at the time of hospital admission and 65% of the tests ordered on subsequent days were avoidable. Further, there is ample evidence to suggest that, reducing unneeded investigations can decrease costs, increase patient satisfaction and quality of care. CHOOSING WISELY® has challenged 70 medical societies to identify 5 tests, procedures or treatments, within each specialty’s clinical domain, that are offered to patients, despite the lack of evidence demonstrating its benefit. The goal is to make positive changes in the actual delivery of patient care without harming the patient. The ASH CHOOSING WISELY® Task Force comprised of 13 individuals, represents a broad spectrum of hematologic expertise including malignant, benign, adult, and pediatric specialists. The five final recommendations of the 2015 ASH Choosing Wisely Campaign is an addition to the 10 prior recommendations made by ASH over the past 2 years. These top 5 recommendations were presented on December 7, 2015, at the 57th annual meeting of ASH, in Orlando, Florida. Practicing hematologists should give due consideration to these recommendations which are evidence based and cost effective.\nDon’t image for suspected Pulmonary Embolism (PE) without moderate or high pre-test probability of PE\nThe American College of Radiology has recommended that assessment of the risk-benefit ratio is important especially with pulmonary embolism and imaging can be avoided for suspected PE, without moderate to high pre-test probability.\nDon’t routinely order thrombophilia testing on patients undergoing a routine infertility evaluation\nWith Nearly 15% couples of patients receiving an infertility evaluation, the American Society for Reproductive Medicine has recommended that even though several population-based studies have found association of infertility or failure of assisted reproduction with thrombophilia, 2 large cohort studies have shown no association between thrombophilias such as Factor V Leiden or Prothrombin gene mutations and assisted reproduction failure or infertility. Further, thrombophilia is not a predictor of who will benefit from Low Molecular Weight Heparin (LMWH) treatment with respect to assisted reproduction and LMWH can be associated with adverse events.\nDon’t perform repetitive Complete Blood Count (CBC) and chemistry testing in the face of clinical and lab stability\nThe Society for Hospital Medicine and Adult Hospital Medicine noted that ordering routine complete blood counts (CBCs) during hospitalization is common practice and is unnecessary. Critically ill patients do not have the bone marrow reserve or erythropoietin stimulus to compensate for iatrogenic blood loss. Reducing the frequency of CBC’s does not result in inferior outcomes and several studies have shown that there is no difference in readmission rates, length of hospital stay and rates of adverse events. In addition to the risks of phlebotomy, this practice is economically disadvantageous, as they may not be reimbursable and will be an additional avoidable cost to dispose the biohazard waste of the blood samples.\nDon’t transfuse red blood cells for iron deficiency without hemodynamic instability\nThe American Association of Blood Banks has recommended against PRBC transfusions for patients with hemodynamically stable iron deficiency anemia. These patients when evaluated in the Emergency Department (ED) can be prescribed oral or IV iron with similar responses noted at 6-8 weeks. The compliance rate in those receiving oral iron may only be 50% due to GI side effects. Therefore parenteral iron may be a better treatment option for certain groups of patients seen in the ED.\nAvoid using positron emission tomography (PET) or PET-CT scanning as part of routine follow-up care to monitor for a cancer recurrence in asymptomatic patients who have finished initial treatment to eliminate the cancer unless there is high-level evidence that such imaging will change the outcome\nProfessional organizations like ASCO, ESMO and NCCN do not include surveillance PET in disease-specific guidelines because, routine use of intensive surveillance does not improve survival or enhance quality of life. Besides cost implications, CT scans may in fact expose patients to small doses of radiation.\nThe ASH Choosing Wisely® Campaign: Top 5 Non-ASH Choosing Wisely® Recommendations of Relevance to Hematology. Presented on December 7, 2015, at the 57th annual meeting of ASH, in Orlando, Florida.']	['<urn:uuid:74643bbe-9535-41dc-9f5e-9e5d452f8a81>']	factoid	direct	short-search-query	similar-to-document	single-doc	expert	2025-05-12T20:11:36.665104	6	44	719
6	what solutions and improvements proposed for making aquaculture feed more sustainable environmental impact	Solutions for more sustainable aquaculture feed include using high protein soy feed instead of fish meal to achieve better fish-in to fish-out ratios. Additionally, main strategies involve improving Feed Conversion Ratio (FCR), modifying feed composition, implementing best management practices, and developing innovative farming systems like Recirculating Aquaculture Systems (RAS) and Integrated Multi-trophic Aquaculture (IMTA) to improve waste treatment and recovery.	['There’s been a closely watched experiment floating and bobbing in the eddies off the Big Island of Hawaii. Since July, an unanchored pen stocked with 2,000 hatchery-born fish known as kampachi (related to the more familiar yellowtail) has been drifting in the open ocean, tended by marine biologists from the aquaculture company Kampachi Farms. Led by industry pioneer Neil Sims, it’s been dubbed the Velella Project, and it is the first and most important attempt at commercializing offshore aquaculture in the U.S.\nMost of today’s marine fish farming takes place close to shore, but many in the industry believe that in order to expand, they need to look further out to the open ocean. And they’re not alone. Aquaculturists in countries like Norway, Ireland, Canada, and Chile are also beginning to explore offshore options, though the technology to accomplish this remains in its infancy.\nCritics of aquaculture often point to problems of pollution, inefficient feed ratios, particularly for carnivorous fish, and worries of escapement. But in the case of the Velella Project, those concerns seemingly have been addressed.\nTo hold the fish, Sims used a specially designed structure called the Aquapod. Its unusual sphere-shaped design helps reduce fish escapes — called “leakage” by the industry — and can withstand tough ocean conditions. The kampachi are native to Hawaiian waters, and travel in schools by nature. If fish escape, they tend to cluster near the pen, and avoid the open blue water. A native species also eliminates the risk of cross-breeding.\nWhile the project hasn’t been without its difficulties — two early trial pens were lost, and unexpected La Niña conditions have made predicting and tracking eddy patterns difficult — initial results look promising. That’s good news for those in the aquaculture industry who are hoping to overcome the technical and regulatory challenges of expanding into federal waters, 3-200 miles from shore.\n“The Velella beta test showed us that the biological performance of fish in a drifter cage system is astonishing,” says Sims.\nIndeed, the fish used in the Velella Project grew to harvest size in half the time; survival rate inside the Aquapod was high; and because Sims used a high protein soy feed, instead of one that relies heavily on fish meal, the ratio of fish in to fish out reached a more sustainable level than many other more traditional farm-raised fish. As for pollution concerns, the pen was located in ocean water up to 12,000 feet deep, and according to Sims, there was no measurable impact on water quality, coral reefs, or marine mammals — concerns that have had some environmental groups deeply worried.\nSo why exactly does this seemingly futuristic project matter to eaters? By 2015, the world will be consuming more farm-raised fish than wild caught. According to a report released last summer by the WorldFish Center and Conservation International, aquaculture may be one of the most efficient methods of producing protein for the world’s burgeoning population, with potentially less environmental impacts than cattle, poultry, or pigs.\nBut access to coastal sites for fish farming has its limitations. In order to meet some of that growth, we must either bring aquaculture onto land, or overcome the challenges of pushing it further out into federal waters. What you’re witnessing in the Velella Project is the first substantial push into deeper U.S. waters.\n“If we’re going to feed the world protein, aquaculture is the best way to do it, and someplace we ought to be looking is offshore,” says Michael Rubino, director of NOAA Fisheries’ Aquaculture Program.\nThat may be easier said than done. In addition to challenges like nearly constant battering by ocean waves, high fuel costs for ships that maintain the pens (in the case of the Velella Project, a manned boat remained tethered to the pod at all times, to ensure it doesn’t float too far off course, and to house the staff monitoring the project), and an array of other technical obstacles, there’s the fact that no regulations have been put in place.\nRubino would also like to see a set of laws that are specific to open water aquaculture. “Under current fisheries laws, aquaculture has been interpreted as fishing. We need new legislation,” he says.\nNOAA is interested in the progress being made by the Velella Project, but stresses the importance of gradual, step-by-step development of this technology. “If we can solve these regulatory issues, we’ll issue a certain number of permits, and if those work, then in the second 10 years, we’ll issue more. [NOAA] has a stewardship mission. This has to be done in the context of healthy oceans,” says Rubino.\nThe other way that offshore aquaculture might expand, says Rubino, is if each regional fisheries management council (there are eight around the country) devises its own management plan for aquaculture. That’s something the Gulf of Mexico Council [PDF] has already done, putting the area first in line for commercial offshore aquaculture.\nChris Mann, director of Pew Environment Group’s Aquaculture Standards Project, says that while the Velella Project does seem to have good results so far, the conversation really needs to be about scale.\n“We appreciate someone like Neil thinking about aquaculture in a different way. What we’re not crazy about is taking the CAFO model of livestock and putting it in the ocean. Fish poop in the water; if you have a massive industry, you get a lot of pollution and problems. If you have small scale, [like the Velella Project] with a lot of water flowing through it, you have fewer problems,” says Mann.\nSims says he recognizes the need for scale, and the fact that offshore aquaculture means making use of a common resource.\nHe’s recently applied for permits for the next step, Velella Gamma, which will involve mooring the pod to a single point on the ocean floor and may require less staff monitoring. This stage will also involve placing the pod closer, into water depths of 6,000 feet. And the goal will be to move toward more automation. “Machines can work better in rough sea than people,” he adds.\nIn the end, economics will likely dictate whether or not offshore aquaculture truly takes off in the next decade.\n“The great promise of aquaculture – the ‘Blue Revolution’ – is that it can produce a healthy and abundant supply of protein for a world that’s going to need a lot more of it, but you can only do that if you make the right choices of species,” says Mann. He worries that producers of larger fish are more motivated by the marketplace, than by a drive to feed the world sustainably.\n“You feed the world with shellfish and tilapia,” he says. “Not salmon or shrimp for $15 a pound.”\nEither way, Sims will continue to push the envelope. As he sees it, there’s been a lot of fear-mongering about aquaculture. And, like all food production, he stresses the how in the equation.\n“There’s a ground swell of recognition that this is a direction we have to go,” he says. “Let’s just do it right.”', 'The MedAID project (Mediterranean Aquaculture Integrated Development) aims to increase the competitiveness and sustainability of the Mediterranean marine fish aquaculture sector throughout its value chain, by improving its technical productivity and economic performance with a market- and consumer-oriented approach, as well as achieving higher social and environmental acceptability and better governance (Aguilera et al., 2019). The development of aquaculture will necessarily involve an increase in the spaces devoted to this activity, due to the expansion of existing businesses and/or the creation of new ones. Conflicts pertaining to the use of marine space and the implementation of existing policies and legislation are two of the main factors hindering aquaculture growth (Galparsoro et al., 2020). Marine spatial management must be improved to facilitate site selection processes, alongside the establishment of transparent procedures and licencing processes, thus reducing the length of time and investment needed to develop new aquaculture activities. In addition, the increase in production will generate a proportional increase in the amounts of feed, which are often produced outside the countries concerned. If aquaculture is to double its production by 2030, the sector must improve its productivity, without compromising environmental performance (Lotze et al., 2019). Aquaculture can affect ecosystems (socially, economically and environmentally) positively or negatively, and aquaculture can be impacted by other human activities. Environmental impacts vary greatly depending on the type of farming in question (inland open flow, RAS, cages in protected areas or offshore) and husbandry practices (species, stocking density, feed composition, etc …). As for marine fish, whereas fry production takes place in inland hatcheries, as well as many pre-ongrowing farms, most ongrowing production takes place in sea cages, where the carrying capacity of the surrounding environment (hydrodynamic circulation, sediment characteristics etc…) is a critical constraint. To ensure sustainable development, ecological carrying capacity should be considered and environmental impacts of aquaculture should be minimized by either improving farm management or production systems, site selection, etc… Furthermore, all other uses of water and natural resources must also contribute to ensuring a sustainable ecosystem. The objective of this report (D8.5) is to review inputs and recommendations from international organizations and recent EU projects on environmental impact assessments, environmental monitoring procedures, as well as to discuss technical solutions to reduce the environmental impacts of Mediterranean fish farming and promote environmentally sustainable development.\nAfter a brief introduction on Mediterranean fish farm production based on MedAID results, the “Ecosystem Approach to Aquaculture” (EAA) framework and general aquaculture constraints at different spatial scales (farm, waterbody and regional scales) are described. We then focus on the key steps of the EAA: Marine Spatial Planning (MSP), Site Selection, Environmental Impact Assessment (EIA) and the Environmental Monitoring Procedure (EMP) before explaining how recent EU projects have developed tools and methods to facilitate these different steps. We reviewed decision support tools and methods tested and developed to facilitate spatial planning ( AQUASPACE), site selection and licencing procedures (TAPAS). We have then listed key indicators selected by stakeholders for Environmental Impact Assessments (EIA) and Environmental Monitoring Procedures (EMP) during different projects (including Indam and PerformFISH).\nFinally, the local (eutrophication) and global (use of fishery resources, carbon footprint) environmental impacts related to feed and fish faeces are discussed. The main strategies and recommendations for minimizing the impact of feed are: improving feed use through improvement of FCR, feed composition or best management practices, implementing innovative farming systems such as recirculating aquaculture systems (RAS) and Integrated Multi-trophic Aquaculture (IMTA) to improve the treatment and recovery of waste. Finally, prevention of fish escapees is reviewed (Prevent-Escape project).\nAccess to the full deliverable']	['<urn:uuid:d7eeacf9-46a4-4cae-be6f-5345cea6ef8d>', '<urn:uuid:205686ff-ae0e-4b7c-ab7a-853ec0a4487d>']	factoid	direct	long-search-query	similar-to-document	multi-aspect	novice	2025-05-12T20:11:36.665104	13	60	1768
7	bee pollination impact food production worldwide	Over 75% of Earth's plants use flowers to reproduce, with 80% of flowering plants requiring animal pollination, primarily by bees. This impacts human food production significantly, as honey bees and other pollinators contribute to crop yields, affecting fruits like raspberries and strawberries. Drone flies and other pollinating insects also play crucial roles in pollinating key crops including coffee, chocolate, tea, bananas, and mangoes.	['Drone Flies: flies in (really good) bee costumes\nBy Vicki Wojcik, Pollinator Partnership\nThe diverse group of flower flies and hover flies (family Syrphidae) includes many successful bee mimics. Drone flies (members of the genera Eristalis) masquerade as bees with various body forms and striping patterns that are almost perfect matches to many common bee species. Often very effective pollinators due to their hairy bodies, flies have keystone roles in many of ecosystems where they occur. Flies are also the dominant (and in some cases only) pollinators of key crops and foods like coffee, chocolate, tea, bananas, and mangoes.\nSo why would a fly want to be a bee? While they are surely happy in their species roles, pollinating beautiful wildflowers in gardens, meadows, and wildlands, and providing us with wonderful fruits and kitchen staples, there is one thing flies cannot do that bees can – sting!\nA sting is a wonderful thing if you are small, soft, and tasty to the many birds, lizards, frogs, and small mammals that you share your ecosystems. If you do not have a sting to keep these predators at bay, you have to be a bit more creative. Outsmarting your adversary might just work. In fact, so many unarmored and undefended species use trickery to stay out of harm’s way scientists have given this system a name: Batesian mimicry. Famous English naturalist Henry Walter Bates came upon this concept during his work in the Brazilian Amazon where he observed numerous non-toxic butterflies that looked identical to a few very potent types.\nMimics take advantage of other species’ reputations as dangerous and difficult to swallow. Getting stung will surly make you think twice about eating a bee; and flies that look like bees can get a free pass. Drone flies have taken Batesian mimicry to the next level.\nEristalis tenax, the common drone fly, can even fool trained scientists when it flies by. Not only does this species look like the honeybee, it has changed its behavior to fly more like a bee by moving back and forth between flowers rather than hovering in place.\nMimicking is a strategy that works well, and many other flies use this survival strategy. Some bee mimics look like leaf cutter bees in the genus Megachile; Eristalis dimidiata is a great example. Some species, like Eristalis flavipes look like bumblebees, complete with fuzzy abdomens and thoraxes.\nFlies in the genus Helophilus mimic another hymenopteran group with a stinger, the wasps. The most wasp-like of these are in the genus Helophilus, mimicking the more painful stings of the Vespidae. Helophilus fasciatus copies the colors and patterns of a common yellow jacket, complete with longitudinal bands of yellow and black on the thorax, and transverse stripes on the abdomen. Any predator that has had a previous Vespula spp. encounter surely will not approach this fly!\nDrone flies, and other bee and wasp mimics, visit flowers in search of nectar to fuel their flight. They are hungry for pollen, especially the females. Like most flies, they have sucking mouthparts that only enable them to drink fluids, but they can absorb pollen grains along with the nectar. Their stomach juices can dissolve some of the outer coating of the pollen and release the nutritious proteins inside. It is known that females need these extra proteins to make eggs. Drone flies are frequent visitors to a great variety of flowers, and since they have several generations each year, they continue visiting flowers through the seasons from early spring to late fall, switching from one type of bloom to another as flowers come and go.\nFor Additional Information\n- Benestad Hågvar. Effectiveness of larvae of Syrphus ribesii and S. Corollae [Diptera: Syrphidae] as predators on Myzus persicae [Homoptera: Aphididae].\n- Bug of the Month: Boston Harbor Islands\n- Images: Syrphid Flies - tribe Eristalini. (1999-2000).\n- Y. C. Golding and M. Edmunds (2000). Behavioural mimicry of honeybees (Apis mellifera) by droneflies (Diptera: Syrphidae: Eristalis spp.). Proc. R. Soc. Lond. B 267, 903-909: doi: 10.1098/rspb.2000.1088', 'Beewatch helps the honey bees\nBeewatch promotes sustainable strategies to help the bees and the environment in the long run.\nHome for honey bee colonies in urban areas where bees lost their habitat\nLocal colonies of honey bees to help build up genetic resistance\nNo chemical treatment of the colony. Ever.\nNo over-exploitation of the bees for their honey or their pollination skills.\nWatching the bees ultimately leads to watching out for the bees.\nBees and pollination\nBees are key to plant and animal survival.\nNo matter where you live in the world, if you walk around in nature, you will see flowers. They come in different shapes, different colors, different blossom, different size. But most of them can be seen as a banner that says “I NEED A POLLINATOR TO REPRODUCE”. And the vast majority of pollinators are bees.\nOver 3/4 of all plants on Earth (350,000 species) use flowers to reproduce. 80 % of those flowering plants use animal for pollination, including birds and mammals, but mostly insects. Bees are the top insect pollinator.\nThe disappearance of pollinators would negatively impact over 65 % of the plants on Earth. This impact would travel up the trophic pyramid and successively affect the herbivores, the carnivores and the omnivores, including us, human beings.\nPollination is how flowering plants reproduce.\nA flower is where male and/or female parts of a plant are displayed. Pollination is the male part (pollen) getting transferred to the female part (stigma) which leads to fertilization and ultimately to seed production. Waiting for this transfer to randomly happen is not a very efficient reproduction pathway. It is of interest for the plant to have this transfer facilitated, and the best way to do so is to recruit helpers: the pollinators. Bees are one of the most efficient pollinators on Earth. When a bee visits a flower, her body gets covered with pollen grain that she carries away to the next flower, hence transferring pollen grains from flower to flower.\nA sweet deal between plants and bees\nBees don’t transport pollen around just to make plants happy. They do it because they get an important reward doing the job: they get FOOD. To reward the pollinators for their important role, plants produce a sweet liquid called NECTAR at the basis of most flowers. Nectar – together with pollen – is the main source of nutriment for bees. Looking for nectar in flowers, the bee gets covered with pollen and helps the plants by transferring the pollen grains to the next visited flower. This is a beautiful example of SYMBIOSIS: plants are helping the bees, and bees are helping the plants. For 130 million years, bees and plants have co-evolved on Earth and are now closely connected.\n… IN NUMBERS …\nof the total volume of food consumed by humans on the planet depend on pollinators\nof the major varieties of crops consumed by humans depend on animal pollination\nin value is added by the honey bees to agricultural crop each year in the United States\nHoney bees are important to us\nIs the honey bee “just” one of those pollinators?\nYes and no. There are about 20,000 species of bees on Earth. All of them play an important role in pollination. Only 7 of those species are honey bees… But those 7 species are very, very important to us, human beings, more than any other pollinators.\nThe honey bees are among the top beneficial insects\nHuman beings have been keeping bees for thousands of years. The first signs of domestication of bees appeared in Egyptian art 4500 years ago. Human beings are very special animals for many different reasons, and one of them is because they are farmers and cultivate the fruits and vegetables they like to eat. The other reason is that there are a lot of human beings on Earth to be fed. Therefore, humans had to find helpers who would help reproduce the variety of fruits and vegetable they liked and in large amount, in order to be able to feed everyone. The honey bee is the best suited for that task…\n- Honey bees live in large colonies. Colonies are on average 60,000 individual, which represent a large work force to help with pollination.\n- Unlike other bees, they overwinter, which means that they don’t die off before winter but stay alive and relatively active instead and are very quick on the following spring to get back to work. Most of the other bee species would instead spend much of spring reestablishing their colony that died off in fall (only the fertilized queen survives winter).\n- They are transportable to allow pollination of different crops in different parts of the world each year, which has a huge impact on the global economy (several billions of dollar).\n- Honey bees make… honey! Even if honey is not the main reason why human beings keep bees nowadays, it is still an amazing treat that most people enjoy.\nAnimal food depends on pollinators, which INCLUDE honey bees.\nBut human food depends on pollinators, MAINLY honey bees.\nTaking care of the pollinators is essential to preserve our planet.\nAnd taking care of the honey bees is essential to take care of our species.\n… HOW INSECTS IMPROVE CROP YIELD …\nAlexandra-Maria Klein et al. 2007, Proceedings of Royal Society\nRaspberries, after NON INSECT pollination (left and middle) and INSECT pollination (right). (Photo by Jim Cane, Bee Research Institute, Longan, USA).\nStrawberries, after INSECT pollination (left) and NON-INSECT pollination (middle and right). (Photo by Kristine Krewenka, Agroecology, Göttingen, Germany).\nHoney bees are dying\nWhy are the honey bees dying?\nFor the past 10 years, beekeepers in North America have been reporting a loss of 20 to 90 % of their bee population every year. Why are the bees dying?\nThere are several major factors accounting for the bee disappearance.\n- Varroa Destructor Mites. Those parasites brought from Asia feed on bees’ blood and transmit viruses to the bee population. Infestation as low as 3 mites for 100 bees can already become a threat for a healthy colony. Varroa has now spread everywhere in the world except for a few isolated places like Australia.\n- Pesticides that are sprayed onto crops and seeds. One major class of dangerous chemical is the nicotinoids pesticides that were shown to be both attractive and destructive to honey bees and bumble bees. That class of pesticide has been banned in Europe.\n- Habitat loss brought about by development of our cities and monoculture agriculture.\n- Bees’ bad nutrition. Bees are moved around in North America and used to pollinate crops. The farms renting bee hives for the service of these pollinators generally practice monoculture, which means the bees have access to only one kind of pollen/nectar for a period as long as one month. This unbalances the bees’ diet and has been pointed to as a factor of weakness of their immune system .\n- When bee hives are transported across the continent, the bees are exposed to a tremendous amount of stress. If you ever witnessed the reaction of bees inside a hive when you knock on the wall of their house, you know how sensitive those creatures are to vibrations. Bees use vibrations as one of their favorite communication systems. It is easy to picture how a trip of thousands of miles on the back of a truck can stress the colony (before finally reaching a farm that practices monoculture…).\n- Lack of genetic variability.\nHelping the bees or treating them\nBecause Varroa mites seem to be the first reason why bees are dying, it is tempting to beekeepers to use chemical miticides to try to get rid of the mites and “save the bees”. But are those treatments really saving the bees? Is “saving the bees” making them dependent on a treatment for their survival? Miticides are not only a threat to the environment, but they also pollute the honey that we eat, and they contribute to the appearance of miticide resistant mites that are even more dangerous. And worst of all, they weaken the bees.\nHoney bees appeared on Earth some 130 million years ago. To become those beautiful and smart insects they are today, they had to develop defense mechanisms to survive the different challenges Nature brought to them over time. Those challenges were as diverse as predators like hornets or bears, nectar dearth, or pathogens like Bacteria, viruses and parasites. Some of those pathogens were most likely as aggressive – or maybe more aggressive- than Varroa mite is today. The bees survived each pathogen without any kind of treatment. They did what every living organism does: they ADAPTED. Natural selection worked its way and helped build STRONG BEES, resistant bees, that would thrive until recently.\nThen they started to die… and they are still dying.\nchemical treatment = short term threat\nPesticides are toxic for the environment and pollutes the honey we eat.\nPesticides also reduces the bee’s natural abilities to fight pathogens.\nno natural selection = long term threat\nThe honey bees are getting genetically weaker and weaker. So weak that one day they probably won’t be able to survive anymore without our “help”.\nIf you’re not part of the genetic solution of breeding mite-resistant bees, then you’re part of the problem.\nA New Bond With Nature']	['<urn:uuid:eb0963e1-139c-4105-b672-d5e73e318208>', '<urn:uuid:27fd7305-4fa7-4514-9eee-9fb8b0fe5749>']	factoid	direct	short-search-query	similar-to-document	three-doc	expert	2025-05-12T20:11:36.665104	6	63	2230
8	What is the key difference between Panamanian Hamentashen and Irish Soda Bread in terms of their leavening methods?	Panamanian Hamentashen uses no leavening agent and is more like a pastry dough made by combining flour with fat and minimal liquid, while Irish Soda Bread relies on the combination of baking soda and buttermilk as a leavening agent to make the bread rise during baking.	"['The following recipe epitomizes the transformation of a Jewish recipe due to migration. I received the Panama WIZO cookbook from a friend of mine in Mexico City. The recipes were all in Spanish but my high school teacher would have been proud! I came across a recipe entitled Orejas de Haman para Purim (Haman’s Ear’s for Purim). But instead of the aforementioned recipe for fried dough, the ingredients and diagram were for Hamantashen. To blur the lines of transition even further, many of the ingredients (including the brandy) were more typical of the Middle Eastern fried dough than the Eastern European pastry Murbeteig or Pate Sucree.\nThere is a large Sephardi population from Syria and Lebanon in Mexico and of course there is a substantial Ashkenazi community as well. Their traditions were co-mingled probably through shared celebrations to produce the following recipe. Add me to the mix with my modern baking techniques and equipment, not to mention the trick of rolling in confectioner’s sugar and you have a further metamorphosis of our culinary culture.\nEnjoy this recipe for its taste as well as for its history. By the way, the author of the original recipe in the book\nHAMENTASHEN DE PANAMA\n3 ¼ cups flour\n½ cup sugar\n½ teaspoon salt\nZest of one small lemon\n1 stick margarine, cut into eighths\n1 stick unsalted butter, cut into eighths\n1 egg yolk\n1 ½ teaspoons vanilla\n2 or more Tablespoons of Brandy or Rum\nCommercially prepared poppy seed, prune or apricot filling\n1. Place the flour, sugar, salt and lemon zest in the work bowl of a food processor fitted with the metal blade. Pulse the machine on and off to combine the ingredients.\n2. Add the margarine and butter and pulse on and off about 20 times or until the dough resembles a coarse meal.\n3. Quickly combine the yolk, vanilla and brandy in a small bowl.\n4. Immediately add the liquid mixture to the processor while it is running and mix only until a ball of dough starts to begin to form. Do not over mix. If the dough looks very dry you may add another Tablespoon of Brandy or some milk. Dough should not be too moist or cookie will be heavy.\n5. Turn the dough out on a lightly floured board and lightly knead the dough into a ball. Divide the dough into two or three portions and refrigerate, covered, for 20 minutes.\n6. Remove the dough and roll out to 1/8th inch thickness on a surface that has been liberally coated with confectioner’s sugar.\n7. Cut dough into 3 inch circles and then place a small amount of prepared filling in the center of each circle. Shape dough into triangles pinching the edges together.\n8. Place cookies on parchment-lined cookie sheets and bake for 12-15 minutes in a preheated 350’F oven until golden brown.\n• Pastry that contains alcohol or fruit juice will taste even better the next day as the flavors need time to mellow.\n• Liquid is necessary, even in small amounts to bind the flour and fat in pastry together. A processor is so efficient that dough could be formed but it will fall apart when rolled or baked without the additional liquid.\n• Always roll sweet pastry in confectioner’s sugar instead of flour. The cornstarch in the sugar prevents sticking and the sugar creates a light, glistening glaze over the finished product.', ""Baking bread has never been easier! The dough can be made in a matter of minutes and thrown in the oven. The outside crust is crumbly and delicious and the inside is moist and buttery. So good!\nI love to bake bread including everything from artisan style bread, to sandwich loaves, biscuits and scones. This Irish soda bread is one of the quickest and easiest loaves to bake. It is perfect for a weeknight dinner and great alongside soup or stew.\nSoda bread is amazing because it is so quick to mix together and it does not require any yeast or rising/proofing time. The combination of baking soda with buttermilk act as the leavening agent to cause the bread to rise during baking.\nThis recipe is not the traditional 4 ingredient soda bread. Classically, this bread is made only with flour, salt, baking soda and buttermilk. While it is great that way, I like to add some butter and a little bit of sugar. This creates a soda bread that is in between, a bread and a scone, in texture and flavour. I also love to mix in some currants because, well, I look for any opportunity to add dried fruit to a recipe.\nOther Irish Inspired Recipes!\nThis Irish soda bread is a great way to celebrate March 17th. Here are some other fun and delicious recipes for St. Patrick’s Day.\nGet the Ingredients Out\nI like to make sure that all the ingredients are out and pre-measured. Making the dough for soda bread is very quick, so it is best to do the measuring first.\nWith regards to the butter, I cut it into small little cubes and place them in the fridge so that they are cold when ‘cut’ into the flour.\nWhisk the flour, sugar, salt and baking soda in a large bowl.\nHow to Make the Dough?\n- Once the dry ingredients have been mixed together in a large bowl, it is time to cut in the cold butter. Use a pastry cutter or a couple of knives to break up the butter into small little bits. Cut the butter, until the flour looks and resembles a coarse meal or crumb.\n- Stir in the currants. This is an optional ingredient and this bread is just as delicious without the currants. Other dried fruits, orange/lemon zest and herbs can be added to enhance the flavour of the bread.\n- Make a well in the middle of the flour mixture and pour in the cold buttermilk.\n- Use your hand, a rubber spatula, or a wooden spoon to mix the buttermilk into the dry ingredients. Be careful not to over-mix, but, you do want to make sure there are no dry pockets of flour.\nTime to Knead Gently\nThis soda bread only needs a very light kneading, which is different than a traditional yeast based bread. I knead it right in the bowl, so that I do not need to dirty up another surface.\nUse your hands to gently knead this dough. It will be fairly wet and tacky. That is okay. If you find that it seems too wet and constantly sticking to your hands, you could add a little extra flour and work it into the dough.\nOnce the kneading is done, place the dough onto a pizza stone or baking sheet. I like to line it with a sheet of parchment paper so that the bread does not stick to the baking surface. Form the dough into a round loaf that is about 7 inches in diameter.\nScore the Top!\nBefore the Irish soda bread goes in the oven, it is important to cut the top of the bread. Use a sharp knife to slice into the top of the bread, marking an X. Make sure the cut is deeper than you might think. I try to have the cross about 1/2 inch deep.\nThis slit helps the bread rise and bake evenly and prevents it from cracking.\nHow Long to Bake?\nI bake the soda bread in a 375F oven for about 55-60 minutes. The bread will rise and puff slightly during baking and the outside will brown up to develop a craggy crust. It looks rustic and delicious.\nI use a digital thermometer to test the doneness for bread. Stick it into the centre of the bread and if it reads 180F, then the bread is done. Take the loaf out of the oven and place it on a cooling rack.\nCool the bread completely before slicing. This bread it great just as it is, but I also love to spread a slice with some butter or honey.\nOther Bread and Biscuit Recipes!\nI am a huge fan of fresh baked good and there are a number of recipes on my site. Here are some of my favourites.\nIrish Soda Bread\n- 3 1/2 cups flour\n- 1/4 cup sugar\n- 1 tsp baking soda\n- 1 tsp salt\n- 1/2 cup cold butter , cut into small cubes\n- 1 2/3 cup buttermilk\n- 1 cup currants , optional\n- Preheat the oven to 375F.\n- In a bowl, whisk together the flour, sugar, baking soda and salt.\n- Add the cold butter to the bowl and using a pastry blender, cut the butter into the flour. Continue until a crumbly consistency is produced. Stir in the currants.\n- Pour the buttermilk into the flour mixture and stir together with your hands, or with a rubber spatula, until combined. Work the dough gently, giving it a very light 'knead', and be careful to not over-mix the dough.\n- Form the dough into a 7 inch round loaf and place on a pizza stone or baking sheet, lined with parchment paper.\n- Cut a cross (X mark) on the top of the dough. The slit should be about 1/2 inch deep into the loaf.\n- Bake for 55-60 minutes.\n- Let the bread cool completely before slicing.""]"	['<urn:uuid:81c28b75-c85e-4fa7-aaf8-2b49af056a74>', '<urn:uuid:61f9e9f8-6a4e-4368-a0c5-864bffd3e5ce>']	factoid	direct	verbose-and-natural	similar-to-document	comparison	expert	2025-05-12T20:11:36.665104	18	46	1567
9	yellowstone wolves eat dead animals how does this affect ravens nearby	The reintroduction of wolves to Yellowstone in 1995 has increased and stabilized raven populations in the park by providing a year-round food source through wolf kills. Interestingly, ravens rely more on wolf-provided carrion during mild winters, while preferring to use human-provided food sources (like dumps) during severe winters. This is different from what researchers expected. Wolf presence has been beneficial for ravens because they provide a consistent food source throughout the year, unlike human hunters who only provide seasonal kills.	['As 2018 draws to a close, I want to dedicate a post to five of the most interesting and important publications about our favorite family of birds that came out this year. For the sake of a brevity, the reported studies are largely condensed with some tests/results omitted and little attention to normally key experimental elements like controls, statistical analyses, etc. Please click on the study title to be directed to the full publication.\n1. Townsed AK, Frett B, McGarvey A, and Taff CC. (2018). Where do winter crows go? Characterizing partial migration of American Crows with satellite telemetry, stable isotopes, and molecular markers. The Auk 135: 964-974\nBackground: Depending on where you live, the answer to, “Do crows migrate?,” can be quite different. For example, most Seattle residents would probably say no, since large numbers of crows can be seen here year round, while someone in say, a southern Canadian province, may notice a sharp decline in the number of crows during the winter. That’s because crows are what’s know as “partial migrant species” meaning that within a population, some individuals may be migratory and others resident with more migratory strategies biasing in areas with harsh winters. Despite the role of partial migration in how scientists currently explain the evolution of complete migration, little is known about the phenomenon. Even elemental questions such as: is this behavior fixed or flexible within individuals, is it environmentally influenced, and how might species use it to adapt to changing conditions remain under-explored.\nMethods: The study looked at two populations of overwintering crows: one in Ithaca, New York and a second in Davis, California. They used a combination of intrinsic (meaning originating in the body) and extrinsic (meaning originating outside the body) markers to track the movement and origin of their 18 tagged subjects over 2-4 years. The intrinsic makers included molecular and stable isotope data, and the extrinsic marker was a satellite tracking device that was attached to the bird via a light backpack. I won’t go into the details of the molecular and stable isotope data, but suffice it to say that stable isotopes were used to identify the place of origin via the unique properties of the local food and water that embed into an individual’s tissue and the molecular data was used to sex individuals and establish relatedness.\nKey findings: Of the 18 tagged crows across both east and west coast populations, they found that almost 78% were migratory. This was a shock to me, TBH. I had no idea just how many crow were making these annual trips. The distance these birds traveled varied widely, with some going as “little” as 280 km (173 miles) and others as much as 1095 km (680 miles). Among resident birds, they found that individuals never ventured further than 25 km (15.5 miles) from the center of their breeding site. For both resident and migratory individuals they found that birds were very loyal to their breeding sites; returning to the same territory year after year. Given this finding, it should not be surprising to learn that individuals did not vary from year to year in whether they were migratory or not. Together these results offer clues to how crows may respond to climate and urbanization induced changes in temperature to their local environments.\n2. von Bayern AMP, Danel S, Auersperg AMI, Mioduszewska B, and Kacelnik A. (2018). Compound tool construction by New Caledonian crows. Nature Scientific Reports 8\nBackground: For decades people considered the use of tools to be a uniquely human feature. Now we know that all sorts of animals, ranging from fish to monkeys, use tools and a handful of animals even create tools. Among the small number of animals that create tools, we have only seen wild individuals modifying a single object. For example, stripping a twig of small leaves or branches in order to probe small holes for insects. Whether any wild animal is capable of making compound tools, those made by combining seperate non-functional parts, is unknown. Even in captivity, this behavior only has limited observation in the great apes. Understanding what animals are capable of this complex task and how they achieve it, might give us insight into the evolution of our own exective functions.\nMethods: This study used eight wild caught captive New Caledonian crows. Like many experiments involving novel objects, this one occurred over multiple different phases. In phase I the birds were provided a long stick and a baited test box where food was within reach when using the stick, but not without it. In phase II the birds were presented with the same baited test box, except that instead of a single long stick, they were given a hollow cylinder and a second, thinner cylinder that needed to be combined in order to generate a tool long enough to reach the food. In phase III, the birds were given the same problem, only now with novel combinable items. In phase IV, the researchers tested whether the birds were combining elements because they understood that they needed to, or if because they derived some other benefit from the process. To do this, they presented birds with a bait box that had two tracks: one where the food was within reach of a single element and one where it required a compound element. In the final phase, birds were presented a bait box that required the combination of more than two elements.\nKey findings: All birds passed the initial tool use phase handily. Given that New Caledonian crows frequently use single element tools in the wild, this was not at all surprising. In the second phase, half of the subjects (four) were able to combine the two elements after no more than two failed attempts. These subjects were then able to transfer this knowledge when presented novel combinable objects. When given a bait box with food presented on the close and far tracks, birds most often only made compound tools when it was necessary, suggesting that they don’t do it just for fun. In the final phase, only one bird succeeded in making a tool that required more than two elements. These findings demonstrate that New Caledonian crows are not only on par with what’s know about compound tool use in the great apes, but actually exceed them.\nUnfortunately what this study does not explicitly answer is whether the birds were able to create the needed tools as a result of mental mapping (i.e imagining the correct tool and how it might be assembled) or by happy accident. Without this knowledge, what their ability to make compound tools suggests about the evolution of things like insight remains mysterious. Given all the other remarkable ways New Caledonian crows show innovation when it comes to tool use, however, both myself and the authors of this study are hedging that it’s indeed cognition behind these behaviors rather than more simple mechanisms.\n3. Boeckle M, Szipl G, and Bugnyar T. (2018). Raven food calls indicate sender’s age and sex. Frontiers in Zoology 15\nBackground: One of the most frequent inquiries that come my way are requests to decipher various crow calls. Given all we know about crows, this doesn’t seem like such an impossible request, but the reality is that crow communications remains one of the most impenetrable black boxes of crow behavior. I’ll save more on this for a future post dedicated to an upcoming publication by my colleague Loma Pendergraft, who spent his MS learning this fact the hard way. But suffice it to say that any progress on this front in the various Corvus species is groundbreaking news. We do, however, know more about raven calls. For example long “haa” calls are thought to recruit other individuals to sources of food. What was unknown at the start of this study was whether these calls encoded any class-specific information about the caller, such as their age or sex. Calls that impart class-level information about the caller have been previously demonstrated in some marmots and monkeys.\nMethods: The researchers recorded hundreds of “haa” calls from wild ravens which had previously been color banded and whose age and sex were known. Using acoustic software they analyzed the vocalizations for patterns in call elements like frequency and inflection rate.\nKey findings: As the study’s title suggests, ravens appear to encode information about their age and sex in “haa” food calls. For animals like ravens that live in “fission-fussion” social systems, meaning flexible social groups where individuals regularly reencounter familiar individuals, but also encounter unfamiliar ones, class-level information helps individuals quickly assess important aspects of a caller’s identity. Such information may be key to helping individuals decide if they want to join a feeding event or not. This decision is particularly important because aggression at feeding events can cause mortal injury, so grouping with a bad crowd can come at a high price.\n4. Kroner A, and Ha R. (2018). An update of the breeding population status of the critically endangered Mariana Crow (Corvus kubaryi) on Rota, Northern Mariana Islands 2013–2014. Bird Conservation International 28: 416-422\nBackground: The Mariana crow or Aga is a native species to the islands of Guam and Rota. After the introduction of the brown tree snake to Guam in the 1940’s, Guam’s entire population of Aga were wiped out leaving only those found on Rota to continue the species. In 1982, the population hovered around 1,300 individuals but things were clearly in decline. In 1984 the Aga was officially listed as endangered and today is considered critically endangered by the IUCN. Unlike on Guam, there is no clear reason why the Aga continues to decline on Rota, though habitat loss, persecution by humans, natural disasters and introduced predators like cats likely all work together.\nMethods: During 2013-2014 researchers counted breeding pairs by surveying all known island territories. During these counts (which took 845 hours of labor and traversed 1,485 hectares!) the researchers also documented any unpaired or subadult birds. Since the entire island could not be surveyed, to ultimately estimate the population size the researchers used models that accounted for missed detections.\nKey findings: Spoiler alert: They are A BUMMER. In all that searching only 46 breeding pairs were detected. Accounting for unpaired birds and detection failures, the researchers estimate that the current population of Aga hovers around 178 individuals. Obviously that number alone is a gut punch but it’s especially true when you consider that that’s a 10-23% decline since 2007 and a 46-53% decline since 1998. Researchers estimate that at least 75 pairs are needed to maintain a viable population of Aga. Without intensive predator management and community level advocacy for these birds, their future is sadly looking grimmer and grimmer.\n5. Walker LE, Marzluff JM, Metz MC, Wirsing AJ, Moskal ML, Stahler DR, and Smith DW. (2018). Population responses of common ravens to reintroduced wolves. Ecology and Evolution 8: 11158-11168\nBackground: One of the most persistent myths about common ravens is that they have a symbiotic relationship with grey wolves; intentionally showing them carcasses they find and then sharing in the bounty together. But while the case is actually that ravens are unwelcome dinner guests at the wolves’ table, there’s no question that the two species have profound effects on one another. The reintroduction of wolves to Yellowstone in 1995 therefore offers a valuable way to study how the presence of wolves affects the spatial distribution and feeding behaviors of park ravens.\nMethods: This study was a collaborative effort between avian and spatial ecologists at the University of Washington and Yellowstone wolf biologists. Using data from 2009-2017 on wolf abundance and prey kills, and raven surveys taken both within the interior of the park and at anthropogenic food sources in surrounding areas (ex: the Gardner town dump), the researchers were able to model raven abundance during both the study period and before the reintroduction of wolves. I won’t go into the details of how these models are created, but suffice it to say that their purpose is to take the data you give them and find what predictors best explain your observed outcomes. For example if, say, you have a bunch of data about where ravens were located at different times, and have data on different possible predictors, say, wolf abundance, weather, carcass abundance, carcass biomass, and distance to anthropogenic food, etc., the right model could help you identify that carcass biomass is the best predictor of raven abundance.\nKey findings: Previous studies have demonstrated that wolves make more kills during severe winters with higher snowpack, because prey have a more difficult time evading them. As a result, the researchers hypothesized that ravens would depend more heavily on wolf kills during severe winters, but this is not what they found. Instead, Yellowstone ravens seem to lean more on consistent, anthropogenic food sources during tough winters, but lean more on wolf provided carrion during more mild winters. Still, the presence of wolves has increased and stabilized the number of ravens in the park, because they provide a second year-round source of food, in contrast to human hunter provided kills which are seasonally limited. These findings are yet another demonstration of the value of top carnivores in stabilizing food webs and providing food for a cascade of creatures.\n6. And as a bonus let’s not forget the most important 2018 study of them all, “Occurrence and variability of tactile interactions between wild American crows and dead conspecifics,” which you can read all about here. 😉']	['<urn:uuid:e8152e00-06a0-4dd9-b00e-a437459307cf>']	open-ended	with-premise	long-search-query	distant-from-document	single-doc	novice	2025-05-12T20:11:36.665104	11	80	2251
10	cooking process take longer caramelizing onions royal court non spicy rice cake ddukbokgi	Caramelizing onions takes 45 minutes to 1 hour, requiring constant stirring every 5 minutes over medium heat. In contrast, cooking Royal Court non-spicy rice cake (Ddukbokgi) takes significantly less time - about 5 minutes on high heat for the rice cake with sauce, plus an additional few minutes until the liquid reduces.	"[""Royal Court Non-spicy Rice Cake, Goong-joong Ddukbokgi\nGoong-joong ddukbokgi is a non-spicy variation of ddukbokgi (Korean stir-fried rick cake). Goong-joong means “royal court” or “royal palace.” Dduk means “rice cake.” Bokgi means “stir-fry.” As the name suggests, this dish originated from Korean royal cuisine unlike today's more common spicy ddukbokgi which is the epitome of Korean street food.\nThis nutritious and flavorful snack dish is great for parties or any meal occasions. Kids love it too!\nYou can buy DdukBokGi ingredients online here.\n|1 lb 2 oz||Sticky Rice Cake Tube, Garaeddeok 가래떡|\n|4 oz||Beef, Brisket 앞다리 (any cut for stir-fry or steak, can use chicken or pork, omit for vegetarian version)|\n|½||Onion (Medium) 양파|\n|5 oz||Carrot (large Korean carrot) 당근 (1 large Korean carrot = about 300g)|\n|1 oz||Shiitake mushroom (dried) 표고버섯 (dried, increase the amount for vegetarian version, 1 dried shiitake mushroom = about 6 g)|\n|¼||Cucumber 오이 (optional. 1 large cucumber = about 300g)|\n|2 tbs||Vegetable Oil 식용유|\n|1¼ cups||Water 물|\n|3 tbs||Soy Sauce (regular) 왜간장|\n|2 tbs||Sugar 설탕 (can use honey instead)|\nmarinade for beef and mushrooms\n|1 tbs||Soy Sauce (regular) 왜간장|\n|1½ tbs||Sugar 설탕 (can use honey instead)|\n|1 tbs||Sesame Oil 참기름|\n|pinch||Black Pepper 후추|\n|½ tsp||Garlic (minced) 다진 마늘|\n|2 tbs||Chopped Green Onion 다진파|\n|1 tsp||Sesame Seeds 깨|\negg garnish (optional)\n|pinch||Salt 소금 (optional)|\n|2 tsp||Vegetable Oil 식용유|\n- 1 large sauce pan or wok\n- 1 small to medium non-stick pan (if making optional egg garnish)\n- a few bowls for soaking and marinating beef and mushrooms\nYou can buy ddukbokgi ingredients online here.\nOptional Ingredients and Substitutions\nSesame seeds are optional.\nShiitake mushroom has a very distinct flavor. You can use other types of mushrooms instead but the dish will somewhat lose the authentic Korean flavor.\nCucumber: can be omitted or replaced by red or green pepper.\nBeef: you can use any beef cut suitable for stir-fry or steak. Of course, the higher quality of the meat, the better it will taste and feel in your mouth. Traditionally this dish is made with beef but you can use chicken or pork. For chicken or pork, cut the meat thinkly and increase the marinating time for a few hours if possible.\nFor a vegetarian, simply replace beef with tofu cubes (firm tofu preferred) or more mushrooms. Shiitake mushrooms are best for this dish because of its rich and robust flavors. But, you can use other mushrooms you love.\nMore questions? Please leave your questions below in the comment section. We will do our best to answer as soon as we can.\nIngredient amounts in the recipe instructions are for the default serving size.\nClick to enlarge photos.\nIngredient amounts in the recipe summary are for the default serving size.\n1. Soak rice cake (optional)\nIf the rice cake is frozen, soak it in room temperature water for at least 30 minutes. Soaking is not necessary for fresh-made (soft) rice cake. Cut the rice cake into 4-5 inch lengths if not pre-cut.\nSoak rice cake (optional)\nCut into 4-5”\n2. Soak mushrooms\nSoak 5 dried shiitake mushrooms in 2 cups of cold water mixed with 1 tablespoon of sugar (optional) for 30 min or until soft. *Sugar quickens the soaking process.\nSoak 5 shiitake mushrooms\n3. Julienne beef\n4. Prepare garlic & green onions\nMince 1 clove of garlic and finely chop 1 green onion to be used in the marinade later.\nMince 1 clove garlic\nChop 1 green onion\n5. Wash mushrooms\nWash soaked shiitake mushrooms thoroughly. Squeeze out water.\nWash shiitake mushrooms\nSqueeze out water\n6. Julienne mushrooms\nJulienne shiitake mushrooms.\nJulienne shiitake mushrooms\n7. Make a marinade\nMake a marinade in a bowl by mixing 2 tablespoons of soy sauce, 1½ tablespoons of sugar (or honey), 1 tablespoon of sesame oil, ⅛ teaspoon of black pepper, ½ teaspoon of minced garlic, 2 tablespoons of chopped green onions and 1 teaspoon of ground sesame seeds.\n8. Marinate beef\nUse ⅔ of the marinade to marinate julienned beef. Mix thoroughly by hand. Let it sit for at least 10 min.\nwith ⅔ of the marinade\n9. Marinate mushroom\nWith the remaining ⅓ of the marinade, marinate mushrooms. Let it sit for at least 10 min.\nwith ⅓ of the marinade\n10. Julienne vegetables\nJulienne ¼ cucumber (80g), ½ large carrot (80g) and ½ onion.\n11. Separate eggs\nCrack 2 eggs. Separate egg yolks from whites. *Egg garnish is optional. If you don’t want to make egg garnish, skip to STEP 15.\nSeparate egg yolks from whites\n12. Make egg mix\nAdd a pinch of salt to each egg mix and beat (yolks and whites separately).\nfor each egg mix\n13. Cook eggs\nPreheat a non-stick pan with 1 teaspoon of vegetable oil on low heat for a min. Pour the egg yolk mix into the pan and cook on low heat as thinly as possible like a crepe. Set it aside to cool it down. Repeat the same for egg whites.\nPreheat non-stick pan\nLow Heat 1 min\nCook egg yolk\nLow Heat 5 min\nRepeat for whites\n14. Cut eggs\nFold the cooked egg yolk crepe in half and cut into very fine strips. Repeat the same with the cooked egg whites.\nFold cooked egg yolk crepe in half\nCut into fine strips\nRepeat for egg whites\n15. Sauté beef\nIn a large sauce pan or wok, sauté julienned beef on high heat for 1 min.\nHigh Heat 1 min\n16. Sauté mushrooms\nAdd julienned mushrooms to the pan with the beef in it and cook on high heat for another minute.\nCook High Heat 1 min\n17. Sauté vegetables\nAdd 1 tablespoon of vegetable oil to the same pan. Add julienned onions, carrots and cucumber and cook on high heat for 2 minutes. Add more vegetable oil if the vegetables start to stick to the pan. Set the cooked beef and vegetables aside in a bowl.\nCook High Heat 2 min\n18. Add rice cake & season\nIn the same (but empty) sauce pan, add rice cake, 3 tablespoon of soy sauce, 2 tablespoon of sugar and 1¼ cup of water. Cook on high heat uncovered for 5 min while stirring. Then, add the beef and vegetables back into the pan. Cook on high heat uncovered until all the liquid is mostly reduced, stirring frequently. *If rice cake is not soft at the end, add a little bit of water and cook until it becomes soft.\nIn sauce pan\nCook High Heat 5 min\nAdd beef & vegetables\nCook High Heat\nuntil liquid is reduced\nServe on a plate. Garnish with egg strips and sesame seeds (optional).\nServe on a plate"", 'All About Caramelizing Onions\nThe most important part of making this soup is to caramelize the onions well. Caramelization is the process of slowly cooking the onions so that they become soft and golden brown. The onions release their natural sugars, concentrating their flavor.\nTheres simply no way to replicate the flavor of caramelized onions other than cooking them slowly from scratch.\nI use yellow onions in this recipe, but you could also use red onions if you prefer their flavor. I would pass on sweet onions, which lack the intense flavor you need to flavor the soup.\nAim for slicing the onions about 1/8 inch thick. The shape, whether whole rounds or halfmoons, does not matter as long as they are evenly and thinly sliced. Slicing them too thin can result in a barely-there consistency and you risk burning them. If too thick, youll have to cook them for a longer time and they wont be uniformly caramelized.\nItll take 45 minutes to 1 hour for the onions to caramelize. Cook them over medium heat the entire time and stir them every 5 minutes or so. After the first 10 to 15 minutes, youll notice the volume of the onions reduce in half, theyll softened, and become translucent.\nAt the half hour mark, the volume of the onions will be even smaller and theyll begin to turn a deep golden yellow with brown bits forming on the bottom of the pot. When the onions are caramelized, they are a deeper golden amber color with even more brown bits on the bottom of the pot.\nSlow Cooker Mushroom Smothered Chicken\nThis easy slow cooker mushroom smothered chicken recipe is one your family will love.\nWe love easy recipes and this one is perfect. Its only 5 ingredients and super easy to make in the slow cooker. Its also budget friendly, which is always a good thing. We love this dish served over rice, egg noodles or mashed potatoes. You can use fresh or canned mushrooms. This dish is the perfect comfort food! You may also love this easy chicken and noodles recipe.\nMake Ahead And Storage Instructions\nThe base soup, without the baguette and cheese, can be prepared ahead of time and stored in an airtight container in the refrigerator for up to five days. Simply prepare the soup as directed, but dont top it with bread or cheese.\nIf youre not going to serve the soup within five days, you can portion it into zip top freezer storage bags or a freezer-safe food container and freeze for up to three months.\nWhen youre ready to eat, use either the stovetop or microwave to thaw and reheat the soup. Then, portion the soup into oven-safe bowls, top with the bread and cheese, and place them under the broiler to melt the cheese just before serving.\nSimply Recipes / Sally Vargas\nRecommended Reading: Rapid Weight Loss Soup Diet\nCreative And Comforting Recipes\nThe creative and comforting part is not only about the end result, but is so much about the cooking process.\nWhile I always loved cooking, it wasnt until I studied it in Culinary school, that the process became more important to me. I realized that creating the best comfort foods was more than just putting ingredients together its all about how theyre put together.\nThis French Onion Mushroom Casserole recipe is the perfect example of this.\nLearning how and when to add certain ingredients to a pan, what to finish with and why it all fascinated me. And I learned patience is imperative.\nYoull need patience for this French Onion Mushroom Casserole recipe, and I promise you, itll be worth every second!\nCreamy French Onion Style Slow Cooker Mushrooms\nI am so happy to be back this month with a new crock pot recipe and am pretty much in love with how GOOD these mushrooms are. Seriously. Straight up drink the sauce delicious.\nTrue story when Mr. Claire and I met many years ago he hated mushrooms. I have mentioned this before over on my blog, A Little CLAIREification and, in a nutshell, he had pretty much decided at some point in his formative years that they were just not something he liked at all.\nNeedless to say, this was sort of a bummer to me since I adore mushrooms. Thank God he loved cheese as much as I do but I wasnt ready to give up hope!\nFortunately, after encouraging him to try all sorts of recipes over the years, he finally saw the light and he is now a full on mushroom loving fool. Talk about a complete 180* turn.\nWhen I mentioned that I wanted to make a slow cooker mushroom side dish there was zero hesitation as he immediately said make a twist on on our Favorite French Onion Soup using a packet of onion soup mix. Whaaa? Who IS this man?? If we werent already hitched Id marry him again.\nAnd thus these French Onion Style Slow Cooker Mushrooms were born and oh. my. word. They turned out so great.\nMushrooms actually have a lot of moisture so most chefs will tell you NOT to wash them but rather to use a small kitchen brush to clean them up a little before cooking which is what we did.\nI doubled the recipe just to be sure Mr. Claire would get a few.\nUntil next month, happy slow cooking!\nAlso Check: Soup Mug With Lid And Handle\nDeglaze Season And Simmer\nWhen the onions have caramelized to a golden brown color, add in the flour and cook, stirring, for 1 minute. Next, deglaze with the dry sherry. Continue stirring until most of the sherry is evaporated then add the thyme, Worcestershire, vinegar, and mushroom stock. Simmer on low for 20-30 minutes. Season to taste with salt and pepper.\nOk The Simple Details\nIf youve ever made french onion soup, you know two things. One, its really pretty straight forward and theres nothing too complicated. Two, the onions take a second to really caramelize down. It can take a little more time than the usual thirty-minute start to finish dinner, but its worth the wait.\nAnd I dont know, something about caramelizing onions on the stove is very relaxing and cozy. However, I am weirdIm probably the only one that feels that way.\nAnyway, Ive made a lot of French onion soup over the years. Sometimes Ive taken shortcuts with the onions, but with this soup, I wanted to deeply caramelize them to ensure they make for the most flavorful French onion soup.\nSo, start with sweet yellow onions, butter, a touch of honey, and dry white wine.\nThe butter is used for obvious reasonsfat, flavor, and deliciousness. The honey helps to cook and caramelize the onions faster. I always find these ingredients to be key when trying to achieve perfectly caramelized onions. Now, the wineuse a dry white wine, which is pretty traditional in most French onion soups. Youll cook about half of the wine into the onions. Not only does this help the onions to caramelize quickly and evenly, it also infuses the onions with incredibly rich flavor.\nIf you prefer not to cook with wine, I recommend using about half a cup of apple cider or apple juice to caramelize the onions. For the remaining amount of wine , use chicken or vegetable broth.\nRead Also: On The Border Chicken Tortilla Soup\nMushroom Broth French Onion Soup\nYou cant get the best result nor depth of flavors without using a really high-quality beef bone broth.\nIf youd like to check out Kettle and Fire, then take a look. And, if you decide to buy, then use my discount code ALLYSKITCHEN at checkout for 20% off your order. And, as a full disclosure, I do receive a small commission from your purchase. Believe me, I appreciate it, and it helps pay the overhead here in the kitchen. Nonetheless, rest assured, that all the opinions regarding this brand of bone broth are totally my own.\nOh, yes, I know very well that you can make your own beef bone broth or purchase less expensive brands, but the reactions youll see in the video of Candice and me eating this soup for lunch, well, its in large part because of the excellent beef bone broth.\nWhy We Love This Chicken And Mushroom Soup\n- Its easy and it uses fresh ingredients you may already have on hand.\n- Its a perfect soup for either rice or pasta. Just depends on how your feeling on your soup day.\n- It makes a large pot of soup which you can enjoy for several meals.\n- Its a good reason to make biscuits. If there is bread, they will come to the table.\nDon’t Miss: Microwave Safe Soup Bowls With Lids\nHealthy Mushroom Onion Soup\nTheres absolutely no reason why you cant make this healthy mushroom onion soup! Once you watch the video, youll say Yes! I can!\nThe type of mushroom best for this soup is the baby bella. If you cant source that mushroom, then regular mushrooms work. Youll want the mushrooms to vary in size, some large, medium and small. Also, another option is to use a mushroom blend like shitake, oyster, beech, baby bellas.\nNow if you want to top this soup with the ultimate croutons, yes, homemade, then you must look at my recipe for big garlicky crunchy croutons!\nRelated To The Japanese Onion Soup Recipe\nEnjoy this Japanese onion soup as a light meal for lunch with a salad at the side such as a daikon radish salad and some bread\nYou can also serve this soup as they do in hibachi restaurants as a starter\nThe soup goes really well with the main course such as a beef steak.\nI wouldnt hesitate to serve the soup with western food too because the flavors are quite universal.\nDear Reader, how are you planning to enjoy this Japanese onion soup with mushroom?\nGlobal Food Recipes\nRecommended Reading: Lipton Onion Potatoes\nRead Also: Simply Campbell’s Chicken Noodle Soup\nWhat Is The Best Type Of Mushroom To Use In This Recipe Would Cremini Mushrooms Taste Good Button Mushrooms\nI would highly recommend using any of the 3 mushrooms in the button family. Please see below for a description of the three button, cremini, and portabello. Any of the 3 mushrooms would taste delicious and are generally easy to find in any grocery store. I have added some additional information regarding different mushroom varieties if you are interested in trying something other than the more common mushrooms.\nHomemade Mushroom Stock Ingredients:\nFYI you dont have to make your own stock for this recipe. In a pinch, boxed stock will totally work. That being said, it is actually very easy to make from scratch and I do think the flavor makes it worth the extra trouble. I have included my mushroom stock recipe below in case you want to give it a go. The ingredients youll need are mushrooms , onion, garlic, carrot, thyme, parsley, cold water, salt, and pepper.\nDon’t Miss: Low Salt Chicken Noodle Soup\nQuick French Onion Mushroom Soup\n- 6-7 cups sweet onions, French slice, thin\n- 4 Tbl. Garlic, minced\n- 6-7 cups mushrooms, heads only sliced, reserve stems to simmer in about a cup of water, 3 bay leaves for broth\n- 2 Tbl. Arrowroot, can use flour or corn starch as thickening agent\n- ½ cup marsala cooking wine\n- 32 oz. beef bone broth, cooking broth\n- Sea Salt and Pepper to taste\n- Fresh celery leaves for garnish, can use fresh thyme sprigs\n- 8 oz. pepperjack cheese, small slices to put atop the baguette slices\n- Red Chili Flakes\n- Freshly grated parmesan regianno cheese\nA Note About Vegetarian Cheese:\nDid you know that not all cheese is vegetarian-friendly? Many kinds of cheese are made with rennet which is an enzyme that comes from the stomachs of cows.Luckily, there are a few brands that make vegetarian-safe cheeses free from rennet. While Gruyére and Emmental are the most traditional cheeses to use for French onion soup I recommend choosing an organic swiss, or even white cheddar cheese for a vegetarian-safe substitute. Organic Valley, Horizon, Applegate, and Tillamook all offer vegetarian cheeses and are readily available at most supermarkets.\nAlso Check: Dolly Parton Weight Loss Soup\nRecipe Tips And Substitutions\n- Heres a guide for How to Easily Peel Pearl Onions. This will help you save a lot of time.\n- While I love doing things from scratch, you can purchase already peeled pearl onions. These are typically found in the freezer section of the market. Be sure theyre completely thawed before using them.\n- Vegetarian? If youd like to make this dish vegetarian, use a very hearty vegetable broth instead of the beef broth.\n- If the bread on top gets nice and toasted before the cooking time is up, lightly cover the pan with foil until its done.\nFrench Onion Mushroom Casserole\nFrench Onion Mushroom Casserole is essentially the best bowl of French Onion Soup youve ever had, only in casserole form. The perfectly toasted French bread, the melting Swiss cheese, the caramelized onions, and all of the hearty, warm flavors of beef, garlic and mushrooms theyre all what make this one of the best comfort foods ever!\nCreative and comforting food for casual entertaining.\nThats what I do here on my blog, and in my kitchen. I love cooking for others, but the entertaining part could be switched out for eating, if your prefer.\nRead Also: Knorr Vegetable Soup Mix Gluten Free\nGet The Latest Recipes Right In Your Inbox:\nSo cozy, so comforting and just so creamy. Best of all, this is made in 30 min from start to finish so quick and easy!\nSorry, I know. Another mushroom recipe. Mushrooms are my favorite kind of vegetables and this is my sad attempt at eating clean for the new year of 2015.\nYes, eating clean as I slurp this amazingly cozy soup in 82 degree weather here in Southern California. And then dunking in some crusty garlic bread. You cant forget that.\nBut then again, I did swap out the heavy cream for half and half. At least thats something, right? Plus, theres carrots and celery too. So yeah, definitely clean eating happening here as I drown myself in this epic creaminess.\nIs Gruyere Cheese Vegetarian\nSome cheeses, including Gruyere, can be made with rennet, an enzyme from calves, which means its technically not vegetarian. Some brands make vegetarian Gruyere cheese without animal-derived rennet.\nIf youre not able to find suitable Gruyere cheese, you can substitute swiss, provolone, or mozzarella cheese instead.\nRecommended Reading: Lipton Onion Soup Mix Dollar General\nIngredient Swaps That Work\nFrench onion soup, including this vegetarian version, is traditionally topped with sliced French baguette and Gruyere cheese. I used mushroom broth in place of beef broth to make this soup vegetarian friendly. Here are other ingredient substitutions that would work:\n- If you cant find boxed mushroom broth in your grocery store, look for mushroom bouillon. Its a seasoned mushroom concentrate. Simply mix it with hot water to make the mushroom broth.\n- I havent tried vegetable broth in this recipe, but you certainly could. Some store-bought vegetable broths can be a little bland, so Id recommend making your own. Otherwise, use a store-bought vegetable broth that youve tried and like.\n- Deglazing the pan with white wine is a traditional technique that adds loads of flavor. Deglazing simply means to add a liquid like the wine to help release stuck-on bits of food from the bottom of the pan. I use white wine, but you could use a dry red wine instead. Instead of wine, you could use extra broth to deglaze the pan.\n- Instead of a French baguette, use any crusty loaf of artisan bread, like sourdough or Italian. You want the bread to be sturdy and have a chewy texture so it will hold its shape and does not fall apart in the soup. Dont use soft sandwich breads or brioche.\n- Its okay to use only Gruyere or only Swiss cheese to top the soup.\n- You can also mix it up and use any cheese that melts well. Some examples are:\nRecipe: French Onion And Mushroom Soup\nFrench Onion and Mushroom Soup\nNote: Caramelizing the onions slowly and using good-quality chicken stock is the key to this recipe. Feel free to leave out the mushrooms if you prefer a more traditional soup. If you have homemade beef broth, use that instead of the chicken variety, if you prefer. From Meredith Deeds.\n3 tbsp. unsalted butter, divided\n1 lb. cremini mushrooms, sliced\n3 large yellow onions, thinly sliced\n3 garlic cloves, minced\n3/4 tsp. salt, or more\n1/2 tsp. freshly ground black pepper, or more\n1/2 c. dry white wine\n8 c. chicken stock or broth\n4 sprigs fresh thyme\n4 c. cubes of rustic French bread\n2 tbsp. olive oil\n1 1/2 c. shredded Gruyère cheese\nIn a 6-quart Dutch oven, heat 2 tablespoons butter over medium-high heat. Add the mushrooms and cook, stirring frequently, until browned, about 6 minutes. Using a slotted spoon, transfer the mushrooms to a bowl.\nAdd remaining 1 tablespoon butter and the onions to the pot over medium-high heat. Cook, stirring frequently, until the onions begin to brown, about 8 minutes. Lower the heat to medium-low and continue to cook, stirring occasionally, for 20 minutes, or until the onions are a mahogany brown. Add the garlic, salt and pepper.\nPreheat the oven to 425 degrees.\nMeanwhile, place the bread cubes on a large baking sheet. Drizzle with oil and toss to coat. Bake for 5 to 8 minutes or until lightly toasted.\nNutrition information per serving:\nRecommended Reading: How To Cook Lentil Soup']"	['<urn:uuid:f30caca7-586e-4a4f-9dd1-6fbda211645a>', '<urn:uuid:f2428ef3-7833-4b11-8695-fb614a9d21b4>']	factoid	with-premise	long-search-query	similar-to-document	comparison	expert	2025-05-12T20:11:36.665104	13	52	4091
11	I work in corporate governance and have noticed many jewelry companies struggling with ethical decision-making and leadership. What are the key challenges in maintaining ethical standards in the jewelry industry, particularly regarding transparency and leadership?	The jewelry industry faces several significant challenges in maintaining ethical standards, particularly in transparency and leadership. From a leadership perspective, ethical conduct is crucial as most people expect their leaders to be models of ethical behavior. The industry has seen that unethical leadership can lead to serious consequences, similar to cases like Enron where poor ethical leadership led to company collapse. In terms of transparency, the jewelry sector is addressing this through initiatives like Crystal Clear Production, which represents a shift towards more transparency in the traditionally mysterious world of diamonds. This is especially critical for millennials, who demand assurance that their purchases haven't negatively impacted humans or the environment. The industry also faces challenges in maintaining professional conduct standards, as it deals with sensitive information and valuable materials. Companies must balance the traditional goal of maximizing shareholder value with broader ethical considerations and corporate social responsibility. The industry is responding through various measures, including the Bureau of Indian Standard on gold hallmarking, which aims to ensure quality checks on jewelry.	"[""Jewelry is like a biography, a story that tells many chapters of our lives. Hence, the gems and jewelry sector plays a significant role in our life as well as in the Economy. Being quite a dynamic and fast-growing industry, consequential changes in jewelry industry are underway, both in the consumer behavior and in the industry itself. In the coming years, the development of large retailers/ brands would largely contribute to the growth of the gems and jewelry sector. Established brands are guiding the organized market and opening opportunities to grow.\nThe global jewelry market has seen continuous growth in the past few years and is projected to reach $480.5 billion by 2025. Jewelry has become one of the fastest-growing divisions in the luxury sector, prompting LVMH’s largest purchase yet: the $16.2bn acquisition of Tiffany in 2019. It contributes 7% to India's GDP and 15% to India’s total merchandise export. It employs over 4.64 million people, which is expected to reach 8.23 million by 2022. Annual global sales of $179 billion are expected to grow at a healthy clip of 5-6% each year, totaling $302.48 billion by 2020. But the jewellery industry is still primarily local. The ten biggest jewelry groups capture a mere 12% of the worldwide market. Only two— Cartier and Tiffany & Co. —are in Interbrand’s ranking of the top 100 global brands. The rest of the market consists of strong national retail brands, like Christ in Germany, Chow Tai Fook in China, and small or midsize enterprises that operate single-branch stores.\nFactors & Challenges:\nThe jewelry market trends research process includes various factors such as government policy, competitive landscape, market environment, present trends of the market, upcoming technological innovation, market risks and barriers, market opportunities and challenges, a growing number of digital buyers, an increasing female population, an increasing middle-class population, and growing tourism. The typical challenges would be in the form of declining rough-diamond mine supplies, e-Commerce fraud, and even delayed marriages.\nAs per the research, four types of consumers driving the growth of branded jewelry are identified as:\n“New money” consumers who wear branded jewelry to show off their newly acquired wealth and maintain a social strata\n“Old money” consumers, who prefer heirlooms or estate jewelry\nEmerging-market consumers, for whom established brands inspire trust and the sense of an upgraded lifestyle—a purchasing factor quoted by 80% of them.\nYoung working consumers who turn to brands as a means of self-expression and self-realization\nTo redefine the future of this industry, heavy jewelry is to be replaced with light-weight and sleek options. In upcoming times, industry manufacturers will focus on more sleek and simple designs, ranging between Rs 5000-25000. For example, the most famous Indian jewelry brand Tanishq has come up with 'Mia by Tanishq' . It is fashioned for the working woman who looks to jewelry to express herself. The charming collections are elegant and exciting, powerful and playful, trendy and tasteful, fine and fun. Their necklaces, bangles, rings, pendants, earrings, and bracelets are all made to mirror the woman who asserts herself in every sphere of her life, and looks great while balancing all aspects of her life – personal & professional together; for the woman of today who is always on the move, hence Mia- Me In Action .\nIn light of this trend, fine jewelers might consider introducing new product lines at affordable prices to entice younger or less affluent consumers, giving them an entry point into the brand. Alternatively, they could decide to play exclusively in the high-end and communicate that message strongly through its advertising, in-store experience, and customer service, like the brand Finnati , crafting ethereal designs with diamonds having supreme cut and clarity.\nDue to the high soaring prices of gold and diamond, artificial jewelry is in more demand and has a great potential to prosper. It is more affordable and convenient to embrace. For example - Voylla, Kushal, Sukkhi, Mirraw, Jumkey, Bluestone, Odara , and many small retailers are into the artificial business. Also, the concept of renting artificial and stoned jewelry is expanding as customers tend to rent the heavy bridal jewelry for the occasion and return it as they are of one-day use. The celebratory mindset of our returning customers looking to celebrate personal milestones will also add to the demand.\nCustomization & Personalization\nIt gives jewelry a special touch because it is designed to meet someone’s requirements. Giorgio Armani Privé, Prada, Dolce & Gabbana, and Hermès all debuted their first high jewelry collections last year. Gucci , owned by Kering, also entered the high jewelry market, debuting a garden-themed Haute Joaillerie collection during the couture shows in Paris with a dedicated space in the Place Vendôme. This collection consists of more than 200 pieces, the majority of which are one-of-its-kind. Louis Vuitton traditionally bought cut gems but has started buying rough stones after seeing a surge in demand for unique creations, working with clients directly to determine their final forms. The success of personalization-driven jewelry brands like Pandora has inspired new businesses that approach the concept with varied aesthetics.\nHigher value on Ethical sourcing\nCrystal Clear Production - This shows a shift towards more transparency in the mysterious world of diamonds: a fascination with the stone, coupled with the increasing demand for sustainable production processes and traceable, ethical supply chains, means new procedures are coming into play. Transparency is critical especially among millennials, who favor products and businesses that have a conscience. The 4 Cs will no longer cut it –– millennials want to be assured that what they are buying has not had a negative impact on humans or the environment.\nMintel’s report demonstrates that sustainability and ethics are top of mind for 55% of UK jewelry buyers, who say it’s important for them that the jewelry and watches they purchase are made ethically. With an increased awareness of sustainability comes a desire for recycled materials. Many contemporary jewelers have been using mainly recycled gold for years, while others –– such as Lilian Von Trapp and Vieri –– work exclusively with it. British brand Lylie’s uses precious metal salvaged from technology waste, dental waste, and clients’ unwanted scrap: a process known as ‘e-mining’.\nInternationalization of brands & Industry consolidation\nThere is an expectation that a handful of thriving national or regional jewelry brands will join the ranks of top global brands by 2021. Swarovski is an oft-cited example. In addition, some local brands will almost certainly become known as globally as a result of industry consolidation. International retail groups will acquire small local jewelers. For example - Luxury goods giant LVHM’s acquisition of Tiffany & Co. for a whopping $16.2 billion at the end of 2019 marked the end of a year characterized by “super mega-mergers” worth over $10 billion in corporate deal-making. LVMH, owned by the wealthiest man in France, along with the addition of Tiffany’s to its global brand has helped bolster its jewelry branch, opening a path to the lucrative bridal and diamond sphere, and expanding the brand’s exposure to luxury consumers in the USA.\nWhen a jewelry business offers to finance their customers, the shopper doesn’t have to postpone a purchase due to a lack of immediately available funds. A brand like Harry Winston is very clear about what it stands for; a lower-priced offering would be dissonant with its image and dilute its brand. Subscription services, like Switch, MintGoose, and Pura Vida jewelry club allow customers to loan high jewelry, for a fraction of the cost of purchasing.\nA small-scale wedding may imply a bigger investment in jewelry. As wedding ceremonies have become a closed knit affair, families will not resort to lavish weddings, they will certainly look at gold as an embodiment of an auspicious beginning. Nowadays, gold and diamond are not considered as a mere investment but holding dignity in owning precious jewels.\nTimeless jewelry is all geared up to be a part of the new round. In this digital era, new standards will be set for after-sales services from home pick to home delivery of the repaired products, there will be a lot of new changes. A change will occur in terms of customer handling. Teams are being trained on how to demonstrate the jewelry to persuade the customers and at the same time, they are being trained how to communicate and channelize smooth experience for customers. The ‘Bureau of Indian Standard’ on gold hallmarking in India from January 2018 to include BIS mark purely in carat and fitness as well as the unit’s identification and the jeweler's identification mark on gold jewelry. The move is aimed at ensuring a quality check on gold jewelry.\nExperiential - More and more jewelers are opening piercing salons, as demand skyrockets, largely due to the trend of multiple ear piercings. Maria Tash at Liberty and Harrods, Lark & Berry in Marylebone and Sacred Gold in Coal Drops Yard is a far cry from the grungy tattoo and piercing studios of Camden High Street –– arguably a rite of passage for many a 15-year-old.\nSocial media marketing & Online shopping\nMost consumers prefer to buy expensive items from brick-and-mortar stores, which are perceived as more reliable and which provide the opportunity to touch and feel the merchandise—a crucial factor in a high-involvement category driven by sensory experience. As for fashion jewelry, it is predicted to have a slightly higher online share of sales, around 10-15% by 2020. The bulk of these sales will come from affordable branded jewelry, a somewhat standardized product segment in which consumers know exactly what they’re getting. Jewelry manufacturers can use digital media as a platform for conveying information, shaping brand identity, and building customer relationships.\nAccording to a recent survey, two-thirds of luxury shoppers say they engage in online research prior to an in-store purchase; one- to two-thirds say they frequently turn to social media for information and advice. According to Gartner research, the share of online sales across the US and Western European jewelry sales doubled over 2019 –– to the detriment of brick-and-mortar brands. More than 1 in 20 US jewelers stopped trading in 2018, according to the Jewelers’ Board of Trade (JBT), as companies were forced to either consolidate or close their doors.\nEg: Jennifer Fisher is praised for her social-media savvy, using her 270k-follower-strong Instagram account to promote her mid-range brass line, including on-trend pieces like statement hoops, sculptural cuffs worn by plenty of celebrities. Her private, direct-to-consumer fine jewelry that fuels the 100-percent self-funded business. Net sales were up 40% year-over-year, while the brand's online sales were up 200%.\nAs gender fluidity becomes more commonplace (35% of Gen Z know someone identifying as non-binary, a 2018 Pew study found), boundaries are being broken in jewelry trends, from watches to wedding rings and beyond. While male jewelry has always existed, more unisex pieces are coming to the fore –– for example, Gucci’s first jewelry collection is targeted at no specific gender.\nTo prosper and progress in life we have to accept changes and face challenges. Despite slowing down in the pandemic, the jewelry and gem industry has remained fluidic for many decades. The pandemic has changed the face of this industry which is more customer-centric and user-friendly. Gold has reaffirmed its faith in long term investment criteria and is a safe haven for bad times as what we are facing right now in post-Covid-19. Gold is beyond an adornment metal, it is a symbol of security. The phenomenal percentage increase in gold prices will eventually turn into an increment in sales in the coming future. As lockdown has given a negative sentiment to every trade and business, it is time to bring the change in all works of the industry.\n‘Jewelry is a treasure that lasts from one generation to another. ’\n– Dana Seng"", 'Business ethics are often discussed today, especially in light of corporate scandals. Often, that discussion focuses on professional conduct or illegal practices. Ethics touch many elements of business. These days, many consumers select products based on ethical considerations. Buyers trust that the companies with which they conduct business are responsible and moral.\nEnron was one of the largest energy producers in the world when it was discovered that Enron had been using unethical and illegal accounting practices. These accounting practices enabled executives to grossly overstate the value of Enron. After getting caught, the company was forced to declare bankruptcy. It all happened because of a lack of ethical leadership.\nEthical Leadership and Decision-Making\nEthical conduct is an integral component of how we view leadership. Most people expect their leaders to be models of ethical behavior. The leadership at Enron was most certainly not ethical. It can be difficult to understand what makes leaders behave unethically to the point of collapsing a large company.\nIn making ethical decisions, there are three approaches: ethical egoism, utilitarianism and altruism. Ethical egoism is the belief that the highest good is to be self-serving regardless of others. On the other end of the spectrum, altruism is the belief that the highest good is to help others. The executives at Enron seemed to be making ethical decisions based on ethical egoism.\nGovernance and Compliance\nGovernments will often step in to regulate ethical standards. In U.S. history, companies operated as monopolies across industries such as steel and oil. This enabled the monopolistic companies to set very high prices while dangerously reducing quality. Antitrust laws were enacted and a federal agency was developed to protect consumers from such unethical business practices.\nIt is important to understand that many practices are not only morally unethical, but they are also illegal. Certain professions are legally bound to ethical standards, such as lawyers, accountants and doctors. Acting unethically in these professions is known as malpractice. One example would be insider trading, where an investor might use nonpublic information to reap higher-than-normal profits.\nCorporate Social Responsibility\nEven when regulations are in effect, ethical considerations are often not black and white. There is considerable debate about whether corporations should focus on anything other than shareholder profits. A common thought in business is the belief that maximizing shareholder value is the most ethical goal. Some ethical theories state that corporations must consider benefits for a wider net of stakeholders and not merely shareholders.\nIncreasingly, ethical theories in business are addressing the topic of corporate social responsibility. There is increasing pressure for companies to develop practices and products that benefit the environment or society. This includes business practices that do not require compliance and that will benefit entities other than the company.\nWeighing Consumer Safety\nConsumer safety is a major factor in considering ethical theories in business. This includes product safety and liability, advertising practices and sales or pricing tactics. Unethical business practices have the potential to seriously harm consumers. For example, fake pharmaceuticals that are not tested for quality are not only unethical, they pose a serious threat to public safety.\nThere are many established laws across the world to protect consumers from fraud and other injury. Unfortunately, innocent consumers are often targeted for fraud by unethical businesses.\nProfessional Conduct Standards\nIt is not only highly visible leaders that are expected to behave according to high ethical standards. Many professionals are bound to ethical codes because of the nature of their work. Professionals working in medicine, law, accounting and financial advising, for example, are all subject to strict ethical conduct. These professions usually deal with very sensitive or privileged information.\nMalpractice in these professions can result in being forced out of the profession. A lawyer that knowingly misrepresents a client can be disbarred from practicing law.\nEmployee Relations and Standards\nLabor laws have not always existed as we know them today. Everything from child labor to the number of hours spent working per day required considerable ethical consideration. Discrimination in hiring and firing spurs major ethical debate today. Some people believe that hiring for “cultural fit” is a form of discrimination, while others find nothing wrong with hiring very similar employees.\nWhistleblowing is defined as exposing information about unethical or unsafe conditions within an organization. Individuals who “blow the whistle” must carefully consider the implications and possible retaliation. It can be easy to ignore unethical behavior in the workplace, but some ethical theorists believe that employees must whistleblow when they encounter unethical behavior.\nThe grossly unethical practices at Enron were exposed through whistleblowing. One employee whistleblower is all it took to bring down the world’s sixth-largest energy company.\nConditions in the Supply Chain\nConsumers are becoming more aware of working and environmental conditions all over the world. Globalization and the internet have provided transparency into global supply chains. There are consumers who specifically seek out products that are produced using fair trade practices. Terms like ""blood diamond"" have been coined to shame industries away from sourcing materials in unethical ways. Blood diamonds are diamond gemstones that have been sourced from conflict zones where bad actors may have profited from the transaction.\nSweatshop labor and ""slash and burn"" agricultural practices are other concerns among ethically minded consumers. It is clear that more consumers expect end-to-end ethical conduct from companies.\nCurrent Ethical Considerations\nFaster paces of market changes across the world mean that ethical considerations may not always be a priority. For example, tech companies aspire to grow as rapidly as possible, but there is considerable debate as to whether large technology companies are monopolistic.\nThere are increasing pushes to make it a standard practice to make ethics and values central to every company. That is no simple feat for companies that are developing services and products in brand new and unregulated industries. Rapid changes in technology and business are creating entirely new considerations for ethical theories in business.']"	['<urn:uuid:877ff67a-928d-44b2-a726-342d2065c9a6>', '<urn:uuid:2287b160-513c-4551-b398-165237fb734a>']	open-ended	with-premise	verbose-and-natural	similar-to-document	three-doc	expert	2025-05-12T20:11:36.665104	35	171	2948
12	How does the Colorado Spruce's growth rate compare to its moisture requirements, and what disease symptoms appear when it receives excessive moisture?	Colorado Spruce is a slow-growing tree (6-12 inches per year) and is very drought resistant, requiring only 15 inches or less of annual rainfall. However, when exposed to more moisture than its natural environment, it becomes susceptible to Rhizosphaera needle cast, which causes needles to turn purplish-brown and fall off starting from the bottom of the tree, and Cytospora canker, which causes scattered dead branches with abundant white, sticky resin leaking from the bark.	"['Growzone: 2 - 7\nYou are currently in growzone:\n|Bare Root Prices|\n|12"" - 18""||$2.75||$2.00|\n|24"" - 36""||SOLD OUT||SOLD OUT|\n|18"" - 24""||SOLD OUT||SOLD OUT|\n|2\' - 3\'||$25.00||$23.00|\n|3\' - 4\'||$35.00||$35.00|\nThe Colorado Spruce is a slow growing (6-12 inches per year) evergreen that has a needle that can be green, bluish, or bluish silver that are about 1 inch long. Color can be very variable and the needles are very sharp and can really “poke” when mowing around larger plants. It does not shed its needles but keeps them for 10 years or longer, with its branches extending to the ground. This tree is a native of the high plains, low humidity areas from New Mexico to Wyoming.\nIt will grow to 60 ft tall and 15 ft wide and is very wind firm due to its large spreading root system and tough flexible wood. This tree can live a long life (100 years+) in its native area, outside of that it varies greatly and many times live no longer than 25 years. In the wild around Durango, Colorado many spectacular species grow over 100 ft tall. Due to its shape, heavy snow and ice storms cause little damage. Deer will not normally eat this species unless nothing else is available.\nIt will grow well in hardiness zones from 2-7 and will do quite well in poor soils that other spruce do not like. It is very drought resistant needing only 15″ or less per year of rain and can tolerate hot dry summers better than many other spruce species. It seems not to care about soil PH as will grow slowly in all of them.\nThe Colorado Spruce as been extensively planted throughout the USA and is hard to find a landscape without at least one plant in it. Has been planted for windbreaks in the past because many nurseries carried only this evergreen. When planted by itself out in the sun and wind, the disease problems are reduced but not eliminated. When this tree is taken out of its natural range of low humidity and rainfall and planted in a windbreak, problems occur.\nWith more moisture, and less sun and wind present, this species develops 2 serious disease problems: needle cast and canker which are fungal diseases. It usually starts at the bottom with dead branches and spreads up the tree, with white sap oozing out of the trunk. I have seen trees as small as 3ft having this disease. Does not seem to completely kill the tree, many times a 30 ft tree has only 3 ft at the top that is green. With no windbreak protection and being totally ugly, most people cut them down.\nI have seen beautiful Colorado Blue Spruce windbreaks get about 25 ft tall and then in only a couple of years they are all dead from needlecast.\nA 2ft tall-potted tree can be 5 ft tall in 5 years, in good soil, with adequate moisture, and weed and grass control around the base. Spacing–single row16ft apart, —West of Iowa only-Double row–16 ft between plants and 20 ft between rows, Multiple rows-20 ft between plants and rows.', ""A needle fungus is one problem that can cause spruce trees to become unsightly.\nBy Christine Engelbrecht\nIowa State University\nSpruces are a favorite evergreen for yards and windbreaks in Iowa. However, they are susceptible to a few problems that can leave them unsightly. Knowing about these common spruce ailments is the first step toward minimizing problems with these beautiful trees.\nRhizosphaera needle cast\nPerhaps the most common disease of spruces in this area is Rhizosphaera needle cast, caused by the fungus Rhizosphaera. This disease is most apparent on older needles on the tree, causing them to turn purplish-brown before falling off. Trees are usually affected first on the inside of the tree, working out, and at the bottom of the tree, working up. Affected trees become sparse over time and can eventually die from this disease.\nYou can scout for Rhizosphaera needle cast by using a hand lens to observe both green and brown needles. Tiny, black dots (fungal growths) are visible in lines along the length of some infected needles.\nDamage by Rhizosphaera needle cast can be minimized by inspecting plants before purchase, spacing the trees properly when planting, weeding and pruning to promote airflow in the canopy, and avoiding shearing when plants are wet. When trees are found to be infected, fungicide sprays can be used to reduce spread. Several common fungicides can be used, including Bordeaux mixture, other copper compounds, or chlorothalonil. Timing of the sprays is especially important; spray once in the last two weeks of May and again four to six weeks later. Sprays must be applied yearly until symptoms no longer appear.\nAnother common problem, mite damage, causes spruce needles to turn a dusky yellowish or gray. Mites are tiny spider-like creatures that suck the sap from spruce needles, so that when viewed under magnification the needles appear speckled with yellow flecks. Affected needles may fall from the branch, leaving a sparse tree.\nSeveral types of sprays may be used to minimize damage from mites, but they must be applied when the mites are actually present. You can scout for mites by shaking a symptomatic branch over a white sheet of paper and then looking for tiny, moving dots (mites). When mites are found, sprays you can use include horticultural oil, insecticidal soap, or malathion. Spraying tree branches with a forceful stream of water can also help to dislodge mites, and using chemical sprays may not be necessary.\nSometimes individual branches of spruces turn brown and die. Scattered, dead branches are the hallmark of Cytospora canker, another fungal disease. A canker is an area of dead bark, and when that dead area grows all the way around the branch, it girdles and kills it. When a canker grows all the way around the trunk of a tree, it can kill the whole tree. On spruces, Cytospora canker is most visible as areas on the bark leaking abundant white, sticky resin on dead branches.\nBranches with resin-leaking cankers should be pruned out to reduce the amount of fungus in the tree. Canker diseases occur on trees that are undergoing other stresses, especially drought, so keeping trees in good vigor (such as by watering during dry periods) can help to prevent Cytospora canker.\nSometimes needles on spruce trees turn brown and no infectious disease or insect damage is apparent. A variety of “abiotic” (noninfectious) stresses can cause damage to spruce trees. Such stresses include drought stress, injuries to the trunk, winter injury, damage from misapplied chemicals, or root problems. A thorough review of the tree’s location and recent history can help to diagnose these problems.\nWith a little awareness of the common problems on spruce, homeowners can enjoy the abundant beauty that spruce trees bring.\nJean McGuire, Continuing Education and Communication Services, (515) 294-7033, email@example.com\nTwo high-resolution photos are available for use with this week's column:\nRhizoNeedleCast.jpg Caption: Spruce tree affected by Rhizosphaera needle cast.\nSpruceCanker.jpg Caption: Scattered dead branches in a spruce caused by Cytospora canker.""]"	['<urn:uuid:5d3e9556-a4ed-480f-9697-24753c6ef794>', '<urn:uuid:17a5643d-6d03-411e-8c4a-57d6d4bffeb5>']	factoid	direct	verbose-and-natural	similar-to-document	multi-aspect	expert	2025-05-12T20:11:36.665104	22	74	1188
13	vinegar vs wax based rust prevention cost effectiveness	Vinegar is a cheaper rust prevention solution compared to wax-based products. Vinegar is readily available and inexpensive for rust removal, though it requires more scrubbing and waiting time. Wax-based products, while more expensive, form a thick protective film that effectively prevents rust formation on metal parts, requiring less maintenance effort.	['If you have tools, you’ve encountered rust! Here’s the low down on what’s causing your beloved tools to rust, the best way to remove rust from tools, and the best way to prevent them from prematurely rusting in the first place.\nYou pop out to the shed, pick up your favourite chisel and see shocking red stains across the previously shiny metal.\nYou reach for your beautifully repaired hand plane and see perfect replicas of your fingerprints in a rusty shade of orange – marring the once polished sides of the sole.\nYour trusty adjustable wrench no longer moves with ease because the shifter is gummed up with those dastardly, dusty, and grainy flakes of red.\nAny of this sound familiar?\nRust is something every owner of tools will struggle with at some point, but there are ways to lessen the struggle and save ourselves from some of the frustration!\nHow does rust form?\nWhen we say “rust” what we are actually referring to is an “iron oxide.” Iron oxide occurs when iron is exposed to both oxygen and moisture. This exposure starts an electrochemical process which changes the material at a molecular level – essentially the metal fuses with oxygen molecules and creates the new material – Fe2O3.\nIron oxide weakens the bonds of the metal – which is why when rust is bad enough, you can literally break a once solid iron bar with your bare hands.\nAn interesting side note: When iron oxide is created it takes up more volume than the original iron, which causes it to expand or “puff out” on the original piece. If you’ve ever seen cracked concrete that has metal rebar exposed, it likely occurred because of the expansion of the metal as it rusted. This is called “oxide jacking.”\nHow long does rust take to occur?\nThe first thing that comes to mind for many of us when we observe a heavily rusted item is “wow, that must be old.” This is why it is surprising the first time you return to your workshop and see a brand new tool already working its way towards looking like you picked it out of a box of your great-uncle’s old tools.\nBut, the reality is rust doesn’t take a long time to form. If you leave your cast iron table saw top unprotected with a piece of green wood on it overnight, you can expect to come back the next morning and see a thin layer of rust already formed! (oops… again)\nA great video from the Canadian Conservation Institute shows how quickly flash rusting can occur when exposing a simple iron putty scraper to moisture over a period of 4 hours.\nTwo types of rust that I watch for\nIn the case of my tools, and antique restoration, there are two types of rust corrosion that I usually look for to distinguish how much I can expect to be able to restore the tool to usable order.\nThe first is surface rust. This is the orange, dusty rust that can easily be brushed or scoured off with minimal effort. While some discolouration of the metal beneath the rust often occurs after surface rust is cleaned away, it rarely affects the strength of the underlying metal to a high degree.\nUnfortunately, the only way to know for sure if it is surface rust is to brush it down to the bare metal.\nThe second type of corrosion is the more nefarious type – pitting. Pitting is a concentrated type of corrosion that forms holes or ‘pits’ in the metal.\nTo really understand the process that causes pitting, let’s go back to what causes rust in the first place and dig a little deeper.\nIn order for rust to develop it requires an anode (for us, a piece of iron that gives up its electrons), an electrolyte that can move the electrons (water, humid air, etc), and a cathode (another piece of iron that accepts the electrons.)\nIn the case of pitting, the corrosion is caused by a lack of oxygen in a particular area of the metal, which causes this area to give up its electrons more readily (an aspect referred to as anodic), and the surrounding areas, which have a bit more oxygen, are readily accepting of these electrons (cathodic). Since rust already requires the above exchange of electrons, this localised form accelerates the progression of the rust in a very concentrated area.\nBecause pitting occurs in a localised area instead of over the entire surface of the metal and extends deeper into the metal, it is often harder to detect and much more damaging to the integrity of the piece.\nUnfortunately, once pitting has occurred the only thing that can be done to “fix it” is to remove the rust from the holes and then fill them with epoxy or weld.\nThis isn’t the greatest option for our hand tools – but depending on how special/expensive your tool is you might be apt to try it.\nRust removal strategies\nThere are many ways to remove rust from your tools – from harsh chemical removers to regular household cleaning products, to more in-depth procedures like electrolysis.\nFor many of my tool restorations, I have used plain old white vinegar. That’s right, the same stuff you have in your cupboard for salad dressings, or beneath your sink for cleaning is also phenomenal for rust removal.\nTo use vinegar as a rust remover, I believe the best method is soaking the tools in a tub with pure white vinegar. Some people add salt to the mix, but I have never found this to be any more effective than just vinegar – which means it’s just an added, unnecessary step for me.\nI’ve left rusted tools in the vinegar anywhere from a few hours to a couple of months (I got distracted, okay?).\nA couple of months was TOO LONG (oopsy). While the acid in the vinegar softens the rust and makes it easy to remove, it also eats at the metal – so it’s important to check your soaking tools regularly and remove them when the rust is easily scrubbed off with fine steel wool or an abrasive pad.\nAfter removing the tools from the vinegar and giving them a scrub, it’s important to thoroughly wash them to remove the acid from the vinegar. I’ve given them a good rinse in water as well as a dip and scrub in baking soda/water solution. The water itself works fine, but the added baking soda helps to neutralize any excess acid and the bubbling also works at getting beneath any areas of flaking that were left over.\nI’ve soaked my tools in vinegar several times and always have good results (except for the above mentioned time when I got distracted and left them for too long) – granted it might take a bit more scrubbing after the fact than the following methods, but it’s cheap and readily available.\nBetter: Chemical rust remover – Evaporust\nThere are many chemical rust removers on the market, such as CLR and naval jelly, but the one I have personally used and observed others using with good results is Evaporust.\nThere are a load of great things about Evaporust, but my personal favourite is that, unlike the vinegar and many other products, it doesn’t use acid to remove the rust. Therefore, I’m not going to run into the same problem of it attacking the metal itself if I leave it a bit too long. It’s also reusable, which means I can use it for several tool restoration products without worrying about too huge of a cost. And, it’s not as toxic or harsh as many of the other products on the market – I still wear gloves when I use it (because I’m paranoid), but Evaporust itself states that gloves and eye protection are not necessary. If you’ve ever used other harsh chemical removers before, you know how caustic they can be – so this is a great bonus.\nTo use it, I simply give my tools a cursory scrub to remove loose flakes of rust and dirt – I mostly do this so that I don’t have to spend time afterward trying to really filter out the liquid before storing it for my next use. I then place the tools in a bucket of the Evaporust, put a lid on, and wait. I’ve never had to wait longer than 24 hours for fantastic results. It does leave a dark residue on the tools, but some quick polishing removes it.\nDoes the added cost make it worth it over the plain old vinegar? If you’re only doing a few rust removal projects here and there, don’t mind waiting longer for results, and putting a bit more work into scrubing after the soak, then I think the vinegar is more than adequate for your purposes. For me, the Evaporust has saved a lot of time and hand power in my restoration projects.\nI’m convinced that electrolysis is the ultimate rust removal method. While I’ve only gotten to see the results of this procedure twice in person, I am excitedly awaiting the day when I can pick up a battery or charger and am able to use it for all of my rust removal purposes.\nThe process of electrolysis for rust removal requires passing a small electrical charge through your rusty metal. This charge stimulates an exchange of ions while the tool is submerged in an electrolyte liquid, effectively stimulating the exact opposite chemical reaction seen in rusting.\nI was initially turned on to this process when I was lamenting some very rusted auto parts that I was dealing with, and my mechanic pal, who had all the necessary materials, told me to come over to see some magic. Jeeze Louise, it was magic. The rust just sloughed right off my headers and oil pan. I was immediately hooked.\nYou’ll need a car battery charger, a plastic or glass bin or bucket, washing soda, and a strip of metal to attach to the positive electrode.\nIt’s very important to have a well-ventilated area for your electrolysis procedure, as the process creates hydrogen and oxygen gas, which are highly explosive if ignited. Ensure that no sparks or open flame are anywhere near the setup, and that no spills of water can come into contact with the battery.\nSo, we’ve cleaned all the previous rust off and our tool is shiny and clean and ready to go right? You set it down on the table and turn around for two minutes only to find a thin layer of rust already developing!\nAfter rust removal, the bare surface of the metal is especially vulnerable to flash rust. That’s why it is important to immediately protect it from further rust.\nWe know that the combination of moisture and oxygen with the metal is what causes rust to occur. So, in order to prevent that, adding a protective layer to the metal that stops moisture and oxygen from contacting it is necessary.\nMy go-to when finishing the rust removal process is to wipe my tools with Camellia oil. I apply the oil on a rag which I store in an old, metal Altoid box – this makes it easy to grab and use whenever I am finished with using my tools. Camellia oil, unlike other machine oils, doesn’t have the tendency to stain the wood that you’re working with, which is a huge plus! After using any of my planes or chisels, I always clean them off and then wipe them down with the oil-soaked rag before storing them.\nFor my table saw surface, I avoid the use of oil as it easily attracts dust, instead, I thoroughly clean the surface with mineral spirits and then apply a dry, silicone-free spray such as Bostik’s Top- Coat or Boeshield T-9, I’ve used both with good results that tend to last longer than a paste wax. After spraying, I give it a good wipe down with a paper towel and I’m good to go!\nProper storage of your tools is another good rust-preventive strategy. Ideally, your tools would be stored in an area with very little moisture exposure. However, since I live in an area with heavy humidity and no environmental control in my shed, I’ve found the silica-gel packs (those little packs that come in the packages of many products and have “do not eat” written on them) to be a good way to prevent moisture from getting at your tools if you are storing them in boxes– toss a few in with them and the packs will suck up the moisture in the air.\nThe silica gel packs lose their effectiveness over time, but I find they work well if being changed out on a semi-regular basis, or after “reviving” the packs by heating them in the oven to dry them out once they have lost their effectiveness. Give it a try!\nIf storing your tools in the open air, I find the best way to keep them from rusting is to hang them – while trying to keep them from touching any raw wood – raw wood just loves to seep moisture out onto your vulnerable metal tools!\nYou can kick rust to the curb\nRust is something as a tool user that you will always be trying to keep ahead of. But, now that you know the strategies to prevent it, and the ways in which it can be removed, you have the power to keep your tools shiny and clean for years to come!\nIf you’ve found this information helpful, or have any questions or comments on rust removal and prevention, please leave us a comment below!', 'The unsightly orangy-brown mess that accumulates on metal is a highly unattractive feature that plagues many different objects, especially cars, trucks, screws, bolts, and tools. Under the appropriate conditions, rust may strike rather quickly or take its time slowly eroding your belongings. When oxygen and moisture come into contact with exposed metal, rust is unfortunately the result. While the corrosion is removable in most cases, it is much better to prevent rust rather than deal with its aftermath.\nWhat is Rust?\nWhen the open-air oxidation of iron takes place, rust is the oxide that forms. While the chemical composition of rust (Fe2O3.nH2O) means nothing to the average homeowner, it is the resulting hydrated iron oxide that gets all of the attention . Rusting is the general term used to describe the corrosion of iron and its alloys, such as steel.\nThe process of rust involves the way various materials react with oxygen, which results in the chemical compound of rust. When iron combines with oxygen, iron oxide forms. Since iron oxide is larger in size than iron, the oxidation process causes it to “puff up” and sometimes flake. When a great amount of rust accumulates, the buildup may create a powerful force that can actually separate adjacent parts. This occurrence is referred to as “rust smacking.”\nThe Negative Effects of Rust\nOne of the first things an individual affected by rust will notice is the sight of unattractive burnt orange/brown that clearly indicates corrosion. The dismal appearance is especially disturbing when it attacks the outside of a car, where speedy attention is needed to avoid further damage and the spread of rust. Once rust begins to develop, it can spread like an infection, until before you know it â€“ you have a worthless item on your hands.\nRust damage on personal possessions (like a set of golf clubs), ruins the integrity of such objects. When rust strikes items and immediate attention doesnâ€™t take place (such as the case of woodworking tools), they are often rendered useless and ineffective.\nNot only is the sight of rust an unappealing aspect, but also more seriously, rust damage can threaten overall safety and health, as it can completely eat away at vital equipment and machinery parts used on a daily basis. Rusted bolts and other affected parts may cause an object to malfunction, break, or facilitate bodily injury. Rust damage in an automobile can create unseen dangers that affect the overall performance and security of a vehicle.\nIt is also possible to suffer from minor sickness and skin irritations when rusted pipes or affected water mains cause the water in a house or building to turn brown, red, orange, or yellow in color . Rusted pipes also create a damaging tint and condition in the water that affects clothes washed at home.\nContributing Factors of Rust\nWhen it comes to rust damage, there are contributing factors that cause steel or iron to rust faster than others. For instance, water is a common culprit responsible for the development of rust that attacks iron and steel. Dissimilar metals will also rust faster than single metals because of the electrochemical reactions that take place. Therefore, steel may rust faster than iron, but the joints between the two will rust much quicker . Salt water also causes rust to develop quicker than fresh water because salt is a better electrical conductor. Heat also causes an increase in the rate of rust development.\nCommon Victims of Rust\nIn order for an item to suffer the damaging effects of rust, a few factors must be present to facilitate the process. Common victims of rust are mostly objects that contain a lot of iron and steel parts â€“ something made completely from plastic will not suffer rust damage. Common victims of rust also come in contact with heat and moisture (the combination of both is much more damaging). Often, objects that spend a great amount of time in the outdoors are also susceptible to rust, especially if they are located within a rainy or humid climate. Some of the most frequent victims of rust include:\na) Cars and Trucks:\nOne of the most common victims of rust and corrosion is a car or truck, where damage occurs on the exterior, under the carriage, and beneath the hood of a vehicle. Rust is often known to strike the paint job, tailpipe, fender, bumper, doors, hood, and braking system of a car or truck.\nSince boats come in contact with extreme moisture in the air, they often become prime targets for rust. Boats suffer the heightened risk for rust accumulation because they not only spend most of their time in the water, but also sometimes travel through saltwater, which causes a higher rate of rust damage.\nThe hammer, screwdriver, and wrench in your toolbox are quite susceptible to rust because of all the outside factors that threaten its appearance and function. The constant contact with hands, humid conditions, and moisture all contribute to the formation of rust on tools, especially those used on a frequent basis, such as woodworking tools.\nThe proper procedure regarding the maintenance of a gun is to clean it after use, but the big threat occurs when the gun is not being used and poor storage conditions create the perfect environment for rust and corrosion to develop.\nHow to Prevent Rust\nRust has the potential to form on anything that is made out of iron, meaning any piece of farm equipment, automobile, house, railroad, ship, bridge, or any other personal item may suffer from its damaging effects. To protect your belongings, you should consider the following rust prevention techniques that have helped many people keep their tools, cars, and other rust-prone possessions in good condition:\na) Car Rust Prevention:\nThe corrosion that takes place on your vehicle can be prevented through frequent cleaning and waxing. Spraying the undercarriage is also a must in order to keep it free of dirt and debris that is often responsible for the collection of moisture . Once a car is washed, sitting it in the sun for a few hours is one of the best techniques of drying to follow. During the winter season, it is important to note that the rise in salty conditions produce a high amount of sodium chloride, which is known to speed up the rusting process .\nSometimes, factories will soak the body of a car in chemicals (usually electrically charged) to create layers of rust protection . Some places galvanize (add a coating of zinc) car bodies, which is done before the primer coat of paint is added to the vehicle. Cars possessing a body-on-frame construction may undergo rustproofing techniques to the frame and its methods of attachment. There are also aftermarket kits sold as sprays to add further protection to cars after a new purchase.\nTo prevent rusting of small steel items, such as firepower, bluing is a technique that offers limited protection. Successful application includes the use of water-displacing oil rubbed onto blued steel.\nc) Metal Coating:\nThe control of corrosion may take place when metal is isolated from the rest of its environment, such as paint. In large structures, the process may include the use of a wax-based product (sometimes called a slushing oil), which is injected into sections of objects that poses concern. The oil often contains rust inhibiting chemicals that works to form a protective barrier.\nd) Rust Prevention Products:\nInvesting in highly recommended rust prevention products is a good way to protect the integrity of your car and other metal-containing possessions. Some items come in an aerosol can, while others are applied as gels and liquids. Application wipes are also a common product to consider. A few suggestions to keep in mind include the Boeshield T-9 12 oz. Aerosol Can; Bull Frog Heavy Duty Rust Blocker Gel; and the Sentry Solutions TUF-CLOTH (and the MARINE TUF-CLOTH option).\ne) Gun Protection: To prevent rusty guns, the use of a specially made cabinet or safe that provides just the right environment for your firepower is highly suggested. Some people also utilize the latest technological advancements to protect their guns, such as a Vapor Corrosion Inhibitors (VCI), which promotes an atmosphere that deters rust and corrosion.\nf) Rust Inhibitor Coating:\nThere are plenty of products on the market that work against the accumulation of rust. These include selections that are scientifically tested to control rust and corrosion. Sometimes, they work before the process of rusting has taken place, while others can be applied over rusty metal to prevent further damage.\ng) Immediately Repair Rust Spots:\nAt the first sign of rust, one may prevent further damage by attending to problem spots, including small bubbles. It is important to react quickly because light rust damage could indicate a higher level of unseen deterioration. This is especially true when it comes to car protection. One of the first ways one may repair a rust spot is to remove loose pieces of rust. The use of a razor blade is a common tool that carefully scrapes loose paint and rust away. After removing loose rust, additional traces of grime should be removed using warm water and soap. The application of a metal conditioner and primer is suggested, which helps to stop further rust. When applicable, touch-up paint should follow.\nh) Tool Rust Prevention Products:\nThe market offers a selection of cream waxes and pastes (such as Automobile Paste Wax), which treats tools, such as woodworking equipment. Some individuals prefer paste-based products, since cream options contain a certain amount of water in their composition. Smaller tools respond well to commercial products aimed at displacing moisture and preventing rust. A few common choices that leave behind a film of light oil-based product includes WD-40, Sprayon Corrosion Supressant, Rustlick 631, Starrett M1, Boeshield T9, Birchwood Casey Sheath, and Rust-X (all of which prevent rust by keeping air and water contact off of the metal surfaces).\ni) A Good Wipe:\nSince the sweat (salt) and oils that come with hand use causes rust to develop, wiping down tools after use is a great way to reduce the amount of rust and moisture that gathers on your metal-based equipment.\nInstalling this type of equipment in a garage, shop, closed room, basement, or cellar will help prevent rust by controlling the moisture in the air.\nk) Prevent Boat Rust:\nRoutine boat maintenance is a great way to prevent rust, which often includes regular habits of washing off the boat with clean fresh water and a mild detergent after each run out on the open sea. Before storing, it is important that all of the metal and glass parts, as well as flat surfaces are well dried. Boat covers also protect vessels from the harshness of sun and rain. The use of a commercial protect that fights against rust is also suggested.\nProducts, such as Boeshield T-9 forms a thick waxy film on metal parts that prevents rust formation on inboard and outboard engines, electronics, deck hardware, and wiring. The use of rechargeable silica gel packs also helps to control the storage moisture that causes rust to form.\nl) Homemade Rust Protection:\nTo make your own rust prevention product at home, you may combine 1 part Anhydrous Lanolin with 5 parts Paint Thinner . While a brush is often used for application, some people will dip their metals into the concoction.\n National Motorist; The Steering Column, Winter ’97']	['<urn:uuid:837b8286-7f66-44af-896c-6247bfbf845a>', '<urn:uuid:5fef5a94-ad4b-4ded-9707-a68a679bbf1a>']	factoid	direct	short-search-query	distant-from-document	comparison	expert	2025-05-12T20:11:36.665104	8	50	4196
14	learning about materials science and nanotechnology benefits to environment what are structure property relationships and environmental applications	Materials science studies how a material's structure relates to its properties - from atomic scale to macro scale. The structure-property relationship is fundamental as it determines how materials perform in applications. Scientists examine structure using various methods like X-rays and microscopy to understand properties. Regarding environmental applications, nanotechnology offers several benefits: It enables more efficient solar cells through nanostructured materials, improved fuel cells using nanocatalysts, better batteries, and advanced water treatment. Specifically, nanoporous membranes with pores smaller than 10nm can be used for water filtration, while magnetic nanoparticles provide an effective method for removing heavy metal contaminants from waste water.	"['ASKELAND MALZEME PDF\nDocuments Similar To 04 Askeland Chap. 05 Askeland Chap Askeland Chap 4 Solution. Uploaded by. Damita de Peña. 12 MAlzeme bilimleri. Uploaded by. Turkish, Malzeme Bilimi D.R. Askeland, , “The Science and Engineering of Materials”, Erdoğan, M. Malzeme Bilimi ve Mühendislik Malzemeleri 5. Dersin Adı, Türkçe, Malzeme Bilimi. İngilizce, Materials Science M. Erdogan ( D. Askeland) “Malzeme Bilimi ve Mühendislik Malzemeleri ” Nobel.\n|Published (Last):||17 July 2010|\n|PDF File Size:||9.84 Mb|\n|ePub File Size:||10.49 Mb|\n|Price:||Free* [*Free Regsitration Required]|\nThe interdisciplinary field of materials sciencealso commonly termed materials science and engineering is the design and discovery of new materials, particularly solids.\nThe intellectual origins of materials science stem from the Enlightenmentwhen researchers began to use analytical thinking from chemistryphysicsand engineering to understand ancient, phenomenological observations in metallurgy and mineralogy. Nalzeme such, the field was long considered by academic institutions as a sub-field of these related fields. Beginning in the s, materials science began to be more widely recognized as a specific and distinct field of science and engineering, and major technical universities around the world created dedicated schools of the study, within either the Science or Engineering schools, hence the naming.\nMany of the most pressing scientific problems humans currently face are due to the limits of the materials that are available and how they are used. Thus, breakthroughs in materials science are likely to affect the future of technology significantly. Materials scientists emphasize understanding how the history of a material its processing influences its structure, and thus the material’s properties and performance.\nThis paradigm is used to advance understanding in a variety of research areas, including nanotechnologybiomaterialsand metallurgy. Materials science is also an important part of forensic engineering and failure analysis ma,zeme investigating materials, products, structures or components malzemf fail or do not function as intended, causing personal injury or damage to property.\nSuch investigations are key to understanding, for example, the causes of various aviation accidents and incidents. The material of choice of a given era is often a defining point. Originally deriving from the manufacture of ceramics and its putative derivative metallurgy, materials science is one of the oldest forms of engineering and applied science.\nModern materials science evolved directly from metallurgywhich itself evolved from mining and likely ceramics and earlier from the use of fire. A major breakthrough in the understanding of materials occurred in the late 19th century, when the American scientist Josiah Willard Gibbs demonstrated that the thermodynamic properties related to atomic structure in various phases are related to malze,e physical properties of a material.\nImportant elements of modern materials science are a product of the space race: Materials science has driven, and been driven by, the development of revolutionary technologies such as rubbersplasticssemiconductorsand biomaterials. Before the s and in some cases decades aftermany materials science departments were named metallurgy departments, reflecting the 19th and early 20th century emphasis on metals.\nThe growth of materials science in the United States was catalyzed in part by the Advanced Research Projects Agencywhich funded a series of university-hosted laboratories in the early s “to expand the national program of basic research and training in the materials sciences. Ceramic, Metal or Polymer.\nThe prominent change in materials science during the last two decades is active usage of computer simulation methods to find new compounds, predict various properties.\nA material is defined as a substance most often malzems solid, but other condensed phases can be included that is intended to be used for certain applications.\nMaterials can generally be further divided into two classes: The traditional examples of materials are metalssemiconductorsceramics and polymers.\nThe basis of materials science involves studying the structure of materials, and relating them to askelnd properties. Once a materials scientist knows about this structure-property correlation, they can then go on to study the relative performance of a material in a given application.\nThe major determinants of the malzee of a material and thus of its properties are its constituent chemical elements and the way in which it has been processed into its final form. These characteristics, taken together and related through the laws of thermodynamics and kineticsgovern a material’s microstructureand thus its properties. As mentioned above, structure is one of the most important components of the field of materials science.\nMaterials science examines the structure of materials from the atomic scale, all the way up to the macro scale. Characterization is the way materials scientists examine the structure of a material.\nThis involves methods such as asekland with X-rayselectronsor neutronsand various forms of spectroscopy and chemical analysis such as Raman spectroscopyenergy-dispersive spectroscopy EDSchromatographythermal analysiselectron microscope analysis, etc. Structure is studied at various levels, as detailed below. This deals with the atoms of the materials, and how they are arranged to give molecules, crystals, etc.\nMuch of the electrical, magnetic and chemical properties of materials arise from this level of structure. The way in which the atoms and molecules are bonded and arranged is fundamental to studying the properties and behavior of any material. This causes many interesting electrical, magnetic, optical, and mechanical properties. In describing nanostructures it is necessary to differentiate between the number of dimensions on the nanoscale.\nNanotextured surfaces have one dimension on the nanoscale, i. Nanotubes have two dimensions on the nanoscale, i. Finally, spherical nanoparticles have three dimensions on the nanoscale, i.\nThe terms nanoparticles and ultrafine particles UFP often are used synonymously although UFP can askelamd into the micrometre range. The term ‘nanostructure’ is often used when referring to magnetic technology. Nanoscale structure in biology is often called ultrastructure. Materials which askelnd and molecules form constituents in the nanoscale i.\nNanomaterials are subject of intense research in the materials science community due to the unique properties that they exhibit.\nMost of the traditional materials such as metals aaskeland ceramics are microstructured. The manufacture of a perfect crystal of a material is physically impossible. For example, any crystalline material will contain defects such as precipitatesgrain boundaries Hall—Petch relationshipvacancies, interstitial atoms or substitutional atoms.\nThe microstructure of materials reveals these larger defects, so that malzeje can be studied, with significant advances in simulation resulting in exponentially increasing understanding of how defects can be used to enhance material properties.\nMacro structure is the appearance of a material in the scale millimeters to meters—it is the structure of the material as seen with the naked eye. Crystallography is the science that examines the arrangement of atoms in crystalline solids. Crystallography is a useful tool for materials scientists.\nIn single crystals, the effects of the crystalline arrangement of atoms is often easy to see macroscopically, because the natural shapes of crystals reflect the atomic structure. Further, physical properties are often controlled by crystalline defects.\nThe understanding of crystal structures is an important prerequisite for understanding crystallographic defects. Mostly, materials do not occur as a single crystal, but in polycrystalline form, i. Because of this, the powder diffraction method, which uses diffraction patterns of polycrystalline samples with a large number of crystals, plays an important role in structural determination. Most materials have a crystalline structure, but some important materials do not exhibit regular crystal structure.\nPolymers display varying degrees of crystallinity, and many are completely noncrystalline. Glasssome ceramics, and many natural materials are amorphousnot possessing any long-range order in their atomic arrangements.\nThe study of polymers combines elements of chemical and statistical thermodynamics to give thermodynamic and mechanical, descriptions of physical properties. To obtain a full understanding of the material structure and how it relates to its properties, the materials scientist must study how the different atoms, ions and molecules are arranged and bonded to each other. This involves the study and use of quantum chemistry or quantum physics. Solid-state physicssolid-state chemistry and physical chemistry are also involved in the study of bonding and structure.\nSynthesis and processing involves the creation of a material with the desired micro-nanostructure. From an engineering standpoint, a material cannot be used in industry if no economical production method for it has been developed.\nThus, the processing of materials is vital to the field of materials science. Different materials require different processing or synthesis methods.\nFor example, the processing of metals has historically been very important and is studied under the branch of materials science named physical metallurgy. Also, chemical and physical methods are also used to synthesize other materials such as polymersceramicsthin filmsetc.\nAs of the early 21st century, new methods are being developed to synthesize nanomaterials such as graphene. Thermodynamics is concerned with heat and temperature and their relation to energy and work.\nIt defines macroscopic variables, such as internal energyentropyand pressurethat partly describe a body of matter or radiation.\nIt states that the behavior of those variables is subject to general constraints, that are common to all materials, not the peculiar properties of particular materials.\nThese general constraints are expressed in the four laws of thermodynamics. Thermodynamics describes the bulk behavior of the body, not the microscopic behaviors of the very large numbers of its microscopic constituents, such as molecules.\nThe behavior of malze,e microscopic particles is described by, and the laws of thermodynamics are derived from, statistical mechanics. The study of thermodynamics is fundamental to materials science. It forms the foundation to treat general phenomena in materials science and engineering, including askdland reactions, magnetism, polarizability, and elasticity.\nThe Science and Engineering of Materials 7e\nIt also helps in the understanding of phase diagrams and phase equilibrium. Chemical kinetics is the study of the rates at which systems that are out of equilibrium change under the influence of various forces. When applied to materials science, it deals with how a material changes with time moves from non-equilibrium to equilibrium state due to application of a certain field. It details the rate of various processes evolving in materials including shape, size, composition and structure.\nDiffusion is important in the study of kinetics as this is the most common mechanism by which materials undergo change. Kinetics is essential in processing of materials because, among other things, it details how the microstructure changes with application of heat.\nMaterials science has received malzzeme attention from researchers. In most universities, many departments ranging from physics to chemistry to chemical engineeringalong with materials science departments, are involved in materials research. Research in materials science is vibrant and consists of many avenues. The following list is in no way exhaustive. It serves only to highlight certain important research mapzeme. Nanomaterials research takes asoeland materials science-based approach to nanotechnologyleveraging advances in materials metrology and synthesis which have been developed in support of microfabrication research.\nMaterials with structure at the nanoscale often have unique optical, electronic, or mechanical properties. The field of nanomaterials is loosely organized, like the traditional field of chemistry, into organic carbon-based nanomaterials such as fullerenes, and inorganic nanomaterials based on other elements, such as silicon.\nMaterials science – Wikipedia\nExamples of nanomaterials include fullerenescarbon nanotubes malzsme, nanocrystalsetc. A biomaterial is any matter, surface, or construct that interacts with biological systems.\nThe study of biomaterials is called bio materials science. It has experienced steady and strong growth over its history, with many companies investing large amounts of money into developing new products. Biomaterials science encompasses elements of medicinebiologychemistry mazleme, tissue engineeringand materials science. Biomaterials can be derived either from nature or synthesized in a laboratory using a variety askleand chemical approaches using metallic components, polymersbioceramicsor composite materials.', 'Environmental impact of nanotechnology\n|Part of a series of articles on the|\nThe environmental impact of nanotechnology is the possible effects that the use of nanotechnological materials and devices will have on the environment. As nanotechnology is an emerging field, there is great debate regarding to what extent industrial and commercial use of nanomaterials will affect organisms and ecosystems.\nNanotechnology\'s environmental impact can be split into two aspects: the potential for nanotechnological innovations to help improve the environment, and the possibly novel type of pollution that nanotechnological materials might cause if released into the environment.\nNanopollution is a generic name for waste generated by nanodevices or during the nanomaterials manufacturing process. Ecotoxicological impacts of nanoparticles and the potential for bioaccumulation in plants and microorganisms is a subject of current research, as nanoparticles are considered to present novel environmental impacts. Of the US$710 million spent in 2002 by the U.S. government on nanotechnology research, $500,000 was spent on environmental impact assessments.\nThe capacity for nanoparticles to function as a transport mechanism also raises concern about the transport of heavy metals and other environmental contaminants. Two areas of concern can be identified. First, in their free form nanoparticles can be released into the air or water during production, or production accidents, or as waste by-product of production, and ultimately accumulate in the soil, water, or plant life. Second, in fixed form, where they are part of a manufactured substance or product, they will ultimately have to be recycled or disposed of as waste.\nScrinis raises concerns about nano-pollution, and argues that it is not currently possible to “precisely predict or control the ecological impacts of the release of these nano-products into the environment.” A May 2007 Report to the UK Department for Environment, Food and Rural Affairs noted concerns about the toxicological impacts of nanoparticles in relation to both hazard and exposure. The report recommended comprehensive toxicological testing and independent performance tests of fuel additives. Risks have been identified by Uskokovic in 2007. Concerns have also been raised about Silver Nano technology used by Samsung in a range of appliances such as washing machines and air purifiers.\nLife cycle responsibility \nTo properly assess the health hazards of engineered nanoparticles the whole life cycle of these particles needs to be evaluated, including their fabrication, storage and distribution, application and potential abuse, and disposal. The impact on humans or the environment may vary at different stages of the life cycle.\nThe Royal Society report identified a risk of nanoparticles or nanotubes being released during disposal, destruction and recycling, and recommended that “manufacturers of products that fall under extended producer responsibility regimes such as end-of-life regulations publish procedures outlining how these materials will be managed to minimize possible human and environmental exposure” (p.xiii). Reflecting the challenges for ensuring responsible life cycle regulation, the Institute for Food and Agricultural Standards has proposed standards for nanotechnology research and development should be integrated across consumer, worker and environmental standards. They also propose that NGOs and other citizen groups play a meaningful role in the development of these standards.\nEnvironmental benefits of nanotechnology \nNanotechnology could potentially have a great impact on clean energy production. Research is underway to use nanomaterials for purposes including more efficient solar cells, practical fuel cells, and environmentally friendly batteries. The most advanced nanotechnology projects related to energy are: storage, conversion, manufacturing improvements by reducing materials and process rates, energy saving (by better thermal insulation for example), and enhanced renewable energy sources.\nCurrent commercially available solar cells have low efficiencies of 15-20%. Research is ongoing to use nanowires and other nanostructured materials with the hope of to create cheaper and more efficient solar cells than are possible with conventional planar silicon solar cells. It is believed that these nanoelectronics-based devices will enable more efficient solar cells, and would have a great effect on satisfying global energy needs.\nAnother example for an environmentally friendly form of energy is the use of fuel cells powered by hydrogen. Probably the most prominent nanostructured material in fuel cells is the catalyst consisting of carbon supported noble metal particles with diameters of 1-5 nm. Suitable materials for hydrogen storage contain a large number of small nanosized pores.\nNanotechnology may also find applications in batteries. Because of the relatively low energy density of conventional batteries the operating time is limited and a replacement or recharging is needed, and the huge number of spent batteries represent a disposal problem. The use of nanomaterials may enable batteries with higher energy content or supercapacitors with a higher rate of recharging, which could be helpful for the battery disposal problem.\nWater filtration and remediation \nA strong influence of nanochemistry on waste-water treatment, air purification and energy storage devices is to be expected.\nMechanical or chemical methods can be used for effective filtration techniques. One class of filtration techniques is based on the use of membranes with suitable hole sizes, whereby the liquid is pressed through the membrane. Nanoporous membranes are suitable for a mechanical filtration with extremely small pores smaller than 10 nm (“nanofiltration”) and may be composed of nanotubes. Nanofiltration is mainly used for the removal of ions or the separation of different fluids.\nMagnetic nanoparticles offer an effective and reliable method to remove heavy metal contaminants from waste water by making use of magnetic separation techniques. Using nanoscale particles increases the efficiency to absorb the contaminants and is comparatively inexpensive compared to traditional precipitation and filtration methods.\nSome water-treatment devices incorporating nanotechnology are already on the market, with more in development. Low-cost nanostructured separation membranes methods have been shown to be effective in producing potable water in a recent study.\nSee also \n- Gyorgy Scrinis (2007). ""Nanotechnology and the Environment: The Nano-Atomic reconstruction of Nature"". Chain Reaction 97: 23–26.\n- Vuk Uskokovic (2007). ""Nanotechnologies: What we do not know"". Technology in Society 29: 43–61. doi:10.1016/j.techsoc.2006.10.005.\n- Royal Society and Royal Academy of Engineering (2004). Nanoscience and nanotechnologies: opportunities and uncertainties. Retrieved 2008-05-18.\n- Tian, Bozhi; Zheng, Xiaolin; Kempa, Thomas J.; Fang, Ying;Yu, Nanfang; Yu, Guihua; Huang, Jinlin & Lieber, Charles M. (2007). ""Coaxial silicon nanowires as solar cells and nanoelectronic power sources"". Nature 449 (7164): 885–889. Bibcode:2007Natur.449..885T. doi:10.1038/nature06181. PMID 17943126.\n- Hillie, Thembela; Hlophe, Mbhuti (2007). ""Nanotechnology and the challenge of clean water"". Nature Nanotechnology 2 (11): 663–664. Bibcode:2007NatNa...2..663H. doi:10.1038/nnano.2007.350. PMID 18654395.\n- Zhang, Wei-xian (2003). ""Nanoscale iron particles for environmental remediation: an overview."". Journal of Nanoparticle Research 5: 323–332. doi:10.1023/A:1025520116015.\nFurther reading \n- Colvin VL (Oct 2003). ""The potential environmental impact of engineered nanomaterials"". Nat Biotechnol. 21 (10): 1166–70. doi:10.1038/nbt875. PMID 14520401.\n- University of California Center for Environmental Implications of Nanotechnology\n- Duke University Center for the Environmental Implications of NanoTechnology']"	['<urn:uuid:3c3952b1-638a-426b-9d46-b8efbf1517d4>', '<urn:uuid:070cf2a9-bbb6-4bca-8984-96222ad21c86>']	open-ended	with-premise	long-search-query	similar-to-document	multi-aspect	novice	2025-05-12T20:11:36.665104	17	100	2996
15	compare visitor activities earth heart park royal highland show	Earth Heart Park offers a quarter-mile heart-shaped walking path with exercise stations and garden beds with vegetables and flowers, while the Royal Highland Show features more diverse activities including livestock competitions, sheep shearing, show jumping, dog trials, falconry, tug-of-war, and shopping at market stalls.	"['Kabat’s Frontier ACE Hardware in Apache Junction recently donated and installed a 200-foot- long chain link fence and gates at Earth Heart Park on the campus of Horizon Health and Wellness, 625 N. Plaza Drive.\nA group of community volunteers recently spent several months making improvements to the 24 garden beds in the park, including new plantings of vegetables and flowers. Unfortunately, desert wildlife has been raiding and romping in the garden, destroying the community garden and new irrigation system, according to a release.\nWhen Horizon reached out to Kabat’s Ace Hardware, 725 W. Apache Trail in Apache Junction, for a quote on fencing to protect the garden area, the business stepped up and offered to donate a chain link fence, gates, posts and related hardware. They also volunteered to install the fencing, the release states.\nOn a warm and sunny Saturday morning, the Kabat’s Ace Hardware team joined Earth Heart Park Garden volunteers and installed the fencing and gates. They even brought out a grill to feed all the volunteers who pitched in, which made for a fun and collaborative project.\nThe new fence will allow community visitors of the park to enjoy the garden beds filled with a variety of unique vegetables and flowers, while protecting the garden from desert wildlife. Kabat’s Frontier Ace Hardware has a history of being an active and supporting partner of Horizon Health and Wellness by donating their time and money for Horizon’s community events.\n“We are grateful for the fence and installation donation made by Kabat’s Frontier Ace Hardware,” Laura Larson- Huffaker, CEO of Horizon Health and Wellness, said in the release. “Their efforts will allow Horizon to continue to share healthful produce from the gardens with our patients and community.”\n“We at Kabat’s Ace Hardware are firm believers in ‘You Get What You Give’ philosophy,” Joseph Burks, business development manager of Kabat’s Supply LLC, said in the release. “Being a part of the community is not only vital to the success of our business, but to the success of our entire team’s well-being. To know our community in every way we can, we create a fundamental vibe that sets us as a unique local business when presented with these opportunities to help someone or be of service to our society.”\nEarth Heart Park is a 4-acre park and community garden. Designed to promote health and wellness, it is a place for the community to enjoy unique desert plants, trees, and gardens, a quarter-mile walking path in the shape of a heart, and exercise stations and benches around the walking path.\nWith the new installation of the donated fencing and gates, the community can benefit from fresh and healthy produce harvested from the garden beds.\nFor more information on Horizon Health and Wellness, go to hhwaz.org.\nWhen Horizon reached out to Kabat’s Ace Hardware, 725 W. Apache Trail in Apache Junction, for a quote on fencing to protect the garden area, the business stepped up and offered to donate a chain link fence, gates, posts and related hardware. They also volunteered to install the fencing. (Submitted photo)\n“We are grateful for the fence and installation donation made by Kabat’s Frontier Ace Hardware.”\n— Laura Larson-Huffaker, CEO of Horizon Health and Wellness\n#KabatsFrontierAceHardware #KabatsKares #EarthHeartPark #Donate #Community #ApacheJunction #HorizonHealthAndWellness', ""What's on this page...\nThe Out About Scotland complete guide to The Royal Highland Show\nCategory: Animals, Event, Exhibition, Industrial\nSuitable for ages: 5 to 10 years, 11 to 18 years, 18+ years, 65+ years\nIdeal for: Couples, Families, Groups, Solo travellers\nI rate it: 9 out of 10\nAbout The Royal Highland Show\nThe Royal Highland Show has been celebrated as the biggest showcase of Scotland’s agriculture and countryside for well over a hundred years.\nOriginally devised in 1822 as a means to display livestock, the show has morphed over the intervening years into the premier event for farming in Scotland and now comprises a wide range of exhibitions, activities and shows.\nRemarkably, the show celebrated its 175th year in 2015 which drew in its biggest crowds to date when nearly 190,000 visitors were recorded entering the grounds to enjoy the mix of livestock, sporting events and Scottish-produced goods.\nVisitors to the show can enjoy the spectacle of competitions where sheep, goats, horses, ponies, donkeys and poultry are exhibited to be crowned best in class, while exciting competitive rural events like sheep shearing, show jumping, dog obedience trials, falconry and tug-of-war are also organised throughout the day.\nAs a family event The Royal Highland Show is probably one of the best in Scotland and you’ll find something for everyone at this enormous venue. Kids will love the animals, dads will love the machinery and mums will love the shopping – or maybe vice versa. But whatever you’re into, I’ll guarantee you’ll find it a great day out.\nThings to do at The Royal Highland Show\nThere are always impressive exhibitions of agricultural machinery at the Royal Highland Show which are a favourite with dads and kids alike, while mums can go on a shopping spree at the smaller stalls in the 13th Avenue Arcade which sell everything from sporting goods and clothes to locally produced food and drink in a huge selection of brightly-coloured market stalls.\nAlso keeping visitors entertained are a variety of different bands and musical groups, with traditional Scottish pipe and drums often playing alongside the fantastic massed ranks of military bands.\nOther events to watch at the show include record-breaking attempts at pole climbing in the Forestry Arena, while the Countryside Arena showcases marquee displays of honey, livestock and outdoor pursuits.\nThe Forge is the place to watch competitors battle against each other in horseshoe making championships while Scotland’s Larder Live is a great venue to enjoy live demonstrations and tastings of Scottish produced delicacies.\nIf you’re arriving by car you might find the car parking charges a bit steep but like the entry tickets you can get a bit of a discount if you book in advance on the website (see below for details).\nThe Royal Highland Show is a really good day out and it’ll easily keep the entire family occupied for the day. It gets extremely busy though so take my advice and get there early.\nWhat I liked about this event\n- There’s an incredible amount to see and do at the Royal Highland Show\n- The food tents are amazing\n- It’ll keep you entertained for a full day\nMy top tips\n- Food is pricey at the event. Take a packed lunch and have a picnic instead to save money\n- The crowds are pretty hectic – get there early to avoid the car parking rush\nAddress and directions map\nCar parking is available on-site and there are regular bus stops along Glasgow Road.\nPrices and opening times\nGates open: 7.30am\nThe Royal Highland Show runs on the 20th, 21st, 22nd and 23rd June 2109. You can choose to visit the show on any day you like as all tickets are single entry to any one show day.\nTickets and car parking can be purchased in advance or on the day.\nGate Ticket Price\nAdult (aged 16+): £29.00 (single entry to any one Show Day), or £24 if booked in advance.\nChildren aged 15 or under will be admitted free of charge when accompanied by a paying adult.\n£10.00 per car per day or £8 if booked in advance.\nTravelling by bus and tram in Edinburgh:\nEdinburgh has a world-leading bus and tram network thanks to the services provided by Lothian buses, with cheap public transport available on clean, well-maintained vehicles. The bus network extends right through Edinburgh and out to the surrounding areas, while the trams provide a fast mode of transport from over 14km from the airport into the city centre.\nVisit Transport For Edinburgh for more information on Edinburgh’s Trams and buses or download the Transport for Edinburgh App. To help you find your way around the bus network more quickly you can get real-time information on the web and on your smartphone.\nTo find out when your bus is due go to:\nGetting there: Bus stop nearby, Car park on-site, Train station nearby\nGetting around: Disabled access, Pushchair access, Uneven paths (on the field)\nOn-site conveniences: Gift shop, Hot drinks, Picnic area, Restaurant or cafe, Snacks, Toilets""]"	['<urn:uuid:d93087a0-994e-4da8-83d9-fecd2a648082>', '<urn:uuid:19d9c194-fb23-45c0-900f-daf9326b7d32>']	factoid	direct	long-search-query	similar-to-document	comparison	expert	2025-05-12T20:11:36.665104	9	44	1388
16	Who was Rhadamanthys and what makes modern solar farms environmentally friendly?	Rhadamanthys was a mythological figure who served as a judge in the afterlife, appointed by Zeus to judge the souls of the dead in the meadow at the dividing road leading to either the Isles of the Blest or Tartarus. He was known for his just and blameless nature, and according to myths, he was wed to Hercules' mother Alcmene in the Elysian Plains. As for solar farms, they can be environmentally friendly through innovative land management approaches that increase pollinator habitat, improve soil quality, and support wildlife. They can host native wildflowers and prairie grasses that attract beneficial creatures like bees, flies, bats, birds, and butterflies, while also allowing for dual uses such as livestock grazing and mushroom cultivation.	"['Kinaithon, fr 1 PEG (Poetarum Epicorum Graecorum 1, ed. A. Bernabé , p. 116):\nKinathon in his epic made Rhadamanthys the child of Hephaistos, Hephaistos the child of Talos, and Talos the child of Kres (translation by Nick Gardner).\nOd (Homer, Odyssey) 7.322-24:\nAye though it be even far beyond Euboea, which those of our people who saw it, when they carried fair-haired Rhadamanthus to visit Tityus, the son of Gaea, say is the furthest of lands (original Greek).\nOd (Homer, Odyssey) 4.564:\nBut to the Elysian plain and the bounds of the earth will the immortals] convey thee, where dwells fair-haired Rhadamanthus, and where life is easiest for men (original Greek).\nHes fr 141 MW (Fragmenta Hesiodea, eds. R. Merkelbach and M.L. West , pp. 68-67):\n…and crossed the salty water…conquered by Zeus’s tricks. [And] the father [mingled with her in love] and gave a gift, a golden necklace that Hephaisos famed for his art…with his [know]ing wits…bringing [it to his fath]er; and he received the gift;…to the [daughter] of illustrious Pheonix…to the slender-ankled Europa he was about to…the father of gods and men…from beside the fair-haired maid. [And she bore sons] to the exceedingly mighty son of Kronos…commanders of many men, [lordly Minos] and just Rhadamanthys [and divine Sarpedon], noble and powerful…counselor Zeus distributed…ruled over [wi]de Lykia…well-inhabited cities…and much honor followed him…great-hearted sheperd of the people…of speech-possessing humans…counselor Zeus loved [him]…and he selected a great host…allies to the Trojans…experienced in war…showing forth omens on the le[ft…Zeus] knowing imperishable counsels…throwing round…it was a portent from Zeus…of man-slaying Hektor…caused woes…to the Argives (translation by Nick Gardner).\nTo the more part of men this is the one virtue, to be rich; all else, it would seem, is nothing worth, not though thou hadst the wisdom of great Rhadamanthus (original Greek)\nPy (Pindar, Pythian 2) 73-74:\nBut Rhadamanthys has prospered, because his allotted portion was the blameless fruit of intelligence, and he does not delight his inner spirit with deceptions (original Greek).\nOl (Pindar, Olympian 2) 74-77:\nWith these wreaths and garlands of flowers they entwine their hands according to the righteous counsels of Rhadamanthys, whom the great father, the husband of Rhea whose throne is above all others, keeps close beside him as his partner (original Greek).\nIbykos, 309 PMG (Poetae Melici Graeci, ed. D. L. Page , p.157):\nIbykos says that Talos became the lover of Rhadamanthys the just (translation by Nick Gardner).\nAnd I began from my greatest offspring, birthing Minos…[next] Rhadamanthys, who of my children is undying; but he does not live in my eyesight, and that which is not present does not hold pleasure for loved ones (translation by Nick Gardner).\nPlato, Gorgias (523-24):\nGive ear then, as they say, to a right fine story, which you will regard as a fable, I fancy, but I as an actual account; for what I am about to tell you I mean to offer as the truth. By Homer’s account,1 Zeus, Poseidon, and Pluto divided the sovereignty amongst them when they took it over from their father. Now in the time of Cronos there was a law concerning mankind, and it holds to this very day amongst the gods, that every man who has passed a just and holy life departs after his decease [523b] to the Isles of the Blest, and dwells in all happiness apart from ill; but whoever has lived unjustly and impiously goes to the dungeon of requital and penance which, you know, they call Tartarus. Of these men there were judges in Cronos’ time, and still of late in the reign of Zeus—living men to judge the living upon the day when each was to breathe his last; and thus the cases were being decided amiss. So Pluto and the overseers from the Isles of the Blest came before Zeus with the report that they found men passing over to either abode undeserving. [523c] Then spake Zeus: “Nay,” said he, “I will put a stop to these proceedings. The cases are now indeed judged ill and it is because they who are on trial are tried in their clothing, for they are tried alive. Now many,” said he, “who have wicked souls are clad in fair bodies and ancestry and wealth, and at their judgement appear many witnesses to testify that their lives have been just. Now, the judges are confounded not only by their evidence [523d] but at the same time by being clothed themselves while they sit in judgement, having their own soul muffled in the veil of eyes and ears and the whole body. Thus all these are a hindrance to them, their own habiliments no less than those of the judged. Well, first of all,” he said, “we must put a stop to their foreknowledge of their death; for this they at present foreknow. However, Prometheus has already been given the word [523e] to stop this in them. Next they must be stripped bare of all those things before they are tried; for they must stand their trial dead. Their judge also must be naked, dead, beholding with very soul the very soul of each immediately upon his death, bereft of all his kin and having left behind on earth all that fine array, to the end that the judgement may be just. Now I, knowing all this before you, have appointed sons of my own to be judges; two from Asia, Minos and Rhadamanthus, [524a] and one from Europe, Aeacus. These, when their life is ended, shall give judgement in the meadow at the dividing of the road, whence are the two ways leading, one to the Isles of the Blest, and the other to Tartarus. And those who come from Asia shall Rhadamanthus try, and those from Europe, Aeacus; and to Minos I will give the privilege of the final decision, if the other two be in any doubt; that the judgement upon this journey of mankind may be supremely just.\n“This, Callicles, is what I have heard and believe to be true; [524b] and from these stories, on my reckoning, we must draw some such moral as this: death, as it seems to me, is actually nothing but the disconnection of two things, the soul and the body, from each other. And so when they are disconnected from one another, each of them keeps its own condition very much as it was when the man was alive, the body having its own nature, with its treatments and experiences all manifest upon it. For instance, [524c] if anyone’s body was large by nature or by feeding or by both when he was alive, his corpse will be large also when he is dead; and if he was fat, it will be fat too after his death, and so on for the rest; or again, if he used to follow the fashion of long hair, long-haired also will be his corpse. Again, if anyone had been a sturdy rogue, and bore traces of his stripes in scars on his body, either from the whip or from other wounds, while yet alive, then after death too his body has these marks visible upon it; or if anyone’s limbs were broken or distorted in life, these same effects are manifest in death. [524d] In a word, whatever sort of bodily appearance a man had acquired in life, that is manifest also after his death either wholly or in the main for some time. And so it seems to me that the same is the case with the soul too, Callicles: when a man’s soul is stripped bare of the body, all its natural gifts, and the experiences added to that soul as the result of his various pursuits, are manifest in it. So when they have arrived [524e] in presence of their judge, they of Asia before Rhadamanthus, these Rhadamanthus sets before him and surveys the soul of each, not knowing whose it is; nay, often when he has laid hold of the Great King or some other prince or potentate, he perceives the utter unhealthiness of his soul, striped all over with the scourge, and a mass of wounds, the work of perjuries and injustice;\n*Antoninus Liberalis 33 = Pherekydes of Athens 3F84 (Die Fragmente der griechischen Historiker, ed. F. Jacoby, 2d ed. ) *p#*\nAP (Palatine Anthology [Greek Anthology]) 3.13:\nHeracles leading his mother Alcmene to the Elysian Plains to wed her to Rhadamanthys, and his own reception into the number of gods.\nBold Heracles gave this his mother Alcmene in holy wedlock to Rhadamanthys.\nEdited by Nick Gardner, Graduate Research Assistant, Department of Classics, Univ. of Georgia, April 24, 2016.', ""Technological advances have transformed the solar energy industry in recent years. Solar panels are significantly more efficient, producing more power in the same amount of space. Meanwhile, prices continue to fall, reducing the cost of solar electricity.\nBut with the introduction of new technologies comes uncertainty. Which solar panels are the most reliable and durable? What technology creates the least amount of pollution in the manufacturing process? Do panel manufacturers use recycled components or provide solar panel recycling options at the end of life? Let’s explore some of these critical issues in the pursuit of the best solar panels on the market.\nWe compared the efficiency, warranty, environmental performance, and more of the following solar panel models in the comparison chart below.\nU.S. solar generation capacity is soaring. Construction began recently on the Samson Solar Energy Center, the largest planned solar energy farm in the United States. When completed, the solar farm will have 1,013 megawatts of generating capacity. This solar farm will be considerably larger than the 690 MW Gemini solar project with battery storage under construction outside of Las Vegas, which had been the largest project in the U.S.\nInvenergy is developing the Samson solar farm, which will span three counties in Northeast Texas near the Oklahoma border. The project will create an estimated 600 construction jobs and $450 million in tax revenue and landowner lease payments. Developers are planning construction in five phases, with a 2023 expected completion date.\nTexas is a leader in renewable energy production in the U.S. due to its excellent solar and wind energy resources. It leads in the nation in installed wind energy capacity and trails California for installed solar energy capacity.\nWho will purchase the energy?\nCorporations have already signed virtual Power Purchase Agreements (PPAs) for Samson’s solar electricity. “The Samson Solar Energy Center is the latest example of what can be achieved when companies and utilities seek an innovative partner to meet their sustainability goals and invest in a clean energy future,” said Ted Romaine, senior vice president of origination at Invenergy.\nThe largest share, 500 MW, will go to AT&T, which says it will be the biggest corporate U.S. solar deal to date. This agreement is a big step forward in AT&T’s goal to be carbon neutral by 2035. The corporation’s multi-facet plan includes transitioning to a low-emissions fleet, increasing energy efficiency, and purchasing carbon offsets.\nIn addition, Honda has an agreement for 200 MW, McDonald’s for 160 MW, and Google for 100 MW. AT&T, Google, and Honda have already been leaders in renewable energy use. According to the Renewable Energy Buyer’s Alliance Top U.S. Energy Buyers of 2019, Google ranks second, AT&T third, McDonald’s ninth, and Honda tenth.\nTech giants have helped lead the way with sourcing renewable energy, due in part to consumer and investor concern over dirty energy powering data centers.\nBy Sarah Lozanova, Solar Energy Writer\nA big concern with large-scale solar farms is the impact on land use. Solar developers often site projects on agricultural land that is taken out of production. Also, the vegetation around solar panels needs to be maintained to prevent shading. In some cases, herbicides are used, contaminating waterways, and mowing generates pollution. If the developer applies gravel or plants turfgrass, the land has little wildlife value.\nAs the local food movement gains steam, isn’t it counterproductive to turn productive cropland into an energy plant? How can the solar energy industry embrace biodiversity while producing clean energy? Is dual use of a solar site possible?\nSolar farms can be managed to increase pollinator habitat, improve soil quality, and even for livestock grazing. Innovative land management approaches enable solar projects to serve multiple purposes, benefitting the local economy. Keeping honeybees, grazing sheep, and even cultivating mushrooms can all complement a solar energy project.\nNative Wildflowers Boost Pollinator Habitat\nResearchers with the Argonne National Laboratory are examining the economic benefits of establishing native vegetation, including wildflowers and prairie grasses, on nearby cropland. Native vegetation attracts crucial critters like bees, flies, bats, birds, wasps, moths, and butterflies, which can be beneficial to crop yields.\nResearchers with the Argonne National Laboratory are examining the economic benefits of establishing native vegetation on nearby cropland, including wildflowers and prairie grasses. A diverse array of native plants benefits wildlife diversity, especially pollinators. These crucial critters include bees, flies, bats, birds, wasps, moths, and butterflies, and can be beneficial to crop yields.\nImage Credit: Danny Piper of Sundog Solar\nBy Sarah Lozanova, Solar PV Writer\nThere is now enough installed solar energy capacity in the U.S. to power 13.5 million homes, and this amount is expected to double in the next five years. The solar energy industry is part of a very dynamic market. Many factors — including government policies, fossil fuel costs, solar energy technology advances, commodity prices, and even public awareness of the climate crisis — impact solar energy deployment across the globe.\nWhat’s in store for the year ahead? Let’s explore some trends in solar energy to better understand what is on tap for 2020.\nSolar Battery Prices Are Falling\nSolar energy is an intermittent energy source. This means that solar panels produce power when the sun is shining and not when it isn’t. Energy storage allows the solar system to supply power when the sun has set or in cloudy weather, expanding the capabilities of solar energy systems.\nThere are two main types of solar batteries: lead-acid batteries (like you have in your car) and lithium-ion batteries. The latter is far more advanced, longer-lasting, and requires less maintenance. Not surprising, lithium-ion batteries have a higher upfront cost, but the price has been decreasing significantly in recent years. The cost of lithium-ion battery storage fell 35 percent from the first half of 2018 to now (December 2019) and 76 percent since 2012. This downward price trend is good news for renewable solar energy in 2020 — and it’s likely to continue.\nNatural gas plants are often used to meet peak energy loads because they can more easily be turned on and off than coal or nuclear power plants. Lower costs make it easier for intermittent renewable energy sources — such as wind and solar — to be cost-competitive with dispatchable fossil-fired power plants. Price decreases in utility-scale battery banks now make solar plus energy storage competitive in many areas on price alone. Battery banks can make it unnecessary to fire up power plants during times of peak demand, reducing fossil fuel consumption. The greater the capabilities of solar, the less attractive and financially viable these peaker power plants become.\nOn the residential side, more homeowners are relying on solar systems with battery storage for emergency power during grid outages than ever before. This is an especially attractive option in areas prone to extended power outages due to natural disasters or with inadequate utility infrastructure, like Puerto Rico.\nImage Credit: Sundog Solar\nTrends in Solar Energy - Clean Energy Writer\nBy Sarah Lozanova, Solar Panel Writer\nThe U.S. has more than 2 million solar installations. This means there are tens of millions of solar panels on roofs and racking systems. Solar energy is fantastic for reducing carbon emissions and promoting energy independence, but what happens at the end of the panel’s 30-year lifespan?\nThere is a looming waste management issue as solar systems age and will eventually be decommissioned. Is the U.S. prepared for large-scale solar panel recycling?\n“Installations two decades ago are nearing their end of life, and that becomes a challenge for the waste industry,” says Garvin Heath, a senior scientist in the Strategic Energy Analysis Center of the National Renewable Energy Laboratory (NREL). “Because it takes a long time to develop technology and policy and solutions to dealing with end-of-life products, this is something we need to start to address today.”\nAccording to Heath, solar panels could comprise more than 10 percent of global electronic waste by 2050.\nSolar panel recycling presents an economic opportunity and can spawn new industries. A study by the International Renewable Agency (IRENA) estimates that by 2050, $15 billion could be recovered from recycling solar panels. There are also repair and reuse opportunities for solar panels that fail prematurely. These repaired solar panels are often sold at a discount, creating opportunities in new markets where affordability is an issue.\nWhat Parts of the Solar Panel Can Be Recycled?\nGlass, plastic, aluminum, and silicon comprise 99 percent of the silicon-based solar panels.\nImage Credit: Nichole McClure\nBy Sarah Lozanova\nGreater energy independence, freedom from fluctuating energy prices, and environmentally friendly living are alluring concepts that motivated my family to examine our housing and our lifestyle. We recently purchased a high-performance home and installed a solar system, making our home net-zero. We now produce as much power as we use over the course of a year.\nRealizing the Dream of a Net-Zero HomeTo realize the dream of a net-zero home, we bought a superefficient home atBelfast Cohousing & Ecovillage, a 36-unit multigenerational community in Midcoast Maine with triple-pane windows and doors, virtually airtight construction, a solar orientation and lots of insulation. The sun, appliances and occupants provide a majority of the heat needed to keep our home cozy.\nOn sunny winter days, our heaters remain off, as the sun gradually warms the house. Electric baseboard heaters kick on as needed, primarily at night or on cold, cloudy days. The home is all electric—with an electric range, hot water heaters and space heaters. Because we don’t use propane, natural gas or heating oil, a solar system can produce all the energy that our home consumes.\nBy Sarah Lozanova\nThe U.S. has enough installed solar energy capacity to power 4.6 million homes. Solar energy accounted for 32 percent of total new power generation in 2014, exceeding coal and wind energy but lagging behind natural gas. In just nine years, the installed cost of solar energy has fallen by more than 73 percent – setting up the industry for explosive growth.\nTriplePundit spoke with Vikram Aggarwal, founder and CEO of EnergySage, the so-called “Expedia of solar,” about solar energy trends and what to expect for 2016 in the residential market.\n1. Unprecedented boom continues\nLast year, analysts predicted that solar would grow by 57.4 gigawatts in 2015. The recent five-year extension of the investment tax credit (ITC) in the U.S. for both residential and commercial installations further enhances the growth trend. Now that solar manufacturing capacity has expanded significantly, the price of solar equipment has plummeted – making solar energy cheaper than grid-supplied power in many markets.\n“The residential solar market is a vibrant $7 billion industry, and on track to generate more revenue by year-end 2016 than Major League Baseball,” Aggarwal said. “The economics of solar are rapidly changing for solar shoppers, installers and financiers alike.”\nGoogle is leading the clean-energy revolution like no other company. It has invested in 22 renewable energy projects to date. In fact, Google is the biggest corporate purchaser globally of renewable energy, with a hand in utility-scale wind and solar projects that span the globe. Google has a goal to power 100 percent of its operations from renewable energy, and it is well on its way.\n“We’re really trying to lead this transition to a cleaner energy economy,” said Michael Terrell, principal for energy and infrastructure at Google. “It’s transforming anyone who touches the energy space. It’s not just about data centers or tech companies.”\nThe Google approach to renewable energy is not unlike how many utilities purchase power. It often enters into power purchase agreements: long-term financial agreements, typically with wind farms, to buy power. The projects that Google has been involved with span the globe, including in Sweden, Iowa, Oklahoma and California, along with a recent $12 million investment in the largest solar energy project in South Africa.\nby Sarah Lozanova\nInstalled solar energy capacity in the U.S. is growing dramatically, with numerous record-shattering years in a row. There is now enough installed solar energy to power over 4.6 million U.S. homes and a new solar project is installed every two minutes. Meanwhile, the cost of solar has fallen significantly, helping to fuel this unprecedented growth.\nSolar energy (r)evolution\nSince 1998, the cost of residential and commercial solar photovoltaic (PV) systems has fallen every year by an average of 6 to 8 percent, according to the National Renewable Energy Laboratory. Since 2006, the installed cost of solar energy has dropped more than 73%. Once a market dominated by environmental motives, many people are now installing solar PV to save money.\nThis 36-unit community may be the nation's first planned development built around Passive House green building standards.\nBy Sarah Lozanova\nEven from the layout of the homes, visitors can tell something is unique about Belfast Cohousing & Ecovillage. “Where are the driveways?” one guest asks. “How strange, these houses don’t have any driveways!”\nBelfast Cohousing & Ecovillage (BCE) is a 36-unit intentional community on 42 acres in Midcoast Maine. Members designed the community from 2008 to 2011, before breaking ground in 2011; GO Logic, a Belfast-based design-build firm that specializes in sustainable building, designed the units and site plan and served as general contractor.\nThe homes are clustered, and a pedestrian path, not a road, runs through the six-and-a-half-acre built area. Despite being a rural property, all the homes are located in two- to four-unit buildings and range from 500 to 1,800 square feet with one to three bedrooms. The community layout encourages social interaction, offers safety for children, and provides open space for food production, wildlife and recreation. With PV solar systems, these highly efficient homes are near net zero.\nPASSIVE HOUSE DESIGNS\nWhen one enters the homes, it becomes obvious that the lack of driveways is only one of many differences between these houses and the average code-built home. Despite being located in Midcoast Maine, the houses have no furnaces.\nFreelance renewable energy writer""]"	['<urn:uuid:e2ec6718-1ed2-48c9-9305-8e24da8eb78a>', '<urn:uuid:fb5ea0d7-f2a4-4041-be77-0275d0442b33>']	open-ended	with-premise	concise-and-natural	similar-to-document	multi-aspect	novice	2025-05-12T20:11:36.665104	11	120	3754
17	I'm curious about Iceland's economy. What are its main industries?	Iceland's economy relies heavily on fishing in the waters of the Atlantic Sea and the Arctic Ocean. The country also benefits significantly from a thriving tourism industry.	['The Atlantic Ocean positions as the second most extensive sea worldwide, occupying an area of 41.1 million square miles, and for that reason dealing with 20% of the Planet’s surface area. The Atlantic Sea possesses the best brackish water of all the seas on earth, with the best salinity developing near the equator. It approximately formed into an s-shape. The ocean lines the Americas to the west as well as Europe and Africa to the eastern.\nThe Atlantic Sea is property to numerous isles that are actually spread throughout the sea. Several of the isle countries situated on the Atlantic are Cuba, the United Kingdom, Iceland, Sao Volume and also Principe, the Bahamas, Peninsula Verde, Ireland, Trinidad and also Tobago, as well as Barbados.\nPeninsula Verde is actually an island nation being composed of 10 excitable isles found in the core location of the Atlantic Ocean. The islands of Cape Verde occupy an overall region of 1,500 square miles.\nMany of the isles in Peninsula Verde encounter dry weather due to their distance to the Sahara Desert. Just a few of the islands are actually agriculturally efficient.\nThe United Kingdom is an individual nation settled off the coastline of mainland Europe. The country is comprised of the isle of Wonderful Britain, the northern location of the Isle of Ireland, and also some other small islands. The United Kingdom is greatly swallowed up by the Atlantic Ocean, the North Sea, the Celtic Sea, and also the Irish Ocean.\nIt occupies an area of 93,600 square kilometers. The country’s coast is 11,073 miles long. The country throws a population of over 66 million citizens. The UK is actually a union four nations particularly the Northern Ireland, Wales, Scotland, as well as England. The country’s weather is temperate with rainfall throughout the year.\nIceland is an isle nation found on the North Atlantic Sea and occupies a region of 40,000 square kilometers. It is Europe’s 2nd biggest isle after the UK. Iceland is actually included one main isle as well as near to 30 little islands. Its populace is approximated at 350,000 people.\nIceland depends on angling in the encompassing waters of the Atlantic Sea and the Arctic Ocean. The country likewise considerably advantages from the vibrant tourist market.\nSao Tome and also Principe\nSao Tome as well as Principe is an African country positioned on the Atlantic Ocean and also off the West African shore. The isle nation is actually consisted of both major islands Sao Tome and Principe. The country’s complete location is actually 386 straight kilometers that makes it the second tiniest country in Africa.\nThe isle country is actually house to a predicted 200,000 locals. Sao Tome and also Principe take pleasure in an exotic weather because of its proximity to the celestial equator. Because of the exotic environment as well as good ground for farming, the country is actually a large manufacturer of hand, coffee, and also cacao bits.\nCuba is actually an Isle nation located at the meeting place of the Basin of Mexico, the Caribbean Ocean, as well as the Atlantic Sea. The nation is comprised of the Island of Cuba, which is the largest, the Isla de la Juventud, and also many other tiny isles. Cuba possesses an acreage of 42,426 straight miles as well as is Caribbean’s biggest island as well as the world’s 17th largest isle. A number of Cuba’s nearby conditions are Haiti, the Bahamas, Jamaica, Cayman Islands, as well as Mexico.\nCuba is residence to greater than 11.2 thousand folks. The yard on the main island primarily features huge plains and also some mountain chains in the southeast. The spot possesses a tropical temperature which is suitable for food development. Cuba is gifted along with several natural resources like natural resources, fertile land, and beautiful seashores. Fishing is amongst the biggest markets in Cuba as a result of the water resources that border the isle nation.\nThe Bahamas is actually a self-governed nation made up of additional than 700 isles and also islets in the Atlantic Sea. The isles have reduced altitudes along with the highest place in the country being actually Mount Alvernia, which advances to about 200 shoes above ocean level.\nTheir low latitude and reduced altitude create the islands warm and comfortable. The nation appreciates as several as 340 times of sun. The most significant market in the Bahamas is tourist. It accounts for 60% of the country’s GDP. Horticulture is actually an additional significant market in the nation. Crops expanded in the Bahamas feature onions, tomatoes, cucumbers, oranges, limes, and also glucose cane.\nBarbados is an Isle country settled in the Caribbean region and also on the North Atlantic Sea. The isle has a standard surface reviewed to its own surrounding islands.\nIt is located in a location risk-free from harsh hurricanes and also hurricanes. Intense weather barely impacts the isle of Barbados. The country’s tourism business is a major income source. Barbados possesses wonderful soft sand coastlines that entice tourists each year.\nTrinidad and also Tobago\nTrinidad and Tobago is a self-governed nation found in the Atlantic Sea. It is actually comprised of both major islands; Trinidad as well as Tobago, as well as several other landforms. The nation occupies a location of 1,981 straight kilometers. The isle’s landscapes largely consist of chain of mountains and expansive plains.\nTrinidad and also Tobago have an estimated populace of 1.4 thousand folks. The paired isles delight in the exotic climate. The country has a dry season that lasts 5 months and a stormy season that lasts seven months. Trinidad as well as Tobago’s main profit power generators are the oil market, manufacturing, as well as tourist. The country is among one of the most built in the Caribbean.\nCape Verde is an isle country being made up of 10 excitable isles located in the core location of the Atlantic Sea. The nation is actually brought in up of the isle of Excellent Britain, the northern location of the Isle of Ireland, and also some various other small islands. The island country is made up of the 2 major islands Sao Tome and Principe. The nation is actually comprised of the Island of Cuba, which is the largest, the Isla de la Juventud, and also lots of various other tiny islands. Cuba possesses a land area of 42,426 straight miles and also is Caribbean’s biggest isle and the planet’s 17th biggest island.']	['<urn:uuid:8751fd50-58fe-477f-a7d9-e4b895d0a676>']	open-ended	with-premise	concise-and-natural	similar-to-document	single-doc	novice	2025-05-12T20:11:36.665104	10	27	1073
18	What kind of support do small Pacific islands receive for climate adaptation, and how was the Mangaia Harbor rebuilt to be more resilient?	The Pacific Adaptation to Climate Change project, funded by the Global Environment Facility and Australian Government, provides support to 14 Pacific island countries. Under the Paris Agreement, equal weight is placed on mitigation and adaptation funding, giving Pacific islands more opportunities for adaptation funding. For Mangaia Harbor specifically, engineers worked with climate scientists to improve its resilience by moving the harbor ramp to a more sheltered location and extending the harbor platform to reduce wear from scouring backwash.	['Providing a Safe Haven\nFACING MOTHER NATURE\nan island nation at risk\nThe second largest of the Cook Islands was stripped of a vital source of trade and economic prosperity in 2005 when tropical cyclones Meena and Nancy destroyed Mangaia Harbour. The transfer of goods to the island was cut off by the natural disasters, leaving the community vulnerable and hindering economic development. These events further isolated the island from foreign and domestic markets and put livelihoods at stake. Faced with limited options to ensure delivery of essential supplies, the remote community (population 300) was forced to use air freight– which came at a very high economic cost — to transport products on and off of the island.\nBUILDING BACK WATERWAYS\na new address: avarua landing\nIn April 2014, Mangaia Harbour was given a second chance: Avarua Landing, as the locals call Mangaia Harbour, could start operating for full again after nine years of reduced capacity. A team of scientists and engineers partnered to design a more climate-resilient harbour infrastructure with improved ability to withstand rough seas, providing a more prosperous future for generations to come.\nThe idea of reducing risks from cyclones and rough seas was tested in the design of the new Mangaia Harbour. Our team of engineers worked with climate scientists to improve not only the strength and durability of the harbour structure, but to improve on its operational features. — Hon. Mark Brown, the Cook Islands Minister for Infrastructure\nminor adjustments yield powerful results\nMinor adjustments to the Mangaia Harbour dramatically improved its resilience to natural disasters: the harbour ramp was moved to a more sheltered location, and the harbour platform was extended to reduce wear and tear of scouring backwash.\na more resilient way forward\nSustainable economic development, modern infrastructure and coastal management planning will reduce the climate change risks that the Cook Islands are facing. This includes using modern methods to construct and maintain coastal infrastructure such as the harbour Avarua Landing. Including Accounting for climate change in coastal management will increase chances that coasts and shorelines are maintained and not eroded by extreme weather events and increasing sea levels. Authorities are preparing guidelines for community development that recognize the risk of climate-related disasters and support preventative, adaptive measures to reduce risk, such as building design restrictions in flood-prone areas caused by wave run up. They also recommend better infrastructure to help evacuate people when disaster strike.\nThe reconstruction provides one of the most essential infrastructure an island community can have: An efficient and operational harbour. In addition, this harbour has been built back better, to withstand extreme weather and rougher seas. The harbour is an example of what “climate proofed infrastructure” means in reality, and hopefully, it will also be a harbour for increased trade and economic prosperity for the Cook Islands. - Gabor Vereczi, UNDP-GEF Regional Technical Advisor\nSUPPORTING A MORE RESILIENT PACIFIC IS A TEAM EFFORT\na regional partnership for a more climate-ready region\nSupported by the United Nations Development Programme (UNDP), the Pacific Adaptation to Climate Change (PACC) project is safeguarding the coasts of the Cook Islands. Funded by the Global Environment Facility’s Special Climate Change Fund (SCCF) and the Australian Government, the PACC project is the largest climate change adaptation initiative in the Pacific region, with demonstration projects in 14 Pacific island countries. In the Cook Islands project execution is ensured by the Secretariat of the Pacific Regional Environment Programme (SPREP) and the Government of the Cook Islands.\nThe Cook Islands is one of many small island developing states that has faced, and will be facing, more extreme weather and increasing sea levels as a result of climate change. The Pacific Adaptation to Climate Change project, covering 14 countries, has brought national representatives together to share experiences and solutions. The Cook Island experience is one UNDP encourages to be shared, and we hope will be replicated by more countries. - Lizbeth Cullity ,UN Resident Coordinator & UNDP Resident Representative, Cook Islands, Niue, Samoa & Tokelau.\nPhoto Essay on UNDP exposure\nEnvironment and Energy News\n- 04 Jun 2015:Resilient Coast: Climate Change and coastal living in Samoa\n- 14 Apr 2015:Conservation of traditional knowledge through “replenishing” the environment\n- 20 Mar 2015:Communities get involved in climate change workshop', 'Understanding the Paris Agreement, where to from here, with S.P.R.E.P\nThe adoption of the Paris Agreement at the recent 21st Conference of the Parties (COP) to the United Nations Framework Convention on Climate Change (UNFCCC) marks a potentially revolutionary moment in the history of the battle against climate change.\nFor the first time we now have a global agreement that obliges all countries who are ‘Parties’ to the agreement to take meaningful action on climate change.\nThe main difference with the Kyoto Protocol, which had obligations for developed country Parties only – meaning that some larger developing countries, such as India, China, and Brazil, for example, had no obligation to reduce their emissions. Under this new Agreement, all countries are now required to reduce their greenhouse gas emissions and take some form of action to address climate change.\nWhat does the Paris Agreement mean for the Pacific? Agreement to limit global temperature change\nAs mentioned above, the Paris Agreement called for all parties to limit global temperature change to well below 2 degrees Celsius above pre-industrial temperature levels. To put this into perspective, scientists have noted that the global temperature has already increased by 0.7 degrees from pre-industrial levels. The Intended Nationally Determined Contributions (I.N.D.Cs) collected before COP 21 in Paris will put the world in a 2.7 degrees pathway.\nUnder the Paris Agreement, countries must take actions to ensure that the temperature increase does not exceed 2 degrees above pre industrial levels. Recognising that this level of warming would be too high for many vulnerable nations, the Paris Agreement also calls for countries to make further progress to limit warming to no more than 1.5 degrees Celsius above pre-industrial levels. This was a key ask from political leaders of the Pacific.\nEfforts aimed at the 1.5 degree target will be supported by the obligation of all countries to look at the long term picture through their low carbon emission development strategies.\nUnder the previous arrangements, the majority of climate change funds focussed on greenhouse gas emissions known as ‘mitigation’ actions, with less emphasis on adaptation funding which is an important source of funding for the Pacific, to help address the impacts of climate change. Under the new Agreement, equal weight will be placed on mitigation and adaptation funding, which will bring about more adaptation funding opportunities to the Pacific.\nSupport for Adaptation\nWhile the Kyoto Protocol focused almost purely on mitigation, the Paris Agreement adopted a global goal for adaptation – to enhance the capacity to adopt, strengthen resilience against impacts and reduce vulnerability to climate change. This has elevated the status of adaptation giving it equal importance as an issue. This is particularly beneficial to countries in the Pacific, who are amongst the most vulnerable to climate change.\nA mechanism to address Loss and Damage caused by Climate Change\nThe Paris Agreement also provides for a mechanism to address loss and damage, caused by the impacts of climate change. While previous COP’s had agreed to such a mechanism known as the Warsaw International Mechanism on Loss and Damage, the danger was that COP decisions could be over turned and may not be permanent. This new mechanism in the Paris Agreement spans issues such as insurance for climate change related events, and how to deal with the displacement of people due to climate change.\nWhat are the obligations of countries under this Agreement?\nUnder the Paris Agreement there are different sets of obligations - some for all countries, some for developed countries only, and some for developing countries such as those in the Pacific island region.\nAll countries will now have to formulate national strategies to reduce their emissions of climate changing gasses - mitigation against climate change. These will be in the form of ‘Nationally Determined Contributions’ where governments decide at the national level how they might best do this, for example, through their energy or transportation sectors, or in comes cases, through the protection of national forests which take carbon out of the atmosphere and store it.\nAll countries will be required to report against the actions that they take to both mitigate against climate change, and also what they are doing to adapt to climate change.\nThis information will be used in a Global Stocktake exercise held every five years. This ‘Global Stocktake’ will be used to measure progress against limiting the global temperature, and also to inform future discussions on whether or not more action is needed. Furthermore, all countries will have to formulate longer term strategies aimed at low carbon emission development.\nDeveloped countries will have much more stringent reporting requirements, and are requested to provide additional resources such as financial, technology and capacity building, to assist developing countries to meet their climate change obligations.\nDeveloping countries, like the Pacific islands will be provided with the assistance to carry out their obligations and to develop and implement strategies to cope with the impacts of climate change.\nThis demonstrates the willingness from the international community to tackle the global issue of climate change. The challenge now is for the region to be proactive and take the necessary steps for ratification and implementation of the Paris Agreement.\nThe Secretariat of the Pacific Regional Environment Programme (SPREP) will continue to work closely with its member countries, regional organisations and development partners to provide direct support to the implementation of this agreement as follows:\n• Assist Pacific Island Countries to sign on to ratify the Paris Agreement\n• Develop national strategies to reduce the emissions of greenhouse gases which are referred to as Nationally Determined Contributions\n• Provide assistance for countries to report against progress of meeting the goals of these strategies, every five years\n• Work with members to develop long term low carbon development strategies\n• Provide on-going support to adaptation and resilience building programmes\n• Work with countries to complete their Adaptation Communications which all countries are required to produce\n• Assist countries to access the financial opportunities provided by the Agreement, including mitigation and adaptation funding under the Global Environment Facility, Adaptation Fund and Green Climate Fund\nTo learn more about the Paris Agreement please access the below. You can also contact Dr Netatua Pelesikoti the Director of the Climate Change Division of SPREP at [email protected]\n• You can read the Paris Agreement here or visit http://unfccc.int/files/home/application/pdf/paris_agreement.pdf\n• To read the Decision adopting the Paris Agreement please click here or visit http://unfccc.int/files/home/application/pdf/decision1cp21.pdf\n• To learn more about The Next Steps for the Paris Agreement please click here or visit http://unfccc.int/files/meetings/paris_nov_2015/application/pdf/paris_agreement_next_steps_post_adoption.pdf\n• For a copy of All COP 21 Outcomes please click here or visit http://unfccc.int/meetings/paris_nov_2015/session/9057.php']	['<urn:uuid:a598c6c8-b77d-49b1-b902-4ef7c16e0f40>', '<urn:uuid:2a00a85d-3d08-43e4-a00b-750bc8865644>']	factoid	direct	verbose-and-natural	distant-from-document	multi-aspect	novice	2025-05-12T20:11:36.665104	23	78	1810
19	As a nutritionist, what are the health benefits of eating fish regularly?	Based on scientific evidence, eating fish provides remarkable health benefits. According to a Harvard School of Public Health meta study, fish consumption reduces the risk of dying from heart disease by 36 percent, which could save approximately 266,000 lives annually in the U.S. The Dietary Guidelines Advisory Committee (DGAC) recommends eating 8 ounces of fish weekly (26 pounds per year) to achieve these favorable health outcomes.	['Two facts about our planet are on a collision course. The amount of land available for growing food on earth is fixed, or mostly so, at about 38 percent of the planet’s non-ocean surface. That land needs to feed a worldwide population expected to grow from seven to nine billion in the next 25 years.\nNot only will there be more people but their composition will change. Wealth is growing rapidly and, with expanding wealth, comes a change in peoples’ desire for what’s on the dinner table. These imminent increases, wealth and population, will place unprecedented demands on our food production resources.\nTo avert potentially catastrophic food shortages, the UN Food and Agriculture Organization (FAO) says we will need 70 percent greater food production by 2050. Others say the needed expansion is a doubling. However, agriculture currently uses about 70 percent of the world’s water along with 38 percent of the world’s land. Neither can be increased by the amount required. It’s clear that our agricultural future is not our current agricultural practice. We need systemic changes in food production.\nHow do we create a future that is not a Hobbesian battle with continual scarcity?\nOne way could be to turn to the sea. After all, the oceans cover 70 percent of the planet but account for only 5 percent of the protein we eat worldwide. Moreover, experts on what constitutes a healthy diet agree that increasing fish consumption makes gobs of good sense.\nIn the US, every five years Dietary Guidelines (DG) are set forth jointly by the USDA and the Department of Health and Human Services. The purpose of the Guidelines is to help us all discover our path to a “healthy eating pattern”.\nEach iteration of the Guidelines begins with an extensive review of scientific literature. In the 2015 version, the authors and contributing scientists of the Dietary Guidelines Advisory Committee (DGAC) were charged to: “. . determine the current composition and quality of the American diet and areas of public health concert; trends in the Nation’s leading diet- and lifestyle-related problems; the established, measurable pact of overall dietary patterns and physical activity on short- and long-term health outcomes; the most effective methods of improving dietary patterns and physical activity to achieve favorable health outcomes in Americans 2 years and older; and sound strategies to help promote a healthy, safe affordable and sustainable food supply.”\nIn meeting the need to, “. . . achieve favorable health outcomes”, the DGAC emphatically recommends we alter our diets to include more fish.\nThe favorable health outcomes of eating fish are remarkable. For instance, in a Harvard School of Public Health meta study, scientists found that fish consumption “reduces the risk of dying from heart disease by 36 percent”. This aggregates to some 266,000 lives saved annually in the U.S.\nTo gain the benefits of fish, the DGAC recommends we eat 8 ounces weekly or 26 pounds per year.\nThis is considerably more than our current intake. The 2013 per capita fish consumption in the US was but 14.5 pounds per year. The demand is clear but how we meet it isn’t. How do we provide the additional 11.5 pounds per person?\nThis, it turns out, is a difficult question.\nThe FAO undertakes a biennial oceans assessment called State of Worlds Fisheries and Aquaculture. In the 2014 edition, they conclude that 90% of wild fisheries are harvested either at or above their sustainable limit. This means there is no way wild fish supplies can keep up with increasing demand. Most likely, they should probably decrease to allow challenged stocks to replace themselves.\nThis puts us in a bit of a pickle: we need to reduce the amount of fish we capture and we need to eat more fish.\nThe former demands that we immediately decrease the pressure on wild fish through approaches such as the creation of marine protected areas and changing fisheries policy. Such measures, though, will diminish the amount of wild caught fish available to us for the foreseeable future. It is neither feasible to capture more wild fish nor is it realistic to expect harvests to remain at their current levels.\nIt’s clear: if we are going to continue to eat fish, we need to farm them like everything else we eat. That we raise animals on land is broadly accepted. Curiously enough, this sometimes (often?) is not an accepted practice for the ocean. However, as a central source of sustained human nutrition, it makes no more sense to rely upon hunting wild animals in the oceans than it does to rely on hunting wild animals on the land.\nThe encouraging news, however, is that the potential productive capacity of aquaculture is enormous. Let’s wander through a thought experiment. Suppose aquaculture provides the difference between our current per capita fish consumption of 14.5 pounds and the 26 pounds the DGAC recommends we should consume. If you design a hypothetical fish farm that is 16 yards deep and has 22 pounds of fish per ton of water, it requires a bit less than 9 square miles of ocean surface area to provide all that fish-11.5 pounds per year for every person in the US. These 9 miles come from US coastal waters of around one million square miles.\nAnother way to think about the hypothetical fish farm is to contrast it to Duplin County, NC. With its 822 square miles and 59,000 people Duplin County produces more hogs-about 2 million per year- than any other county in the nation. The result is about 280 million pounds of marketable pork. Our hypothetical fish farm? It produces 3.7 billion pounds of marketable filets. I’m the first to admit that this is a loose comparison, however, the one thing it does do is to call our attention to the productive capacity of aquaculture.\nAs I opened this article I mentioned that our agricultural system soon needs provide much more but without using commensurately more resources. Perhaps we even need to use less. It’s axiomatic but I’ll say it anyway. All agricultural development must use the most environmentally attentive practices available.\nThat’s good but it’s not enough if you view that sustainability is a process rather than a goal. Constant improvement is also needed. This is true for agriculture done on the land or in the ocean. In my next article, I’ll outline the challenges aquaculture faces on it’s sustainability journey.\n©Scott Nichols 2016']	['<urn:uuid:4e6a5f38-1246-4455-a5b0-be0eeb86d9cc>']	open-ended	with-premise	concise-and-natural	similar-to-document	single-doc	expert	2025-05-12T20:11:36.665104	12	66	1071
20	What is the traditional French winemaking method for making Rosé?	In French, saignée (meaning 'to bleed') is a traditional Rosé winemaking method where red grapes are crushed and the lightly colored juice is 'bled' off after just a moment of skin contact.	['Products 1-11 of 11\nItem #: 7430208371 -\nNotes: Phantom Chardonnay entices with its rich layers. Green apple and pear transform into spicy flavors of freshly baked apple pie, while barrel fermentation imparts a creamy, luscious mouthfeel.\nItem #: 7430278924 -\nWINEMAKING NOTES: Thirteen months aging in American oak imparts subtle accents of vanilla and baking spices, while maintaining fruit vibrancy. TASTING NOTES: This richly hued garnet beauty opens with notes of creme brûlée, Bing cherries, sassafras, and hints of pipe tobacco. A soft, smooth entry gives way to a plush, velvety middle and full mouthfeel. FOOD PAIRINGS: This versatile wine pairs well with a variety of foods like margherita pizza, aged white cheddar and duck breast with a raspberry...\nItem #: 7430279453 -\nWINEMAKING NOTES: Our Cabernet Sauvignon grapes lingered on the vines well into Autumn, soaking up sunfilled California days and cool, crisp nights to achieve their complex flavors. We think the wait was worth it. TASTING NOTES: Opening with dense fruit notes of dried cherry and plum compote, our Cabernet Sauvignon captivates the nose and palate. Hints of clove, nutmeg and burnt caramel are imparted through 14 months of aging in American oak barrels. Dusty tannins give way to a concentrated...\nItem #: 743020523818 -\nWINEMAKING NOTES: The deep, dark color of this wine creates an expectation of weight and purpose, however, the Essential Red’s first impressions are subtle and nuanced. Faint notes of sweet garden mint and eucalyptus tantalize the senses, while hinting at the complexity yet to come. TASTING NOTES: Upon the first sip, the rich, juicy mouthfeel takes over, expressive with flavors of blueberry and blackberry. After aging in American oak for 12 months, the wine finishes with round, plush and...\nItem #: 7430282132 -\nWINEMAKING NOTES: Classic techniques, such as cold fermentation in stainless steel tanks and reductive winemaking, showcase the crisp and vibrant character of this Sauvignon Blanc. TASTING NOTES: A lively entry of boxwood and freshly cut grass greets the senses, while ripe, tropical passion fruit softens the mouthfeel. Just a touch of Pinot Grigio combines to create a playful midpalate, while flavors of quince and lime are mouthwatering, from first sip to refreshing finish. FOOD PAIRINGS: This...\nItem #: 7430283200 -\nWinemaking Notes: Bogle winemakers have sourced fruit from the best growing regions in California for the varietal. The cool Russian River Valley and coastal Monterey hills both grow fruit of character and distinction. Complementing a core of premium Lodi fruit, the resulting wine is an elegant, classic Pinot Noir. Tasting Notes: Heady scents of ripe summer strawberries make a lovely first impression, while hints of crushed violets and sweet dried herbs resonate on the nose. Refined and...\nItem #: 7430283158 -\nWINEMAKING NOTES: Our California Chardonnay, the most popular wine we produce, is a crowd pleaser with wide appeal. With welcoming aromatics, bright and juicy apple and pear flavors, a soft and creamy texture and toasty, spicy tones of American oak, a glass of this has a little something for everyone. Handcrafted and award-winning, this bottle will delight wine drinkers with its lovely, approachable style. TASTING NOTES: Green apple and pear aromas classically characterize this wine as...\nItem #: 7430285473 -\nA Hauntingly Delicious Wine:Phantom, the mysterious apparition of ripe berry and relentless spice, returns to haunt wine lovers. Full of concentration and intensity, this wine will tease, tantalize and linger long after the last sip is gone.\nItem #: 743020523918 -\nWINEMAKING NOTES: Ripe and juicy Merlot grapes are crushed, pressed and fermented to retain their vibrant freshness, then aged for twelve months in American oak, creating a well-balanced wine known for its soft, round and silky texture. TASTING NOTES: This richly-hued garnet beauty opens with aromatics of Bing cherries, sassafras, and hints of pipe tobacco. The smooth and supple red fruits on the entry give way to a plush velvety middle, while this approachable and appetizing wine finishes...\nItem #: 7430288847 -\nWINEMAKING NOTES: Head-trained, dry farmed Zinfandel vines planted by early California pioneers have endured for generations, unwavering through a myriad of challenges and the test of time. Producing small, concentrated clusters of grapes, these vines demonstrate the reward for patience and determination. It is these noble vines that Bogle winemakers rely on to create our intriguing Old Vine Zinfandel. TASTING NOTES: Classic Zinfandel is typically characterized by spice, and this wine is no...\nItem #: 7430290519 -\nWINEMAKING NOTES: In French, saignée literally means “to bleed” and has been used as a traditional method of Rosé winemaking for centuries. Red grapes are crushed, then the lightly colored juice is “bled” off after just a moment of skin contact. This classic winemaking process has yielded Bogle’s crisp and refreshing Rosé. TASTING NOTES: True to the Provence-style of winemaking, this Rosé just faintly colors the glass with hues of soft, pale salmon. Aromatics of watermelon, strawberry patch...']	['<urn:uuid:988fd16c-5ada-4da8-92f1-fbbdec5bf6c1>']	factoid	direct	concise-and-natural	distant-from-document	single-doc	expert	2025-05-12T20:11:36.665104	10	32	810
21	how long wind rotor blades service life	Rotor blades are usually designed for a service life of 20 years	"['By J.R. Weitzenböck (Eds.)\nAs a mode of becoming a member of with financial, performance-related and environmental merits over conventional welding in a few functions, adhesive bonding of joints within the marine setting is more and more rising in popularity. Adhesives in marine engineering offers a useful review of the layout and use of adhesively-bonded joints during this demanding environment.\nAfter an advent to using adhesives in marine and offshore engineering, half one specializes in adhesive resolution layout and research. the method of choosing adhesives for marine environments is explored, through chapters discussing the explicit layout of adhesively-bonded joints for send functions and wind generators. Predicting the failure of bonded structural joints in marine engineering can also be thought of. half reports trying out the mechanical, thermal and chemical homes of adhesives for marine environments including the moisture resistance and sturdiness of adhesives for marine environments.\nWith its amazing editor and overseas crew of specialist individuals, Adhesives in marine engineering is a vital advisor for all these fascinated with the layout, construction and upkeep of bonded constructions within the marine atmosphere, in addition to proving a key resource for educational researchers within the field.\n- Provides a useful evaluation of the layout and use of adhesively-bonded joints in marine environments\n- Discusses using adhesives in marine and offshore engineering, adhesive resolution layout and research, and the layout of adhesively-bonded joints for send purposes and wine generators, between different topics\n- Reviews trying out the mechanical, thermal and chemical houses of adhesives for marine environments, including the moisture resistance and sturdiness of those adhesives\nRead or Download Adhesives in Marine Engineering PDF\nSimilar polymers & textiles books\nMan made fibers account for approximately half all fiber utilization, with functions in each box of fiber and cloth know-how. even though many periods of fiber according to man made polymers were evaluated as probably important advertisement items, 4 of them - nylon, polyester, acrylic and polyolefin - dominate the industry.\n""Written for graduate scholars, researchers, and practitioners, this booklet presents an entire advent to the technological know-how, engineering, and advertisement purposes of polymer-clay nanocomposites. beginning with a dialogue of basic strategies, the authors outline particular phrases utilized in the sphere, delivering novices with a powerful origin to the realm.\nThat allows you to adapt the homes of dwelling fabrics to their organic features, nature has built exact polyelectrolytes with notable actual, chemical and mechanical habit. particularly polyampholytes will be appropriate components to version protein folding phenomenon and enzymatic task so much of organic macromolecules as a result of the presence of acidic and simple teams.\nA desirable perception into why polymer items fail, and the way we will examine from the error of the earlier. This e-book describes many of the mechanisms of polymer degradation, and illustrates every one failure mechanism with a few case stories. This e-book used to be written with the aid of the united kingdom division of alternate and undefined.\n- Recent Advances in Smart Self-Healing Polymers and Composites\n- Handbook of antiblocking, release, and slip additives\n- Fatigue and Tribological Properties of Plastics and Elastomers\n- Application of Fracture Mechanics to Polymers, Adhesives and Composites, Volume 33\nExtra info for Adhesives in Marine Engineering\n1 Cross-section of a rotor blade (schematic). Adhesive joints are shown as hatched areas. See text for further details. 2 Requirements for adhesively bonded joints for wind rotor blades Rotor blades are usually designed for a service life of 20 years. Within this time span, materials and joints are subject to 108–109 load cycles due to wind and waves. In order to avoid fatigue cracks which may develop at such high cycle numbers, stresses need to be kept sufficiently low. This is usually referred to as high cycle fatigue (HCF).\nAttachment of bulkheads to the hull shell and hull transverse frames including the structural fillet prior to overlamination. 4 shows the structural fillet in a transverse frame. • Attachment of deck and wheelhouse longitudinal and transverse stiffeners. The RNLI typically use the top hat type but L flange type stiffeners have also been used. © Woodhead Publishing Limited, 2012 38 • • • Adhesives in marine engineering Connection of structural soles to hull/wheelhouse. This is largely by gluing the sole bearers to the hull with the soles then being glued to the bearers.\nData of a two-part methacrylate adhesive tested at different environmental conditions are depicted in Fig. 3. 2 using un-notched bulk adhesive tensile bars. The nominal tensile stress is shown. At 40°C and 80% relative humidity, the S–N curve is located beneath the S–N curve measured at 23°C, whereas at −40°C, the S–N curve is located above the S–N curve measured at 23°C. This indicates that low temperatures increase fatigue resistance and high temperatures reduce fatigue resistance with respect to room temperature.']"	['<urn:uuid:f4d151df-bfc6-460a-aba0-2aab7b9ec230>']	factoid	direct	short-search-query	similar-to-document	single-doc	novice	2025-05-12T20:11:36.665104	7	12	800
22	I'm worried about my dad's diet since he was diagnosed - what are some brain-healthy foods I should try to include in his meals?	Several foods can help reduce dementia symptoms as part of a balanced diet. Leafy green vegetables like spinach, kale, and Swiss chard are excellent sources of folate (Vitamin B9), which improves cognition and helps ward off depression. Berries contain anthocyanin that protects the brain from damage. Fish rich in omega-3 fatty acids is particularly beneficial - studies show people 65 and over who ate fish three times weekly had a 26% lower risk of brain lesions that can cause dementia. Other beneficial foods include nuts (rich in omega-3s and vitamin E), dark chocolate (contains flavanols), and spices like cinnamon and sage that can enhance cognitive processing.	['People living with Alzheimer’s or dementia often eat less than they used to. Reasons for this may include problems with chewing, swallowing or digesting food. We have compiled a list here of 8 practical tips for helping someone with dementia to eat more.\nSometimes people just lose interest in food. This can happen for a long list of reasons including loss of taste, the ability to smell, memory loss, and thinking they have already eaten. Certain medications can also affect appetite.\nEating is affected as the disease progresses and ensuring someone living with dementia eats a nutritious meal, or to eat enough, can become a real practical and emotional issue for the carer. We hope these ideas help.\nWin a SureSafe Personal Alarm!\nEnter our competition for the chance to with a Family Monitored SureSafe Personal Alarm\n1. The plate matters\nWhat colour plate are you serving food on?\nIn a study conducted at Boston University, researchers found that patients eating from red plates consumed 25 percent more food than those eating from white plates. This appears to be connected with the way someone living with dementia sees food on a plate, and if you can’t really see food on a white background you are much less likely to eat it.\nYellow plates have also shown promising results, with the NHS trailing an innovative program across three hospitals in 2016. The use of colour helps to stimulate interest in dementia patents, as often they have trouble distinguishing between colour. If the food is too close to the colour palette of the plate, dementia suffers can also struggle to distinguish the contrast between the two and realise their is food to be eaten.\nA company called Eatwell tableware have a fantastic selection of innovative table wear designer for those with dementia or motor impairment.\n2. Make eating easier\nAt some stage in the dementia patients life, there’s a good chance eating will get harder, and if so using utensils can also become more difficult. Consider finger food to help them eat more frequently, little and often.\nSome examples include:\n- Fruits (Raspberries, Strawberries, Banana and Grapes are great examples and have strong contrasting colours).\n- Nuts (Almonds and Brazil Nuts are protein packed and contain essential fats).\n- Crisps (Healthier options are available, even vegetable crisps).\n- Fish fingers, chicken pieces (within reason).\nMaking your parent feel comfortable as possible at the table should help with their eating, helping them to focus on the food.\nWhen you sit down at the table, sit directly in front of them, make eye contact, smile and wait for them to smile back at you. Then you can start eating without talking (You start first). Try to keep quiet, be patient , keep making eye contact and wait for them to follow your lead.\nIt’s important to remember that ‘being patient’ is the most important part here. You might have to do this for a while before it starts working. Hopefully the more frequently you follow this routine the easier it will become for both of you.\nHave you considered Live-in Care?\nLive-in care is a good option for older people living with dementia who need more help. Live-in care agencies can provide dementia-specialist carers who can help with eating and much more. Read our guide to the Best Live-in Care Agencies.\n3. Best food to eat for Dementia patients\nThere are lots of fads and daily “news” on the latest food to help slow down Dementia…. advice from Alzheimer’s Society and others is clear: there are foods that can help reduce some of the symptoms, but mostly it’s common sense – a healthy balanced diet – with treats of course. Some suggestions include:\nVeggies such as spinach, kale and Swiss chard are all great sources of folate, or Vitamin B9, which is shown to improve cognition in older adults. Folate helps ward off depression (a common dementia side-affect) by contributing to serotonin levels. The Vitamin E in leafy green vegetables has also shown positive affects on the brain.\nBroccoli, cauliflower, bok choi, cabbage and brussel sprouts help retain memory. They contain carotenoids and folate, which lowers levels of homocysteine, an amino acid linked with cognitive impairment.\nBerries And Cherries\nAll varieties of berries contain anthocyanin, a phytochemical that protects your brain from damage caused by free radicals, inflammation and radiation. Blueberries are packed with the most antioxidants, as well as plentiful amounts of Vitamin C and E.\nFlavanols, the antioxidant in cocoa powder, help improve blood flow to the brain. The darker the chocolate, the better for you, since you’ll be getting more flavanols and less added sugar.\nA study found that people 65 and over who ate three or more weekly servings of omega-3 rich fish had a nearly 26 percent lower risk of having brain lesions that can cause dementia, compared to those who never eat fish. The high levels of the omega-3 fatty acids eicosapentaenoic (EPA) and docosahexaenoic (DHA) keep the brain in tip-top shape.\nA small handful of nuts packs a ton of nutrients, including omega-6 and omega-3 fatty acids, Vitamin E, folate, Vitamin B6 and magnesium. These nutrients help protect against age-related memory loss, as well as work to improve mood. All varieties of nuts, including peanuts, cashews, hazelnuts, walnuts, almonds and pecans, offer these benefits.\nSeeds provide lots of Vitamin E, a vitamin associated with lower rates of agre-related cognitive decline. Choline, a compound found in sunflower seeds, helps improve brain function. The zinc present in pumpkin seeds improves memory and cognitive function, while the tryptophan fights depression. Flaxseeds are excellent alternative to fish, since they’re packed with memory-boosting omega-3s.\nCertain spices not only add flavor to your favorite dishes, but also add antioxidants and memory-boosting compounds. The mere scent of cinnamon, for instance, enhances cognitive processing. In a study, participants who consumed sage performed better on memory tests. And curry lovers can rejoice; Curcumin, a main ingredient in turmeric, has been shown to break up brain plaque and reduce inflammation that can cause memory problems.\n4. Don’t get cross while trying to convince someone to eat\nTrying to convince a person living with Alzheimer’s who is at the point of not eating, that they must eat is counterproductive. Trying to explain why is also detrimental.\nYou need to be the food guide. Your role as the guide is to show this person how to eat each and every bite, just like it’s the first time they have ever eaten. Keep using strong eye contact and a nice big smile and not disrupt the person by talking.\nIt can be frustrating when you are trying to help someone and it is not working as effectively as you may hope. It’s like teaching a child to tie their shoelaces, or of course, to eat their vegetables!\nThey will watch how you do it and slowly copy, but if you don’t show them a demonstration they are not going to be able to learn. If you find yourself becoming agitated, take a deep breathe, and have another try.\n5. Arrange the food on the plate\nYou may need to experiment with different sizes, textures and flavours of food to see which the person responds to the best. Here are some tips to help you change it up.\n- Add variety in the colour of food, different colour vegetables help to really brighten up the food.\n- Try less quantities of food and fewer individual items on the plate.\n- Think about what types of food they have always enjoyed in the past. Put it on the plate with another food right next to it\n- Cut up the food (especially meat) into small pieces\n- Change the texture of the food (potatoes could be mashed, boiled, baked for example)\n6. Praise the food\n“How is your dinner/lunch/breakfast?” – we all want to know if someone is enjoying their food. If someone is visibly enjoying their meal it encourages others round the table to dig in.\nA simple ‘This food is delicious’ can spark an interest in others and encourage them to try their food. Try this the next time you sit down together, taking the lead by eating first and giving that positive comment right away.\n7. Stop talking\nPeople living with Alzheimer’s and dementia are easily distracted and can get confused if you try to get them to multi-task. We want to make the job of eating their food to be as simple as possible and for them to feel comfortable and relaxed while doing so.\nStop talking to the person while you are eating with them, small comments about the food are beneficial but not too much. Make sure that they can focus on the task at hand, one thing at a time.\n8. Eat small all day long\nContrary to what we believe, we do not need 3 main meals a day. Research shows that there is no major differences between 3 regular meals a day, 2 large meals a day or 5 little ones.\nIn fact 5 little meals can help to regulate steady blood pressure which is an added bonus. If you can only get your parent to eat small amounts, that’s not a problem as long as this is at regular periods throughout the day. It’s all about finding what works best for you.\nRead and download the NHS’ helpful Dementia Care Guide – Support with eating and drinking (PDF). This guide talks about the common problems those living with dementia can have at meal time, and offers some tips to resolve them.\nAnother great tool that carers can use is The DMAT (Dementia Mealtime Assessment Tool) “The DMAT enables carers to assess, select interventions and generate a person centred care plan to support mealtime eating abilities and meal behaviours in people with advancing dementia.” You can learn more about the DMAT and it’s benefits on their website.\nExpert advice on Dementia with Dr Alex Bailey. In this episode of the Age Space Podcast, we talk to Dr Alex Bailey, who is an old age psychiatrist working in Westminster, sharing his thoughts and advice on Dementia. This includes details of memory services, supporting those with Dementia to live well, psychological therapies, supporting carers and much more. Please click here to listen.']	['<urn:uuid:ebf768fd-6fb8-4256-bd30-b0e71d7b34ae>']	open-ended	with-premise	verbose-and-natural	distant-from-document	single-doc	novice	2025-05-12T20:11:36.665104	24	106	1717
23	build modify maintain digital product guidelines	A design system is a collection of design rules, patterns and reusable elements that help web applications develop faster and correctly. Design systems need to be continuously updated and maintained, or they will quickly become obsolete. They help organizations preserve brand guidelines, speed up time-to-market, and improve user experience. According to experts, a design system is actually a living thing that evolves, and it's not just a component library - it requires people's participation and understanding to function effectively.	['Insurance company Fremtind organised a design system meetup on November 4th. The name of the event is Beyond Design Systems. The text below summarises the purpose of the gathering:\n“The digital product industry has been welcoming design systems for a while. Many organizations have been better able to preserve brand guidelines, speed up time-to-market, or improve the user’s experience by establishing and maintaining a functioning design system.\nThis event will attempt to look beyond: To uncover the good, the bad, and the ugly experiences from inside our design systems. What has thrilled users? Where has it gone wrong? How did we prevail – if at all?”\nBecause of Corona measures, the event was broadcasted online via company’s youtube channel. The experts who are located in different parts of the globe have been available via a YouTube URL. That was quite comforting. If you are eager to learn about design systems, you can watch the whole session below:\nwhat is a design system?\nThere are several definitions out there, but I will describe it with my words: A design system is a collection of design rules, patterns and reusable elements which help a web application develop faster and in a correct way.\nFor example, almost every web application requires a loading animation, footer, date box, etc… Most of these elements are common in every project and they all require time to build, consistent style, accessibility, validation, standard UX, UI guidelines and more. So, the design system assists your project to cover those needs at reusable parts of the final outcome. Therefore, digital product industry has been recently moving towards design systems.\n“Design systems should be built on collective culture, not individual achievement.”\nAbove is a quote from Glenn Brownlee who is the leader of design system, Jøkul. The inclusion of people are needed to make a design system work. If people do not participate, then there would be no progress. Brownlee basically underlines that a design system is actually a living thing. It evolves and the people need to understand why the system is necessary. As the process matures, actors of design system will not follow the rules just because they are told to do so, but they will see these rules as an expected way of doing. In other words, the rules will become their new normal.\nBrownlee also added: “If all you have is a component library, you don’t have a design system. However, once people become part of the movement, that means you have a design system.”\nA similar thought was also mentioned by another participant of the meetup, Mike Hall (UX Lead for the Service NSW Transactional Design System). Hall said that if a system was not updated and maintained, it would die very quickly.\nI should confess that my favourite presentator of the meetup has been Eirik Fatland (Lead Designer at Fremtind). He mostly shared some of his interesting experiences while expressing his thoughts about design systems. That was truly engaging.\n“In Fremtind, we for example have weekly design critique sessions that are important while establishing culture. In design critique session, anyone can bring any design they are working on and they receive feedback on it. Sometimes feedback leads to discussions. The discussions involve everyone in the room and they become wiser.” ~Eirik Fatland\nAnd finally, Morgane Peng had a stunning presentation. She is the director of UX at Societe Generale. I think the Dunning-Kruger effect was a perfect example while she was explaining their design system journey.\nThe Dunning–Kruger effect is a bias in thinking, usually where a person is unaware of how badly they grasp a subject, not understanding that they are failing at it. Peng says that she and her co-worker thought they built some useful tool in the beginning and afterwards they started to receive feedback and complaints from the users of their design system. Therefore, they fell to the valley of despair because of their mistakes. This experience helped them understand that building design systems is not easy. After putting more work and engagement with additional actors, they started to understand what they were actually building and before they reach to the plateau of sustainability, more partners joined them. As a result, Peng says that a design system is 20% craft and 80% people.\nAll in all, I hope there would be more interesting meetups soon. And if you are curious about design systems, I recommend you to take a look at this blog post: Everything you need to know about design systems.']	['<urn:uuid:07b3ba72-915b-44c0-8b99-60c1e319c292>']	open-ended	direct	short-search-query	distant-from-document	single-doc	expert	2025-05-12T20:11:36.665104	6	79	752
24	What methods effectively prevent bloat in cattle?	Bloat can be prevented through a combination of pasture management and antifoaming agents. The proportion of bloat-causing legumes in pasture should be 51% or less. Animals should be fed grass, hay, or tannin-containing forage before grazing high-legume pastures. Poloxalene (Bloatguard) is commonly used as an antifoaming agent in molasses licks or feed concentrates. Antibiotics and natural oils can also prevent bloat, though oils require frequent dosing. In Australia, antibloat capsules that release foam-dispersing detergent for 24 days are available.	"['The microbial fermentation of forage material in the rumen leads to the production of a large amount of gas which is then absorbed into the rumen wall, passed into the omasum or, most commonly, expelled by eructation (belching). The eructation of gas is prevented by the development of a stable viscous foam in the rumen. This foam may hold the gases in large pockets (free gas bloat) or small pockets (foamy bloat), and it may be caused by feeding a high-concentrate ration in the feedlot, by feeding legume hay, or, most commonly, by grazing pastures which contain a large portion of leguminous material.\nFailure to expel the gases held in the foam results in the rumen becoming distended so that it either presses against the diaphragm, immobilizing it, or constricts the main dorsal vessels through which blood passes to and from the heart. In either case, animal death is due to asphyxiation. Bloat is the cause of substantial losses to cattlemen throughout the world. In New Zealand, some 17,000 to 20,000 dairy cattle die from bloat each year. In the United States, it is estimated that farmers lose livestock valued at about $80 million yearly, while in Canada, annual losses are in the region of $22 million. Consequently, means of preventing bloat have been widely studied. The most common approach has been to try to prevent or reduce the development of foam in the rumen by reducing, by plant breeding methods, the amount of foam-causing substances in the plant material.\nThe saponins, which are capable of increasing surface tensions to levels that can withstand the gas pressures which develop in the rumen, were the first substances to be considered as the main causal agents of bloat. While they do cause increased and irregular respiration, which is frequently found in bloating animals, it has proved difficult to associate saponin levels with incidents of bloat.\nMacArther and Multimore, in 1966, working in British Columbia, presented evidence to show that another important foaming agent was a plant protein known as 18S. They showed that incidents of bloat were associated with high 18S levels in a range of legume species. Alfalfa contains 4.5% to 5.2% 18S protein. Other legumes known to cause bloat (red clover, white clover, sweetclover, and alsike clover) contain similar amounts. Legumes which do not cause bloat (birdsfoot trefoil and sainfoin) and the grasses contain less than 1% 18S protein. This protein substance, which is so named because its sedimentation coefficient is 18 Svedberg units, is also known as the fraction I protein. It has a molecular weight of 500,000 and, since it is one of the cytoplasmic proteins, is readily digestible. Also, any reduction in fraction I protein would seriously impede photosynthetic rates, since it consists of ribulose 1,5-diphosphate carboxylase, the main carboxylating enzyme in the C3 pathway. The heritability of this trait is high, and selection would rapidly decrease 18S protein levels; however, both quality and quantity of forage production would decline if 18S protein levels were reduced by plant breeding methods.\nStudies of 18S protein indicate that stable foam found in the rumen results from protein molecules, which are spherical when in solution, reaching the surface of the rumen fluid without being broken down by microbial action. Under these circumstances, the molecules uncoil, become insoluble, and are then capable of stabilizing the foam. Studies in New Zealand show that both fraction I and fraction II proteins (a mixture of proteins with molecular weights between 10,000 and 200,000) act in the same way to stabilize foam. Recent work has shown that certain tannins are capable of precipitating the proteins found in bloat-causing foam. The nonbloating legumes contain large amounts of these tannin substances. While the plants are alive, the tannins are held in vacuoles to prevent precipitation of the plant\'s own protein. The world collection of the genus Medicago, which is available at the University of Alberta, has been surveyed for tannin content. None of the Medicago species, nor any of the cultivars in common use, has been found to have high tannin levels. Consequently, it would seem that for the present, management, rather than plant-breeding solutions, must be used to combat bloat.\nBloat prevention may be achieved by a combination of pasture management and the use of antifoaming agents. The proportion of bloat-causing Iegumes in a pasture should be 51% or less. Before turning an animal into a pasture containing a high proportion of a bloat-causing legume (over 30%), it is wise to feed a grass, hay, or forage containing tannins (e.g., sudangrass).\nThe antifoaming agent most commonly used is poloxalene (a polyoxypropylene- polyoxyethylene block polymer), which is sold under the name Bloatguard. It is available in molasses ""licks"" or may be mixed into the concentrate part of the feed. Antibiotics will also prevent bloat, as will a number of natural oils, such as soybean oil, corn oil, peanut oil, or olive oil. However, these substances are rapidly degraded in the rumen and consequently call for large and frequent doses, which are costly to administer.\nIn Australia, antibloat capsules are available. These are 15-cm gelatin cylinders, 4 cm in diameter, which split down the middle and are hinged on one side. The capsule, which splits open in the rumen, becomes too large to be regurgitated, and releases a foam dispersing detergent at the rate of 6 g/day for 24 days.']"	['<urn:uuid:84a09a38-e2d5-4191-83e2-280a593144d3>']	open-ended	direct	concise-and-natural	similar-to-document	single-doc	expert	2025-05-12T20:11:36.665104	7	79	892
25	sucralose vs sugar dental decay mechanism	Unlike sugars, artificial sweeteners do not react with the bacteria in the mouth, which means they do not form acids and therefore do not cause tooth decay. Research specifically shows that sucralose is less likely to cause tooth decay than sugar. The European Food Safety Authority (EFSA) states that all artificial sweeteners, when consumed in place of sugar, neutralize acid and help prevent tooth decay.	"['Artificial sweeteners are often the cause of heated debate.\nOn one hand, they\'re claimed to increase the risk of cancer and negatively affect your blood sugar and gut health.\nThis article reviews the evidence on artificial sweeteners and their health effects.\nArtificial sweeteners, or sugar substitutes, are chemicals added to some foods and beverages to make them taste sweet.\nPeople often refer to them as ""intense sweeteners"" because they provide a taste that is similar to table sugar but up to several thousand times sweeter.\nBottom Line: Artificial sweeteners are chemicals used to sweeten foods and beverages. They provide virtually zero calories.\nThe surface of your tongue is covered by many taste buds. Each taste bud contains several taste receptors that detect different flavors (2).When you eat, the different food molecules contact your taste receptors.\nA perfect fit between a molecule and a receptor sends a signal to your brain, allowing you to identify the taste (2).\nFor example, the sugar molecule fits perfectly into the taste receptor for sweetness, like a lock and key, allowing your brain to identify the sweet taste.\nThe molecules of artificial sweeteners are similar enough to sugar molecules that they fit on the sweetness receptor.\nHowever, they are generally too different from sugar for your body to break them down into calories. This is why they have a sweet taste without the added calories.\nOnly a minority of artificial sweeteners have a structure that your body can break down into calories. Because only very small amounts of artificial sweeteners are needed to make foods taste sweet, you consume virtually no calories (1).\nBottom Line: Artificial sweeteners taste sweet because they are recognized by the sweetness receptors on your tongue. They provide virtually zero calories because most cannot be broken down by your body.\n- Aspartame: 200 times sweeter than table sugar. Aspartame is known under the brand names Nutrasweet, Equal or Sugar Twin.\n- Acesulfame potassium: 200 times sweeter than table sugar. Acesulfame potassium is suited for cooking and baking and known under brand names Sunnet or Sweet One.\n- Advantame: 20,000 times sweeter than table sugar, suited for cooking and baking.\n- Aspartame-acesulfame salt: 350 times sweeter than table sugar, and known under the brand name Twinsweet.\n- Cyclamate: 50 times sweeter than table sugar. Cyclamate is suited for cooking and baking. However, it\'s been banned in the US since 1970.\n- Neotame: 13,000 times sweeter than table sugar. Neotame is suited for cooking and baking and known under the brand name Newtame.\n- Neohesperidin: 340 times sweeter than table sugar. It is suited for cooking, baking and mixing with acidic foods. It is not approved for use in the US.\n- Saccharin: 700 times sweeter than table sugar. It\'s known under the brand names Sweet\'N Low, Sweet Twin or Necta Sweet.\n- Sucralose: 600 times sweeter table sugar. Sucralose is suited for cooking, baking and mixing with acidic foods. It\'s known under the brand name Splenda.\nBottom Line: Many different types of artificial sweeteners exist, but not all are approved for use everywhere in the world. The most common include aspartame, sucralose, saccharin, neotame and acesulfame potassium.\nArtificial sweeteners are often popular among individuals who are trying to lose weight.\nHowever, their effects on appetite and weight vary among studies.\nEffects on Appetite\nSome people believe artificial sweeteners might actually increase appetite and promote weight gain (5).\nThey think artificial sweeteners may be unable to activate the ""food reward pathway"" needed to make you feel satisfied after you eat (6).\nAdditionally, some scientists think you\'d need to eat more of an artificially sweetened food, compared to the sugar-sweetened version, in order to feel full.\nIt\'s even been suggested that sweeteners may cause cravings for sugary foods (5).\nIn fact, several studies have found that participants report less hunger and consume fewer calories when they replace sugary foods and beverages with artificially sweetened alternatives (14, 15, 16, 17, 18).\nBottom Line: Recent studies find that replacing sugary foods or drinks with artificially sweetened ones may reduce hunger and calorie intake.\nEffects on Weight\nWhat\'s more, choosing artificially sweetened foods instead of those with added sugar may reduce the amount of daily calories you consume.\nArtificially sweetened drinks can be an easy alternative for regular soft drink consumers who want to decrease their sugar consumption.\nHowever, opting for diet soda will not lead to any weight loss if you compensate by eating larger portions or extra sweets. If diet soda increases your cravings for sweets, sticking to water might be best (27).\nBottom Line: Replacing sugar-containing foods and beverages with artificially sweetened ones may help you lose some weight.\nThis may seem contradictory, but it\'s important to note that all of the studies are observational. They can\'t prove artificial sweeteners cause diabetes, only that people likely to develop type 2 diabetes also like to drink diet soda.\nSo far, only one small study of Hispanic women found a negative effect.\nWomen who drank an artificially sweetened drink before a sugary drink had 14% higher blood sugar levels and 20% higher insulin levels, compared to those who drank water before the sugary drink (39).\nHowever, the participants weren\'t used to drinking artificially sweetened drinks, which may partially explain the results. What\'s more, artificial sweeteners may have different effects based on people\'s age or genetic background (39).\nFor example, research shows that replacing sugar-sweetened beverages with artificially sweetened ones produced stronger effects among Hispanic youth (40).\nThis could be related to the unexpected effect seen on Hispanic women above.\nAlthough not unanimous, the current evidence is generally in favor of artificial sweetener use among diabetics. That said, more research is needed to evaluate the long-term effects in different populations.\nBottom Line: Artificial sweeteners can help diabetics reduce the amount of added sugar in their diets. However, more research is needed into the effects on different populations.\nMetabolic syndrome refers to a cluster of medical conditions including high blood pressure, high blood sugar, excess belly fat and abnormal cholesterol levels.\nThese conditions increase your risk of chronic diseases such as stroke, heart disease and type 2 diabetes.\nSome studies suggest diet soda drinkers could have up to a 36% higher risk of metabolic syndrome (41).\nOne recent study provided overweight and obese participants with either a quarter gallon (1 liter) of regular soda, diet soda, water or semi-skimmed milk each day.\nBy the end of the six-month study, participants drinking the diet soda had striking differences compared to those drinking regular soda.\nThey weighed 17–21% less and had 24–31% less belly fat, 32% lower cholesterol levels and 10–15% lower blood pressure (44).\nWater had the same benefits as diet soda, compared to regular soda (44).\nBottom Line: Artificial sweeteners are unlikely to promote metabolic syndrome. Replacing sugary drinks with artificially sweetened ones might actually decrease the risk of several medical conditions.\nYour gut bacteria play an important role in health, and poor gut health is linked to numerous problems.\nIn one recent study, the artificial sweetener saccharin disrupted the gut bacteria balance in four out of seven healthy participants not used to consuming them.\nThe four ""responders"" also showed worse blood sugar control as little as five days after consuming the artificial sweetener (53).\nWhat\'s more, when the gut bacteria from these people were transferred into mice, the animals also developed poor blood sugar control (53).\nOn the other hand, the mice implanted with the gut bacteria from ""non-responders"" had no changes in their ability to control blood sugar levels (53).\nAlthough interesting, this is the only study to date showing these effects in humans. More studies are needed before strong conclusions can be made.\nBottom Line: Artificial sweeteners may disrupt the balance of gut bacteria in some people, which could increase the risk of disease. However, more studies are needed to confirm this effect.\nA debate has raged since the 1970s about whether there is a link between artificial sweeteners and cancer risk.\nThe debate was ignited when animal studies found an increased risk of bladder cancer in mice fed extremely high amounts of saccharin and cyclamate (54).\nLuckily, the metabolism of saccharin is different in mice and humans.\nOne such study followed 9,000 participants for 13 years and analyzed their artificial sweetener intake. After accounting for other factors, the researchers found no link between artificial sweeteners and the risk of developing various types of cancer (55).\nA recent review analyzed studies that had been published over an 11-year period. It also did not find a link between cancer risk and artificial sweetener consumption (58).\nOne exception is cyclamate, which was banned for use in the US after the original mouse bladder cancer study came out in 1970.\nSince then, extensive studies in animals have failed to show a cancer link. However, cyclamate was never re-approved for use in the US (1).\nBottom Line: Based on the current scientific evidence, artificial sweeteners are unlikely to increase the risk of cancer in humans.\nDental caries — also known as cavities or tooth decay — occur when the bacteria in your mouth ferment sugar. Acid is produced, which can damage tooth enamel.\nUnlike sugars, artificial sweeteners do not react with the bacteria in the mouth. This means they do not form acids and therefore do not cause tooth decay (60).\nResearch also shows that sucralose is less likely to cause tooth decay than sugar.\nThe European Food Safety Authority (EFSA) states that all artificial sweeteners, when consumed in place of sugar, neutralize acid and help prevent tooth decay (28).\nBottom Line: Artificial sweeteners, when consumed instead of sugar, decrease the likelihood of tooth decay.\nSome artificial sweeteners may cause unpleasant symptoms such as headaches, depression and seizures, at least in some individuals.\nThis individual variability may also apply to aspartame\'s effects on depression.\nFor instance, individuals suffering from mood disorders may be more likely to experience depressive symptoms in response to aspartame consumption (67).\nBottom Line: Artificial sweeteners are unlikely to cause headaches, depression or seizures in most people. However, some individuals could be more sensitive to these effects than others.\nArtificial sweeteners are generally considered safe for human consumption (1).\nThey are carefully tested and regulated by US and international authorities to make sure they are safe to eat and drink.\nThat said, some individuals should avoid consuming them. For example, aspartame contains the amino acid phenylalanine.\nIndividuals with the rare metabolic disorder phenylketonuria (PKU) cannot metabolize it. People who have PKU should therefore avoid aspartame.\nIn addition, some people are allergic to the class of compounds that saccharin belongs to, called sulfonamides. For them, saccharin may lead to breathing difficulties, rashes or diarrhea.\nBottom Line: Artificial sweeteners are generally considered safe but should be avoided by people with phenylketonuria or those allergic to sulfonamides.\nOverall, the use of artificial sweeteners poses few risks and may even have benefits for weight loss, blood sugar control and dental health.\nThese sweeteners are especially beneficial if you use them to decrease the amount of added sugar in your diet.\nThat said, the likelihood of negative effects can vary from one individual to another.\nSome people may feel bad or experience negative effects after consuming artificial sweeteners, even if they are safe and well-tolerated by most people.\nIf you\'d like to avoid artificial sweeteners, make sure to check out these four healthy, natural sweeteners that are actually good for you.']"	['<urn:uuid:ff8b7fa8-df75-4c5a-86ca-037aa2b0e339>']	open-ended	direct	short-search-query	distant-from-document	single-doc	expert	2025-05-12T20:11:36.665104	6	65	1900
26	I'm working on attaching the limbs to my sock monkey, but I'm unsure about the correct placement of the arms and tail - where exactly should they be positioned and how should they be secured?	The arms should be placed directly under the ears. To attach them, you should fold the top under, pin each arm in position, and stitch them in place. As for the tail, it should be placed directly in the center of the Body Part Heel Red area. When attaching the tail, turn the rough edges under, pin it in place, and then stitch it. For secure attachment of all body parts, you should use Carpet Thread and sew two times around.	"['1. Use one Pair of Fox River Mills Socks.\n2. Turn socks inside out.\n3. Lay the Sock for the Body Part with the heel upright and make sure the cuff is evenly divided for the legs, pin in place along the cuff part.\n4. Use an ink marker and draw the layout for the body of the sock monkey onto the first sock, measure 1 inch inch from the heel and then mark a line down to the end of the cuff for the legs. Next put a 1 inch slit in the area where the neck will be, this is to turn right side out after sewing and cutting out. When sewing the legs make sure to sew the cuff end with a round ending so the legs are round at the end and they are more easily stuffed. Turn the sock over and where the neck area will be pick up the material and sew an almond shape across the sock this will make the head look straight forward.\n5. Turn the sock inside out, sew a line around the hole in the back of the neck of the sock monkey to keep the material from unraveling and so it will not stretch too large.\n6. You are ready to Stuff the body with Fiber Filling.\n7. Now for the reminder of the body parts, using the second sock lay it out on the side flat, pin the sides and the end. Here is where the arms are laid out with a part of the arms are the cuff, as before when marking these making the cuff ends round, the cuff end should be divided in equal halves and marked all the way to the end of the sock toe , This will be make the tail as a part of one of the arms. Make a line across the sock just below the heel part, plus a half inch around the heel of the brown that will show outside the white part. Take a water bottle and cut the top off about 2 in inches from the top and use as a pattern for drawing the ears with half of the round as the template. Sew all of these parts before cutting any of them out. Sew leaving at least a inch inch seam inside the marks. Cut out the parts and turn inside out. The ears have to be cut from the edge for the opening and then put a small amount of fiber in each and pin closed.\n8. Take red yarn and cut a piece to make the neck and tie around the body where the neck will be.\n9. Take the ears and pin in place with the sides open, stitch the back in place then raise the ear upright and pin in place while stitching the front in place, next stitch a ""C"" inside the ear to make it rigid. Repeat for the second one.\n10. The mouth work, pin the sock heel that has already been sewn around for stability, line it up evenly between the ears and evenly from the toe of the sock. Pin in place and turn the rough edges under and stitch half way around, now put some fiber in this to make it firm and finish turning under the edges and stitch in place.\n11. The Sock heel (mouth) is stitched with embroidery stitch to make the mouth in the RED portion; Two beads are sewn in the center of the white part as nostrils and tied off.\n12. Eyes (Buttons) are to be located as close to the edge of the white part of the toe as possible, they should be located equal distance between the ears as possible.\n13. Eyelashes are stitched in even position around both eyes (buttons) and tied off.\n14. Now comes the arms and tails, first thing stuff them, fold the top under and pin each arm directly under the ears and stitch in place.\n15. Tail is last, place it directly in the center of the Body Part Heel Red area, turn the rough edges under and pin in place and stitch in place.\n16. FINAL NOTE: Sew all seams two times on the machine and all hand stitching attaching body parts use Carpet Thread and sew two times around.']"	['<urn:uuid:dbde44dc-9686-4e43-9eac-fc1a47f0e787>']	open-ended	with-premise	verbose-and-natural	similar-to-document	single-doc	expert	2025-05-12T20:11:36.665104	35	81	722
27	evening morning radio wave differences	Radio wave propagation characteristics vary significantly between evening and morning hours. During early evening (1500-1900 local time), afternoon transequatorial propagation occurs with strong, stable signals up to 60 MHz and limited distortion, allowing for voice communications. Later in the evening (1900-2300), propagation supports even higher frequencies but with intense rapid fading and severe distortion, making only morse code communication possible. By morning hours (around 0300), the nighttime ionospheric irregularities that enable these unusual propagation modes have mostly disappeared. Additionally, during daytime there are two ionospheric layers (F1 and F2), which merge into a single F layer at sunset and split again at sunrise.	"['Past months propagation improved with nice HF propagation. Those who have DX ears must have noticed that 10m long distance propagation acted in a more or less remarkable way. Fellow DX’ ers on forums reported signals with multiple echoes that are certainly not from a CB echo mike. Some say it must be a signal that travels all around the globe that causes the echo. Well, it’s all got to do with short path, long path and chordal hop propagation. I will try to explain.\nSignificant signal loss on normal multihop paths\nLet’s pretend we have two stations making QSO on 10m, PA9X near Rotterdam and imaginary station VK5ZQZ down in Adelaide. The solar flux is high enough (140) and K-index low enough (1) to allow propagation between the two stations. We are making a nice QSO, I have got my antenna directed at 70° bearing and he has got his antenna directed at 340°. The signal follows the shortest path with a distance of about 16,500km from The Netherlands across north east Europe, Asiatic Russia, China, south east Asia to Adelaide. It hops between the F2 layer, which floats between roughly 250-400km altitude and the Earth’s surface. Using F2-layer propagation one single hop can do a distance between 1800km and 3500km on 10m. So for the complete path to Australia, the signal makes about 5 to 6 hops. With each hop, bouncing off the Earth’s surface, there is significant signal loss, especially bouncing of land. But there is another path, the long path.\nLong path propagation signals can be stronger than short path\nThe long path runs the other way around the globe. Starting from The Netherlands, along the Azores, Atlantic Ocean northern Brazil, Peru, Pacific Ocean, across New Zealand’s Southern Island into Sydney, a distance of roughly 23,500km. The long path to Australia would be 7 to 8 hops in theory. But but why are long path signals sometimes remarkably stronger than short path signals when they make more hops? Could be that because the long path runs mainly over salt water, but this only counts for this specific situation. But there is more going on, a propagation mode that is believed to responsible for long path propagation with strong signals is called chordal hop propagation.\nSignal that bounces along the ionosphere\nChordal hop propagation is a propagation mode involving the daylight F2 layer and night time F layer. At daytime there are two upper layers in the ionosphere, the F1-layer at ± 150-200km and the F2 layer at 250-400km. Shortly after sunset these two layers merge into the F layer and split up again into F1 and F2 layer at sunruse. During night time the F layer loses it’s ionization density, and it’s ability to reflect signals back to Earth. But sometimes the F-layer is just dense enough to reflect the signal back, but with a less steep angle, causing the signal to be directed to another part of the ionosphere thousands of km’s ahead, not touching the ground. Here is a picture I have drawn to visualize.\nThe purple line represents the ionosphere, the orange line the short path and the green line the long path. A part of the green line is chordal hop propagation, there where the signal does not touch Earth’s surface but reflects of the F-layer.\nLess attenuation with chordal hop\nWith chordal hop propagation you have much less attenuation due to the fact the signals does not reflect against Earth’s surface. In this occasion the signal that uses long path propagation arrives at the other station with much less attenuation, thus with a stronger signal than the short path. one Funny thing is that a station at night time, like imaginary station PY4FTL has signals travelling above him along the ionosphere, but he cannot receive them.\nThere are also scientist that believe that chordal hop could be the result of signals that travel between the F2 or F layer and a sporadic-E layer. One more theory is that a signals travels within the F2 or F layer in an kind of duct for thousand of km’s.\nSignal with many different components\nThe multiple echoes and hollow modulation you hear, are the result of multiple signals of different phases that arrive at the receiver end and mix together. One single signal can consist of many different components, for example:\n- Long path propagation signal\n- Short path propagation signal\n- Short path propagation signals that also travelled along the long path (went all around the globe)\n- Long path propagation signals that also travelled along the short path (went all around the globe)\n- Signals that went multiple times around the globe in both directions\n- Backscatter that travelled the short path while beaming short path\nThese many “components” all add up into a sometimes spectacular combination of echoes. In one rare occasions I received an S9 signal. It had so many echoes that it was barely readable, an R2/S9 report! In another occasion I received a station from Moscow via the long path (37,800km) while the short path signal (2,200km) did not appear to be present at all!\nLong path and chordal hop propagation are beautiful phenomenon that not too many operators on 10m band know how to take advantage of. Hope this post will help.\n73 de PA9X Jean-Paul Suijs', 'Transequatorial Radio Propagation\n- Transequatorial Propagation\n- The Equatorial Ionosphere\n- aTEP (Afternoon TEP)\n- eTEP (Evening TEP)\n- Diagnostics for TEP\n- Australian Beacons Suitable for TEP Investigations\n- Further information\nMost ionospheric models consider the ionosphere as a series of horizontal layers that vary only slowly with time and geographical location. Propagation modes that are based on such a model are called normal propagation modes. However, the real ionosphere does not always conform to this simple model, particularly in the equatorial and polar regions. Anomalies that exist in these region give rise to what are called \'unusual propagation modes\'. Features of the ionosphere that give rise to these unusual modes include sporadic-E, the equatorial ionisation enhancements, ionospheric tilts at twilight, and ionospheric irregularities such as equatorial spread-F.\nTwo main features of the equatorial ionosphere give rise to the phenomena known as transequatorial propagation or TEP.\nMilitary and amateur radio operators in the 1940\'s may have been the first to discover that it was possible to communicate from north to south and vice versa across the equator over intercontinental distances using frequencies in the VHF band (QST, October 1947). At times of high sunspot number, the F2 layer may support normal modes up to 45 MHz, but frequencies considerably higher than this were found useable on transequatorial circuits. Although use was made of this phenomenon, it was not until several decades later that the actual mode of propagation was determined.\nRadio amateurs soon recognised TEP as a mode worth working. The first large scale TEP communications probably occurred around 1957-1958 during the peak of solar cycle 19. Around 1970, the peak of cycle 20, many TEP contacts were made between Australian and Japanese radio amateurs. With the rise of cycle 21 starting around 1977, amateur contacts were made between Greece/Italy and Southern Africa (both South Africa and Rhodesia/Zimbabwe), and between Central and South America by TEP.\nIt was observed that there were two distinctly different types of TEP that could occur: The first type occurred during the late afternoon and early evening hours and was generally limited to distances under 6000 km. Signals propagated by this mode were limited to the low VHF band (<60 MHz), were of high signal strength and suffered moderate distortion (due to multipath). Single sideband voice communications were possible with this mode.\nThe second type of TEP occurred from around 1900 to 2300 hours local time. Contacts were made at 144 MHz, and even very rarely on 432 MHz.\nThe signal strength was moderately high, but subject to intense rapid fading, making morse code (narrow band CW) the only possible communication mode. One amateur described the signal quality in the following words: ""we tried SSB but there was so much distortion that not a single word could be identified. [this mode] has a lot of flutter and fading and ... even the morse comes through like a breathing noise, not a clear tone"" (from the Dawn of Amateur Radio in the UK and Greece by Norman F Joly).\nThe Equatorial Ionosphere\nFor convenience, the ionosphere can be divided into three zones for the purposes of characterising its behaviour: the equatorial zone, the temperate zone and the polar zone. The temperate ionosphere is, as its name implies, the ""best behaved"". It is also the best studied, because most of the technological societies of the world are located in this area (at least in the northern hemisphere).\nCompared to the temperate zone, the Sun is more directly overhead in the equatorial zone, and so we should expect to find higher ionospheric critical frequencies than in the temperate zone. We might also expect to find a smaller variation of the ionosphere with the seasons (since essentially the tropics do not have a well defined summer and winter). At the other extreme, we would expect the polar ionosphere to be less dense (because of the high solar zenith angle), and to show the greatest variability between summer and winter. Although these expectations are essentially true, they do not explain the many interesting features of these regions. And the most important layer of the ionosphere, the F region, does not always obey these reasonable assumptions.\nThe polar and equatorial ionospheres are both subject to a wider range of normal and unexpected behaviour. The high latitude ionosphere was subjected to intensive study before and during the International Geophysical Year (IGY 1957-8). One of the main causes of the unexpected behaviour is the Earth\'s magnetic field. In the polar regions, the magnetic field lines are almost perpendicular to the Earth’s surface, while in the equatorial region, the magnetic field lines are horizontal to the Earth’s surface at the magnetic equator. (Note: to confuse the picture even further, the geomagnetic and geographic equators do not usually coincide, and they may be up to 12 degrees apart. At Asian longitudes the geomagnetic equator is the above the geographic equator, whereas at American longitudes it is below.)\nThe most interesting feature of the tropical ionosphere is the region commonly called the equatorial anomaly. Historically, this name arose because the ionisation peak was not expected – it’s presence disobeyed the simple mid-latitude model people had devised for the ionosphere. While we now know better what causes the ionisation crests, the old name still sticks. This is where a high electron concentration is observed on each side of the magnetic equator at magnetic latitudes at around 10 to 20 degrees. These crests of ionisation give rise to higher ionospheric critical frequencies (foF2) than exist at the geomagnetic equator. They are also at lower altitudes than is the peak of the F-layer at the geomagnetic equator.\nThe equatorial anomaly is caused by the combined action of electric and magnetic fields. When the overhead Sun creates intense ionisation in the region, the electric field starts these charges moving. The magnetic field (which only has an effect on moving charges) then causes them to drift upwards. Finally, the particles diffuse outwards, following the geomagnetic field down to where it intersects the normal F-layer This process starts immediately after sunrise and by mid afternoon the buildup in ionisation is clearly present and persists until after sunset, when no more ionisation is produced by the Sun.\nDuring the early evening hours, while the enhanced ionisation is decreasing, vast ionisation irregularity regions can be formed by dynamic processes. It is generally thought that an instability starts low in the ionosphere, grows and propagates upward. These irregularities are blown around by ionospheric winds, breakup, and by the morning hours (0300 LT), have mostly disappeared. Generally, ionisation irregularities can be seen on an ionosonde as a thickening or spreading of the F2 layer trace. This is referred to as range or frequency spreading (depending on the mechanism involved - and it sometimes difficult to separate the two). All spreading is believed to be due to ionisation irregularities in the ionosphere. These irregularities, which (at least in the equatorial ionosphere) occur only at night-time, usually start to develop in the evening hours as a disturbance at the bottom of the ionosphere and then grow upward. They may be in the form of expanding plumes, and/or as small scale bubbles or pockets. They are aligned with the geomagnetic field lines (and are thus often referred to as field aligned irregularities FAI). These plumes, tubes, bubbles or pockets form holes, or biteouts in the local ionisation and radio waves are refracted by these discontinuities in the ionosphere. Not only do these irregularities affect HF radio propagation but they can also cause scintillations on L-band (low microwave) satellite to ground transionospheric signals.\nThe equatorial anomaly and the irregularities are used to explain transequatorial propagation.\naTEP (Afternoon TEP)\nAfternoon transequatorial propagation is believed to by a super F mode (designated FF), in which the signal from the transmitter is first reflected by the concentration of ionisation at one of the equatorial anomaly crests to the second crest in the opposite hemisphere. From there it is reflected down to the ground receiving station. It thus suffers no ground reflection (as would be the case in the normal 2F mode), and it also passes through the absorptive D-layer only twice (instead of 4 times for the 2F mode).\nBecause the intermediate ray is between two parts of the F layer, the grazing angle at the ionosphere can be substantially smaller than for a ray reflected back to the ground. This in turn implies that a higher frequency may be reflected (fr= foF2*sec(i)). Here i is the angle of incidence at the ionosphere, and as this approaches 90 (the grazing angle g=90-i tends to zero), the maximum possible reflected frequency (fr) becomes larger. Another way of saying this is that the obliquity factor of the circuit is higher. The smaller grazing angle is also made possible because the increased ionisation at the anomaly crest follows the magnetic field lines and is tilted slightly upward toward the equator.\nHigh signal strength observed for afternoon TEP are due to the smaller number of passages through the D-layer, and because the anomaly crests support propagation of signals from a wider range of elevation angles than with usual propagation modes, the distribution of ionisation in the equatorial anomaly tending to focus these along the path.\nThe characteristics of aTEP are:\n- Maximum useable frequency (MUF) up to about 60 MHz, which is usually about 15 to 25 MHz above the 2F mode frequency for the same path.\n- Occurs from around 1500 to 1900 local time. It is more prevalent near the equinoxes and at times of high sunspot number.\n- Typical path lengths will be from 5000 to 6500 kilometres.\n- Signals will normally be strong with limited fading and distortion (from multipathing or Doppler spread).\neTEP (Evening TEP)\nEvening transequatorial propagation generally supports much higher frequencies than aTEP and has, on rare occasions, been reported on the 432 MHz amateur frequency band (low UHF). Evening TEP is strongly correlated with the existence of range spreading, called equatorial spread F, seen on equatorial ionograms. Evening TEP propagation is not as well understood as aTEP but it is believed to take place via a whispering gallery or field-guided mode which relies on the existence of ionospheric bubbles, tubes or plumes that have an electron concentration lower than the surrounding area. Rays are reflected from the surfaces of the bubble walls, at all times staying within the ionosphere until they finally emerge on a path down to the ground.\nThe characteristics of evening TEP are:\n- Occurs around 2000 to 2300 local time, and is more frequent around the equinoxes and especially at times of high sunspot number.\n- Although signal strengths are high, the signal is subject to deep and rapid fading and very strong distortion (from multipathing and large Doppler motions). Doppler spread up to 2kHz has been observed on a CW signal.\n- Path lengths may vary from 3000 to 8000 kilometres.\n- Frequencies supported are higher than for aTEP and may very occasionally rise into the low UHF band.\nDiagnostics for TEP\nIt is not yet possible to predict the occurrence of TEP with any certainty, so further investigations into these propagation circumstances are required.\nWe know some necessary conditions for TEP, but we also know that these are not sufficient to ensure that TEP will occur. Some of these are:\n- For the highest frequency support, the circuit should be symmetric with respect to the geomagnetic equator. That is, the receiver and transmitter should be located at equal distances from the magnetic (dip) equator.\n- The path must be within about 15 degrees of geomagnetic north-south.\n- The occurrence rate is greatest around the equinoxes.\n- Occurrence rate is greatest at times around the maximum of the solar cycle. This is a time of higher solar EUV output (which leads to more intense ionospheric ionisation).\n- Occurrence rate decreases as the circuit frequency increases.\n- Range spreading on equatorial ionograms appears to be a necessary but not a sufficient condition for eTEP. However, this probably depends on thestation location.\n- The higher the F2 layer over the geomagnetic equator the higher the occurrence rate of TEP. In fact, this is thought to be one of the best predictors for eTEP.\n- The further the equatorial anomaly crests are from the geomagnetic equator the higher the probability that aTEP will be present. This geometry favours enhancement of the FF mode.\n- Quiet geomagnetic conditions appear to favour the development of ionospheric irregularities, and thus eTEP. A 27-day periodicity has been noted on some paths (probably related to geomagnetic activity of solar origin). The higher the circuit frequency, the more important it appears to be to have quiet geomagnetic conditions. (Note: geomagnetic disturbances are usually more prevalent and more intense around the equinoxes, and thus we have two conflicting conditions).\nNote that some of the above diagnostics are for aTEP but many relate to eTEP.\nAustralian Beacons Suitable for TEP Investigations\nThe ideal beacon for TEP investigations is a continuous wave (CW) transmitter. This allows measurements of signal strength, Doppler shift and spread, to be made without any confounding factors introduced by the signal modulation. Failing a CW beacon, an AM transmitter is the next best substitute, as the carrier is a fixed frequency and relatively constant in power (although the total radiated power may be constant, the ratio of energy spread across carrier and sidebands may change). An FM transmitter is normally not appropriate as a TEP beacon (although the total radiated power is constant, this is spread over a wide frequency range [eg 250 kHz], and there is no energy peak at the nominal carrier frequency when significant modulation is present).\nThe list of Australian beacons shown below have been chosen as potential candidates for Australia- Japan TEP investigations.\nAmateur Radio Beacons\nLeo F McNamara, The Ionosphere: Communications, Surveillance, and Direction Finding, Kreiger (Orbit Books) 1991, ISBN 0-89464-040-2.']"	['<urn:uuid:02a594e1-e9fd-43d1-be32-0fb0fa680a93>', '<urn:uuid:ffad3040-565f-40e2-a12d-8019b0ff86b6>']	open-ended	direct	short-search-query	distant-from-document	multi-aspect	novice	2025-05-12T20:11:36.665104	5	103	3206
28	why crops rotate prevent disease spread	Crop rotation is important because fungal pathogens can survive in soil and plant debris for many years. For instance, Fusarium can persist in soil for extended periods, requiring clean land or 5-year rotation cycles on infested land. Research has shown that continuous corn or corn-soybean rotation fields can develop increased Sudden Death Syndrome (SDS) due to the survival of Fusarium virguliforme on corn kernels left during harvest. Fields with higher harvest losses, such as seed corn fields, show increased disease risk.	['|Disease (causal agent)\n||Survival of pathogen and effect of environment\n||Comments on control\n(Fusarium oxysporum f. sp. callistephi)\n|Plants yellow and wilt, often on one side.\nBrown discoloration of vascular system develops. Disease also causes damping‑off of young seedlings at soil temperatures of 75° to 80°F.\n||Commonly seedborne. In soil for many years. Disease is most severe when soil and/or air temperatures are high.\n||Use disease‑free seed. Fumigate the\nseedbed with chloropicrin-methyl bromide combination or solarize soil. Grow\non clean land, or only once every 5 years on infested land. Treat seed with a\nfungicide. more info *\n|Brown, water‑soaked decay of flowers.\nWoolly gray fungus spores form on rotted tissues. Fungus also attacks base of plant.\n||In plant debris. Favored by cool, wet conditions.\n||Avoid overhead irrigation. Mist blooms with\niprodione or fenhexamid. more\n|Circular, irregular, brown spots appear on lower leaves. Leaves may die.\n||In plant debris. Airborne spores require long (48 hrs), damp periods for infection.\n||Avoid low‑lying areas where air\nmovement is poor. Do not use overhead irrigation. Protect foliage with a fungicide such as mancozeb.\n|Plants wilt or suddenly collapse. Roots\ndecay. Blackish discoloration of leaves, stems, and roots occurs. Also causes damping‑off of seedlings.\n||In soil. Favored by heavy, waterlogged soils.\n||Avoid planting in poorly drained fields. Plant on raised\nbeds. Do not overirrigate and keep hose ends off the ground. Drench seedlings\nwith mefenoxam. more info: Pythium Root Rot, Phytophthora Root and\n|Orange pustules of powdery spores form on\nundersides of leaves. On living plants and possibly from spores from alternate host (three‑needle pines).\n||Favored by free moisture from rain, dew, or fog. Very common in cut-flower-growing areas around San Jose.\n||Avoid overhead irrigation. Treat at the\nfirst signs of rust and continue until conditions are no longer favorable for\nthe disease. Grow seedlings away from main crop. more info *\n|Sclerotinia rot or Cottony rot\n|Infection girdles stems. Cottony, white\nfungal growth and large, black sclerotia develop on and inside stems. Stems take on a bleached‑white color.\n||Airborne spores produced by sclerotia in\nsoil, but infection more common from growth of hyphae from sclerotia. Favored by wet weather.\n||Avoid overhead irrigation. Treat planting\narea with PCNB. Spray plants with iprodione or thiophanate-methyl before\nrainy periods and at 2- to 4-week intervals during wet weather. Remove plant\ndebris from field. more info: Cottony Rot, Southern\n(Rhizoctonia solani, Botrytis cinerea)\n|A brown decay develops at the soil line and affects the basal leaves and stem.\n||Soilborne and in plant debris. Gray mold (B.\nby cold, damp conditions. Disease development can be rapid under high temperature conditions.\n||Before planting or transplanting, mix PCNB\nor Trichoderma spp. into top inch of soil. Spray bases of seedlings with\nthiophanate-methyl, iprodione, or Trichoderma spp.\n|Symptoms are almost identical to Fusarium wilt. Not a common disease of asters in California.\n||In soil for many years. Symptoms most severe during warm weather that follows a cool period.\n||Avoid planting in fields where fungus has\noccurred or fumigate soil as described for Fusarium wilt. more info *\n|Virus or viruslike diseases\n||Host range and natural spread\n||Comments on control\n(Aster yellows phytoplasma)\n|Infected plants produce an upright basal\nrosette of yellow shoots. Sometimes one-sided. Flowers are deformed and remain green. Sporadic disease of asters in California.\n||Aster yellows phytoplasma has a wide host\nrange. Vectored by leafhoppers.\n||Locate seedbed away from weedy areas. Control weeds and\nleafhoppers in noncropped areas. more info *', 'Category Archives: Diseases\nAccording to research, wheat is susceptible from flowering through soft dough development stage. “Typical” fungicides used for control of fungal leaf diseases are off-label thus illegal to apply once the wheat has flowered and they do not have activity on the Fusarium fungus causing scab of wheat. Management for scab includes the use of the preventive fungicides Caramba or Prosaro. Both are labeled for headed and flowering wheat. There’s a 30 day pre-harvest restriction for both. Rainfast varies from ¼ hour to 2 hours or when dry depending on environmental conditions. Both fungicides can help prevent scab and control rust on the plant.\nResearch from the US Wheat and Barley Scab Initiative (which is a combined effort of several Universities in the U.S. and Canada) has found that the best prevention using these products occurs when wheat is headed and 30% of the plants are in the beginning flower stage. Application within five days of these criteria still showed positive results. This research also showed that application before or after this time period greatly reduced effectiveness of preventing scab. Understandably, the economics of fungicide application are difficult in wheat, yet, if you are aiming to make one application, this could be your best option for both scab prevention and controlling rust in your plants. The risk map for scab can be found at: http://www.wheatscab.psu.edu/. With wheat at heading to beginning flower and rain/humidity this risk in reality could be higher for us.\nGrazing corn residue provides many benefits to both livestock and grain farmers, yet many corn stalks in our area are not grazed for various reasons. With as much hail as we’ve had this fall, grazing is also an option to remove ears and kernels that were lost, preventing volunteer corn next season. Normally there is less than a bushel of ear drop per acre, but we most likely have more than that in some of our fields this year. Two kernels per square foot or one ¾ pound ear in 1/100 of an acre is the equivalent of 1 bu/ac yield loss. In 30” rows, 1/100 of an acre is 174’ long if you count in one row or 87’ if you count in two rows.\nWhat may also be of interest to you is a recent finding between corn grain loss pre-and during harvest and sudden death syndrome (SDS) of soybean. Many asked me this this year, “Why did I see SDS this year when we’ve never had it in this field before?” It’s a great question and I often responded by saying we need to sample the areas affected with SDS for soybean cyst nematode (SCN) as the two diseases are synergistic. Sampling for SCN still remains free through your Nebraska Soybean Board Checkoff dollars and you can stop by the Extension Office for free sampling bags. Crop consultants should contact the UNL Plant and Pest Diagnostic lab directly at (402) 472-2559 if you are requesting 10 or more sampling bags.\nAnything that moves soil can transport the fungal soil-borne pathogens causing these diseases. But recent research from Iowa State University also suggests that the fungal pathogen causing SDS (Fusarium virguliforme) survives on grain lost during the harvest process in fields and that SDS management in soybean actually needs to begin at corn harvest.\nStudies were conducted for two years in greenhouse and in field plots with nine treatments to determine the survivability of Fusarium virguliforme (Fv) on corn and soybean residue. The treatments were: 1-Corn kernels + Fv; 2-Corn roots + Fv; 3-Corn stem/leaves/husk + Fv; 4-No residue + Fv; 5-Soybean seeds + Fv; 6-Soybean stem/leaves/pods + Fv; 7-Soybean roots +Fv; 8-Corn stalk on soil surface + Fv; 9-Corn kernels and stalk on soil surface + Fv. The researchers consistently found in both the greenhouse and field experiments that Treatment 1 of corn kernels at average harvest loss resulted in the most SDS. Treatment 2 consistently resulted in the second most SDS.\nThis helps to explain why some farmers are finding SDS in fields that have been continuous corn for a period of years, are finding SDS in corn and soybean rotation when little or no SDS was previously observed, and why SDS has increased in seed corn fields that may have higher harvest losses. They did not experiment with tillage systems and their recommendation is to reduce harvest losses to reduce the risk of SDS.\nGrazing residues can reduce your risk from these harvest losses and for those losses which were incurred with the hail/wind storms we’ve experienced since Labor Day. When grazing corn residue, cattle are selective. They will eat the grain first followed by the husk and leaf followed by the cob and stalk.\nIt’s also important to be aware of grazing restrictions from herbicides applied to row crops; you can read more about that in this post.\nRadio advertisements, email blasts, and other media are warning of corn diseases and the need for fungicides. Two months of humid, wet weather has allowed for disease development. It’s important to know what diseases truly are in your field before spraying a fungicide, particularly with today’s economics. Here’s what we’re seeing in fields right now in the Clay, Nuckolls, Thayer, Adams county area. Based on the diseases we’re seeing, we would recommend you scout your fields to know whether you have mostly bacterial or fungal diseases present. Consider disease pressure, where on the plant the disease is occurring, growth stage, and economics. We have had southern rust show up in 10 of the last 11 years I’ve been serving in this area. If you spray a fungicide at tassel, you may not have enough residual to ward off southern rust when it appears later, potentially resulting in the need for a second application. In our area thus far, I’m not seeing enough disease pressure in many fields to warrant a fungicide at tassel; consider delaying an application till later for economic and resistance-management reasons. Ultimately this decision needs to be done on a field by field basis. Please also see this UNL CropWatch article regarding fungicide application and corn growth stage. Although I don’t have a photo of it, I’ve also seen common rust in the mid and lower portions of corn canopies thus far.']	['<urn:uuid:37905203-21c3-42e6-b7e2-959445e84c35>', '<urn:uuid:a5b75ea7-d828-4569-a784-5b4bb9ae571a>']	factoid	with-premise	short-search-query	distant-from-document	three-doc	novice	2025-05-12T20:11:36.665104	6	81	1616
29	composting methods newspaper organic garden waste vs coffee grounds ratios decomposition process	Newspapers and coffee grounds offer different composting approaches in organic gardening. Newspapers are primarily used for weed control by being layered directly on top of weeds to block sunlight, eventually decomposing into the soil. Coffee grounds, however, require specific composting ratios - 1/3 coffee grounds, 1/3 green material (like grass clippings and flower stems), and 1/3 dried leaves. Coffee grounds need to cool before being added to compost and require approximately three months of aging before soil application. Both materials ultimately decompose into the soil, but their application methods and purposes differ significantly.	"['Growing your own organic fruits and vegetables is a great way to save money and adopt a healthier diet, but it also requires efforts from you. Additionally, there are a variety of seeds with different planting times to consider. This article contains tips that will ensure you have everything you need to start an organic garden.\nYour children can help you with your garden. A garden can be a great learning experience for your children, and it gives you a chance to bond while producing healthy food.\nPlanting organic strawberries is a great way to encourage your children to get involved. Children love to snap up these sweet juicy fruits for themselves and will be much more willing to eat other foods you’ve planted as well.\nPine can make surprisingly great mulch. A number of plants commonly grown in garden settings do best in an acidic soil. For such plants, pine needles function both as a handy mulch and as a soil amendment to lower the pH. Using several inches of needles to cover your beds provides acid for the soil as they begin to decompose.\nUse an aged laundry basket when you want to collect your produce. The laundry basket can be used as a colander for your produce. Rinse off your produce while it’s in the laundry basket, and any excess water will be strained out through the basket’s holes.\nKilling weeds the natural way? You can easily control weeds by using layered sheets of newspaper. Weeds require sunlight to continue growing. When you pile up layers of newspapers right on top of the weeds, they will suffocate and die. In time, the newspaper will decompose into the soil. Try adding some mulch on top of it to make appear more attractive.\nA useful technique for organic gardening, is to gently disturb your seedlings by using your fingers or a piece of cardboard one or two times daily. Even though it sounds strange, it will help plants get bigger.\nA beer trap is an effective way of of dealing with any slugs that invade your organic garden. Take a glass jar and bury it so that the soil is even with its mouth. Fill that with beer just under an inch from the top of your jar. The slugs will be attracted to the beer, and will then become trapped within the jar.\nGet your organic garden certified so you can credibly claim that your crops are organic. That way, you will realize greater sales volume and demonstrate the value of your produce to potential and returning customers.\nWater demands from plants will depend on the season and the climate in your area. When watering your plants, consider the time you are watering them, the kind of soil you are using and how good the water is. For instance, if you live in a humid climate where it never goes below 30 degrees Celsius, refrain from watering the leaves, as this will inevitably invite leaf fungus. Make sure you give the roots plenty of water.\nAcquiring a good understanding of organic gardening is necessary to grow all kinds of healthy fruits and vegetables. Use the tips from above to take a step in the right direction when it comes to organic gardening, and start your family on a healthier path to better living.', ""When deciding whether or not your plants would like the remains of your morning coffee, consider your overall climate. Generally speaking, most plants do prefer soil that is slightly acidic, and coffee grounds can be slightly acidic. Plants that like lots of water, such as those grown in areas with high rainfall, also like acidic soil because rain can wash nutrients out of the soil.\nPlants that prefer an acidic soil include those that grow in all types of light. Lily of the valley (Convallaria majalis) and maidenhair fern (Adiantum pedatum) both like partial to full shade in U.S. Department of Agriculture plant hardiness zones 3 through 8. Wild strawberry (Fragaria vesca) grows in either full sun or partial shade in USDA zones 5 through 9. And moss phlox (Phlox subulata) likes full sun in USDA zones 3 through 9.\nShrubs that grow well in acidic soils include azalea (Rhododendron arborescens) for USDA zones 4 through 7 and camellia (Camellia japonica) for USDA zones 7 through 9; both grow best in partial shade. Highbush blueberry 'Duke' (Vaccinium 'Duke') thrives in USDA zones 5 though 8 in full sun to partial shade.\nUse Grounds Directly on the Soil\nEven though they can be slightly acidic, coffee grounds vary in their acidity, so there is no guarantee of their pH level. To use the grounds most effectively, work them from 6 to 8 inches into the soil before planting. Follow these tips for adding coffee grounds to the soil when your plants are already in the ground.\n- Let the grounds cool before adding them to the soil. Beneficial bacteria and microbes can be killed by heat.\n- Apply only a thin layer, less than 1/2 inch, or a light sprinkling of grounds to the soil. A thick layer can compact and form a barrier that keeps water and air from getting through to the plant's roots.\n- Cover the coffee grounds with a layer of organic mulch, such as shredded leaves or wood chips. The mulch helps the coffee grounds to decompose and release their nitrogen into the soil more quickly. Apply up to 4 inches of mulch.\nUse Grounds in Compost\nComposting coffee grounds before adding them to the soil lets them age enough to release their nitrogen into the compost. Here are some tips for composting with the grounds:\nLet the grounds cool before adding them to your bin.\n* Use a ratio of about 1/3 coffee grounds, 1/3 green material, such as grass clippings and flower stems, and 1/3 dried leaves for compost.\n* Let the compost age for about three months before spreading it on the soil.\n- Sunset: Acid or Alkaline Soil: Modifying pH\n- Agriculutre and Natural Resources University of California: Wake Up and Use the Coffee - grounds, That Is!\n- University of Illinois Extension: Acid Loving Plants\n- Missouri Botanical Garden: Convallaria Majalis\n- Missouri Botanical Garden: Adiantum pedatum\n- Missouri Botanical Garden: Phlox Subulata\n- Missouri Botanical Garden: Fragaria Vesca\n- Missouri Botanical Garden: Rhododendron Arborescens\n- Missouri Botanical Garden: Camellia Japonica\n- Missouri Botanical Garden: Vaccinium 'Duke'\n- Washington State University Extension: Using Coffee Grounds in Gardens and Landscapes\n- Jupiterimages/Photos.com/Getty Images""]"	['<urn:uuid:4b08660c-089d-4a0a-a276-ef0fc2c734e5>', '<urn:uuid:993e594c-4953-4fa8-a89d-779006b299b1>']	open-ended	direct	long-search-query	similar-to-document	comparison	expert	2025-05-12T20:11:36.665104	12	93	1086
30	I've been following the recent migrant crisis and heard about Turkey's policy change. When exactly did Turkey remove its border restrictions with Greece, and what was the immediate outcome?	Turkey removed its border restrictions with Greece on February 29, 2020. As an immediate outcome, thousands of people began making their way across Turkey to the Greek border, where they were met with tear gas, and the Greek coastguard fired warning shots at boats attempting to cross the Aegean sea.	['The ongoing refugee and migrant crisis in the Aegean has taken a dramatic turn in recent days with an escalating humanitarian situation on the land and sea borders between Greece and Turkey.\nILAY ROMAIN ORS, UNIVERSITY OF OXFORD\nISRF Independent Scholar Fellow 2019-2020\nAfter Turkey removed its border restrictions with Greece on February 29, thousands of people began to make their way across the country to the Greek border. They have been met with tear gas, and warning shots fired by the Greek coastguard at boats trying to cross the Aegean sea.\nThe latest “crisis” started suddenly – yet migration in the region has been going on for many years, if not millennia. As an ancient route of cultural and trade interchange, the Aegean has always been a sea of overlapping waves of migrations – and the rich history of this criss-crossing is ever-present in the region today.\nMy ongoing research in the Greek islands and mainland suggests the living memory of previous experiences of displacement forms a vivid background to the current arrival of refugees, who have been coming since the Syrian civil war intensified in around 2015.\nOn February 29, Turkey woke up to the news that at least 30 of its soldiers had been killed in an air attack at an army base in Idlib in northern Syria. Turkish political leaders responded by promising to retaliate in what is another escalation of the military conflict in the region.\nBut the Turkish president, Recep Tayyip Erdoğan, also made good on a previous threat and declared that no migrant attempting to leave the country via the border with Greece would be stopped. This was a major shift in policy since the signing of a 2016 deal between Turkey and the EU, under which Erdoğan agreed to regulate and reduce the migrant flows to Europe in exchange for financial support.\nIn a matter of two days, tens of thousands gathered at the main checkpoints at the land and sea borders, only to find that the Greek side was closed. In response, the government announced that Greece would not accept any more irregular migrants, nor would it process any asylum applications for a month.\nDespite criticism from humanitarian agencies and European parliamentarians over the legality and legitimacy of such measures, the Greek government stood firm. On March 3, the EU Commission’s president, Ursula von der Leyen, travelled to the border city of Evros and thanked Greece “for being our European aspida.” By using the Greek word for shield, and reiterating that the Greek borders were European borders, she gave the Greek prime minister a strong message of unity and support.\nHalf open, half closed\nThose who wish to believe that a half-closed border is still half open continue to wait for their ever-slimmer chances to enter Greece. Thousands of people are spending days and nights in near freezing temperatures in the buffer zone between the two borders with only limited humanitarian assistance provided by locals and NGOs.\nIn the Aegean islands, the situation is even thornier. As of January 31, 2020, there were 115,600 refugees and migrants in Greece, according to the UNHCR. So far, there have been 8,432 arrivals in 2020. While the numbers are not at the levels they were in 2015, when Greece was caught off guard in the initial phases of refugee flows, it’s not the quantity of the migrants but the changes in the quality of their reception that matters.\nIn the past five years, the irregular flow of refugees arriving in Greek shores with dinghies has continued with some fluctuations. Greece established five migrant hotspots in its Aegean islands, yet these have not addressed the needs of those arriving. With multiple accounts documenting the appalling conditions in various refugee camps, especially at the Moria camp on the island of Lesvos, this has led to criticism of Greece’s ability or willingness to deal with the migration issue.\nThe new government of Kyriakos Mitsotakis, the Greek prime minister, vowed to take drastic measures and passed a new migration law in November 2019 which came into effect in January. This was followed by a plan to build closed reception centers in the islands of Chios, Samos and Lesvos which would replace the current open camp structures of the hotspots.\nThese measures have been presented as effective solutions to accelerate the asylum procedures and to “decongest the islands”. But they have been met with anger by locals, who protested extensively against the central government’s decisions, leading to a general strike on February 25.\nThe rising tension has heightened the ideological polarisation among the locals on the Aegean islands. Anti-migrant protesters, alongside far-right extremists, have demonstrated that they are prepared to use violent means to protect their borders. In early March, some angry protesters tried to block refugee boats from arriving into harbours and block roads. Cars and buildings have been burnt and journalists attacked.\nThe opposing camp condemns the use of refugees as bargaining chips for political ends. They are appealing to concepts such as hospitality, civilisation and humanity to underline their stance in solidarity with the migrants, using slogans such as “open the borders” and “no human is illegal”.\nBoth anti-migrant groups and those in solidarity with migrants are using the region’s history to promote their own ideological positions.\nThose in solidarity claim that migration is not a crime, but rather an element of the human condition that has occurred repeatedly throughout the region’s history. They recall how during the second world war, thousands of Greeks crossed the Turkish border to escape the German occupation and seek refuge in the Middle East.\nThe Aegean islands were also where boats filled with Greek Orthodox residents of Asia Minor came in the wake of the Convention of the Forced Exchange of Populations of 1923 between Greece and Turkey, signed after the first world war. Following the arrival of more than 1.5 million people in Greece, the population of the islands almost doubled to the extent that many locals still have family members from among the group originally and still known as the “Asia Minor refugees”.\nThe ongoing tensions in the region have once again made it into a place where complex negotiations take place over ideology and identity. The shifting way the past is being imagined stands as a testimony to how the history of overlapping migrations is currently being kept alive in the Aegean.\nDr Ilay Romain Ors\nISRF Independent Scholar Fellow 2019-2020\nIlay Romain Ors is a research affiliate of the Centre of Migration, Policy, and Society (COMPAS) at the University of Oxford. She is currently based in Greece, working on her research project on Aegean Migrations, funded by the ISRF and under contract by Berghahn Publishers. Her previous publications include Diaspora of the City: stories of cosmopolitanism from Istanbul and Athens (Palgrave, 2018). Dr Ors is an Associate Professor in Social Anthropology, holding a PhD degree in Anthropology and Middle Eastern Studies from Harvard University.']	['<urn:uuid:caec3931-910b-4425-9f8d-285d4e347883>']	factoid	with-premise	verbose-and-natural	similar-to-document	single-doc	expert	2025-05-12T20:11:36.665104	29	50	1158
31	How do underground coal mines manage both crusher performance optimization and ventilation control, and what parameters need to be monitored for each?	For crusher optimization, operators must regularly monitor the correlation between motor power, chamber pressure and tonnage to ensure expected performance and wear part lifetime. For ventilation control, mines monitor airflow velocity/direction and methane gas levels, especially around electrical installations. Continuous measurements of air velocity can vary based on location and conditions, while methane monitoring is crucial in active mining areas. The monitoring is typically done through control rooms where operators can track both crushing and atmospheric parameters.	"[""Coal mining - Underground mining Britannica\nCoal mining - Coal mining - Underground mining: In underground coal mining, the working environment is completely enclosed by the geologic medium, which consists of the coal seam and the overlying and underlying strata. Access to the coal seam is gained by suitable openings from the surface, and a network of roadways driven in the seam then facilitates the installation of service facilities\nPerfecting the Performance of Secondary Crushers E & MJ\nOperators should also regularly check the crusher’s feed curve and the correlation between the motor power, chamber pressure and the tonnage. If the tonnage, motor power or crushing pressure is not what you would expect, there is room to optimize your crusher performance and the lifetime of its wear parts.\nplc based control coal crushing and conveyor\nExperience7.90 Мб. Material handling equipment is required for crushing, grinding, transportation from the mine to theof two transfer conveyors, three belt filter discharge flop gates, and a PLC based control system.The Coal Handling System included coal receiving, storage, …\nOTTER CREEK MINE EXHIBIT 308C: MINE FACILITIES\nOct 21, 2014· Highway 484 to a secondary crusher inside the rail loop. Like the primaries, the secondary crushers will employ dual crushers of 5,000 tph capacity each. Secondary crushing will crush the coal to a top size of three inches, and a third belt will carry the coal to the top of two storage silos, each of 15,000-ton capacity.\nBenchmarking Longwall Dust Control Technology And Practices\ncoal mine workers and the magnitude of respirable dust overexposures in longwall mining occupations illustrate the need for NIOSH and the mining industry to improve existing dust control technology on longwalls. NIOSH researchers recently completed a research effort to quantify and document dust levels being generated by\nResearch on short wall continuous mining technology with\nApr 05, 2021· Short wall mining technology is an important method to coal resource mining in the United States, Australia and other countries, and also one of important methods to boundary coal resource recovery in China. Short wall mining technology through leaving a large number of coal pillars to support and control roof, induces a series of accidents, such as large area roof suspended, …\nProfession Mine control room operator - 123test\nMine control room operators perform a range of tasks from the control room of a mine. They monitor the processes through electronic representations shown on monitors, dials, and lights.\nABB process control and automation solutions for mines\nAutomated system based on ABB Ability™ System 800xA crusher no longer requires an operator present at controls, all monitored from the operator room ABB in decision support systems R&D project for the Bogdanka coal mine Effective predictive diagnostics program and advanced data analysis for …\nGlossary of Mining Terms - Coal Education\nA. Abutment - In coal mining, (1) the weight of the rocks above a narrow roadway is transferred to the solid coal along the sides, which act as abutments of the arch of strata spanning the roadway; and (2) the weight of the rocks over a longwall face is transferred to the front abutment, that is, the solid coal ahead of the face and the back abutment, that is, the settled packs behind the face.\nMonitoring & Ventilation Control Equipment Coal Mining\nThe Coal Mining industry looks to Conspec Controls for monitoring and ventilation control equipment to improve indoor air quality and harmful gas detection. 1-800-487-8450 Request a …\nNFPA 120, Standard for Fire Prevention and Control in Coal\n2015 Edition. Provide fire and explosion protection in coal mines with NFPA 120's updated provisions on sprinkler systems. NFPA 120, Standard for Fire Prevention and Control in Coal Mines provides requirements to safeguard lives and property from fire and explosion in underground bituminous coal mines, coal preparation plants that prepare coal for shipment, surface buildings and facilities\nLongwall mining - Wikipedia\nLongwall mining is a form of underground coal mining where a long wall of coal is mined in a single slice (typically 0.6–1.0 m (2 ft 0 in–3 ft 3 in) thick). The longwall panel (the block of coal that is being mined) is typically 3–4 km (1.9–2.5 mi) long and 250–400 m (820–1,310 ft) wide.\nCrushing plant for coal mining in Russia,Mobile coal\nCoal Mining Industry in Russia. In 2011, Russia produced about 336.3 million tons of coal materials. This illustrates 4% growth over the 2010 total production, so 2011 became a record level year in coal production in Russia. 90% of the coal output was provided by just the 18 largest producers in the coal mining industry, demonstrating the concentrated nature of the Russian coal mining business.\nCoal Mines - an overview ScienceDirect Topics\nJoule Bergerson, Lester Lave, in Encyclopedia of Energy, 2004. 4.6.3 Coal Mine Fires. Coal mines contain hazardous and explosive gases, and there is a potential for long-lasting fires. The OSM estimates that there are currently 4163 acres burning, including 94 sites where hazardous or explosive gas is being emitted from underground mine fires, which can have an effect on humans in the vicinity\nCrawler Reclaimer - SHANGHAI SANME MINING MACHINERY …\nCapacity. 800tph. Max feeding size-Raw materials. Ore, rock, construction waste, steel slag, tailings and etc. Application. Widely used in the transportation and production lines of cement, mining, metallurgy, chemical industry, casting, building materials and other industries, as well as hydropower station construction sites and ports and other production departments\nMSHA - Coal Mine Fatal Accident Report for Coal Fatal #3\nCOAL MINE SAFETY AND HEALTH District 10 Report of Investigation (Surface Coal Preparation Plant) A CB radio was in Milan's truck that was parked near the control room. This was the last conversation Knight and Milan had prior to the accident. of refuse material separated during the crushing process was located below the ramp on the\nCN201496102U - Wind power attal filling system of goaf of\nThe utility model discloses a wind power attal filling system of a goaf of a coal mine. A wagon tipper and a jaw type crusher are arranged nearby an underground mining area, an inlet of an attal hopper is provided with two hammer type crushers, a ventilation opening on a coalface is provided with a pneumatic attal filling machine, and a filling type working bracket is arranged on a\nDEPARTMENT OF LABOR MINE SAFETY AND HEALTH …\nAug 07, 2019· vacuum contactor located in the 8th floor motor control center (MCC) room in a coal preparation plant. The accident occurred because the mine operator engaged in, and assisted miners in the performance of, electrical work while knowing the energized electrical circuit on which electrical work was performed was not locked and tagged out.\nBrookside Coal Mine, Washer & Crusher (Foundations), Mount\nBrookside Coal Mine, Washer & Crusher (Foundations), Mount Olive Road, North of Five Mile Creek Bridge, Brookside, Jefferson County, AL Control Number use the following steps to determine whether you need to fill out a call slip in the Prints and Photographs Reading Room to view the original item(s). In some cases, a surrogate\nSharpens In-Pit Crushing Focus E & MJ\nIt is fully automatically controlled and adjustable from the operator’s control room. has used new and patent-pending technology to achieve a unique and recently developed stability concept for the PF300. This unique idea allows hydraulic excavators or rope shovels to load the crusher without any temporary support.\nRoll Crushers McLanahan\nTriple Roll Crushers. Triple Roll Crushers are ideal for producers who want to accomplish two stages of reduction in one pass. They can be used in coal, salt, coke, glass, and trona operations, among others. Triple Roll Crushers combine a Single Roll Crusher with a Double Roll Crusher to form a crusher that is capable of achieving a 6:1 reduction ratio in the primary stage and a 4:1 reduction\nJay F. Colinet and Edward D. Thimons National Institute\nemerging control technologies. This paper will summarize control technologies routinely being used in underground coal mine operations, as well as provide a review of new controls that are being pursued. Introduction. The Mine Safety and Health Administration (MSHA) establishes and enforces regulations for the mining industry in the United States.\nMSHA - Coal Mine Fatal Accident Investigation Report\nThe mining method used is room and pillar with full pillar block extraction. Coal is mined utilizing one continuous mining machine, three shuttle cars and belt conveyors. The blowing mine ventilation fan produces 167,631 cubic feet per minute (cfm). There is 58,812 cubic …\nIPCC mobility: Mobile Crushing Station FL\nThe plant can be equipped with a sizer, double-roll or hybrid crusher to suit varying demands. The PF300 is the most mobile design, able follow hydraulic- or rope-shovels at the mining face. The stable triangular footprint of the crawler tracks lets the crusher operate without any additional ground support.\nAppendix F: Underground Coal Mining Methods and\nFIGURE F-1 Schematic of a room-and-pillar coal mine section. SOURCE: Arch Coal, Inc., 2012. FIGURE F-2 Details of the longwall face. SOURCE: EIA, 1995. There are several variations of each method. However, ever since the introduction of continuous miners in the late 1940s, room-and-pillar continuous mining has been gaining ground over conventional room-and-pillar mining.\nWEEKLY INCIDENT SUMMARY\nSep 04, 2020· materials mine Fire or explosion A control room operator saw a fire in the crusher house through the closed-circuit television (CCTV). The operator entered the crusher house to try to extinguish the fire. A short time later, a loader operator saw the control room operator leave the crusher house and collapse. The control room\nppt draft.docx - ENSURING CUSTOMER SUCCESS LONG-TERM …\nENSURING CUSTOMER SUCCESS. LONG-TERM. There are ten open-pit coal mine shafts and 18 limestone quarries in North America. In 2018, 38.5 million tons of coal and 46 million tons of limestone were transported. We have more draglines than anywhere else in the United States (about 50), and control 161,000 open-pit mines, more than 194,000 tons of coal and mineral rights, and involve …\nDust Control in Underground Coal Mines - Industry Review\nFeb 24, 2021· Dust control in underground coal mines targets coal mining processes. Underground coal mining opens up one or more shafts into the earth. These shafts follow coal seams too deep for surface mining methods. There are two main methods for underground coal mining. Room-and-Pillar on seams that are flat or dipping.\nCrushing and conveying equipment - Surface mining\nIdeal for coal, pet coke, aggregates, and other industrial minerals; Throughput ranges vary by material density, with capacities up to 4,000 MTPH in coal; Heavy-duty engineered conveyor chain; Multiple bolt-on/plug-in options/upgrades available; Low installation cost: locate on solid, level ground, connect to power, and operate.\npower plant coal conveyor coal link plc control system\nProject Report On Plc Based Coal Crusher Conveyer System . World Engineering Services Private project report on plc based coal crusher conveyer system.Your loKuntangion equipment used in mining mineral ores > automation . Chat Online; an intelligent conveyor control system for coal handling . block diagram of how a coal thermal plant working.\nMines Along the Montour\nPreparation Equipment Mechanical Screens, Picking Tables, Loading Booms & Coal Cleaners Montour No. 10 Mine P.O. Library, Allegheny Co. Pa. Ship Library, M. R. R. Seam Pittsburgh 62 in. Thick Kind of Opening Drift Daily Cap 5000 T. Preparation Equipment Mechanical Screens, Picking Tables, Loading Booms & Coal Cleaners Moon Run Mine\nControl Coal Dust\neasily in water at room temperature, readily rinses from equipment, has a long shelf life without special storage loader, crusher and shearer to aid in coal wetting and biodegradable wetting agent for dust suppression and control in longwall coal mining operations. 3M™ SDS2 Dust Suppressant\nFACT SHEET: Biden Administration Outlines Key Resources to\nApr 23, 2021· Coal communities and workers could be well-positioned to see new industrial jobs extracting critical materials from the waste left behind by coal mining and coal …\nPrevention Miners' health matters\nFor more information about dust control in: underground coal mines, see Recognised standard 15: Underground respirable dust control (PDF, 2.3MB) surface coal mines, see Recognised standard 20: Dust control in surface mines; mineral mines and quarries, see QGL02 Guideline for management of respirable dust in Queensland mineral mines and quarries.\nABB process control and automation solutions for mines\nABB Mine Hoisting Systems at Longgu Coal Mine, China Advanced control system based on AC800M, state-of-the-art drive system and robust mechanical solution 800xA control system for gearless mill drive at Cajamarca mone in Peru\nNFPA 120, Standard for Fire Prevention and Control in Coal\nHelp ensure you are up-to-date with cutting-edge criteria for elevating fire safety in coal mining operations. NFPA 120, Standard for Fire Prevention and Control in Coal Mines, is a vital resource for everyone, from facility and risk managers to engineers, installers, and enforcers. Keep your knowledge on the current by placing an order for\nCoal preparation plant - Wikipedia\nCoal needs to be stored at various stages of the preparation process, and conveyed around the CPP facilities. Coal handling is part of the larger field of bulk material handling, and is a complex and vital part of the CPP.. Stockpiles provide surge capacity to various parts of the CPP. ROM coal is delivered with large variations in production rate of tonnes per hour (tph).\nPrimary Crusher - an overview ScienceDirect Topics\nIn general, coal crushers are categorized into two types and three types of primary crushers, as follows: Primary coal crusher: Used for larger coal size. The primary coal crushers may be of different types such as: (1) coal jaw, (2) coal hammer, and (3) ring granulator. Secondary coal crusher: Used when the coal coming from the supplier is\nPortable Rock Crushers - Metallurgist & Mineral Processing\nDec 16, 2020· Mining plans relying on drilling and blasting for fragmentation control will, no doubt, show greater uniformity in size of oversize, but great variations are to be expected in the size distribution of ROM ore from mine to mine. Assuming a successful crusher can avoid direct attack of the three-to-five font major fragment dimension indicated in\nNoise Assessment of Stone/Aggregate Mines: Six Case Studies\nPrimary jaw crusher, B JCr1 Outside control room 93 consists of face drilling, shooting and min- Primary jaw crusher, B JCr1 Inside control room 75 ing the main limestone bench, followed Primary jaw crusher, B JCr1 Lower levels 88-105 by drilling, shooting and removing the …\n(PDF) Application of SCADA Systems in the Coal Mining Industry\nquality of the ore crushing in crusher plant the control room, proce ssing Industrial Research Organization and are used in the coal mining industry to delineate the coal seams"", 'Mining Topic: Monitoring Ventilation Parameters and Accumulations of Combustible Gas\nWhat is the health and safety problem?\nThe proper control and distribution of ventilation air in working areas of underground mines is crucial to the health and safety of mine workers. Many underground coal mines cover vast areas where workers are not located. Continuous knowledge of the ventilation system status, along with information on the presence of methane or products of combustion from fires, is critical for detecting and correcting problems in the mine atmosphere in their earliest stage.\nTo this end, monitors can be used in targeted areas of the mine to collect environmental data on levels of combustible gas or products of combustion that may provide an early indication that heating is in progress. In addition to levels of combustible gas, monitoring airflow velocity and direction continuously can improve the safety of the underground workforce. However, continuous measurements of air velocity may not always provide accurate assessments of ventilation conditions, as these values can change rapidly depending on measurement location and airflow conditions.\nWhat is the extent of the problem?\nUnder MSHA regulations for atmospheric monitoring systems, monitoring in active mining areas usually involves methane and products of combustion around electrical and belt installations. Monitoring of ventilation airflow velocity is less common, with little information gathered on airflow direction. Large portions of many underground operations may not be continuously monitored for airflow velocity and methane accumulations.\nHow is OMSHR addressing the problem?\nThe Office of Mine Safety and Health Research (OMSHR) is investigating the use of ultrasonic velocity instrumentation in the underground mine environment. Ultrasonic anemometers are known for their accuracy, capability to measure low airflows, continuous measurement characteristics, and ability to provide a directional sign to the air velocity. This can be especially beneficial in situations with low air velocity in that airflow reversals may occur.\nCurrent OMSHR research efforts are focused on the installation of integrated airflow and methane monitors in active areas of underground coal mining operations. This work is identifying specific sampling protocols for these devices and is assessing the behavior of these instruments in relation to the presence of airway obstructions such as personnel and equipment. OMSHR is also identifying potential applications and limitations of integrated air velocity and methane monitoring systems and developing guidelines to interpret air velocity and methane monitor outputs.\nWhat are the significant findings?\nOMSHR laboratory testing showed that air velocity readings from continuous recording instruments correlated well with those obtained using standard measurement techniques. Obstructions upstream of a continuous recording anemometer led to noticeable impacts on air velocity readings, suggesting that movement of these obstructions in the ventilation airstream could be correlated to changes in instrument readings. In conditions with steady airflow, recorded velocities showed little variation over time, whereas turbulent conditions produced greater variability. This work also identified the minimum number of instrument readings needed to obtain an accurate assessment of ventilation air velocity in both steady and turbulent airflow conditions.\nWhat are the next steps?\nFuture work in this area will deploy sensors in underground coal mine environments to collect long-term data on airflow velocities and directions and on methane levels. This information will be analyzed for any trends that suggest the development of ventilation conditions conducive to accumulations of methane gas.\nNoteworthy Publications & Products\n- Composition Change Model for Sealed Atmosphere in Coal Mines (2010-06)\nThis paper presents a mathematical model based on the conservation of mass principle describing the flow of air (nitrogen and oxygen), methane, and carbon dioxide into and out of a sealed atmosphere and time-dependent changes in gas concentration.\n- Methods to Determine The Status of Mine Atmospheres - An Overview (2006-03)\nThis paper serves as an overview to remind and/or instruct readers about gas-sampling methodologies and gas analyses to assist in determining the status of underground atmospheres.']"	['<urn:uuid:afd028b6-188a-4d95-a344-cf06863bb40b>', '<urn:uuid:9c6b6e23-fb39-4165-880a-4befdcb479e1>']	factoid	direct	verbose-and-natural	distant-from-document	multi-aspect	expert	2025-05-12T20:11:36.665104	22	77	3065
32	how many exams needed cissp security plus certification compare	The CISSP and Security+ certifications have different exam requirements. The CISSP certification requires taking one 6-hour exam with 250 questions (or 100-150 questions for the computer-adaptive English version). In contrast, the Security+ certification requires only one exam, which is an advantage compared to other certifications like Microsoft's MCSA or Cisco's CCNP that require multiple exams.	['We will discuss CISSP Certification Cost, Requirements, and Training in this article. Many people want to get the Certified Information Systems Security Professional (CISSP) certificate, but it is one of the most difficult and sought-after ones in IT. Today, many enterprises have highly sought-after expertise in the design, implementation, and management of world-class cybersecurity initiatives. Therefore, companies are eager to hire CISSP-certified employees.\nTo be successful on the exam, candidates must have a thorough understanding of cybersecurity and practical expertise in the industry. Getting the CISSP certification on your first try is possible if you know what to expect from the exam.\nAs a result, you can expect to land a job as a security consultant, security auditor, security consultant, or security system engineer after obtaining the CISSP credential. As a CISSP, you’ll be tasked with developing workplace procedures and methods for safeguarding computer networks. To protect the assets from the outside world’s threats, you’ll be integrating security mechanisms into the IT networks.\nWhat Is the CISSP?\nThe acronym CISSP refers to a person who has achieved certification in information system security. The International Information Systems Security Certification Consortium, also known as (ISC)², assesses IT professionals’ knowledge of advanced IT security threats, controls, and technologies.\nCISSP certification benefits from being vendor agnostic. In this way, you may get the hands-on experience you need without having to settle with just one platform.\nThe CISSP exam is six hours in length. ” Each of the eight domains of the (ISC)² Common Body of Knowledge is represented by a set of 250 multiple-choice and advance questions (CBK).\nHow Much Does the CISSP Certification Cost?\nThe CISSP certification exam typically costs $749, although the exact price and additional fees vary depending on where you sit for the exam. An additional $50 fee will be assessed for rescheduling your exam. You’ll have to pay $100 if you have to cancel.\nFor the next three years, you will be able to keep your certification if you complete the exam. During that time, you’ll have to pay an annual fee and submit 40 CPE credits each year for maintenance.\nAfter three years, you’ll have to recertify if you don’t renew your certification.\nCISSP Courses Cost\n|Location||CISSP course fee|\n|USA / Canada||US$ 2000 – US$ 2800|\n|India / Pakistan||US$ 300 – US$ 600|\n|Europe||US$ 2600 – US$ 3200|\n|UAE / Saudi Arabia||US$ 800 – US$ 1300|\n|New Zealand / Australia||US$ 2000 – US$ 2600|\nCourse providers in your area that provide CISSP classes will assist you if you want to learn in person. These organizations may offer the CISSP course regularly, and some may also provide customized training for individuals. You can talk to them about your alternatives and pick the best one for you.\nHowever, CISSP classroom courses are relatively expensive. The cost of classroom-based training is substantially more than that of online or self-paced choices; therefore, this kind of training may increase the total cost of your CISSP certification.\nWhat Are the CISSP Certification Exam Requirements?\nThe CISSP certification has two primary prerequisites. The first and most important thing to remember is to complete the exam. In a moment, we’ll go into greater depth on this.\nIn the meantime, let’s talk about the other requirement: work history. (ISC)² mandates that CISSP certifications be for experienced professionals only, as indicated above. CISSP certification requires five years of full-time job experience in two of the eight CISSP domains listed above to be eligible for certification. A year of experience can be substituted for a college degree or another (ISC)² approved certification, and internships and part-time work can contribute towards this criterion. The fine print can be found on the (ISC)² website.\nTo earn and retain your CISSP certification, you’ll have to pay fees, which we’ll discuss later in this article.\nWhat are the CISSP domains?\nThe subject matter that the CISSP certification covers is called CISSP domains. CISSP is an advanced certification. Thus, it isn’t for everyone who wants to pursue it. Experienced cybersecurity practitioners, managers, and executives can use it to certify that they have a thorough understanding of the various principles of cyber security.\nThe domains in the CISSP certification are:\n- computer security\n- communications and network security\n- information security – operations\n- information security – design\n- physical (environmental) security\n- legal, regulations, policy, and governance\n- business continuity management; risk management\n- applied cryptography\n- software development security.\nWho should get a CISSP Certification?\nThe “gold standard” of security certifications, the CISSP, has been referred to by some as just that. You’ll typically find a CISSP is a requirement or at least highly recommended while screening cybersecurity positions. It’s beneficial if you’re considering a United States federal government career. The CISSP certification is a sign of an infosec generalist because of the wide range of technical expertise it requires.\nCISSP isn’t for everyone, however. A CompTIA Security+ certification may be a better option for those just starting in their careers in CISSP due to its lower entry-level requirements in terms of technical knowledge and work experience. The CISSP exam also tests your knowledge of management and technical skills, which is why you’ll need some work experience before you begin the CISSP path.\nTips for Passing the CISSP Certification Exam\nThe CISSP exam is notoriously difficult. You must score at least 700 out of a possible 1,000 points to demonstrate your knowledge and expertise in each topic and pass the certification.\nCISSP certification exams are difficult to study for; therefore, here are the tips for passing the CISSP certification exam to help you succeed:\nLocate a Variety of Research Resources\nAn outline, a study guide, sample tests, and more are all available to help you prepare for the (ISC)2 certification exam. Download an official study software, buy the official textbook and use the CISSP flashcards for additional preparation options. Even though all of these resources are beneficial, passing the exam on your own is difficult.\nTo succeed on the CISSP exam, you’ll need to use various study methods, including self-study, CISSP Boot Camp, and formal CISSP Training. CISSP Courses can be taken in a classroom with other professionals, in a small group with friends, or privately with an instructor.\nForm a Team With Your Friends\nStudying in a group can lead to wonderful group conversations and help you better understand subjects you don’t understand. Attending a CISSP training course lets you meet others going through the same subject as you.\nYou can also search for discussion forums online or start your study group. Attend meetings for security professionals to make new friends and learn from others who have already passed the exam.\nMaintain a Healthy Work-Life Balance\nThere is a lot of work involved in preparing for the CISSP exam. You’re probably juggling your studies with a full-time job and other commitments outside of school. Scheduling particular study times is the most effective technique.\nTo be fully prepared for the exam, it’s essential to balance studying and completing practice exams. The week before your exam, plan to study for at least a month to avoid cramming in the last few days.\nThe CISSP exam\nThere are about equal numbers of questions from each domain listed above in the CISSP exam. The CISSP exam outline includes a breakdown of the content and a description of what to expect on test day. “Advanced inventive things” are referred to as multiple-choice questions. Identifying diagram elements and dragging and dropping responses from one side of the screen to boxes on the other are two types of questions that sound more difficult than they are.\nThe exam is computer-adaptive, and this is how it is administered in English (CAT). Essentially, this means that when you take the exam, a computer monitors your progress and adjusts to your questions. In this version of the test, there are between 100 and 150 questions, and it takes around three hours to complete. There are 250 questions, and it takes roughly six hours to complete the test in all other languages, which is linear (meaning that the questions are the same no matter how you answer). A 700-point minimum is required to pass either type of test.\nIt’s easy to schedule your exam with the help of (ISC)²’s resource page, which also provides information on exam formats and what to expect when you take the test. Check out Dex Yuan’s LinkedIn article and the (ISC)2 community forums and Reddit if you’re looking for real-world test-takers reports on how the exam experience went. It’s nice that you can see a preliminary result of your performance at the testing location so that you can plan accordingly.', 'It does not matter what cybersecurity certification is done, the question of if the certification is really worth the effort and time would inevitably arise.\nThe CompTIA Security+ certification is no doubt one of the most popular certifications out there, and it is no wonder that numerous prospective certification applicants ask if the Security+ certification is truly worth it.\nSo, is the Security+ certification worth it? A short answer would be yes.\nThe CompTIA Security+ certification is definitely worth the time and effort if you have intentions of pursuing a career in cybersecurity, or if you are interested in adding security qualifications to your CV.\nIt is also useful if you require knowledge of network security for your current role.\nWhile this might sound pretty straightforward, it still doesn’t help answer what exactly it is about Security+ that makes the effort put in worthwhile, at least compared to the many other certifications.\nThe aim of this article is to delve deeper into the numerous aspects of Security+ to find out why it is a great certification to attain.\nWhat is CompTIA Security+ Certification?\nThe Security+ certification is an IT security certification which develops your expertise and skills in network and computer security domains such as network security, IT risk management and cybersecurity. It should be seen as a lower level cybersecurity certification which covers the following topics:\n- How to recognise risks in a connected network\n- How to utilise tools, techniques and technologies to safeguard hardware and software assets from hostile parties and hackers.\nA Security+ certification can help you gain jobs in the network security and cybersecurity fields, with roles such as information security specialist, penetration tester, security engineer, security administrator, and network administrator.\nWhat are the skills measures by the Security+ certification?\nThe integral skills that the CompTIA Security+ certification validate include threat analysis, configuration and installation of secure applications, cryptography, risk mitigation techniques, understanding types of attacks and proffered solutions, cybersecurity law and policy, network protocols and layers, mobile security, forensics and architecture design with maximum risk mitigation.\nEvery one of the skills covered is bound to make you knowledgeable of the fundamentals to the point where you are able to recognise the possible risks found in cybersecurity.\nYou will also understand the required solutions that every organisation can utilise to safeguard their data, hardware and software assets.\nGiven that the Security+ certification comes with performance and lab-based questions, you have the opportunity to gain hands-on introspect and experience in discovering solutions to complicated problems associated with cybersecurity and modern networks.\nYou do not need experience to attempt the Security+ exam\nWhen you climb up the ranks in the cybersecurity industry, you will discover that quite a number of the renowned certifications come with prerequisites like having a particular number of years of recorded experience.\nIt could also be that you have to have completed a sanctioned training before you can even take the CompTIA Security+ certification exam. There are also certain exams that require you to be sponsored by an individual that has already been certified.\nA wonderful benefit that the Security+ certification offers is that there really is no previous training or experience required before you are able to take the exam. You can simply select a date for the exam and take it.\nWhat this means, however, is that you have to be adequately prepared before taking the test. Nevertheless, this is a wonderful benefit for individuals just entering the computer networking or cybersecurity industry, as they are able to attain a certification in a time when adequate roles for job opportunities can be scarce and college degrees can take longer to finish.\nThe CompTIA Security+ certification is something that can be done in about 60 days, as long as you prepare and study well.\nSecurity+ meets 8570 Requirements and is approved by the United States Department of Defence\nThe US DoD has a directive known as 8570 which offer guidance on how its direct employees and those working on behalf of the Department of Defence in information assurance or cybersecurity tasks have to be trained and gain certification.\nThis directive ensures that Security+ is the foundational certification for the majority of the work. What this means is that the Department of Defence understands and recognises the value and validity of the Security+ certification. This is clearly evident that it actually requires it for particular roles.\nWhen a certification is recognised and cleared by such an important part of the national governments, contractors and vendors are likely to reap the rewards of having a CompTIA Security+ certification.\nSecurity+ training can be offered almost everywhere\nThe Security+ certification is so prevalent that the majority of universities and colleges offer training for the exam at certain times in their academic calendar. It is even possible to use online training resources.\nMoreover, there are numerous resources for online courses available, so prospective Security+ candidates can prepare and study for the exam. The quality offered by these courses tend to vary, so it is important to pay close attention to the training resources you utilise.\nNow that the wonderful features that make the CompTIA Security+ certification a worthwhile endeavour, it is now time to consider other questions you might have concerning this certification.\nCompared to numerous other certifications, Security+ is cheaper\nCertifications are not cheap by a long shot, and certain cybersecurity-related certifications tend to cost a minimum of $500, which is quite a large sum of money to part with if you are not sure you are going to pass.\nThankfully, CompTIA seems to understand this and has ensured that its prices are reasonable, particularly for Security+ certification which costs a bit more than $300.\nThere are also discounts that can be had as CompTIA lets students take the Security+ certification for about $200 in certain instances.\nIs it possible to retake the Security+ if you don’t pass the first time?\nCompTIA enables certification candidates to retake the Security+ exam if you don’t pass. It even offers the opportunity to purchase a testing voucher with a retake option.\nNevertheless, if you are unable to pass the exam on the second try, you will be temporarily blocked from taking the exam. Generally, this block lasts about 2 weeks.\nHow much time do you have to devote to studying for the Security+ certification?\nHow much time you need to devote to studying and the effort required typically differs from one individual to the next.\nIt depends on your experience and knowledge, however, a great yardstick is typically around 50 hours of devoted study, before you are truly ready to take the Security+ certification exam.\nYou only have one exam to take with the Security+ certification\nThis is a great benefit of this certification. You only require a single exam. Certain mid to high-level certifications like Microsoft’s MCSA or Cisco’s CCNP requires numerous exams.\nWhile there is nothing wrong with that, those multiple exams do not necessarily equal a large increase in either career prospects or salary. This can be a negative considering just how much work is required for them.\nWith a certification that has just one exam like the CompTIA Security+, you only need to prepare for a single exam and once you pass it you are certified. That makes it a much more enticing prospect to take on.\nWill having a Security+ certification instead of the Network+ raise eyebrows in the job market?\nQuite a lot of people become concerned about having certain certifications and skipping others.\nIn the majority of cases, employers do not seem to question why a prospective candidate doesn’t have a particular certification.\nThe lack of a supposed lower certification is not something that is held against a prospective employee.\nThe majority of employers actually value certifications, particularly higher-level ones and they tend to presume the candidate in questions is knowledgeable enough in the lower aspects.\nDo I require experience to successfully take the Security+ certification?\nAs stated earlier in the article, it is possible to take the exam at any point, with no need for previous work experience. However, this does not mean it should be the case.\nIf you are interested in taking the Security+ exam and be successful without any previous cybersecurity-related work experience, it is imperative that you have a robust knowledge of computer networking.\nThis knowledge should include the understanding of protocols, the OSI model, port numbers as well as the functions of networking hardware components like routers, firewalls and switches.\nWhat this means is that you have to be particularly strong on the majority of CompTIA’s Network+ topics before you attempt to take the Security+ exam.\nThis, however, doesn’t mean you have to gain this knowledge from an actual cybersecurity-related job, or that you need to first take the Network+ certification exam.\nWhat it does mean is that you will have to show proficiency in these areas, before you can move onto the topics Security+ has.']	['<urn:uuid:455e73c3-285d-4ec5-9eeb-ab099891d50c>', '<urn:uuid:e3a92fdb-7b05-45a0-a561-d03b1e8b521a>']	open-ended	with-premise	long-search-query	distant-from-document	comparison	novice	2025-05-12T20:11:36.665104	9	55	2932
33	As a city planner interested in heat monitoring infrastructure, I'd like to understand the key differences between fixed weather station placement requirements versus smartphone-based urban temperature monitoring coverage - what are the main tradeoffs to consider?	Weather stations have specific placement requirements - they need wide transmission ranges to send data from rooftops or backyards to their consoles, must be elevation-friendly to work in various locations, and require weather-proof construction with shielded outdoor sensors to maintain accuracy. In contrast, smartphone-based monitoring offers more flexible coverage through already-deployed devices throughout urban areas. While traditional meteorological observation stations are limited to fixed locations and satellites only collect data at set intervals, smartphones provide widespread, real-time temperature data across neighborhoods. The smartphone approach helps avoid issues like vandalism that could affect urban thermometer deployments, though it requires large numbers of users to average out individual phone usage effects. Weather stations offer precise measurements at specific points, while smartphones enable broader urban coverage to study how different neighborhoods heat up and how temperatures change throughout the day.	"['How To Monitor Urban Weather...With Your Smartphone\n(Inside Science) -- Smartphones let us send messages, play games and watch cat videos at any time or place. Now these sophisticated instruments can boast another feat: measuring urban temperature.\nTracking the temperatures in cities is particularly important because of the so-called urban heat island effect. Simply put: a city tends to be warmer than its surrounding area. This temperature boost comes from a number of factors, including the heat-absorbing materials common to cities like cement, asphalt, and buildings.\nThat extra heat means city dwellers, who make up about 80 percent of the U.S. population, must use more electricity to stay cool, while suffering discomfort and health risks. The effect also exacerbates heat waves, which can cause many deaths, particularly among the very young and the elderly.\nYet the global population continues to urbanize—as of 2007, most humans live in cities. Building cities that resist the urban heat island effect means learning more about how existing cities heat up. Do certain neighborhoods stay cooler than others? How does the temperature change throughout the day?\nTo answer those questions, scientists need sensors spread throughout the urban area, capable of capturing information in real time. That rules out meteorological observation stations, which sit in one place, and satellites, which only collect data at set time intervals. Even an urban deployment of thermometers could fall victim to vandalism.\nInstead, why not use the ubiquitous smartphone? ""It\'s a very innovative way to look within urban areas,"" said Matei Georgescu, who researches the environmental impact of urbanization at Arizona State University, in Tempe. He was not involved with this study.\nSensors keep track of smartphones\' battery temperatures, which reflect their surroundings. Of course, the battery temperature is also affected when a phone is used heavily or held in a pocket. But when the data from hundreds of thousands of phones is combined, these sources form a constant noise signal that can be measured and eliminated.\n“The important thing is that you have enough users, so that when you average over that pool of users, you get an even readout, instead of being influenced by one guy who\'s just using his phone a lot and heating up the battery,"" explained James Robinson, a co-author of the paper, and the co-founder and chief technology officer of OpenSignal.\nOpenSignal is an application that collects smartphone data to learn about the factors that weaken signal strength. One source of this data, at least in Android systems, is the battery.\nAccording to Robinson, ""We started to analyze the temperature data. The strongest correlation we found was between the battery temperature and the air temperature of the location the reading was taken that day.""\nOf the estimated million battery temperature readings OpenSignal collects every day, the team focused on phones in eight cities around the world. The average battery temperature in any given city varies with the outdoor air temperature, but tends to run a little warmer. To translate from battery to environmental temperature, OpenSignal wrote an algorithm relating the two.\nThen collaborators at Wageningen University, in the Netherlands, and the Massachusetts Institute of Technology, in Cambridge, refined the algorithm and turned it into a heat transfer model, to account for the factors that influence battery temperature, such as the phone\'s insulation, the owner\'s body temperature, and of course the outside temperature.\nThe model, which was published in Geophysical Research Letters, accurately translated battery temperature into air temperature. When the researchers put in battery temperature data and asked the model to give them the air temperature, in most of the cities, such as Los Angeles, it was only off by about one degree Celsius.\nThe model did not fare as well, however, with the data from Moscow and Paris. To fix these errors, the researchers suspect they may have to take seasons into account. For example, during the winter months, people spend more time indoors than they do during the balmy days of summer. This change in behavior could skew the results.\nEven with its inaccuracies, the model can still help urban heat island researchers like Georgescu. He envisions the smartphone estimates of air temperature as a complement to other data sources, such as satellites. ""Satellite overpasses don\'t occur every minute,"" he explained. ""With smartphones, you basically have real-time information.""\nOn the other hand, Georgescu\'s ASU colleague David Hondula, who was not involved in this study, wants to use smartphones to track the heat exposure of individuals. ""If your phone is tracking your weather conditions, you might have a personalized alert that you have experienced excessive heat as you\'ve moved through your daily life,"" said Hondula. When informed of their heat exposure, people can take steps to protect their health, and thus reduce heat-related mortality.\nIn Hondula\'s words, ""There\'s certainly the potential for many important questions to be answered, if everyone\'s walking around with a potential environmental monitor in their pocket.""\nSophie Bushwick is a freelance science writer based in New York City. Her work has appeared in numerous print and online outlets. She tweets at @sophiebushwick.', 'A weather station is normally used to give information on the weather conditions of a particular area at a particular time. For the weather station to work right and be able to give information that is as accurate as possible, the right one has to be chosen for the job. Below are factors that can help you to select the right station.\nWhen it comes to the right weather station, its accuracy is a very important determinant. Apart from being able to provide you with accurate information on the current weather conditions, it should also be able to provide you with a very accurate forecast.\nSensors of various types\nA good weather station is one that is able to provide you with information on the various weather elements like wind, humidity, and temperature and so on and so forth. For a weather station to be able to give this information, it needs to be fitted with sensors to read this information it is thus important that you check the sensors that a certain station comes with in regard to the information that you will want to be collecting.\nWide transmission range\nFor one to be able to get accurate reading form their station, they might be forced to place them far like on top of a roof or out in the backyard. This means that the station will be quite far away from its console. It needs to have a wide transmission range that will allow it to easily send information back to the console from wherever it has been placed.\nHigh elevation friendly\nIt is important to work with a weather station that can be placed anywhere, even high on hilly areas and mountains and still be able to transmit the required data. This can only be possible if the weather station has a high elevation feature.\nFast transmission and updating speed\nFor the data that you are collecting to be accurate, the station will need to have very fast updating and transmission speed. Remember that weather conditions can fluctuate very fast and they keep on changing, your weather station has to be able to keep up with these fast changing speeds.\nOrganized forecast and reading display\nIt goes without saying that organized data will be much easier to read and understand.\nShielded outdoor sensors\nRemember that the sensors usually come exposed as they form the basic part of the weather stations and they are the ones doing all the measuring. It is however that they are well protected to maintain their quality and accuracy as their being exposed also means that they will be affected by the weather conditions out there such as too much heat. This will affect the accuracy of the information thus it is important to ensure that the sensors are well shielded.\nStrong construction and weather-proof\nYou need to ensure that the materials used in constructing the station are of very high quality to enhance durability and to also ensure that they will not be adversely affected by the various weather conditions.\nAdditional features are definitely an advantage as you will get to have much more information on the weather conditions.\nIt is important to understand that these stations happen to be extremely fragile and caution would be taken to select just eh right one. This can only be achieved when one understands the various features of the stations and knows exactly what they will need the stations for.']"	['<urn:uuid:8c61662f-b796-48d9-aedf-377942d2b1b4>', '<urn:uuid:376a2161-74f6-441f-adc0-c560cf5951f5>']	open-ended	with-premise	verbose-and-natural	similar-to-document	comparison	expert	2025-05-12T20:11:36.665104	36	137	1418
34	bond strength comparison break junction molecular electronics gold sulfur vs gold gold	The Au-S (gold-sulfur) bond is stronger than the Au-Au (gold-gold) bond, which causes gold atoms to be pulled and migrate to the electrode ends forming elongated tips when the electrodes are separated.	['S. Wu, M.-T. González, R. Huber, S. Grunder, M. Mayor, Ch. Schönenberger & Michel Calame\nUniversity of Basel, Klingelbergstrasse. 82, CH-4056 Basel\nAdapted from S. Wu et al., Nature Nanotech. 3, 569 (2008).\nIf individual molecules are to be used as building blocks for electronic devices, it will be essential to understand charge transport at the level of single molecules. Most existing experiments rely on the synthesis of functional rod-like molecules with chemical linker groups at both ends to provide strong, covalent anchoring to the source and drain contacts. This approach has proved very successful, providing quantitative measures of single-molecule conductance, and demonstrating rectification and switching at the single-molecule level. However, the influence of intermolecular interactions on the formation and operation of molecular junctions has been overlooked. Here we report the use of oligophenylene ethynylene molecules as a model system, and establish that molecular junctions can still form when one of the chemical linker groups is displaced or even fully removed. Our results demonstrate that aromatic π-π coupling between adjacent molecules is efficient enough to allow for the controlled formation of molecular bridges between nearby electrodes.\nTo determine the electronic properties of devices based on single molecules [1, 2, 3], a single or a few molecules need to be wired between at least two electrodes, a source and a drain electrode. We use a gold wire with a constriction in its center that is continuously stretched, resulting in a narrowing of its diameter at the constriction (Fig.1a & b). This process is carried on down to the atomic scale and ends with the breaking open of the gold bridge, giving access to two atomic contacts . This technique, termed mechanically controllable break junction (MCBJ), can be used to form molecular junctions in a liquid environment [5, 6]. Traditionally, molecules are synthesized with two terminal anchor groups (typically -SH) at both ends which allow the immobilization of the molecule between the two atomic contacts. The electrical conductance G is measured while opening the junction. When the Au bridge is stretched, G(z) decreases, showing conductance plateaus for G values above the quantum conductance unit G0 ≡ 2e2/h (Fig.1c). There is a so-called “last plateau” at G ≈ G0. This last plateau corresponds to a single atom Au bridge. If the junction is elongated further, it breaks open. The down-jump in conductance typically stops at a value of G ≈ 10-3 G0, when electron tunneling between the electrodes sets in. Electron tunneling with a constant tunneling barrier height results in a linear dependence of log(G) versus z as observed in the measurements. To form molecular junctions, the breaking process is performed in presence of a solution containing molecules bearing two anchor groups. Because a Au-S bond is stronger than a Au-Au bond [7, 8], Au atoms are pulled and migrate to the ends of the Au electrodes forming elongated tips when the electrodes are further separated apart. This process continues until the force which has been built up exceeds the limit given by the Au-Au bond. Then, the molecular junction breaks open. In our study we use conjugated oligo-phenylene ethynylene (OPE) molecules as a model system. Conjugated molecules are interesting candidates for electron transport due to the delocalization of electrons throughout the molecular backbone [9, 10]. Such a structure results in a lower HOMO-LUMO gap (~ 3 eV) as compared to that of saturated molecules (~ 7 eV), leading to a higher charge transport efficiency through the molecule. Measuring the electrical conductance G during the breaking process in presence of molecules, we can anticipate that G will stay approximately constant during stretching when a molecular junction forms until the junction breaks open (Fig.1d). A statistical approach is used, in which sequential open-close cycles are performed to repeatedly form molecular junctions. Conductance histograms are then built as shown in Fig. 1c & d and Fig. 2. A peak in the histogram, e.g. at G ≈ 10-4 G0 in Fig. 1d, represents the signature of the molecular junction formation. For molecules with a single anchor group, one would a priori presume that no stable metal-molecule-metal junction can form since the molecules cannot attach on both sides of the junction. The data in Fig. 2 compare conductance histograms obtained for OPE molecules bearing two (top) or only one (bottom) anchor group. While the peak in the top histogram is expected due to the presence of the two anchor groups, that of the second histogram comes more as a surprise. It clearly shows that stable molecular bridges can form, even if a single anchor group is present in the molecule. We think that the connection between the electrodes is made possible by π-π stacking interaction between a pair of adjacent molecules [11, 12, 13]. If one molecule is anchored via its thiol linker group on e.g. the left electrode, another one bound to the right electrode can complete the mechanical assembly of the junction via π-π coupling through the phenyl rings. This interpretation is supported by the shift of Gpeak to lower values by more than one order of magnitude. A reduced average conductance value can indeed be expected because a molecular bridge formed by a stacked pair of molecules will be longer (~ 29.1 Å) than a single dithiol molecule anchored between Au electrodes (~ 20.7 Å). The distance that electrons have to tunnel between the Au electrodes is therefore slightly larger for the stacked bridge. From the study summarized above, we can infer that intermolecular π-π stacking interaction between monothiol molecules composed of alternating phenylene and ethynylene units is strong enough to induce the formation of molecular junctions. This is a significant finding for molecular electronics. Intermolecular aromatic stacking plays a determinant role in stabilizing nanoobjects. The importance of π-π overlap has long been recognized in thin-film organic electronics, molecular mechanics, and especially in biomolecular and supramolecular chemistry. We show here that π-π stacking can also be used as the dominant guiding force for the formation of molecular bridges in few molecules electronic junctions. These experimental findings provide a strong ground for the design of future electro-mechanical and sensing devices operating at the single molecule level.\n Ratner, M. A. & Aviram, A., Molecular rectifiers. Chem. Phys. Lett.29, 27–283 (1974).\n Joachim, C., Gimzewski, J. K., & Aviram, A., Electronics using hybrid-molecular and mono-molecular devices. Nature 408, 541–548 (2000).\n Nitzan, A. & Ratner, M. A., Electron transport in molecular wire junctions. Science 300, 1384–1389 (2003).\n Agrait, N., Yeyati, A. L., & van Ruitenbeek, J. M., Quantum properties of atomic-sized conductors. Phys. Rep. 377, 81–279 (2003).\n Grüter, L., González, M. T., Huber, R., Calame, M., & Schönenberger, C., Electrical conductance of atomic contacts in a liquid environment. Small 1, 1067–1070 (2005).\n Grüter, L., Cheng, F., Heikkilä, T. T., González, M. T., Diederich, F., Schönenberger, C., & Calame, M., Resonant tunnelling through a C60 molecular junction in a liquid environment. Nanotechnology 16, 2143 (2005).\n Xu, B., Xiao, X., & Tao, N., Measurements of single-molecule electromechanical properties. J. Am. Chem. Soc. 125, 16164–16165 (2003).\n Huang, Z., Chen, F., Bennett, P., & Tao, N., Single molecule junctions formed via au-thiol contact: Stability and breakdown mechanism. J. Am. Chem. Soc. 129, 13225–13231 (2007).\n James, D. K. & Tour, J. M., Molecular wires. Top. Curr. Chem. 257, 33–62 (2005).\n Weiss, E. A., Wasielewski, M. R., & Ratner, M. A., Molecules as wires: Molecule-assisted movement of charge and energy. Top. Curr. Chem. 257, 103–133 (2005).\n Kim, K., Tarakeshwar, P., & Lee, J., Molecular clusters of π-systems: Theoretical studies of structures, spectra, and origin of interaction energies. Chem. Rev. 100, 4145–4186 (2000).\n Watson, M., Fechtenkotter, A., & Müllen, K., Big is beautiful-”aromaticity” revisited from the viewpoint of macromolecular and supramolecular benzene chemistry. Chem. Rev. 101, 1267–1300 (2001).\n Hoeben, F., Jonkheijm, P., Meijer, E., & Schenning, A., About supramolecular assemblies of π-conjugated systems. Chem. Rev. 105, 1491–1546 (2005).\n[Released: March 2009]']	['<urn:uuid:c1ef986b-f371-4b77-8190-51fb00e0a816>']	factoid	direct	long-search-query	distant-from-document	single-doc	expert	2025-05-12T20:11:36.665104	12	32	1305
35	how manufacturing systems prevent quality problems inventory costs animal feed	Manufacturing systems can prevent quality and inventory issues through two main approaches. First, by implementing Just-in-Time production, which involves producing only what is needed when it is needed, manufacturers can reduce inventory costs and decrease waste from storage of raw materials. Second, by tracking metrics like takt time (customer demand rate) and cycle time (process capability), organizations can better match production to customer needs while maintaining quality. The shorter production runs associated with JIT enable quicker detection of quality issues like ingredient errors or contamination risks, preventing large-scale product waste and costly recalls that could affect animal health.	"['Are you managing “by the numbers”? And more importantly, if you are, are you using the right metrics to gauge your productivity?I think most people understand the concept of managing by the numbers, which usually means that their boss shares targets and they then do whatever necessary to hit ""the numbers"" especially those tied to performance evaluation, bonus, wage increase, or promotion. I am always amazed when I see this in organizations (which is a frequent condition): people find a way to meet their targets because it’s simply human nature to be result driven. The problem lies when we are asked to sift the sand to see if there is any gold there—most often there isn’t. In other words, a better question than “did you make the numbers?” might be: “what are the most important metrics you should pursue?”\nI was hired at TMMK before we were able to sell the vehicles we produced. This gave me the opportunity to see how all the components (of TPS thinking) came together to determine how we measured how efficient we were in our processes; while bringing the waste to the surface in order to improve and meet customer need. I learned that doing business this way meant that we couldn\'t mask problems very easily.\nMy trainers helped me to see how tracking certain key metrics helped understand what the customer needs from our organization (internal and external), and how well each process works to meet that.\nMy trainers helped me to see how tracking certain key metrics helped understand what the customer needs from our organization (internal and external), and how well each process works to meet that. We started by focusing on our “takt time” (a German not Japanese term). We needed to know how fast the customer was pulling from us (this can be any service, output or product). Determining this number invariably involves other departments (silos) within the organization like sales, purchasing, engineering, human resources, R&D, and suppliers. The process of figuring this out is often as rewarded as it is challenging. It certainly took discipline and accountability from our leadership. I encourage folks to work diligently to create an ""us""- not a ""we and they"" atmosphere!\nIn our case we determined takt from the previous three-month average pull. This dialed us in to the customer and helped us understand what we were capable of regarding other key components such as machine or process capacity (cycle time). If you aren’t capable of meeting what the customer needs when they want it, consider it a red flag. Alas, most organizations can’t tell you this much-- they just run wide open and stockpile inventory, which looks really good on paper if that’s what you measure.\nIt’s important for your organization to be able to differentiate cycle time and takt time. Cycle is what it takes for your process to meet the takt time (determined by customer demand). The two measures do not necessarily have to be the same, based on certain factors (leveling, mold or equipment changes). In my experience working in the plastics department we had factored in mold change time so our cycle time was actually faster than takt time to accommodate for “planned” downtime (of course always trying to minimize that time). It’s also crucial to perform production capacity studies for each process (manpower/equipment/machinery). Again, you must know production capability to recognize gaps to the customer need. (Please note the difference between total capacity; meaning I can just run the equipment 24/7 (if you are running to total capacity as the norm then common sense will tell you there will be problems meeting customer need), versus process capacity which can be a normal working day time requirement.)\nIf you were to create an ideal state you would want to know what your customer pull is, and then purchase the specific equipment that meets that need (cycle). Keep in mind however that even if I meet the ideal state today, tomorrow that may change. Built in to our production system at Toyota was the ability to adjust when the customer demand changed either way. We had to build in flexibility in our processes in order to remain competitive and not pass cost onto the customer. We did this by always understanding takt, cycle, capacity and manpower for every process.\nWhile most do not have that luxury of knowing the answers to all the questions above, they represent a vital starting point for anyone starting their Lean journey. These measure help an organization grasp the current state and understand how to identify waste, see ways to embed kaizen in those areas, and explore what other options are possible to effectively meet the customer (manpower, equipment upgrades, or outsourcing to name a couple). If this is the journey you are going down then it’s important to have leadership on board.\nOnce the takt, and process capacity are understood then it’s time to develop standardized work to assist in determining the manpower necessary for production needs. Each process knowing its capability must have standard work that involves specific steps with times to complete the cycle time. After these are developed, then Job Breakdown sheets are created for the key points and reasons. This allows each person to use Job Instruction training (JIT/TWI) to fully understand expectations so they can see abnormality at a glance and recognize potential improvements as they work the process every day.\nWe used a work combination table (example below)\nas the tool to help visualize the cycle time, equipment involved and standardized work. We referred to this board to see what the machine was doing and when, what the worker was doing and how much time per step, along with any walk time (even wait time) involved to fully visualize the complete cycle. We all shared this as the benchmark for future kaizen. This was done for all processes that created outputs. When you think about it, how you can do business effectively, sustain this for the long-term, and be flexible without understanding these key components? Without clear and simple metrics like this you can never measure how you are doing based on the customer’s expectation, and be flexible to their ever-changing needs.\nSo what does all this mean?\nOrganizations and the individuals within should know where they stand compared to the standard on an hourly /daily basis in order to make any meaningful improvements. In my experience we used “plan versus actual” boards for each process which visualized this information each hour, factoring in downtime we had that could have been equipment related, (such as training related activities, or andon pulls, etc.) We also tracked a variable called “wait Kanban”: the assembly shop we were providing parts for had downtime which in turn didn’t allow them to return their carts (kanbans) for replenishment (pull system), so instead of continuing to run and “stack” parts, we stopped. This time was not calculated as downtime, but as “wait kanban”, which didn’t count against our production efficiency. These activities were important to our work, and necessary to include so that we could extrapolate all this information.\nAfter our shift every day we were responsible for a daily report to calculate the productivity for our group which contributed to a department need; which supported the plant need. This report factored in our capability, our run time, downtime, repair, scrap, delay work, wait kanban, and supplier/vendor shortages. This gave us our daily “parts per hour” efficiency rate, which we based on the expectation which gave us our productivity rate for the day.\nWe knew every day where we fell short of the standard and used PDCA to avoid replicating the same problems every day. This was considered grasping the problem situation, or the first step of problem solving.\nIf a person doesn’t understand daily expectations based on takt time, cycle time, production capabilities, and standardized work then they are just haphazardly running till the next shift comes in to take over (vicious cycle). Every day we managed to the customer need, and not to a random number pulled from a hat that met an objective that might have looked good on paper short term. This approach marked the beginning of our leaning a new way of thinking. If a person doesn’t understand daily expectations based on takt time, cycle time, production capabilities, and standardized work then they are just haphazardly running till the next shift comes in to take over (vicious cycle). They are doing work that is not sustainable for long-term growth, nor are they developing the capability to understand how to improve. Although I’m describing a manufacturing setting this thinking can be applied in any industry where services or outputs are created and a customer has an expectation.', 'How the Just-in-Time inventory model may benefit animal feed manufacturers\nThe animal nutrition and feed industry is worth an annual £4.4bn to the UK economy. The UK has approximately 200,000 animal feed businesses involved in importing, producing, handling, storing and distributing animal feed.\nAnimal food production is not only important to the welfare of our pets and livestock, but also critical to the human food chain, it has implications for the composition and quality of livestock products such as milk, meat and eggs that people consume.\nTraditionally manufacturers have forecasted demand for their products into the future and then have attempted to smooth out production to meet that forecasted demand. At the same time, they have also attempted to keep everyone as busy as possible producing output so as to maximise “efficiency” and reduce costs. This type of production involves holding high inventories, often tying up space and cash flow.\nStoring raw materials and finished goods for any length of time in any business can lead to high storage costs, an increased head count to manage the storage and the implementation and management of storing processes. Storage can also create losses such as accidental damage, handling damage, obsolete stock and issues due to poor rotation.\nIt is not just a matter of acquiring space and implementing procedures to store the goods, it is a far more complex issue and may contribute to rising costs in terms of inspection, prevention and control further down the supply chain and accounts for some of the biggest losses to animal feed production.\nIn the case of animal feed, losses fall under the main categories of: weight loss; quality loss – which can lead to health risks such as contamination, and economic loss.\nStandards of animal feed production in the UK are extremely high and manufacturers have to adhere to strict guidelines, polices and regulations, so if product is stored, economical losses occur in the management, inspection, prevention and control of the feed to maintain the required standards.\nIn order to minimise these losses during storage, strict control must be taken of climate, hygiene, humidity, oxidisation, chemical changes in the atmosphere, turnover and rotation, traceability and the physical stacking of goods; all of these controls contributing to increasing costs.\nProducing goods to order can reduce inventory investment and waste. Producing goods to forecast may result in production runs being very long, with manufacturers believing that they are maximising efficiency. The problem with long production runs is that issues are often identified late in the process, whether that be an ingredients error or a potential cross-contamination risk, many tonnes of product might have to be scrapped. Worse case scenario is that product has already been shipped, meaning a costly re-call, potential public relations disaster or legal cases arising from unhealthy or deceased animals.\nJust-in-time manufacturing (JIT), also known as just-in-time production or the Toyota Production System (TPS), is a production and inventory strategy that companies employ to increase efficiency and decrease waste by receiving goods only as they are needed in the production process, thereby reducing inventory costs from the storage of raw materials and work-in-progress. Just-in-Time means producing what is needed, when it is needed, and in the amount needed.\nThe shorter production runs associated with JIT manufacturing enables quicker response times to specific customer orders and seasonal changes, whilst minimising inventory costs, reducing waste and mitigating many of the issues associated with storage.\nThis method does require manufacturers to accurately forecast demand, respond to the market quickly and develop strong relationships with suppliers, creating sustainable supply chains through collaboration. The just-in-time inventory model, by design, enables manufacturers be more efficient with their supply chains, lowering costs throughout the production process.\nWith this in mind, if animal feed manufacturers were considering the JIT model, they would need a reliable production platform, minimising the risk of production disruption, otherwise they may fail to deliver. With JIT there needs to be contingency built in with production schedules and order delivery dates.']"	['<urn:uuid:1efa39fc-5549-4073-bf09-d4e3c223b8ec>', '<urn:uuid:f7a79627-d038-40ef-bc11-03116b6af711>']	factoid	direct	long-search-query	distant-from-document	multi-aspect	novice	2025-05-12T20:11:36.665104	10	98	2122
36	different types of bees nesting habits tunnels	Different bee species have distinct nesting habits. Carpenter bees drill perfectly round, half-inch diameter holes in soft wood, creating tunnels that go inward for about an inch before turning and following the wood grain for six more inches. About 70% of solitary bee species nest underground in tunnels and burrows, while 30% nest aboveground in holes in logs and stems. Mason bees use pre-made holes and seal their nests with mud or clay, while leafcutter bees use pieces of leaves. Some species like sweat bees and carpenter bees prefer to excavate their own holes in various materials including ground, logs, reeds, or dead raspberry canes.	"[""- Other Rooms\n- Lawn & Garden\n- Deck & Patio\n- Repair & Install\nHow to Deal with Carpenter BeesBy: Julie Day\nChances are you’ve been buzzed by a large, intimidating-looking bee when working on the outside of your home. You may also have the telltale holes in your woodwork where these bees build their nests.\nAbout Carpenter Bees\nCarpenter bees are large black and yellow bees that resemble bumblebees. But while bumblebees are fuzzy all over; carpenter bees have a large, shiny, solid black abdomen.\nWhile they may look scary, male carpenter bees – the ones most likely to dive-bomb your head with a whitish spot on their face – don’t even have stingers, and females generally won’t sting unless you aggravate them.\nCarpenter bees get their names by the perfectly round, half-inch or so diameter holes that they drill in wood for their nests. In the spring, the bees emerge from their winter nests, mate, and set about building this year’s nest.\nThe female bee uses sharp teeth to excavate a perfectly round tunnel in soft wood, or she may choose instead to remodel an existing tunnel. She then lays her eggs in the tunnel, where they develop as her life cycle comes to an end.\nIn late summer, the new bees emerge to feed on plant nectar, then crawl back into their hole for the winter.\nCarpenter Bee Damage\nCarpenter bees prefer to excavate their nests in soft, unpainted wood – such as the back side of fascia boards, siding, window trim, and porch ceilings. They also bore into decks, outdoor furniture, fence posts, and swing sets. Softer woods – like pine, cedar, redwood, and cypress – are more attractive for nests while treated lumber and hardwoods are less inviting.\nThe holes typically go inward for about an inch, then the tunnel turns and follows the grain of the wood for about six more inches. The tunnel might branch into smaller ones that are shared by multiple bees.\nIn addition to tunnels, you might also find:\n- Fresh sawdust outside the hole.\n- Scraping sounds from inside the wood.\n- Fan-shaped stains outside the openings.\nIf your infestation is limited to a tunnel or two, there’s nothing to worry about. Even though they technically do bore into wood, carpenter bees don’t systematically destroy a structure like termites or carpenter ants.\nHowever, if the infestation is extensive or has been going on for years, the sheer number of tunnels can cause problems, including:\n- Woodpeckers: Insect eating birds can be drawn to the enticing sounds of bee nesting and larvae, which can invite much more severe damage.\nHow to Prevent Carpenter Bee Damage\nHere are some tips for making your home less attractive to carpenter bees:\n- Fill Cracks: Before painting or sealing, fill all cracks, nail holes, divots, and splintered wood with caulking or putty; since these are attractive starting places for bees.\nHow to Deal with Carpenter Bees\nIf the bees are already at work on your home:\n- Stay Alert: There’s no way to completely prevent or eliminate carpenter bees. But by taking these steps and staying alert to new activity, you can keep damage to a minimum.\n- Carpenter Bees (North Carolina State University)\n- Carpenter Bees (University of California Integrated Pest Management)\nPlease Leave a Comment\n6 Comments on “How to Deal with Carpenter Bees”\nYou can follow comments to this article by subscribing to the RSS news feed with your favorite feed reader.\nWe want to hear from you! In addition to posting comments on articles and videos, you can also send your comments and questions to us on our contact page or at (800) 946-4420. While we can't answer them all, we may use your question on our Today's Homeowner radio or TV show, or online at todayshomeowner.com."", 'The super-pollinators of the garden are … native bees! While honey bees have their place, it’s our native solitary bees—such as mason bees and leafcutter bees—which are most vital to our flowers and food. Learn more about these amazing heroes of pollination—and see how to bring these docile bees to your garden.\nSolitary Bees: The Heroes of Pollination\nMost of us grew up learning about the sophisticated social structures of honey bees and bumblebees, and we’ve come to think that their lifestyle represents all bee behavior. The truth is, the world is home to more than 20,000 species of bees, and a whopping 90% of them do not live together in hives.\nInstead, most of the world’s bees are solitary, meaning they live alone. Unlike social bees, each female solitary bee has to gather pollen and nectar, build nests, and lay eggs all on her own, without the help of hundreds or thousands of doting workers. And although honey bees tend to get all the credit for keeping our crops going, native solitary bees are almost two to three times more effective pollinators!\nSo, if 90% of bees don’t live in hives, where do they live? Well, about 70% of solitary bee species nest underground in tunnels and burrows, while the remaining 30% nest aboveground, in holes in logs and stems.\nMason bee nesting holes capped with a layer of mud and clay. Photo by Dominicus Johannes Bergsma/Wikimedia Commons.\nTwo of the most common hole-nesting bee species used for crop pollination are alfalfa leafcutter bees and blue mason bees. In the wild, both species nest in pre-made holes, such as old grub tunnels, crevices in peeling bark, or broken branches. As suggested by their names, leafcutter bees use pieces of leaves to build their nests, while mason bees use mud or clay. Other types of hole-nesting bees, such as sweat bees and carpenter bees, prefer to excavate their own holes in the ground, logs, reeds, or the dead canes of raspberry bushes.\nA curious carpenter bee (Xylocopa spp.)\nMost solitary bees have a short lifespan as flying adults. Male mason bees only fly for about two weeks—just long enough to mate—and females only live a few weeks longer. With such a short adult lifespan, solitary bees have to use their time wisely! They do not have time to make honey, nor do they like to fly too far from home, meaning they spend the bulk of their time preparing their nests and pollinating flowers within a relatively small area. A big chunk of a solitary bee’s life is spent in their mother’s nesting site, hibernating over winter in their cocoons.\nMason bee (Osmia spp.).\nBackyard Bee Houses\nSome reports indicate that nearly 40% of bees are facing extinction today, leaving many people wondering what they can do to help. Fortunately, the best thing you can do is to start local, in your own backyard. Making your garden as bee-friendly as possible is as easy as adding things like native wildflowers and native bee nesting sites, including bee houses.\nSimilar to birdhouses, bee houses (or hotels) provide vital and otherwise missing nesting habitat. They are relatively simple in form, consisting of a birdhouse-like structure containing a series of exposed, reed-like tubes that the bees can lay their eggs in. Hole-nesting bees desperately search for appropriate nesting sites, sometimes even nesting in the ends of old garden hose nozzles, openings in metal garden furniture, or the hollow ends of wind chimes. Bee houses provide a more natural structure for the bees, and also allow for a bit of human assistance when necessary.\nBee houses can be an eye-catching addition to your garden.\nOn an annual basis, bee houses do need to be maintained and managed, or else they’ll become uninhabitable. Don’t worry—maintaining a bee house is pretty simple: Just remove the bee-cocoon–filled nesting materials and store them in a cool place over winter. Then, in the spring, remove the cocoons from the old materials and place them alongside new materials in your bee house. The new bee generation will emerge and get right to work. In exchange for pollinating all of our fruits and vegetables, a little housecleaning and maintenance is the least we can do!\nFor more advice on maintaining a bee house in your garden, see How to Maintain a Bee House to Increase Pollination.\nSolitary Bee Pests and Diseases\nLike any other animal, hole-nesting bees are susceptible to a number of pests, diseases, and predators. The three greatest threats that the bees face are pollen mites (they eat the bee larva’s food supply before the bee can), chalkbrood (a fungal infection that converts a larva into a mass of fungal spores), and parasitic wasps (gnat-sized wasps that lay eggs inside of healthy larvae).\nTo reduce the spread of pests and diseases in bee houses, simply harvest cocoons and separate healthy ones from infected nesting chambers. As you harvest cocoons, you’ll learn how to identify infected chambers and keep healthy cocoons safe.\nFor more information on native bees and bee houses, see our top tips for maintaining a backyard bee house.\nTo learn about keeping honey bees, see our Beekeeping 101 series.']"	['<urn:uuid:7d6b1364-aa56-4176-9e36-1ca70b271b67>', '<urn:uuid:c6eb2b25-a3ce-4b11-be5b-1bb8038ffc29>']	open-ended	with-premise	short-search-query	similar-to-document	three-doc	novice	2025-05-12T20:11:36.665104	7	105	1500
37	I'm planning to host a sushi party this weekend and need help with the rice. What's the basic recipe for making proper sushi rice?	For 2 pounds of sushi rice, use 19 oz Japanese short grain white rice, 32 fl oz water, 1-inch square of Kombu, 3 tbsp sugar, 2 tsp kosher salt, and 4 fl oz rice vinegar. Cook rice with water and Kombu until water level matches rice, then cover and cook 10-12 minutes on low heat. Let stand covered for 10 minutes. Separately, dissolve sugar and salt in vinegar and simmer. Spread cooked rice on a sheet pan, add vinegar mixture while fanning to cool to room temperature. Never refrigerate sushi rice.	"['Sushi is suddenly really popular, fairly affordable and very fun to make. All of those things make me very happy and I love everything about sushi: Spicy tuna, California roll, Eel roll... I love it all.\nI recently had someone tell me that Spunky Kitchen should do a sushi-related article. I thought it was a great idea, but needed some guidance from an expert.\nChef Justin Kinziger, a chef instructor at the Culinary Institute of Michigan, graciously came in on his day off (for which his children will never forgive me) and showed Spunky Kitchen how to make Futo Maki which literally means ""fat roll.""\nWe used vegetables, rather than fish, only because we wanted to start out with something simple before we went all ""Spicy Tuna"" on you guys.\nLet me start with the Top 5 things Chef Justin said one must know about sushi. The photo gallery and video should be your tutorial for step-by-step directions on how to do it right. You\'ll need a bamboo sushi mat which should be covered in plastic wrap to keep all the rice pieces from falling through the cracks. They aren\'t fun to clean. They can be ordered online on Amazon.com, other sites or found at any World Market store.\nHere\'s the Top 5 tips from Chef Justin:\n- It\'s all about the rice: ""It\'s really the most important part,"" Kinziger said. ""You have to have good rice and learn the method. Once you\'re good at it you can roll anything.""\nHe suggests investing in good, Japanese short grain sushi rice. You can find it at Meijer. It needs to be cooked just as instructed so it\'s nice and bulky and sticky.\nHere\'s his recipe for 2 lbs of sushi rice:\n- 19 oz Rice, White, Short Grain, Japanese\n- 32 fl oz Water\n- 1 ea Kombu, 1x1 Inch Square, Wiped Clean\n- 3 tbsp Granulated Sugar\n- 2 tsp Kosher Salt\n- 4 fl oz Rice vinegar\n- Prepare rice for cooking (see below).\n- Combine rice and water in a small saucepan. Place the Kombu on top, bring to a boil, and then immediately remove the Kombu.\n- Cook, uncovered, until the water level is almost level with the rice.\n- Reduce heat to low and cover with a tight fitting lid. Cook for another 10 - 12 minutes.\n- Remove from heat and let stand, covered, for 10 minutes. This resting makes the rice easier to toss.\n- Dissolve the sugar and salt in the vinegar, bring to a simmer and reduce slightly. Do not let the mixture boil, or the flavor will be compromised.\n- Transfer the rice to a sheet pan. Spread into a thin layer.\n- Pour the vinegar dressing over the cooking rice and cut into the rice with a wooden spatula, while fanning the rice to cool it. Do not use all the dressing right away or the rice may get mushy. Keep fanning, stirring, and adding vinegar dressing until the rice is at room temperature. (Note: You may not need to use all of the dressing)\n- The rice is ready to make sushi when it has cooled to room temperature.\n**Use white short grain Japanese rice. Wash rice thoroughly under cold running water, rubbing it well between the palms of both hands. Drain and repeat the process 3 times until the water runs clear. Drain, set in a colander (china cap) to air dry for 30 minutes, tossing once or twice for even air circulation, before cooking. This process produces firmly cooked rice, perfect for tossing with vinegar dressing. Rice for sushi should not be soaked.\n**Use good quality rice vinegar.\n**Never refrigerate sushi rice, or it will become unpleasantly firm.\n** To prevent cooked sushi rice from becoming sticky when handling, keep your hands damp with tezu (hand-vinegar), which is a combination of water and vinegar that can be used to dip your fingers into while making sushi. To create tezu, combine 1 tbsp sushi dressing with 3 tbsp cold water.\n- For the love of God, use sharp knives: Without sharp knives, your sushi just won\'t end up in the nice, round bites that it\'s supposed to. You need a good sharp knife to cut through the nori. He also advises to have a damp dish towel nearby and wipe the knife with the damp rag in between slices.\n- Pay attention to the nori: You want to put the shiny side down on the bamboo roller and the grainier side facing up. You also want the thick, longer imprints on the nori vertical when rolling. Kinziger also waved the nori over a hot burner a few times to soften it so it rolls easier and subtly toasts the nori.\n- Don\'t start with fish: Until you get the method down, don\'t start rolling expensive pieces of fish like tuna, smoked Salmon or whitefish. Start with vegetables and work up from there. ""Once you have it down, use your imagination. You can use virtually any ingredient.""\n- Have fun: While some of the steps are tedious, sushi making should be fun. Once you know what you\'re doing, you can do all sorts of fun stuff: Dessert sushi, over-the-top seafood creations and sushi using purple jasmine rice. ""It should be fun. It shouldn\'t have to be too strict.""\nWhen you\'re ready for sushi with fish, you need to know what you\'re doing. PBS wrote an excellent article on the topic. Here\'s some highlights:\n""Ultimately, what it comes down to is how much you trust your fishmonger to understand the best practices for handling fish meant to be consumed raw, and how much they trust their suppliers to hold the same standards.""\nHere are a few things to remember when buying fish to ensure you have a safe and delicious sushi-dinner:\n- Observe and see for yourself whether they\'re cutting their sushi-grade fish on the same cutting board as their other fish, without changing gloves or disinfecting their knife and board first.\n- Ask whether they fillet the fish you\'re looking to buy themselves, or if they\'re getting them pre-filleted.\n- If you are buying salmon, ask if they can produce logs that show the times and temperatures that the fish was frozen.\nThe article states that ""sushi grade"" fish isn\'t really a thing after all. Be very leery of that and take note;\nI hope that helps. If you choose to make sushi, please send me your photos of the final product at email@example.com or add them to the comment section below.\nIn the meantime, I leave you with this delicious cucumber salad recipe from Kinziger as well. It serves 4:\n- 6 oz Cucumber, Peeled, Seeded, Cut Lengthwise in Half and Sliced Very Thin\n- 1 oz Green Onions, Sliced on a Bias, 1/8 Inch\n- 2 tbsp Shoyu or Soy Sauce\n- 1 tbsp Sesame Oil\n- 1 tbsp Granulated Sugar\n- 4 tbsp Rice Vinegar\n- TT Kosher Salt\n- 1 tsp White Sesame Seeds, Toasted\n- 1 tsp Black Sesame Seeds, Toasted\n- Gather all mise en place.\n- Combine all ingredients except sesame seeds and let stand for 1 hour.\n- Sprinkle with toasted sesame seeds for service.\nPlease check out The Spunky Kitchen Facebook page for more foodie fun. Spunky Kitchen is a column by Heather Lynn Peters featuring her own recipes and her family recipes for MLive. Heather also works with the chefs from the Culinary Institute of Michigan for a special recipe or holiday menu to present to Spunky Kitchen readers each month.']"	['<urn:uuid:8000bd0f-bef8-4514-9196-ffd9e40a68c6>']	factoid	with-premise	verbose-and-natural	distant-from-document	single-doc	novice	2025-05-12T20:11:36.665104	24	91	1262
38	What techniques compensate for geometry variations in detectors?	LabSOCS software compensates for geometry variations through ray tracing and vacuum-point efficiencies unique to each detector, while C-arm systems use calibration phantoms to determine projection matrices that compensate for mechanical distortions, with additional correction for non-reproducible errors through trajectory comparison.	"['Need information or have questions about our products, services, training, or pricing? We are here to help. Please fill out this form or view our office locations for regional phone contacts.\nMeasuring the efficiency with sources requires an inventory of sources that matches the geometries that are being measured. It is possible to acquire sources of common laboratory geometries, for example liquid solutions for beakers or epoxy matrices for Marinelli beakers. But there are geometries that are not practical to manufacture with sources or there may be cases of odd-shaped sources that are only going to be measured a few times or when parameters of the sample are different from the calibration source. In all of these cases mathematical calibrations can be used instead, such as Monte-Carlo simulation (e.g. Monte Carlo N-Particle eXtended [MCNPX] transport code) or the CANBERRA™ LabSOCS (Laboratory SOurceless Calibration Software) and ISOCS (In-Situ Object Counting Systems) methodology. The interaction of photons with matter and its cross sections is well known and is tabulated for all naturally-occurring atoms for a wide range of energies. See Figure 11-1 and Experiment 3.\nUsing the equation:\nthe attenuation of photons passing through a material of length x can be calculated. The attenuation of photons for a particular energy originating from a point in space directed to a point on the detector can be calculated from the cross sections, the densities, the atomic composition and the path length through all the materials between the point and the detector. This method, utilized by LabSOCS and ISOCS, is called ray tracing. In order to calculate the efficiency for the point it is also necessary to know the efficiency of the point without the materials, i.e. in a vacuum. With all of these parameters known it is possible to calculate the efficiency for this point in the measurement scenario. Points are generated uniformly in the radioactive part of the geometry and the efficiency is calculated for each of these points. The efficiency for the whole sample is the average of these points. When additional points are generated the sample efficiency will converge and the calculation finishes.\nOne of many benefits of mathematical modeling is that it is easy to refine the model if the sample does not agree with the modeled geometry. For example if the fill height of a beaker is not the same as the modeled fill height, the model can change to accommodate the difference. This contrasts a source-based calibration, where either the reference source or the sample measurement geometry must be physically modified, or the uncertainty in the sample activity must be increased significantly to accommodate the bias introduced by the discrepancy between the reference source and the sample. All other parameters can be changed in a similar manner for modeled efficiencies.\nFigure 11-1: Linear attenuation coefficient of photon interaction in iron as a function of energy.\nIntroduction to LabSOCS (Mathematical Efficiency)\nLabSOCS and ISOCS require that the detector to be used has been characterized by the detector manufacturer; the characterization is based on high-precision measurements and simulation optimization. This provides the vacuum-point efficiencies unique to each detector, and the calibration software then determines the attenuation correction for the specific measurement geometry. LabSOCS and ISOCS include a number of source templates to choose from such as point source, sphere, cylinder, beaker, box etc.\nThe user selects a template geometry, determines the relevant physical dimensions, and identifies a range of energies to calculate efficiency values. Figure 11-2 shows a picture of the different source geometries available in LabSOCS. Table 11-1 shows a typical listing of the energies and efficiencies from LabSOCS while Figure 11-3 shows a typical efficiency graph based on the values in the table. Refer to References 9 and 10 on Page 77 for more information on setting up and executing LabSOCS geometries.\nFigure 11-2: Selection of LabSOCS sample geometry templates.\nFor an efficiency calibration to be accurate it is critical that the geometry used to generate the efficiency agrees with the geometry of the sample count. For mathematical modeling this means that the model created needs to resemble as closely as possible the measured sample. This includes the dimensions of the detector, the sample, any absorbers or collimators, the material composition and density. Extra care should be taken to accurately model the materials between the source and detector and the location of the source and materials (particularly close to the detector where the efficiency is sensitive to small changes in location). Many commonly-used beakers don’t have flat bottoms but a slightly curved bottom. If the sample is positioned directly on the end cap and the bottom of the beaker is not accurately modeled then this can lead to significant bias in the calculated efficiency and therefore the final measurement result. Parts of the sample that are not directly between the radioactive part of the geometry and the detector crystal do not influence the peak efficiency and therefore do not require as much attention.\nTable 11-1: LabSOCS calculated efficiencies for a point source at 20 cm from the surface of a 2x2 NaI detector.\nFigure 11-3: Modeled efficiency for a 2x2 NaI detector with a point source located 20 cm from the endcap.\n1. Using LabSOCS, replicate the germanium detector experimental set-up of Experiment 8 and extract the efficiency values.\n2. Plot these in Excel or another graphing application.\n3. Compare the modeled efficiency curve with the measured efficiency curve from Experiment 8. Comment on any differences.\n4. Estimate the uncertainties in the measured efficiency (statistical uncertainty and certificate uncertainty). See the LabSOCS manual for the modeled uncertainties. Do the measured and calculated efficiencies agree within the uncertainties? If not measure the dimensions of the sample again and refine the model.\n1. Ensure that the Lynx (with the HPGe detector connected) is connected to the measurement PC either directly or via your local network.\n2. Using the adjustable source holder, place the 152Eu reference standard at a distance of about 20 cm from the endcap of the high-purity germanium detector. Record this distance. Remove any material between the source and detector before counting. Place one of the materials from the absorber kit between the source and the detector.\n3. Open the ProSpect Gamma Spectroscopy Software and connect to the Lynx.\n4. Configure the MCA settings as recommended in Experiment 7.\n5. Use the software to apply the recommended detector bias to the HPGe detector.\n6. Set the PHA conversion gain to 32768 channels.\n7. Adjust the coarse and fine gain of the MCA such that the 1408 keV peak is visible in the upper part of the spectrum.\n8. Acquire data, ensuring that at least 10 000 counts are achieved in several of the major peaks throughout the spectrum. Use the spreadsheet to calculate the energy calibration coefficients. Enter these into ProSpect using the Energy Calibration tab.\n9. Measure the net peak area and uncertainty for each of the major peaks. Calculate the ratio of count rates for each peak to the count rates for the data collected in Experiment 8.\n10. Model the experiment setup of Step 2 in LabSOCS Geometry Composer and extract the efficiency values. Calculate the ratio of the efficiency values for this geometry to the efficiency values extracted in Exercise 1. How do these compare to the ratios calculated in Step 9?\n1. Place a volumetric sample of unknown activity on the germanium detector.\n2. Count the sample for enough time to collect 10 000 counts in several significant peaks. Measure the net peak area and uncertainty for each of the major peaks, and identify candidate nuclides in the sample.\n3. Using LabSOCS Geometry Composer, identify a template that is consistent with the measurement geometry. Measure the physical parameters and input them into LabSOCS. Extract the efficiency results.\n4. Use the LabSOCS efficiency results, the peak area counts, and known photon intensities for the candidate nuclides, calculate the activity of the nuclides in the sample. Discuss the possible sources of error and uncertainty.\n1. In LabSOCS Geometry Composer create a model of a volumetric sample (such as the simple beaker template) on the detector end cap or use the one from the previous exercise and calculate the efficiency for a range of photon energies.\n2. Calculate the efficiency of the same sample in Step 1 but at a distance of 10 cm from the detector.\n3. Calculate the efficiency of the same sample in Step 1 but at a distance of 10 cm from the detector and with an absorber between the sample and the detector.\n4. Compare the three modeled efficiencies to the two measured efficiencies for a low-, a medium- and a high-energy peak. For the different energies which is the most important parameter, the source-to-detector distance or the materials between the source and the detector?\n5. In one of the models created decrease the fill height of the sample by 50% and calculate the efficiencies. Then compare it with the original model for a high-, medium- and low-energy peak. For which energy does the efficiency change the most? Explain the result.', ""Easy To Use Patents Search & Patent Lawyer Directory\nAt Patents you can conduct a Patent Search, File a Patent Application, find a Patent Attorney, or search available technology through our Patent Exchange. Patents are available using simple keyword or date criteria. If you are looking to hire a patent attorney, you've come to the right place. Protect your idea and hire a patent lawyer.\nMethod for correcting non-reproducible geometric errors occurring during\noperation of a C-arm device\nIn a method for correcting non-reproducible geometric errors occurring\nduring the operation of an x-ray C-arm device, having a C-arm carrying an\nx-ray source, during orbital displacement of the C-arm during a scan for\n3D reconstruction of a subject volume, a 2D dataset, which is not\nimpaired by non-reproducible geometric errors during the scan, is\ndetermined. This 2D dataset is compared to known projection matrices of\nthe x-ray C-arm device, which compensate reproducible geometric errors of\nthe C-arm device. The result of the comparison is used to modify the\nprojection matrices of the scan to compensate non-reproducible geometric\nerrors that occur during the scan.\nPrimary Examiner: Kao; Chih-Cheng G\nAttorney, Agent or Firm:Schiff Hardin LLP\nI claim as my invention:\n1. A method for correcting non-reproducible geometry errors during operation of an x-ray C-arm apparatus having a C-arm on which an x-ray source and an x-ray detector\nare mounted, said non-reproducible geometry errors occurring during orbital movement of said C-arm during a scan implemented for 3D reconstruction of a subject volume, comprising the steps of: during said scan, determining a 2D dataset impaired by a\nnon-reproducible geometry error; comparing said 2D dataset with known projection matrices for said x-ray C-arm apparatus that compensate reproducible geometry errors due to said orbital movement of said C-arm, thereby obtaining a comparison result; and\nautomatically using said comparison result to modify said projection matrices of said scan to compensate said non-reproducible geometry error.\n2. A method as claimed in claim 1 comprising: generating a sinogram representing a trajectory T1 of a high-contrast subject during a scan involving said orbital movement of said C-arm; generating a trajectory T2 of a virtual subject point from\nsaid known projection matrices; and automatically determining a difference function between said trajectory T1 and said trajectory T2, and using said difference function to modify said projection matrices to compensate for said non-reproducible geometry\n3. A method as claimed in claim 2 comprising generating said sinogram as a 3D sinogram.\n4. A method as claimed in claim 2 comprising generating said trajectory T2 based on projection matrices acquired with a calibration phantom using said C-arm apparatus.\n5. A method as claimed in claim 1 comprising: acquiring a first 2D dataset during a scan at a first orbital position of said x-ray source, and acquiring a second 2D dataset, correlated to said first 2D dataset, at a subsequently assumed second\norbital position; determining an existence of a deviation between said first and second 2D datasets that is not caused by said first and second orbital positions being different and, if said deviation exists, comparing said second 2D dataset with said\nknown projection matrices of said second orbital position to obtain a further comparison result; and using said further comparison result to alter the projection matrices of said second 2D dataset to compensate said non-reproducible geometry error.\n6. A method as claimed in claim 1 comprising correcting only geometry errors that occur in a plane proceeding at a right angle to a projection axis of said C-arm apparatus.\nOF THE INVENTION\n1. Field of the Invention\nThe present invention concerns a method for correction of non-reproducible geometry errors of an x-ray C-arm, which occur due to orbital movement of the C-arm during a scan implemented for 3D reconstruction of a patient volume.\n2. Description of the Prior Art\nAn x-ray C-arm apparatus of this type has a base frame on which the C-arm is supported such that it can move orbitally around an isocenter. One end of the C-arm carries an x-ray source and the other end an x-ray receiver, for example a planar\ndetector. Such apparatuses (used in a mobile or stationary manner) are used for, among other things, the 3D reconstruction of a patient volume. 3D exposures are acquired from a number of different angle positions and the patient volume of interest is\nreconstructed with known calculation methods. The image quality of 3D reconstructions is decisive for its usability, for example for diagnostic purposes. An important parameter for error-free imaging of the 3D world in a 2D image plane is the optimally\nerror-free reproducibility of the position and orientation of the x-ray receiver relative to the isocenter of the C-arm. Due to its own mass and the masses of x-ray source and x-ray receiver, the C-arm twists more or less severely depending on the\norbital position. These distortions (that can lie in the centimeter range) are typically compensated in that the movement track of the C-arm is calibrated in an offline method. For this purpose the projection matrices of the C-arm or, respectively, of\nthe x-ray system borne by it are determined using a calibration phantom. Mechanical distortions of the C-arm that occur in at least approximately the same manner given every orbital movement of the C-arm can thereby be compensated. Depending on the\ntype of bearing and drive of the C-arm, a non-reproducible wobbling of the C-arm (ascribed, for example, to tolerance-dependent play) cannot be avoided. A geometry error caused by wobbling leads to a limitation of the spatial resolution to values of\napproximately 7 to 10 lp/cm for a scan implemented for 3D reconstruction of a patient volume.\nSUMMARY OF THE INVENTION\nAn object of the invention is to provide a method for correction of non-reproducible geometry errors in such C-arm systems.\nThis object is achieved according to the invention by a method in which a 2D data set impaired by a non-reproducible geometry error during the scan is determined; this data set is compared with the known projection matrices of the x-ray C-arm\napparatus which compensate for reproducible geometry errors of the C-arm apparatus; and the result of this comparison is utilized to alter the projection matrices of the scan so that a non-reproducible geometry error occurring during the scan is\ncompensated. The 2D data set determined in one of the methods described further below can be compared with the known projection matrices in a simple manner via known methods and a correction of the 2D data set can be effected. An increase of the\nresolution of the 2D data sets and correspondingly a 3D reconstruction generated from these can be achieved in this manner.\nIn a preferred method variant a sinogram reflecting a trajectory T1 of a subject is generated during a scan of the high-contrast subject arranged within a patient volume, which scan is implemented for 3D reconstruction of the patient volume. A\ntrajectory T2 of a virtual subject point is generated from known projection matrices of the C-arm apparatus that already compensate for reproducible geometry errors and the difference function between the trajectory T1 and the trajectory T2 is\ndetermined. The difference function now corresponds precisely to the non-reproducible portion of the C-arm movement, such that the projection matrices of the scan effected on the patient can be optimized such that a non-reproducible geometry error of\nthe C-arm that does not occur during a scan is compensated. An advantage of such a method is that a geometry correction can thus be improved without technical expenditure on the part of the apparatus in a simple manner with the aid of known and\nfrequently used algorithms from the field of 3D reconstruction. Moreover, the inventive method makes it possible to use C-arm apparatuses with lower requirements for their mechanical stability or for the movement precision of the C-arm.\nIn principle it is possible to implement the proposed correction of geometry errors with the aid of 2D sinograms. However, in a preferred method a 3D sinogram is generated which supplies information about the spatial coordinates of a\nnon-reproducible path deviation of the C-arm or of the x-ray system supported thereby. The generation of the trajectory T2 is advantageously based on projection matrices which were acquired with the aid of a calibration phantom on the C-arm apparatus.\nIn a further preferred method variant a first 2D data set acquired for a first orbital position of the x-ray source is correlated with a second 2D data set acquired given a subsequent second orbital position during a scan. If a deviation between\nthe two 2D data sets exists that is not dependent on the different orbital positions, the second 2D data set is compared with the known projection matrices of the second orbital position and the result of this comparison is used to alter the projection\nmatrices of the second 3D data set for compensation of a non-reproducible geometry error. In this method variant, no sinogram is generated from individual 2D exposures made at different orbital positions of the x-ray radiator apparatus; rather,\nsuccessive 2D data sets in the movement direction of the C-arm are compared with one another and a comparison of the respective incorrect 2D data sets with the known projection matrices and a corresponding correction are effected given a significant\ndeviation. Here as well known methods and algorithms can be used.\nIt is conceivable to apply the proposed method variants to a correction of further degrees of freedom of movement of the C-arm. However, non-reproducible geometry errors are advantageously only corrected in a plane proceeding at a right angle to\nthe projection axis of the C-arm apparatus. Path deviations in this plane reduce the spatial resolution significantly more strongly than deviations in the remaining degrees of freedom of movement of the C-arm. A correction in the cited plane is\npossible with relatively slight computation effort. In contrast to this, a correction of further degrees of freedom of movement would entail only a comparably slight improvement of the spatial resolution of a 3D reconstruction given a high computation\nBRIEF DESCRIPTION OF THE DRAWINGS\nFIG. 1 is a schematic representation of an x-ray C-arm apparatus.\nFIG. 2, 3 are schematic representations to explain the generation of a sinogram.\nFIG. 4 shows a sinogram acquired during a patient scan.\nFIG. 5 shows an averaged sinogram of a C-arm apparatus acquired using a calibration phantom.\nDESCRIPTION OF THE PREFERRED EMBODIMENTS\nThe x-ray C-arm apparatus (called C-arm apparatus 1 in the following for short) shown in FIG. 1 is mobile or stationary and comprises a base frame 2 on which a C-arm 3 is supported such that it can move in an orbital direction 5 around an\nisocenter 4. An x-ray source 6 is arranged at one end of the C-arm 3 and a planar detector 7 is arranged as an x-ray receiver at the diametrically-opposite end. The radiation or, respectively, projection axis 8 of the x-ray source 7 intersects the\nisocenter 4 given an ideal geometry. The acquisition surface 9 (formed from a number of individual detectors) of the planar detector 7 extends at a right angle to the projection axis 8 or, respectively, at a right angle to the spatial axis designated\nwith X in FIG. 1 and is established by the coordinate axes u and v of the C-arm 1.\nDuring the 3D reconstruction of a patient volume, a patient is located on a table (not shown) such that the isocenter 4 is located within the body region of interest. Starting from the start position shown in FIG. 1, the C-arm 3 is moved\nclockwise in the orbital direction 5 until its end position and thereby generates a number of 2D data sets of the patient volume. For such a scan, a 3D sinogram of an easily-identifiable subject of the patient volume is now acquired. Suitable subjects\nfor this are, for instance, bones, air enclosed in colon, stomach or lungs, vessels enriched with contrast agents, orthopedic endoprostheses and markers. Either a punctiform subject or the edge of a high-contrast subject (for instance the border region\nof a bone) is now tracked during a scan to generate the 3D sinogram. During a scan the projection of a subject point (provided with the reference character 10 in FIG. 1) passes through a detector line 11 (FIG. 2) extending in the direction of the u-axis\ngiven an orbital movement of the C-arm 3 on the acquisition surface 9 of the planar detector 7. If such a detector line 11 is plotted over the time t, given a maximum travel angle of the C-arm 3 a curve of approximately 190.degree. results that\napproximately corresponds to half a sine curve. The individual time segments t.sub.1 through t.sub.n thereby correspond to different orbital positions of the x-ray source 6 or of the planar detector 7 or to different alignments of the projection axis 8\nrelative to the subject point 10. If only one detector line corresponding to FIG. 3 is plotted, a 2D sinogram arises which can be extracted from the relative movement of the subject point 10 or its projection onto the acquisition surface 9 in the form\nof a trajectory T1. Assuming that the subject point 10 lay in the orbital plane of the C-arm 3 spanned by the x- and y-coordinates, a geometry error acting only in this plane would be recognizable as a more or less strongly pronounced deviation 12 (FIG.\n4) from the curve course of the trajectory T1. In contrast to this, a wobbling acting at a right angle to the orbital plane or in the direction of the v-axis would be less well recognizable, for instance as an interruption of the curve course of the\ntrajectory T1, because the deviation 12 would effectively extend out from the paper plane or into this. However, if a 3D sinogram is acquired which implements a plurality of further detector lines flanking the detector line 11 in the direction of the\nv-axis (not shown in FIG. 3), the cited path deviations of the C-arm 3 (acting in the present case from the orbital plane in the z-direction or in the direction of the v-axis) can also be recognized and quantitatively detected.\nIn comparison to CT scanners, a somewhat less sturdy mechanism for the orbital revolution of the x-ray source 6 an the x-ray receiver is used in x-ray C-arm apparatuses. For example, the open shape of the C-arm entails that this widens in the\nsituation shown in FIG. 1, whereby the projection axis 8 experiences a slight inclination relative to the x-axis. Therefore x-ray C-arm apparatuses used for 3D reconstructions must be calibrated in an offline method. The projection matrices are thereby\ndetermined once using a calibration phantom. A number of orbital movements are effected in order to eliminate non-reproducible geometry errors.\nFor correction of non-reproducible geometry errors occurring during a patient scan, these projection matrices are now resorted to and a trajectory T2 of a virtual subject point is generated. The subject point is thereby located in the region of\nthe isocenter 4 or in a space that corresponds to the patient volume to be reconstructed. The trajectory T2 corresponds to the trajectory T1 except for phase, amplitude, offset and deviation 12. A fit of both trajectories T1 and T2 in these three\nparameters directly yields the difference function between T1 and T2. The difference function now precisely corresponds to the non-reproducible portion of the C-arm movement and allows the projection matrices determined during an offline calibration to\nbe corrected so that a non-reproducible path deviation of the C-arm is eliminated. Another possibility to eliminate non-reproducible path deviations with the aid of the cited difference function is to interpret the deviation of the two trajectories T1\nand T2 in the direction of the u-axis and the v-axis as a corresponding translation of the planar detector 7.\nA further possibility to detect the occurrence of a non-reproducible geometry error provides that during a patient scan successive 2D exposures or 2D data sets, thus 2D exposures or 2D data sets made in successive orbital positions of the x-ray\nsystem (in particular of a planar detector 7) in the movement direction of the C-arm, are correlated with one another. Due to the number of the 2D data sets generated during a scan, two successive 2D data sets differ only slightly with regard to the\nacquisition angle or the x-ray radiation direction, such that an image point alters its position in the direction of the u-axis and/or v-axis only by a slight measure (and which measure is known due to the known C-arm geometry) relative a preceding 2D\ndata set. However, if a geometry error (for example as a series of wobbles) occurs given the movement of the C-arm from the one orbital position to the next, the position deviation of an image point resulting from this is significantly larger and thus\nsuch an event can easily be recognized. As in the first described method variant, here as well the known projection matrices of the respective C-arm are resorted to for correction or, respectively, compensation of the occurred error and a correction of\nthe incorrect 2D data set is effected, whereby known methods and algorithms are again available for this purpose.\nAlthough modifications and changes may be suggested by those skilled in the art, it is the invention of the inventor to embody within the patent warranted heron all changes and modifications as reasonably and properly come within the scope of his\ncontribution to the art.""]"	['<urn:uuid:1cd683b7-c7de-40d6-a865-408a9fe30254>', '<urn:uuid:866eb53a-1bcf-4349-9eb8-760c7fd06b35>']	factoid	direct	concise-and-natural	similar-to-document	multi-aspect	expert	2025-05-12T20:11:36.665104	8	40	4418
39	osha inspection types whistleblower protection details	OSHA conducts both on-site physical inspections without advance notice and phone/fax investigations for minor violations. Additionally, OSHA enforces whistleblower provisions under 22 statutes protecting employees who report violations across various sectors including worker safety, securities laws, and financial reform. Employees who face retaliation can file complaints with OSHA's Whistleblower Protection Program.	"['- What Laws Govern Missouri OSHA Violations?\n- Who is Subject to OSHA Regulations?\n- What are a few Important Details Regarding OSHA Inspections?\n- What Types of Things are OSHA Inspectors Looking For?\n- How is an OSHA Phone or Fax Investigation Conducted?\nWhat Laws Govern Missouri OSHA Violations?\nThere are 28 states that have worked with the U.S. Department of Labor (Occupational Safety & Health Administration) to prepare an OSHA-approved state plan. Missouri, however, was not one of those 28 states. Because Missouri has not provided a plan that has been approved, OSHA federal laws and regulations are directly applicable to Missouri construction projects.\nWho is Subject to OSHA Regulations?\nContrary to typical belief, are not limited to work performed by public entities or work performed on a public project. OSHA regulations extend to private sector employers, contractors, construction companies and other employers performing work that requires safety for its workers. The imposition of OSHA regulations on private sector employers either comes through an OSHA program set up by the federal government or a plan that has been prepared by the state and approved by U.S. Department of Labor – Occupational Safety & Health Administration. A number of states have approved plans (about 28), but Missouri is not one of the states that has an approved OSHA plan. Therefore, Missouri utilizes the United States OSHA plan and is governed by the federal regulations set forth in the Code of Federal Regulations.\nWhat are a few Important Details Regarding OSHA Inspections?\n- An inspection for OSHA violations could occur at anytime. OSHA will not call and give you forewarning. The investigations are sometimes conducted on the spur of the moment, so do not be expecting advanced notice before an OSHA inspector arrives on one of your projects.\n- There are different types of investigations conducted by the OSHA inspectors. At times the inspector will be physically present and will conduct an on-site inspection. At other times, if the circumstances lend to it, the OSHA inspector will conduct an investigation via phone or facsimile.\n- The OSHA inspectors who conduct the investigations on behalf of the Occupational Safety and Health Administration are well-versed in the regulations. These officers are trained in the regulations, are experienced in working on jobsites, and know the ins and outs of compliance. It will be difficult to hide any violations, so the best course of action is to know the OSHA regulations and comply with the same.\nWhat Types of Things are OSHA Inspectors Looking For?\nWhenever an OSHA inspector visits a job site, the inspector has a checklist that he scours to ensure that the employer is in compliance with the regulations. There is an endless list of OSHA regulations, so where does the employer begin to avoid these claims? The following is a short list of the highest priority items on the OSHA inspector’s violation list:\n■ Imminent danger\nWhen an OSHA inspector walks onto the job, they’re looking for a situation where a worker is in imminent danger of bodily harm or injury.\n■ Catastrophes / fatalities\nIt should be no surprise that a catastrophe or a fatality on a construction project may prompt an investigation by OSHA. When the inspector arrives at your workplace, the focus will be on the instrumentality, or lack thereof, that caused the catastrophe or fatality. However, the inspector will already be out on the job and will thoroughly inspect all work being performed, as well as the job site in general, to ensure OSHA compliance.\n■ Worker complaints and referrals\nWhen a worker makes a complaint to OSHA about his or her safety on a job site, OSHA takes on a duty to inspect the employer’s operations to assure that all regulations are being followed.\n■ Targeted inspections – high injury/illness rates, severe violators\nIf a jobsite has an inordinate amount of injuries or occurrences that subject the workers to injury, OSHA is going to be all over the jobsite scrutinizing any potential cause. When OSHA has been informed that something is wrong on the project, they will come to investigate and specifically target whatever they believe the root of the problem is. This is what’s commonly referred to as a targeted inspection. As an employer on a construction project, you need to find the root of the problem before the OSHA inspectors do. If you need legal counsel in preventing OSHA violations, contact an OSHA regulations attorney.\n■ Follow-up inspections\nWhenever there are violations on one of your jobs, the inspectors will usually follow up the with a subsequent inspection to ensure that the OSHA violations were remedied. These inspections are usually aimed at the specific violations that previously occurred, so the employer should be aware of exactly what the OSHA inspector will be investigating.\nHow is an OSHA Phone or Fax Investigation Conducted?\nIf the alleged OSHA violation is seemingly minor in the eyes of OSHA, then the administration may simply conduct an investigation via telephone, followed up by a fax detailing the investigation.\nIf an employer is prompted with such an investigation, time is of the essence.\nOSHA requires that the employer respond within 5 working days in writing. The response must include the problems that the employer was able to discern, and the employer must detail the corrective actions taken to rectify OSHA’s concerns.', 'OSHA News Release - Table of Contents|\nOSHA News Release – Region 2\nU.S. Department of Labor\nUS Department of Labor OSHA investigation finds that New York company\nviolated Sarbanes-Oxley Act by firing worker who reported investor fraud\nSpongeTech Delivery Systems Inc. ordered to pay back wages of $31,835\nNEW YORK – SpongeTech Delivery Systems Inc. of New York has been ordered by the U.S. Department of Labor\'s Occupational Safety and Health Administration to pay $31,835.33 in back wages to a former employee who was fired after she reported apparent investment fraud to her superiors.\n""The point here is sharp and important: No employee should be fired or otherwise penalized for reporting investor fraud,"" said Robert Kulick, OSHA\'s regional administrator in New York. ""This employee did the right thing and must be compensated for the unjust job and wage loss that resulted.""\nIn November 2009, the complainant was sent to the Netherlands to represent the company in a trade show. She discovered that the company had no sales in Europe and communicated her concerns about this to company officials. At the time, SpongeTech was under investigation by the Securities and Exchange Commission for publicizing fictitious sales and product orders for the purpose of selling stock. The complainant was terminated on Jan. 11, 2010. She filed a whistleblower complaint with OSHA on April 7, 2010.\nOSHA\'s investigation found that the employee engaged in protected activity under the whistleblower provisions of the Sarbanes-Oxley Act* when she raised concerns to her employers about the veracity of the sales information, and that her protected activity was a contributing factor in her firing. As a result, OSHA is ordering the payment of the back wages. Both the company and the complainant have 30 days from receipt of OSHA\'s finding to file an appeal with the department\'s Office of Administrative Law Judges.\nMichael Metter, the company\'s chief executive officer, and Steven Moskowitz, its chief operating officer, pled guilty in January 2013 and August 2011, respectively, to criminal charges related to the investor fraud that led to the employee\'s termination.\nOSHA enforces the whistleblower provisions of Sarbanes-Oxley and 21 other statutes protecting employees who report violations of various airline, commercial motor carrier, consumer product, environmental, financial reform, food safety, health-care reform, nuclear, pipeline, worker safety, public transportation agency, maritime and securities laws. Employees who believe that they have been retaliated against for engaging in protected conduct may file a complaint with the secretary of labor to request an investigation by OSHA\'s Whistleblower Protection Program. Detailed information on employee whistleblower rights, including fact sheets, is available at www.whistleblowers.gov.\nUnder the Occupational Safety and Health Act of 1970, employers are responsible for providing safe and healthful workplaces for their employees. OSHA\'s role is to ensure these conditions for America\'s working men and women by setting and enforcing standards, and providing training, education and assistance. For more information, visit www.osha.gov.\n# # #\nEditor\'s note: The U.S. Department of Labor does not release the names of employees involved in whistleblower cases.\nU.S. Department of Labor news materials are accessible at http://www.dol.gov. The department\'s Reasonable Accommodation Resource Center converts departmental information and documents into alternative formats, which include Braille and large print. For alternative format requests, please contact the department at (202) 693-7828 (voice) or (800) 877-8339 (federal relay).\n* Accessibility Assistance Contact OSHA\'s Office of Communications at 202-693-1999 for assistance accessing PDF materials.\n|OSHA News Release - Table of Contents|']"	['<urn:uuid:1f703805-b5df-4806-a09c-17a7d4892210>', '<urn:uuid:41e5c77f-2d9a-41e9-b3f7-be4bd820c4e2>']	factoid	with-premise	short-search-query	similar-to-document	multi-aspect	expert	2025-05-12T20:11:36.665104	6	51	1462
40	what causes african forest loss congo basin role water environment	While the Congo Basin has relatively low deforestation rates (0.17% annually) compared to other rainforests, it faces increasing threats from new concessions for timber, minerals, oil and palm oil. This is particularly concerning because the Congo Basin forests serve as a critical mechanism for pumping water from multiple sources (Atlantic Ocean, underground, and soil) and generating rainfall both locally and on a sub-continental scale. Disruption of these forests would severely impact regional water cycles and the African monsoon.	['The just concluded U.S.-Africa Leaders Summit focused attention on Africa’s promises and challenges, including energy, agriculture and the $14 billion in investment pledged by companies. The visiting heads of state—just shy of 50—also discussed climate change and its effects on crop production, nutrition and food security. New research by the World Resources Institute and Rights and Resources Initiative on the climate dividends of secure community land rights can help Africa address these challenges.\nAgriculture is a mainstay of most African economies. Ninety-six percent of all agriculture in Africa is rain-fed. Sixty percent of the population is rural, with farming central to local livelihoods. Agricultural losses due to climate change are expected to cause the loss of between 2 and 7 percent of GDP. With so much at stake, many African governments are focused on climate change adaptation, including compensation for losses from climate change, new financial resources, technology transfer and capacity building to respond to the range of climate change impacts, including more severe and prolonged droughts, and crop-killing heat waves.\nLost in this discourse is Africa’s contribution to addressing climate change. The world’s more than 4 billion hectares of forests (about 10 billion acres) store approximately 860 gigatons of carbon, or as much as 45 percent of all the carbon stored on land. Deforestation and other land use changes account for 11 percent of all greenhouse gas emissions. Seventeen percent of the world’s forest land is in Africa. The forests of the Congo Basin, the world’s second largest block of high-canopy rain forest—only the Amazon is larger—hold almost two-thirds of Africa’s biomass carbon.\nIn a global context, annual deforestation rates are relatively low in the Congo Basin compared to rainforests in Southeast Asia and South America – from 2000 to 2005 the net annual deforestation rate in Central Africa was 0.17 percent (as compared to 2 percent in Indonesia and 0.6 percent in Brazilfor the same period). However, with the world’s growing appetite for timber, minerals, oil and palm oil, governments have granted new concessions which now cover much of the Congo Basin forests, threatening the wellbeing of rural populations with customary claims or ownership of much of the forests. These developments could also undermine global efforts to mitigate climate change.\nCommunities Help Curb Deforestation\nA recent WRI and RRI report, Securing Rights, Combating Climate Change: How Strengthening Community Forest Rights Mitigates Climate Change, shows that when local communities have secure rights over their land and forests, and those rights are protected by government, deforestation rates are lower and the forests are healthier than those outside community control. The carbon content of community forests also tends to be higher than in the forests outside these areas.\nTwo of the 14 forest-rich countries analyzed in the report are in Niger and Tanzania. In both countries, the law provides communities with legal rights to their land and trees, and supports their efforts, by, for example, providing community- based organizations with technical assistance and capacity-building. The results have been dramatic.\nTanzania. Over the past 20 years, the Tanzanian government has promoted participatory forest management (both community-based forest management and joint forest management of government reserves) as a major strategy for managing forests for sustainable use and conservation. More than 1,800 villages are engaged in legally recognized management of forests, covering 3.6 million hectares (about 8.9 million acres) or about 10 percent of the country’s total forest area. And participatory forest management leads to improved forest conditions.\nNiger. When the government strengthened the rights of farmers to manage trees on cropland, tree cover increased significantly. Over past 20 years, Niger has added 200 million new trees, storing an additional 30 million tons of carbon. Community rights not only prevent deforestation, they also encourage restoration of agroforestry systems across agricultural landscapes, and in the process have helped to improve food security and resilience for millions of rural households.\nDespite Successes, More Protection Needed\nDespite these successes, Africa lags behind Latin America and Asia in implementing and protecting community land and forest rights, with Indigenous Peoples and communities having legal rights to only 6 percent of forests in 12 African countries for which there is reliable data—compared to 39 percent in Latin America and 37 percent in Asia. From a climate change perspective, the focus must be on the Congo Basin, which contains the largest concentration of stored carbon in Africa. In 1994, Cameroon ushered in a new Forest Code that for the first time recognized communities’ legal rights for some forest land. Other Central African countries followed Cameroon’s lead, including Equatorial Guinea and the Democratic Republic of Congo. Today there are at least 340 Community Forests in Cameroon to which communities have legal rights, although they cover just 1.2 million hectares—1/20th of the forest, with another 50 legally-recognized Community Forests in Equatorial Guinea.\nWhile many African countries recognize customary tenure arrangements over land and forests in their national laws, the process of on-the-ground implementation and protection of community forest rights and land rights is slow. Few governments have established the strong legal protections required, and even when recognized by law, they often fail to protect community land from encroachment or support villagers in securing their land. This makes community land and resources vulnerable to mineral extractors, ranchers, illegal loggers, and other developers.\nAfrican countries have a golden opportunity to promote community development, protect forests, and mitigate climate change by strengthening community rights to forests and building the capacity to protect these rights. If African governments do this, the continent’s climate negotiators can show the world they are contributing to global efforts to curb climate change by reducing carbon dioxide emissions from deforestation and maintaining forests as carbon sinks.', 'Responses to climate change are grouped into two main categories: mitigation (reducing greenhouse gas emissions responsible for climate change) and adaptation (adjusting livelihoods and life styles due to the influences of climate change). Amongst adaptation strategies, ecosystem-based adaptation (EBA) is an approach that promotes ways to use natural resources and biodiversity to help develop adaptation strategies for vulnerable communities. In this context, recent studies highlight the role that the Congo Basin forests play in generating rainfall, both regionally and in the continent as a whole.\nRainfall in an ecosystem originates from three main sources: moisture that is already in the atmosphere, moisture from outside the region, and evapotranspiration from surfaces within the ecosystem (forests and other land uses). Pokam et al. studied how the climate of the Congo Basin is primarily a result of moisture from the Atlantic Ocean and the recycling process of evapotranspiration. Previous studies had established that tropical forests such as the Congo Basin can evaporate up to 1 to 2 meters of water per year. In addition, research by Makarieva et al. suggests that forest cover in the region acts like a pump, moving oceanic moisture towards the continent to eventually become rainfall in that region.\nWith this role of forests as rainmakers in mind, Nogherotto et al. explored the impacts of deforestation in the Congo Basin on regional hydrological cycles, including the effects on the African monsoon. They modeled scenarios where the Congo Basin is forested versus those in which it is deforested (that is, modeling a situation where forest cover is transformed to short grass cover). Their findings indicate that deforestation in the Congo Basin would lead to modifications in rain behavior in the Sahel and over southern equatorial Africa.\nTaken together, these three studies show how the Congo Basin acts as a mechanism for pumping water from the Atlantic Ocean, from underground sources, and from the soil. These sources then mix with atmospheric water, leading to the generation of rainfall both locally and on a sub-continental scale. Without the extensive forests of the Congo Basin, this process would be severely disrupted. In addition, these studies help highlight other important functions of the Congo Basin forests beyond biodiversity conservation and climate change mitigation.\nWith the growing scientific evidence of the multiple roles that the Congo Basin forests play at regional, continental and global scales, the case for forest conservation, for good forest management, and for multiple income streams to support the forests’ multiple roles becomes stronger. Given their rainmaker role, sustaining the Congo Basin forests might be one of the foundations for EBA to climate change for much of Africa.\nDenis Sonwa is a scientist with CIFOR’s Forests and Environment Programme. Denis can be contacted at email@example.com\nWe want you to share Forests News content, which is licensed under Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International (CC BY-NC-SA 4.0). This means you are free to redistribute our material for non-commercial purposes. All we ask is that you give Forests News appropriate credit and link to the original Forests News content, indicate if changes were made, and distribute your contributions under the same Creative Commons license. You must notify Forests News if you repost, reprint or reuse our materials by contacting firstname.lastname@example.org.']	['<urn:uuid:70b96a7a-5923-46b8-a44f-28114c3c98d9>', '<urn:uuid:133b8951-3beb-4cbf-b5ba-c0d510fadc89>']	factoid	direct	long-search-query	distant-from-document	multi-aspect	novice	2025-05-12T20:11:36.665104	10	78	1479
41	As a pediatrician, I need to explain the main differences in contagiousness between outer ear infection and hand, foot and mouth disease. Which one spreads more easily through direct contact between people?	Hand, foot and mouth disease spreads more easily through direct contact than outer ear infection. Hand, foot and mouth disease is highly contagious and can spread through contact with infected fluid from sores, saliva, or feces, as well as through airborne droplets from coughs and sneezes. The virus remains in the body for weeks after symptoms resolve. In contrast, outer ear infection (swimmer's ear) is not described as contagious and is primarily caused by irritation or scratching of the ear canal followed by infection, rather than through person-to-person transmission.	"['Outer ear infection (swimmer’s ear) and what you can do\n|Bret Haymore, MD FAAAAI, FACAAI\nBoard Certified Allergist\nAbout the author: Dr. Bret Haymore is an allergist-immunologist in Midwest City, Oklahoma and is affiliated with multiple hospitals in the area, including Hillcrest Medical Center and Integris Baptist Medical Center. He received his medical degree from Penn State College of Medicine and has been in practice for 12 years. He is one of 7 doctors at Hillcrest Medical Center and one of 3 at Integris Baptist Medical Center who specialize in Allergy & Immunology.\nWhat it is: An outer ear infection is a condition that can cause pain in the ear canal. An outer ear infection is sometimes called “swimmer’s ear.” But an outer ear infection does not happen only in people who swim. People who do not swim can also get it.\nWhat causes it: When the skin in the ear canal gets irritated or scratched, and then gets infected.\nSome examples of what can cause this to happen:\n– Putting cotton swabs, fingers, or other things inside the ear\n– Cleaning the ear canal to remove ear wax\n– Swimming on a regular basis. Water can soften the ear canal, which allows germs to infect the skin more easily.\n– Wearing hearing aids, headphones, or ear plugs that can hurt the skin inside the ear\nWhat are the symptoms:\n– Pain inside the ear, especially when the ear is pulled or moved\n– Itching inside the ear\n– Fluid or pus leaking from the ear\n– Trouble hearing\nHow it is treated:\n– Ear drops – Be sure to finish all the medicine, even if you feel better after a few days.\n– Medicines to relieve pain\nWhen you use ear drops, you should:\n– Lie on your side or tilt your head.\n– Make sure the ear drops go into the ear canal.\n– Stay in the same position for 20 minutes (after the ear drops are in)\nIt is important to keep the inside of your ear dry while the infection heals. You should not swim for 7-10 days after starting treatment. But you can take a shower. To keep the ear dry during a shower, put some petroleum jelly (e.g. Vaseline) on a cotton ball, and then put the cotton ball in your outer ear, covering the opening of your ear canal. Do not push the cotton ball into the ear canal.\nYou should also avoid wearing hearing aids or headphones in the infected ear until your symptoms improve.\nCall your doctor or nurse if:\n– Your symptoms get worse\n– Your symptoms are not better 2 days after starting treatment\nCan I prevent it? You can reduce your chances of getting an outer ear infection by:\n– Not sticking things in your ears or cleaning inside your ears – The inside of the ears do not usually need to be cleaned. It is normal to have some ear wax in your ears. Ear wax protects the ear canal. But if you are worried that you have too much ear wax, talk to your doctor or nurse.\nFollowing these tips if you swim a lot:\n– Shake your ears dry after you swim\n– Blow dry your ears on a low setting, holding the dryer 12 inches away.\n– Use ear drops that can prevent infections after you swim; these are available without a prescription\n– Wear ear plugs made for swimming to prevent water from getting in your ears.', ""Hand, foot and mouth disease is a common, mild illness caused by a type of virus called an enterovirus.\nThe disease gets its name from the non-itchy rash that develops on the palms of the hands and soles of the feet. It can also cause ulcers in the mouth and overall make you feel generally unwell.\nHand, foot and mouth disease is highly contagious so spreads easily.\nWHO IS AFFECTED?\nIt is quite common in children under the age of 10, however adolescents and adults can also be affected.\nMost adults are immune to the virus type that causes the disease as they have been previously exposed to it during their childhood.\nIt is possible to catch hand, foot and mouth disease more than once, however, children are unlikely to catch it again during the same outbreak.\nGenerally, hand, foot and mouth disease is a mild and short-lasting illness. Treatment is usually not needed as the body’s immune system clears the virus and symptoms go away after about 7 to 10 days.\nIs it the same as foot and mouth disease?\nFoot and mouth disease affects cattle, sheep and pigs. The two infections are unrelated and you cannot catch hand, foot and mouth disease from animals.\nSymptoms tend to appear 3 to 5 days after becoming infected with the virus and last for 7 to 10 days before disappearing on their own.\nSome people with hand, foot and mouth disease do not develop any symptoms.\nAdults who develop hand, foot and mouth disease tend to experience milder symptoms than children.\nThe first symptoms of hand, foot and mouth disease:\n- fever & feeling unwell\n- loss of appetite\n- sore throat\n- small red spots in the mouth, throat and on the skin\nAfter one or two days, red spots in the mouth will develop into painful ulcers, particularly around the tongue, gums and inside of the cheeks. It may be difficult to eat, drink and swallow.\nAny red spots on the skin will turn into a non-itchy rash over the following one to two days. The spots are flat or raised, sometimes with blisters, and smaller than chickenpox sores.\nThe rash develops on the palms of the hands, the soles of the feet and between the fingers and toes. In some cases, spots also develop on the buttocks and genitals.\nCAUSES & HOW IT IS SPREAD\nHand, foot and mouth disease is usually caused by the coxsackie A virus, but it is sometimes caused by the coxsackie B virus or the enterovirus 71.\nThese viruses remain in the body for weeks after symptoms have gone away, so infected people can pass the disease to others even when they appear well.\nThe viruses that cause hand, foot and mouth disease are contained in the millions of tiny droplets that come out of the nose and mouth when someone with the disease coughs or sneezes.\nThese droplets hang suspended in the air for a while, then land on surfaces. Anyone who touches these surfaces can spread the virus by touching something else.\nPeople usually become infected by picking up the virus on their hands from contaminated objects and then placing their hands near their mouth or nose. It is also possible to breathe in the virus if it is suspended in airborne droplets.\nOther ways of catching it\nYou can also become infected with hand, foot and mouth disease if you have contact with fluid from the sores, saliva or faeces of someone who is infected.\nThe virus stays in the faeces for approximately 4 weeks after the person has recovered. It is, therefore, vital that adults and children wash their hands thoroughly after going to the toilet or handling nappies.\nSeveral different viruses can cause sores and ulcers in the mouth. A GP will normally be able to distinguish hand, foot and mouth disease from other viral infections by:\n- The age of the affected person. Hand, foot and mouth disease is most common in children under the age of 10\n- The pattern of symptoms. Symptoms begin with fever and a sore throat; spots then develop in your mouth and later on the palms of your hands and soles of your feet\n- The appearance of sores. The sores are smaller than chickenpox sores\nA throat swab or stool sample may be taken and sent to a laboratory to determine which enterovirus has caused hand, foot and mouth disease. The result usually takes a few days.\nThere is no specific treatment for hand, foot and mouth disease. The condition usually clears up by itself after 7 to 10 days. As it is viral, it cannot be treated with antibiotics.\nYou can ease symptoms by:\n- getting plenty of rest\n- drinking plenty of fluids (water or weak squash are best)\n- taking medication to relieve symptoms\nIf you or your child has a fever or sore throat, paracetamol should relieve pain and bring down a temperature. Children's paracetamol can be used to treat your child.\nAspirin must not be given to children under 16 years of age.\nThe pain of mouth ulcers can be numbed with anaesthetic mouthwashes or sprays, such as benzydamine hydrochloride (ask your CarePlus Pharmacist for Difflam). Choline salicylate gel can be used in adults and children aged 16 and over.\nComplications of hand, foot and mouth disease are uncommon, but can include:\nThe sores that develop in your throat and mouth may make it difficult for you to drink and swallow. As a result, dehydration can occur. Therefore, it is important to drink plenty of fluids as often as you can. If your child has the disease, ensure that they do the same.\nInfection of sores\nIf the sores are scratched, they may become infected. If this happens, your GP may prescribe antibiotics to treat the infection.\nIn rare cases, hand, foot and mouth disease can lead to viral meningitis. Viral meningitis is an infection of the meninges (membranes that cover the brain and spinal cord).\nViral meningitis is less severe than bacterial meningitis and most people will make a full recovery within two weeks. Symptoms include fever, drowsiness, headache, neck stiffness, vomiting and difficulty looking at bright lights. There is no specific treatment.\nIn very rare cases, hand, foot and mouth disease can lead to encephalitis. Encephalitis is an infection that causes the brain tissue to swell and become inflamed. It can cause brain damage and is potentially life threatening.\nEarly signs of encephalitis are flu-like symptoms, which can develop in a few hours or over a few days. Other symptoms include:\n- drowsiness or confusion\n- seizures (fits)\n- dislike of bright lights\nIf you develop encephalitis, you will need to be admitted to hospital. Often those affected make a full recovery.\nPregnant women who catch hand, foot and mouth disease just before giving birth may pass it to their baby. However, babies born with the disease will usually only experience mild symptoms.\nIn extremely rare cases, catching hand, foot and mouth disease during your pregnancy may result in miscarriage.\nHand, foot and mouth disease is highly contagious. The best way to avoid catching and spreading the disease is to avoid close contact with people who have the disease and to practise good hygiene:\n- Always wash your hands after going to the toilet and handling nappies, and before preparing food. If your child has hand, foot and mouth disease, encourage them to wash their hands regularly as well\n- Avoid sharing utensils with people who are infected with the disease\n- Make sure that shared work surfaces are clean\nKeeping your child off school\nIf you child has hand, foot and mouth disease, keep them out of school or playschool while they are feeling unwell. They can go back to school when their symptoms get better.""]"	['<urn:uuid:7400bbfd-92e0-4d99-9ca9-2223feed130c>', '<urn:uuid:b38156ea-8be7-4523-87ab-acc76b9f8325>']	factoid	with-premise	verbose-and-natural	distant-from-document	comparison	expert	2025-05-12T20:11:36.665104	32	89	1888
42	What are the fundamental differences between CAPM and APT in portfolio construction, and how do these theories relate to contemporary quantitative equity market neutral strategies?	CAPM assumes all investors should construct portfolios based on the risk-free asset and market portfolio, while APT doesn't make this assumption and allows for multiple risk factors. In modern quantitative equity market neutral strategies, this theoretical difference is reflected in how managers use fundamental factors for stock selection, similar to traditional analysis but with the ability to process more data and make more independent bets. These strategies typically construct portfolios to be neutral to broader markets and may constrain exposure to various risk factors such as market cap, geography, value, or momentum - an approach that aligns more closely with APT's multi-factor framework than CAPM's single-factor model.	"[""Arbitrage Portfolio Theory (APT) – A Multifactor Macroeconomic Model\nArbitrage Portfolio Theory (APT) came along after CAPM as a multifactor model to explain returns.\nAPT explains returns under the construct where:\nMultiple risks with an excess return above the risk free rate of return can be priced.\nAny security or portfolio has its own beta coefficient to each of the priced risk variables in the model.\nThere is a linear relationship between the security's return and the priced risk (a basic assumption of multi-variable regression).\nAPT calculates the alpha value, or y-intercept of the model graph.\nComparing CAPM vs. APT\nAPT is less restrictive in CAPM, as:\nAsset returns can be described using a multifactor model (CAPM being a single factor model).\nDiversification eliminates the security specific risk of the individual securities in a multi-asset portfolio.\nAssets are priced such that arbitrage profit does not exist.\nThe factor sensitivities of the assets in an arbitrage portfolio equal zero and the portfolios expected return is zero.\nNote: If the investor believes that the expected return on the arbitrage portfolio is not equal to zero, then a single factor or multifactor APT style model can be used to capture risk free profit.\nStep 1: Identify and purchase the undervalued asset or portfolio.\nStep 2: Finance the long position with a short sale of overvalued assets.\nStep 3: Close the long and short positions once the assets return to their APT determined equilibrium model values for zero return.\nWhenever two portfolios have the same risk but different expected returns or the same expected return, but different risks, an arbitrage opportunity may be possible.\nIt is important to note a couple of key differences between CAPM and APT as these modeling techniques and their variations are extensive in financial research.\nCAPM assumes that investors agree on asset returns, risks, and correlations: E(R), σ, and ρ.\nAPT does not assume this, making the theory less restrictive than CAPM.\nCAPM assumes that all investors should construct a portfolio based on the risk free asset and the market portfolio.\nAPT does not necessarily assume this\nOther Challenges for CAPM vs. Reality:\nCAPM ignores transaction costs and taxes, which is not realistic for investors.\nInvestors rarely can borrow at the risk free rate.\nNot all investors can short sell.\nRisk Aversion as Common Ground: Both APT and CAPM assume that investors are risk averse and will take the highest return for a given amount of risk.\nPricing E(RP) with an APT Model\nAn APT model can be thought of an equation where alphas (the excess return of the risk factors) are applied to betas (the sensitivity of the portfolio or security to the risk factor itself).\nE(RP) = RF + λiβP,i + λjβP,j\n- λi = Factor risk premium return above the risk free rate; the compensation to the investor for accepting the risk.\n- βP,i = Coefficient representing the portfolio (or security) return's sensitivity to the risk factor\n- CFA Level 2: Portfolio Management – Introduction\n- Mean-Variance Analysis Assumptions\n- Expected Return and Variance for a Two Asset Portfolio\n- The Minimum Variance Frontier & Efficient Frontier\n- Diversification Benefits\n- The Capital Allocation Line – Introducing the Risk-free Asset\n- The Capital Market Line\n- CAPM & the SML\n- Adding an Asset to a Portfolio – Improving the Minimum Variance Frontier\n- The Market Model for a Security’s Returns\n- Adjusted and Unadjusted Beta\n- Multifactor Models\n- Arbitrage Portfolio Theory (APT) – A Multifactor Macroeconomic Model\n- Risk Factors and Tracking Portfolios\n- Markowitz, MPT, and Market Efficiency\n- International Capital Market Integration\n- Domestic CAPM and Extended CAPM\n- Changes in Real Exchange Rates\n- International CAPM (ICAPM) - Beyond Extended CAPM\n- Measuring Currency Exposure\n- Company Stock Value Responses to Changes in Real Exchange Rates\n- ICAPM vs. Domestic CAPM\n- The J-Curve – Impact of Exchange Rate Changes on National Economies\n- Moving Exchange Rates and Equity Markets\n- Impacts of Market Segmentation on ICAPM\n- Justifying Active Portfolio Management\n- The Treynor-Black Model\n- Portfolio Management Process\n- The Investor Policy Statement"", 'Opening up the quant box\nLike the yeast-based spread, much goodness might be missed by lumping all quantitative funds into one bucket all too often labelled a ‘black box’. Investors eschew these systematic traders for a variety of reasons, some of which are made on fear-based assumptions, which is ironic because one of the premises of quantitative investing is that decisions driven by emotions are eliminated from the investment process.\nOne of the first points to make is this: ‘quantitative investing’ is not really a strategy. The term is used to distinguish ‘how’ the strategy is created and implemented, namely a systematic method that seeks to eliminate the arbitrariness that so often pervades discretionary investing strategies. Secondly, it appears that many investors still believe the term ‘quantitative managers’ to be synonymous with trend-following CTAs/managed future funds.\nOnce you lift the lid on managers that adopt a systematic approach to managing money there is a significant variety not only in the strategies that underlie the generic term ‘quant’ but in the ways quantitative strategies are implemented. For example trading speed, average holding period, portfolio construction and risk management techniques are all keys to a universe of potentially accretive and non-correlated return streams.\nDispelling myths and quashing fears about systematic managers is a challenge, not least because quants are often associated with a lack of transparency – the term ‘black box’ adding to the perception of opacity.\nMost quants are fiercely protective of their secrets because their proprietary datasets and research are their lifeblood and integral in their quest to maintain an ‘edge’ over the competition.\nTo move beyond fear to maximising potential returns, it is important to dispel the belief that this protectiveness of intellectual property means that nothing is known about how quants make money. The variety in the strategies they deploy can be seen further down below under Quant Strategy Highlights.\nFor some, the memory of the ‘Quant Meltdown’ of August 2007 is still fresh. In the week of 6th August 2007, a number of quantitative long/short equity hedge funds experienced losses of greater than 20%.\nThe true trigger of the moves is not known, but it seems likely that losses from the credit crisis led to systemic deleveraging that then saw some firms taking down market exposure across their full range of strategies. This created a negative feedback loop in the quant space causing further losses and deleveraging.\nTwo big lessons have been learned. The first is that the common factor risk between quants was much higher than investors and the quant funds themselves had appreciated. The second lesson revolved around excessive use of leverage. To achieve returns and volatility targets, quants had created a toxic mix of incrementally increasing leverage and assets, all chasing small opportunities. So when the shock came, the effect was amplified causing violent P&L losses.\nAlthough many of the factors and stocks that had been so violently hit in the deleveraging quickly reversed and some quant managers saw lucrative trading opportunities in the aftermath,for most the ensuing panic changed the quant trading landscape. Post August 2007, we have witnessed a significant reduction of assets investing in longer-term quantitative equity models and a massive reduction in leverage.\nToday, however, managers are far less complacent and more diversified than they were previously. An interesting side-point revolves around the recent proliferation of alternative beta and risk parity products; is the recent massive rise in ‘alternative beta’ and ‘risk-premia’ harvesting strategies sowing the seeds of an August 2007 style scenario?’ The enormous asset flow into highly commoditised areas combined with leverage is a story we have seen before.\nSo how do you pick a winner in the quant space? Rishi Narang, author of Inside the Black Box, provides one perspective: “the systematic implementation of trading strategies that human beings create through rigorous research.” He highlights an important aspect of quantitative investing: namely that people, not machines conduct the research, decide the strategies, and select the universe of securities to trade and what data to use.\nIronically, by investing in a quantitative manager an investor is making a highly qualitative decision. What may have worked to produce 95% up months in the fund 10 years ago is unlikely to be what they are doing today, given the constant changes witnessed in markets. Never has the adage ‘adapt or die’ been more true than when dealing with the quantitative trading space.\nGiven the inherent necessity for quant managers to be able to evolve, any investor is essentially taking a bet on the manager’s robust research process; ability to innovate and continue to add to alpha sources; manage risk; and adapt to ever changing and more complex fragmented global markets. So how can investors mine the nuances of the quant universe’s diversity to access a potentially important tool in the alpha harvesting armoury?\nSome questions to consider. How diversified are the sources of alpha and of those, how many are unique to the manager? Is the manager diversifying across models, region and time horizon? Correlation of the alpha sources and crowding in the space are also critical elements to assess, as August 2007 highlighted. What is the capacity of the fund if many of the sub-strategies are highly capacity-constrained? This last one is the ‘million dollar question’.\nAs with discretionary managers, strong references are clearly essential, however, having experienced allocators, can give an investor a significant information edge. Aurum has been investing in quant strategies for 20 years and the CIO and analysts have developed long-standing relationships with leading thinkers and practitioners in this space.\nOne very important factor when looking for quant teams is to make sure they have a balance of analytical/mathematical skills in combination with experience of having traded real markets, preferably over several cycles, testing their approach through extreme periods and tail events: a portfolio manager with a pure academic approach and no ‘real-world’ experience is a ticking time bomb.\nWhen it comes to the assessment of historical performance, investors should also look at returns on the gross book deployed and take into account of the use of leverage. Here one also needs to look at how frequently the manager is turning over the book and how many independent ‘bets’ are made over any given time period.\nEvery quantitative investment strategy can be explained without giving away the ‘secret sauce’, but knowing how they manage risk and what protocols they have in place for sudden market dislocations such as August 2007 is paramount.\nWe believe that if one takes the time to lift the lid on the so-called quant ‘black boxes’, understanding them is not so daunting a task as it may first appear. It can ultimately give investors access to a whole new ‘tool-kit’ and can open up new sources of diversifying returns that in today’s world of ultra-low interest rates is worth its weight in gold.\nQuant Strategy Highlights\nQuantitative equity market neutral: Alpha is predominantly provided by fundamental factors that drive stock selection; not entirely dissimilar to the way equity analysts normally look at stocks – also sometimes known as quantitative long/short. Here quant managers indicate that they have a significant advantage in being able to crunch infinitely more data than a single human being could ever process and make a lot more independent ‘bets’ where they believe they have a slight ‘edge’. The process is stable and repeatable and theoretically should deliver a higher Sharpe ratio over the long run at lower risk as these portfolios are typically constructed to be neutral to the broader markets. More sophisticated approaches tend to go even further, attempting to constrain the portfolio’s exposure to other generic ‘risk-factors’, such as market cap, geography, value or momentum biases etc. Others attempt to ‘time’ or dynamically adjust exposure to such risk factors in the hope it will be accretive to returns.\nStatistical arbitrage: Typically where the inputs are price based and/or other technical signals, e.g. where one would expect certain pairs or clusters of stocks to behave in a similar fashion/predictable manner. Statistical arbitrage uses various mathematical techniques to identify significant explanatory relationships between the movements of different instruments; they then construct trading strategies to benefit from discrepancies between model predictions and the observed market. The most ubiquitous models are variants of mean reversion and momentum strategies.\nEvent-driven: The identification of certain market participant actions/events in the market that typically results in prices moving in a predictable manner, e.g. price action of stocks around the announcement of index rebalancing, merger targets, analyst earnings estimate revision announcements, tax-year end dates, option expiry dates, futures rolls, etc.\nWith the exception of trend-following, futures and specialist FX managers may trade more technical relative value strategies between instruments over time frames ranging from fractions of a second to a day; they use countertrend, carry and econometric models; and global asset allocation strategies that use both technical and fundamental information.\nFX managers may also use econometric techniques as well as various carry models and fundamental analysis of countries’ macroeconomic data\nQuant macro: This includes various combinations of the above, plus cross asset relative value and may add other areas like volatility trading, fixed income curve models and fundamental commodity models using supply/demand metrics and predictors.\nHigh frequency trading and intra-day trading: (HFT): No quant article would be complete without mentioning HFT; but it is important to distinguish between the above strategies and HFT, which has had a lot of a ‘bad press’, as well as ultra high frequency traders who simply make money by being the fastest, and those whose strategy uses low latency execution technology infrastructure in order to exploit temporary anomalies in pricing. HFTs are here to stay as they ultimately get paid an economic rent to provide liquidity to markets. In the strictest definition of the word pure high frequency trading arbitrage strategies is the domain of proprietary trading shops; while there are a handful of sophisticated hedge funds that are trying to identify and capture very short term anomalies (not HFT). This whole space is highly capital/capacity constrained, and while it is associated with providing very high Sharpe returns, it is often very difficult for typical allocators to quant to gain access.\nSource: Aurum Funds']"	['<urn:uuid:2ef20cfe-84af-4792-98c2-902aa44cec71>', '<urn:uuid:f69eefbd-248d-4442-b9dd-1b571084f7b5>']	factoid	direct	verbose-and-natural	similar-to-document	multi-aspect	expert	2025-05-12T20:11:36.665104	25	107	2380
43	What items were used in ancient Mexican and Mayan tombs?	In Colima, Mexico, tombs contained ceramic figurines with elaborate headdresses (including a male figure possibly representing a shaman and a female figure holding a pot), ceramic pots, and human remains. The Mayans placed offerings in tombs including ceramics, food, flowers, and personal items of the deceased, with evidence showing repeated visits to royal tombs for veneration.	"['A tomb containing the skulls of 12 individuals and sealed with human bones has been unearthed in Colima, Mexico, archaeologists have announced. Ceramic figurines with ornamental headdresses were also discovered, along with two ceramic pots. The mass grave has been dated to around the fourth century, which makes it pre-Columbian in origin.\nThe Daily Mail reported this week that archaeologists have excavated an ancient tomb that just might shed some light on the peoples of western Mexico prior to the coming of Columbus and the conquistadors, a time that saw massive and widespread destruction of the indigenous populations’ culture. The tomb predates Columbus by a millennium and contains the bones and skulls of a dozen individuals. There appears to have been no effort made toward ritualized placement, the remains seemingly piled atop each other in a haphazard fashion. Interestingly, all of the skulls showed signs of deformation.\nThe tomb was discovered at a Seventh-day Adventist Church in the Colima city center undergoing remodelling, according to archaeologists with the National Institute of Anthropology and History (INAH). And since the tomb had been sealed (with a combination of human bones, stones, and grinding artifacts), the grave had remained untouched for nearly two millennia.\n“This allowed us to look at the skeletal remains first-hand, to observe the lesions, deformations, and to gather more information to understand their way of life,” the researchers said. “There may be more pits, since the entire valley of Colima was inhabited form the start of the Capacha era (1500 BC) right up until the Spanish arrived (1500 AD).”\nAlong with the human remains were four ceramic artifacts, all ceramic. Two were figurines, a male and a female with elaborate headdresses. Both were discovered face-down in the grave, prompting scientists to believe that they were ceremonial offerings.\nThe male figurine is 39 centimeters tall (15 inches) and 14 centimeters wide (5.5 inches). The headdress has a horn protruding from it and the figure seems to be holding an axe. Researchers say it could have been fashioned into the likeness of a shaman.\nThe second figurine found is of a woman with a wide body, triangular head, and a hooked nose. Her dimensions are somewhat smaller than the male figurine, standing as she does at 32 centimeters tall (12.5 inches) and 14 centimeters wide (5.5 inches).\nShe is adorned with a band-style headdress and is holding a pot.\nThe other artifacts, as noted, were ceramic pots.\n“The presence of these pieces in the altar offers a glimpse into the views of the people that lived in the valley of Colima at the time,” said Rafael Platas Ruiz of the INAH Colima Center. “The sculptures and their attributes served as offerings to protect the deceased, as we can see from the male sculpture representing a shaman. The other objects were requirements needed to reach the underworld.”\nThe artifacts appear to be from the Comala period, which extended from 0-500 CE. The tomb itself is a rare find, according to the researchers, because it has not been looted or disturbed like so many other ancient sites.\nThe Colima find comes just weeks after scientists announced the carbon dating of ancient Mayan ruins at the ancient Mayan Royal Palace of Ceibal, an archaeological site located in northern Guatemala’s Peten Department. According to the Inquisitr, after testing 154 samples, it was determined that instead of there being a massive collapse of Mayan civilization in the ninth century (a mysterious situation where the ancient civilization seemed to nearly vanish), there had actually been two collapses. Researchers found that the collapses followed similar patterns, where the first collapse, which was a bit smaller in scope, came in waves of instability followed by a short recovery prior to the major collapse.\nThe Classic and Pre-Classic Periods of the Maya would have been contemporaneous with the Comala Period and its half-century arc, but it is unclear how much interaction there would have been between the two cultures or if the Mayans had dominated the region.\n[Featured Image by wdeon/Shutterstock]', ""Ancestor worship definition\nAncestor worship relies on the belief that ancestors’ spirits still in exist in the natural world and that they have the power to influence lives of human beings who are alive. Ancestor worship has been in existence from ancient civilization times. Romans and Mayans practiced ancestor worship. Ancestor worship among the Mayans and Romans was shown in their artworks. There was creation of sculptures, portraits, death masks and other works of art by the living. The artworks were then worshiped, exalted, glorified and granted utmost respect. The Mayans and Romans believed that by showing their respect for portraits and other artworks representing ancestors, the ancestors will be appeased and bless the living.\nAmong the Mayans, ancestor worship was exemplified in different forms. There was the building of idols; ashes of dead bodies were placed in the idols as a form of ancestor worship. There was the building of temples over an urn. Ancestor worship also involved human sacrifices. There are Mayan artworks that depict different deities. There was manifestation of deities in a variety of forms using different artworks. The forms that were used to depict deities were a reflection of the spheres of control and responsibilities that the deities had. The deities could be manifested in form of animals, thunder, fire, plants and lighting. According to the Maya, animals were put into four categories: flyers, crawlers, swimmers and insects. Maya deities could take formations of any of the four categories of animals. Paintings displaying manifestations of the deities were then done and worshipped (Plume 12).\nThere is evidence that suggests that ancestor worship was prevalent among the Mayan ruling class. The evidence suggests that members of the Mayan ruling class were buried in places of importance; later, veneration was done on their gravesites. There is evidence that suggests that visitations were done in the Mayan royal tombs. It is not far- fetched to suggest that those who repeatedly visited places ancestors were buried were involved in ancestor worship (Plume 21).\nHire a custom writer who has experience.\nIt's time for you to order amazing papers!\nWhat is ancestor worship\nAncestor worship among the Mayans involved going to gravesites of dead relatives and ancestors. After making visits in those places, the living would then leave offerings such as flowers, ceramics, food and items that belonged to the deceased in their former life. Mayans were involved in contacting their ancestors through rituals involving animal and human sacrifices. These rituals were done to appease the ancestors’ spirits. There is a Mayan artwork known as Yaxchilan Lintel; in the artwork, a lady by the name of Kabal calls out her lineage founder spirit by performing a bloodletting ritual. This is a true manifestation of ancestral worship among the Mayans (Plume 32).\nRomans practiced ancestor worship in several ways. They sometimes venerated bodies of deceased relatives. Romans sometimes made sculptures and death masks of dead people. They would then worship the sculptures and death masks as a way of remembering their ancestors. Romans also made portraits of their ancestors. Professional mourners used death masks made out of portraits of the deceased together with regalia of the deceased as a way of ancestor worship. Lifelike paintings of the deceased were put on the tomb of the deceased and worshipped (Plume 35).\nDeath masks made out of wax were preserved and used in ancestor worship. During funeral times, the death masks were brought together, displayed during funeral processions and used for worship to appease the ancestors. Romans held their ancestral lines with high esteem. Romans’ artworks are still in use for ancestor worship include Aulus Metellus bronze statute. Aulus Metellus bronze is of the size of a living human being. It is a work of art representing Aulus Metellus, a well-known orator in ancient Rome (Plume 39).\nAmong the Romans, marble sculptures of ancestors’ portraits were made and used in ancestor worship. The artworks were put in public display and used to worship the deceased. It was common to see people with masks of the deceased in public during ancient Rome. Most of the time, the masks were those of deceased close relatives Plume 45). The Romans had private art sculptures of their heroes and gods that they used to worship. Romans adeptly developed the art of portraiture. Romans history was documented on sarcophagi, arches and columns.\nPlume, Marguerite. Ancestor Worship. Cambridge, Massachusetts. 1965. Print.""]"	['<urn:uuid:22c12a70-fe80-4f0c-8630-d525ee80a5ad>', '<urn:uuid:aa986af4-e442-45e5-ad37-c9417c1ae36b>']	factoid	direct	concise-and-natural	similar-to-document	three-doc	novice	2025-05-12T20:11:36.665104	10	56	1403
44	help me understand difference regular internet connection vs mpls network speed reliability cost	Regular internet connections (like business-class internet) and MPLS networks differ significantly in several aspects. Regarding cost, MPLS is much more expensive - a 30 Mbps MPLS port costs about $656.24 per month, while a 150 Mbps business-class internet connection costs only $79.99 per month. In terms of speed, Local Area Networks using regular internet can achieve speeds up to 1000 Mbps, while MPLS networks provide dedicated links with guaranteed performance levels through Service Level Agreements (SLAs). For reliability, MPLS networks have well-defined service guarantees, particularly for latency and jitter-sensitive applications. However, modern alternatives like SD-WAN can achieve similar reliability by using multiple regular internet connections simultaneously, spreading the risk across multiple circuits to meet defined SLA requirements.	['In this write up we will give you a brief note on computer networking in the easiest way. Here, we will cover what is a computer network, types of computer network, components, properties, Uses and disadvantages of computer networking.\nWhat is a Computer Network?\nCommunication between two or more computers is called a computer network.\nSometimes multiple computers are connected to share the data, resources, or information. This feature of devices to communicate with one another is known as Networking.\nThere are different ways that help in transferring communication between devices and they are known as Network devices.\nExamples are Bridge, Router, Hub, and Switch. In the current era of computer technology, devices are used in all fields like education, agriculture, technology, medicine, transport and so on.\nHence all these organizations will certainly need the inter-network connection of their devices to share data and other resources.\nComponents of Computer Network\nFew of the very important computer network components are as follows: –\nConnecting devices – These are the middleware between computers and networks. which binds them together. Some of the connecting devices are –\nServers – These are computers with a high configuration that can handle the resources in the network. These are the machines that hold different kinds of programs or files that can be accessed by another computer that is present in the network. Some of the servers are database server, file server, print server, etc.\nClients – These are computers that request or get the information or service from the servers to access the network resources.\nAccess points – This allows the computer to connect to the wireless network, ie. Without the use of cables. This provides flexibility to mobile users.\nShared data – These are the data that is shared between the client like files, programs, and email.\nNetwork interface card – this controls the data flow between the computer and network. A network interface card can send and receive data.\nTransmission media – These are the mode through which data is transferred from one device to another. Transmission media can be fiber optic cables, coaxial cables or infra-red waves, microwaves, etc.\nTypes of Computer Network\nThere are three types of computer networks based on different attributes such as mode of connection, size, types of devices, etc.\nFollowing are the types of computer network that is based on size.\n- Local Area Network (LAN)\n- Metropolitan Area Network (MAN)\n- Wide Area Network (WAN)\nLocal Area Network (LAN)\nLAN (Local Area Network) is a type of computer network where a group of computers is connected to a small area. Mostly they are restricted to that place and it cannot be accessed outside.\nA few of the examples are hospitals, apartments, schools where they have access just in a building or organization.\nIn this case, only a few computers are connected and they are connected through switches and routers.\nLAN is always secure as no one outside the connection has access to this network. Hence data shared in this network is secure.\nLAN is comparatively smaller in size hence they are faster and speed ranges from 1000 Mbps.\nEarlier LAN was limited to wire connection, but these days LAN allows the local networks to work on a wireless connection. As it covers a small geographical area it is easy to maintain at low cost as well.\nMetropolitan Area Network (MAN)\nMAN (Metropolitan Area Network) network usually covers a larger area than a LAN but lesser than a WAN (Wide Area Network). This has a large number of computers connected and is a combination of more than one LAN.\nDue to size, in this type of network more administration is required. MAN generally covers an area of a city.\nFor example, the MAN network may cover school LAN, Hospital LAN, and a college LAN together.\nWide Area Network (WAN)\nA Wide-area network (WAN) usually provides large area transmission of data. WAN has a lower rate of data transfer compared to that of LAN or MAN as they cover a larger area.\nThe internet is a very good example of WAN. These normally do not belong to one company but exist with distributed ownership.\nThe setup cost of a WAN in a remote area is higher as the connection needs to be setup.\nHowever, the setup cost for a public network would be cheaper by using software (VPN). The best example of a WAN is a mobile broadband connection.\nWAN network allows us to choose the bandwidth of the network based on our needs. Larger organizations can choose large bandwidth for faster data while small industries can choose according to their need.\nThe main disadvantage of WAN is that we may unknowingly get the virus as our systems are connected to a large number of other systems.\nResolving an issue in the network may take more time as it is difficult to identify the exact location where the issue has occurred since WAN covers a larger area.\nUses of Computer Network\nSome of the fundamental benefits of computer network are –\n- It helps the user to share information quickly.\n- It helps to send data to other users through email.\n- It helps to share the email to other, scanners, and printers.\n- Users can connect to multiple computers to send and receive using the network.\n- Provides high reliability\nProperties of Computer Networking\nSome of the properties of the computer network are –\n- Scalability – By adding more processors, the scalability increases the system performance.\n- Reliability – In case there is a failure in the hardware or issues in connectivity, reliability makes it easy to use other sources of data communication.\n- Security– Users can take proper security actions, to keep the network safe.\n- Robustness – If there are by chance any defective nodes, the design of the network should allow it to function normally.\n- Migration – if the user decides to change the network, it should not affect its properties or operation and this is an advantage.\n- Sharing resources –A computer network can share the resources from one computer to another.\nDisadvantages of Computer Networking\nSome of the drawbacks of the computer networking is as follows –\n- Constant administration is required which requires time.\n- Some components of the network that has been designed may not work after some years and that needs to be replaced.\n- Initially, when the setup is getting ready, the investment cost for hardware and software can be costly.\n- Proper computer security precautions like firewalls and file encryption are required regularly, as intruders can steal the data.\nOther Popular Posts from Author:-', 'Software Defined Wide Area Networking (SD-WAN) is a buzzword that seems to be here for the long haul. It carries the same longevity like other recently coined words such as Next Generation Firewall, Cloud Access Security Broker, etc. It signifies a shift from the traditional (dare I say legacy) ways that enterprises conduct their business in regards to networking. With as much attention that this word gets, there is still a disconnect as to the purpose it serves enterprise networks these days.\nTraditionally, network sensitive applications were typically handled through the use of Multi-Protocol Label Switching (MPLS) networks. These are dedicated links provided by a telecommunications company that had well defined Service Level Agreements (SLAs) by the provider to ensure the business that the network would meet a certain threshold when it comes to network characteristics such as latency and jitter. These types of circuits are known to be very expensive but were used by businesses because there were really no better ways to guarantee network performance.\nTo get an understanding of why there is a huge shift to adopt SD-WAN, let’s identify some of the issues that plague this legacy paradigm:\nMulit-Protocol Switching Label (MPLS) WAN links are expensive\nDoing a quick search in Google, I discovered a PDF that lists costs for Verizon MPLS ports. According to this document, a 30 Mbps bandwidth port will cost the business $656.24 per month! Compare this with commodity business class Internet (Verizon Fios) that supports 150 Mbps bandwidth for $79.99 per month! Looking at the difference in price, can theoretically get 8 Fios circuits for the price of one MPLS circuit.\nIntelligent Failover Requirements\nThe increased usage of latency/jitter sensitive applications (such as video conferencing and VoIP) that are less tolerant to sub-optimal network conditions, require a solution that is able to monitor the network beyond the standard link up/down and probe (ping) response. Also, networks need sub-second network convergence when a link does fail or show conditions that are not conducive to support the requirements for a network application.\nMPLS WAN links are slower to provision than commodity circuits\nA quick search in Google shows that on average, MPLS provisioning can take upwards to 60 days, and that is in the US. In other countries, it could take upwards to 120 days depending on the infrastructure and capabilities of the provider in those environments. However, many commodity circuits can be provisioned within a week and in some cases, the same day the circuit is ordered (i.e. dealing with wireless providers). This greatly shortens the amount of time needed to make an office operational with connectivity back to headquarters or the cloud.\nBetter control (security/path selection) over different traffic types\nTraditional path control relies on using normal network based path control based on the network address where the destination network resides. In more intelligent platforms, the layer four (port) information of the protocol could be leveraged as well.\nHowever, in cloud connected environments where multiple applications may be hosted in the same network address space on the same port (think web applications), this does not allow granular control with routing. SD-WAN solutions address this by using deep packet inspection to identify the application of the packet and use that criteria to define policy to route the traffic.\nIn addition to that, SD-WAN solutions have capabilities to help control Quality of Service (QoS) by giving order and precedence to certain traffic in case of link congestion.\nIn order to take advantage of SD-WAN capabilities, there are a certain level of requirements that must be met by the environment SD-WAN will be deployed in.\nSD-WAN Requires Multiple Circuits for Connectivity\nSD-WAN overcomes the lack of SLA by commodity circuits by spreading the risk over multiple circuits at the same time. The thought process is that a virtual circuit bundle comprised of multiple circuits will meet a defined SLA within in a certain time period, given that one or more circuits will meet the SLA requirements of the network applications utilizing that virtual circuit bundle.\nSD-WAN Requires an Overlay Network\nOne of the consequences with leveraging many disparate internet circuits is that there is no control over the paths those circuits take to get to a common network. To accommodate for this, most SD-WAN solutions require an overlay network to route packets over to ensure a common topology for routing between networks. Creating this overlay network gives the SD-WAN administrator more control to provide a predictable path (built on top of the underlay network) for routing packets between two locations. This overlay network is commonly built with some type of tunneling/encapsulating protocol such as IPSec.\nSecure SD-WAN with Fortinet\nSD-WAN solutions typically provide a standard feature set comprised of:\n- Network measurement (through active or passive probing)\n- Intelligent path control to route traffic based on network characteristics like latency, jitter and packet loss\n- Forward Error Correction (FEC) for adding reliability to data transmission\n- Application awareness via Deep Packet Inspection (DPI) of network streams\n- Low (Zero) Touch Deployment and orchestration of SD-WAN solutions.\nThe FortiGate’s main differentiation when it comes to SD-WAN is that it is one of the few providers that has a “secure” SD-WAN solution. FortiGates is classified as a Next Generation Firewall (NGFW) with an embedded SD-WAN solution. This provides a distinctive advantage with this platform to allow you to implement enterprise class network security with intelligent path control built-in to the platform. Below are some highlights to capabilities specific to the FortiGate.\nSource Based SD-WAN\nThe FortiGate does not need an overlay controller to provide the intelligent path control within its SD-WAN configuration. All decisions regarding the SD-WAN is made from the source of the traffic thus allowing the SD-WAN to be decentralized and not necessarily requiring a complete vendor lock-in to utilize the SD-WAN feature.\nThis is specifically useful when performing load balancing of WAN connections from disparate ISPs. These load balancing methods can support a variety of methods to distribute load across multiple WAN circuits.\nEnterprise Security Features\nThe FortiGate’s pedigree of being a NGFW gives it a distinct capability of supporting common enterprise grade security features. These features consist of the following:\n- Intrusion Prevention\n- Content Filtering\n- Application Control\n- Cloud-Application Inspection\n- User Identity via Single-Sign On\n- DNS Inspection\n- Web Application Firewall\nDedicated Hardware for Overlay Network\nThe FortiGate has purpose built hardware that is used to accelerate throughput associated with IPSec VPN (commonly used for the overlay network). This allows for high throughput application with minimal delay (latency) with network applications that are sensitive to the sub-optimal conditions. Due to this, lower end FortiGates can be leveraged in remote locations providing a cost effective solution to enable highly capable solutions.\nThis initial post serves are an overview of SD-WAN as well as a means to annotate the capabilities associated with the Fortinet SD-WAN solution. Subsequent posts will detail the configurations associated with this solution as well as provide demonstrations of the solution in action.']	['<urn:uuid:d11ea839-71f4-4826-9ead-944d879b0672>', '<urn:uuid:b7b92e4e-4852-45fb-a1b4-046a62ead479>']	open-ended	with-premise	long-search-query	distant-from-document	comparison	novice	2025-05-12T20:11:36.665104	13	117	2274
45	What is special about Esslingen's Christmas market?	Esslingen hosts an extraordinary Medieval Christmas Market with merchants dressed in period costumes demonstrating classic handicrafts using Middle Age methods. The market features blacksmiths, glass blowers, potters, woodcarvers, minstrels, and traditional fair games.	['If you’re looking for a trip that puts you in a holiday mood, visit a Christmas market. It’s one of our favorite ways to get the season started.\nThrough the years at My Itchy Travel Feet, we’ve shared many festive travel ideas, including how to visit Germany’s top Christmas markets. Or why we think that an early December Rhine River cruise is so much fun.\nMITF Featured writer, Debi Lander of ByLanderSea, adds to our collection. If you’re planning a holiday trip to Stuttgart, read Debi’s tips for three Christmas markets to add to your itinerary.\nYou’ll discover that Stuttgart at Christmas is a delight!\nI’d long wished to visit some of Europe’s famous Christmas Markets and recently got my chance. I learned I could experience three different markets from the same home base — Stuttgart — in the Baden -Württemberg region of southwest Germany.\nI looked forward to shopping for hand-made German wooden crafts and those colorful character nutcrackers. At one point in my life, I’d amassed quite a collection of nutcrackers but then passed them along to my grown children. Hopefully, they will survive and get passed down to my grandchildren.\nAnother travel treat would be German sweets and pastries. I have a fondness for gingerbread and cookies made from cookie molds.\nBoomer Travel Tip\nIn today’s travel climate, trip insurance is a must. Compare policies and rates at InsureMyTrip.\nThe history of German Christmas Markets\nThe winter markets harken back to the thirteenth century and the celebration of the winter solstice. Munich claims to be the oldest in Germany, dating back to 1310.\nAs we might call them today, the pop-up stores serve much the same function they have for centuries. They provide a lively meeting place for locals and a market for shoppers.\nThe festive markets bring all the delights associated with Christmas: Old World aromas of roasting chestnuts, the taste of sweet and spicy gingerbread, the sounds of Yuletide carols, the sight of colorful lights, plus the interaction of people in good cheer. The little shops feature old-fashion wooden toys, seasonal decorations, and tree ornaments.\nMany of them are exquisitely hand-carved while the less expensive variety machine manufactured. Hand-knit winter accessories are also popular, and, naturally, traditional German food and drink await in abundant supply.\nStarting with the Stuttgart Christmas Market\nI began exploring Stuttgart’s downtown market bustling with nearly 300 wooden huts. Their roofs were covered with 3-D figures and designs, often animated characters, and festooned with lots of fresh greenery (no artificial garland).\nEach booth is filled with goodies, usually hanging from the ceiling or strung from beam to beam. I started to browse in the late afternoon, getting my first look at items I might want to buy. No surprise, I snacked on a gingerbread cookie.\nOf course, a massive downtown Christmas tree glimmered with hundreds of lights, but I was attracted to the life-size Nativity figures in a large rotating pyramid. Nothing like an Egyptian pyramid, German Christmas pyramids are circular, descending wooden tiered platforms that rotate by the heat of burning candles.\nSmaller versions of these become cherished annual centerpieces in German homes. They are intricately carved and almost always include the Nativity family, animals, and figures (shepherds, wise men) associated with the manger.\nThe big rotating pyramid in the center of Stuttgart mesmerized me. I’d never seen one like that before.\nMarketgoers stop to drink Glühwein — hot mulled wine, served in a mug — along with sausages, pretzels, nuts, or sweets. The grab and go meal felt like eating at a fair, but some people dine in the downtown restaurants.\nA holiday light show, against the backdrop of the Old Palace, included typical yuletide music and dancing designs. You can’t miss the floodlit models of a Mercedes and Porsche. After all, the city is home to those luxury automakers, which makes a fun trip to Stuttgart for car lovers.\nBoomer Travel Tip\nMedjetAssist Members who are hospitalized 150 miles from home receive medical transport to a home-country hospital of choice. Memberships from $99.\nVisiting the Baroque Christmas Market at Ludwigsburg\nAfter sunset, I rode a commuter train about ten miles to reach Ludwigsburg, a smaller town with around 100,000 residents. Ludwigsburg presents a Baroque Christmas Market.\nA walk through the modern downtown, illuminated by an angel theme, soon brought me to the holiday market. Folks passed through arched greenery, near two illuminated baroque churches, and a glittery angel hovered above the stalls.\nThe Ludwigsburg market shops were very family-friendly, and I wished my grandchildren were with me. Young ones debated their choices while picking out gifts, candy, or ornaments to hang from their tree.\nI heard singing, the sweet sound of children’s voices, and followed it until I came to an outdoor stage. I listened to elementary-aged students perform a medley of holiday songs, all in German, except Jingle Bells, sung in English. Equally, heart-warming was the delighted faces of parents and grandparents beaming with pride and love.\nAs I stood there, I felt the spirit of Christmas rekindle within me, spreading like a warm glow. I smiled and thought, “This is what it’s all about.” I connected with Ludwigsburg and its residents, a nice feeling for a tourist.\nHad I come earlier in the day, I could have toured Ludwigsburg’s Palace, nicknamed the “Versailles of Swabia.” It’s is a 452-room palace complex of 18 buildings and gardens — the largest palatial estate in the country.\nAlthough I think the Baroque style is a bit over the top, I wished I had toured. But this gives me a reason for a return trip.\nExperiencing a Medieval Christmas market at Esslingen am Necker\nThe next evening, I took the train a bit farther to historic Esslingen am Neckar. Picturesque doesn’t capture the magic of this town.\nEsslingen lies on the Neckar River, and nearly every building in the historic center remains a half-timbered architectural gem. The 200 fairytale-like buildings were thankfully left largely undamaged during World War II.\nIn fact, Esslingen lays claim to the oldest row of half-timbered houses in the entire country, dating back almost 700 years.\nEsslingen hosts an extraordinary Medieval Christmas Market, even earning the title of Best Christmas City back in 2018 for its size category. The big beautiful red Rathaus, old City Hall, makes the perfect backdrop for the theme fully embraced by merchants dressed in period-looking costumes.\nThey demonstrate and sell their classic handicrafts using Middle Age methods. Blacksmiths, glass blowers, potters, woodcarvers, minstrels, and more engage the crowds. I immediately fell under its spell, feeling like I was walking through a medieval fair frozen in time.\nA midway of traditional fair games called for active participation. The egg toss and archery had folks waiting in line.\nI kept running into jugglers, fire eaters, and crowd performers. A special children’s area even fits in a small Ferris wheel, kid-sized games, and puppet shows.\nScents from a whole pig roasting on a spit wafted through the night air, mixing with aromatic cinnamon and pine. The Medieval Market was alive and oh so much fun, by far the favorite market of my trip.\nThe Stuttgart or Baden -Württemberg Christmas markets were all I imagined and hoped for, and I sure wish I were going again this year.']	['<urn:uuid:b1c02da5-3115-4267-90c5-ba932123b344>']	factoid	direct	concise-and-natural	distant-from-document	single-doc	novice	2025-05-12T20:11:36.665104	7	33	1213
46	single use vessels filter testing advantages disadvantages pharmaceutical industry importance and integrity test methods	Single-use vessels offer significant advantages in pharmaceutical industry, including reduced cleaning and validation requirements compared to traditional materials, saving time and water resources. These vessels arrive clean and validated, maintaining sterility until use. However, they have a crucial disadvantage: the possibility of defects in the plastic vessel or seams, particularly problematic when vessels contain products worth millions of dollars. Filter testing is vital, as failures can occur in both the filter medium and sealing arrangements. Various testing methods exist, from basic porosity tests to more advanced techniques. The helium integrity test (HIT) method can detect defects as small as 10 μm, crucial since microbes can pass through holes as small as 15 μm. HIT testing is particularly valuable as it can test fully assembled vessels including tubing sets and ports, unlike traditional pressure-decay methods which require removal of these components.	"[""OR WAIT 15 SECS\nGraham Rideal is managing Director at Whitehouse Scientific.\nGraham Rideal of Whitehouse Scientific explains the importance of filter testing and offers some considerations with regards to choosing filter test methods.\nHow important is filter testing in the pharma industry?\nFiltration in the pharmaceutical industry covers a wide range of applications from powder processing to liquid purification, such as saline or dextrose solutions. In the case of the former, the advantage of good filtration processes mainly lies in recovering valuable drugs, which can cost in excess of $1 million per kilogram. The advantage here is purely commercial.\nFor intravenous products, however, good filtration is a matter of life and death; a simple internet search on recalled drugs reveals particulate contaminants from the clearly visible, such as glass fragments, wood fibres and mould, to the invisible, and potentially lethal, bacteriological contamination.\nThe primary test of a filter begins with the filter medium itself to ensure it is fit for purpose. Thereafter the filter system must be tested in situ. Failure can occur in the filter medium or in the sealing arrangement when assembled.\nOne of the simplest methods of comparing filter media is to test porosity either by air or water flow under prescribed conditions. Lower flow rates or higher back pressures indicate better filter performance.\nThe flow rate method is refined in the so-called bubble point test. Here, the filter is saturated with a liquid and then a gas (usually air) is pressurised from below. As the pressure increases, the air finds the single largest pore, which is blown out, forming a bubble on the surface: the bubble point. The Washburn equation is then applied to convert the applied pressure into the diameter of the pore.\nPorometry takes the bubble point one step further by continuing the pressurisation of the filter beyond the bubble point. Successively smaller pores are blown clear by the air pressure until the smallest pore is finally evacuated. The applied pressure versus flow rate profile can then be interpolated to provide complete pore size distribution.\nChallenge testing, as the name implies, involves challenging the surface of a filter with actual particles, which could be solid or liquids such as oil mists. Particle sizes are measured upstream and downstream of the filter. The maximum size particle passing reflects the largest pore, while the reduction in concentration determines the efficiency of the filter.\nAlthough conceptually easy to understand, challenge testing can give varying results depending on the particles used and the method of measurement. Irregular-shaped particles have a number of dimensions determined by their shape, so it is important to specify the method of particle size analysis. Furthermore, the method of measuring particle concentration can give different results; averaging by volume, for example, will give a completely different result compared with a number average.\nHow should a filter test method be chosen?\nSelecting an appropriate methodology for testing filter performance depends very much on the application of the filter. For non-critical applications where a simple quality assurance test is required, a basic porosity or bubble point test may be all that is required. For example, in the manufacture of syringe and water filters or in belt filters for product collection, the maximum pore size or the effective cut point of the filter medium is often the process control parameter.\nIf a comprehensive analysis of the poresize distribution in a filter is needed (e.g., dialysis membranes or air vents in bodily fluid aerosol traps used in theatre, then a porometer has to be used. However, because the pore sizes obtained are derived theoretically, they do not necessarily correspond to the true geometric size of the pores. There may also be variations from instrument to instrument, and operator to operator.\nNevertheless, the porometer is one of the most powerful tools in filter testing because it can measure pores from the sub-micron level to several hundred microns. Another advantage is that the test is nondestructive. As the pore structure is only invaded by an easily cleanable liquid, the actual filter used in the test can be reused in the final filter application.\nSo long as care is taken in particle selection and the statistics of measurement, challenge testing is one of the most robust methods of filter testing. If spherical particles are used and analysed by microscopy, where any out-of-shape particles can be eliminated, it is the most unambiguous test of them all. A particle either passes or is trapped by the filter and its size can be traced back to international standards of length. The range of measurement is restricted only by the particles available, so measurements could go down to a few nanometres using gold sols.\nGenerally speaking, challenge testing is restricted to measuring the cut points or maximum pore sizes, although new developments can provide pore size distributions in some cases. The measurement technique is called the 'near mesh' method where the spherical particles become lodged in the pores during the challenge test. When released by tapping or ultrasonic energy, the diameters of the 'near mesh' microspheres reflect the diameters of the filter pores.\nIn critical applications, challenge testing is the method of choice. However, unlike porometry, the actual filters used in the test cannot be used in the final application because of particle contamination.\nWhat developments do you expect to see in the future?\nWe now seem to be living in the nanotechnology generation where science is moving to smaller and smaller sizes. Filtration is no exception and there are already several filters available that use nano fibres, which, when used in syringe filters, can turn black Indian ink into pure water with a gentle press of the thumb (see nanofibre filtration now being offered by many filter media manufacturers).\nWhere as material science was once the predominant technology, chemistry is becoming the predominant discipline as molecular cages are built, rather like nano-lobster traps, to selectively remove contaminants or harvest valuable nano products. This exciting new technology known as Metal Organic Frameworks or MOFs, chemically constructs links or cages around a pore that are reduced in thickness to that of a single chemical bond of sub-nano proportions. As a consequence, these structures have the highest known surface areas, typically up to 6000 m2 /g. Put into perspective, this is equivalent to having the surface area of six football pitches packed under the nail of your little finger!\nAt such small sizes, porometry would not be appropriate because of the small pore size and the physical structure of the filter, which is usually in powder form. The only filter testing method available in these instances is challenge testing. Particle size analysis then becomes of paramount importance. However, there are now 'new kids on the block' rising to meet the challenge, such as ultra-high speed analytical centrifuges and nanoparticle tracking devices.\nIn the case of the analytical centrifuge running at up to 24000 rpm (CPS Disc centrifuge), a 'line start' technique is used, rather like a 100 m sprint. The sedimentation is followed optically as the larger particles sediment faster and so arrive at the finishing line before the smaller ones.\nIn the nanotracking method (NanoSight) individual nano particles scintillate light as they are illuminated by a laser in a liquid. Rather like floating dust particles scintillate a beam of sunlight in a room. This Brownian motion can then be tracked by microscopy and the diffusion rates used to calculate particle size.\nIn both of these methods, particle sizes distributions down to a few nanometres can be measured. The technologies are being used in the study of gold sols in the treatment of cancer and rheumatoid arthritis.\nHowever, in the MOF porous structures, even these methods fall short and either transmission electron microscopy or calculations from chemical bonds must be used to determine the pore sizes. At the moment, this exciting technology is something of a 'solution looking for a problem', but there is no doubt that the most significant applications in the future will be found in the pharmaceutical industry, most likely as highly specific catalysts in producing new active ingredients.\nDr Graham Rideal is CEO, Whitehouse Scientific and Science Correspondent for The Filtration Society."", 'Single-use vessels drastically reduce requirements for cleaning and validation compared to stainless steel or glass tools, which saves a great deal of time and resources, such as water. The vessel arrives clean and validated and stays that way until it is plumbed into the process train. As long as the vessel has no holes, sterility is assured. The downside of single-use vessels, however, is the possibility of defects in the plastic vessel (i.e., bag) or its seams.\nAs the biopharmaceutical industry has become more confident in using single-use technologies, the scale--and the value--of what is contained in single-use biovessels has grown. It is not uncommon to see single-use vessels on the market that accommodate up to 2000 L of product. In these cases, the value of what is in the vessel could be more than hundreds of thousands or even millions of dollars. In these instances, if a defect were to be present in the bag, the potential financial impact would be enormous, regardless of whether the vessel is being used for storage or as a bioreactor.\nThe rapid uptake of single-use vessels for use in bioprocessing applications has made assuring integrity that much more crucial. Although major failures in the seams or large punctures in a vessel should be visible to the naked eye, the real problem lies in the potential presence of microscopic holes. These smaller imperfections are often undetected and can cause not only leaks, but also the ingress of microbial or other contaminants into the vessel, thus ruining the entire batch.\nIt is crucial that each vessel is tested prior to use to assure that there are no holes or leaks in either the vessel’s walls, seals, or--importantly--the joints and seals between the valves and tubings that enter and leave the vessel. These joints are the most vulnerable points at which leaks can occur.\nHistorical integrity test methods\nFor many years, the only nondestructive technique available for detecting leaks was the pressure-decay method. To conduct this test, the vessel is filled with air to a predetermined pressure and left to stabilize for a set amount of time. The pressure is then remeasured; a drop in pressure indicates that some of the air has escaped from the vessel. The drop in pressure and the time elapsed can be used to calculate the total size of any defect(s) present in the vessel through which the air escaped. Typically, holes that are 250 µm (for a 200-L vessel) or 500 µm (for a 1000-L vessel) can be identified in this way.\nClearly, this level of accuracy is not sufficient to guarantee sterility in a vessel being used in bioprocessing applications. ATMI LifeSciences conducted studies to find a reasonable cut-off point for the bags used as bioprocessing vessels. The studies began with guidance from the food sector, where it has been shown that microbes can pass through holes as small as 15 µm. Using 15 µm as a marker, ATMI coordinated independent, aerosol microbial-challenge laboratory studies. These studies confirmed that microbes are unlikely to penetrate sterile vessels with hydrophobic film surfaces through any defects of 12 µm or smaller; microbes proved to be unable to penetrate a vessel defect of 10 µm or smaller (1).\nA new solution was required to detect defects as small as 10 µm. ATMI developed a solution using helium tracer gas (HIT, ATMI). Unlike the pressure-decay method that measures pressure loss in a vessel, this method measures the amount of tracer gas leaking through a defect. The amount of leaking gas can be correlated to defect size. Although helium was well established in leak-testing protocols in the automotive, aerospace, and vacuum industries, ATMI’s method was the first use of helium in integrity testing of flexible vessels used in the bioprocessing industry.\nFigure 1shows the ATMI HIT system.\nTo carry out HIT testing, the vessel is placed inside a well-sealed, rigid chamber and connected to a helium inlet valve. The seals ensure no air or other gas can enter or leave while the test is in progress. All the air is then pulled from the chamber with a vacuum until there is a negligible amount of helium in the chamber. A predetermined amount of helium is injected into the vessel. If there are no defects in the vessel, the helium will remain inside it. If there are any defects (e.g., holes or splits), the vacuum will cause helium to escape from the vessel into the chamber. If this happens, the helium is detected using mass spectrometry; the amount of helium detected correlates precisely to defect size.\nDefects down to 10 µm can be identified in this manner. In the rare case that a lower detection limit is required, the HIT equipment can be designed to meet those requirements, although it would be more technically challenging to set up. For these types of studies, the user must account for the influence of the tiny amount of background helium in the air.\nDetection limits of HIT, however, are not the only advantage over the constrained-plate method. In the real world, a single-use vessel is not just a vessel with a single inlet. It will have a number of different inlet ports, all attached to connectors and tubes, particularly if it is being used as a bioreactor instead of a product-storage container. These ports and connectors are the most vulnerable part of the vessel in terms of leaks, holes, and other defects. To run a constrained-plate pressure-decay test, however, all of these ports and inlets must be removed from the vessel before it can be placed between the plates. Therefore, not only does the constrained pressure-decay method offer lower sensitivity, it also omits testing of the ports and inlets that have shown to be most likely to fail. In contrast, the HIT method allows a fully assembled vessel to be tested. The entire vessel, including the tubing sets, is placed inside the test chamber. If end connectors have porous membranes on the tube ends, the outlets are blocked using a plug or sealing mechanism to allow testing of the joints between the tube and end connector.\nIn addition to being offered for the preshipping validation of vessels and manifolds manufactured by ATMI, HIT technology is now available to end-users for testing in their own facilities of single-use systems from any supplier. On-site testing enables processors to verify the integrity of vessels at the point-of-use, immediately prior to use, giving a further failsafe to ensure that expensive batches of biopharmaceutical products are not spoiled by microbial contamination or lost through a leak. HIT testing can currently be applied to vessels up to 200 L, with plans to extend this to 2000 L.\nAs the penetration of single-use systems in the marketplace continues to expand, the need to assure the integrity of those systems has become more important. Supply-chain security and product integrity are core components of biopharmaceutical manufacturing success. Innovations like the HIT platform are important to advancing the potential of the industry as a whole, regardless of single-use provider.\n1. ATMI, Internal research report.']"	['<urn:uuid:cac3c98c-f291-40a7-a779-f3b8d8b8eafc>', '<urn:uuid:90461960-eedf-474c-b553-bb3384da3e67>']	open-ended	with-premise	long-search-query	similar-to-document	multi-aspect	novice	2025-05-12T20:11:36.665104	14	140	2535
47	tension required french cornrow braiding	French braiding requires enough tension to keep the strands together while adding hair, but for cornrows, precise braiding with light tension is crucial - pulling too hard from the scalp can cause pain, hair loss, and permanent traction alopecia. Cornrows should never hurt or cause raised scalp.	"[""There are many variations of French braiding or plaiting, like the whole head plait straight down the centre of the head, placing the braid to one side or both and partial head French braids as in the images below. The difference between this type of plait and the simple three-strand plait is that you are constantly picking up more hair and adding it to the French plait (or 'braid'). To create another hair style plait variation you can reverse the directions below and have an 'inside out' braid. You will need plenty of practice as with any hair plaiting it gets easier and faster as you do, if you are planning to do this to your own hair a few muscles should be gained so that you can hold your arms up for the time needed)!\nA side French braid can be glamourous or for school! This hair style from Paloma Garcia\nRebecca Woloszek a French braid used to hold back masses of curls in this hair style\nJoharn Cuthbert combines a French braid with a top knot for this hair style\nTools and other items you will need:\n- Covered elastic bands\n- Paddle or similar brush to remove tangles\n- A little hair spray\n- A clip to keep hair not being plaited out of the way\n- Possibly a few pins or grips or odd pin if you have layers to secure any stray bits!\nSimple French Braid\nThis is a single braid that follows the curve of the back of the head. Your hair needs to be quite long for this, a few layers won't matter as long as they reach from the side of the head to the centre back and pass it by about 10cm. You will need to know how to do a basic plait to understand the instructions. If you need some help with this first then return to simple plaiting.\n- Separate the front (including sides) of your hair from the back. Some people prefer to separate the top leaving only the sides to be added in later but this is a little more difficult. Draw the separated hair back to the crown of your head and divide it into 3 equal strands.\n- Do a quick left over center, right over centre (as in the simple plait) to start.\n- As you continue, add extra hair taken from directly underneath each new section about to be crossed over. Then go ahead and cross it over. The amount of hair you pick up should be about the same each time.\n- Your braid will naturally follow the curve of the head as you pick up and include the new hair. Keep going until you reach the nape and all of your hair has been included.\n- Finish with a simple plait and secure with your covered band when you reach the ends of your hair .\n- Don't try to make the braid too neat to begin with, master the technique then improve on the neatness.\n- Try to keep your tension and section size the same when working on neatness.\n- Use your little finger to section, this will allow you to hold the rest of the hair firmly whilst picking up the new hair.\n- For a variation, you can finish the braid just above the nape and turn it under to form a roll that can be pinned in place.\nSide French Plaits (as shown in image at top of page).\n- Find your normal parting or create a straight down the middle parting (if you have any shorter pieces of hair that may not reach the center back if parted to either side).\n- From the crown to the nape separate your hair into two halves.\n- Clip one side out of the way and begin to plait the other.\n- Divide this side into three sections at the front hair line and plait it as you would for a normal French plait (see above).\n- When you get as far as the nape, remove the clip from the other side and use it to hold the plait in place whilst you do the second side.\n- Once you have both sides plaited as far as the nape, combine the hair and plait as you would for a simple plait until you reach the ends. Secure with a band.\n- If your hair is shorter, then secure with the band at the nape and tuck it under itself, holding it in place with a few pins.\nVisitors to this page also went to:\nThe Simple Plait\nPlaiting gallery of hair styles\nStyling with flat irons or crimpers\nHair style galleries\nCuts to suit you\nHair coloring, perming and chemical straightening\nHair salons - getting the most from your next salon visit\nMore about your hair type"", 'Cornrows are a popular West African style of braiding the hair along the scalp. They are also known as “underhand track braids”. It’s a traditional art that anyone who has the patience can learn, but it takes some time and skill to master. This article will help you get started with the basics while avoiding some of the mistakes beginners often make.\n- Plan your style. Having in mind what your end goal looks like will help you to form a path for getting there. You can do this in your head, draw a picture, or make some marks on a Styrofoam wig holder. The easiest amount to begin with will probably be four to six sections from the front to the back of the head.\n- Spritz some water, or water mixed with detangler, on the hair. Comb or brush it through to remove all major tangles. The hair should be slightly damp, but not too wet. The reason for this is that you don’t want to have to pull the hair a lot to create the tension needed to hold the style together. Hair contracts when it’s wet and expands as it dries. Despite what some people say about a tight braid, this is the best way to achieve it – not by pulling the hair hard away from the scalp.\n- Part a section of hair that you would like the cornrow to follow along. put the sides of the hair that you aren’t braiding in two pigtails so they don’t get in your way. Move other hair out of the way so that you have a clear path to follow. Then take a small section of hair where you want the cornrow to begin. Don’t take too much, especially near the hairline, or you will have to pull too hard to continue.\n- Separate that small section into three strands and make a normal braid of about 2 “stitches” to get it started.\n- Holding the two outer strands aside, reach down under this initial braid to add a little hair to the middle strand. Fully merge this new hair to the middle strand so that it becomes a part of it, and you again have 3 strands. Make a braid stitch out of these strands.\n- Continue braiding, each time adding a little more hair to the middle strand, and repeat this until you’ve run out of hair to add. If you’ve reach the end and there is still hair left over, then continue with a regular 3 strand braid.\n- Secure the cornrow with a snap bead, hair clip, end bar, barrette, bolo tie tip, or whatever you like, just so long as you will be able to easily remove it later. Uncovered rubber bands (elastics) are not recommended unless they are the kind made specifically for hair. The ones made for office use will break off the hair.\n- People with straighter or slicker hair may need to use what is called “aqua wax” or a protein hair gel to help the style last, and to preclude the need for extra tension during styling.\n- If you do not wish to wet the hair first, you may consider at least using a hair cream or hair mask or leave in conditioner. (However if you wet fragile hair it could cause breakage because hair shrinks as it dries.) Some people don’t like to wet the hair, and this is fine. However, if you don’t, you will need to be extra mindful of tension. Use a featherlight touch.\n- Precise braiding, not pulling hard from the scalp, is what makes a long lasting style. Pulling too hard from the scalp does nothing to help a style to last. It only leads to pain and hair loss.\n- Hair can be washed with cornrows in. Simply wear a stocking cap over it, and use a shower massager or water pick to force water underneath the braids. Use a diluted shampoo and water mixture to wash, clear water to rinse, and spritz the hair with a leave in conditioner or hair oil afterwards.\n- If you tuck the hair under, you are making an “invisible” track braid, and if you reach for hair from beneath outwards, you are making a “visible” track braid.\n- People with fine or sleek, straight hair may want to use some sort of braid spray to help make the hair “stickier” so that the braid does not fall out as it’s being put in the slicker, fine hair.\n- Narrow braids stay in longer than larger ones.\n- This style should not hurt. If you can see the scalp is raised, or the person is complaining that it’s painful, back it up and start over. Too much braid tension can cause traction alopecia (a specific type of hair loss), and it might be permanent. It can also lead to infections and irritations.\n- To keep braids in place, use a moisturizing flexible hair gel or aqua wax during styling.\n- Wear a scarf or durag while sleeping, and braids will stay for up to one week. After that they’ll probably need to be redone, depending on how thick the rows are.\n- For curlier or “kinky” hair, braids can last for up to a month.\nThings You’ll Need\n- A rat tailed comb. These can be found at many beauty supply stores, but if you can’t find one, the end of a tint brush may suffice.\n- Coated rubber bands for hair.\n- Plenty of time.\n- Be careful in the sun. It is very easy to get a sunburn on your scalp, because the rows will expose the skin. Rub in sunscreen or wear a hat.\n- You may need hair grease and/or aqua wax.\n- For straight hair you may need to use a hair wax that is based on distilled water, not oil. It looks sort of like a firm gel. Aquarius Wax and Ice Wax are good ones. Butch Wax may do the job, but it is a little oily.\n- Very tightly curly hair should be treated with a moisturizer that does not break down too soon. This excludes most hair creams. The best thing to use is a hair food or hair grease, or a natural oil mixture.\n- Spray bottle to dampen if hair gets too dry.\n- How to French Braid Hair\n- How to Dreadlock Straight Hair\n- How to Put up a Mohawk or Liberty Spikes\n- How to Fishtail Braid\n- How to Make a Rope Braid\n- How to Remove and Prevent Split Ends\nSources and Citations\n- ↑ http://www.untrainedhairmom.com/cornrowing-for-beginners-tips-and-easy-styles/\n- Article provided by wikiHow, a wiki how-to manual. Original article: How to Braid Cornrows']"	['<urn:uuid:2ce5ebdf-89cc-465b-aca5-a21943763cc5>', '<urn:uuid:c4be908e-ab12-4693-a231-67d89c66d88b>']	factoid	with-premise	short-search-query	distant-from-document	comparison	expert	2025-05-12T20:11:36.665104	5	47	1934
48	What changes should you make to bike settings for wet conditions, and what mandatory gear adjustments are needed for wet weather hiking?	For biking in wet conditions, reduce rebound to help absorb impact, lower tire pressure by 2-3 PSI to increase grip, and consider using either wide aggressive tires that can grab terrain or skinny tires (1.95 or 1.75) that can cut through mud. For hiking in wet conditions, mandatory gear adjustments include using non-cotton materials (no Coolmax or lycra) for thermal tops, ensuring jackets have fully taped waterproof seams and hoods, and keeping all mandatory clothing dry in lightweight dry sacks or multiple NEW Ziploc bags.	['(When I told someone I didn’t have time to do a post for the 50km people my wife offered to do one. thanks Honey! – Adam)\nI have run the UTA 50 three times and the UTA 100 once, plus crewed for Adam for the UTA100 too many times to remember.\nI have put what I will be using for gear, but I do work for an outdoor supplier, so I am slightly biased. Please note that I am not a gear freak (YOU HAVE A WHOLE CLOSET FULL OF OUTDOOR JACKETS- editor) (I know someone who will argue against that!) but I like to collect it. I don’t know about the weights of my gear, but I buy most of my gear on sale and try to get the most packable and durable that I can afford at the time.\nIf you are not sure about a product, ask the question before you buy the gear – most major outdoor stores have moved into the trail running scene and the staff should be willing to find out the answer if they don’t already know it. Or ask other runners via the SUFR page .\nI like the local independent stores in Sydney – Pace Athletic and Frontier Adventure or online stores like Wild Earth or Running Warehouse or Globe Trekker. If you are lucky to live in Melbourne, the outdoor outlet stores in Collingwood, can save you heaps of money.\nRULE NUMBER ONE – USE ALL THE GEAR BEFORE YOU DO THE RACE.\nRULE NUMBER TWO – If there is any discrepancy between my explanations and the official line, the officials win. No arguments.\n1 x long sleeve thermal top (polypropylene, wool). Cotton, Coolmax and lycra garments are NOT suitable. Compression garments and thermal compression garments are NOT suitable. Compression garments may still be used in the race but they are in addition to your mandatory thermal top and do not replace it. Refer to ‘Thermal Garment Requirements’ link for more information. Refer to this link for an explanation.\nI use a Patagonia mid weight capilene long sleeved shirt. If it’s cold at the start, I’m usually wearing it. I treat it really badly and it still loves me. Totally worth the $25 I paid for it on sale at the Patagonia outlet in Melbourne.\n1 x Waterproof and Breathable Jacket with Fully Taped (Not Critically Taped) Waterproof Seams and Hood. The breathability must be provided by the material itself and not exclusively by mesh panels. Minimal underarm vents are allowed if the jacket material itself is technical and breathable. Large mesh panels, even if covered by flaps are NOT permitted. A premium jacket would have a waterproof rating of over 15,000mm hydrostatic head and breathability MVTR rating of 20,000g/m²/25hrs however much lower ratings are completely acceptable. Any non-membrane jacket must still be in very good condition with waterproof coating intact. The jacket must fit you. Plastic rain ponchos, wind jackets, water resistant jackets are NOT suitable.\nI have a Salomon Bonatti that gets used once a year but carried everywhere. I love it. I have a Kathmandu Zeolite, but the Salomon is a smaller fit and fits me better.\n1 x beanie, balaclava or buff. Beanie, Balaclava or Head Sock (Buff)\nI am the Buff queen and own over 25 pieces. I hate beanies. But will happily wear 2-3 Buffs when it is cold! They make a wool version, which is great for the colder weather. As a tip, beanies/balaclavas/head socks need to be made of a synthetic material or wool. No cotton – you will get cold if it gets wet.\n1 x High-Visibility Safety Vest that complies with Australian Standard AS/NZS 4602:1999 – D/N Class for Day and Night Time Wear. It must be made of a combination of retro reflective and fluorescent materials. This is not a running vest but a work wear vest. The vest must have either AS/NZS 4602:1999 or AS/NZS 4602:2010 or AS/NZS 4602.1:2011 as well as Class “D/N” on the tag. The vest must be clearly visible from both the front and back, even when wearing your backpack so you must have an oversized vest that covers your whole torso AND your backpack (your race number must still be visible on your front and over the top of your vest). Vests can be purchased at hardware stores, work wear stores, the UTA shop or Race Check-In. Refer to ‘Suitable Vest’ link for images. UTA100 runners must carry the vest at ALL times whether they need to wear it or not. The item is weather dependent for UTA50. For specific details of when you are required to wear your vest, refer to the Competitor Briefing document, which is available one month prior to the event.\nI got mine from a building site that someone had left behind – score 1 for me!\nAny hardware store sells these, just make sure they comply with the Australian Standard. Buy the XL – mine slides a bit, but its fits over my pack when it’s fully loaded.\n1 x headlamp- Test your headlamp on bush tracks at night prior to the event to make sure it provides enough light to both see the track and the course markings. Make sure batteries are new or fully charged and you have enough battery capacity / spare batteries. Note that waist lamps are not permitted, as they will obscure your race number.\nI have several Petzl headlights – but for the UTA 50, I use a Petzl Zipka. It’s tiny and fits really well into the corner of my pack. I used it as a back up light for the UTA 100 and it came in very handy when ran out of batteries and had forgotten to put in spare batteries …\n(If you want to ask me about the different styles that Petzl make – PM me.)\nMost UTA 50 runners will not need their headlamps, but its better to have a good one, just in case you are still out there when it is getting dark. Please note that it can get dark earlier than you think due to the cliff faces being on the eastern side – the sun disappears about 4pm. I have seen someone’s very cheap Chinese headlight implode on the start line – luckily they had a back up handy and were able to start the race.\nIf you by a new headlight – read the instructions, test it out even if you don’t use it during the race. See rule number 1.\n1 x Mobile Phone in Working Order with Fully Charged Battery Mobile phone coverage over the course varies from excellent at most escarpment / cliff top locations to non-existent when in deep valleys or when directly below cliffs. Generally you can get reception on hills and ridges across the whole course especially when you have views directly to Katoomba. We strongly recommend you have a Telstra phone. If you need to buy a new phone or a pre-paid SIM card or borrow a phone, Telstra is preferable as it works on approximately 90% of the course. Optus works on approximately 50% of the course. Vodaphone works on less than 30% of the course. For specific details of phone coverage on the course, refer to the Competitor Briefing document, which is available one month prior to the event.\nYes we all hate Telstra – it’s a national sport. However they do have the best coverage on course if you want to send proof of life photos during the event.\n1 x Compass- used only in the very unlikely event that you get lost. While we recommend a good quality compass such as the Silva Field 7, you can bring any compass as long as the magnetic needle will settle quickly and will point to magnetic North. A waterproof watch compass is allowed as long as you can calibrate it and know how to use it. An iPhone compass is not acceptable as it is not waterproof and the batteries may be needed for making emergency calls.\nWords stolen directly from Adam as this is where I got my compass from –\nSmallest compass I have been able to find is these at 12 for < $2, being totally honest, some of them don’t point north as quickly as you might like. Buy a set, save the ones that work properly, chuck out any that don’t meet the spec, give a few away.\nMost running packs have a whistle built into them –please look for it on your pack before you get to the race (see Rule Number 1) Otherwise head to Rebel or a sports store for one.\n“Order something like this which has whistle, compass and backup light all in one\nOr this which is locally available and has the compass and whistle”\n1 x emergency space blanket, light bivvy sack or equivalent\nI got mine from Kathmandu. Any good camping store should have them.\nCompression Bandage (Minimum Dimensions 7.5cm Wide x 2.3m Long Unstretched) The wrapping should list ‘heavy weight cotton crepe bandage’ or ‘heavy cotton elastic bandage’ or ‘heavy weight elastic support bandage’. This item is used for the treatment of sprains or snake bite. Generally the pink coloured bandages are suitable and the white bandages not. There will be compression bandages available for purchase at Race Check-In (mostly as a service for international runners). Refer to ‘Suitable Bandages’ link for images.\nBuy from a chemist – just make sure that if you unwrap to pack it better in your pack, keep the packaging with it as well in case they do a gear check.\n1 x lightweight Dry Sack This is to keep the compulsory clothing dry (multiple NEW Ziploc plastic bags work well for compressing your clothing and being able to see the item through the plastic; useful for random gear checks). Sea to Summit Ultrasil dry sacks are also a good option (refer to the ‘Ultrasil Dry Sack’ link)\nI use the IKEA zip lock bags – they are more durable than the ones from Woolies/Coles.\nI have been known to dump all the clothes into one big zip lock bag – especially when I know the weather is going to be good.\nIf it looks like rain, then I separate the compulsory clothing into its own zip lock bag and mark on the outside what is in there.\n1 x waterproof map case or any other way to keep your maps protected such as map contact\nAgain a good quality zip lock bag works well here. If you are paying attention, you should not need to use the maps in this race.\n1 x Ziploc bag for personal rubbish\nI usually stuff this in with the maps.\n1 x set of maps and course descriptions (provided by organisers). At registration, you will be provided with one set of maps and course notes. You will need to protect these from getting wet (using item below)\nNo extra info needed for this- make sure it is in a waterproof package and chuck it in your pack- you won’t need it EXCEPT if you’re asked for it as part of a gear check, so know where it is. Yes this has happened. More than once.\n1 x A5 Participant Emergency Instructions card on waterproof paper PROVIDED BY ORGANISERS IN YOUR RACE PACK. The card is A5 on waterproof paper\nSelf-explanatory. Don’t need to memorise it, just know where it is if you get in trouble.\n1 x Race Number PROVIDED BY ORGANISERS IN YOUR RACE PACK. Must be worn on your front, over your belly or chest and be visible at all times over the top of your outermost layer of clothing such as waterproof jacket, high-visibility vest or other garment (safety pins are also provided in your race pack). The race number must NOT be worn on your pants or leg. The race number has a single disposable timing tag already stuck to its rear side. Do not fold, bend, cut or pierce the race number as you may damage the timing tag. It must be worn as is, unfolded. A recommended method of securing your race number is to use a race belt, which allows you to easily have your number visible over the top of your outermost item of clothing. You will need to provide your own race belt if you choose to do this.\nRemember if you aren’t near the front of the pack you will most likely experience a few weather changes during your event, and you’ll probably want to change clothes. If the extra fleece and waterproof pants are declared mandatory during he race you could be changing both your bottom and top clothing, meaning that the best way of having your race number visible at all times is to have it on a race belt or a SPI-Belt. Don’t buy the Spi-Belt with the little things to hold gels on the side. These will wobble themselves out and you’ll not only run out of food, you could be disqualified for littering. Seriously, these are a dumb idea, but you can keep salt tabs or a gel or two inside the Spi-Belt pockets.\n1x Timing Tag for Backpack (Bag Tag) PROVIDED BY ORGANISERS IN YOUR RACE PACK. This is a disposable timing tag, which needs to be secured to the back of your running backpack (a cable tie is also provided in your race pack). Due to the bag tag, it is preferable not to swap backpacks during the event but if you plan to do so you will need to have your own side-cutters or scissors to cut the bag tag cable tie off the first backpack and your own spare cable tie / zip tie to attach the bag tag to your second backpack. Refer to ‘How to attach your bag timing tag‘ for images.\nRecommended items via UTA website:\n- Vaseline, Body Glide or other body lubricant\n- Cap or sun hat\n- Spare socks\n- Spare headlight batteries\n- Additional warmer clothing at supported checkpoints\n- A spare headlamp in case your main light stops working.\n- More substantial first aid kit (sterile dressings, roll of strapping tape, blister care such as blister block patches, Compeed or Fixamol, antiseptic wipes, painkillers, and any relevant personal medications).\n- External battery charger for your phone\nGoing to the Toilet on the Course\nAn issue of great concern is toilet paper and human faeces being left visibly on the track. There are toilets at Scenic World, the start at Queen Victoria Hospital and one toilet at the emergency aid station. If you have tricky bowels we suggest you buy a Go Anywhere Toilet Kit (“Wag Bag”) from an outdoor retailer. We encourage you to purchase one of these kits as we simply cannot have people leaving faeces and toilet paper in this pristine environment. Remember Leave No Trace!\nThe Go Anywhere Toilet Kit is a portable, waste collection system that turns solid and liquid waste into a hygienic, odourless, biodegradable substance. The waste collection bags are pre-loaded with non-toxic Poo Powder which treats up to 900g of liquid and solid waste allowing for multiple uses. The Poo Powder contains a decay catalyst that controls odours and breaks down solid waste into a spill proof, bin friendly substance.\nAfter using the kit, carry it with you to the next checkpoint where there will be a waste bin for disposal.', 'We’ve officially entered one of our favorite seasons… the birds are chirping, the days are longer, and, depending on where you are, you might be seeing the ground for the first time in months. The good news: it’s time to get back on your bike. The bad news: the weather can be finicky. But your ride plans don’t have to be foiled from a little pesky H2O. Follow these 7 steps to ride the wet with maximum fun and minimal damage to the trails and your gear, plus the added bonus of boosting your skills on the bike.\nIt doesn’t matter how much pep you’ve mustered to head out for a soggy excursion, you need to determine how the trails hold up to being ridden in wet conditions. Some trails are built to withstand it with excellent drainage – these generally have rocky or loamy surfaces. Other soil types will absorb water turning into mud pits that become giant, double track bogs. When it eventually dries out the results are deep, ruts – AKA a trail builder’s worst nightmare that will take countless hours to repair.\nYes, that’s a lot of variables. If you’re not sure, check with your trail advocacy group or swing by your local bike shop to gather trail beta and load up on any last-minute gear you might need.\nTerri Watts and Emily Cox at Windrock Pro GRT\n1. Gear up\nYou’ve heard it before: there’s no such thing as bad weather, just bad gear. Invest in a quality rain jacket that rides the line between water resistant and windproof while still being breathable. Layer up with a few thin layers that you can peel off on the climbs and put back on for chilly descents. As with cold-weather riding, if you’re warm hanging out in the parking lot getting ready to roll out, you’re probably wearing too much.\nWe get it – rain jackets are expensive. If you go without one, just keep in mind that your layers will hold water, get heavy, and drag you down so less is more in that case!\nDo you live in a place where you ride in the wet regularly? It might be worth adding waterproof shorts or pants, shoes, and gloves to your gear closet to keep you more comfortable.\n- A rain fly for your pack will protect your necessities and is much easier to clean than your pack\n- Put your phone in a sealable sandwich bag to protect it from the elements. You’ll still be able to use the touch screen to navigate.\n2. Keep it clear\nMany trail helmets nowadays have visors that will shield some of the rain from hitting your glasses (we love Rudy Project’s Protera helmet). Pack a sunglasses wipe and try riding without them for the climb to keep them fog-and rain-free saving the best visibility for when you need it most – the way down. Anti-fog products like Rain-X work wonders if you’re struggling to keep the steam and fog at bay, but it usually clears up when you get moving and have some air flow.\nPhoto: Terri Watts\n3. Protect your bike\nFender – A fender is key for keeping most of the slop out of your face and eyes. It also protects your fork seals increasing the time between fork services and improving the life of your shock. Ground Keeper Fenders has some wild designs to accessorize your rig and it’s low-profile enough to install once and run year around.\nLube – Use a thick chain lubricant. A thin chain lube can wash away rather quickly, but a thicker one will stay put, even when coated in mud. They’re usually clearly marked for the conditions they’re designed for so there’s no need for guesswork or in-store sampling!\n4. Perfect your bike\nReduce your rebound to help absorb impact and lower your chance of slipping on obstacles. Lower your tire pressure by by 2 to 3 PSI to increase grip, confidence, and control. The bigger your tire contact patch (or your bike’s footprint) the more it can conform with the trail and obstacles to give you much-needed grip in the slick.\nTire selection – there are two schools of thought:\n- Option 1: Ride a wide, aggressive tire that can grab the terrain (while shedding mud). If you have narrow chain/seat stays (and/or fork), the mud can build up between your tire and the stay eventually slowing you down to a halt. If you have plenty of clearance, this is a good option.\n- Option 2: Skinny is better (1.95 or 1.75). The idea is that the skinny tire can cut through the mud and find some terrain to connect with. Also, this provides for good clearance between your chain/seat stays thus less mud build up.\n5. Slow your roll\nRiding in slippery conditions will be more challenging so dial back your speed to focus on how your bike handles on the varying dirt conditions and obstacles. You’ll be improving balance, control and bike handling skills, plus those “Oh sh*t” moments will teach you how to recover. You’ll need to adjust your center of gravity to maintain traction which will boost your dry-weather technical riding, too!\nNavigate obstacles with your bike upright and ride directly at them at a 90 degree angle (think about making your bike “T” with a root and not “Y”). OR avoid the slip factor altogether with a front wheel lift or bunny hop! Brake a bit early before corners because brake pads will not be as responsive as in dry conditions.\nWhen it comes to puddles or extra soft spots in the trail, ride through them and not around them to avoid widening the trail (and the puddle or hole). If you’re running into them frequently, it’s probably best to avoid that particular trail when it’s wet since there are drainage issues.\n6. Peel it off\nIf you’re not prepared with a pack a towel and change of clothes then it’s going to be a long, dismal drive back home! For an extra-cush experience, keep a changing mat in your car to stand on in the parking lot as you swap your nasty gear for the fresh stuff and use a trash bag or reusable shopping bag to keep the dirty bits contained until you get to a hose.\nPhoto: Terri Watts\n7. Ride, rinse, repeat\nClean your bike as soon as possible after your ride with soft brush to get the majority of the debris off. Be careful not to spray water directly into any part of your bike that has a bearing (bottom bracket, steering tube, hubs) or your fork because it can damage bearings (even sealed ones). Always lubricate your bike directly after cleaning it.\nSame goes for your gear. Be sure to rinse the grime off your kit and throw it straight in the wash or you’ll risk having the dirt set in for a whole new shade of “ochre” or “cocoa” (although those colors are on trend this year!). Hose down your helmet, pads, and shoes. Remove your shoe insoles and stuff them with newspaper to speed up the drying process and minimize the soggy stink that can set in.\nHappy Spring riding! How do you get stoked to ride in less-than-ideal conditions?']	['<urn:uuid:e867bb43-fdbd-4deb-b2b5-51d4cc358e70>', '<urn:uuid:9185cb5f-0b91-4f95-8548-7c7fdb7f0e03>']	factoid	direct	verbose-and-natural	distant-from-document	multi-aspect	novice	2025-05-12T20:11:36.665104	22	85	3795
49	What attractions were included in the Madu Ganga river safari?	The Madu Ganga river safari, which took place on a river that opens into a large lagoon leading to the Indian Ocean, featured two major attractions: a Buddhist temple and the Cinnamon Island.	['The 7th day of IAO 2018 is dedicated to visiting the ancient city of Anuradhapura. The tour starts with an early breakfast in the hotel. The first stop was Ranmasu Uyana (Royal garden) to witness the Stargate of Sri Lankan is called “sakwala chakraya” which is a stone carving, and this has similar features of stargates at Peru and at Abu Ghurab in Egypt. Then the participant experience of scenic beauty Thisa wawa a manmade reservoir situated next to the Ranmasu Uyana. Then the crowd headed towards Dana Salawa an alms hall used to feed over 5000 monks. The next stop was the Abayagiriya Stupa one of the largest manmade structures in the ancient world. The participants had the opportunity to visit Jaya Sri Mahabodiya a sacred fig tree which is the southern branch of the Sri Maha Bodhi at Buddha Gaya in India under which Lord Buddha attained Enlightenment. The final stop was the Ruwanweli seya another large ancient stupa in Sri Lanka.\nThe practical rounds of the IAO 2018 were held in the 6th day. The evening was spent leisurely with recreational activities and local shopping.\nThe day 5 of IAO 2018 is a very enjoyable day. In the morning, the participant headed towards the down south for a river safari in Madu Ganga. Mada Ganga is a river open up into a large lagoon on its way to the Indian ocean. Buddhist temple and the Cinnamon Island were the major attractions in the river safari. In the evening an open mic session was held at the hotel hosted by the FOS Media. Every country performs an act related to their culture.\nMore images click here\nThe 4th day of IAO 2018 is a very long day for the participants. In the morning, participants were able to visit the Galle Face Green, an ocean-side urban park. While enjoying the view of the Indian Ocean, participant had an opportunity touch and feel local pythons and watch snake dance. Then the crew witness the Independent Square and them marched towards the Sri Lanka Planetarium for a special show. After lunch, Mr. Mahesh Herath conducted a very interesting lecture on exoplanets. The most challenging event of the day was the observational rounds. The observational rounds were conducted at the Arthur C Clarke Institute of Modern Technology. It was a long examination and the whole rounds were ended at 4.30 am of the next day.\nFor images click here\nThe 8th of October is the 3rd day of the event. The day started with the theoretical round. The examination was 4 h long. Then students, team leaders, and observers went to a shopping tour to Colombo. In the evening students had an opportunity to play with the telescope that they supposed to use in the observational rounds.\nFor images click here\nThe 7th of October 2018, day 2 is the inaugural day of the event. The inauguration was held at the Eagle’s Lakeside, Aththidiya. All participants escorted by Sri Lankan police from Pegasus Reef Hotel, Wattala. The inauguration was graced by guest including Professors from the Universities, Council member of SLAAS, Council members of the IPSL, Secretaries of the ministries and Scientist from the Arthur C Clarke Institute of Modern Technology. Professor Chandana Jayaratne the Chairman of the Local Organizing Committee welcomes the guests followed by the official declaration of the event by the Dr. Michael Gavrilov the Chairman of the International Astronomy Olympiad. The contestant parade was the next attraction of the event. The gathering was entertained by the cultural events and Sri Lankan martial arts performance. The event was concluded after-dinner reception.\nImages click here\nThe competitors from all over the world are participating for the 23rd International Astronomy Olympiad in Colombo, Sri Lanka from 6th to 14th of October. Yesterday, the first two teams arrived and they joined with their local student guides. All the participants are welcome to Sri Lanka.\nPhotographs Click Here\nThe XXIII International Astronomy Olympiad (IAO) press conference was held at the chambers of Sri Lanka Association for the Advancement of Science on 2nd October 2018. The aim was to brief on the 23rd International Astronomy Olympiad which is to be held from 6th to 15th of October 2018 in Colombo.\nProf. Chandana Jayaratne the Chair of International Astronomy Olympiad Local Organizing Committee expressed on the on the nature of the program and its expectations on science. Dr. Michael Gavrilov the Chairman of the International Astronomy Olympiad who is now in Sri Lanka to direct the competition aired his views over the aims and history of the International Astronomy Olympiad.\nDr. Chinthaka De Silva the Secretary of the International Astronomy Olympiad local Organizing committee explained its significance over Sri Lanka and the benefits possessed as the hosting country while Mr. Isuru Gunawardhana the coordinator too explained on the activities of the program. Reporters from Derana TV, Hiru TV, Siyatha TV, ITN, FOS Media as well as Leading newspaper representatives were providing the media coverage and the members of the IAO Local organizing committee along with Dr. Hiran Jayaweera and Treasurer Mr. Rushan Abeygunawardana were also present at the press conference.\nOn 18/04/2018 A special meeting was held with Minister Dr. Sarath Amunugama at the ministry of special assignments. Prof Chandana Jayaratne, Chair of IAO-LOC, Mr. HMPB Herth, Additional Secretary to the Ministry of Science, Technology and Research along with Mr. Dharmathilaka, Director, Ministry of Science Technology and Research also participated for the meeting and This event was reported by the national newspapers-Daily News, Dinamina and Thinukaran in all the three languages(English, Sinhala and Tamil Language) on 18/04/201']	['<urn:uuid:4f1247f0-4ec8-4e00-bb97-f82738fd667b>']	open-ended	with-premise	concise-and-natural	similar-to-document	single-doc	expert	2025-05-12T20:11:36.665104	10	33	932
50	afib symptoms treatment disability status	Atrial fibrillation symptoms include chest pain, shortness of breath, and dizziness, with episodes that can last up to 7 days. While there is no cure for AF, it can be managed through various treatments including medication, cardioversion, and catheter ablation. AFib can be considered a disability by the Social Security Administration, and patients can qualify for disability benefits if they can no longer work and meet the medical qualifications outlined in the SSA's Blue Book.	['Yoga May Help Those With Atrial Fibrillation\nA new research study finds that yoga is an effective intervention to improve quality of life among people who experience atrial fibrillation, an irregular, often rapid heartbeat that often causes poor blood flow.\nParoxysmal atrial fibrillation can occur at anytime without warning. Symptoms associated with atrial fibrillation include chest pain, shortness of breath, and dizziness. The fear of experiencing these complications often limit what a person chooses to do and can severely compromise quality of life.\nThe new study, published in the European Journal of Cardiovascular Nursing, discovered the practice of yoga reduced heart rate and blood pressure and improved quality of life.\nResearchers believe the benefits may stem from providing individuals a method to gain some self-control over symptoms instead of feeling helpless.\n“Many patients with paroxysmal atrial fibrillation (AF) can’t live their lives as they want to — they refuse dinners with friends, concerts, and traveling — because they are afraid of an AF episode occurring,” said Maria Wahlström, a nurse and Ph.D. candidate at the Karolinska Institute in Stockholm, Sweden.\n“AF episodes are accompanied by chest pain, dyspnoea, and dizziness,” said Wahlström. “These symptoms are unpleasant and patients feel anxious, worried, and stressed that an AF episode will occur. Most patients are still working and take sick leave to visit the hospital. Many patients with AF use complementary therapies so it is necessary to find out if they actually help.”\nAF is the most common cardiac rhythm disorder, affecting 1.5 to two percent of the general population in the developed world. There is no cure for AF, and management focuses on relief of symptoms and the prevention of complications such as stroke using cardioversion, ablation, and medication.\nPatients with paroxysmal AF experience episodes of rapid heart rates that usually last less than 48 hours and stop by themselves, although in some patients they can last up to seven days.\nThe current study included 80 patients with paroxysmal AF who were randomized to yoga or a control group that did not do yoga. Both groups received standard treatment with medication, cardioversion, and catheter ablation as needed.\nYoga was performed for one hour, once a week, for 12 weeks in the hospital with an experienced instructor. The yoga program included light movements, deep breathing, and meditation.\nQuality of life, heart rate, and blood pressure were measured in all patients at the start and end of the study. Quality of life (physical and mental health) was assessed using two validated questionnaires, the Short-Form Health Survey (SF-36) and the EuroQoL-5D (EQ-5D) Visual Analogue Scale (VAS).\nAfter 12 weeks, the yoga group had higher SF-36 mental health scores, lower heart rate, and lower systolic and diastolic blood pressure than the control group.\nWahlström said, “We found that patients who did yoga had a better quality of life, lower heart rate, and lower blood pressure than patients who did not do yoga. It could be that the deep breathing balances the parasympathetic and sympathetic nervous system, leading to less variation in heart rate. The breathing and movement may have beneficial effects on blood pressure.”\nWithin the yoga group, both the EQ-5D VAS scores and SF-36 mental health scores improved during the study, while there was no change in the control group between the initial and final measurements.\n“Yoga may improve quality of life in patients with paroxysmal AF because it gives them a method to gain some self-control over their symptoms instead of feeling helpless,” said Wahlström. “Patients in the yoga group said it felt good to let go of their thoughts and just be inside themselves for awhile.”\nNew research will include a larger study of patients with symptomatic paroxysmal AF who will be randomized to yoga, music relaxation, or a control group. This will clarify whether the movement and deep breathing in yoga are beneficial or only the relaxation.\nInvestigators say it will also address the potential for group therapy (itself beneficial) as people with the condition may feel safe and secure when they meet others with the same illness.\nSaid Wahlström, “A lot of the patients I meet who have paroxysmal AF are very stressed. Yoga should be offered as a complementary therapy to help them relax. It may also reduce their visits to hospital by lowering their anxiety until an AF episode stops.”\nNauert PhD, R. (2018). Yoga May Help Those With Atrial Fibrillation. Psych Central. Retrieved on September 21, 2020, from https://psychcentral.com/news/2016/03/15/yoga-improves-quality-of-life-for-some-cardiac-issues/100473.html', '- Should I worry about tachycardia?\n- Which is worse atrial fibrillation or ventricular tachycardia?\n- What is the life expectancy of someone with AFib?\n- Does drinking water help with AFib?\n- How do you fix atrial tachycardia?\n- What is the safest blood thinner for AFib?\n- Is AFib a disability?\n- Why does AFib happen at night?\n- What is the safest antiarrhythmic drug?\n- Can you reverse atrial fibrillation?\n- What is the safest medication for AFib?\n- Is atrial tachycardia life threatening?\n- What can you do to stop atrial fibrillation?\n- Is walking good for AFib?\n- Does AFib weaken the heart?\n- What’s the difference between V fib and V Tach?\n- Does AFib shorten your life expectancy?\n- Can stress cause atrial tachycardia?\nShould I worry about tachycardia?\nWhen to see a doctor See your doctor if you or your child has any tachycardia symptoms.\nIf you faint, have difficulty breathing or have chest pain lasting more than a few minutes, get emergency care, or call 911 or your local emergency number.\nSeek emergency care for anyone experiencing these symptoms..\nWhich is worse atrial fibrillation or ventricular tachycardia?\nIs AFib or VFib more serious and dangerous? By far, VFib is more serious. If ventricular fibrillation isn’t treated immediately, the patient will have a “sudden death” or “cardiac arrest” and die.\nWhat is the life expectancy of someone with AFib?\nSeeking treatment and maintaining regular visits with your doctor can typically improve your prognosis when you have AFib. According to the American Heart Association (AHA), 35 percent of people who don’t receive treatment for AFib go on to have a stroke. The AHA notes that an episode of AFib rarely causes death.\nDoes drinking water help with AFib?\nDrinking plenty of water throughout the day helps to maintain the fluid level of the body. There can be several other reasons for AFib like Fatigue, illness, exercise, medication.\nHow do you fix atrial tachycardia?\nHow is atrial tachycardia treated?Treatment of any underlying conditions.Catheter ablation to destroy specific patches of heart muscle that are incorrectly producing electrical signals; usually performed at the same time as an electrophysiological study.More items…\nWhat is the safest blood thinner for AFib?\nNon–vitamin K oral anticoagulants (NOACs) are now recommended as the preferred alternative to warfarin for reducing the risk of stroke associated with atrial fibrillation (AFib), according to a focused update to the 2014 American Heart Association/American College of Cardiology/Heart Rhythm Society Guideline for the …\nIs AFib a disability?\nAFib can be considered a disability from the Social Security Administration (SSA). If you have AFib and you can no longer work, AFib is considered a disability and in order to qualify you have to meet the medical qualifications for AFib outlined in the SSA’s Blue Book.\nWhy does AFib happen at night?\nA: It’s not uncommon for atrial fibrillation (AFib) to occur at night. The nerves that control your heart rate typically are in sleep mode, and that’s when your resting heart rate drops. Under these conditions, pacemaker activity from areas other than the normal pacemaker in the heart can trigger the onset of AFib.\nWhat is the safest antiarrhythmic drug?\nDronedarone was the best tolerated of the antiarrhythmic drugs, with the lowest rates of severe adverse events and a significant reduction in the risk of stroke. It is our practice to use propafenone, flecainide, sotalol, and dronedarone as first-line therapies in patients without structural heart disease (Table 4).\nCan you reverse atrial fibrillation?\nAtrial fibrillation can be caused by many things, and some of those causes are reversible, which means a patient’s symptoms can improve or stop entirely without additional heart rhythm medications or a surgical procedure.\nWhat is the safest medication for AFib?\nPotassium channel blockers, which slow the electrical signals that cause AFib:Amiodarone (Cordarone, Nexterone,Pacerone),Dofetilide (Tikosyn)Sotalol (Betapace, Sorine, Sotylize)Feb 25, 2021\nIs atrial tachycardia life threatening?\nLike most arrhythmias, atrial tachycardia is not life threatening, but it may be the sign of a more serious heart condition.\nWhat can you do to stop atrial fibrillation?\nWays to stop an A-fib episodeTake slow, deep breaths. Share on Pinterest It is believed that yoga can be beneficial to those with A-fib to relax. … Drink cold water. Slowly drinking a glass of cold water can help steady the heart rate. … Aerobic activity. … Yoga. … Biofeedback training. … Vagal maneuvers. … Exercise. … Eat a healthful diet.More items…•Dec 13, 2017\nIs walking good for AFib?\nIn fact, walking can prove quite beneficial to the health and longevity of a person living with AFib. Why? Aside from its long-term health benefits, such as lower blood pressure and resting heart rate and improved mental well-being, walking can help reduce the onset of AFib symptoms.\nDoes AFib weaken the heart?\nCardiovascular and circulatory systems Over time, AFib can cause the heart to weaken and malfunction. The heart’s ineffective contractions cause blood to pool in the atria. This can increase the risk of clotting.\nWhat’s the difference between V fib and V Tach?\nVentricular tachycardia (v-tach is treated similarly to v-fib. The difference is that ventricular tachycardia continues to make the heartbeat regularly, but it goes so fast that the heart never gets a chance to fill with blood.\nDoes AFib shorten your life expectancy?\nUntreated AFib can raise your risk for problems like a heart attack, stroke, and heart failure, which could shorten your life expectancy. But treatments and lifestyle changes can help prevent these problems and manage your risks.\nCan stress cause atrial tachycardia?\nAny of the following can increase your risk for atrial tachycardia: A heart condition, hypertension, or fatigue. Anxiety, stress, or pain. Large amounts of caffeine from coffee, tea, and energy drinks.']	['<urn:uuid:fa494279-70e6-4c54-a2e6-9a1e3a99fbd2>', '<urn:uuid:109c82e8-d337-4b2b-99cc-4bf87bb79903>']	open-ended	direct	short-search-query	similar-to-document	multi-aspect	expert	2025-05-12T20:11:36.665104	5	75	1698
51	calendar spread options trading definition	A calendar spread involves buying two options with the same strike price but different expiration dates to take advantage of the increased rate of time decay on the shorter dated option. For example, this could involve going long a June dollar/yen call and short a dollar/yen February call option.	"['American Style Currency Option - An option that may be exercised at any time up to and including the day of expiration.\nAt-the-money (ATM) Option - Used to describe an option whose strike price is roughly equal to the current market price of the option.\nBoston Option - An option whereby the premium is payable on maturity and not up front as is usually the case. The LIFFE exchange uses this approach on many of their contracts.\nButterfly Spread - A strategy to occupy both sides of the market using spreads. It involves going long a bull spread and long a bear spread.\nCalendar Spread - This involves buying two options with the same strike price but different expiration dates to take advantage of the increased rate of time decay on the shorter dated option. Example, long a june $/yen call and short a $/yen february call option.\nCollar - This is a strategy commonly used in hedging. It involves purchasing a put and selling an otm call to pay for the put option purchased. Of course, the reverse can be constructed as well with the purchase of a call and sale of a put option.\nDelta - Is a measure that indicates the change of an option price relative to a change in the currency price. A delta of .5 would indicate that the long option holder is long the equivalent of 1/2 of a futures currency contract.\nDelta Neutral Spread - Refers to a market position constructed with options that result in a neutral position, without bias. By matching the delta of a put option with the delta of a call option, your net delta should equate to zero. Thus, you would be unaffected by small price movements. Of course this position would have to be adjusted if the market would move significantly in any one position. Some market players sell both calls and puts equally and try to achieve a zero delta and thus try to capitalise on time decay and/or lower implied volatility.\nEuropean Style Currency Option - An option that may only be exercised on the expiry date, with settlement usually two days later, in keeping with the spot settlement cycle.\nGamma - A term used to denote the measure of change of the delta of an option.\nImplied Volatility - Is a measure of an option volatility. The volatility of an option reflects the market expectation of a possible future outcome. An analogy may be the fixed ""odds"" for a football match which suggests what the public feels the outcome may be. Implied volatility is also significantly influenced by the market maker.\nIn-the-money (ITM) Call - A call option with a strike price lower than the present market value of the currency. Thus, you would be able to purchase the currency for a price lower than the market value.\nIn-the-money (ITM) Put - A put option with a strike price higher than the present market value of the currency. Thus, you would be able to sell the currency for a price higher than the market value.\nIntrinsic Value - This refers to the difference between an in-the-money call/put and the strike price.\nLong Straddle - An option market position that consists of purchasing equal units of calls and puts with the same strike price and expiration date.\nLong Strangle - This is similar to a long straddle, except the strike price of the call and put would be different.\nLong Volatility - A strategy whereby one tries to capitalize on an increase in option implied volatility. The market position is ideally entered when option volatilities are at historical lows. One attempts to purchase those options that are most sensitive to an increase in implied volatility. Thus, long expiration dates are most sought after. An equal number of puts and calls are purchased which also roughly equates to delta neutrality. Of course, if one had a strong bias, bullish or bearish, then different strike prices would be chosen to reflect the anticipated price action.\nOut-of-the-money (OTM) Call - The reverse of ITM call, being when a call option strike price is higher than the current market value of the currency price.\nOut-of-the-money (OTM) Put - An put option whose strike price is lower than the current value of the currency price.\nPremium - This refers to the payment or purchase price of an option. Premium is affected by volatility, interest rates, strike price, and expiration date. The premium can be quoted in a number of ways. In exchange contracts, they are usually quoted in points of currency, whereas in the otc market, vanilla options are usually quoted in percentage of currency and/or volatility terms.\nRatio Spread - This refers to an option combination where one holds a different amount of units of long options than short options. It is sometimes used as a hedge strategy. Example, you\'re long call options or underlying asset and the market begins to drop, you could sell two or more call options for each call option you own. In the case of being long the underlying, you could sell as many call options as necessary to achieve a negative delta.\nShort Straddle - This is simply the opposite of a long straddle. Instead of buying an equal number of puts and calls, you would sell an equal quantity of both calls and puts with the same strike and expiration date.\nShort Strangle - A market position which is constructed of a short call and short put in equal amounts with the same expiration date, but with different strike prices.\nSynthetic Call Option - A position constructed by going long the underlying currency and long a put option. The risk/reward profile resembles that of a plain call option, thus the term ""synthetic"".\nSynthetic Put Option - A position constructed with a short underlying currency and a long call option. Here too, the risk/reward profile is almost identical to a long put option.\nTheta - The option sensitivity which refers to the time decay of options. Options lose value very slowly up until approximately 40 days or so, when the option begins to deteriorate at an increasing rate.\nVega - The sensitivity that refers to the volatility of an option. In general, the more time remaining until expiration, the more sensitive is the option to a change in underlying volatility.\nVertical Bear Spread - A market position that consists of long a put option and short another put option that is further out of the money, in the same month. This position results in a debit to your account and has a maximum loss and maximum profit as a profile.\nVertical Bull Spread - A market position similar to a bear spread, only that it is constructed with calls. Example, long call and short a call further out of the money in the same month. Risk is limited and profit is limited as well to the difference between the two strike prices.']"	['<urn:uuid:2aaa3c20-29bf-4532-b336-5593c7c6f52f>']	open-ended	direct	short-search-query	similar-to-document	single-doc	expert	2025-05-12T20:11:36.665104	5	49	1160
52	Are Tokyo and Jakarta similar in population growth patterns and administrative structure?	While both are major metropolitan areas, they have distinct characteristics. Jakarta grew from 11.91 million in 1980 to approximately 34 million by 2018, with suburban areas accounting for 84% of growth between 2000-2010. Tokyo's administrative structure is unique as a merged city-prefecture (called 'to') with special wards, established in 1943 when Tokyo City was abolished and Tokyo-fu became Tokyo-to. Today, Tokyo's special wards have almost the same independence as Japanese cities, making its administration only slightly different from other prefectures.	"['The Megacity of Jakarta increased from 11.91 million in 1980, 17.14 million in 1990, and 20.63 million in 2000 to 28.01 million in 2010. The megacity in 2010 was 11.79 percent of Indonesia’s total population but this population resides in less than 0.3 percent of Indonesia’s total area.\nWhat year did Jakarta become a megacity?\nThe Jakarta megacity. Like other megacities in Southeast Asia (Spreitzhofer, 2005), the Jakarta megacity has also grown rapidly. In 1980, the total population in the Greater Jakarta area was 11.4 million, and by 2018, it had increased to 34 million, with 10 million in the city of Jakarta.\nHow did Jakarta Indonesia become a megacity?\nThe population of Jakarta has increased dramatically since 1940. Much of that increase is attributed to immigration, which has transformed Jakarta into one of the world’s largest urban agglomerations.\nIs Jakarta a megacity?\nJakarta and its metro area (Jabodetabek), with more than 30 million people, is the second largest megacity in the world in 2020. The suburban areas seem to be where much of the population growth is happening, making up about 84% of the total population growth in the metropolitan area between 2000 and 2010.\nWhat was the population of Jakarta in 1950?\nJakarta, Indonesia Metro Area Population 1950-2021\n|Jakarta – Historical Population Data|\nIs Indonesia a megacity?\nCities of more than eight million people are known as megacities and most are in poorer or developing countries. By 2015, there will be 33 megacities – 27 of them in developing countries. Jakarta, the capital of Indonesia, is one of these megacities.\nWhat is the biggest megacity?\nTokyo (Japan) is currently the largest ‘megacity’ in the world with 37.4 million inhabitants. In 2100 it will be Lagos (Nigeria) with 88 million.\nWhat is the size of Jakarta?\nThe population of Sydney is 4.5 million. It is not yet a ‘megacity’. … The rate of economic growth in Sydney is very low. Over 150 cities in the world are growing at a faster rate.\nWhat is the old name of Indonesia?\nFormal Name: Republic of Indonesia (Republik Indonesia; the word Indonesia was coined from the Greek indos—for India—and nesos—for island). Short Form: Indonesia. Former Names: Netherlands East Indies; Dutch East Indies.\nWill Jakarta stop sinking?\nAs Indonesia’s capital and most populous megacity, Jakarta needs rapid solutions to tackle the problems of land subsidence and sea-level rise. A recent study by the National Research and Innovation Agency (BRIN) stated that, without aggressive effort, around 25 percent of the capital area will be submerged in 2050.\nWhat type of city is Jakarta?\nJakarta is the dynamic capital city of the Republic of Indonesia, a country composed of more than 17,000 islands with a population of over 210 million.\nIs Jakarta a big city?\nThe 1990 Population Census showed that Jakarta had 8.2 million inhabitants. The initial response to the accelerated growth of Jakarta was to expand the boundaries of Jakarta in order to accommodate the growth. The expansion took over some of the areas of the Bogor, Tangerang, and Bekasi regencies.\nWhat was the population of Jakarta in 2010?\nJakarta has an estimated population of over 10 million people in 2016, up from 9,607,787 recorded during the 2010 Census.\nWhat is the population of Indonesia 2021?\nThe current population of Indonesia is 277,646,675 as of Monday, December 6, 2021, based on Worldometer elaboration of the latest United Nations data. Indonesia 2020 population is estimated at 273,523,615 people at mid year according to UN data. Indonesia population is equivalent to 3.51% of the total world population.', 'Prefectures of Japan\n|Populations||5,626,722 (Tottori) – 12,059,237 (Tōkyō)|\n|Areas||718.81 square miles (1,861.7 km2) (Kagawa) – 32,221.60 square miles (83,453.6 km2) (Hokkaido)|\n|Government||Prefecture Government, Central Government|\nJapan\'s 47 prefectures form the country\'s first jurisdiction and administrative division levels. They consist of 43 prefectures (県 ken?) proper, two urban prefectures (府 fu ?, Osaka, and Kyoto), one ""circuit"" or ""territory"" (道 dō ?, Hokkaido) and one ""metropolis"" (都 to ?, Tokyo). The Meiji Fuhanken sanchisei administration created the first prefectures to replace the provinces of Japan in 1868.\nUnder the current Local Autonomy Law, each prefecture is subdivided into cities (市 shi?) and districts (郡 gun?) and each district into towns (町 chō/machi?) and villages (村 son/mura?). For example, Hokkaido has 14 subprefectures that act as branch offices (支庁 shichō?) of the prefecture. Some other prefectures also have branch offices that carry out prefectural administrative functions outside the capital. Tokyo, the capital, is a merged city-prefecture; it has features of both cities and prefectures.\nThe West\'s use of ""prefecture"" to label these Japanese regions stems from 15th-century Portuguese explorers\' and traders\' use of ""prefeitura"" to describe the fiefdoms they encountered there. Its original sense in Portuguese, however, was closer to ""municipality"" than ""province"". (Today, in turn, Japan uses its word ken (県), meaning ""prefecture"", to identify Portuguese districts while in Brazil the word ""Prefeitura"" is used to refer to a City Hall.)\nThose fiefs were headed by a local warlord or family. Though the fiefs have long since been dismantled, merged, and reorganized multiple times, and been granted legislative governance and oversight, the rough translation stuck.\nThe Meiji government established the current system in July 1871 with the abolition of the han system and establishment of the prefecture system (廃藩置県 haihan-chiken). Although there were initially over 300 prefectures, many of them being former han territories, this number was reduced to 72 in the latter part of 1871, and 47 in 1888. The Local Autonomy Law of 1947 gave more political power to prefectures, and installed prefectural governors and parliaments.\nIn 2003, then-Prime Minister Junichiro Koizumi proposed that the government consolidate the current prefectures into about 10 regional states. The plan called for each region to have greater autonomy than existing prefectures. This process would reduce the number of sub-prefecture administrative regions and cut administrative costs. The Japanese government is also considering a plan to merge several groups of prefectures, creating a sub-national administrative division system consisting of between nine and 13 states, and giving these states more local autonomy than the prefectures currently enjoy. As of August 2012[update], no reorganization has been scheduled.\nJapan is a unitary state. The central government delegates many functions (such as education and the police force) to the prefectures and municipalities, but retains the overall right to control them. Although local government expenditure accounts for 70 percent of overall government expenditure, the central government controls local budgets, tax rates, and borrowing. Fiscal transfers, directed by the central government, account for around one-third of local government revenue.\nTypes of prefecture\nHistorically, during the Edo period, the Tokugawa shogunate established bugyō-ruled zones (奉行支配地) around the nine largest cities in Japan, and 302 township-ruled zones (郡代支配地) elsewhere. When the Meiji government began to create the prefectural system in 1868, the nine bugyō-ruled zones became fu (府), while the township-ruled zones and the rest of the bugyo-ruled zones became ken (県). Later, in 1871, the government designated Tokyo, Osaka, and Kyoto as fu, and relegated the other fu to the status of ken. During World War II, in 1943, Tokyo became a to, a new type of pseudo-prefecture.\nDespite the differences in terminology, there is little functional difference between the four types of local governments. The sub-national governments are sometimes collectively referred to as to-dō-fu-ken (都道府県?) in Japanese, which is a simple combination of the four terms.\nTokyo is referred to as to (都), which is often translated as ""metropolis."" The Japanese government translates Tōkyō-to as ""Tokyo Metropolis"" in almost all cases, and the government is officially called the ""Tokyo Metropolitan Government"". But there are some people who call Tōkyō-to ""Tokyo Prefecture"" in English.\nFollowing the abolition of the han system, Tōkyō-fu (an urban prefecture like Kyoto and Osaka) encompassed a number of cities, the largest of which was Tokyo City. Tokyo City was divided into 15 wards. In 1943, Tokyo City was abolished, Tōkyō-fu became Tōkyō-to, and Tokyo\'s wards became the special wards, local authorities falling directly under the prefecture in hierarchy, each with their own elected assemblies (kugikai) and mayors (kuchō). A number of suburban villages and towns were converted to wards, bringing the total number of special wards to 35. The reorganization\'s aim was to consolidate the administration of the area around the capital by eliminating the extra level of authority in Tokyo. The central government wanted to have greater control over Tokyo due to Japan\'s deteriorating position in World War II and the possibility of emergency in the metropolis.\nAfter the war, Japan was forced to decentralize Tokyo again, following the general terms of democratization outlined in the Potsdam Declaration. Many of Tokyo\'s special governmental characteristics disappeared during this time, and the wards took on an increasingly municipal status in the decades following the surrender. Administratively, today\'s special wards are almost indistinguishable from other municipalities.\nThe postwar reforms also changed the map of Tokyo significantly: In 1947, the 35 wards were reorganized into the 23 special wards, because many of its citizens had either died during the war, left the city, or been drafted and didn\'t return.\nThere are some differences in terminology between Tokyo and other prefectures: police and fire departments are called chō (庁) instead of honbu (本部), for instance. But the only functional difference between Tōkyō-to and other prefectures is that Tokyo administers wards as well as cities. Today, since the special wards have almost the same degree of independence as Japanese cities, the difference in administration between Tokyo and other prefectures is fairly minor.\nIn Osaka, several prominent politicians led by Tōru Hashimoto, mayor of Osaka City and former governor of Osaka Prefecture, are currently proposing an Osaka Metropolis plan, under which Osaka City, and possibly other neighboring cities, would be replaced by special wards similar to Tokyo\'s.\nHokkaido is referred to as a dō (道) or circuit. This term was originally used to refer to Japanese regions consisting of several provinces (e.g. the Tōkaidō east-coast region, and Saikaidō west-coast region). This was also a historical usage of the character in China. (In Korea this historical usage is still used today and was kept during the period of Japanese rule.)\nHokkaido, the only remaining dō today, was not one of the original seven dō (it was known as Ezo in the pre-modern era). Its current name is believed to originate from Matsuura Takeshiro, an early Japanese explorer of the island. Since Hokkaido did not fit into the existing dō classifications, a new dō was created to cover it.\nThe Meiji government originally classified Hokkaido as a ""Settlement Envoyship"" (開拓使 kaitakushi), and later divided the island into three prefectures (Sapporo, Hakodate, and Nemuro). These were consolidated into a single Hokkaido Department (北海道庁 Hokkaido-chō) in 1886, at prefectural level but organized more along the lines of a territory. In 1947, the department was dissolved, and Hokkaido became a full-fledged prefecture. The -ken suffix was never added to its name, so the -dō suffix came to be understood to mean ""prefecture.""\nWhen Hokkaido was incorporated, transportation on the island was still underdeveloped, so the prefecture was split into several ""sub-prefectures"" (支庁 shichō) that could fulfill administrative duties of the prefectural government and keep tight control over the developing island. These sub-prefectures still exist today, although they have much less power than they possessed before and during World War II: they now exist primarily to handle paperwork and other bureaucratic functions.\n""Hokkaido Prefecture"" is, technically speaking, a redundant term, although it is occasionally used to differentiate the government from the island itself. The prefecture\'s government calls itself the ""Hokkaido Government"" rather than the ""Hokkaido Prefectural Government"".\nOsaka and Kyoto Prefectures are referred to as fu (府). The Classical Chinese character from which this is derived implies a core urban zone of national importance. Before World War II, different laws applied to fu and ken, but this distinction was abolished after the war, and the two types of prefecture are now functionally the same.\n43 of the 47 prefectures are referred to as ken (県). The Classical Chinese character from which this is derived carries a rural or provincial connotation, and an analogous character is used to refer to the counties of China and counties of Taiwan and districts of Vietnam.\nLists of prefectures\nBy Japanese ISO\nThe prefectures are also often grouped into eight regions (Chihō). Those regions are not formally specified, they do not have elected officials, nor are they corporate bodies. But the practice of ordering prefectures based on their geographic region is traditional. This ordering is mirrored in Japan\'s International Organization for Standardization (ISO) coding. From north to south (numbering in ISO 3166-2:JP order), the prefectures of Japan and their commonly associated regions are:\nBy English name\n- The default alphabetic order in this sortable table can be altered to mirror the traditional Japanese regions and ISO parsing.\nNotes: ¹ as of 2000; ² km²; ³ per km²\nTerritories lost after World War II\nNote: Due to the division of Korea, Kōgen (Kangwon/Gangwon) and Keiki (Gyeonggi) are divided between North Korea and South Korea. While both Koreas each has its own Kangwon/Gangwon Province, the North Korean portion of Gyeonggi has been absorbed into other provinces.\n- List of Japanese prefectures by population\n- List of Japanese prefectures by GDP\n- List of Japanese prefectures ranked by area\n- List of prefectural capitals in Japan\n- List of Prefecture songs of Japan\n- ISO 3166-2 codes for Japan\n- List of governors of Japan\n- Nussbaum, Louis-Frédéric, 2002: ""Provinces and prefectures"" in Japan encyclopedia, p. 780.\n- Mabuchi, Masaru, ""Municipal Amalgamation in Japan"", World Bank, 2001.\n- ""Doshusei Regional System"" National Association for Research Advancement.\n- Mochida, ""Local Government Organization and Finance: Japan"", in Shah, Anwar (2006). Local Governance in Industrial Countries. World Bank.\n- See ISO 3166\n- 都庁の所在地 Shinjuku is the location of the Tokyo Metropolitan Government Office.But Tokyo is not a ""municipality"".Therefore, for the sake of convenience, the notation of prefectural is ""Tokyo"".\n|Wikimedia Commons has media related to Prefectures of Japan.|\n- (English) National Governors\' Association Website\n- Map of Japan showing prefectures\n- Japanese essay on types of prefectures\n- (English) Local Authorities for International Relations\n- CityMayors.com profile of prefectures']"	['<urn:uuid:99d8a3ad-0993-4f7b-a9ba-a72723bc8d09>', '<urn:uuid:5793b49f-7590-4941-90cd-2de52049ad89>']	open-ended	direct	concise-and-natural	distant-from-document	comparison	expert	2025-05-12T20:11:36.665104	12	80	2363
53	need info what country is vladivostok in	Vladivostok is located in Russia, as was evident in the case of a Russian businessman who departed from Vladivostok to seek protection elsewhere.	['If someone were to ask me to nominate the most frequently abused statement of principle in Australian legal history, I would refer them to paragraphs 24 and 25 of the judgment of Gummow and Callinan JJ in Dranichnikov v Minister for Immigration and Multicultural Affairs. In those paragraphs, their Honours stated that:\nTo fail to respond to a substantial, clearly articulated argument relying upon established facts was at least to fail to accord Mr Dranichnikov natural justice … The question remains however whether what occurred, either characterised as a failure to accord natural justice or as that, and more, which we consider it to be, including a constructive failure to exercise jurisdiction, entitles Mr Dranichnikov to relief under s 75(v) of the Constitution.\nThere is nothing wrong with what Gummow and Callinan JJ said in Dranichnikov. Clearly, if a decision maker is obliged to afford a person an opportunity to be heard before it makes a decision, it will fail to comply with that obligation if it ignores what the person says about why particular facts are relevant to whether the decision should or should not be made. More fundamentally, it seems obvious that a decision maker cannot properly exercise jurisdiction without considering what a person whose interests are liable to be affected by its decision says about the significance of any factual conclusions the decision maker has reached.\nHowever, in the 18 years since Dranichnikov was decided, the obvious proposition that a decision maker will constructively fail to exercise jurisdiction if it fails to engage with ‘a substantial, clearly articulated argument relying upon established facts’ has (at least in the minds of a succession of federal Ministers and their legal representatives) somehow been transformed into the wholly erroneous proposition that this is the only way in which a decision maker can constructively fail to exercise jurisdiction. While this mistaken proposition has occasionally been corrected by the Federal Court, it nevertheless continues to shamble through federal judicial review proceedings like a jurisprudential zombie. But in recent years, State appellate courts have explained the true nature of jurisdictional error in the form of constructive failure to exercise jurisdiction, including why that form of error is not limited to cases in which a decision maker ignores ‘established facts’. The recent judgment of the New South Wales Court of Appeal in Day v SAS Trustee Corporation, handed down on 28 April 2021, is a useful addition to this body of State administrative law jurisprudence. But before turning to the judgment in Day, it is useful to consider what Gummow and Callinan JJ actually said (and did not say) in Drachnikov itself and to look at the Victorian Court of Appeal’s comprehensive explanation in Chang v Neill of the principles underlying judicial review for constructive failure to exercise jurisdiction.\nThe reasoning of Gummow and Callinan JJ in Dranichnikov\nIn revisiting Dranichnikov, it is important to recall what the case was actually about. In 1997, the applicant, his wife and their daughter departed Vladivostok, Russia, where the applicant had operated a small business. The applicant claimed that from about 1993, a culture of lawlessness and corruption had taken hold in Vladivostok. He claimed that he had made public efforts to raise this issue with the local authorities, and that this had resulted in his being stabbed and severely injured. The applicant stated that he had been pressured by police not to take any further action in respect of this assault, to the point where he had signed a letter requesting that police discontinue any investigation into the circumstances of the assault. Based on these matters, the applicant submitted that he was entitled to a protection visa on the grounds that if he was required to return to Russia, there was a real chance that he would persecuted because of his membership of a particular social group.\nThe Tribunal accepted all of the primary allegations of fact made by the applicant. However, it refused to grant the applicant a protection visa on the grounds that it did not accept that there was evidence before it of ‘general persecution of businessmen in Russia’. The problem with the Tribunal’s line of reasoning was that this finding did not address the case that was actually put to it by the applicant. That case was not that ‘businessmen in Russia’ were at risk of persecution. It was that a particular group of businessmen – being those businessmen who had publicly criticised Russia’s law enforcement apparatus – were at risk of persecution. Thus Gummow and Callinan JJ stated that:\nThe Tribunal … decided another question, whether Mr Dranichnikov’s membership of a social group, namely, of “businessmen in Russia” was a reason for his persecution and relevantly nothing more. The Tribunal should have decided the matter which was put to it, whether Mr Dranichnikov was a member of a social group consisting of entrepreneurs and businessmen who publicly criticised law enforcement authorities for failing to take action against crime or criminals.\nIt is in this context that their Honours’ reference to ‘established facts’ has to be understood. What Gummow and Callinan JJ said in their judgment is that where a decision maker accepts the factual allegations made by a person, but misapprehends why the person says those facts should result in the making of a particular decision, it will constructively fail to exercise its jurisdiction (and fail to comply with the hearing rule). Their Honours emphatically did not say that a decision maker will only constructively fail to exercise jurisdiction if it fails to respond to ‘established facts’. Such a principle would be absurd. In this regard, the role of an administrative decision maker will generally require it to make findings on factual allegations put to it. It is only once those findings have been made that any facts (other than truly obvious facts – such as that Vladivostok is in Russia, or that Russia was once part of the Union of Soviet Socialist Republics) can be said to have been ‘established’. To say that a decision maker is only required to respond to a claim based on ‘established facts’ is to put the cart before the horse – it is to say that a decision maker’s obligation to consider the case before it somehow arises after it has made findings of fact, notwithstanding that making those findings is a key step in discharging its jurisdictional obligation to consider the case. Plainly that cannot be correct. The judgment of Gummow and Callinan JJ in Dranichnikov thus merely describes one type of situation in which a decision maker may fall into jurisdictional error – it does not purport to delimit the circumstances in which a decision maker may constructively fail to exercise jurisdiction.\nAnd yet, in federal judicial review proceedings, Dranichnikov is almost inevitably cited by the Minister administering the Migration Act 1958 (Cth) in support of the manifestly wrong proposition that a decision maker can only constructively fail to exercise jurisdiction by failing to respond to a claim based on established facts. At least in my view, this misunderstanding is less a good faith disagreement about the meaning of Gummow and Callinan JJ’s judgment than it is an unfortunate example of Upton Sinclair’s dictum that ‘it is difficult to get a man to understand something, when his salary depends upon his not understanding it.’\nThe judgment in Chang v Neill\nIn Chang v Neill, the Victorian Court of Appeal reviewed a substantial number of Federal Court authorities addressing the question when an administrative decision maker will fall into jurisdictional error by misapprehending or failing to consider some aspect of a case put to it a by a person whose interests are liable to be affected by a decision (including the seminal judgments in Minister for Immigration and Citizenship v SZRKT and Minister for Immigration and Border Protection v MZYTS). Based on this review of the authorities, the Court stated that:\n[A] factual error may constitute jurisdictional error if it amounts to a constructive failure to perform the statutory function conferred on the decision-maker … Factual errors that may constitute jurisdictional error include a failure by the decision-maker to have regard to relevant factual material and the taking into account of such material in a manner that misconstrues its nature or effect (the latter may be described as a constructive failure to have regard to the material). Whether such a factual error amounts to a constructive failure to perform the statutory function conferred on the decision-maker will depend on the importance of the material to the exercise of the function and the seriousness of the error.\nAs the Court emphasised, failing to address or properly understand some aspect of the factual substratum of a case – whether that aspect of the case is characterised as a ‘claim’, a ‘submission’, or a matter of ‘evidence’ – will give rise to jurisdictional error if it indicates that the decision maker has not truly engaged with the case in a meaningful way. This is a qualitative failure that can arise in a range of different circumstances. The failure need not relate to any ‘established facts’ – it can just as easily involve overlooking evidentiary material that is important to the resolution of a controversial question of fact, or failing to grapple with and reach conclusions on a factual issue that must be determined in order to provide a proper factual foundation for the decision.\nThe judgment in Day v SAS Trustee Corporation\nIn Day, the appellant appealed on a question of law from a judgment of the District Court holding that he was not entitled to a superannuation allowance under the Police Regulation (Superannuation) Act 1906 (NSW). In this regard, the District Court held that while the appellant had sustained some form of psychiatric injury during the course of his work as a police prosecutor in or about August of 1998, and that he had left the police force because of this condition, that injury had ceased to cause the appellant any infirmity after a short period. One of the appellant’s grounds of appeal was that in making this finding, the District Court had fallen into jurisdictional error by failing to consider three ‘key issues’, being: (1) the appellant’s submission that he had continued working throughout the duration of his psychiatric illness because of his ‘stoicism’; (2) the appellant’s claim that while his psychiatric illness did not prevent him from working, it prevented him from working around police officers; and (3) the appellant’s submission that while his condition had not prevented him from becoming a lawyer and working in private practice after he left the police force, it had left him unable to do the work of a police prosecutor.\nIn addressing this aspect of the appellant’s appeal, Meagher JA (with whom Payne and White JJA agreed) stated the relevant principles as follows:\n[A] constructive failure to exercise jurisdiction (or a purported exercise, in the sense that there is an appearance of an exercise of jurisdiction) as alleged by the appellant is not a mere failure to consider evidence or to address an argument or submission, which may be contingent or otherwise insignificant, but a failure to understand and determine a case or claim. The ultimate question is whether a failure to consider and address certain issues or arguments involved a failure to address central or critical elements of the case or claim.\nLike the judgment of the Court of Appeal in Chang, this formulation of the relevant principles emphasises that whether there has been a constructive failure to exercise jurisdiction depends on a qualitative characterisation of the material that is alleged to have been overlooked or misapprehended, and upon an assessment of whether that material was of such significance that the decision maker could not fully engage with the case put before it without considering and properly understanding it.\nHaving set out the relevant principles in such a clear and concise manner, Meagher JA proceeded to say in relation to the first key issue identified by the appellant that ‘[a] fundamental difficulty for the appellant in relation to this issue is that its factual basis, that the appellant was a “stoic”, is not established: cf Dranichnikov at .’ One might conclude from this that Meagher JA, having rightly acknowledged that constructive failure to exercise jurisdiction can arise in a range of different ways, went on to assess the appellant’s case on the basis that he could only succeed on this issue by establishing that the District Court had failed to respond to ‘established facts’. However, this remark has to be read in light of the fact that the appellant expressly invoked the ‘established facts’ aspect of Gummow and Callinan JJ’s judgment in Dranichnikov. As such, it should be read as reflective of the submissions that were made by the appellant, rather than as of a misapprehension on the part of the Court of Appeal that Dranichnikov limits the circumstances in a which a constructive failure to exercise jurisdiction may arise. In this regard, Meagher JA noted that the District Court had in fact given extensive consideration to the evidence concerning the appellant’s psychiatric history and had concluded from this history that the appellant had not suffered any real infirmity after about November of 1998. In these circumstances, his Honour considered that the appellant’s claimed stoicism ‘never arose for separate determination’ – it was simply a part of the fundamental factual question raised by the appellant’s case, which was whether he had suffered any significant psychiatric infirmity. The District Court had engaged extensively with the evidence on that issue and thus had not constructively failed to exercise jurisdiction.\nSimilarly, Meagher JA considered that the District Court had engaged with each of the the second and third key issues raised by the appellant, in that it had considered and rejected the appellant’s evidence that after he left the police force, he had developed a phobia of police that would have rendered him unable to work as a police prosecutor. In this regard, the District Court found that the appellant had worked as a defence lawyer for four years after leaving the police force, had been appointed as a magistrate, and had returned to private practice in 2003 – during the entirety of this period, the appellant had engaged with police officers with no apparent sign of psychiatric illness. In light of these findings, it could not be said that the District Court had not engaged with the second and third key issues.\nLike Chang, Day is a useful distillation by a State appellate court of a potentially confusing group of authorities on constructive failure to exercise jurisdiction. It emphasises that evaluating a claim of constructive failure to exercise jurisdiction requires a court engaged in judicial review (or hearing an appeal on a question of law) to consider the significance of any material – whether that material is characterised as a claim, a submission or an item of evidence – that is alleged to have been overlooked or mischaracterised by the decision maker. This in turn requires the court to make both a factual judgment as to whether the material was overlooked or mischaracterised and an evaluative judgment as to whether the decision maker could genuinely engage with the case without properly understanding the material. The judgment is a qualitative one that depends on the significance of the material in the context of the issues arising before the decision maker – it cannot be made by reference to fixed classes of case, and the scope of judicial review for constructive failure to exercise jurisdiction is not limited to cases in which the decision maker has failed to respond to ‘established facts’.']	['<urn:uuid:6c94a3af-aea7-4635-9e86-a611112dd50a>']	factoid	with-premise	short-search-query	distant-from-document	single-doc	novice	2025-05-12T20:11:36.665104	7	23	2613
54	What are the main differences between how secular humanist sociologists and social control theorists view their role in studying and influencing society?	Secular humanist sociologists see themselves as catalysts for social improvement, actively working toward the betterment of society through promoting peace, equality and social justice. Their approach is inherently political and action-oriented. In contrast, social control theorists take a more analytical approach, focusing on understanding how social bonds and connections prevent deviant behavior. They examine how attachments, commitments, involvements and beliefs tie individuals to conventional society, but don't necessarily advocate for specific social changes. The key distinction is that humanist sociology deliberately aims to transform society, while social control theory aims to explain existing social mechanisms.	"['4 edition of On humanist sociology found in the catalog.\nOn humanist sociology\n|Statement||editedand with an introduction by Robert Bierstedt.|\n|Series||The heritage of sociology|\n|The Physical Object|\n|Number of Pages||309|\n4. A Humanist Civilization Appendix HUMANIST MANIFESTO I, HUMANIST MANIFESTO II, Reference Notes Selected Bibliography Index Note: In this Eighth Edition of The Philosophy of Human-ism, the terms. B. C. (Before Christ) and. A. D. (Anno Domini) have been changed to. BCE (Before the Common Era) and. CE.\nPower plant siting, its impact on system planning and the environment\nEducation statistics from DES.\n[Port of Bristol Authority]\nAll My Dangerous Friends\n20th International Conference on VLSI Design\nschizaeaceae of the south of England in early tertiary times.\n100 programs for the BBC microcomputer\nBody and soul\nCharles Dickens Great expectations\nThe Association for Humanist Sociology is a community of sociologists, educators, scholars, and activists who share a commitment to using sociology to promote peace, equality, and social justice. AHS was founded in in response to a growing disenchantment with mainstream sociological organizations and a belief that sociological practice.\nBook your room now for the Association for Humanist Sociology Annual Meeting. Event Summary: Association for Humanist Sociology Start Date: Tuesday, November 3, End Date: Sunday, November 8, Last Day to Book: Sunday, Octo Jackson Downtown Convention Center Hotel for $96 USD per night.\n56 rows This list of sociology awards is an index to articles about of notable awards given to persons. (shelved 1 time as humanistic-psychology) avg rating — 1, ratings — published Want to.\nBooks shelved as humanism: The God Argument: The Case against Religion and for Humanism by A.C. Grayling, Enlightenment Now: The Case for Reason, Science. When it comes to humanist sociology, secular humanist sociologists adopt as their goal the betterment of society. In this sense, they work not as mere observers, but as catalysts.\nPatricia Hill Collins believes that “the discipline of sociology thus is highly political.” 2 The concern of science is adding to our present knowledge, not. Find many great new & used options and get the best deals for Humanist Realism for Sociologists by Terry Leahy Paperback Book at the best online prices at eBay.\nFree shipping for many products. Association for Humanist Sociology. 1, likes. Founded inAHS was founded on the principle that sociological research can be used to serve humanity.5/5(1). Humanity & Society, the official journal of the Association for Humanist Sociology, was first published in and has been published quarterly since Humanity & Society is a peer-reviewed Sage journal with abstracts of published articles appearing in Sociological Abstracts.\nIt features “humanist sociology,” which is broadly defined as a sociology that views people not only as. Co-editor (with Martin Schwartz), The Humanist Sociology Resource Book, 4th ed., American Sociological Association, August, Response to Jonathan Imber, “Values, Politics, and Science: The Influence of Social Movements on Sociology: Other-Directed Rebels” (with response from Imber), Contemporary Sociology, Vol.\n28, No. 6 (May, by Nathan G. Alexander Review by David Chivers • 28 April BY NATHAN G. ALEXANDER NYU PRESS, Nathan G. Alexander’s Race in a Godless World: Atheism, Race, and Civilization, – is a better book than its title, however accurate, might indicate.\nThe book is notable for its criticism of Marx, particularly of his account of revolution. (), all of which further developed his distinctive Freudo-Marxian inspired humanist sociology. Looking back on Fromm’s legacy today, at a point where sociologists and Marxists are increasingly returning to his work, it is clear that what Fromm.\nPages can include limited notes and highlighting, and the copy can include previous owner inscriptions. An ex-library book and may have standard library stamps and/or stickers. At ThriftBooks, our motto is: Read More, Spend Less.\nToward Humanist Sociology by Alfred McClung Lee A copy that has been read, but remains in clean condition. Lee, Toward Huamnist Sociology, Prentice-Hall, ). Rather, I provide an autobiographically-generated list of what I have learned to value in sociology—and observe that it is among those who have banded together under the rubric of “humanist sociology” that I have—on average—found the greatest acceptance and support for what I value.\nTable of Contents. Chapter 1 Introduction: Sociology, Humanist and Scientific Chapter 2 Sociology and Choice Chapter 3 The Social Construction of Life-Cycle Crises Chapter 4 Overcoming Cultural Impediments to Human Survival Chapter 5 A Humanistic Perspective on Science and Society Chapter 6 Toward a Grounded Theory of Humanist Organization Chapter 7 Community and Social Author: Walda Katz Fishman.\nThis page book is written in the same format as the Bible with the modern numbering of sections of the bible. While the title listed in the add is ""The Good Book: A Secular Bible,"" The title appearing on my copies are, ""The Good Book: A Humanist Bible.""/5(63). Toward humanist sociology By Alfred McClung Lee Toward humanist sociology By Alfred McClung Lee A comprehensive introduction to the issues and problems of our.\nOn October 11 Shannon Elizabeth Bell accepted the Association for Humanist Sociology book award for at the AHS conference in Cleveland, Ohio.\nBell’s Our Roots Run Deep as Ironweed was selected as a co-winner for the award out of more than 60 book submissions, based on four criteria: theoretical foundations in social science, systematic and innovative research methodologies, critical.\nMichelle Jacob is dedicated to teach and research in ways that empower communities by working towards social justice. In all efforts, she seeks to understand how indigenous peoples can be empowered to heal from wounds inflicted by colonialism.\nAssociation for Humanist Sociology Book Award, Association for Humanist Sociology. Nominee Location: University of Oregon, Eugene, COVID Resources. Reliable information about the coronavirus (COVID) is available from the World Health Organization (current situation, international travel).Numerous and frequently-updated resource results are available from this ’s WebJunction has pulled together information and resources to assist library staff as they consider how to handle coronavirus.\nHumanistic, humanism and humanist are terms in psychology relating to an approach which studies the whole person, and the uniqueness of each individual.\nEssentially, these terms refer the same approach in psychology. The humanistic approach in psychology developed as a rebellion against what some psychologists saw as the limitations of the.\nAuthor: Terry Leahy; Publisher: Routledge ISBN: Category: Social Science Page: View: DOWNLOAD NOW» Recent critiques treat humanism as a mistaken value framework. Indeed, the concept of human nature is in fact essential for sociology, but is often being denied at the same time as it appears without acknowledgement.\nProfessor Berger places sociology in the humanist tradition and recognizes it as a ""peculiarly modern, peculiarly timely form of critical thought."" Without underestimating the importance of scientific procedures in sociology, he points out its essential affinity with history and philosophy, and he shows how sociology in this sense can Reviews: Humanity & Society, the official journal of the Association for Humanist Sociology was first published in and has been published quarterly since Humanity & Society is a peer-reviewed journal with abstracts of published articles appearing in Sociological Abstracts.\nInvitation to Sociology: A Humanistic Perspective - Ebook written by Peter L. Berger. Read this book using Google Play Books app on your PC, android, iOS devices. Download for offline reading, highlight, bookmark or take notes while you read Invitation to Sociology: 4/5(8). The NOOK Book (eBook) of the Humanist Realism for Sociologists by Terry Leahy at Barnes & Noble.\nFREE Shipping on $35 or more. Due to COVID, orders may be delayed. Thank you for your patience. Book Annex Membership Educators Gift Cards Stores & Events Help Auto Suggestions are available once you type at least 3 letters. Walda Katz Fishman is Associate Professor of Sociology at Howard University.\nShe is Editor of Humanity and Society, official journal of the Association for Humanist Sociology, and an officer in the Marxist Sociology Section of the American Sociological research interests are in the areas of inequality, the rise of right wing reaction and fascism, and social transformation to a Cited by: 3.\nThe Association for Humanist Sociology is pleased to announce their Betty and Alfred McClung Lee Book Award. Authors, publishers, and AHS members may nominate books for consideration.\nThe winner will be recognized at our annual meeting October 30 through November 3, in El Paso, Texas.5/5(1). In this groundbreaking book, Aldon D. Morris\'s ambition is truly monumental: to help rewrite the history of sociology and to acknowledge the primacy of W.\nDu Bois\'s work in the founding of the discipline. Calling into question the prevailing narrative of how sociology developed, Morris, a major scholar of social movements, probes the way in which the history of the discipline has. Humanist Manifesto i n 2 In thi s document, In his book e B attle for the Mind, sociology of rel igion also look s at religion and it s relation to other va lue spheres,Author: Tom Kaden.\nBook Description. Traditionally, Sociology has identified its subject matter as a distinct set – social phenomena – that can be taken as quite different and largely disconnected from potentially relevant disciplines such as Psychology, Economics or Planetary Ecology.\nBook Review Submissions. Humanity & Society (H&S) offers a book review section that aims to offer thoughtful summaries and appraisals of recently released books germane to humanist sociology.\nReviewers are asked to keep the H&S mission in mind when reviewing. Professor Berger places sociology in the humanist tradition and recognizes it as a ""peculiarly modern, peculiarly timely form of critical thought."" Without underestimating the importance of scientific procedures in sociology, he points out its essential affinity with history and philosophy, and he shows how sociology in this sense can.\ntion for Humanist Sociology. Rather than just representing another ""flavor"" of aca demic sociology, humanism is a different paradigm. Humanistic sociology is about making a better world. What had become mainstream sociology was the tame aca demic sociology. Humanistic sociology in the s began in the counterculture.\nSociological paradigms and organisational analysis: elements ofthe sociology ofcorporate lite. Organization I. TitleGareth \'5 HMl31 ISBN 0 6Hbk Pbk Printed and bound in Great Britain by Athenaeum Press Ltd, Gateshead, Tyne & Wear Contents Ust of figures List ofTables Acknowledgements Introduction page v.\nThe authors argue in this book that social theory can usefully be conceived in terms of four broad paradigms, based upon different sets of meta-theoretical assumptions with regard to the nature of social science and the nature of society.\nThey provide extensive reviews of functionalist, interpretive, radical humanist and radical structuralist paradigms which derive from quite distinct. is the daily online news site of the American Humanist Association. Founded in upon the merger of the AHA’s weekly e-zine Humanist Network News and the website of the Humanist magazine, is the online hub for news, politics, science, and culture from a humanist perspective.\nserves as a general-interest website for. C. Wright Mills is best remembered for his highly acclaimed work The Sociological Imagination, in which he set forth his views on how social science should be pursued. Hailed upon publication as a cogent and hard-hitting critique, The Sociological Imagination took issue with the ascendant schools of sociology in the United States, calling for a humanist sociology connecting the 5/5(2).\nThis work has been declared by the AHA board as historic, and is superseded by Humanist Manifesto III The Manifesto is a product of many minds. It was designed to represent a developing point of view, not a new creed.\nThe individuals whose signatures appear would, had they been writing individual statements, have stated the [ ]. A MANIFESTO FOR A CRITICAL HUMANISM IN SOCIOLOGY ON QUESTIONING THE HUMAN SOCIAL WORLD Ken Plummer (Emeritus Professor of Sociology, University of Essex, U.K.) First Presented at the VI Congreso Andaluz de Sociologiá, University of Cadiz, November Published in Daniel Nehring: Sociology: A Text and Reader (Pearson, ).\nThis is the. His book, At Home on the Street, was awarded honorable mention (first runner up) for the Annual Book Award from the Association for Humanist Sociology. He also co-directed a documentary film, ""American Refugees: Homelessness in Four Movements"" ().SAGE Video Bringing teaching, learning and research to life.\nSAGE Books The ultimate social sciences digital library. SAGE Reference The complete guide for your research journey. SAGE Navigator The essential social sciences literature review tool.\nSAGE Business Cases Real world cases at your fingertips. CQ Press Your definitive resource for politics, policy and people.Ana Raquel Minian is an Associate Professor in the Department of History. Her first book, Undocumented Lives: The Untold Story of Mexican Migration (Harvard University Press, ) received the David Montgomery Award for the best book in labor and working-class history, given jointly by the Organization of American Historians and the Labor and Working-Class History Association; t he.', '“Attaching and Belonging”\nTwo broad bodies of research-based theory have directly informed youth development policy and practice, and in turn helped to inspire the the Positive Youth Justice Model: social learning theory (e.g., Bandura 1977) and social control theory (Hirschi 1969).\nSocial control theory suggests that the strength and durability of an individual’s bonds or commitments to conventional society inhibit social deviance (Hirschi 1969; Simpson 1976). The need for belonging and attachment to others is fundamental, influencing many behavioral, emotional, and cognitive processes. Numerous studies highlight the association between attachments and positive youth outcomes. Early sociologists argued that the various forms of social deviance, including criminal behavior, emerge when the connections between individuals and the larger society are weak (Durkheim 1947).\nIn one of the foundational applications of social control theory to the field of crime and delinquency, Hirschi (1969) argued that the most important question is not “why do they do it?” (i.e., why do criminals commit crime), but rather “why do the rest of us not do it?” Social control theory offers an explanation—social bonds. When an individual’s bonds to society are strong, they prevent or limit crime and other deviant behavior. When bonds are weak, they increase the probability of deviance. Weak or broken bonds do not “cause” delinquency, but rather allow it to happen (Whitehead and Lab 2009: 89). Hirschi proposed four elements that help to shape the social bonds between individuals and their society:\n- Attachments—expressed concern about what others think, or “sensitivity to the opinion of others”(Hirschi 1969: 22) that would lead individuals to avoid crime and negative behavior in order to avoid disappointing a respected individual or group (e.g., teachers or parents);\n- Commitments— “investment of time, energy and oneself” in a particular form of conventional activity and awareness that deviant behavior would place such investment at risk (Whitehead and Lab 2009: 89);\n- Involvements—sufficient time and energy spent on conventional activities such that less time remains for delinquent behavior; and\n- Beliefs—the extent to which an individual “has been socialized into and accepts the common belief system” (Whitehead and Lab 2009: 89), assuming there is “a common value system” within the society or group” (Hirschi 1969).\nAlthough theoreticians continue to debate the relative strength or salience of the particular elements of social bonds (e.g., involvements), the basic tenets of social control theory are strongly predictive and have been supported by rigorous research for decades (e.g., Wiatrowski, Griswold and Roberts 1981). The strength of an individual’s social bonds decreases the propensity for criminal or deviant behavior. In other words, youth are less attracted to criminal behavior when they are involved with others, learning useful skills, being rewarded for using those skills, enjoying strong relationships and forming attachments, and earning the respect of their communities. As these social bonds become internal, they build social control, which deters individuals from committing unlawful acts.\nDurkheim, E. (1947). The division of labor in society (George Simpson, Trans.). New York: The Free Press.\nHirschi, T. (1969). Causes of delinquency. Berkeley, CA: University of California Press.\nSimpson, A.L. (1976). Rehabilitation as the justification of a separate juvenile justice system. California Law Review, 64(4), 984–1017.\nWhitehead, J. and S. Lab (2012). Juvenile justice: An introduction. Elsevier.\nWiatrowski, M., D. Griswold and M. Roberts (1981). Social control theory and delinquency. American Sociological Review, 46, 525-541.']"	['<urn:uuid:a2f6e609-b61a-41e1-b1f7-691d1d39e9d7>', '<urn:uuid:80697fa8-e6c9-48bd-a99f-e3c7f078826c>']	open-ended	direct	verbose-and-natural	similar-to-document	comparison	novice	2025-05-12T20:11:36.665104	22	95	2661
55	how long do solar water heaters typically last	Solar water heaters, when properly serviced and maintained, have a life expectancy of at least 20-25 years or more.	['It should be simple to understand, but when it comes to solar renewables, particularly solar water heaters, and to a lesser extent rooftop solar PV, the question of what it ‘actually’ costs seems to get lost. Almost always the first question is ‘What does it cost?”. End consumers and even government tenders are fixated by the ‘sticker’ cost, in much the same way as looking at a car in the showroom, or even a utility item like a washing machine. The wrong question is being asked. The far more meaningful question is “What is the Return on the Investment”.\nThe majority of consumers both domestic and business will only really consider consumer renewables in the form of solar water heaters and solar rooftop PV, if they are connected to the grid, on the basis that it can save them money.\n1 litre of hot water (assuming it is the same temperature), irrespective of whether it was made over a fire, in an electric kettle or an electric geyser is still 1 litre of hot water.\nIn the same way 1 kWh generated by Eskom or solar PV farm or wind power is still 1 kWh. 1 kWh of energy saved through a solar water heater is still 1 kWh of energy and 1 kWh generated by rooftop solar PV is still 1 kWh.\nThe difference is that with transmitted power there will be efficiency losses, meaning that 1 kWh used by the end user will have required more than 1 kWh put down the line.\nAlthough this is simplistic and obvious, solar water heaters, while being relatively simple technology are surprisingly complex to evaluate from a Return on Investment (ROI) perspective. In contrast rooftop solar PV, which is arguably more complex from a technology perspective, is simpler in that its performance rating is given in watts or kilowatts.\nThis blog post will concentrate on solar water heaters ROI, and a subsequent one for Rooftop solar PV where there are traps waiting for the unwary.\nSolar Water Heaters and the Return On Investment (ROI)\nThe simple concept of catching the heat from the sun, or insolation, converting it into hot water and storing it for later use is straightforward enough. It can be done using numerous different types of solar collectors and the insulated tank to store and retain the heated water is similar to a thermos.\nThe starting point for calculating the Return on Investment has to be the power output of the system(s). The specific heat of water formula enables this to be calculated by taking the temperature rise in the tank (volume × temp rise ÷ 860 = kWh). When done over a number of days and with no water being drawn off, starting at cold and measuring at the end of the day, the (deemed) kWh output can be determined.\nWith the kWh output and applying a monetary value to the kWh saved, the total savings per day, or year, or over a number of years can be calculated.\nTaking a number of systems and doing the same exercise on each enables a comparison and one factor of the ROI to be determined.\nThe table below is illustrative of a simple analysis.\nExercise 1 - Cost per kWh per day, year, number of years, by taking the kWh output and dividing the cost by the kWh’s saved.\nExercise 2 - The output of the SWH in kWh multiplied by the cost of the kWh being saved per day and divided into the cost of the SWH system to arrive at the payback point.\nThe Return on Investment can be determined in the tables above, and in Table 1 and Table 2 System C is the winner, although System A was the cheapest ‘on the sticker price”. Projecting forward over a number of years, increasing the cost of electricity each year, enables both IRR and ROI calculations to be done.The graph below is illustrative of this:\n- It is assumed that the warranties or guarantees on the systems are the same.\n- The life expectancy of all the systems, assuming that they are serviced and maintained is also the same, which should be at least 20-25 years or more.\nThe ROI however also needs to take into account a number of other factors. The most important of these is: does the system produce hot water that is useable for washing?\nOn a purely financial basis (ROI) System C was the winner in the illustrative analysis, but as can be seen in Table 3 (below) the temperature increase in the tank from cold with 10 kWh output will only raise the cold water at 11 °C start temperature by 28,67 °C to 39,67 °C.\nAs most people wash at 40 °C System C which was the winner (on a pure ROI basis) fails in the objective of heating water to a temperature that is useable, unless electrical back up is used.\nOne can therefore conclude that if all the factors are taken into account, System D or System B are the best buys, and System C may be the best ROI for many months of the year (but not in winter), and the cheapest System A on the sticker price, is the worst.\nGuidelines for Buying Solar Water Heaters\nMaking an investment in a solar water heater is without doubt potentially a fantastic return on investment. However choosing a system is open to sales speak, and being influenced by the sticker price. It is also unreasonable for a buyer to understand all the factors outlined above, (there are others too).\nThe questions a buyer should be asking rather than “what is the cost?” or “what is the price for the solar water heater” are:\n- “How much hot water (at 40° C) will this solar water heater generate each average day for washing with?”\n- “How many kWh of energy will this SWH save each day?”\n- “How much money will this SWH save me?”\n- “When do I recover the cost of this investment?” (the payback point)\n- “What are the projected financial savings?”\n- “What is my projected return on Investment over, 5, 10, 20 years?”\nIf the sales people can provide these answers, with supporting evidence, the potential purchaser can then buy with confidence. If not, the probability is that the expectations of reductions in electricity bills of 40% or more will not be achieved and the consumer will be disappointed.\nIt really is no different to buying a car. If you don’t ask the questions, fuel consumption, how fast, service periods, warranties, depreciation, you may end up disappointed.\nSolar Water Heaters Without Electrical Back Up\nWhen the return on investment is done on solar water heaters without electrical back up even greater attention is required.\nThe South African government’s worthy holistic socio economic uplift program for the roll out of solar water heaters to low income homes, which stopped at the end of 2012 had many problems, but the one which only came to light later, was that most homes didn’t actually get hot water from the installed systems.\nThis was partly down to bad installations, but the reality was that the performance of the majority of LP SWH, did not have the necessary solar water output to generate hot water in winter, when the days are shorter and the cold water is colder than in summer.\nEffectively government was spending money on SWH systems that was akin to buying cars without wheels. The lower sticker cost was a motivating factor while ignoring what it actually could do or rather couldn’t do.\nSolar waters heaters that do not have electrical backup, as those supplied to low income homes, require a solar power output that is adequate to ensure that hot water is available. Using winter months when cold water is at 11 °C and when days are shorter, the required output is as shown below.\nWhat is the Price of the Solar Water Heater? - Conclusion\nA simple question can result in a complex answer. Solar water heaters are not the same as an electrical geyser that heats water up on demand. It needs to be sized correctly to the user’s requirements. The performance of the system needs to be looked at in its entirety, not the solar collector or tank in isolation.\nFrom an investment perspective the overall Return On Investment needs to be taken into account. Ask the right questions and you will end up making the right decisions. Don’t ask and you are likely to be disappointed.\nA general guideline in solar thermal is that bigger is better, as long as it is sufficiently powerful in kWh output.\nAs the government is in process of implementing a new revived solar water heating roll out, tenders were recently announced for Social Facilitation and Technical Feasibility, the chosen panel of service providers really do need to understand the ROI equations and the minimum hot water heating outputs.']	['<urn:uuid:e2b01f03-a1d2-4ea9-8c99-afca144ae7b0>']	open-ended	with-premise	short-search-query	similar-to-document	single-doc	novice	2025-05-12T20:11:36.665104	8	19	1493
56	pregnant woman symptoms nausea vomiting morning sickness ramadan fasting effects management	During pregnancy, nausea and vomiting commonly occur between 5-18 weeks, affecting 50-90% of women. While called 'morning sickness,' symptoms can occur throughout the day. If you're fasting during Ramadan while pregnant, it's important to manage both conditions carefully. Islamic law permits pregnant women to opt out of fasting if there are health concerns. If you choose to fast, focus on staying hydrated during non-fasting hours, watch for signs of dehydration like dark urine or dizziness, and consume foods that release energy slowly during suhoor. For managing nausea, try eating small, frequent meals high in protein or carbohydrates when not fasting, avoid triggers like strong odors or spicy foods, and consider safe treatments like vitamin B6 supplements or ginger. Always consult your healthcare provider about fasting and managing pregnancy symptoms.	"['If you find a broken link in this list, please report it to the webmaster.\nIf you are a Muslim woman who is pregnant, or is planning to become pregnant, you may be wondering whether you should still fast during Ramadan. Hopefully the responses to the frequently asked questions below will help provide you with the information you need.\nDo I have to fast?\nIslamic law gives permission for pregnant and breastfeeding women to opt out of fasting if she fears that it will harm her health or the health of her baby.\nMissed days of fasting can be made up at a later date, or if this isn’t possible, a ‘fidyah’ can be paid by providing food for someone in poverty for every missed day of fasting. However, some pregnant Muslim women decide to fast during Ramadan. This is a very personal decision and will depend on your own circumstances such as the stage of pregnancy, how you are feeling and if you have experienced any problems so far in your pregnancy. Fasting should be discussed with your midwife or doctor so that you can have a health check, identify any potential complications you may be at risk of when fasting and get their advice on whether fasting is likely to harm you or your baby’s health. The time of year Ramadan falls (e.g. during long hot summer days) and work commitments may also affect your decision.\nIs fasting during pregnancy safe?\nResearch is still ongoing in this area and although the evidence is not clear cut, many experts believe it is not a good idea to fast during pregnancy. There is some evidence to suggest that pregnant women who fast during Ramadan may have smaller placentas and/or babies with slightly lower birth weights, compared to women who don’t fast. Fasting may also increase the risk of becoming dehydrated, especially if Ramadan falls during the summer, and this may affect the way your kidneys function and the amount of fluid surrounding your baby. However, other studies have not found any differences between babies who are born to mothers who have fasted and those who have not fasted during Ramadan. The impact of fasting during pregnancy may depend on the overall health of the mother, the stage of pregnancy and the time of year Ramadan occurs. More research is needed to fully understand what impact fasting may have on the health and development of the baby and what that may mean for the child’s health in later life.\nIf I decide to fast, is there anything I can do to make it more manageable for me and my baby?\nPregnancy is quite a demanding time for your body in terms of nutrients and fluids it needs. If you are considering taking part in Ramadan during pregnancy, make sure you let your midwife and/or doctor know so that they can offer you some advice and perform any necessary health checks. If you do decide to fast during Ramadan, you may wish to consider fasting on some but not all days of the month e.g. fasting on alternate days or at weekends to try and make it a bit more manageable.\nIf you are fasting, dehydration is something to watch out for, especially if Ramadan falls during long hot summer days. Feeling thirsty or having dark-coloured urine can be early signs of dehydration, other symptoms may include dizziness, headache, tiredness, dry mouth and passing small amounts of urine infrequently (less than three or four times a day). If you feel dizzy, faint, weak, confused or tired during fasting, even after resting, then you should break your fast with a sweet drink, to replace lost sugar and fluids, and a salty snack, to replace lost salt, or an oral rehydration solution and contact your doctor. To try to reduce the risk of dehydration; stay cool in the shade, don’t over-exert yourself, and try to drink plenty of fluids once you have broken your fast and at ‘suhoor’. Remember that during pregnancy, the amount of fluid you need may increase by an extra one or two glasses a day. On top of drinking lots of fluids, including foods which have a high water content such as fruits, vegetables, soups, stews and porridge in your ‘suhoor’ and ‘iftar’ meals may also help to keep you hydrated. It is also a good idea to avoid consuming too many salty foods, especially first thing in the morning, as this may make you feel even more thirsty.\nMake sure you are still taking your supplements (folic acid and vitamin D) and eating a healthy balanced diet during Ramadan so that you are getting all the nutrients you and your baby need. Also try to eat foods which release energy slowly (low glycaemic index foods) such as wholemeal pasta, wholemeal bread, oat and bran based cereals, beans and unsalted nuts, especially at suhoor.\nIf you have decided to fast during Ramadan and then begin to feel unwell, it is important to contact your midwife or doctor as soon as possible and consider breaking your fast.\nFor more information on the sources used in this text, please contact firstname.lastname@example.org', 'Patient education: Nausea and vomiting of pregnancy (Beyond the Basics)\n- Judith A Smith, PharmD, BCOP, CPHQ, FCCP, FISOPP\nJudith A Smith, PharmD, BCOP, CPHQ, FCCP, FISOPP\n- Associate Professor and Director\n- Women\'s Health Integrative Medicine Research Program, Department of Obstetrics and Gynecology\n- Jerrie S Refuerzo, MD\nJerrie S Refuerzo, MD\n- Associate Professor\n- Division of Maternal Fetal Medicine\n- Department of Obstetrics and Gynecology\n- University of Texas Health Science Center at Houston\n- Susan M Ramin, MD\nSusan M Ramin, MD\n- Section Editor — Obstetrics\n- Professor of Obstetrics and Gynecology\n- Baylor College of Medicine\nNausea and vomiting of pregnancy commonly occur between 5 and 18 weeks of pregnancy. Between 50 and 90 percent of women have some degree of nausea, with or without vomiting. The severity of these symptoms can vary.\n""Morning sickness"" is the term often used to describe mild nausea and vomiting while ""hyperemesis gravidarum"" is the term used to describe a more severe condition. Hyperemesis may cause you to vomit multiple times throughout the day, lose weight, and usually requires treatment in the hospital.\nThis article discusses treatments available for nausea and vomiting during pregnancy. A more detailed article is available by subscription. (See ""Clinical features and evaluation of nausea and vomiting of pregnancy"" and ""Treatment and outcome of nausea and vomiting of pregnancy"".)\nMORNING SICKNESS VERSUS HYPEREMESIS\nMorning sickness — Nausea and vomiting often develop by five to six weeks of pregnancy. The symptoms are worst around nine weeks, and typically improve by 16 to 18 weeks of pregnancy. However, symptoms continue until the third trimester in 15 to 20 percent of women and until delivery in 5 percent of women . Although mild pregnancy-related nausea and vomiting is often called ""morning sickness,"" you may feel sick at any time of day and many women (80 percent) feel sick throughout the day.\nInterestingly, women with mild nausea and vomiting during pregnancy experience fewer miscarriages and stillbirths than women without these symptoms.\nHyperemesis gravidarum — Hyperemesis gravidarum is the term used to describe more severe nausea and vomiting during pregnancy. Women with hyperemesis often vomit every day and may lose more than 5 percent of their pre-pregnancy body weight. In most cases, women with hyperemesis gravidarum will have blood and urine tests that show evidence of dehydration.\nCAUSE OF NAUSEA AND VOMITING IN PREGNANCY\nThe cause of pregnancy-related nausea and vomiting is not clear. Several theories have been proposed, although none have been definitively proven. Increased hormone levels, slowed movement of the stomach contents, and psychological factors are among the more common theories.\nSome women are more likely to develop nausea and vomiting of pregnancy, including women who:\n●Developed these symptoms in a previous pregnancy\n●Experience nausea and vomiting while taking estrogen (for example, in birth control pills) or have menstrual migraines\n●Experience motion sickness\n●Have a history of gastrointestinal problems (ie, reflux, ulcers)\nWHEN TO SEEK HELP\nMany women, especially those with mild to moderate nausea and/or vomiting, do not need to see a healthcare provider for treatment of nausea and vomiting. The suggestions below may help to reduce symptoms and prevent dehydration. (See \'Treatment of nausea and vomiting in pregnancy\' below.)\nWomen with more severe nausea and vomiting sometimes need to be evaluated by their primary care or obstetrical doctor or nurse. Seek help if you have one or more of the following:\n●Signs of dehydration, including infrequent urination, dark-colored urine, or dizziness with standing\n●Vomiting repeatedly throughout the day, especially if you see blood in the vomit\n●Abdominal or pelvic pain or cramping\n●If you are unable to keep down any food or drinks for more than 12 hours\n●You lose more than 5 pounds (2.3 kg)\nOne or more tests may be recommended to investigate the cause and determine the severity of the nausea and vomiting, including blood tests, urine tests, or an ultrasound.\nTREATMENT OF NAUSEA AND VOMITING IN PREGNANCY\nThe treatment of pregnancy-related nausea and vomiting aims to help you feel better and allow you to eat and drink enough so that you do not lose weight.\nTreatment may not totally eliminate your nausea and vomiting. You may need to try several types of treatment over a period of weeks before finding what works best for you. Fortunately, symptoms generally resolve by mid-pregnancy, even if you do not use any treatment. (See ""Treatment and outcome of nausea and vomiting of pregnancy"".)\nDietary changes — Avoiding food or not eating may actually make nausea worse. Try eating before or as soon as you feel hungry to avoid an empty stomach, which may aggravate nausea. Eat snacks frequently and have small meals (eg, six small meals a day) that are high in protein or carbohydrates and low in fat. Drink cold, clear, and carbonated or sour fluids (eg, ginger ale, lemonade) and drink these in small amounts between meals. Smelling fresh lemon, mint, or orange or using an oil diffuser with these scents may also be useful.\nAvoid triggers — One of the most important treatments for pregnancy-related nausea and vomiting is to avoid odors, tastes, and other activities that trigger nausea. Eliminating spicy foods helps some women. Other examples of triggers include:\n●Odors (eg, perfume, chemicals, coffee, food, smoke)\n●Heat and humidity\n●Visual or physical motion (eg, flickering lights, driving)\n●Consuming large amounts of high-sugar foods/snacks\n●Consuming spicy foods and high-fat foods\nBrushing teeth after eating may help prevent symptoms. Avoid lying down immediately after eating and avoid quickly changing positions.\nIf you take a prenatal vitamin with iron and this worsens your symptoms, try taking them at bedtime. If symptoms persist, stop the vitamins temporarily. If you stop taking your prenatal vitamin, take a supplement that contains 400 to 800 micrograms of folic acid until you are at least 14 weeks pregnant to reduce the risk of birth defects.\nMedications — Medications that reduce nausea and vomiting are effective in some women and are safe to take during pregnancy. None of the medications discussed below are known to be harmful. Make sure you talk with your healthcare provider before taking any new over the counter or prescription medications, including nutritional and herbal supplements.\n●Vitamin B6 and doxylamine — Vitamin B6 supplements can reduce symptoms of mild to moderate nausea, but do not usually help with vomiting. Doxylamine is a medication that can reduce vomiting, and may be combined with vitamin B6. Doxylamine is available in the United States in some non-prescription sleep aids (eg, Unisom, Good Sense Sleep Aid) and as a prescription antihistamine chewable tablet (Aldex AN). Combinations of vitamin B6 and doxylamine formulations are available for the initial treatment of nausea (eg, Diclectin in Canada and Diclegis in the United States).\n●Antihistamines and other anti-nausea medications — Antihistamines and other anti-nausea medications are safe and effective treatments for pregnancy-related nausea and vomiting. The following medications may be recommended:\n•Diphenhydramine (Benadryl), but this drug may cause drowsiness\n•Meclizine (Bonine), but this drug may also cause drowsiness\n●Other anti-nausea medications that are available by prescription include:\n•Promethazine (Phenergan) — Promethazine is available in pill, oral solution, injectable solution, or rectal suppository form. It is usually taken every four hours, and may cause drowsiness and dry mouth. Rare side effects include muscle contractions that cause twisting or jerking movements.\n•Metoclopramide (Reglan) — Metoclopramide speeds emptying of the stomach and may help to reduce nausea and vomiting. It is available in a pill, oral solution, and injectable usually taken 30 minutes prior to meals and at bedtime.\n•Ondansetron (Zofran) — Ondansetron is an anti-nausea medication that is usually taken by mouth or injection every eight to 12 hours. Ondansetron is an expensive anti-nausea medication (approximately $500 for 30 pills in the United States) and it may not be covered by some insurance plans.\nFluids and nutrition — If you are unable to hold down food or liquids, you may be treated with intravenous (IV) fluids. This may be done in your doctor or nurse\'s office or in the hospital, depending upon the severity of your vomiting. For a short time, you may be advised not to eat or drink anything, to allow the gut to rest. You can slowly begin to eat and drink again as you begin to feel better, usually within 24 to 48 hours.\nIf you continue to lose weight despite treatment, your doctor may consider other forms of feeding, such as the use of a nasogastric tube (a tube that is inserted through your nose into the stomach) or supplemental nutrition through an IV line.\nComplementary treatments — The following treatments may be useful when used with the treatments described above.\n●Acupuncture and acupressure — Acupressure wristbands (picture 1) and acupuncture have become a popular treatment for nausea and vomiting caused by pregnancy, motion sickness, and other causes. Studies have not shown these wristbands to be more effective than sham (fake, look-alike) wristbands , although some women find them helpful. Acupuncture and acupressure have no known harmful side effects.\n●Hypnosis — Hypnosis has been reported to be helpful in some people. Counseling may be helpful for women with anxiety.\n●Ginger — Powdered ginger or ginger tea may help to relieve nausea and vomiting in some women. However, further studies are needed to confirm that this treatment is both safe and effective. Until more data are available, we suggest the use of ginger containing foods (eg, ginger lollipops, ginger ale) for mild nausea and vomiting.\nMost women with pregnancy-related nausea and vomiting recover completely without any complications. Women with mild to moderate vomiting often gain less weight during early pregnancy. This is rarely a concern for the baby unless the mother was very underweight before pregnancy (at least 10 percent under the ideal body weight).\nNormal weight gain during pregnancy depends upon your pre-pregnancy weight. For women of normal weight (body mass index 18.5 to 24.9 kilogram/meter2), the recommended weight gain is between 25 and 35 pounds (11.5 to 16.0 kilograms) for a singleton pregnancy.\nIn women with severe nausea and vomiting (hyperemesis gravidarum) who are hospitalized multiple times and who do not gain weight normally during pregnancy, there is a small risk that the baby will be underweight or small.\nWomen who have hyperemesis gravidarum in one pregnancy are at risk of severe nausea and vomiting in future pregnancies. The risk is between 15 and 20 percent. Women who do not have severe nausea and vomiting in the first pregnancy are unlikely to have it in future pregnancies .\nWHERE TO GET MORE INFORMATION\nYour healthcare provider is the best source of information for questions and concerns related to your medical problem.\nThis article will be updated as needed on our web site (www.uptodate.com/patients). Related topics for patients, as well as selected articles written for healthcare professionals, are also available. Some of the most relevant are listed below.\nPatient level information — UpToDate offers two types of patient education materials.\nThe Basics — The Basics patient education pieces answer the four or five key questions a patient might have about a given condition. These articles are best for patients who want a general overview and who prefer short, easy-to-read materials.\nPatient education: Morning sickness (The Basics)\nPatient education: Pregnancy symptoms (The Basics)\nPatient education: Taking over-the-counter medicines during pregnancy (The Basics)\nPatient education: Motion sickness (The Basics)\nPatient education: Hyperemesis gravidarum (The Basics)\nBeyond the Basics — Beyond the Basics patient education pieces are longer, more sophisticated, and more detailed. These articles are best for patients who want in-depth information and are comfortable with some medical jargon.\nProfessional level information — Professional level articles are designed to keep doctors and other health professionals up-to-date on the latest medical findings. These articles are thorough, long, and complex, and they contain multiple references to the research on which they are based. Professional level articles are best for people who are comfortable with a lot of medical terminology and who want to read the same materials their doctors are reading.\nApproach to the adult with nausea and vomiting\nCharacteristics of antiemetic drugs\nClinical features and evaluation of nausea and vomiting of pregnancy\nTreatment and outcome of nausea and vomiting of pregnancy\nThe following organizations also provide reliable health information.\n●National Library of Medicine\n(www.nlm.nih.gov/medlineplus/ency/article/001499.htm, available in Spanish)\n●Society of Obstetricians and Gynecologists of Canada\n●Organization of Teratology Information Specialists\n- Association of professors of gynecology and obstetrics. Nausea and vomiting of pregnancy. Association of professors of gynecology and obstetrics, Washington, DC 2001.\n- Matthews A, Haas DM, O\'Mathúna DP, Dowswell T. Interventions for nausea and vomiting in early pregnancy. Cochrane Database Syst Rev 2015; :CD007575.\n- Dodds L, Fell DB, Joseph KS, et al. Outcomes of pregnancies complicated by hyperemesis gravidarum. Obstet Gynecol 2006; 107:285.\n- Magee LA, Mazzotta P, Koren G. Evidence-based view of safety and effectiveness of pharmacologic therapy for nausea and vomiting of pregnancy (NVP). Am J Obstet Gynecol 2002; 186:S256.\n- Goodwin TM. Nausea and vomiting of pregnancy: an obstetric syndrome. Am J Obstet Gynecol 2002; 186:S184.\n- Holmgren C, Aagaard-Tillery KM, Silver RM, et al. Hyperemesis in pregnancy: an evaluation of treatment strategies with maternal and neonatal outcomes. Am J Obstet Gynecol 2008; 198:56.e1.\nAll topics are updated as new information becomes available. Our peer review process typically takes one to six weeks depending on the issue.']"	['<urn:uuid:9e61dcaf-0ecf-4fa2-b580-3f37b63aa044>', '<urn:uuid:0c266a1a-2768-4804-861b-93aa64437ddb>']	open-ended	with-premise	long-search-query	similar-to-document	three-doc	novice	2025-05-12T20:11:36.665104	11	129	3077
57	What historical publications and biographical works did the Ecuadorian historian and journalist from Baños de Agua Santa write during his career?	He wrote several historical publications including 'History of the Republic', 'Brief History of Ecuador', 'Brief General History of Ecuador', 'General History of America', 'Animated History of Ecuador', 'Baños del Tungurahua from its origins to the Cabildo', and 'The last seven years'. His biographical works included 'Life of Juan Montalvo', 'Life and work of Manuel J Calle', and the educational booklet 'History and Geography of the Ecuadorian East' in collaboration with Professor Francisco Terán.	['Who was Oscar Efren Reyes?\nHistorian, journalist and teacher born in Baños de Agua Santa, who rendered invaluable services to his country through research, publication of books, opinion articles in the press and teaching in schools and universities.\nHis historical publications: History of the Republic, Brief History of Ecuador, Brief General History of Ecuador, General History of America, Animated History of Ecuador, Baños del Tungurahua from its origins to the Cabildo, The last seven years. Biographical works: Life of Juan Montalvo, Life and work of Manuel J Calle, as well as booklets for free educational distribution such as “History and Geography of the Ecuadorian East” in collaboration with Professor Francisco Terán.\nCollaboration with research and literary direction in: General History of America by Ricardo Levene, Argentina, Encyclopedia Britannica, The Province of Tungurahua in 1928, respectively.\nThroughout his sixty years of life, he wrote historical, pedagogical and opinion articles in newspapers: El Guante, El Universo de Guayaquil, La Prensa, El Comercio and El Día in Quito and other newspapers and magazines in Latin America.\nHe was founding Rector of the Montufar National School, Rector and professor at the Mejía National School.\nProfessor at the National Bolivar School in Ambato, and Juan Montalvo School in Quito.\nProfessor of the Faculty of Philosophy and Letters of the Central University of Ecuador.\nHonorary member of the Academies of History of Chile, Argentina, Mexico and Ecuador.\nCorresponding member of the National Academy of History of Ecuador.\nActive member of cultural societies, among others: The Ecuadorian Athenaeum. The America Group.\nHis life has been the subject of opinions of several intellectuals of the twentieth century who appreciated his work and knew his whole personality, one of them affirms what constituted the ethical orientation of all his work, quoting him in a phrase of his: “tell the truth and tell it whole”.\nThe Oscar Efren Reyes Foundation main house\nThe history of the Foundation’s headquarters is linked to the historian’s personal life. Oscar Efrén Reyes acquired the house from a relative in the early sixties. Since then, after retiring from his working life at the Central University and the rectorship of the Mejia School, he returned to his beloved birthplace: Baños de Agua Santa. He lived there with his wife Clara for a few years until his death in 1966.\nUpon his death, his heirs, by common agreement, decided that the house would become the property of Francisca Reyes Torres, youngest daughter of the historian and teacher, who in turn, donated it in her will to the foundation that bears the name of Oscar Efrén Reyes and thus perpetuate the memory of the illustrious Ecuadorian.\nIn early 2013, the house was declared a heritage property by the INPC.\nHistory of the Oscar Efren Reyes Foundation\nThe Oscar Efrén Reyes Foundation was born as an initiative of the historian’s daughter: MGS. Francisca Reyes Torres, who in her will stipulated its conformation and headquarters in the house that once belonged to Oscar Efrén Reyes in Baños de Agua Santa.\nOscar Efrén Reyes opens the doors of this virtual house so that his dream of testimony of a people with historical identity, ancestral culture and abundant nature can be spread throughout Ecuador and the world.\nThe house he lived in during his last years is now the headquarters of the foundation that bears his name.\nThere you can visit the biographical room, use the auditorium, look at the garden with the tree planted by the historian fifty years ago and a sober lodging for those who want to join as volunteers in this important work.\nOn January 10, 2000, it obtained the legal approval by the Ministry of Social Inclusion through the agreement Nº 534, and began its activity with cultural programs with the subscription of 33 founding members.']	['<urn:uuid:0b119f63-560f-42b7-9ca3-7aaf4ff36dc9>']	factoid	direct	verbose-and-natural	similar-to-document	single-doc	expert	2025-05-12T20:11:36.665104	21	73	629
58	What challenges do parents of disabled Muslim children face?	Parents of children with Intellectual Disabilities face great hardship daily. Many have to stop visiting family to protect their children from misinformed attitudes and bullying within the Muslim community. They are told they brought shame to the family for having a 'different' child and face bullying, mockery, and derogatory names. They also experience physical and verbal abuse when their children show challenging behavior in public. Due to intolerance, many parents feel forced to hide their children from the community and seek help from non-Muslims instead.	['Children and adults with Intellectual Disabilities, as well as their families, suffer unprecedented levels of prejudice and discrimination throughout their lives. Such individuals are frequently victims of aggressive abuse, hostility, humiliation, conflict, teasing and stares. They also experience social isolation, lack of support from their extended families, and problems accessing education, employment and healthcare services. These cases of discrimination are not limited to non-Muslims, but are also prevalent within the Muslim community. Today, being World Autism Awareness Day, offers an opportunity for us to reflect seriously on the question: have we failed as a community when it comes to Intellectual Disabilities?\nDespite rising cases of Muslim children being born with Intellectual Disabilities – by 2021 it is estimated that up to 7% of children with Intellectual Disabilities in the UK will be Muslim – limited awareness and understanding of Intellectual Disabilities still exists within the Muslim community today. The prevalent belief among Muslims is that Intellectual Disabilities are caused by mental illness, possession by Jinns, supernatural phenomena and punishment for previous sins. However, such beliefs are borne out of ignorance, and only serve to reinforce stigma, negative attitudes and discrimination. This not only has the potential to limit the quality of life of those with Intellectual Disabilities, resulting in low self-esteem and negative self-evaluations, but it also impedes their inclusion and social acceptance into mainstream society. To tackle this problem there is a vital need for greater awareness and understanding of Intellectual Disabilities, especially focusing on the impact they have on individuals and their families within the Muslim community.\nAn Intellectual Disability is defined as a significant impairment in intellectual functioning and socially adaptive behaviour, which has an onset before adulthood. It can take the form of a number of conditions, some of which are Autism, Down’s syndrome, Asperger’s syndrome and Fragile X. These are primarily caused by biological factors, either through genetics, brain abnormalities or complications at birth, none of which necessarily have theological or supernatural foundations. In fact, neither the Qur’ān nor the Sunnah negate this, and those who state otherwise misinterpret verses from the Qur’ān and ahādīth to support their ill-conceived opinions. In light of these misconceptions, there has been a long history of families being exploited within the Muslim community by “spiritual healers” (rāqīs), who falsely claim that these disabilities are curable through spiritual healing (ruqya). For financial gain and out of pure ignorance, many spiritual healers exploit vulnerable families by persuading them that their child has a “disease” that can be cured. They cite the well-known hadīth that there is no disease that Allāh has created, except that He has also created its treatment. However, they fail to explain to families that Intellectual Disabilities are not diseases but rather, biological conditions that have no known “cure”.\nHolding onto the belief that Intellectual Disabilities are curable can be extremely detrimental to the people affected. It not only gives parents false hope that their children will one day be “cured”, but there is a major risk of their children not being given the social and developmental skills specific to their disability to prosper and lead independent lives. In essence, the only difference between them and non-disabled people is that they are programmed differently and thus, are no different to their non-disabled counterparts in every other way. If given the opportunity and right support, they can live extremely happy and independent lives and in some cases, outdo their non-disabled peers in different facets of their lives. For example, many Muslims are surprised to know that people with Intellectual Disabilities have special abilities, such as photographic memory and are able to perfectly mimic Qur’ān reciters. They are able to perform a host of other incredible feats that most non-disabled people would struggle to achieve. However, a lack of knowledge and an unwillingness to break preconceptions regarding Intellectual Disabilities encourages acts of discrimination towards them, reducing them and their parents to the fringes of their Muslim communities.\nBeing a parent of a child with an Intellectual Disability is extremely challenging. On a daily basis they go through great hardship, and sacrifice a large part of their lives to take care of their children. Many parents have to stop visiting family to protect their children from misinformed attitudes and bullying that is rife within the Muslim community. They are told they have brought shame to the wider family network for giving birth to a “different” child and are constantly bullied, mocked, cursed, called derogatory names and looked down upon. Many parents are also physically and verbally abused because their child is viewed as abnormal when they show instances of challenging behaviour in public. On many occasions, Muslim parents with “normal” children boycott such families in fear for their own children because misconceptions drive their understanding of Intellectual Disabilities and thus perpetuate the stigma further. Additionally, due to the constant intolerance and abuse they suffer, many parents feel the need to hide their children from the community, in order to protect them. As a result, Muslim parents often have to look for help from non-Muslims, who they find to be far more tolerant of disabilities.\nIn light of these problems and challenges, it is imperative the Muslim community understands the implications their words and actions have on families and their children with Intellectual Disabilities. While most people will only see such children for a few minutes in their daily lives, parents have to see their children grow up with these conditions. They also have to face up to the reality that they will face a great deal of discrimination in their lives, and will not be able to have the same future as other children. Additionally, taking care and managing the behaviour of children with these challenges is not an easy task, and one that is both emotionally and physically demanding. On many occasions, it can lead to the breakdown of the family and a host of other problems. In many instances parents who have children with learning disabilities have to cope with their own mental health issues or an Intellectual Disability. Just like their children, they are also greatly disadvantaged in terms of access to education, employment and healthcare and can easily fall through the cracks of a system that should provide them with the support they require. Therefore, a concerted effort needs to be made by the Muslim community to help support families in this situation. This can be achieved in a number of ways, one of which is simply reaching out to families by visiting and supporting them.\nDue to the immense struggles they go through on a daily basis, most parents can sometimes forget about the great reward they are attaining whilst they care for their child. Bearing in mind that Allāh (‘Azza wa Jall) praises parents for their sacrifice towards their non-disabled children, one can only imagine the reward parents gain for their sacrifice towards their child with an Intellectual Disability. Such children are very close to Allāh (‘Azza wa Jall) because many remain in an innocent state (ma’sūm) and will not be held accountable for their actions. Therefore, by loving them, exhibiting patience, not being resentful and thanking Allāh (‘Azza wa Jall) for this opportunity given to them, they can be a means for their parents and their carers to attain Paradise. They should also understand that no believer is struck with any form of distress, other than it being a means for them to expiate their sins. Additionally, Muslims must understand that such difficulties are a test from Allāh, not just for families with such children but also for the wider Muslim community who will be held accountable for their attitudes and actions towards them.\nIn relation to the treatment of people with Intellectual Disabilities, there is much that can be learnt from the Qur’ān and the Sunnah. Allāh tells the believers that people with special needs have value and have rights upon them, and they have been ordered to give them attention and be gentle with them. Therefore, it is an obligation upon Muslims to take care of vulnerable individuals and treat them with kindness. During the time of the Prophet (sallallāhu ‘alayhi wasallam), many of the Sahāba (radiallāhu ‘anhum) suffered from a range of disabilities. ‘Abdullāh b. Ummi Maktūm (radiallāhu ‘anhu) was blind, ‘Amr b. Jamūh (radiallāhu ‘anhu) suffered from a severe limp and Julaybīb (radiallāhu ‘anhu) was described as being deformed and ‘repulsive’. Unlike what we may see in the Muslim community today, they were not neglected or abused by their fellow Muslims, nor were they ostracised from the community. Instead, the Prophet made every effort to accommodate them and made them feel a part of the Ummah. For example, in the case of Abdullāh b. Ummmi Maktūm the Prophet gave him the responsibility of being one of two callers to prayer and made the congregationoal prayer obligatory upon him, and as a result he did not feel like an outcast. The Prophet (sallallāhu ‘alayhi wasallam) was even admonished by Allāh in the Qur’ān for unintentionally overlooking Abdullāh b. Ummi Maktūm in Makkah, so one can only imagine the sin for discriminating against disabled people. In the case of ‘Amr b. Jamūh, he was given permission by the Prophet (sallallāhu ‘alayhi wasallam) to fight on the battlefield, even though he was told it was not obligatory upon him to do so, as a result of his disability. Furthermore, the Sahāba made great efforts to take care of disabled members of their community and even competed with each other to do so. The Prophet (sallallāhu ‘alayhi wasallam) even stated that one of the best ways to draw closer to Allāh is to take care of, and help the mentally challenged and disabled within the community.\nTo further point out Islām’s stance towards disabled people, and the care and support they received when Muslims were the leading nation in the world, it is important to draw attention to one of the greatest Caliphs in Islamic history, ‘Umar b. ‘Abd al-‘Azīz (rahimahullāh). Whilst today the Muslim community lags behind the rest of the world in their treatment of the disabled, ‘Umar b. ‘Abd al-‘Azīz (rahimahullāh) established legislation that made it obligatory for the community to take care of disabled people. Such laws were so influential that they were embraced and implemented much later by the West, leading to laws such as the Equality Act 2010 in the UK, and have remained in place until today. Much of what is done for the disabled and the care given to them in the West today originated from the laws he pioneered, using the Qur’ān and Sunnah as a framework. Under his rule, the disabled were given a companion who would be responsible for them, in the same way care workers today are given the role of taking care of a disabled person. That was the condition of the Muslims during that period in Islamic history, a stark contrast to the ignorant, individualistic attitudes much of the Ummah has adopted today.\nIt is precisely because of this example set out for Muslims by these early Caliphs of Islām that a far greater effort needs to be made by the Muslim community to help children and adults with Intellectual Disabilities. This is not only a recommended act, but also a duty and obligation that Allāh has made incumbent upon all Muslims. A conscious effort also needs to be made to create awareness of Intellectual Disabilities within the Muslim community. This can be done in a number of ways, such as lobbying our Muslim leaders to highlight this issue during khutbahs, lectures and religious gatherings. However, the responsibility does not lie solely on the shoulders of leaders and Imāms, but it lies with the Muslim community as a whole. The more that Muslims speak about this issue with family and friends during social gatherings, the greater level of awareness it will bring. It cannot be reiterated enough that serious Intellectual Disabilities are nothing to be afraid of. They are conditions that Allāh has given to thousands of children and adults in the UK, and thus, the Muslim community should be the first to embrace, support and love them and their families. It is hoped that such changes in the outlook of the Muslim community will lead to greater levels of social acceptance and inclusion of individuals that, still unfortunately, remain on the fringes of the Muslim community today.\nNotes: Jahoda, A., & Markova, I. (2004). Coping with social stigma: People with intellectual disabilities moving from institutions and family home. Journal of Intellectual Disability Research, 48, 719–729.  Larkin, P., Jahoda, A., MacMahon, K., & Pert, C. (2012). Interpersonal sources of conflicts in young people with and without mild to moderate intellectual disability at transition from adolescence to adulthood. Journal of Applied Research in Intellectual Disability, 25, 29–38.  Werner, S., Corrigan, P., Ditchman, N., & Sokol, K. (2012). Stigma and intellectual disability: A review of related measures and future directions. Research in Developmental Disabilities, 33, 748-765.  Alexander, L. A., & Link, B. G. (2003). The impact of contact on stigmatising attitudes toward people with mental illness. Journal of Mental Health, 12(3), 271-289.  Goreczny, A. J., Bender, E. E., Caruso, G., & Feinstein, C. S. (2011). Attitudes towards individuals with disabilities: Results of a recent survey and implications of those results. Research in Developmental Disabilities, 32, 1596-1609.  Emerson, E. & Hatton C. (1999). Future trends in the ethnic composition of British society and among British citizens with learning disabilities. Tizard Learning Disability Review, 4, 28–31.  Patka, M., Keys, C. B., Henry, D. B. McDonald, K. E. (2013). Attitudes of Pakistani community members and staff towards people with intellectual disabilities. American Journal on Intellectual and Developmental Disabilities, 118, 32-43.  Croot, E. J., Grant, G., Cooper, C. L. & Mathers, N. (2008) Perceptions of the causes of childhood disability among Pakistani families living in the UK. Health and Social Care in the Community, 16 (6), 606-613.  World Health Organisation (1990). International Classification of Diseases and Related Health Problems (10th Revision). Geneva, World Health Organisation  Sahīh al-Bukhāri, Volume 7, Book 71, Number 582  http://www.ncld.org/types-learning-disabilities/treatments-therapies/cure-learning-disabilities-therapy-research  http://www.nydailynews.com/new-york/autistic-artist-stephen-wiltshire-drawing-new-york-city-memory-article-1.380539  https://www.youtube.com/watch?v=eXH–CmXgwI&feature=youtube_gdata_player  Ansari, Z. A. (2002). Parental acceptance–rejection of the disabled children in non-urban Pakistan. North American Journal of Psychology, 4(1), 121–128.  Dura-Vila, G., & Hodes, M. (2012). Ethnic factors in mental health service utilisation among people with intellectual disability in high-income countries: systematic review. Journal of Intellectual Disability Research, 56(9), 827-842.  Sahīh al-Bukhāri, Vol. 7, Book 70, Hadith 545  Al-Qur’ān 80:1-3  Al-Qur’ān 80:6  Sunan Abī Dāwūd and Ahmad  Al-Qur’ān 80:1-16  http://www.youtube.com/watch?v=9ZYdl2aRauo  Crone, P. (2005), Medieval Islamic Political Thought, Edinburgh University Press.  The Legacy of the Prophet in dealing with people with disabilities By Shaikh Ahmad Kutty']	['<urn:uuid:0c45a471-6ac9-452a-8f80-6f5d1a326d06>']	open-ended	direct	concise-and-natural	similar-to-document	single-doc	novice	2025-05-12T20:11:36.665104	9	85	2445
59	whiskey ships relation environment pollution effects	Lagavulin and Caol Ila, two whisky distilleries located on Islay, share similar production processes but differ in their aging practices - while Caol Ila ships their spirit to mainland Scotland, Lagavulin ages a significant amount on Islay with over 6,000 casks on property, 6,000 at Port Ellen Distillery, and 3,000 at Caol Ila. This shipping of spirits across waters relates to a broader environmental concern, as studies show that large vessels like ships contribute significantly to marine pollution. Ships release chemicals from sewage and other channels into oceans, while their noise affects marine animals like whales and dolphins, disrupting their communication patterns and causing changes in diving patterns and migrations.	"['In many ways, Lagavulin and Caol Ila are brothers. Both distilleries are owned by Diageo, located within a a few miles of each other, and use the exact same malt from the Port Ellen Maltings (peated to the exact same level). But like brothers, even though Lagavulin and Caol Ila may share the same parents, each distillery has its own unique style, focus, and path. While Caol Ila is a servant to the never-ending thirst of Johnnie Walker, who uses whisky from Caol Ila in many of its blends, Lagavulin operates outside the constraints of blend production.\nLocated on the coast of Kildalton, the southeastern edge of Islay, Lagavulin occupies an important historical space. The bay that fronts the distillery was a key shipping lane for the Lord of the Isles and the former site of Dunyvaig Castle. The distillery site is thought to have been an early place of brewing beer and ultimately distilling long before John Johnston founded the first legal distillery in 1816. Lagavulin is also known for housing the legendary Malt Mill Distillery built in 1908 by Peter Mackie. Malt Mill recently was a central character in Ken Loach’s film “The Angel’s Share” where one of the main characters goes in search of a priceless lost cask from the distillery.\nWhile there still is a final bottle of new make spirit from Malt Mill stored the Lagavulin historical archives, much of Peter Mackie’s Malt Mill whisky was ultimately used in blended whisky called Mackie’s Ancient Scotch. Lagavulin has taken over much of the old Malt Mill distillery with the exception of the floor malting room, which is used as one of the main concert halls for the Lagavulin Islay Jazz festival, held every September.\nAs with its brother, Caol Ila, Lagavulin starts with the exact same malted barley from Port Ellen Maltings. This malt is milled and separated into the husk, the grist, and the flour. The proportion of these is important and impacts how well the mash tun process can extract fermentable sugars from the malt. Malting barley encourages the grain to naturally unlock the starches it would use as the source to produce a barley plant. The process is halted by the drying the malt, which is also when the grain is peated. Milling the malt further breaks down the barriers around the starch so it can be turned into fermentable sugars.\nThe process of converting the grain starches into fermentable sugars happens in the mash tun. A mash tun is a giant stainless steel tank where the milled malt is combined with warm water, soaked, stirred, and drained. During this process enzymes from the outside of the husk of the grain convert the grist, which contains the starch, into a fermentable sugar called the wort. This process is repeated for each batch of grain to ensure that all the fermentable sugars are extracted. This process is also where the peat reek from the husk of the grain is transferred into the wort. Aside from converting starch into sugar, the mash tun process also creates a peaty barley tea which greatly impacts the final flavor of the whisky. While similar, the mash tun process at Lagavulin is different than at Caol Ila. Lagavulin’s mash tun is the smallest on the island of Islay and they run it with much longer, slower heat times (the process in whole runs about five hours). It’s here where the paths of the two brother distilleries begin to diverge.\nAfter the mash tun process, the wort is transferred into large wooden fermentation tanks called washbacks made from American oak. In the washback, liquid yeast is added and the mix is left to ferment. Lagavulin has one of the longer fermentation cycles, which runs 55 hours. At around 40 hours some of the natural bacteria which grows and is stored in the wood washbacks begins to interact with the wort. This natural bacteria helps deliver some of the citrus notes that you find in Lagavulin. The result of the fermentation process is a wash that is 8% alcohol. It’s often referred to as distiller’s beer, and although it’s not designed to be consumed, it is quite tasty. The wash has nice barley notes combined with clear citrus and nice smoke.\nThe wash is transferred into one of two wash stills where it’s very slowly distilled (a 10 1/2 hour process). Although the stills at Lagavulin are similar to those at Caol Ila, how they are used really helps dictate the final character of the whisky. At Lagavulin, the wash stills are filled to 95% of capacity which gives less contact between the alcohol vapors and the copper sides of the still. The copper in the still attracts some of the heavier elements from the vapor, and so the more contact a spirit has with vapor, the cleaner and softer it’s going to taste, and the less contact, the more congeners you can get. Lagavulin fills their stills to 95% while Caol Ila fills them to 50-60%, and that makes a huge difference in the final product. The ‘cut,’ or what’s taken out of the spirit still, is also wider at Lagavulin, with the heads and tails (in Scotland called foreshots and tails).\nHow and where Lagavulin is aged is different as well. Caol Ila ships their spirit to the mainland of Scotland to be put into barrels and aged, while Lagavulin ages a significant amount of their whisky on Islay. There are over 6,000 casks of Lagavulin aging on the property, 6,000 aging over at the old Port Ellen Distillery, and another 3,000 casks aging (ironically) at Caol Ila. There are more casks aging on the main island of Scotland, but a large portion is on Islay.\nLike many whisky producers, Lagavulin uses previously used bourbon barrels. Lagavulin cleans the barrels, fills them with grain spirit, and lets them set for a full three years. The grain spirit is dumped out and the new make Lagavulin is added and aged. A final bottle of Lagavulin will be a combination from these ‘first fill’ barrels and barrels that have previously held Lagavulin. While most Scottish whisky producers offer a 10 or 12 year whisky as their entry point, Lagavulin’s entry level whisky is 16 years old. Lagavulin does release a 12 year old whisky annually (typically around October) as a Limited Edition with the precise vintage of the whisky and at a higher proof (the 2011 was at 57%).\nAs their entry point offering, Lagavulin 16 is superb.The nose is amazing with honey, vanilla, malt, cereal grain, and dried plum. The entry is flavorful without the peat bomb that you get from many Islay whiskies. The opening is complex with toffee, caramel, dried plum, and slight smoke. The midplate features some nice rich red fruits along with clear cereal grains. The smokey peat really isn’t the star of the show until the finish where it balances superbly with the other flavors of the whisky. Lagavulin 16 is a fully integrated and balanced whisky that is a fantastic entry point for exploring Islay whisky.\nLagavulin has a number of other special releases including a Distiller’s Edition that is finished in Pedro Jimenez casks, and a special annual Jazz Festival release which is only available at the distillery during the Islay Jazz Festival every September. Lagavulin will also soon release a special 21 year release that ranks among the best whisky we’ve tried, however the price tag that goes along with it is absurdly well over $500 a bottle (350 pounds a bottle).\nIt’s is truly amazing that two distilleries, owned by the same company and using the exact same peated malt, can produce such dramatically different whiskies. Both Caol Ila and Lagavulin produce great whisky, but Lagavulin manages to do something really spectacular: they find the perfect balance of flavors and manage the peat smoke in a way where it’s an important part of the whisky, but doesn’t overpower it.', ""Traveling on a cruise ship sounds like a great idea for a vacation. However, what seems like an exciting journey has a massive effect on our environment.\nCruise ships are some of the largest ships in the world, and studies have shown that one cruise ship releases a carbon footprint greater than 12000 cars3. Cruise ships are also a major source of pollution in the marine environment. But just how bad are they?\nRead on as we examine the environmental impacts of cruise ships on our environment.\nThe cruise industry comprises businesses in the travel and tourism sector that facilitate voyages on large passenger ships. It involves cruise lines, cruise operators, and companies manufacturing cruise ships. The cruise industry also covers businesses that specialize in cruise entertainment.\nA cruise line is a company that operates cruise ships and sells cruises to customers. This is distinct from passenger ships and passenger lines that link destinations. Cruise lines offer round trips primarily focusing on pleasure, while passenger ships focus on travel from one location to another.\nCruise lines provide various cruise packages, including accommodation, food, and entertainment. Some of the biggest cruise companies include Carnival Corporation (including brands like Princess Cruise lines, Costa Cruises, Holland America Line, etc.), Royal Caribbean International, MSC Cruises, and Disney Cruise lines.\nOver the years, the cruise industry has grown, generating billions of dollars in revenue. However, the sector dipped during the global COVID-19 pandemic affecting tourism and the international shipping industry.\nIn 2022, the global cruise ship industry market was $7.67 billion and is expected to be worth $15.1 billion by 20284.\nMillions of passengers every year board cruise liners all over the world. Although passenger numbers took a dip during the global pandemic, cruise lovers have come back on board.\nHowever, the activities of the cruise industry have raised eyebrows among travelers and tourists. Many eco-conscious travelers wonder just how much cruise liners impact the environment.\nEnvironmental groups have described cruise ships as “floating cities'' that contribute massively to pollution. Several cruise ships have been caught dumping trash, sewage, and fuel into the ocean.\nHere are some major environmental impacts of cruise ships:\nCruise ships contribute significantly to the pollution of our environment. This includes air pollution, water pollution, sewage pollution, solid waste pollution, etc.\nFindings reveal that on a seven-day trip, passengers on an Antarctic cruise can release as much carbon emissions per traveler as the average European in an entire year. These greenhouse gas emissions, predominately a product of burning fuel to keep the cruise ships moving and powered, get into the atmosphere, reducing air quality. High levels of air pollution can also affect human health.\nCruise liners accommodate thousands of passengers and crew members.\nAccording to Friends of the Earth, the US Environmental Protection Agency estimates that a cruise ship of 3000 passengers and crew members generates about 21,000 gallons of sewage daily, an amount large enough to fill ten backyard swimming pools annually.\nWhereas cruise liners typically treat sewage on board, in some cases, they may be allowed to discharge treated sewage into the ocean, provided they comply with the regulations and the discharge occurs at a safe distance from shore.\nFurthermore, older ships may be equipped with lesser processing capability, with the Friends of the Earth's latest cruise ship report card scoring popular liners no higher than a “C.”\nSewage pollution can lead to infectious diseases and illnesses through contaminated water and seafood. Marine wildlife can also suffocate due to excess nitrogen and phosphorus from sewage entering the ocean.\nAccording to research, a 3000-passenger ship can generate up to around 50 tons of solid waste in a week1.\nThese solid waste materials include plastic, paper, aluminum, and other debris. These solid waste materials can choke and kill marine wildlife if they enter the oceans.\nGenerally, throwing solid waste into the ocean is illegal. However, some ships have been caught dumping trash into the ocean. Also, some ships gather recyclable materials, while others burn non-recyclable materials causing air pollution.\nAs these materials burn, they release harmful emissions like carbon dioxide, lead, and mercury, polluting the atmosphere and affecting human health.\nMore cruise ships on the sea mean more trouble for the animals underwater. Noises from moving ships travel as far as the ocean floors and reverberate back onto the surface. These vibrations affect the entire ecosystem of marine animals.\nResearch reveals that noise disturbances from ships affect how animals like whales and dolphins communicate.\nFindings suggest that cruise ship noises could affect the already endangered killer whales found near shipping lanes. Even a slight increase in sounds can make echolocation difficult for whales2.\nApart from a disruption in communication, noise pollution can also lead to a change in diving patterns, migrations, and panic responses.\nCruise ships also release chemicals from sewage and other channels, causing pollution. Toxic chemicals from daily operations, industrial products, and other substances find their way into the oceans, posing a threat to marine creatures.\nOils can also contribute to pollution. Cruise ships burn heavy fuel oils which contain harmful chemicals and substances like sulfur and heavy metals, which can leak into the oceans.\nIf a cruise ship has a faulty system or improper repair work, oils can leak from these areas and penetrate the oceans, causing pollution and threatening marine life.\nBallast water is essential for the safe operation of cruise liners. Ballast water helps to keep the ship floating in an upright and safe condition. It also gives the ship some stability as it maneuvers the oceans. However, while ballast water is crucial, it can also harm the environment.\nBallast water contains organisms like bacteria, eggs, microbes, small invertebrates, and other species.\nAs ships load and unload ballast water, these organisms get released into the local environment. The problem is that they can travel in the ship’s ballast to ecosystems where they might prove harmful. Under the right conditions, these organisms can flourish and threaten resident populations.\nOne example is the Zebra Mussels Invasion in the Great Lakes in Canada. These species are native to the Black and Caspian Seas in Europe but arrived in Canada due to a ship ballast water discharge. They proliferated, outnumbering local species like the native mussels. Feeding on the same food source, these introduced species disrupted the local food chain and hindered the growth and development of these local species.\nThe transfer of invasive species is one of the biggest threats to the ecological well-being of the planet. This is because it causes an imbalance in the coastal ecosystems.\nWater from showers, sinks, laundry, cleaning utensils, etc., are all classified under grey water. Cruise ships release large amounts of grey water from regular activities like bathing and laundry.\nSadly, the accumulation of grey water contains harsh chemicals, metals, and other particles that can pollute the oceans. Through grey water, detergents, pharmaceuticals, microbeads, oils, etc., can find their way into the sea. Not only does this cause pollution, but the waste produced can also poison marine life.\nMoreover, food waste from passengers, boats, and shipyard staff may also enter the ocean. Food waste contains chemicals unsuitable for the oceanic ecosystem and marine animals.\nIn 2018, a Holland America ship reportedly discharged 25,000 gallons of greywater into Glacier Bay National Park in Alaska and was hit with a fine of $17,000.\nCruise ships can be a significant threat to Coral reefs. Tourist vessels and anchoring of cruise ships on areas of coral reefs around the world have led to several destructive incidents.\nIn 2017, a British-owned cruise ship crashed into a coral reef, damaging approximately 13,500 square meters of coral reef in Indonesia.\nBesides pollution and damage to our ecosystem, these large cruise liners can also threaten aquatic life. These ships are responsible for injuring marine animals. Also, solid waste like glass, plastic, etc., ends up in the digestive systems of these marine animals, leading to their death.\nThe cruise industry negatively impacts our water, air, coastal communities, and fragile habitats. Although various environmental regulations exist for cruise lines worldwide, compliance is only sometimes achieved.\nFriends of the Earth, an environmental group, shared a Cruise Report Card, which took a controversial turn. The group gave each cruise line a grade from A-F based on these elements of sustainability:\nFriends of the Earth also goes ahead to provide more specific information about the environmental impact of some of the top cruise lines in the world:\nCarnival Corporation cruise line is one of the largest cruise companies with ten cruise lines.\nUnfortunately, Carnival Corporation is notorious for violating environmental regulations.\nAccording to Friends of the Earth, this cruise line was hit with a fine of $40 million for illegal waste disposal and put on federal criminal probation.\nCarnival Corporation was charged with dumping food and plastic waste in Bahamian Waters, illegally dumpling gallons of wastewater in the Glacier Bay National Park in Alaska, and releasing over 11,000 gallons of food waste and 500,000 gallons of sewage illegally, amongst other charges.\nRoyal Caribbean is also one of the world's largest cruise companies and is notorious for paying criminal fines.\nFindings from FOE state that this cruise company was forced to pay a fine of $18 million for 21 federal colonies in 1999. This was due to the dumping of waste oil and chemical waters in coastal waters.\nAlthough this cruise company has cruise ships with scrubbers that help reduce air pollution and get around harmful greenhouse gas emissions, it is only a case of converting air pollution into water pollution.\nDisney is one of the few cruise companies open about their environmental impact. The cruise line efficiently utilizes fuel with a 0.1% sulfur content, ultimately reducing its carbon footprint.\nUnfortunately, Disney isn't perfect, either. The cruise company plans to work on a massive cruise ship port at Lighthouse Point in the Bahamas. Community groups within the region oppose this port as it will cause harm to their marine ecosystems.\nThe Bahamian Island of Eleuthera has over 200 bird species, four endemic plant species, beautiful clear blue water, lemon sharks, sea turtles, etc. The introduction of Lighthouse Point could bring many tourists and travelers, ultimately adding water, air, and noise pollution to this unscathed region.\nMSC Cruises is part of the Hydrogen Council - a global initiative working toward the use of hydrogen fuel. The company has also launched the MSC World Europa ship, which LNG will power. The ship sails with less harmful emissions like carbon dioxide, sulfur oxide, etc.\nHowever, MSC Cruises still has much to do to reduce its environmental footprint.\nWith 19 ships in its fleet, 15 use scrubbers that turn air pollution into water pollution, only 12 have advanced sewage treatment systems installed, six travel to ports with shoreside power, and only 8 of their ships have shoreside plug-in capability.\nThe cruise industry has had a negative environmental impact for many decades, putting human health, coastal communities, and our environment at risk.\nToday, the cruise industry is making environmental efforts to reduce its carbon footprint and create a positive environmental change.\nThese include using Liquefied Natural Gas (LNG) ships, exhaust gas cleaning systems, and other new technologies. Here are some ways the cruise industry is going green:\nCruise ships release large amounts of harmful gas emissions, contributing significantly to air pollution and global warming.\nAir pollutants like nitrogen oxide and sulfur from cruise ships pollute the air and contribute to respiratory problems. To reduce air pollution and improve air quality, cruise lines have turned to exhaust gas cleaning systems (EGCS).\nManufacturers designed the EGCS to reduce sulfur oxide and nitrogen levels. They remove these harmful gases from the ship's engine exhaust.\nAlso, The International Maritime Organization (IMO) 2020 established a regulation allowing only a sulfur content of only 0.5% in marine fuel to ships worldwide.\nCruise lines can meet this requirement by going for cleaner fuel alternatives like Liquified Natural Gas (LNG) or using EGCS (scrubbers).\nHowever, some environmental groups have raised concerns about scrubbers that discharge wastewater back into the sea, turning air pollution into water pollution.\nA large cruise ship can generate tons of waste, polluting our environment. Cruise liners have begun to reduce or ban the use of single-use plastics to reduce plastic waste.\nMost ships have recycling bins that store solid waste materials like plastic, glass, and cardboard until they get to the next port and offload.\nAs we already know, the cruise industry has had major negative environmental and human health impacts. But it's not all doomsday for the cruise world.\nMany shipping lines are now taking more significant steps toward more sustainable cruising. Some cruise operators now build ships that run on LNG. Also, some cruise shipping lines invest in greener fuels like hydrogen fuel cells and biofuels.\nWhile the cruise travel industry has a long way to go, some cruise lines are working hard to make cruise travel significantly sustainable. Here are some eco-friendly cruise shipping lines leading the pack:\nHurtigruten is a Norwegian travel company that incorporates sustainability at its core. The company aims to have the world's first zero-emissions cruise ship by 2030. The company has since ditched unsustainable fuel sources for greener alternatives like biofuels and marine gas oil.\nHurtigruten launched the world's first hybrid battery electric-powered cruise ship in 2019 and aims to convert the seven ships of the Norwegian Coastal Express fleet to hybrid battery power or biofuels (which partly comes from dead fish).\nThe company also excels in single-use plastic reduction, as it has been scrapped from its operations altogether. They select their suppliers based on the merits of their sustainability.\nOn board, the company only serves food locally and sustainably sourced. In addition, crew members have uniforms made from recycled fishing nets.\nPonant is a French cruise shipping line that carbon offsets 100% of all its emissions. The 245-passenger Le Commandant Charcot, Ponant’s hybrid expedition ship, is one of the most eco-friendly ships, running on LNG and electric battery power.\nThe company has also stopped the use of single-use plastics. They take sustainable cruise tourism seriously and carefully plan all itineraries with local communities.\nPonant is also the first cruise company to get a Green Marine certification which they award to companies that commit to minimizing their environmental impact.\nHavila Voyages, a Norwegian cruise ship, has launched two of four of its planned hybrid ships in 2022, which use some of the biggest batteries combined with LNG fuel.\nWith this, the ships can travel for up to four hours on the sea without emissions or noise before recharging at the next port cities.\nThe company also recharges its batteries sustainably using clean hydropower energy from local grids. They also aim to run emission-free with vessels designed to switch entirely to hydrogen power as technology improves.\nEver thought of a cruise that operates on wind power? Star Clippers, a Monaco-based company, takes sustainability to a unique level with tall cruise ships that usually operate on wind power. At other times, their ships use low-sulfur gas oil.\nStar Clippers is also one of the first cruise lines with a Pura Vida Pledge certification approved by the Costa Rican Tourism Board.\nA large part of reducing the cruise industry's environmental impact starts with cruise lines keeping specific environmental standards. However, you can do your part as you vacation on the high seas by making small changes.\nHere are some ways you can reduce your environmental impact while on a cruise:\nRelated: 15 Best Tips For Eco-Friendly Travel & Sustainable Adventures.\nLarge amounts of harmful gas emissions and environmental pollution can be attributed to cruise ships. This is not to mention the waste produced by passengers and the cruise industry. All these activities from the cruise industry hurt our environment.\nMany cruise lines claim to be taking steps towards more sustainable travel. However, not many of these cruise shipping lines are taking steps towards effecting any significant change.\nA few companies are leading the way to eco-friendly cruising thanks to technological advancements.\nButt, Nickie. (2007). The impact of cruise ship generated waste on home ports and ports of call: A study of Southampton. Marine Policy. 31. 591-598. 10.1016/j.marpol.2007.03.002.\nVeirs S, Veirs V, Wood JD. 2016. Ship noise extends to frequencies used for echolocation by endangered killer whales. PeerJ 4:e1657\nJosep Lloret, Arnau Carreño, Hrvoje Carić, Joan San, Lora E. Fleming. Environmental and human health impacts of cruise tourism: A review. Marine Pollution Bulletin, 2021; 173: 112979 DOI: 10.1016/j.marpolbul.2021.112979\nCruise Market Size, Share & Growth Report, 2022-2028. (n.d.) Grand View Research\nJen’s a passionate environmentalist and sustainability expert. With a science degree from Babcock University Jen loves applying her research skills to craft editorial that connects with our global changemaker and readership audiences centered around topics including zero waste, sustainability, climate change, and biodiversity.\nElsewhere Jen’s interests include the role that future technology and data have in helping us solve some of the planet’s biggest challenges.""]"	['<urn:uuid:057fa9eb-c199-4dc7-b55b-9882d28703d7>', '<urn:uuid:50996485-733b-449a-a2ac-7723b6c80e52>']	open-ended	direct	short-search-query	distant-from-document	multi-aspect	novice	2025-05-12T20:11:36.665104	6	110	4142
60	What museums can you visit in Greece to see ancient artifacts?	In Greece, there are several notable museums showcasing ancient artifacts. In Santorini, the Archaeological Museum in Fira displays tools, ceramics, weaponry, marble statues, and Minoan frescoes found across the island. In Kastelli Kissamos, the Archaeological Museum, housed in a Venetian monument, exhibits Minoan, Hellenistic and Roman items including household items, pottery, coins, jewelry, gravestones, sculptures, and mosaics. The Archaeological Museum of Chania, located in a former Venetian monastery, features artifacts from early Minoan to Roman periods, including pottery, glass, coins, jewelry, metal ware, sculptures, and a notable clay tablet with Linear A script from 1450 BC.	['The island of Santorini has a reputation of being one of the most beautiful of the Aegean, with charming villages perched on steep cliffs from which you can enjoy astounding views of the sunset over the azure waters. This has made it one of the most sought-after touristic destinations, particularly popular among those who want to have their wedding in an exotic and romantic location. However, this has not managed to spoil its pristine natural beauty yet. If you happen to travel there, either as a wedding guest or on your own, make sure not to leave without visiting several of the island’s spectacular natural attractions.\nHow to tour the Santorini island in Greece\nIf you go to Santorini, try not to stay always in the same location, no matter how luxurious or attractive you think it is. There are several different villages on the island, and each one of them has its particular charms, making a tour of the island a must. There is a cable car system designed to get visitors to the higher parts of the island, where the most stunning views can be observed. However, they can get pretty crowded when a large cruise ship is on harbour. Although the journey itself takes about 3 minutes, it can take much longer to get a seat in the first place, so take this into consideration if you are in a hurry. A fun alternative to the cable car is a mule ride. The locals used mules to carry everything up and down. But today the mules are part of the native charm of the island and this ride may as well prove to be one of the highlights of your trip, if you are willing to cope with the smells and the occasional flies.\nVisiting archaeological sites\nSantorini has a rich past and a visit to one of the archaeological sites scattered throughout the island is highly recommended for those travellers interested in getting to know the place a bit better. Two of the most spectacular ones are Akrotiri, dating back to Neolithic times, and Ancient Thera, the classical ruins of the island. It is considered to be the most important prehistoric settlement found anywhere in the Eastern Mediterranean, due to excellent state of preservation and the amount of artefacts found on site. Ancient Thera lies at the top of the embattled peak of Mesa Vouno, which is 396 meters above the sea level and belongs to the pre volcano core of the island. The views from here are spectacular as you may expect, and the presence of the atmospheric classical ruins makes the trip an unforgettable cultural and sightseeing experience. If what you see on site tickles your curiosity, you can learn more by visiting the Archeological Museum in Fira, where many of the ancient artefacts found scattered across the island are on display, allowing you to put what you have seen into context. These include tools, ceramics, , weaponry, marble statues and even some frescoes from the Minoan age.', 'Eduardo Paolozzi is all over London; from the mosaics at Tottenham Court Road tube station to the sculpted head outside The Design Museum, colossal sculptures outside The British Library and on Royal Victoria Dock, and abstract pieces in Kew Gardens and Pimlico amongst others. It seems almost overdue that a London Gallery should dedicate an exhibition to the irreverent artists’ works – and Whitechapel Gallery have filled that void collating over 250 of Paolozzi’s artworks in their current retrospective. The ground floor focusses on his early career in London and Paris and his experimentation with various mediums as industrial bronze sculptures are displayed alongside pop-art inspired collages, screen-prints, tapestries and textiles, and moving film. Despite this diversity constant themes do emerge, evident in his fascination with pattern, layering and texture – and as the ground floor galleries come to an end, an inimitable Paolozzian style full of graphic prints and geometric designs emerges. His evolution as an artist is focussed on in the upper floor galleries which explore later developmental pieces in chrome and a playfulness with reflective surfaces and mirrors. It then goes on to draw out his obsession with the creative process itself, and it is interesting to view similar shapes through both two dimensional sketches and prints and three dimensional sculptures sharing the same space. Hints of the artist as a person – and indeed as a rebel – are also present in ‘Avant Garde?’ where each letter of the term is filled with a colourful cartoon figure, and ‘Jeepers Creepers’ which pokes fun at artistic terminology by featuring a row of plaster clowns each labelled with a different term. Iconic pieces mix with lesser known experiments, and the exhibition closes with the original sketches for the infamous tube mosaics. Get over to east London before 14th May to catch this exhibition and appreciate Paolozzi’s fun, colourful and incredibly innovative contributions to 20th century art!\nApproximately 40 kilometres west of Chania old town, you’ll find the town of Kastelli Kissamos. Despite being smaller, it is no less rich historically, and its name alone bears both the ancient Greek (Kastelli) and Venetian (Kissamos) monikers. A narrow road littered with tavernas, traditional fishmongers and cafés leads to a square where The Archaeological Museum is located, in an imposing repurposed Venetian monument known as Diikitirio – ‘the Headquarters’. The museum focusses on the areas’ Minoan, Hellenistic and Roman periods and displays household items, pottery, coins, jewellery, gravestones (stele), relief sculptures, marble free standing sculptures and mosaics. Minoan artefacts from excavations at nearby Nopigia which date back to 9th – 8th century BC dominate the opening gallery, and the historical development of western city-states in Crete is explained through the evolution of these objects from primitive Minoan artefacts onto more advanced examples from the Hellenistic era (4th – 1st century BC). This development is evident in one of my favourite items on display in the museum; a Hellenistic marble sculpture of a Satyr in which the sculptor has managed to capture the impish nature of the subject to perfection. As you move to the second floor, a small excavation taking place under the stairs of the building itself, highlights how inescapable archaeology is in this area! The second floor is devoted to findings from Kissamos, and houses two stunning floor mosaics from local Greco-Roman urban villas. The first is huge measuring 9.7 metres by 8 metres and features Dionysus surrounded by hunting and drinking scenes associated with Dionysiac worship, and the second depicting Horae and the four seasons is more humble in scale but in perfect condition. Despite only stopping in Kissamos to buy a drink, I’m so pleased I did, and got to experience another archaeology museum putting local history in the limelight with some outstanding finds.\nA sense of history pervades Chania old town; layer upon layer from early Minoan ruins to later Classical Greek and Roman archaeological sites, Byzantine remains, the Venetian lighthouse and shipyard buildings, wonderful examples of Ottoman architecture, as well as evidence of the destruction of World War II all survive. In the centre of all this you’ll find The Archaeological Museum of Chania, aptly situated in a stunning stone building and former Venetian monastery of St. Francis. The museum focusses on the city’s earliest Minoan civilisation through to the Roman period – comprising pottery, glass, coins, jewellery, metal ware, sculpture and mosaics. The vast majority of finds come from excavations in the city itself or nearby, which helps contextualise and humanise the artefacts on display and offers visitors a sense of where and how these items were used by people thousands of years ago. As you explore under each archway numerous standout antiquities can be seen in glass cabinets, notably a clay tablet dating back to 1450 BC inscribed with Linear A script (an early Minoan text academics have still not deciphered), decorative gold disks from a female burial site, as well as an array of seal stones offering lucid images and comprehension of each era. Outside of the display cases you can find numerous painted clay sarcophagi from cemeteries across Western Crete, stone stele (grave stones), a mosaic floor depicting Dionysos and Ariadne from the 3rd century AD, and a marble bust of the Roman emperor Hadrian. A final treat is provided in the small garden which houses an unusual octagonal ablution fountain from when the building was turned into a mosque during the Ottoman period. Costing just two euros admission, the museum not only provided much needed relief from the Cretan afternoon sun but also offered a fascinating insight into Chania’s rich and unbroken past.']	['<urn:uuid:474cc17a-aebc-4414-b1d8-b272b7af323c>', '<urn:uuid:66b0e81a-5344-4e53-893f-77f74df52596>']	open-ended	direct	concise-and-natural	similar-to-document	three-doc	novice	2025-05-12T20:11:36.665104	11	96	1437
61	how many african migrants died trying reach canary islands from senegal 2006	In 2006, an estimated 6,000 West Africans lost their lives attempting to pursue a better future in Europe by traveling to the Canary Islands.	['Mask Magazine October 2015 #21 | Cover Photo Kukka Ranta\nOn the cover Palestinian girl reading in occupied East Jerusalem after her house was bulldozed to the ground by Israeli soldiers. Photo by Finnish freelance journalist and photographer Kukka Ranta. Interview with photographer to follow.\nGlobal Initiative Against Transnational Organized Crime | 8.5.2015\nPhoto credits and research: Kukka Ranta.\nThe populations of West Africa, home to one of the world’s most resource-rich marine environments, is suffering widespread poverty and a paucity of legitimate livelihoods in part due to high levels of foreign illegal fishing and overfishing.\nWhen local fish stocks collapsed in Senegal in 2005, about 5,000 West Africans fled poverty to the Canary Islands in wooden handmade boats that the fishermen couldn’t afford to use commercially anymore. The following year, that number climbed to more than 31,000 migrants. In 2006, an estimated 6,000 West Africans lost their lives attempting to pursue a better future in Europe.\nMost of these migrants were from Senegal and Mauritania, the countries with which the EU has its largest and oldest fisheries agreements. The roots of the current illicit migration crisis in the Mediterranean is, in many ways, a crisis of Europe’s own making.\nWest Africa is estimated to have the highest levels of illegal, unreported and unregulated (IUU) fishing on Earth. Foreign vessels are taking advantage of some of the world’s poorest countries, which can’t afford to guard their own territorial waters and where the corruption index is often among the highest in the world.\nSince the world’s leading fishing powers have emptied their own waters, the problem of industrial overfishing is being exported to distant seas. The European Union made its first bilateral fishing agreement in Africa with Senegal in 1979, and soon afterwards Chinese trawlers and other Asian vessels entered West African waters, many of them operating illegally. Many of the vessels involved in illegal activities in West Africa are operating under flags of convenience. According to the Environmental Justice Foundation, a significant number of these vessels are originally owned by European companies.\nWest African coastal states are losing $1.3 billion annually and 37 % of their annual catch to IUU fishing. Most of the illegally-caught fish is taken to the EU and China, the world’s biggest fish markets, where demand is constantly growing. At the same time, industrial overfishing is destroying the livelihoods and food security of some of the world’s poorest people, forcing them to seek new, and hopefully more secure futures elsewhere.\nThe Black Fish & Global Initiative Against Transnational Organized Crime\nApril 2015 | Cover Photo by Kukka Ranta\nPublished at the 13th United Nations Congress on Crime Prevention and Criminal Justice in Doha, Qatar, 12-19 April 2015.\nCover photo and photography illustration partly by Kukka Ranta.\nEuropean Centre for Development Policy Management 19.2.2015 | Kukka Ranta\nSenegal was the first sub-Saharan African nation to sign a fisheries agreement with European Community back in 1979. The agreement was not renewed in 2006 after the local fish stocks collapsed – a result of massive overfishing by European and other foreign vessels in the region.\nAfter an eight-year pause, the EU and Senegal have recently signed a new five-year Fisheries Partnership Agreement starting in 2015. According to the agreement, up to 38 EU boats catching mainly tuna enter Senegalese waters in return for a EUR 8,69 million EU payment.\nLocal fishermen were not included in the negotiations and they have strongly opposed the agreement along with Greenpeace. By looking back to the recent history can one find a better understanding why?\nThe Reality for Senegalese Fishermen\nAccording to Senegalese fishermen I interviewed during the winter of 2011-2012, in the 1990s there was enough fish for everyone. You could get a decent catch within a few hours just five to ten kilometers offshore.\nBy 2000, the fishermen began noticing the alarming disappearance of local fish population. Now fishermen in Senegal must travel at least forty kilometers out to sea, which means more fuel costs but usually less incomes with dramatically declined catches.\nBy 2005 the incomes of local fishermen crashed and centuries-old fishing beaches began to fill with deserted boats. Every day local fishermen watched European and Asian trawlers ploughing the coastline. Handmade wooden boats can’t compete with these subsidised industrial vessels.\nAdded to this, local fishermen have often been forced to turn back their boats because of vast rafts of bycatch by foreign vessels. Tons of unwanted or juvenile fish, dolphins, sharks and turtles, are often dumped back in the sea already dead. Bycatch in West Africa is estimated to be at its worst 75 percent from the total catch.\nFishing now employs some 600,000 or nearly one million Senegalese when you take into account all the entire production chain. In comparison, the European fisheries sector generates about 139,000 full-time jobs, mainly in Spain, Italy, Greece, Portugal and France.\nMany Senegalese fish processing factories have recently had to close down because of insufficient catch. Clients from the EU and Asian markets are disappearing, and when local factories close down, everyone in the local fisheries sector loses.\nThe annual consumption of fish continues to increase in most continents, especially in the largest fish markets like the EU and China. But in sub-Saharan Africa consumption is declining. This may have a knock on effect on nutrition security in a region with a fast growing population, as it is represents a vital source of animal protein.\nSenegalese fisher families now have to survive on one or two meals a day, depending on what the sea provides. If there is no fish, many are forced to eat only sugared rice. Fewer families can afford to educate their children or cover medical expenses. For many, poverty has become a self-perpetuating cycle.\nDevelopments in EU Policy\nWhile the EU was renewing its Common Fisheries Policy in 2002, the World Wildlife Fund reported a 50% decline of deep-sea fish stocks in West Africa. But, from the European side there was not enough political will to change the course to reflect on the impact of their policies in West Africa.\nNow many overfished species like shrimps, cephalopods and small pelagic species like sardine, sardinella and horse mackerel are being left out from the EU-Senegal Fisheries Partnership agreement, as an improvement in sustainability.\nBut the problem is the use of the Fish Aggregation Devices (FADs) in EU tuna seiners that cause high amount of bycatch. Also there are two bottom trawlers targeting demersal hake, regardless the CRODT – Oceanographic Research Center recommend to limit fishing that overfished stock in the inter-ministerial council.\nAccording to the European Union’s new common fisheries policy (CPF) agreed in 2013, one core principle is a ban on discarding fish at sea, which will be set in EU waters starting gradually from 2015. But the discard ban is only applicable in EU waters, not in the territorial waters of Africa.\nWhere Coherence in Fisheries Policies Matters for Development, and Life\nWhen the fish stocks collapsed in 2005, around 5,000 West Africans fled poverty in wooden boats to the Canary Islands, with a hope of better future in Europe. That number rose to over 31,000 in a year, some 6,000 people drowned in the Atlantic Ocean.\nMost of these migrants were from Senegal and Mauritania, the EU ’s two biggest fisheries agreement partners. When there is a total collapse of fish stocks, it causes an eco catastrophe and destroys food security and livelihoods for millions of the world’s poorest people. Do we ever learn that bad politics and the endless contest for markets is no good for our collective interest and the common good?\nKukka Ranta is a PhD Candidate and Researcher at the University of Helsinki, Investigative Journalist, Nonfiction Author and Photojournalist from Finland.\nThe views expressed here are those of the author and not necessarily those of ECDPM\nPhoto Courtesy of Kukka Ranta\nAfrica-Europe Relations #Post2015\nA Blog on Africa – Europe Relations —- #AfricaEU2015 #EYD2015 #Post2015\nIs illegal fishing a form of organised crime? In which circumstances should it be treated as such? What approaches are necessary to tackle it? These are some of the main questions driving new research carried out by The Black Fish in partnership with the Global Initiative Against Transnational Organized Crime, investigating the links between illegal fishing and organised crime.Photo by Kukka Ranta\nA consultation draft of a new report on the issue was presented at an expert seminar on organised crime and sustainable development, held last week at the Permanent Mission of the Kingdom of the Netherlands to the UN in New York.\nAccording to the UN Food and Agriculture Organization (FAO) a major threat to marine life is illegal, unreported and unregulated fishing with annual global catch estimated at $10-30 billion (USD). In 2005 the FAO estimated that 75% of the world’s fish stocks were fully exploited, overexploited or depleted. In Europe, 85% of fish stocks are in trouble and roughly half of fish products traded through Europe is believed to have illegal origins.\n“A comprehensive recognition of the various drivers behind illegal fishing is needed in order to find the right tools to stop transnational criminal organisations from destroying our oceans. This research plays a major role in that effort.” said Wietse van der Werf, International Director at The Black Fish.\nThe upcoming report, which is set to be published in April, will highlight how the highly organised nature of illegal fishing operations justify treating this activity as a form of organized crime. Illegal fishing is also highly transnational in its scope and is supported by a wide range of illicit activity, including money and fish laundering, tax evasion, fraud, corruption, bribary and violence. Furthermore, illegal fishing operations have also been found to be linked to other forms of organised crime, including drug smuggling, human trafficking and forced labour.\nVan der Werf: “Our work is uncovering some shocking facts about the dark side of the fishing industry. Case studies focused on different forms of transnational fishing crime have come from all over the world, including Europe and the US.”\nLast week’s seminar brought together law enforcement and development specialists to discuss how organised crime obstructs good implementation of the United Nation’s Sustainable Development Goals. Mark Shaw, one of the founders of the Global Initiative talked about the need for the international community to act fast.\n“Lack of cooperation was one of the main reasons to start this debate”, said Shaw on the topic, which was recently highlighted at the UN level. “The real challenge is how to make progress in multiple sectors. The debate is just starting and there is a lot of work to be done.”\nKukka Ranta, doctoral student at CRADLE, was granted by the UniPID and the Finnish Society for Development Research the 2014 Master’s Award in Development Studies 2nd place for her thesis: Robbed Sea – The consequences of the EU fisheries agreements for the Food Security and Livelihood of local fishermen in Senegal, completed at the University of Helsinki, Department of Political and Economic Studies.\nIn a picture (left) 2. awarded Kukka Ranta (University of Helsinki, development studies), emeritus professor Rauni Räsänen, 3. awarded Lauri Heimo (University of Tampere, social policy), emeritus professor Reijo E. Heinonen, 1. awarded Henry Salas Lazo (University of Helsinki, Latin American studies) and emeritus professor Olavi Luukkanen (photo: Tommy Standun).\nThe selection board was comprised of emeritus professor Rauni Räsänen, emeritus professors Reijo E. Heinonen and Olavi Luukkanen. The board gave special attention to the societal applicability and impact, and innovativeness of the work:\n“Ranta’s study reveals how the international politics on fishing also have an impact on emigration, on understanding environmental sustainability and on reaching the Millennium Development Goals. In media and immigration studies, the focus has usually been on the effects of immigration on societies and countries which have not been able to assimilate newcomers into their social environment. Ranta, on the contrary, examines the reasons behind emigration from the viewpoint of unemployed Senegalese fishers. She gives a voice to the “voiceless subalterns” and points out their lack of democratic influence in issues in what were formerly their own fishing waters. The ethnological method applied has demanded intensive commitment to the life situations of the fishers and innovativeness in interviewing. Ranta’s study highlights the importance of intercultural dialogue and necessity of fieldwork. For this positive endeavour, Ranta’s study delivers an outstanding qualification.”\nKukka Ranta’s earlier non-fiction book Kalavale about overfishing written with Emma Kari was partly popularized from Ranta’s academic research data. The book was awarded with Kelpoa kehitystä (Good Development) prize in 2012 by Kehys for the significant work in promoting policy coherence for development. The book can be downloaded for free. Her other publications, articles and photo reports about overfishing and immigration can be seen from her website.\nMore information: www.unipid.fi']	['<urn:uuid:351b47f6-cd85-40e5-ae5b-0efe31f089af>']	factoid	with-premise	long-search-query	distant-from-document	single-doc	novice	2025-05-12T20:11:36.665104	12	24	2131
62	best mixing temperature process cement hardening cold weather effects	The temperature significantly affects cement hardening. In cold weather below 5 degrees Celsius, the hydration process slows down significantly, and below zero degrees, water may freeze, stopping the setting process completely. This leads to increased porosity and decreased strength. To ensure proper hardening, concrete should be mixed at optimal temperatures. In hot and dry conditions, concrete should be covered with plastic sheets after initial firmness, while in cold weather, it should be covered with blankets to prevent freezing. Chemical accelerators can be added to speed up the curing process in cold conditions.	['The need for accuracy in your cement mix ratio hinges on the durability of your concrete. Getting your cement mix ratio right and knowing how to mix the concrete correctly helps to ensure your job will last long.\nWhat Is Concrete?\nA building material which comprises the mixture of cement, gravel or broken stones and sand with water and gets hardened with time is referred to as concrete. Its vast application in construction work makes it essential to understand the recommended quantities of the materials needed in the concrete as found appropriate for a particular use.\nIn this respite, it is essential to consider the possible strengths of concrete which range from C40 down to C10. C40 is regarded as the strongest possible and C10 is regarded as the least strong.\nWhat’s The Best Concrete Mix Ratio For General Domestic Jobs?\nThe C20 concrete mix is a medium-strength concrete recommended for basic household constructions such as retention of posts, garden walkways and many more. This cement and sand mix is, however, not recommended for use in constructing a foundation and other construction works that would need super resilient concrete.\nIf you have any major structural adjustments or construction project, the architect, quantity surveyor or a building control officer should recommend the concrete strength to be used. This will be based on some parameters they will have to explore.\nThe appropriate ratio for C20 includes the following:\n- Cement – 1 part\n- Sand (fine aggregate) – 2 parts\n- Gravel (coarse aggregate) – 4 parts\nWhen premixed ballast is required, then 6 parts are required for 1 part of cement.\nIdeally, the water to be used should be 55 percent of the net weight of the cement if all other materials will be delivered absolutely dry. However, in reality, the materials are usually damp, especially the sand. You may then have to reduce the quantity of water added.\nFixing a small fault in your fence definitely does not need up to a bag for the cement mix. So it may be a waste if you will not be using what is left anytime soon. Check the DIY shed in your neighbourhood if they have ready mixed mortal stock of DIY concretes which may be appropriate for the repair.\nWhat Are The Precautions You Should Take?\nCement is not the best of powders for your skin. So, you have to be mindful while working with it. You should wear protective clothing and make use of appropriate materials.\nEnsure you take adequate precautions while adding water because the volume of water to be added is an estimation.\nWhat Other Strengths And Types Of Concrete Mix Can You Have?\nVarious strengths and types of concrete mix are available based on the recommended use. The role the concrete is to serve is the most important factor considered before a type of concrete can be chosen.\n- C10 Concrete Mix\n- C15 Concrete Mix\n- C25 Concrete Mix\n- C30 Concrete Mix\n- C35 Concrete Mix\n- C40 Concrete Mix\nWhat materials do you need to make a concrete mix?\n- Aggregates: You can request a lorry to deliver aggregates to you after buying it loose. Another option is aggregate that is ready-mixed cement, which can be bought from builders’ merchants. It is generally cheaper if you buy it loose. But, check with your local merchants. They may also have great deals for you. A ballast is a mixed aggregate which contains small stones and sharp (or grit) sand.\n- Cement: Bags of cement come in either 25kg or 50kg and can be gotten from builders’ merchants. Cement is made from limestone, which undergoes many steps of processing.\n- Water: This is indispensable in making concrete. Despite its importance, it is not needed in abundance. Only the calculated volume of water is needed to ensure the concrete comes out strong.\nHow To Mix Concrete\nProper mixture and combination of materials are essential to have good concrete. It is kind of tricky when ensuring appropriate quantities of materials are used. The ballast or aggregate will heap well on the shovel but the cement will not because it will slide down the heap. Meanwhile, 1 part of ballast has to be the same as 1 part of cement.\nIt is then recommended to use a bucket or headpan instead of the shovel to prevent errors in measurement.\nMethods Of Mixing Concrete\nMixing can be done manually with your hands or a concrete mixer. If you are going to be preparing more than one batch of concrete, ensure that they are equally wet. Otherwise, the concrete will not get dry evenly. This may leave some cracks and shrinkage in your concrete.\n1. Mixing Concrete By Hand\nThis is okay if it’s a small quantity of cement. Otherwise, you may not be able to mix it thoroughly enough, and the quality of the concrete may turn out bad. So, you need to know how to mix cement.\nIf you ever have to mix with your hands, you should follow these steps:\ni. Cover up your work area with a tarp or any other suitable material. You may also use a mixing board that can be reused. You can also improvise with a piece of wood.\nii. As explained earlier, deploy the use of the ratios and consider the precautions. In the recommended ratio, put the materials at the centre of the board or on the tarp.\niii. Mix the aggregates and the cement thoroughly till you have a uniform product – the aggregate blended well with the cement. Scoop your pile of the mix on another side of the board. Break lumps while you work your way through the pile, gently heaping a scoop over the other.\niv. Collect the mixture together in the centre of your work area and then scoop out the required amount of cement.\nv. Sprinkle around a third of the cement over the top of your pile and then mix by folding as you did above until cement is mixed thoroughly and then repeat the process another two times for the remaining two-thirds. If there are any lumps, break them up finely. The most important part of this is ensuring the cement is evenly mixed with the sand and aggregate.\nvi. Spread out the pile to make a hollow at the centre. The hollow area is to hold the water you are to use. Just only a third of the estimated water to be used should be added.\nvii. Working around the heap, slowly scoop from the outer side of the mix into your hollow crater with water. Mix it well with the water by running your shovel through or hand trowel.\nviii. Make it into a pile again and have your hollow at the centre, add the second third of water then work your way around it like the first time.\nix. Sprinkle the remaining water over the mix, turning it inside out as you do so. This should be done until the “molten” mix is uniformly saturated with water.\n2. How To Mix Concrete With Cement Mixers\nYou can hire a cement mixer. If you do a lot of cement mixing, it could be more economical in the long run to buy one. If you no longer need the mixer, you can sell it.\nA mixer gives the best quality of mixes as it thoroughly churns the concrete to give a uniform product with no stress. Mixers are available in different volume capacities.\nTo use the mixer, you need to take the following steps:\ni. To prevent messing up the floor, you should cover the area with a tarp and position the mixer in the middle of the area. Connect it to a power source using a suitable extension cable. Turn it on to ensure it is working.\nii. Start by filling with three-quarters of the water required for the concrete, then add half the quantity of gravel and sand.\niii. You should be conscious of not putting the shovel fully into the rotating mixer as you add your materials. The mixer could overpower you and twist the shovel out of your hand. The end of the shovel can hit you while it rotates with the mixer. So you should learn to throw the materials into the mixer without the shovel extending into its drum.\niv. After that, add all the cement needed. Cement is like dust, and while you turn it into the mixer, you could get a cloud of lime which can irritate your skin and eyes as well as make you cough.\nv. Finally, add what is left of your sand and gravel. Then, leave the mixture to churn for half a minute before checking to see if the water is enough.\nvi. If need be, add only a small quantity of water each time while watching out for the consistency and leave it to mix for about a minute before adding water again.\nvii. You may also use a sprinkler to wet the concrete and try to gauge the quantity of water you are adding. Never use a hose to add water to a cement mixer.\nviii. When the concrete has mixed up satisfactorily, empty the content of the mixer into a wheelbarrow by tilting the drum of the mixer slowly. Try and get out everything in the mixer. You can use a hand trowel to clean it out when the mixer had stopped, of course. Rinse out the drum with water when you are done mixing.\nA slump test can be carried out to check out the quality of your mix.\nHow Do You Protect Your Concrete Until The Concrete Is Cured?\nDepending on the condition under which you’re working, there are measures you need to understand to help your concrete cure well.\nGenerally, when your concrete dries too fast, it leaves your concrete with cracks. It will also be weaker than it ought to be. But when it dries out slowly, it tends to come out stronger.\nHowever, in freezing temperatures, your concrete cures really slowly but this doesn’t mean it would be stronger. The water in the concrete freezes and reduces the binding capacity of the cement. So when the weather gets warmer and the ice pieces lodged in your concrete melts, it leaves your concrete with many minute cavities. These cavities limit the strength of your concrete.\nHence, you have to target optimal temperatures to work under to get the best result. But, if you cannot wait for favourable weather, you can do the following depending on the scenario:\n- On hot and dry days, let your concrete dry up till it is a little firm. Then, cover up with plastic sheets or bubble wrap to reduce how fast it dries.\n- On cold days, you can cover with blankets to keep warm and prevent the water from freezing. You may also decide to work in the warmest time of the day, so the concrete is firm enough before the cold.\nThe right composition of your concrete with the materials in the recommended ratio is instrumental in having good concrete. Using a proper mixing technique and helping your concrete cure well will also make the concrete durable.', 'Cold weather Concrete placement and its important point\nCold weather Concrete placement\nIt is true that concrete is widely used in modern construction, but it is not a perfect material! In addition to high weight, concrete is vulnerable to air temperature. While concrete placement can be done at any time of the year and almost anywhere, there is an ideal temperature range for concrete placement. If concrete placement is done in cold weather, special measures must be taken to implement it so that the overall health of the concrete is not endangered in the short and long term. Join us to delve deeper into the topic of cold concrete placement.\nHydration and concrete temperature in cold weather\nAggregates (Gravel and sand) and cement paste made from a mixture of water and cement, which contain all the constituents of concrete. When cement paste and aggregate are mixed together, it forms a flexible material that can be poured in place to form the desired mold. Then, after a while, it becomes a heavy, resistant and hard mass.\nConcrete is known by a process called hydration. Hydration, which is the same as water-cement mixture, guarantees the final strength of concrete. During this process, the compounds in the cement react with the water and crystallize, and in terms of chemical reaction, separate the cement particles together, resulting in a hard product.\nThe initial hardening process takes place during the first few days after concrete placement, but the concrete curing process continues for many years thereafter. That is why it is said that concrete strengthens over time. Cold air can cause problems in the process of hydration and setting of concrete, but analysis of this issue requires engineering knowledge.\nAt temperatures above zero ° C, the development of strength in fresh concrete is very slow. If the ambient temperature drops below zero degrees, some of the water in the concrete may freeze. The setting stops until the chemical reaction is completed, and this cessation of hydration increases the porosity and decreases the strength and durability.\nConcrete placement problems in cold weather\nThe ninth article in the National Building Code states: “Concrete placement for cold weather is a period when the average daily ambient temperature is below 5 degrees Celsius for more than 3 consecutive days. The average daily temperature is the average of the highest and lowest temperatures during the period from midnight to midnight the next day. “When temperatures above 10 degrees Celsius occur in more than half of the 24-hour period, it is no longer considered cold air.”\nThe first three days after concrete placement are crucial. During this time, the semi-liquid concrete hardens and performs most of its function by bonding the cement with water. Because the presence of water is very important for the curing process, concrete placement in winter causes many problems.\nIn the season when the temperature may drop below zero, the ratio of water to cement and the percentage of aggregates can change. At low temperatures, freezing of water in the concrete mix leads to problems that pose a potential risk to the project. In cold weather, concrete cannot be placed on site without additional preparation and equipment!\nLow temperature and freezing hazards of concrete\nThe hydration process that hardens the concrete involves a chemical reaction that is affected by heat. Although the excessive heat of summer can negatively affect the performance of concrete, the low temperature in winter will cause more damage to the concrete.\nAt temperatures below 40 degrees Fahrenheit, the initial setting process can take more than 2 to 24 hours, and in very severe weather conditions, this time can be much longer. At this time, the concrete placed at freezing temperatures can cause problems with freezing and expansion of the water inside the mixture.\nThese problems often include flaking (that is, flaking of the polished surface) and crushing of concrete members under the weight of the building. Concrete placement in cold weather may prevent the concrete from hardening in normal times and cause numerous structural defects. For these reasons, concrete placement in the cold seasons of the year is usually done by professionals who know what precautions to take.\nSolutions and requirements for concrete placement in cold weather\nIf the area is very cold, chemical accelerators that speed up the curing process can be added to the concrete mix before placement. Hot concrete mixes and cement mixes that are specific to frozen environments can be used in a variety of applications.\nHere are some tips and techniques for concrete placement in cold weather:\n1.Reducing the permeability of concrete:\nOf all the factors affecting the frost resistance of concrete, permeability plays the most important role. Impermeable concrete has only a small amount of free moisture in its pores, and therefore the destructive function of freezing and expanding water is largely eliminated.\nThere are three basic ways to reduce the permeability and thus increase the cold resistance of concrete, which include the following:\n- The use of bubble additives prevents the movement of capillaries in the concrete.\n- Reducing the water-to-cement ratio and using lubricants in turn reduces the rate at which the concrete bleeding\n- The use of Pozzolans such as fly ash to replace part of the cement (generally fifteen to twenty percent) can reduce the permeability of concrete.\n2- Early strength of concrete:\n: The rate of increase of concrete strength at low temperatures is relatively low and this can affect the speed of construction. Methods of achieving faster setting times and early strengths of concrete differ with specific applications. To overcome this problem, there are several ways to produce early resistance\n- Use of accelerator additives\n- Use hot water in concrete mix\n- Cover or heat the molds before placement\nDespite the problems that can arise during the concrete placement process, concrete placement in cold weather can still be an effective process if the basic measures and requirements are followed. First, the place where the concrete is placed must be prepared.\nProject managers often use thermal mats to heat the molds to a more stable temperature, and then use a special concrete blanket to cover and keep the concrete mix warm for the first few days.\n3- Increase the amount of cement\nIn some projects, people use a combination of Portland cement types I and II. Other people may use Type III cement, which is cement with high early strength. This cement is finer and has a faster hydration reaction. Therefore, the increase in early compressive strength of concrete made with type III cement is greater. The amount of use of each of these cements must have an economic justification for the project\nConcrete Curing in Cold Weather\nAccording to the ABA Regulations and Article 9 of the National Regulations, the application of fresh concrete must continue for at least 24 hours until it reaches strength of 50 MPa. The following methods can be used to operate fresh concrete and protect it from freezing:\n- Use of protective insulation coating\n- Heating concrete using electric heaters or hot water spraying on concrete members\n- Other methods approved by the monitoring device\nFresh concrete must also be protected against wind. The use of insulating coatings for this purpose is excellent.\nThe final conclusion:\nConcrete placement in cold weather has many challenges, so it is important to deal with each problem in principle. Also, it is essential that all personnel be prepared to deal with the cold and use appropriate clothing and equipment. Concrete placement is recommended in the middle of the day, especially at noon when the temperature is higher than other hours. Other precautions are described in detail above.']	['<urn:uuid:09afd8f6-8c0a-475e-871f-33e73d508601>', '<urn:uuid:a9daa671-6b52-4604-be53-19c6d221fdf5>']	factoid	direct	long-search-query	distant-from-document	multi-aspect	novice	2025-05-12T20:11:36.665104	9	92	3151
63	why is genre important for authors looking to sell books explain benefits	Genre is vitally important for authors because it helps them locate agents, publishers, and readers interested in their specific types of stories. Genre is one of the first markers that classify books, and it helps authors promote their work to the right audience. Authors can join forces with fellow writers in similar genres to cross-promote through advertisements, boxed sets, or special social media communities.	['G is for Genre.\nGenre is vitally important to authors. Genre allows us to locate agents, publishers, and readers who are interested in the specific types of stories we write. Genre is one of the first markers that classify books. (Other markers might include author’s name, publisher, price, etc.) Specific genres rise and fall in popularity, as readers define what they’re willing to buy. Some genres get re-cast, in an effort to freshen sales. (For example, “chicklit” is generally considered to be passé, but “romantic comedy” is on the rise in popularity. A book that would have been categorized chicklit in 2001 is likely called romantic comedy today.)\nIt used to be easy to define classify books by genre: Just figure out where to shelve them at the local bricks-and-mortar bookstore. Authors (and their readers) could make straightforward declarations: This book belongs in Mystery, that one in Travel.\nBut bookstore classifications aren’t really that simple.\nShould there be a single section called Science Fiction, and a separate one called Fantasy? Or can Fantasy piggy-back on SF? What about Horror, where does it fit in? Should we create a category for movie tie-ins? Should we include card games like Magic? Character-driven games like Dungeons and Dragons with their handbooks and guides and manuals? And what about non-fiction genres, like History? Should History be broken down by geographic region? By general time periods? Solely by author’s last name?\nAnd why does anyone really care?\nIn the bricks-and-mortar setting, genre is vitally important. Bookstores must decide where to shelve their stock. They can’t sprinkle one copy in every section where a book potentially fits. (The order for most new books is at most three, and “singleton” books often get damaged by handling.) Therefore, physical bookstores rely on genres to cue readers about where to find the books they want.\nObviously, shelf space and physical layout is not crucial to online sales. Books can be classified in multiple genres, with simple electronic links taking potential buyers from one section of the store to another. There’s no danger of lonely single books getting lost or damaged.\nPerhaps as a result, genres are fracturing.\nAll books—print or electronic—sold online are typically organized using codes. In the United States, the industry relies on BISAC (“Book Industry Standards and Communications”) codes, which are promulgated by the Book Industry Study Group. BISAC Codes are organized into 52 major sections. They consist of three letters defining the broad genre, followed by six numbers defining a sub-genre. One book might be described by several BISAC codes. For example, my novel Perfect Pitch can be listed as FIC038000 (Fiction/Sports.) It can also be described as FIC027020 (Fiction/Romance/Contemporary).\nSimilarly, British libraries rely on BIC codes, which are promulgated by a trade association, Book Industry Communication. Those codes are less specific than BISAC codes; they consist of one to three letters, with longer codes being more specific than shorter ones. For example, Perfect Pitch is listed as FR—Fiction/Romance. (While FRH would indicate Fiction/Romance/Historical, there is no three-letter code for Fiction/Romance/Contemporary.)\nHow are these genre codes used?\nOnline booksellers rely on BISAC or BIC codes as one tool to classify books, adding those classification links that connect buyers to multiple books in a genre. They also rely on all of a book’s other metadata — the title, sub-title, “back of the book” copy, keywords, etc. As a result, customers are able to “drill down” to books they are likely to enjoy. Instead of browsing through all the possible romance novels available, customers can search just sports romances. The result is a win-win—readers are more likely to find books to enjoy, and authors are more likely to sell books.\nThe explosion of metadata associated with books has resulted in narrower and narrower sub-genres. For example, Jennifer Stevenson’s A Hinky Taste of You, which has a heroine who skates in Roller Derby, is classified in the following categories on Amazon:\n- Kindle Store > Kindle eBooks > Nonfiction > Sports > Individual Sports > Rollerskating & Rollerblading\n- Books > Sports & Outdoors > Individual Sports > Rollerskating & Rollerblading\n- Kindle Store > Kindle eBooks > Romance > Sports\nTry to imagine a bricks-and-mortar store with a specific aisle dedicated to books about rollerskating and rollerblading! Also note that the Amazon classification technically considers Ms. Stevenson’s light paranormal novel to be “Nonfiction.” In a bricks-and-mortar store, mis-shelving a novel with nonfiction books would typically destroy any patron’s chance of finding the novel, but online customers are still able to locate the work they desire.\nAuthors can take advantage of these relatively narrow definitions of genre. They can join forces with fellow authors of similar books to cross-promote, either in traditional advertisements, boxed sets, or special user communities in social media. At the same time, they can launch separate promotional efforts for their novels in separate sub-genres. (On a personal note, I can promote my baseball books separate from my light paranormal, without boring or offending any of my readers.)\nThe splintering does raise a potential problem: It’s possible to define sub-genres on too granular a basis. For example, Perfect Pitch could be placed in a sub-genre of romances set in Raleigh, North Carolina with heroines who are beauty queens and heroes who have blue eyes. In theory, vendors could set up a genre system to pull together all books that meet those specific markers. But how many other books meet those requirements? What use is a genre populated by a single book? Buyers’ tastes aren’t that restrictive. Therefore, it’s not worth vendors’ or buyers’ time to create such detailed systems.\nHow about you? Have you found your definitions of genre changing over time? Do you classify books in a more general or more specific way than you used to? Are your reactions different as an author (if you are one!) than as a reader?']	['<urn:uuid:b436ad82-64d2-4efa-b1d1-6384c2aedb4a>']	factoid	with-premise	long-search-query	similar-to-document	single-doc	novice	2025-05-12T20:11:36.665104	12	64	978
64	What are benefits and reconditioning options for engine balancing?	Engine balancing provides several benefits: it reduces internal loads and vibrations that stress metal, prevents component failure, creates a smoother-running and more powerful engine, and reduces stress on motor mounts. For reconditioning, there are specific processes like equalizing reciprocating mass in cylinders by weighing and matching pistons and rods, using digital scales for precision. Modern balancing machines can achieve accuracies to hundredths of a gram. The process includes weighing components, making corrections, and final testing. This applies to both engine components and turbocharger parts, where reconditioning services include shaft repair, blade restoration, and precision balancing using specialized equipment like SCHENCK machines.	"['The Hines HC-500 Balancer utilizes a rugged hard-bearing suspension that\ngives high sensitivity and permanent calibration. Part set ups are menu\ndriven, allowing an operator to quickly change between single- and two-plane\nbalance operations. With the addition of the drill option, on-machine corrections\nare fast, accurate and easy. The balancer will calculate amount, angle\nand multiple-hole corrections.\nBalancing goes hand-in-hand with performance engine building. Balancing\nreduces internal loads and vibrations that stress metal and may eventually\nlead to component failure. But is it worth the time and effort for mild\nperformance applications, everyday passenger car engines or low-buck rebuilds?\nFrom a technical point of view, every engine regardless of the application\nor its selling price can benefit from balancing. A smoother-running engine\nis also a more powerful engine. Less energy is wasted by the crank as it\nthrashes about in its bearings, which translates into a little more usable\npower at the flywheel. Reducing engine vibration also reduces stress on\nmotor mounts and external accessories, and in big over-the-road trucks,\nthe noise and vibration the driver has to endure mile after mile.\nThough all engines are balanced from the factory (some to a better degree\nthan others), the original balance is lost when the pistons, connecting\nrods or crankshaft are replaced or interchanged with those from other engines.\nThe factory balance job is based on the reciprocating weight of the OE\npistons and rods. If any replacements or substitutions are made, there’s\nno guarantee the new or reconditioned parts will match the weights of the\noriginal parts closely enough to retain the original balance. Most aftermarket\nreplacement parts are ""balanced"" to the average weight of the\nOEM parts, which may or may not be close enough to maintain a reasonable\ndegree of balance inside the engine. Aftermarket crank kits are even worse\nand can vary considerably because of variations within engine families.\nIf the cylinders are worn and a block needs to be bored to oversize, the\nlarger replacement pistons may be heavier than the original ones. Some\npiston manufacturers take such differences into account when engineering\nreplacement pistons and try to match ""average"" OE weights. But\nothers do not. Most high performance pistons are designed to be lighter\nthan the OE pistons to reduce reciprocating weight for faster acceleration\nand higher rpm. Consequently, when pistons and rods are replaced there’s\nno way of knowing if balance is still within acceptable limits unless you\nIf you’re building a stock engine for a passenger car or light truck\nthat will spend most of its life loafing along at low rpm, your customer\nmight question the value of balancing such an engine. But if you the customer\nvalue durability and smooth operation, the decision to balance should not\nbe to difficult.\nOn the other hand, if you’re building a performance engine, a stroker\nengine or an engine that’s expected to turn a lot of rpm\'s or run\na lot of miles, balancing is an absolute must. No engine is going to survive\nlong at high rpm\'s if it’s out of balance. And no engine is going\nto last in a high mileage application if the crank is bending and flexing\nbecause of static or dynamic imbalances.\nForces In Action\nTo better understand the mechanics of balancing, let’s look at the\ntheory behind it. As everybody knows, a rotating object generates ""centripetal\nforce."" Centripetal force is an actual force or load generated perpendicular\nto the direction of rotation. Tie a rope to a brick and twirl it around\nand you’ll feel the pull of centripetal force generated by the ""unbalanced"" weight\nof the brick. The faster you spin it, the harder it pulls. In fact, the\nmagnitude of the force increases exponentially with speed. Double the speed\nand you quadruple the force.\nThe centripetal force created by a crankshaft imbalance will depend upon\nthe amount of imbalance and distance from the axis of rotation (which\nis expressed in units of grams, ounces or ounce-inches). A crankshaft\nwith only two ounce-inches of imbalance at 2,000 rpm will be subjected\nto a force of 14.2 lbs. At 4,000 rpm, the force grows to 56.8 lbs.! Double\nthe speed again to 8,000 rpm and the force becomes 227.2 lbs.\nThis may not sound like much when you consider the torque loads placed\nupon the crankshaft by the forces of combustion. But centripetal imbalance\nis not torque twisting the crank. It is a sideways deflection force that\ntries to bend the crank with every revolution. Depending on the magnitude\nof the force, the back and forth flexing can eventually pound out the main\nbearings or induce stress cracks that can cause the crank to snap.\nCentripetal force should not be confused with ""centrifugal"" force,\nwhich is the tendency of an object to continue in a straight trajectory\nwhen released while rotating. Let go of the rope while you’re twirling\nthe brick and the brick will fly off in a straight line (we don’t\nrecommend trying this because its difficult to control the trajectory of\nBack to centripetal force. As long as the amount of centripetal force\nis offset by an equal force in the opposite direction, an object will rotate\nwith no vibration. Tie a brick on each end of a yardstick and you can twirl\nit like a baton because the weight of one brick balances the other. If\nwe’re talking about a flywheel, the flywheel will spin without wobbling\nas long as the weight is evenly distributed about the circumference. A\nheavy spot at any one point, however, will create a vibration because there’s\nno offsetting weight to balance out the centripetal force.\nThis brings us to another law of physics. Every object wants to rotate\nabout its own center of gravity. Toss a chunk of irregular shaped metal\ninto the air while giving it a spin and it will automatically rotate\nabout its exact center of gravity. If the chunk of metal happens to be\na flywheel, the center of gravity should be the the flywheel’s\naxis. As long as the center of gravity for the flywheel and the center\nof rotation on the crankshaft coincide, the flywheel will spin without\nBut if there’s a heavy spot on the flywheel, or if the flywheel\nisn’t mounted dead center on the crank, the center of gravity and\naxis of rotation will be misaligned and the resulting imbalance will create\nOkay, so how does all this scientific mumbo jumbo translate into the real\nworld dynamics of a spinning crankshaft? A crankshaft, like a flywheel,\nis a heavy rotating object. What’s more, it also has a bunch of\npiston and rod assemblies reciprocating back and forth along its axis\nthat greatly complicate the problem of keeping everything in balance.\nWith inline four and six cylinder engines, and flat horizontally opposed\nfours and sixes (like Porsche and Subaru), all pistons move back and forth\nin the same plane and are typically phased 180° apart so crankshaft\ncounterweights are not needed to balance the reciprocating components.\nBalance can be achieved by carefully weighing all the pistons, rods, wrist\npins, rings and bearings, then equalizing them to the lightest weight.\nOn V6, V8, V10 and V12 engines, it’s a different story because the\npistons are moving in different planes. This requires crankshaft counterweights\nto offset the reciprocating weight of the pistons, rings, wrist pins and\nupper half of the connecting rods.\nInternal or External\nWith ""internally balanced"" engines, the counterweights themselves\nhandle the job of offsetting the reciprocating mass of the pistons and\nrods. ""Externally balanced"" engines, on the other hand, have\nadditional counterweights on the flywheel and/or harmonic damper to assist\nthe crankshaft in maintaining balance. Some engines have to be externally\nbalanced because there isn’t enough clearance inside the crankcase\nto handle counterweights of sufficient size to balance the engine. This\nis true of engines with longer strokes and/or large displacements.\nIf you’re rebuilding an engine that is internally balanced, the\nflywheel and damper have no effect on engine balance and can be balanced\nseparately. But with externally balanced engines, the flywheel and damper\nmust be mounted on the crank prior to balancing.\nYou should find out what type of engine balance you have (internal\nor external), and be cautious about indexing the position of the flywheel\nif you have to remove it later for resurfacing. Owners of externally balanced\nengines should also learn about installing different flywheels or\ndampers and how it can upset balance.\nIn recent years, the auto makers have added balance shafts to many four\nand six cylinder engines to help cancel out crankshaft harmonics. The\ncounter-rotating balance shaft helps offset vibrations in the crank created\nby the firing sequence of the engine.\nOn these motors, make sure the balance shaft is correctly ""phased"" or\ntimed to the rotation of the crank. If the shaft is out of sync, it will\namplify rather than diminish engine vibrations.\nBalance shafts are not a substitute for normal engine balancing, nor do\nthey reduce the vibration and stress the crankshaft itself experiences\nas it turns.\nThe process of balancing begins by equalizing the reciprocating mass in\neach of the engine’s cylinders. This is done by weighing each piston\non a sensitive digital scale to determine the lightest one in a set.\nThe other pistons are then lightened to match that weight by milling\nor grinding metal off a non-stressed area such as the wrist pin boss.\nThe degree of precision to which the pistons are balanced will vary from\none engine builder to another, and depends to some extent on the application.\nBut generally speaking pistons are balanced to within plus or minus 0.5\ngrams of one another.\nNext the rods are weighed, but only one end at a time. A special support\nis used so that the big ends of all the rods can be weighed and compared,\nthen the little ends. As with the pistons, weights are equalized by grinding\naway metal to within 0.5 grams. It’s important to note that the direction\nof grinding is important. Rods should always be ground in a direction perpendicular\nto the crankshaft and wrist pin, never parallel. If the grinding scratches\nare parallel to the crank, they may concentrate stress causing hairline\ncracks to form.\nOn V6 and V8 engines, the 60 or 90 degree angle between the cylinder banks\nrequires the use of ""bobweights"" on the rod journals to simulate\nthe reciprocating mass of the piston and rod assemblies. Inline four and\nsix cylinder crankshafts do not require bobweights. To determine the correct\nweight for the bobweights, the full weight of a pair of rod bearings and\nthe big end of the connecting rod, plus half the weight of the little end\nof the rod, piston, rings, wrist pin (and locks if full floating) plus\na little oil are added together (100 percent of the rotating weight plus\n50 percent of the reciprocating weight). The correct bobweights are then\nassembled and mounted on the crankshaft rod journals.\nThe crankshaft is then placed on the balancer and spun to determine the\npoints where metal needs to be added or removed. The balancer indexes the\ncrank and shows the exact position and weight to be added or subtracted.\nThe electronic brain inside the balancer head does the calculations and\ndisplays the results. The machines we have use graphical displays that\nmake it easy to see exactly where the corrections are needed.\nIf the crank is heavy, metal is removed by drilling or grinding the counterweights.\nDrilling is usually the preferred means of lightening counterweights, and\na balancer that allows the crank to be drilled while still on the machine\ncan be a real time saver.\nIf the crank is too light, which is usually the case on engines with stroker\ncranks or those that are being converted from externally balanced to\ninternally balanced, heavy metal (a tungsten alloy that is 1.5 times\nas heavy as lead) is added to the counterweights. This is usually done\nby drilling the counterweights, then press fitting and welding the heavy\nmetal plugs in place. An alternate technique is to tap the hole and thread\na plug into place. Drilling the holes sideways through the counterweights\nparallel to the crank rather than perpendicular to the crank is a technique\nmany prefer because it prevents the metal from being flung out at high\nAfter drilling, the crankshaft is again spun on the balancer to determine\nif additional corrections are required. If the crank is for an externally\nbalanced engine (such as a big block Chevy), the balancing will be done\nwith the flywheel and damper installed. On internally balanced engines,\nthe flywheel and damper can be balanced separately, or installed on the\ncrank and balanced as an assembly once the crank itself has been balanced.\nNew machinery has been introduced that both simplifies the balancing process\nand increases the accuracy of the job. Electronic equipment that allows\naccurate measurement of not only the amount of unbalance force, but also\naccurately reports the unbalanced vector position is now available to engine\nrebuilders. Typically, balancing machines have assumed that the unbalance\nforce was equally opposed, so they would require the technician to correct\nthe excessive amounts of unbalance on the excess side to the point of making\nthem equal. Technicians have had to ‘stair-step’ the corrections\nequally until the final tolerance was attained.\nTechnology such as that in the Hines HC 500 eliminates this requirement.\nSoftware and hardware are combined to allow radical differences to be reported\nat each end of\nthe crankshaft (including any rotational positioning or vector position\nof the unbalanced force). Because the position and unbalance amounts are\nreported correctly the technician can make changes to the crankshaft with\nconfidence that he will not over shoot the correction. In most cases the\nrequired cycles of analysis and correction are reduced by 80 to 90 percent.\nThe unbalance amount and position are imported into a special computer\nprogram called ""Heavy Metal Analysis"" (HMA). This program allows\nthe technician to plot the position and amount of material that will be\nrequired to correct the crankshaft. The program lets rebuilders create\nmultiple scenarios based on rotation and radius position, weight amounts\nand sizes of Mallory – all of which can be simulated without having\nto cut the first chip.\nHow long does it take to actually balance an engine? A typical Chevy small\nblock V8 might take anywhere from 45 minutes to an hour-and-a-half depending\non how much work is needed and the degree of accuracy you’re trying\nto achieve. You’re obviously going to spend more time on a motor\nthat’s going into a NASCAR Winston Cup racer than one that’s\ngoing into Grandma’s grocery getter.\nThough a balancing accuracy of plus or minus one gram is typically good\nenough for most production street engines, many balancers today can achieve\nbalancing accuracies in the tenths or even hundredths of a gram!\nThe most time-consuming part of the job is weighing and matching the pistons\nand rods. A four cylinder engine takes half as much time for this step\nas a V8. The next most time consuming part is making up the bobweights\nfor a V6 or V8. This step isn’t needed with a straight six or four.\nThe actual setup on the machine takes only a few minutes, and the initial\nspin and readings take only a couple of minutes more. The time required\nto perform the necessary weight corrections will depend on the crank (weight\nremoval goes much faster than adding weight). And if you’ve done\nyour work carefully, the final spin will require no further corrections\nbecause the balance will be right on the mark.\nMost shops charge $150 to $300 to balance a V8. If heavy metal is required,\nadd $40 to $75 per slug. Some shops charge less to balance engines, but\nthese tend to be shops that are trying to compete in the budget rebuild\nmarket, not the performance market and they are using old soft bearing technology. .\nAt RPM Machine we charge $210.00 for a standard V/8 and\nV/6 (2 more bob weights). This price will take care of balancing a rotating\nassembly up to 40 grams out of balance. For performance race engines and\nstroker assemblies that are farther out than this a price per hour will\nFor more information or to schedule your engine for balancing please feel\nfree to give us a call at 1-877-354-3812 and we look forward to helping', 'Being an expensive piece of equipment, turbocharger reconditioning is a cost-effective alternative to renewing partly or completely. In addition, the advances in engine development, in order to meet emission requirements and reduce fuel consumption, have prompted development of new turbocharger types with higher pressure ratios and efficiency. This has caused earlier turbocharger types to be discontinued by the manufacturers, in some cases making it difficult to readily obtain new parts for such types still in service.\nIn order to meet the above-mentioned challenges, we offer turbocharger reconditioning of most types. At a fraction of the cost of new components, this allows the clients to maintain their turbochargers in good condition throughout the full service life. Apart from being cost effective, and for older turbocharger types often the only viable alternative to retrofitting a newer type, reconditioning is also less of a drain on the natural resources and thus being more sustainable than just replacing with new components.\nTurbocharger Reconditioning Services Include:\n- Turbocharger Shafts\n- Blades and Nozzle Rings\n- Turbocharger Balancing\nReconditioning of Turbocharger Shafts\nWe offer repair and reconditioning of all rotating shafts by hot and cold metal spraying. All shaft journals, threaded areas, pump spigots and seal areas can be repaired by metal spraying and then machine to the original specification. Unlike a cold repair process, metal spraying allows a shaft to be restored to its original dimensions and strength.The metal spraying procedure employed by us is approved by Lloyd’s Register.\nReconditioning of Blades and Nozzle Rings\nWhen reconditioning turbochargers, the key competence is the ability to rebuild turbine blades, compressor wheels and nozzle rings. We do this by a specialized procedure, approved by Classification Societies, carried out by highly qualified technicians.\nThe turbine blades are dismantled, cleaned ultrasonically and accurately measured. A non-destructive inspection by ultra-violet light is performed to detect possible fractures. Fractured blades are replaced with new ones.\nOur turbine blade welding is based on a procedure developed through extensive research by metallurgists. The method used is TIG (Tungsten Inert Gas) welding – a technique using a non-fusing electrode at low amperage, shielded by an inert gas.\nThe welding is carried out under strictly controlled conditions. Only a small zone may be exposed to heating, and the operation must be carried out in a closed welding chamber, free from dirt particles and air turbulence, as any draught would disperse the shielding gas and result in poor welding quality.\nAfter welding the turbine blade is machined and ground back into its specified shape, inspected for cracks using both ultra-violet light and dye penetrant inspection and the weight is verified.\nIn order to achieve the correct weight distribution, a computer calculates the sequence in which the blades should be mounted on the rotor in preparation for the balancing.\nSimilar processes are employed to recondition nozzle rings and compressor wheels.\nIn case of high-speed rotating components, turbocharger balancing is of the essence – both for the sake of safety and time between overhauls. We employ state of the art equipment – not only for the balancing of turbocharger rotors, but for balancing of high-speed rotating components for a wide variety of applications.\nWe balance all components separately and correct for static unbalance. In order to check the balance after assembly, the complete unit will finally be checked for dynamic unbalance.\nWe have the latest equipment for balancing components in two planes, from the smallest size up to a length of 3,000 mm and a diameter of 1,200 mm, with three balancing machines available to cater for every need:\n- SCHENCK H20/H40 BUTL with CAB 920\n- SCHENCK H10BU with CAB 610\n- SCHENCK HEC121 with CAB590']"	['<urn:uuid:d58ea3b1-3ea8-439c-9ca6-d2bc6602003d>', '<urn:uuid:48419811-ba17-407f-93b1-a01972222595>']	open-ended	direct	concise-and-natural	similar-to-document	multi-aspect	novice	2025-05-12T20:11:36.665104	9	101	3353
65	looking for info how hiv tricks body and hiding mechanisms explain both	HIV employs two main deceptive mechanisms: First, it tricks the body into overproducing regulatory T cells (Tregs) that suppress virus-killer cells (CD8+ T cells), preventing them from effectively fighting the infection. Second, HIV can hide from the immune system by establishing latent infection in CD4+ T cells and by expressing non-immunogenic glycans on key antibody epitopes, making it invisible to CD8+ T cells and anti-HIV antibodies.	['Human bodies are naturally equipped with elaborate defense mechanisms to squelch intruding microorganisms. But some viruses, like HIV, are able to slip under the immune system radar and set up permanent residence in a human host. Exactly how this feat is accomplished is not known for certain, but a better understanding of virus war tactics is key to developing effective medical treatments.\nSome evidence suggests that viruses like HIV are able to avoid destruction by tricking the body into overproducing a type of cell that suppresses the immune system. A new study in PNASProceedings of the National Academy of Sciences substantiates this theory and offers insight into how medicine can counter this.\nThe immune system is complicated, but you can sort of think of it with a Pac-Man analogy (Disclaimers: I don’t wish to make light of this subject, however I am having trouble thinking of another analogy and the immune system is confusing. Also, though I minored in biology in college, I am an organic chemist, not an immunologist. I also haven’t played much Pac-Man). Here are the participants:\nPac-Man gets the honor of being a virus-killer cell – called a CD8+ T cell. This is one of the cells produced by the body’s immune system. Here, the white circles are virus particles, and the CD8+ T cells are responsible for killing them. Now unfortunately, these virus-killers are also capable of killing normal human cells if allowed to run amuck. To prevent this, there has to be some kind of checks-and-balances system. So, there’s a second type of immune system cell, called a Treg (regulatory T cell). The Tregs are able to suppress the activity of the CD8+ T cells, to make sure they don’t just kill everything in sight. This is what a normal virus infection might look like:\nHowever, it seems that some viruses can trick the body into producing way too many Tregs. With so many Tregs around, the CD8+ T cells can hardly do any killing at all, not even of viruses. Thus, an infection with a virus like HIV might look something like this:\nDuring a chronic virus infection, the CD8+ T cells become “exhausted” (technical term!), presumably from the extra Tregs around. Researchers have been wondering – if you can get rid of some of those Tregs, could the exhausted CD8+ T cells recover and start killing viruses again?\nThe answer is yes, according to the authors of this paper. They infected mice with a type of mouse virus (FV)friend virus that, like HIV, is able to establish a chronic infection.\nThey didn’t use run-of-the mill mice, though. These mice had been genetically modified so that their Tregs had an Achilles heel. The Tregs could be selectively eliminated at will by the researchers via injection with a particular toxin, a toxin that would not normally affect Tregs. This genetic modification allowed the researchers to control Treg levels – something that wouldn’t be easily achievable with regular mice.\nAfter the FV infection had set in, and Treg levels were presumably high due to the virus’s influence, the researchers injected the mice with the Treg-suppressing toxin. In theory, the resulting drop in Treg activity would allow the CD8+ T cells to recover and start killing viruses again.\nAnd this is exactly what seemed to happen. As the mouse’s Treg activity started to decrease, the concentration of virus particles in the mice started to get lower. The researchers found evidence that the CD8+ T cells were doing the virus-killing when they noticed increased levels of certain signalling molecules produced by CD8+ T cells.\nThe results of this study suggest that chronic infections in humans, such as HIV, could potentially be treated by supressing human Treg cells. However, the use of such a therapy would first require a lot more research. Although HIV, like FV, is believed to suppress the immune system by increasing Treg activity, it paradoxically stimulates the immune system in other ways. It is possible that medically depleting Tregs could actually exacerbate disease progression in an HIV-infected individual. Further research is certainly in order.\nDietze, K., Zelinskyy, G., Gibbert, K., Schimmer, S., Francois, S., Myers, L., Sparwasser, T., Hasenkrug, K., & Dittmer, U. (2011). Transient depletion of regulatory T cells in transgenic mice reactivates virus-specific CD8+ T cells and reduces chronic retroviral set points Proceedings of the National Academy of Sciences DOI: 10.1073/pnas.1015148108', 'Human immunodeficiency virus (HIV), a member of the retrovirus family, is the causative agent of acquired immunodeficiency syndrome (AIDS). HIV invades various immune cells (e.g., CD4+ T cells and monocytes) resulting in a decline in CD4+ T cell numbers below the critical level, and loss of cell-mediated immunity − therefore, the body becomes progressively more susceptible to opportunistic infections and cancer.\nHIV invasion of immune cells\nHIV infects T cells via high-affinity interaction between the virion envelope glycoprotein (gp120) and the CD4 molecule. The infection of T cells is assisted by the T-cell co-receptor called CXCR4 while HIV infects monocytes by interacting with CCR5 co-receptor (Figure 1). As illustrated in Figure 2, after gp120 binds to CD4 on the T cell (1). Nucleocapsids containing viral genome and enzymes enters the target cell (2). Following the release of viral genome and enzymes from the core protein, viral reverse transcriptase catalyses reverse transcription of ssRNA to form RNA-DNA hybrids (3). To yield HIV dsDNA the viral RNA template is partially degraded by ribonuclease H and the second DNA strand is synthesized (4). The viral dsDNA is translocated into the nucleus and integrated into the host genome by the viral integrase enzyme (5). Transcription factors transcribe the proviral DNA into genomic ssRNA (6), which is exported to cytoplasm (7). In the cytoplasm, host-cell ribosomes catalyse synthesis of viral precursor proteins (8). The viral precursor proteins are cleaved into viral proteins by viral proteases (9). HIV ssRNA and proteins assemble beneath the host-cell plasma membrane (10) forming virion buds from it (11). Maturation occurs either in the forming buds or after budding from the host cell (12). During maturation, HIV proteases cleave the poly-proteins into individual functional HIV proteins. The mature virions are able to infect another host cell.\nFigure 1. Interaction between HIV and coreceptors of a T cell and a monocyte\nFigure 2. Overview of HIV infection of a target cell (e.g. T cell)\nInnate immune response to HIV\nInnate immune cells (e.g., dendritic cells and natural killer cells) are the first line of defence which HIV encounters upon entry to the body.\nMacrophages. Tissue macrophages are one of the target cells for HIV. These macrophages harbour the virus and are known to be the source of viral proteins. However, the infected macrophages are shown to lose their ability to ingest and kill foreign microbes and present antigen to T cells. This could have a major contribution in overall immune dysfunction caused by HIV infection.\nDendritic cells (DCs). DCs are large cells with dendritic cytoplasmic extensions. These cells present processed antigens to T lymphocytes in lymph nodes. Epidermal DCs, expressing CD1a and Birbeck granules, are probably among the first immune cells to combat HIV at the mucosal surfaces. These cells transport HIV from the site of infection to lymphoid tissue. The follicular DCs, found in lymphoid tissue, are also key antigen-presenting cells that trap and present antigens on their cell surfaces. In the lymph node follicles, DCs provide signals for the activation of B lymphocytes.\nNatural killer (NK) cells. NK cells have lytic activity against cells that have diminished expression of major histocompatibility complex (MHC) I antigens. Because the presence of MHC class I is required for peptide presentation to T cell receptors, NK cells are important line of defence when HIV escapes the cellular immune response. NK cells proliferate in response to type 1 interferon secreted by DCs. These stimulated NK cells release cytokines such as interferon γ (IFN-γ), tumour necrosis factor α (TNF-α), and chemokines to activate T-cell proliferation (cellular immune response). NK cells also inhibit viral replication by releasing IFN-γ.\nAdaptive immune response to HIV\nCellular immune response to HIV. The cellular immune response is induced upon the entry of HIV into the target cells (e.g., T cells) and synthesis of viral proteins (Figure 1). MHC class I on the cell surface displays the intracellularly degraded HIV peptide fragments for recognition by T-cell receptors (TCR) on CD8+ T cells (Figure 3). CD8+ T cells lyse HIV infected cells and secrete cytokines, i.e. interferon-γ (IFN-γ), tumor necrosis factor α (TNF-α), and chemokines, i.e. MIP-1 α, MIP β and RANTES, that inhibit virus replication and block viral entry into CD4+ T cells. Development of CD8+ T cells is crucial for control of HIV replication. This results in declining viraemia after primary infection. In the early stages of infection, CD4+ T cells lose their proliferative capacity and therefore their contribution to viral control is minor. However, during chronic infection CD4+T cells are present and secrete interleukin-2 (IL-2) or cytokines, such as IFN-γ, to control viraemia.\nHumoral response to HIV. The humoral immune response occurs later in infection; therefore, the level of antibodies during the acute infection is very low. Non-neutralising antibodies to structural proteins (i.e. P17 and P24) are first to appear and generally do not persist. Later neutralising antibodies specific to proteins, involved in the entry of the virus into the cells, will be generated. These antibodies are specific to: (1) the variable region of gp120 (V3); (2) CD4 binding sites and chemokine receptors (i.e., CXCR4 and CCR5); (3) the transmembrane protein gp41. Potent neutralizing antibodies have been shown to play a major role in controlling HIV infection in a few symptom-free HIV+ individuals who maintain high level of CD4+ T cells and low viral load.\nFigure 3. Cellular & humoral immune responses to HIV\nWhy does the immune system fail to fight the HIV virus?\nThere are various reasons which can contribute to the failure of the immune system to control HIV infection and prevent AIDS development. By infecting CD4+ T cells, HIV is able to replicate predominantly in activated T cells and paralyse one of the main components of adaptive immune system. HIV can also establish latent infection in CD4+ T cells and remain invisible to CD8+ T cells and therefore replication can occur later in the infection and generate new virions. Antigenic mutation within the T-cell epitopes can affect the binding capacity of MHC molecules to the viral peptides, resulting in the inability of the TCRs to recognise the MHC-peptide complex. Finally, HIV is able to hide from anti-HIV antibodies by expressing non-immunogenic glycans on key antibody epitopes.\n© The copyright for this work resides with the author']	['<urn:uuid:46c2e102-9e79-4368-a545-93c0899b7a7d>', '<urn:uuid:b20cc9c2-5a61-4e34-a9d7-92c7da8cae73>']	factoid	with-premise	long-search-query	distant-from-document	multi-aspect	novice	2025-05-12T20:11:36.665104	12	66	1769
66	As someone interested in plant reproduction, I'd love to know what's the main difference between how olive trees and waterlilies make copies of themselves in terms of genetic diversity?	The key difference is that waterlilies use both sexual reproduction (pollination) and vegetative propagation (creating clones), while olive trees primarily rely on cuttings and seeds. Sexual reproduction in waterlilies provides greater genetic diversity and disease resistance, while vegetative propagation produces identical clones. For olive trees, cutting-propagated trees will be genetic clones that closely match the parent tree, while seed propagation introduces genetic variation.	"['Life will find a way, as Jeff Goldblum’s character in Jurassic Park famously said. The need to reproduce is a basic motivation that drives all life on Earth, and modern aquatic plants have demonstrated this better than most of Earth’s other creatures. In fact, they have found several ways, which I’ll shine a light on here with a couple of examples of plants for which even two reproductive strategies are not enough.\nThe two forms of reproduction used by most aquatic plants are classic sexual reproduction, or pollination, and one or many forms of vegetative propagation. Vegetative propagation is a broad category describing various forms of asexual reproduction, resulting in the creation of clones of the parent plants.\nThe most common forms of natural vegetative propagation that we see in the pond trade involve the formation of tubers, (e.g., Nelumbo, tropical Nymphaea) stolons and runners (e.g., Nymphoides) and rhizomes (e.g., hardy Nymphaea, iris and Typha).\nSexual reproduction is a superior method of propagation, because the population benefits from the exchange of genetic material. Greater genetic diversity leads to greater resistance to disease and adaptability to changes in the environment. Conversely, most forms of vegetative propagation simply produce clones of the parent plant. This offers the plant a faster strategy to colonize an area with good growing conditions and allows it to produce fewer but larger offspring with a higher likelihood of reaching maturity than seedlings. But, this comes at the cost of possibly reducing the genetic diversity of the overall population. By taking advantage of both sexual and vegetative methods, plants can establish themselves quickly in a particular area, maintain genetic diversity and extend their range to new areas.\nFor those who produce cultivars of waterlilies or lotuses with very specific characteristics, vegetative propagation is a major key to maintaining the purity of a cultivar. Another vegetative strategy we growers may exploit is a fascinating process known as vivipary. There is more than one definition of vivipary in the world of botany. The narrower definition describes true vivipary as a form of sexual reproduction whereby germination takes place while the seeds are still attached to the parent plant; pseudo-vivipary, on the other hand, is the formation of plantlets in place of sexual reproductive structures. The wider definition, which I will focus on in this article, includes any scenario in which plantlets are formed while still attached to the parent plant.\nIn addition to the production of tubers or rhizomes, some species of waterlily also create fully formed plantlets from either the leaves or the flower.\nNymphaea micrantha is an African waterlily species that produces plantlets from the center of the leaves. These plantlets start from a small, gelatinous nub at the center of the leaf. N. micrantha utilizes this strategy to such a degree that the plantlets even produce plantlets of their own while still attached to the parent plant.\nMore than 30 tropical waterlily hybrids employ this method of propagation to a lesser or greater degree, and all those are descendents of N. micrantha. Some popular examples are N. ‘Lindsey Woods’, N. Panama Pacific, N. ‘Daubenyana’ and N. ‘Margaret Mary.’\nIn hybrids, production of these plantlets seems to be triggered by the decaying of the parent leaf. Plantlet formation can even be induced by removing the leaf from the parent plant. With the right growing conditions, this can be used as a viable way of propagating certain hybrids. Sean Stevens describes a method where the leaves are removed from the parent plant and placed on the bottom of a 35-gallon propagation tank under artificial lights. He reports that the plantlets started developing within days, and in just a few weeks’ time, they were large enough to be transferred to individual pots.\nSome hardy waterlily cultivars display a form of vegetative propagation that more closely resembles pseudo-vivipary. These plants produce plantlets directly from the flower. Two slightly different forms of this have been observed.\nIn the first form, the plantlet grows from the side of the flower bud. In this case, the plantlet starts to form before the parent flower even opens. What I have found in observing this behavior in N. ‘Colorado’ is that these plantlets often form blooms of their own before they form roots or leaves. The parent flower’s ability to produce pollen and seed does not appear to be inhibited.\nIn the second form, the plantlet grows from the center of the bloom. In this case, the reproductive parts of the flower are modified, and the development of the plantlet replaces the flower’s ability to produce pollen or seed.\nMany of the hardy hybrids that use this strategy can be directly or indirectly linked to Nymphaea mexicana. N. mexicana has been observed to produce plantlets off the flower, but it is very rare. The hybrids that don’t have a clear link to N. mexicana are descendants of N. ‘Colonel A.J. Welch’ or N. ‘Perry’s Fire Opal.’ Unfortunately, according to the Water Gardeners International (WGI) list of names, both of these varieties have gaps in their genealogy.\nSome examples of hybrids that have exhibited this behavior are N. ‘Barbara Dobbins’, N. ‘Cherokee’, N. ‘Georgia Peach’, N. ‘Innerlight’ and N. ‘Colorado.’\nThe Big Question\nSo, what triggers this behavior? Can it be induced, as it is in tropical hybrids? It is not commonly observed in any naturally occurring species of the subgenus Nymphaea. The one species that this has been recorded in appears to exhibit this behavior far less frequently than the descendant hybrids. This is the opposite with N. micrantha, which propagate from the leaves more readily than its descendants. This could mean that the formation of plantlets from the flowers in hardy waterlilies is more of an artifact of hybridization than a naturally developed strategy.\nThis is not to say that this strategy doesn’t occur in the wild. Pseudo-vivipary off the flowers in waterlilies has been observed in N. lasiophylla and N. prolifera; however, these tropical species are in the subgenus Hydrocallis and are not known to have been used in propagation. In both N. lasiophylla and N. prolifera, this vegetative propagation replaces sexual reproduction to the point where these plants in the wild rarely produce seed. Instead, they produce several generations of clones without leaving the parent plant.\nWithout knowing what triggers vivipary in hardy lilies, it is less likely to be a reliable propagation method for growers. I am currently experimenting with some N. ‘Colorado’ plantlets that were removed from the parent plant at different levels of development. These plantlets don’t seem to develop any substantial roots while still attached to the parent plant, so because of this, they don’t get established as easily. One curious thing that I’ve noted is that the first flower of the plantlet often doesn’t look like N. ‘Colorado.’\nSeveral species of the sedge family (Cyperaceae) also exhibit alternate propagation strategies where new plantlets are grown from the base of the inflorescence or from the stems.\nUmbrella palms Cyperus alternifolius and C. involucratus, Dwarf Papyrus (Cyperus isocladus) and Giant Papyrus (Cyperus papyrus) will produce new growth from the base of the inflorescence. This occurs mainly when the inflorescence is brought in contact with water. It’s not necessary to remove the stem from the parent plant to trigger this behavior. In C. isocladus, I have even seen it occur when the stem was merely bent down without actually touching the water.\nA different member of the Cyperaceae family uses a slightly different strategy. Dwarf Bamboo (Dulichium arundinaceum) creates plantlets along the stem when floated in water. The plantlet will sprout from between the segments of the stem.\nTo encourage this behavior, simply bend the stems downward so that at least part of the stem is floating horizontally. Once the new plantlets have formed, the parent stem can be cut off and pushed gently or slightly submerged into wet soil.\nSeveral species of the genus Nymphoides are also known to propagate viviparously by production of plantlets from leaves separated from the parent plant. This is part of why N. peltata (floating heart) can be so invasive and difficult to remove in earth-bottom ponds. The native water snowflake (N. cordata) also exhibits this behavior.\nThese propagation strategies allow aquatic plants to multiply quickly and take advantage of good growing conditions. It is not surprising that these plants have gotten so creative. In a natural environment, good growing conditions can be limited by many factors, such as water depth, clarity, temperature, sunlight, nutrients and predation. Plants are constantly jockeying for position, especially at the water’s edge, and they use these vegetative propagation strategies to try to outcompete their neighbors. As growers we can use these behaviors to our advantage, and we may even enhance or inhibit them through hybridization.', ""How to Propagate Olive Trees\nWith their evergreen foliage and spreading growth habit, olive trees (Olea europaea) add year-round ornamental value to landscaping within U.S. Department of Agriculture plant hardiness zones 8 to 10. Olive trees propagate reliably in several ways, although cuttings and seeds are the simplest means of growing new trees at home. Both methods will yield a transplantable tree within one year; however, cutting-propagated olive trees will mature faster and more closely resemble the parent tree in terms of size, growth habit and fruit production.\nPropagation From Cuttings\nPropagate olive trees from cuttings in summer once the current season's growth has begun to harden. Wait until after the blossoms have faded and the fruit has set, if the parent plant is a fruiting olive tree.\nPrepare a rooting container. Fill an 8-inch nursery container with a mix of half washed sand and half milled peat. Saturate the mix with water and press it to expel the excess. Poke a 4-inch-deep hole in the moistened mix.\nGather an 8-inch-long semi-hardwood cutting from the tip of a healthy olive branch. Choose one with a 1/4-inch diameter. Sever it 1/8 inch below a leaf node. Remove all the leaves from the base of the cutting, leaving just six or so at the tip.\nCoat the severed end of the olive cutting in 0.2-percent IBA rooting talc. Flick the stem to knock off the excess powder. Insert the cutting into the hole in the moistened sand mixture. Firm the mix against the stem.\nPlace the pot on a propagation mat inside a lightly shaded, well-ventilated cold frame or outdoors on a sheltered garden bench with light shade. Set the temperature on the propagation mat to 70 degrees Fahrenheit.\nMist the foliage twice daily with a spray bottle. Check the moisture level in the sand mixture whenever you mist the foliage. Add water if the sand feels mostly dry in the top inch.\nCheck for roots in approximately three months by gently tugging on the base of the olive cutting. Turn off the propagation mat after it roots. Continue to grow it in the cold frame with weekly watering during the winter.\nMove the olive to a shaded area of the garden in spring after the last frost. Grow it under lightly shaded conditions with 1 inch of water per week during the summer. Transplant it into a permanent bed in autumn.\nPropagation From Seed\nPropagate olive trees from seed in spring approximately four weeks before the last frost. Start multiple seeds to ensure at least one will germinate, since olive seeds have naturally low viability.\nCrack the end of each olive seed with a pair of bolt cutters or nail clippers. Compress the bolt cutters into the tip of the seed until the endocarp, or pit, begins to crack, but do not cut all the way through.\nPlace the cracked olive seeds in a bowl of warm water. Soak the seeds overnight to hydrate the embryo and prompt germination. Drain the seeds on a sheet of paper for a few minutes before sowing them.\nSow the olive seeds in individual 4-inch plastic pots filled with a moist mixture of half washed sand and half seed compost. Sow them at a depth of 1/4 inch. Gently firm the growing mix.\nPlace the pots near a bright, sunny window in an unheated room where temperatures stay around 55 degrees Fahrenheit. Keep the seeds under cool conditions for four weeks. Moisten the growing mixture whenever it feels dry below the surface.\nWarm the pots with a propagation mat set to 68 degrees. Continue to water the olive seeds whenever the growing mixture dries out. Avoid overwatering, since the seeds will go dormant or rot.\nMove the pots outdoors after the last spring frost. Wait until daytime temperatures top 65 degrees. Place the pots under light shade. Provide shelter from strong winds, since they will dry out the growing mixture.\nWatch for germination in approximately one to two months; however, don't be discouraged if it takes significantly longer for the sprouts to emerge. Continue to grow the olive seedlings under light shade for their first summer.\nTransplant the olive seedlings into 8-inch nursery containers filled with a mix of half loam and half washed sand once they grow to 3 inches in height. Acclimate the olives to full sun exposure over the course of one week.\nTransplant the olive trees into a permanent bed in the spring or autumn of their second year, or once they possess several sets of mature leaves and a thick, sturdy lead stem. Space multiple olives at least 15 feet apart.\n- 8-inch nursery container\n- Washed sand\n- Milled peat\n- Pruning shears\n- 0.2-percent IBA (indolebutyric acid) rooting talc\n- Cold frame\n- Propagation mat\n- Spray bottle\n- Bolt cutters or nail clippers\n- 4-inch plastic pots\n- Seed compost\n- Jupiterimages/Photos.com/Getty Images""]"	['<urn:uuid:31a07eae-a315-4777-9b32-2742e2164250>', '<urn:uuid:3ce5cfaa-7ae8-451c-8623-6494ae5e8859>']	factoid	with-premise	verbose-and-natural	distant-from-document	comparison	novice	2025-05-12T20:11:36.665104	29	63	2272
67	historical importance economic success middle ages france third largest city	Provins reached its peak during the reign of Thibaud IV of Champagne (1201-1253), when it had 80,000 inhabitants. Its structured economic system made it the third largest city in France after Paris and Rouen, and it became one of the commercial capitals of Europe. The city's success was built on its international trade fairs and textile industry, particularly known for its dark blue woolen sheet called the 'ners de Provins.'	"['Provins, Town of Medieval Fairs - Visit time : 1 day.\nLess than an hour from Paris, Provins is a remarkable site that plunges visitors into the heart of the Middle Ages.\nA time when the counts of Champagne were powerful enough to organize international fairs and brought together traders from the four corners of Europe, North Africa and the Middle East. The town’s structure was planned in the 12th and 13th century to accommodate the thousands of traders, and even today the town preserves this wonderful architectural ensemble, a unique testimony in its specificity and richness. But Unesco has also paid tribute to the cultural mix that allowed the fairs of Champagne to flourish. A place for commercial trade but also intellectual and cultural exchange, these fairs irreversibly influenced the changing Western societies.\nThe fortified medieval town of Provins is situated in the heart of the former territory of the powerful Counts of Champagne. It bears witness to early developments of the international trade fairs and the wool industry. Provins has preserved its urban structure, designed specifically to host the fairs and related activities.\nToday, the urban area of Provins is the most authentic testimony to medieval history from the 11th to the 13th century. Its structural and architectural heritage illustrates a formative period of Western history linked to the development of economic and cultural exchanges in Europe. As such, Provins offers a unique example of this exceptionally rich period of economic, cultural and intellectual exchange. In 2001 it was classified as a World Heritage Site by Unesco under criteria (ii) and (iv).\nCriteria for selection\nCriterion (ii): At the beginning of the 2nd millennium, Provins was one of several towns in the territory of the Counts of Champagne that became the venues for great annual trading fairs linking northern Europe with the Mediterranean world.\nCriterion (iv): Provins preserves to a high degree the architecture and urban layout that characterize these great medieval fair towns.\nSource : UNESCO / ICOMOS\nHistory of Provins\nThe site of the present town of Provins has been occupied since the Palaeolithic era. Its strategic position on a rocky overhang, as well as the rivers nearby, made it a privileged site for the installation of human communities. But the history of the town really began in the 10th century when the city was included in the county of Champagne.\nIn 996, the relics of Saint Ayoul were discovered at the foot of the Upper Town, near a chapel dedicated to Saint-Médard. This miracle was behind the construction of a monastery and a church dedicated to Saint Ayoul and many other richly endowed religious institutions – churches, chapels and monasteries – emerged. The Lower Town then developed at the confluence of the Durteint and the Voulzie rivers. But the political and economic ambitions of the Counts of Champagne would forever change the fate of Provins, and they took advantage of the strategic location of the city, at the crossroads of major thoroughfares between northern and southern Europe. The region is the obligatory passage between the ports of the North Sea and the Mediterranean. Flanders initiated trade with Northern Europe and the East, and Italy opened the doors of Byzantium, Africa and the Orient. They would create and develop an international trade fair in Provins and in three other cities in the region (Troyes, Lagny and Bar-sur-Aube). This is the beginning of the golden age of Provins and of the famous fairs of Champagne.\nFrom the 11th century onward, cloth, leather and cutlery manufacturing was developed. With the fairs, the reputation of these workshops extended across Europe and helped the city prosper. Provins is famous for the product of its textile industry: a dark blue woolen sheet called the ""ners de Provins."" The town was fortified to ensure the safety of residents, merchants and their goods, but also to affirm the power of the counts. The spectacular ramparts, part of which is still visible today, date back to the 13th century and protected the city for nearly 5 kilometers. The city reached its peak during the reign of Thibaud IV of Champagne (1201-1253), when there were no fewer than 80,000 inhabitants. Its structured economic system allowed the city to become the third in France after Paris and Rouen, and was one of the commercial capitals of Europe.\nUnderground quarries were mined for clay soil, called ""fuller\'s earth"", which was used to degrease the wool. To fully imbibe the cloth, it had to be fulled, hence the name given to this clay.\nThe fairs also accompanied the development of a multitude of activities that together inspired and encouraged a particular kind of urban fabric. The fairs influenced a cityscape designed to accommodate the many merchants (at the time Provins accounted for over 3,000 artisans, grouped on streets or in districts) with wide streets for the convoys and the location of the stalls, the 3-floor merchant houses with sumptuous vaulted rooms for warehouses are examples from this city organized and dedicated to the fairs.\nFrom the late 13th century onward, the commercial importance of Provins would gradually fade. Trade routes moved southward, and new trade fairs thrived in Flanders and the Rhine Valley, competing with the fairs of Champagne. In the early 14th century, when the region became a part of the kingdom of France, the Champagne fairs were gradually deserted. The abolition of merchant privileges, religious wars and epidemics also put an end to the Provins fair, but also those of Troyes, Lagny and Bar-sur-Aube. Of these four cities, Provins is the only one that has so beautifully preserved the architecture and urbanism that characterize these great medieval fair cities. From that point on, farming became the main economic activity of the city.\nThe fairs of Champagne\nThe Counts of Champagne decided to establish a system of biannual fairs that lasted several weeks. These fairs attracted merchants from all over Europe, North Africa and the Orient to exchange all sorts of objects : wool, linen, wine, silk, spices, furs, dyes, silverware ... Even the church imported ivory and precious woods from Africa and precious stones from the East that decorated religious objects. Fairs were places for wholesale trade among professionals, unlike the weekly or daily markets for individuals and consumers. They also become major banking centers. Provins created its own currency, the denarius Provins, which was one of the few currencies that was widely accepted throughout medieval Europe. Many European bankers established trading posts in Provins.\nFrom the 12th century onward the city had its own currency but also its own yardstick, weight and grain measurements. At that time the trips were long and perilous. And so the Counts of Champagne offered to escort merchant convoys at their expense on their territory called the ""fair conduits."" The privileges granted to merchants quickly established a reputation for the fair and in part ensured its success. On site, the counts provided security with fair guards.\nThe fair was also an opportunity for parties with live music and juggling. But above all, beyond commercial trade, it was the cultural exchanges that were essential to the development of Western societies. The wealth of the West was born of this prosperous period was accompanied by an increasingly sophisticated cultural demand. The fairs of Champagne played a leading role in the fields of literature, art and gastronomy.\nProvins Tourist Office\nProvins Tourist Office\nChemin de Villecran, 77160 Provins\nWebsite - tel : 01 64 60 26 26.\nNovember through March: 9 a.m. to 5 p.m. (from 9:30 a.m. on weekends) ;\nApril through October: 9 a.m. to 6:30 p.m.\nExtensive information on site. Brochures, 3D model of the city ...\nThe Tourist Office is the starting point for guided tours and the Mini Train. It also offers thematic tours. Dynamic and professional staff.\nThe app ""Iono"" to download from the Apple Store or Google Play. For a connected visit of the city in French, English or Spanish.\nGuided Tours offered by Provins Tourist Office\nPrice: € 8 / 4-12 years: € 5. 1h30.\nThe tours offered by the Tourist Office are many and are renewed regularly: classic or themed tours, storytelling walks or dramatized. Just make your choice.\n""Provins, City of Medieval Fair"":\nThis tour of the walled city showcases the history of Provins and provides an overview of the richness of its heritage. Information at the Tourist Office.\nTips for visitors\nThe Pass Visit\nAdvantageous to visit the four monuments of the medieval formulas: the Tour Césear, the Grange aux Dîmes (barn), the Souterrains and the Musée de Provins.\nPrice Pass Provins: €12 / 5 to 12 years old: €8.50. Family Pass: €35.50.\nThe Pass Visite also give you access to discounts on other services: train rides, medieval shows ...\nMain museums of Provins\nLa Grange aux Dîmes\n2 Rue Saint Jean, 77160 Provins\nPrice: €4.30 / 4 to 12 years old: €2.80. Free for children under 4 years old.\nNovember through March on weekends: 2 p.m. to 5 p.m.\nApril through August, daily from 10 a.m. to 6 p.m. ;\nSeptember and October, every day. Weekends: 10 a.m. to 6 p.m. Weekdays: 2 p.m. to 6 p.m.\nAudio guide in 5 languages.\nThis merchant\'s house (The Tithing Barn), built in the 13th century, housed a covered market during fairs. The lower hall served as a warehouse, the first level, a shop and the final level, housing. This stone building is representative of the Gothic architectural style developed at this time throughout the city: vaulted rooms and sculpted arches on three levels. It was subsequently used to collect taxes on crops, especially in the 17th century, called tithing. The building has kept its name ever since.\nIt now houses an exhibition of wax figures depicting various scenes of daily trades and medieval life: merchant, stone mason, potter, changer, public writer, wool worker ... The tour will show you everything you need to know about the fairs of Champagne and its participants. Fun visit for the whole family.\nMusée de Provins et du Provinois\n7 Rue du Palais, 77160 Provins\nTel : 01 64 01 40 19.\nFull price / reduced price: €4 / €2\nNovember through january, on weekends and during school holidays: 12 p.m. to 5:30 p.m. ;\nFebruary through mid-June, every day: 12 p.m. to 5:30 p.m. ;\nMid-June through mid-September, every day: 11 a.m. to 6:30 p.m. ;\nMid-September through October, every day: 12 p.m. to 5:30 p.m.\nLocated in the Maison Romane from the 12th century, certainly the oldest civil building in the city (along with the Hôtel de la Bufette), the Museum of Provins has a collection of archaeological artifacts and objects d’art from the 12th to the 19th century. In the basement and on the ground floor reside an archaeological collection that includes beautiful sculptures, ceramics and objects attesting to the ancient occupation of the site (flint weapons, prehistoric jewelry, Merovingian sarcophagi, a Gallo-Roman statue, religious objects ...). This room also presents furniture from the 17th to the 19th century, another is dedicated to clay, and a third room illustrates life in Provins in the 19th century. The first floor reveals many objects from the 13th to the 19th century (keys, locks, weapons, gems, coins and Masonic objects). The second floor is devoted to religious art from the Middle Ages to the Renaissance. You will also discover the treasure of Saint Quiriace.\nMain events of the year in Provins\nThe Médiévales de Provins (over one weekend in June)\nWebsite - tel : 01 64 00 39 39.\nThe Medieval Festival of Provins: Relive the feasts of medieval times and plunge into the atmosphere of knights and troubadours. Entertainment, crafts, jugglers, banquets ... Information at the Tourist Office.\nThe Fête de la Moisson (End of August)\nWebsite - tel : 01 64 00 59 00.\nThe Harvest Festival: In a festive atmosphere, the harvest festival takes place in the heart of the village, as it did long before. Grand parade of floats decorated with wheat, folk dances, traditional threshing, vehicles, tractors and old equipment.\nChristmas in Provins (December)\nStorytelling, ice-skating rink, entertainment ...\nCovered market under the Halle du Minage (Mining Hall) on Wednesday mornings.\nAt the Halle du Minage, Place Saint Ayoul and on adjacent streets on Saturday mornings.\nGetting to Provins\nHighway A4: Paris - Metz, exit 13 Serris - Provins.\nHighway N4: Toward Nancy, exit Provins.\nHighway A5: Toward Paris - Troyes, exit 16 Châtillon-la-Borde. Toward Troyes - Paris, exit 17 Forges.\nHighway A6: Lyon – Paris, then interchange with the A19 toward Sens, then N6 toward Pont-sur-Yonne.\nNearby classified sites\nParis - Banks of the Seine: 90 km (56 mi), time: 1 hour, 30 minutes\nChâteau de Fontainebleau: 60 km (37 mi), time: 1 hour, 20 minutes\nReims Cathedral: 113 km (70 mi), time: 2 hours, 15 minutes\nGare SNCF de Provins\nAvenue Jean Jaurès, 77160 Provins\nWebsite - tel : 36 58.\nFrom Paris : Gare de l\'Est.\nIt is recommended that you begin your tour of the city with the Eglise Saint Ayoul (church), going directly to the lower town from the Provins train station. Probus (urban bus network in Provins): from train station take line 9, then circuits C and D. Price: €2.\nFrom Apil to October a shuttle takes you from the Provins train station to the upper towm (€2.50).\n2 Rue Georges Dromigny, 77160 Provins\nWebsite - tel : 01 60 67 30 67.\nBus tours from Paris\nParis City Vision\n2 Rue des Pyramides, 75001 Paris\nWebsite - tel : 01 44 55 61 00.\nExcursions from Paris in partnership with the Regional Tourism Committee of Paris.\nGetting Around Provins\nThe walled medieval upper town is not very large, and can be easily explored on foot.\nAmple parking is available in front of the Tourist Office. Free on weekdays. Fees applicable on weekends and holidays (€4 per day) from April to October. Free parkings in the lower town.\nThe Mini Train\nPrice: €6.50 / 5 to 12 years old: €4.50.\nOperates daily from May through August and on weekends in April, September and October.\nA 30-minute tour leaves from the Tourist Office with various stops in the city. Please note: the ticket is valid all day, unlimited trips. This is a great way to discover the city and its history.\nAccommodation in Provins\nThe selection of accommodation proposed below consists of establishments that offer quality services at competitive rates, they are considered as references in their respective categories. These addresses are nearby or inside the classified area. The prices shown are for the off season, on the basis of 2 people.\nHostellerie Aux Vieux Remparts ***\n3 Rue Couverte, 77160 Provins\nWebsite - tel : 01 64 08 94 00.\nRoom from €199.\nOnly hotel in the upper town.\nB&B ""La Demeure des Vieux Bains""\n7 Rue Moulin de la Ruelle, 77160 Provins\nWebsite - tel : 06 74 64 54 00.\nRoom from €125.\nRestaurants in Provins\nThe selection of restaurants proposed below consists of restaurants that offer a good price/quality value. These addresses are inside the classified area. “Formule” corresponds to a lunch special with a starter and a main course, or a main course and a dessert. The “menus” usually consist of a starter, a main course and a dessert,"" for lunch or dinner.\nL’Appart - wine bar and bistro\n37 Rue du Val, 77160 Provins\nTel : 01 64 08 32 91.\n""Formule"" €25. ""Menu"" €32.\nLa Table Saint Jean - traditional cuisine\n3 rue Saint Jean, 77160 Provins\nWebsite - tel : 01 64 08 96 77.\n""Menu"" from €21.90.\nHostellerie de la Croix d’Or - delicate cuisine\n1 Rue des Capucins, 77160 Provins\nTel : 01 64 00 01 96.\n""Lunch menu"" during weekdays: €19.90. ""Menu"" from €31.90.\nLe Bistrot des Remparts - bistronomy\n3 Rue Couverte, 77160 Provins\nWebsite - tel : 01 64 08 94 00.']"	['<urn:uuid:bda1b819-2f39-44e4-bcc6-b54bd79e091e>']	open-ended	direct	long-search-query	distant-from-document	single-doc	novice	2025-05-12T20:11:36.665104	10	70	2647
68	zara production volume compared boutique designers	Zara produces 10,000 new items every year, which is significantly higher than boutique designers who typically release between 50 and 100 pieces annually.	"['The Business of Fast Fashion\n| OnlineMBA.com Staff Updated on June 28, 2022\nThe Business Of Fast Fashion\nThe Business Of Fast Fashion\n\'Fast Fashion\' refers to clothing and accessories that are designed to reflect current industry trends, yet produced using less expensive materials to ensure a low price tag. For the last two decades, clothing retailers like H&M, Zara, and Forever 21 have popularized Fast Fashion among everyday consumers. However, not everyone is a huge fan of the trend.\nMany designers, for instance, have complained that Fast Fashion has reduced conceptual originality within the clothing industry in order to produce a higher volume of garments and accessories. Zara, for example, churns out 10,000 new items every year; most boutique designers, by comparison, release between 50 and 100 pieces.\nThe Fast Fashion trend has also led to environmental concerns. Every year, the clothing industry produces 2 million tons of waste, emits 2.1 million tons of carbon dioxide, and uses 70 million tons of water; these figures have significantly risen in the years since Fast Fashion became a retailing standard. To make matters worse, the quality of these garments is typically so low that most are discarded or donated to charity by the wearer within two years of the original purchase.\nRoughly 300 retailers have signed on to the Sustainable Clothing Action Plan (SCAP), a collective that aims to reduce wastefulness in the fashion industry. SCAP seeks to produce and sell clothing without producing undesirable environmental effects; another goal of the group is to exclusively limit international business deals and projects to countries that have established strict labor regulations.\nStores like H&M, Zara, and Forever 21 are perfect examples of fast fashion - cheap clothes that bring catwalk styles to the public in a matter of weeks. Zara employs a large team of fashion designers and churns out 10,000 new pieces each year compared to a boutique luxury designer that might oversee production of just 50-100 pieces each year.\nSome fashion moguls embrace the trend, such as Proenza Schouler who became well-known after developing an inexpensive line for Target. But not everyone is so keen. Here are some of the ways ""fast fashion"" is impacting the marketplace.\nExcess: Not all designers are anti-fast fashion, but many agree it\'s led to a lack of authenticity in new clothing lines and a lack of quality in production. Even CEO of fast-fashion Topshop, Sir Phillip Green, says designers and consumers should stop the rapid production and ""pause for a breath now and then so fashion has a chance to become more timeless and less in one month and out the other.""\nThe Environment: The fashion industry produces 2 million tons of waste each year, 2.1 million tons of CO2, and 70 million tons of water. Fast fashion and the rapid clothing consumption caused these ballooning waste amounts and most fashion purchases - 2 million tons annually-wind up in the trash within a year or two because they are too low quality to last any longer.\nA Changing Trend: After two decades of fast-fashion popularity, many designers are signing on to make a change. Three hundred retailers have signed up to be a part of the sustainable clothing action plan which includes producing, selling, and disposing of waste without damaging the environment and only working with countries with strict labor fairness regulations.']"	['<urn:uuid:a18cbdf8-acb9-4bd1-98df-f8d9ad3c4184>']	open-ended	direct	short-search-query	distant-from-document	single-doc	expert	2025-05-12T20:11:36.665104	6	23	557
69	recycling organizations texas kansas city	Several organizations assist with recycling in Texas, including the Texas Commission on Environmental Quality (TCEQ), State of Texas Alliance for Recycling (STAR), and Keep Texas Beautiful (KTB). In Kansas City, there are specific Material Recovery Facilities (MRFs) that process recyclables, and organizations like RecycleSpot that provide recycling information and resources for the metro area.	"['Resources for Communities without Recycling\nDo you live in a community where plastics and paper pile up? What should you do with waste you know doesn’t have to go in the trash? Where are your recycle bins? Who in your city is in charge of making recycling happen? What can you do to proactively make a change toward recycling?\nWhile many areas of Texas offer recycling options, you might live in a community without direct recycling resources. For residents without a local recycling program, here are some proactive steps below to help bring your community together to recycle.\nCollaboration with Organizations\nMany Texas organizations can assist communities with recycling resources. Whether they offer direct recycling assistance, advocacy support, community event planning, or educational information, these organizations exist to help communities looking to better manage waste.\n- Texas Commission on Environmental Quality (TCEQ)\n- TCEQ’s Regional Solid Waste Grants Program\n- State of Texas Alliance for Recycling (STAR)\n- Keep Texas Beautiful (KTB); Local Affiliate Groups\n- Texas Association of Resource Conservation and Development Areas (Texas RC&D)\n- Resource Exchange Network for Eliminating Waste (RENEW)\nCheck out these organizations’ websites and areas of service for more information on how they can help you meet your community’s recycling goals!\nLocalized Community Action\nIt is important to keep in mind that the market value for certain recyclable materials fluctuates. Not all materials will have recyclable value, which is often true for certain plastic types. However, there might be local businesses and organizations that want to support recycling, or may be interested in using certain recyclable items. For instance, grocery stores and big box stores have a lot of recyclable shipping materials (i.e. cardboard boxes, paper packaging, or certain plastic containers). For these businesses, recycling not only reduces their waste, but can also support green “corporate social responsibility” programs.\nAlternatively, you might have local organizations, like a thrift store, metal scrap business, or bio-diesel producer, which would be interested in recyclable materials to use for their business. Some of the more lucrative recyclable materials include metals (i.e. steel, aluminum), most PET plastics like soda bottles, and newspaper. Research and contact these local organizations directly before dropping off materials. This saves times and prevents recyclable materials from going to landfills if they cannot handle your specific recyclable materials.\nAnother means for change is to address recycling, or lack thereof, with your city council and fellow residents. This will require a lot of work and local buy-in. You might find there are more people who want to recycle if they realize others do too. One way to emphasize your recycling needs could be to attend a city council meeting, or contact a county commissioner, and address recycling logistics as a localized service. These logistics could determine if a recycling program is feasible, or another creative route needs to be taken. A petition in support of recycling efforts can also support your recycling mission.\nShare the Logistical Load\nIf your town cannot afford to recycle, share the expense and logistics on a larger level. Explore partnerships at a county or regional level through your council of governments (COG) to achieve your recycling goals. This will require a lot of logistical coordination and local buy-in, but it can help drive cost down and make recycling more achievable. COGs can potentially obtain or use grant money toward creating a recycling program. Designated recycling sites near public buildings or monthly community recycling events may need to be established depending on when recycling can be picked up.\nA great example of a community-driven recycling program is the Kiowa Recycling Center in the small town of Booker, Texas. This student-led recycling initiative garnered support from the community, and they were recognized for their work, earning the 2016 Texas Environmental Excellence Award.\nPlan to Recycle\nRecycling is achievable for communities without recycling resources, it just takes a proactive approach to make it happen! Before you take your first steps, outline your goals and make a plan. Research your options, get your fellow residents on board, and contact your local government waste department to gather all the information and support you will need to be successful.\nHere are some additional resources depending on your recycling goals:\n- TCEQ Recycling Webpage\n- TCEQ Guidance on Texas Electronics Recycling Webpage\n- TCEQ Study on the Economic Impacts of Recycling (2017)\n- TCOT Reduce Your Waste Webpage\n- TCOT Recycle at School Blog Post\n- TCOT State of Recycling in Texas Blog Post\n- Texas School Recycling Guide Publication PDF\n- What Do I Do With It Now? A Quick Guide to Recycling Resources Publication PDF\n- Keep Texas Beautiful—find your local affiliate', 'Frequently Asked Questions\nYou’ve got questions, we’ve got answers\nQ: I\'m just one person …how much of a difference can I make?\nA: You can make an enormous impact simply by making small changes in your habits. Each of us throws away about seven pounds of waste a day. Most of what you place at the curb as trash can actually be recycled, reused or composted. So, instead of tossing those items into the garbage bin, recognize that recycling grows the economy; reduces pollution; conserves energy, landfill space and natural resources; and preserves the environment for future generations.\nQ: Where can I find a list of everything that’s recyclable in the KC metro area?\nA: Visit What do I do with…? for a complete list of everything that is recyclable, reusable, and compostable in the KC metro area. For an up-to-date list of what can and cannot go in your curbside recycling bin, download the Recycle More, Recycle Better flier.\nQ: If I’m not sure whether something is recyclable, should I go ahead and throw it in the bin?\nA: No, items that aren’t accepted by your recycling program contaminate and lower the value of the items that are accepted and can damage recycling equipment. Some of the worst offenders that should not go in your curbside bin include long, stringy items (“tanglers”) like garden hose, rope, extension cords, and shower curtains. To learn what specific items can and can’t be recycled curbside, check out our Recycle More, Recycle Better flier.\nQ: Are the materials I recycle really being recycled?\nA: Yes. Some haulers empty curbside recycling bins into the classic trash or ""packer"" trucks. Others collect recyclables and trash in ’split’ trucks. But don’t worry, it all gets sorted out at the Material Recovery Facility — your recyclables are being recycled!\nQ: Why do I have to pay to recycle?\nA: Recycling is a service just like any other we service we pay for: trash pickup, gas, water, electricity, etc. Revenues cover the cost to collect (such as fuel, personnel and vehicle maintenance), process, and ship recyclable materials.\nQ: I used to take my recyclables to a bin up at the local school/church, but it’s not there anymore. What happened to it?\nA: In the past, schools and churches could raise funds (get rebates) by recycling. But now that it costs money to recycle, you’d just be increasing their costs instead of helping them raise funds. So schools and churches have either stopped recycling or removed their bins from public use. Do you need to find a recycling drop-off center? We’ve compiled a list of recycling centers located in the Kansas City metro area. Be sure to call first to confirm hours and/or residency requirements.\nQ: If an item has a recycling symbol on it or says it’s recyclable, then it’s recyclable, right?\nA: Not necessarily. It all depends on if there is an end market for that material. Visit Plastics Recycling for a complete list of plastics that are and are not recyclable in the Kansas City area. For an up-to-date list of what can and cannot go in your curbside recycling bin, download the Recycle More, Recycle Better flier.\nQ: Can I put plastic bags in my recycling bin?\nA: No, bags and film can only be recycled at grocery stores or big box stores that accept them. Search RecycleSpot.org for the nearest location. Recycling processing facilities are not set up to recycle bags and film. They end up getting stuck in the recycling equipment and cause frequent shutdowns.\nQ: Should I bag recyclables before putting them in my recycling bin?\nA: No, keep items loose, do not put in bags or boxes. Putting them in bags or boxes doesn’t allow for proper sorting at the processing facility.\nQ: What should I do with paint and other household chemicals that I no longer need?\nA: Products labeled with DANGER, WARNING or CAUTION such as paint, cleaners, lawn and garden products, automotive fluids, bug sprays, fluorescent lights, batteries and other chemical products can all be safely disposed through your Household Hazardous Waste (HHW) program.\nQ: Are empty paint and chemical containers recyclable?\nA: No, containers labeled with DANGER, WARNING or CAUTION that contained products such as paint, lawn and garden products, automotive fluids and other chemical products are not recyclable. If they are completely empty they should be disposed in the trash. If they still contain product, they should be disposed through your Household Hazardous Waste (HHW) program. The only exception are aerosol cans - as long as they are empty and make no “hiss” sound can be recycled curbside.\nQ: Can I recycle prescription drugs?\nA: Prescription drugs aren’t recyclable, but they need to be properly disposed of. For options, visit Prescription Drugs.\nQ: Can I recycle sharps like needles?\nA: Sharps like needles, syringes, lancets and injection pens aren’t recyclable, but they need to be properly disposed of. For options, visit Prescription Drugs.\nQ: Can I recycle diapers?\nA: No, diapers should be disposed in the trash.\nQ: Are paper food containers such as frozen food, fast food, and takeout recyclable?\nA: No, these types of paper food containers have a thin plastic coating that makes them non-recyclable. The only exception are aseptic boxes (milk cartons, juice cartons, and soup, broth, wine, and juice boxes) that have a specialized end market. Aseptic boxes have either a gabled or flat top and a plastic cap. They can be recycled in your curbside bin or at recycling centers.\nQ: Are disposable plates, cups and cutlery recyclable?\nA: Disposable plastic plates, cups and bowls are recyclable, but only if they are clean. Plastic cutlery (forks, knives, spoons, etc.), because of their small size and shape, fall through the sorting machinery are not recyclable. Paper tableware has a thin plastic coating that makes them non-recyclable.\nQ: Are pizza boxes recyclable?\nA: Only the clean part (usually just the top) is recyclable. Throw the rest of it in the trash or backyard compost bin if you have one. Cardboard is accepted in all curbside recycling programs and at recycling centers.\nQ: Are paper towels, napkins and facial tissues recyclable?\nA: No, tissue products should be disposed in the trash.\nQ: Do I need to rinse containers before recycling?\nA: Yes, rinse all containers (cans, bottles, tubs, etc.). A quick swish of water is all they need, no need to wash them thoroughly. With peanut butter jars, you have several options:\n- Wipe residue out with a paper towel.\n- Scrape residue out with a spatula.\n- Scrub residue out with soap and water.\n- Run the jar through the dishwasher when washing a load of dishes.\nQ: So, what’s so bad about putting food and liquids in a recycling bin?\nA: Food and liquids contaminate other recyclables lowering or totally negating their value. Liquids, especially hazardous liquids, could cause an entire truckload of recyclables to have to go to the landfill. Always make sure recyclable containers are empty and rinsed before putting them in your recycling bin.\nQ: What kinds of plastic are and are not recyclable?\nA: Visit our Plastics Recycling page for a detailed list of plastics that can and cannot be recycled in the Kansas City metro area.\nQ: Is Styrofoam™ recyclable?\nA: Expanded polystyrene (EPS) food containers such as cups, egg cartons, takeout cartons, and meat trays are not recyclable due to food contamination and low value. Clean, white EPS block, sheet and packing peanuts are recyclable at ACH Foam Technologies, in Kansas City, Kansas, (913) 321-4114. EPS peanuts can also be recycled at select UPS Stores.\nQ: Is shredded paper recyclable?\nA: Shredded paper is not accepted in curbside recycling or at recycling centers because recycling processing facilities are not set up to sort it. Recyclers can take shredded paper from commercial shredders because it bypasses the sorting process. Here’s the best way to deal with paper that you need to shred:\n- Only shred private information - If you shred at home, rip off and shred only the parts of documents that contain private information: credit card numbers, bank account information, or Social Security numbers. Consider reusing it as a packing material or put it in your compost bin. Otherwise, place it in the trash. Place the rest of the document in your recycling bin.\n- Shredding services – Most office supply stores including FedEx Office Print & Ship Center, Office Depot / Office Max, Staples, and The UPS Store provide shredding services for a fee and will recycle what they shred. Many communities offer paper shred events for their residents. Check with your community to find out about upcoming paper shredding events.\nQ: Should I put caps back on or leave them off plastic bottles before recycling?\nA: Put lids back on plastic bottles. For all other plastic caps and lids, store them in a plastic jar or tub. When the container is full, toss it into a recycling bin. These practices help ensure they don’t fall through the processing machinery.\nQ: Are metal caps and lids recyclable?\nA: Yes, but to help ensure they don’t fall through the processing machinery, collect them in a steel (tin) can and crimp the top shut before recycling.\nQ: Why do I need to break down boxes before recycling them?\nA: To save space, a couple boxes can take up an entire recycling bin. Also, breaking down boxes prevents other recyclables from getting trapped inside during the sorting process. If your flattened box is still too big for the bin, place it under or tightly between bins to keep wind from blowing it away.\nQ: Why can’t I recycle glass curbside?\nA: Your regular hauler doesn’t pick up glass food and beverage containers because it’s dangerous to workers, contaminates other recyclables when it breaks, and because of the low value of materials from which it is made. You can drop off your glass at Ripple Glass bins located throughout the metro area. There are also several glass collection companies in the metro area that offer curbside pickup for a fee: Atlas Glass KC, GlassBandit, Glass Gone, KC Curbside Glass and Wright Brothers Curbside Glass Recycling.\nQ: When recycling paper, do I need to remove things like staples and paper clips?\nA: Staples and paper clips do not need to be removed. You should remove larger items such as binder clips and spiral binding (whether metal or plastic).\nQ: Do I need to remove labels from containers before recycling?\nA: No, they get removed during processing.\nQ: What types of trash and recycling services are available in my community?\nA: Most communities in the Kansas City metro area offer their residents a combination of services either through curbside collection or drop-off facilities. Visit your community’s page to see what services are offered in your city or county.\nQ: Why can\'t I recycle certain materials?\nA: To successfully recycle a material, three conditions must be met: the ability to collect it, the ability to process it, and a viable end market where companies purchase the material and make new products with it. If any one of these three is not available, the material cannot be recycled.\nQ: Once recycled material leaves my curb or the recycling center, what happens to it?\nA: Materials are taken to a materials recovery facility (MRF). At the MRF, every item is separated according to type and baled. The MRF then ships the materials to brokers or directly to manufacturers. The recyclables are then made into new products, closing the recycling loop. There are three MRFs that serve the Kansas City metro area. This Waste Management video shows the step-by-step process of how materials are processed once they reach a MRF.\nQ: Can someone give a presentation on waste reduction and recycling to my group?\nA: Our Recycle More At Work program offers presentations and many other free resources. Contact us today!\nQ: My company or organization accepts materials for recycling and/or resale, how can I get listed on RecycleSpot?\nA: Visit Add Your Services.\nQ: I need some reliable facts and figures on recycling, can you help?\nA: Yes, the following sources offer reliable recycling facts and figures:\nFor more detailed information on specific materials, search for trade associations, coalitions, councils and institutes. Some examples: American Forest & Paper Association, Carton Council, Glass Packaging Institute, Steel Recycling Institute, The Aluminum Association and The Association of Plastic Recyclers.']"	['<urn:uuid:5137904e-f447-4e3a-a016-7f03d6fc0579>', '<urn:uuid:2742ae4d-8b65-4f50-ab6f-aeef251d635f>']	factoid	direct	short-search-query	distant-from-document	multi-aspect	novice	2025-05-12T20:11:36.665104	5	54	2850
70	ways criminals trick employees download malware	Cybercriminals commonly use several tactics to trick employees into downloading malware. One method is through invoice scams, where they send emails that appear to be from colleagues or business associates with fake invoice attachments, typically as PDF, Word documents, or zip files. When opened, these attachments initiate malware downloads. Another tactic is 'drive-by-downloads,' where criminals scan for browser vulnerabilities on websites to install ransomware without user knowledge or consent.	['The risk of ransomware attacks has never been higher as cybercriminals have evolved their attacks and malware to be more difficult than ever to detect and prevent. The latest Sophos report “The State of Ransomware 2021” sums it up with around half of respondents believing that: “…ransomware attacks are getting increasingly hard to stop due to their sophistication.”\nTechnology intervention is helping somewhat in preventing ransomware attacks, but it is not enough. The Sophos report highlights the fact that ransomware attackers are using a mix of off-the-shelf ‘spray and pay’ and individual targeting. It is this targeting of individuals that adds a layer of difficulty in using technology to prevent attacks.\nBecause of the focus on the human, individual employees play a crucial role in defending an organisation from cyber attacks of all kinds, including ransomware. Here are our top seven suggestions for empowering employees with the know-how needed to reduce the risk of a ransomware attack.\nHow Employees Can Help Thwart The Risk of Ransomware Attacks\nRansomware attacks come in many forms, but a regular target of a hacker is an individual. Our employees are a doorway into the organisation if a cybercriminal knows how to open that door. The key to that door is often social engineering and phishing that starts the process of access to protected areas of a network. By empowering our employees with an understanding of how ransomware attacks begin, an organisation can reduce the likelihood of ending up a victim of malware. Here are seven top ways for employees to reduce the risk of a ransomware attack:\nTrain Employees to Spot the Tell-Tale Signs of Phishing\nPhishing is a facilitator of ransomware attacks. Phishing and the results of phishing and spear-phishing, such as stolen login credentials, form the starting point of many ransomware attacks. Once an attacker has access rights, they can use those credentials to send internal emails (containing malware or links to malware infected websites) and/or login to enterprise systems using the Remote Desktop Protocol (RDP). A recent Windows vulnerability demonstrates how easy it is to escalate privileges using stolen login credentials. The vulnerability, known as PrintNightmare, facilitates the use of non-privileged login credentials to escalate the privileges of the user to allow install of malware across a network.\nThe act of empowering employees with knowledge of how phishing emails and spoof websites work can help stop a ransomware infection at the starting point of an attack. One tool that is used to help to train employees spot a phishing attempt is to use phishing simulations. This is a tool that can be tailored to your specific corporate needs to teach employees how to identify a phishing threat and reduce the risk of a ransomware attack.\nReport any Suspicious Emails Immediately\nIf an incident does happen, for example, an employee clicks a phishing link and enters credentials into a spoof site, timely action is of the essence. A company has a short window of opportunity to mitigate the threat and stop an incident from becoming a ransomware infection. A report by Agari found that once credentials are stolen two-thirds of email accounts will be compromised the same day.\nIncident reporting should be part of your organisation’s culture. But incident reporting needs to be encouraged and made simple by using a workflow-based reporting system that is designed to take the incident information quickly and simply before sending the data to the most appropriate person to deal with to help minimise the risk of ransomware attacks.\nDon’t Overshare Personal Information\nPhishing messages are often tailored to specific employees to make them more effective. Attackers may use social engineering tricks to obtain personal information to create these personalised spear-phishing messages. Teach your staff not to give out personal data unless necessary. This includes posting information on social media platforms, which are trawled for the data of employees of targeted companies.\nDon’t open Suspicious Attachments in Emails\nEmployees must not open attachments unless they are sure they have come from a legitimate source. Phishing emails can also be used to deliver ransomware directly using malicious attachments. One example of ransomware delivery via email attachment is that of invoice scams. The email looks like it has come from a colleague or a business associate, and it contains what looks like an invoice, typically as a PDF or Word document or sometimes a zip file. If an employee clicks to open the attachment, this action initiates a malware download via a link in the attachment file. The malware will exploit any vulnerabilities (known or unknown) to execute the code and infect the machine.\nOnly use Verified and Known Sources for File Downloads\nEmployees should never download files or media from unvetted sites. The tactic known as a drive-by-download is used by cybercriminals to download malicious software without the knowledge or consent of the user. Drive-by-download attacks use scanners to look for vulnerabilities in browsers and other device software that is then exploited to install ransomware. Whilst your security team can put in measures such as keeping software patches up to date, employees should be trained to work with security policies that recognise the online dangers of drive-by-downloads to prevent the risk of ransomware attacks.\nUse a VPN when using Public Wi-Fi\nIf an employee works remotely, they need to be aware of the dangers of using unsecured public Wi-Fi hotspots. If a private and secure Wi-Fi is not available, users should switch on a Virtual Private Network (VPN), or better still, always have a VPN running. A VPN creates an encrypted tunnel between the user’s browser and the internet. The VPN prevents any ‘Man-in-the-Middle’ attacks whereby a malicious outsider steals data, such as login credentials or personal information, or even potentially, injects malicious code such as ransomware.\nDon’t Reuse Passwords\nStolen passwords are available for sale on the dark web and are behind 81% of hacking-related data breaches according to Verizon. These ‘passwords for sale’ are from previously phished credentials and hacked databases. The cybercriminals who buy these lists then use automation tools to hack into existing accounts. Even hashed passwords from database breaches can be broken. Forced reset of passwords also doesn’t work as many employees simply add an extra number or letter to the end of a previously used password. Corporate password policies need to be enforced through a culture of understanding that security is important to everyone.\nBuild a Ransomware Force Field with Your Employees\nRansomware is big business and cybercriminals behind the attacks are serious about it. In a recent exposé, a new ransomware gang, BlackMatter was found to be offering $100,000 for exclusive access to an organisation’s network to deploy ransomware and exfiltrate data. The prevention of ransomware is not a simple fix; however, your staff can be an effective front-line defence against attacks. It is only by including your employees in a 360-degree approach to prevent the risk of ransomware attacks that an organisation can hope to stave off an attack.']	['<urn:uuid:bec7fc27-fa48-4e8f-8557-88fcfcadb22f>']	open-ended	direct	short-search-query	distant-from-document	single-doc	novice	2025-05-12T20:11:36.665104	6	69	1159
71	compare navigation techniques ancient mediterranean versus vikings fog overcast conditions	In the Mediterranean, ancient sailors restricted sea travel between October and April partly due to unreliable skies, relying mainly on celestial bodies and soundings in clear conditions. Vikings, however, potentially navigated through fog using sunstone crystals (like Iceland Spar) to filter polarized light and determine the Sun's position, although this technique required careful calibration and worked best during specific times like summer solstice.	"['For half a century, historians have wondered if Viking sailors travelled long distances under dense cloud cover under the guidance of a crystal called a sunstone.\nConclusive evidence might be forever buried in the past, but a new study details the kinds of conditions ancient mariners might have used this tool, demonstrating it was possible for Vikings to navigate through fog using little more than a polarising rock and some mathematical know-how.\nResearch conducted by Eötvös Loránd University in Hungary evaluated several types of polarising crystal under a range of meteorological conditions throughout the year to test a hypothesis first proposed back in 1967.\nAccording to Danish archaeologist Thorhild Ramskou, Vikings managed to pinpoint the Sun\'s position through cloud cover by holding up a birefringent material – such as crystallised calcium carbonate called Iceland Spar – to filter the polarised light.\nSunlight passing through the translucent object might then be used in conjunction with a solar compass to work out the relative time and direction, which would aid in determining a position on a map.\nIt was a convenient solution to the mystery surrounding the journey of ancient Scandinavian seafarers to destinations as far away as North America.\nUnder perfect conditions the Sun, stars, marine life, and coastal features could be enough to mark off the milestones needed to navigate such epic treks across the expanse of an ocean.\nThe thing is, the conditions up around the icy North Atlantic aren\'t always ideal. If the rolling expanses of fog don\'t make life inconvenient for the navigator, the frequent cloud cover will certainly create problems.\nThat might be less of an issue if they\'d had magnetic compasses to point to the poles, but there\'s no evidence Vikings possessed such technology.\nThere is mention of objects called ""solar stones"" in various medieval texts, however.\nWhile historical writings hint at their use in locating the Sun through the clouds, they don\'t tend to provide many details on exactly how they were used.\nRamskou was the first modern academic to suggest these objects were crystals that could detect the Sun\'s precise position based on their refraction of polarised light.\nAs sunlight hits materials in the atmosphere, such as drops of water vapour, it scatters, making it all but impossible to tell the Sun\'s position.\nLuckily the orientation of the scattered light is slightly different to the sunlight that slips through the clouds. By rotating a polarising filter in front of the eyes, it\'s possible to map the sky\'s brightness and trace the Sun\'s location.\nBirefringent materials split incoming light into two, giving a sort of double image. The intensity of each image will vary, depending on the angle and polarisation of the light source.\nIt\'s not quite as simple as holding up a rock to unveil the Sun hiding behind the fog, requiring some careful calibration and a keen eye.\nThe hypothesis has plenty of supporters, even if it\'s a little short on hard evidence. No confirmed samples of sunstone have been uncovered, for instance, and only a single fragment of a potential Viking-age solar compass has ever surfaced.\nTo help fill in a few details on what is a highly speculative proposal, the Hungarian researchers tested three types of birefringent crystal under 1,080 variations of Sun angle and degrees of cloud cover set under laboratory conditions in a planetarium.\nCalcite, cordierite, and tourmaline crystals were all up to the task of determining the angle of the Sun\'s elevation, especially at times close to dawn and dusk. On average, calcite was found to be the most accurate.\nAt some times of the year, such as summer solstice or spring equinox, and at particular elevations and cloud covers, tourmaline and cordierite produced in smaller uncertainties.\nThese results highlight the ideal conditions sunstones could have been used, and the degree of uncertainties they\'d produce.\nIn short, if the Vikings used birefringent crystals at all, they would have found them most useful when the Sun was relatively low in the sky near summer solstice, perhaps around early to mid morning, when it\'s obscured by low level fog or cloud cover.\nThat\'s far from proof the Norse navigators used them at all. The data was also collected on solid ground inside a room, and not out on the rolling seas.\nBut nothing in the research rules out the possibility at least, adding a degree of credibility to the hypothesis that Vikings ruled the icy Atlantic waves with a sword in one hand and a gem in the other.\nThis research was published in the Proceedings of the Royal Society A.', 'A compass (or mariner\'s compass) is a navigational instrument for finding directions on the earth. It consists of a magnetized pointer free to align itself accurately with Earth\'s magnetic field, which is of great assistance in navigation. The cardinal points are north, south, east and west. A compass can be used in conjunction with a chronometer and a sextant to provide a very accurate navigation capability. This device greatly improved maritime trade by making travel safer and more efficient. An early form of compass was invented in China in 271 C.E. and is one of four great inventions of ancient China. The familiar mariner\'s compass was invented in Europe around 1300.\nMore technically, a compass is a magnetic device using a needle to indicate the direction of the magnetic north of a planet\'s magnetosphere. Any instrument with a magnetized bar or needle turning freely upon a pivot and pointing in a northerly and southerly direction can be considered a compass. A compass dial is a small pocket compass with a sundial. A variation compass, a specific instrument with a delicate construction, is used by observing variations of the needle. A gyrocompass or astrocompass can also be used to ascertain true north.\nPrior to the introduction of the compass, directions at sea were determined primarily by the position of celestial bodies. Navigation was supplemented in some places by the use of soundings. Difficulties arose where the sea was too deep for soundings and conditions were continually overcast or foggy. Thus the compass was not of the same utility everywhere. For example, the Arabs could generally rely on clear skies in navigating the Persian Gulf and the Indian Ocean (as well as the predictable nature of the monsoons). This may explain in part their relatively late adoption of the compass. Mariners in the relatively shallow Baltic made extensive use of soundings.\nDue to the place of its first appearance, most scholars credit at present the invention of the compass to China. Since there has been frequently confusion as to when a compass was introduced for the first time, it may be appropriate to list the important events leading up to its invention in chronological order:\nThere is much debate on what happened to the compass after its first appearance with the Chinese. Different theories include:\nThe latter two are supported by evidence of the earlier mentioning of the compass in European works rather than Arabic. The first European mention of a magnetized needle and its use among sailors occurs in Alexander Neckam\'s De naturis rerum (On the Natures of Things), probably written in Paris in 1190. Other evidence for this includes the Arabic word for ""Compass"" (al-konbas), possibly being a derivation of the old Italian word for compass.\nIn the Arab world, the earliest reference comes in The Book of the Merchants\' Treasure, written by one Baylak al-Kibjaki in Cairo about 1282. Since the author describes having witnessed the use of a compass on a ship trip some forty years earlier, some scholars are inclined to antedate its first appearance accordingly. There is also a slightly earlier non-Mediterranean Muslim reference to an iron fish-like compass in a Persian talebook from 1232.\nThere have been various arguments put forward whether the European compass was an independent invention or not:\nArguments that support independent invention:\nArguments against independent invention:\nIn the Mediterranean the practice from ancient times had been to curtail sea travel between October and April, due in part to the lack of dependable clear skies during the Mediterranean winter (and much of the sea is too deep for soundings). With improvements in dead reckoning methods, and the development of better charts, this changed during the second half of the thirteenth century. By around 1290 the sailing season could start in late January or February, and end in December. The additional few months were of considerable economic importance; it enabled Venetian convoys, for instance, to make two round trips a year to the eastern Mediterranean, instead of one.\nAround the time Europeans learned of the compass, traffic between the Mediterranean and northern Europe increased, and one factor may be that the compass made traversal of the Bay of Biscay safer and easier.\nIn 1936 Tuomas Vohlonen of Finland invented and patented the first successful portable liquid-filled compass designed for individual use.\nA magnetic rod is required when constructing a compass. This can be created by aligning an iron or steel rod with Earth\'s magnetic field and then tempering or striking it. However, this method produces only a weak magnet so other methods are preferred. This magnetised rod (or magnetic needle) is then placed on a low friction surface to allow it to freely pivot to align itself with the magnetic field. It is then labeled so the user can distinguish the north-pointing from the south-pointing end; in modern convention the north end is typically marked in some way, often by being painted red.\nFlavio Gioja (fl. 1302), an Italian marine pilot, is sometimes credited with perfecting the sailor\'s compass by suspending its needle over a fleur-de-lis design, which pointed north. He also enclosed the needle in a little box with a glass cover.\nModern hand-held navigational compasses use a magnetized needle or dial inside a fluid-filled (oil, kerosene, or alcohol is common) capsule; the fluid causes the needle to stop quickly rather than oscillate back and forth around magnetic north. Most modern recreational and military compasses integrate a protractor with the compass, using a separate magnetized needle. In this design the rotating capsule containing the magnetized needle is fitted with orienting lines and an outlined orienting arrow, then mounted in a transparent baseplate containing a direction-of-travel (DOT) indicator for use in taking bearings directly from a map. Other features found on some modern handheld compasses are map and romer scales for measuring distances and plotting positions on maps, luminous markings or bezels for use at night or poor light, various sighting mechanisms (mirror, prism, etc.) for taking bearings of distant objects with greater precision, \'global\' needles for use in differing hemispheres, adjustable declination for obtaining instant true bearings without resort to arithmetic, and devices such as inclinometers for measuring gradients.\nThe military forces of a few nations, notably the United States Army, continue to utilize older lensatic card compass designs with magnetized compass dials instead of needles. A lensatic card compass permits reading the bearing off of the compass card with only a slight downward glance from the sights (see photo), but requires a separate protractor for use with a map. The official U.S. military lensatic compass does not use fluid to dampen needle swing, but rather electromagnetic induction. A \'deep-well\' design is used to allow the compass to be used globally with little or no effect in accuracy caused by a tilting compass dial. As induction forces provide less damping than fluid-filled designs, a needle lock is fitted to the compass to reduce wear, operated by the folding action of the rear sight/lens holder. The use of air-filled induction compasses has declined over the years, as they may become inoperative or inaccurate in freezing temperatures or humid environments.\nOther specialty compasses include the optical or prismatic hand-bearing compass, often used by surveyors, cave explorers, or mariners. This compass uses an oil-filled capsule and magnetized compass dial with an integral optical or prismatic sight, often fitted with built-in photoluminescent or battery-powered illumination. Using the optical or prism sight, such compasses can be read with extreme accuracy when taking bearings to an object, often to fractions of a degree. Most of these compasses are designed for heavy-duty use, with solid metal housings, and many are fitted for tripod mounting for additional accuracy.\nMariner\'s compasses can have two or more magnetic needles permanently attached to a compass card. These move freely on a pivot. A lubber line, which can be a marking on the compass bowl or a small fixed needle indicates the ship\'s heading on the compass card.\nTraditionally the card is divided into thirty-two points (known as rhumbs), although modern compasses are marked in degrees rather than cardinal points. The glass-covered box (or bowl) contains a suspended gimbal within a binnacle. This preserves the horizontal position.\nLarge ships typically rely on a gyrocompass, using the more reliable magnetic compass for back-up. Increasingly electronic fluxgate compasses are used on smaller vessels.\nSome modern military compases, like the [SandY-183 http://www.orau.org/PTP/collection/radioluminescent/armycompass.htm](the one pictured) contains the radioactive material Tritium (3H) and a combination of Phosphorous. The SandY-183 contained 120mCi (millicuries) of tritium. The name SandY-183 is derived from the name of the company, Stocker and Yale (SandY).\nSmall compasses found in clocks, cell phones (e.g. the Nokia 5140i) and other electronic gear are Solid-state electronics usually built out of two or three magnetic field sensors that provide data for a microprocessor. Using trigonometry the correct heading relative to the compass is calculated.\nOften, the device is a discrete component which outputs either a digital or analog signal proportional to its orientation. This signal is interpreted by a controller or microprocessor and used either internally, or sent to a display unit. An example implementation, including parts list and circuit schematics, shows one design of such electronics. The sensor uses precision magnetics and highly calibrated internal electronics to measure the response of the device to the Earth\'s magnetic field. The electrical signal is then processed or digitized.\nA bearing compass is a magnetic compass mounted in such a way that it allows the taking of bearings of objects by aligning them with the lubber line of the bearing compass.\nLike any magnetic device, compasses are affected by nearby ferrous materials as well as by strong local electromagnetic forces. Compasses used for wilderness land navigation should never be used in close proximity to ferrous metal objects or electromagnetic fields (batteries, car bonnets, engines, steel pitons, wristwatches, and so forth.)\nCompasses used in or near trucks, cars or other mechanized vehicles are particularly difficult to use accurately, even when corrected for deviation by the use of built-in magnets or other devices. Large amounts of ferrous metal combined with the on-and-off electrical fields caused by the vehicle\'s ignition and charging systems generally result in significant compass errors.\nAt sea, a ship\'s compass must also be corrected for errors, called compass deviation, caused by iron and steel in its structure and equipment. The ship is swung, that is rotated about a fixed point while its heading is noted by alignment with fixed points on the shore. A compass deviation card is prepared so that the navigator can convert between compass and magnetic headings. The compass can be corrected in three ways. First the lubber line can be adjusted so that it is aligned with the direction in which the ship travels, then the effects of permanent magnets can be corrected for by small magnets fitted within the case of the compass. The effect of ferromagnetic materials in the compass\'s environment can be corrected by two iron balls mounted on either side of the compass binacle. The coefficient <math>a_0</math> representing the error in the lubber line, while <math>a_1,b_1</math> the ferromagnetic effects and <math>a_2,b_2</math> the non-ferromagnetic component.\nFluxgate compasses can be calibrated automatically, and can also be programmed with the correct local compass variation so as to indicate the true heading.\nThe simplest way of using a compass is to know that the arrow always points in the same direction, magnetic North, which is roughly similar to true north. Except in areas of extreme magnetic declination variance (20 degrees or more), this is enough to protect from walking in a substantially different or even opposite direction than expected over short distances, provided the terrain is fairly flat and visibility is not impaired. In fact, by carefully recording distances (time or paces) and magnetic bearings traveled, one can plot a course and a return to one\'s starting point using the compass alone.\nHowever, compass navigation used in conjunction with a map (terrain association) requires a different compass method. To take a map bearing or true bearing (a bearing taken in reference to true, not magnetic north) to a destination with a protractor compass, the edge of the compass is placed on the map so that it connects the current location with the desired destination (some sources recommend physically drawing a line). The orienting lines in the base of the compass dial are then rotated to align with actual or true north by aligning them with a marked line of longitude (or the vertical margin of the map), ignoring the compass needle entirely. The resulting true bearing or map bearing may then be read at the degree indicator or direction-of-travel (DOT) line, which may be followed as an azimuth (course) to the destination. If a magnetic north bearing or compass bearing is desired, the compass must be adjusted by the amount of magnetic declination before using the bearing so that both map and compass are in agreement. In the given example, the large mountain in the second photo was selected as the target destination on the map.\nThe modern hand-held protractor compass always has an additional direction-of-travel (DOT) arrow or indicator inscribed on the baseplate. To check one\'s progress along a course or azimuth, or to ensure that the object in view is indeed the destination, a new compass reading may be taken to the target if visible (here, the large mountain). After pointing the DOT arrow on the baseplate at the target, the compass is oriented so that the needle is superimposed over the orienting arrow in the capsule. The resulting bearing indicated is the magnetic bearing to the target. Again, if one is using \'true\' or map bearings, and the compass does not have preset, pre-adjusted declination, one must additionally add or subtract magnetic declination to convert the magnetic bearing into a true bearing. The exact value of the magnetic declination is place-dependent and varies over time, though declination is frequently given on the map itself or obtainable on-line from various sites. If not, any local walker club should know it. If the hiker has been following the correct path, the compass\' corrected (true) indicated bearing should closely correspond to the true bearing previously obtained from the map.\nBecause the Earth\'s magnetic field varies at different latitudes, compasses are often balanced during manufacture. Most manufacturers balance their compass needles for one of five zones, ranging from zone 1, covering most of the Northern Hemisphere, to zone 5 covering Australia and the southern oceans. This balancing prevents excessive dipping of one end of the needle which can cause the compass card to stick and give false readings. Suunto has recently introduced two-zone compasses that can be used in one entire hemisphere, and to a limited extent in another without significant loss of accuracy.\nOriginally, many compasses were marked only as to the direction of magnetic north, or to the four cardinal points (north, south, east, west). Later, mariners divided the compass card into 32 equally spaced points divided from the cardinal points.\nThe 360-degree system later took hold, which is still in use today for civilian navigators. The degree dial spaces the compass markings with 360 equidistant points. Other nations adopted the \'grad\' system, which spaces the dial into 400 grads or points.\nMost military defense forces have adopted the \'mil\' system, in which the compass dial is spaced into 6400 units (some nations use 6000) or \'mils\' for additional precision when measuring angles, laying artillery, and so forth.\nSome different compass systems:\nAll links retrieved June 11, 2013.\nNew World Encyclopedia writers and editors rewrote and completed the Wikipedia article in accordance with New World Encyclopedia standards. This article abides by terms of the Creative Commons CC-by-sa 3.0 License (CC-by-sa), which may be used and disseminated with proper attribution. Credit is due under the terms of this license that can reference both the New World Encyclopedia contributors and the selfless volunteer contributors of the Wikimedia Foundation. To cite this article click here for a list of acceptable citing formats.The history of earlier contributions by wikipedians is accessible to researchers here:\nNote: Some restrictions may apply to use of individual images which are separately licensed.']"	['<urn:uuid:7e43b7f3-790f-4e02-86f9-15254200b307>', '<urn:uuid:1b8c6216-856b-465d-bc9b-cd1ad598cd9d>']	factoid	direct	long-search-query	similar-to-document	comparison	expert	2025-05-12T20:11:36.665104	10	63	3451
72	native plants green infrastructure environmental benefits	Native plants in green infrastructure serve multiple environmental functions. In the Rain Yard installation, carefully selected native species like goldenrod, aster, and blue flag iris help manage stormwater. More broadly, such vegetation provides crucial benefits including improving air quality by reducing ground-level ozone and particulate pollution, creating wildlife habitat, and helping mitigate urban heat island effects by shading surfaces and releasing moisture into the atmosphere.	['by Stacy Levy\nRain Yard is an interactive artwork on permanent display in the Schuylkill Center’s Sensory Garden. This innovative artwork serves both a practical function—mitigating stormwater runoff from our building—and an interpretive function—highlighting the critical role that soil and plants play in the water cycle.\nThrough interdisciplinary collaboration with ecologists, engineers, designers, educators and horticulturists, this artwork was created.\nView video by Ben Kalina\nIn Rain Yard, we can see every part of the rain’s journey. But we don’t have to just look: we can also play with the pump and hoses to see how water filters over a variety of surfaces. Pump the pump handle and collected rainwater goes from the cistern through a hose and into the lower spiral. Follow the red pipe from the cistern to the pump, and create your own rain.\nThe plants below the platform grow in a basin, or bowl, dug into the ground. The size of the basin was calculated to hold the water that comes off the roof in a typical heavy rain fall. The basin holds the rainwater long enough for the plants and ground to soak it up.\nThe openwork platform allows rain to filter down, plants to grow up, and people to hover in between. The plants in the garden were carefully selected. They are all native to the area, and include goldenrod, aster, cardinal flower, blue flag iris, and rushes, grasses and ferns.\nRain Yard is meant to engage and educate. Along with the installation is an engaging graphic book about water, stormwater runoff, and information about the project.\nRain Yard is a collaboration with the rain. It captures the rain from the roof and leads the rain to a planted place to soak into the ground. Rain needs time and space to soak in, but in most of our built world, we do not give any space for the rain to act like rain—instead we pipe it away. This artwork is making a home for the rain.\nMy work is about making metaphors for people to understand how nature works. I always hope my pieces will give someone a new avenue to understand something about nature. I think that everyone deserves a re-explanation of the everyday workings of the world. In some of my work, art can be an important new way to fix things that are not working well on a site. Rain Yard is trying to fix a rainwater issue in an artful way. An engineer might fix a rainwater problem one way, and a landscape gardener would do it another way. I have tried to take all of those perspectives and to solve the problem while making an intriguing spatial and visual experience out of the solution.\nAbout the artist\nStacy Levy is an artist who works with natural processes of the surrounding nature. She received a BA at Yale (1984) where she majored in sculpture with a minor in forestry. This combination of art and nature has remained entwined throughout her path as an artist. Stacy co-founded Sere Ltd., a design firm specializing in native landscape restoration for municipal, corporate and private landscapes across the mid-Atlantic region. The firm works to bring the architecture of a healthy ecosystem back to disturbed forest landscapes. In 1988, Levy attended Skowhegan School of Painting and Sculpture and earned an MFA from Tyler School of Art, Temple University (1991) while working as a forester. The combination of forestry and art has continued to inform her practice.\nFor more of Stacy’s work, visit: http://www.stacylevy.com.\nRain yard was made possible by generous support from Arcelor Mittal, Johnson & Johnson, Penn Engineering, Sherwin Williams and the National Endowment for the Arts.', 'Green infrastructure is a cost-effective and resilient approach to our water infrastructure needs that provides many community benefits.\nWater Quality and Quantity\nWater Quality: Stormwater from urban areas delivers many pollutants to our streams, lakes, and beaches - including pathogens, nutrients, sediment, and heavy metals. In cities with combined sewer systems, high stormwater flows can also send untreated sewage into our waters. By retaining rainfall from small storms, green infrastructure reduces stormwater discharges. Lower discharge volumes translate into reduced combined sewer overflows and lower pollutant loads. Green infrastructure also treats stormwater that is not retained.\nFlooding: Conventional stormwater infrastructure quickly drains stormwater to rivers and streams, increasing peak flows and flood risk. Green infrastructure can mitigate flood risk by slowing and reducing stormwater discharges.\nWater supply: Rainwater harvesting and infiltration-based practices increase the efficiency of our water supply system. Water collected in rainwater harvesting systems can be used for outdoor irrigation and some indoor uses and can significantly reduce municipal water use. Water infiltrated into the soil can recharge groundwater, an important source of water in the United States.\nPrivate and Public Cost Savings: When stormwater management systems are based on green infrastructure rather than gray infrastructure, developers often experience lower capital costs. These savings derive from lower costs for site grading, paving, and landscaping, and smaller or eliminated piping and detention facilities. In cities with combined sewer systems, green infrastructure controls may cost less than conventional controls, and green-gray approaches can reduce public expenditures on stormwater infrastructure.\nGround Level Ozone: Ground level ozone or smog, is created when nitrogen oxides (NOx) and volatile organic compounds (VOCs) interact in the presence of heat and sunlight. Smog conditions are usually worst in the summer and can lead to respiratory health problems. Vegetation can reduce ground level ozone by reducing air temperatures, reducing power plant emissions associated with air conditioning, and removing air pollutants.\nParticulate Pollution: Particulate matter refers to the tiny bits of dust, chemicals, and metals suspended in the air we breathe. Because particulate matter is so small, it can enter into the lungs and cause serious health effects. Trees, parks, and other green infrastructure features can reduce particulate pollution by absorbing and filtering particulate matter.\nHealth Effects: Breathing ground level ozone and particulate pollution can cause respiratory ailments including chest pain, coughing, aggravation of asthma, and even premature death. In their triple bottom line study on the benefits of green infrastructure, the City of Philadelphia found that increased tree canopy would reduce ozone and particulate pollution levels enough to significantly reduce mortality, hospital admissions, and work loss days.\nEnergy and Climate Change\nUrban Heat Island: Urban heat islands form as cities replace natural land cover with dense concentrations of pavement, buildings, and other surfaces that absorb and retain heat. Trees, green roofs, and other green infrastructure features can cool urban areas by shading building surfaces, deflecting radiation from the sun, and releasing moisture into the atmosphere.\nEnergy Use: By reducing local temperatures and shading building surfaces, green infrastructure lessens the cooling and heating demand for buildings, reducing energy needs and decreasing emissions from power plants.\nClimate Change: As different parts of the country become drier, wetter, or hotter, green infrastructure can help communities adapt to climate change by increasing the capacity of drainage systems to handle large storms, increasing the resilience of water supply systems in times of drought, and mitigating the urban heat island effect. Urban vegetation can also mitigate climate change by reducing the levels of greenhouse gases in the atmosphere.\nWater/Energy Nexus: Treating and moving drinking water and wastewater takes a lot of energy. By reducing stormwater inflow into sewer systems, recharging aquifers, and conserving water, green infrastructure can significantly reduce energy use.\nHabitat and Wildlife\nHabitat Improvement: Vegetation in the urban environment provides habitat for birds, mammals, amphibians, reptiles, and insects. Even small patches of vegetation such as green roofs can provide habitat for a variety of insects and birds. By reducing erosion and sedimentation, green infrastructure also improves habitat in small streams and washes.\nHabitat Connectivity: Large scale green infrastructure, such as parks and urban forests, also help to facilitate wildlife movement and connect wildlife populations between habitats. Learn how Loxahatchee, Florida is protecting the local watershed and conserving native ecosystems through the Loxahatchee Regional Greenways System.\nGreen jobs: Green infrastructure can reduce a community’s infrastructure costs, promote economic growth, and create construction and maintenance jobs. As demand for green infrastructure skills increases, a range of new training and certification programs has emerged.\nHealth Benefits: More green space and parks encourages outdoor physical activity, reducing obesity and preventing associated chronic diseases such as heart disease, high blood pressure, stroke, Type II diabetes, arthritis, and certain kinds of cancer.\nRecreation space: Green infrastructure’s vegetation and trees can increase publicly available recreation areas, allowing urban communities to enjoy greenery without leaving the city. Additionally, green infrastructure’s vegetation and permeable pavements can reduce noise pollution by damping traffic, train, or plane noise.\nProperty values: By utilizing green infrastructure in construction and increasing vegetation and tree cover, green infrastructure can increase property values']	['<urn:uuid:279e991b-c8db-480e-b353-bc77146d5bb4>', '<urn:uuid:9cb7dab0-7fea-445e-bbe3-af81dae72270>']	factoid	direct	short-search-query	similar-to-document	multi-aspect	expert	2025-05-12T20:11:36.665104	6	65	1458
73	phone bank apps perks dangers comparison	Mobile banking apps offer several advantages including real-time access to account information, time savings, and convenience of managing finances from anywhere. However, they also face security challenges, particularly due to third-party components that may expose banks to risks. These components can have vulnerabilities that lead to data breaches, as seen in cases like British Airways. Security measures include biometric authentication, encryption, multi-factor authentication, and transaction alerts. Additionally, banks implement tamper detection, integrity protection, and code review to protect against threats. When using these apps, it's crucial to avoid public Wi-Fi networks and use secure connections to prevent data theft.	['This post was written in collaboration with Neal Michie, Director, Product Management, Verimatrix.\nBanks are facing massive disruption and change from many directions. The rise of app-only banks has made the need for traditional banks to have compelling app services an imperative. Banks have of course been building mobile apps for several years. If not already, they will soon be the most important channel for engaging with and serving customers. However, mobile banking apps will also become the primary focus of hackers, intent on getting access to other people’s information and money.\nHow have mobile banking apps evolved?\nFrom lightweight apps supporting basic banking functions they have now evolved into full service branches including all manner of sophisticated features such as biometric identification, payments, loyalty and personal finance management driven by data science and machine learning.\nSome of the things you might expect to find feature rich mobile apps include:\n- Numerous APIs supporting the large number of features – creating a large attack surface through API Abuse.\n- A mixture of native code, managed and web-code – providing different levels of control over how sensitive data is handled.\n- Dozens of dependencies on third party SDKs and libraries – each outside of the direct control of the bank.\n- Hundreds of thousands of lines of code – making isolating and auditing critical security functionality very difficult.\n- Use of WebViews to render web-code – providing great flexibility but making the app more reliant on external services for its security.\nThird-party components are a particular issue\nThe use of third-party components may expose banks to new security risks. If these risks are not carefully managed and the customer data is compromised, then banks risk regulatory fines and reputation damage, not to mention the inconvenience and worry caused to customers.\nTo give you an idea of the size of the issue – third-party components can provide all manner of functions such as remote user monitoring, instrumentation, error and crash reporting, profiling, binding the user to the device, analytics, cryptography, UI enhancing, financial charts and many more. Components may be proprietary or open source code. They are integrated into the app where they may gather or process data and connect to third-party backend servers that may or may not be under the control of the bank.\nThe risk isn’t just theoretical. The well-publicised attack on British Airways breached the airline through known vulnerabilities in third party code. The business risk: a fine equivalent to 1.9% of revenue and long-term reputation harm – we are still talking about it 2 years later.\nWhile not exhaustive, some of main security issues to watch out for include:\n- Auditing: Banks may not have access to the source code of proprietary third-party SDKs and libraries. This makes the task of ensuring that the software is safe much more difficult. Penetration testing techniques can be used to monitor the behaviour of the component, but this is no substitute for a source code review.\n- Deployment model: Often third-party components need to connect to the third-party backend services. Sometimes those backend services have to be operated by the third party. Other times it may be possible to bring those backend services in-house. Obviously bringing them in-house gives more control to the bank to ensure any sensitive data stays within their infrastructure. Where this is not possible, then the bank may unwittingly introduce a “data processor” in GDPR terms into their service without the necessary oversight.\n- Known vulnerabilities: Third party components may in turn utilise various other proprietary or open source libraries. Some of these dependencies may have known vulnerabilities which leave the bank app exposed.\n- Potential information disclosure in transit: Third party SDKs may not have adequate transport security enabled. For instance, no or weak TLS certificate pinning may be implemented, making any communication between mobile apps and the backend susceptible to man-in-the-middle attacks. This can potentially leak sensitive information in transit.\n- Potential information disclosure at rest: Third party SDKs may gather and handle sensitive customer or bank information, and this may be cached in the persistent storage without encryption or without being cleared from memory after use. A backup of the device/app can potentially expose sensitive information if not adequately protected.\n- Potential information disclosure due to misconfiguration: Backend service endpoints necessarily allow connections from the mobile apps. If these are misconfigured, then they may potentially leak sensitive information. A recent example was the exposure of personal data by Google Firebase. Data exposed included email addresses, usernames, passwords, phone numbers, full names, GPS information, IP addresses and street addresses.\nHow can we mitigate the identified security risks?\n- Risk Assessment: A risk assessment of third-party components will help to determine whether using “black box” components is acceptable. Consult Hyperion’s Structured Risk Assessment (SRA) methodology is designed for just this, ensuring that technical decisions have the right business context.\n- Code review and penetration testing: Source code review and penetration testing should be a standard process in any bank, employed for every release. Banks should go beyond the use of automated scanning and analysis tools. These may help in catching common issues but will not cover everything. For third party components, the options available could include reverse engineering (which is costly) or dynamic testing, where the behaviour of the component and its external communications are monitored real-time as it is used.\n- Tamper detection and integrity protection: Banks should also utilise tamper and integrity protection to protect code and builds – often known as App Shielding or In-App Protection. This should cover the integration of any third-party components where possible – either separately or as a whole. By anchoring third party libraries and obfuscating their boundaries, vulnerabilities become much hard to find and exploit. This additional layer of security can safeguard mobile banking apps and provide security assurance against reverse engineering and runtime hacks.\nProtecting the personal and financial information of customers is a fundamental responsibility of banks. Doing so in mobile banking apps is more important than ever. The use of third-party components makes this more challenging. The contract a bank has with a third party may only provide very limited protection when things go wrong and it will certainly not cover the detrimental risk of losing customer trust and business. Great care is needed therefore, when choosing, integrating and deploying third party components. On the other hand, third party components allow banks to provide their customers with better services quicker, so it is imperative that they employ best practice to ensure they serve their customers well, maintaining their trust.\nAbout the Co-Author\nNeal Michie serves as Director of Product Management for Verimatrix‘s award-winning code protection solutions. Helping countless organisations instil trust in their IoT and mobile applications, Neal oversees Verimatrix’s foundational security products that are relied upon by some of the world’s largest organisations. He champions the need to position security as a top-notch concern for IoT companies, seeking to elevate code protection to new heights by serving as a sales enabler and brand protector. Neal brings more than 18 years of software development experience and spent the last decade building highly secure software solutions, including the first to be fully certified by both Mastercard and Visa. A graduate of Heriot-Watt University with an advanced degree in electrical and electronics engineering, Neal is an active and enthusiastic participant in Mobey’s expert groups.', 'Are you bored with ready in lengthy strains on the financial institution or making an attempt to squeeze in a go to throughout your busy work schedule? Look no additional than cellular banking apps.\nWith only a faucet in your cellphone, you’ll be able to have entry to your entire monetary info and carry out transactions proper from the palm of your hand. Cell banking apps supply a mess of benefits over conventional banking strategies.\nNot solely do they prevent time, however in addition they present straightforward accessibility and comfort. On this final information to cellular banking apps, we’ll discover the options to search for in an app, safety measures which can be in place, and examine a number of the high cellular banking apps available on the market.\nSo sit again and prepare to take management of your funds with only a few faucets in your cellphone!\nIf you happen to’re within the technical aspect of those apps, and even contemplating creating your individual, you may discover this text on tips on how to create a cellular banking app useful.\n- Cell banking apps present straightforward accessibility and comfort, saving time and permitting use from anyplace.\n- Safety features comparable to MFA, biometric authentication, and transaction alerts present added safety for customers’ accounts.\n- Key options to search for in a cellular banking app embrace real-time steadiness and transaction historical past, fund switch choices, and robust safety measures.\n- Maximizing the cellular banking expertise entails utilizing a safe community, organising alerts for account exercise, and exploring all of the options of the app. Nonetheless, warning should be taken when utilizing public Wi-Fi as a result of vital danger of information theft.\nThe Benefits of Cell Banking Apps\nWith cellular banking apps, you’ll be able to simply handle your funds on-the-go, saving time and rising comfort. Gone are the times while you needed to go to a financial institution department or ATM to verify your account steadiness, switch funds, or pay payments. With only a few faucets in your smartphone display, you’ll be able to accomplish all these duties and extra from just about anyplace.\nOne of many key benefits of cellular banking apps is that they can help you keep on high of your funds in real-time. You now not have to attend for paper statements or log into a pc to see how a lot cash is in your accounts. With a cellular app, you’ll be able to immediately view your balances, transaction historical past, and different account particulars as quickly as they turn into accessible.\nOne other advantage of cellular banking apps is that they provide enhanced security measures in comparison with conventional strategies of banking. Most apps require customers to arrange multifactor authentication (MFA), which provides an additional layer of safety towards fraudsters who may attempt to entry your accounts with out authorization. Moreover, many banks use encryption know-how to safeguard delicate info comparable to passwords and account numbers from being intercepted by hackers.\nIf you happen to’re on the lookout for a handy and safe option to handle your funds whereas on-the-go, cellular banking apps are undoubtedly value contemplating. They provide real-time entry to account info and transactions, enhanced security measures like MFA and encryption know-how, and the power to carry out varied monetary duties from anyplace with an web connection. So why not give them a attempt at present?\nOptions to Search for in a Cell Banking App\nWhen selecting a cellular banking app, it’s essential to search for options that make managing your funds simpler and extra handy. One of the essential options to contemplate is the power to view account balances and transaction historical past in real-time. This lets you preserve monitor of your spending and keep away from overdraft charges by figuring out precisely how a lot cash you could have accessible.\nOne other characteristic to search for is the power to switch funds between accounts or ship cash to family and friends simply. Many cellular banking apps supply fast and safe methods to switch cash with out having to go to a bodily financial institution location or use third-party companies like Venmo or PayPal. This will prevent time and problem, particularly if it is advisable ship cash urgently.\nSafety must be a high precedence when selecting a cellular banking app. Search for apps that supply biometric authentication, comparable to fingerprint or facial recognition, in addition to two-factor authentication for added safety. It’s additionally essential that the app makes use of encryption know-how to guard your monetary info whereas it’s being transmitted over the web.\nBy choosing an app with sturdy safety measures in place, you’ll be able to have peace of thoughts figuring out that your delicate info is protected from cyber threats.\nSafety Measures in Cell Banking Apps\nDefend your monetary info from cyber threats by selecting a cellular banking app with sturdy safety measures. These measures embrace biometric authentication and encryption know-how. Biometric authentication ensures that solely you’ll be able to entry your account via fingerprint or face recognition. Encryption know-how protects your information by changing it into unreadable code. These options make it tough for hackers to steal your private info and use it for fraudulent actions.\nOther than these primary safety measures, some cellular banking apps additionally supply further options like transaction alerts and two-factor authentication. Transaction alerts notify you each time a transaction is constituted of your account, permitting you to detect any unauthorized exercise instantly. Two-factor authentication entails utilizing two totally different strategies to confirm your identification earlier than granting entry to the app, making it virtually unattainable for anybody else to log in with out authorization.\nAll the time be cautious when utilizing public Wi-Fi, because it poses a big danger of information theft. Keep away from logging in to delicate accounts like on-line banking when utilizing public Wi-Fi networks or unsecured units. When accessing cellular banking apps outdoors safe networks, use a VPN (Digital Personal Community) that encrypts all visitors between the machine and the web connection.\nBy taking these precautions and selecting a dependable cellular banking app with sturdy safety measures, you’ll be able to benefit from the comfort of digital banking with out compromising on security.\nIdeas for Maximizing Your Cell Banking Expertise\nTo get essentially the most out of your cellular banking expertise, you’ll need to make the most of some easy but efficient ideas.\nAt the beginning, be sure to’re utilizing a safe community when accessing your cellular banking app. Keep away from utilizing public Wi-Fi or different unsecured networks that might probably compromise your private info.\nOne other useful tip is to arrange alerts for any exercise in your account. This will embrace notifications for deposits, withdrawals, and even low balances. By staying on high of those alerts, you’ll be able to shortly determine any fraudulent or unauthorized exercise in your account.\nLastly, don’t be afraid to discover all of the options of your cellular banking app. Many apps supply further instruments comparable to budgeting and monetary planning sources that may enable you handle your cash extra successfully. Take a while to familiarize your self with all the pieces the app has to supply, so you’ll be able to absolutely maximize its potential and benefit from your cellular banking expertise.\nThe Way forward for Cell Banking Apps\nAs we delve into the way forward for cellular banking apps, it’s clear that innovation and know-how will proceed to drive this sector ahead. The digital banking panorama is evolving at an unprecedented tempo, with new developments rising which can be set to redefine the way in which we handle our funds.\nOne vital pattern is the combination of synthetic intelligence (AI) in cellular banking apps. AI can improve personalization, making banking extra intuitive and user-friendly. For example, AI-powered chatbots can present instantaneous customer support, whereas machine-learning algorithms can supply customized monetary recommendation primarily based on a person’s spending habits and monetary targets.\nOne other rising pattern is the convergence of banking with different monetary companies. Increasingly more, we’re seeing cellular banking apps providing a one-stop-shop for all monetary wants, together with insurance coverage, investments, and even cryptocurrency transactions. This pattern in direction of ‘tremendous apps’ is especially prevalent in Asia and is more likely to unfold globally.\nBiometric authentication is one other space the place we are able to anticipate to see developments. Whereas fingerprints and facial recognition are already utilized by some banks, future apps could incorporate extra subtle biometric applied sciences, comparable to voice recognition and even heartbeat evaluation, for enhanced safety.\nBlockchain know-how, recognized for its position in cryptocurrencies, additionally holds potential for cellular banking, providing potentialities for safe, clear transactions and lowered fraud.\nThe way forward for cellular banking apps is undoubtedly thrilling, promising a world the place managing funds is extra seamless, safe, and tailor-made to particular person wants than ever earlier than.\nSo there you could have it – the final word information to cellular banking apps! By now, you need to be satisfied of the various benefits of utilizing a cellular banking app.\nNot solely does it save time and problem, however it additionally offers you larger management over your funds. When buying round for a cellular banking app, keep in mind to search for options comparable to straightforward navigation, real-time updates, and robust safety measures.\nAnd don’t neglect to match totally different apps to seek out the one which most closely fits your wants. With the following tips in thoughts, you’re properly in your option to maximizing your cellular banking expertise.\nSo why not give it a attempt at present? You may simply be shocked at how a lot simpler managing your cash will be when all the pieces is true at your fingertips!']	['<urn:uuid:4bab8ad2-1615-4748-815d-4c7bea6fcb3e>', '<urn:uuid:ad6225c9-a712-4135-a360-d744f025f3fe>']	open-ended	with-premise	short-search-query	distant-from-document	three-doc	novice	2025-05-12T20:11:36.665104	6	99	2852
74	I'm curious about math in everyday life - how do numbers help GPS work?	GPS navigation relies on several mathematical principles involving pi and complex numbers. The GPS satellites need to account for time dilation effects predicted by Einstein's field equations, where pi appears in describing spacetime curvature. Additionally, complex numbers are crucial in electronics and electromagnetic applications that enable GPS functionality, as they help describe circuit elements' states through voltage and current relationships. The Fourier transform, which also involves pi, is essential for processing the signals used in GPS systems.	"['March 14 is no regular Pi Day this year. Math enthusiasts everywhere are celebrating the event as Ultimate Pi Day, because the date will correspond to the first five digits of pi (3.1415) rather than just the first three. This mathematical confluence won\'t come around again for a century, on March 14, 2115.\nDefined as the ratio of a circle\'s circumference to its diameter, pi (π) is both a transcendental and an irrational number, meaning it can never be written as the ratio of two whole numbers, and it continues indefinitely without any repeating pattern. Pi isn\'t the only irrational number—there\'s also Euler’s number (e) and the golden ratio (φ, or phi), for example. But it continues to be a source of fascination because its origins are easily explainable, says Mario Livio, an astrophysicist at the Space Telescope Science Institute in Maryland.\n“Everyone can understand how pi is derived. All of the other numbers are more complex. The number phi, for example, involves a particular division of a line, and the number e requires you to know what a logarithm is,” says Livio, author of the book Is God a Mathematician?\nAnother big part of pi’s appeal is that it has an uncanny knack for appearing in mathematical formulas, many of which are important for everyday processes from image processing to GPS navigation. Here is just a small sampling of the commonly used formulas that include pi:\nThe Fourier Transform\nNamed for French mathematician Jean-Baptiste Joseph Fourier, this mathematical tool decomposes a signal into its component frequencies—rather like how a musical chord can be broken down into its component notes. In essence, Fourier transforms are ideal for processing wave-based signals such as sound or light and finding patterns. That makes the Fourier transform a fundamental tool in the modern digital world.\n“It’s been called the single most important algorithm ever developed by mankind. Now that may be hyperbole, but maybe it’s not,” says Glen Whitney, founder and director of the National Museum of Mathematics in New York City. Fourier transforms are used all the time to clean up digital images, to Auto-Tune pop stars, and to find far-off planets orbiting other stars. The tool is also crucial for the voice-to-text features that are now standard on smartphones. “When you use Siri or Google Now, one of the first steps is to take your voice and do a Fourier transform on it … it turns out to be much easier to recognize vowels when you look at their Fourier transforms than when you look at the original signals themselves,” Whitney says.\nPi appears in the Fourier transform because one of the component parts, or expressions, of the formula is associated with sine and cosine and the angles created by a particle traveling around a circle. “Whenever you have a formula that deals with circles or angles, you are not going to be surprised when pi shows up,” Whitney says.\nHeisenberg Uncertainty Principle\nOne of the pillars of quantum mechanics, Heisenberg’s uncertainty principle states that an observer can\'t know both the position and speed of a subatomic particle simultaneously. Instead, the more precisely a particle’s position is known, the less can be known about its velocity.\nThe appearance of pi in Heisenberg’s uncertainty principle makes sense when you realize that in the formula, position and momentum are Fourier transforms of one another, Whitney says. The uncertainty principle is important in the modern world because it describes the behavior of light particles, or photons, in fiber optics communications systems. “What it tells us is that we can’t know both the position and the momentum of photons with extreme precision. You can’t design communications protocols that violate Heisenberg’s uncertainty principle, because they won’t work.”\nStoke’s law calculates the force needed to move a small sphere—that is, a three-dimensional circle—through a viscous fluid at a certain velocity. It has applications in fields ranging from Earth sciences to medicine.\n“The law is specifically about the effect of viscosity on a sphere in the fluid,” Whitney says, which is how pi comes into play. As for practical uses of Stoke’s law, look no further than your car. “For decades, the way companies made sure your motor oil had the right viscosity for your car was to literally drop a series of test spheres into the oil and measure the time it takes them to fall through the liquid,” Whitney says. Today, the most common way to measure oil viscosity involves a tool called a capillary tube viscometer, no spheres needed—but it still reports the outcome in units of measurement called centistokes.\nNamed after Swiss mathematician Leonard Euler, the version of this formula that includes pi gathers some of the most intriguing numbers in mathematics in one place:\n“Everyone just thinks this is incredible. All of these numbers that we consider special appear in one beautiful equation,” Livio says. While this pithy formula can inspire awe in mathematicians, the more useful form of the equation is slightly longer:\nThis unpacked version of Euler’s formula is an incredible tool, Whitney says. For example, it’s important for designing electronics that use alternating current, or AC. “Euler’s formula in the expanded form means you can use complex, or imaginary, numbers to analyze and design AC circuits,” Whitney says. That\'s because in an alternating current circuit, the voltage is a quantity that oscillates over time—typically 60 times per second, for example, in standard U.S. electrical supply. “The full version of Euler\'s formula teaches how we can use complex numbers as a convenient shorthand for modeling oscillating phenomena,"" Whitney says.\nEinstein’s Field Equations\n“Describing that curvature involves geometry, and since the original definition of pi comes from geometry, it’s appearance in this equation is not that surprising,” says Livio. In addition to revealing a fundamental truth about how the universe works, general relativity has many practical applications. For example, the satellites that make up the Global Positioning System used for navigation would be hopelessly out of sync with one another if engineers did not take into account the time dilation effects predicted by the theory.', ""Complex numbers are numbers that consist of two parts — a real number and an imaginary number. Complex numbers are the building blocks of more intricate math, such as algebra. They can be applied to many aspects of real life, especially in electronics and electromagnetism.\nThe standard format for complex numbers is a + bi, with the real number first and the imaginary number last. Because either part could be 0, technically any real number or imaginary number can be considered a complex number. Complex does not mean complicated; it means that the two types of numbers combine to form a complex, like a housing complex — a group of buildings joined together.\nReal numbers are tangible values that can be plotted on a horizontal number line, such as fractions, integers or any countable number that you can think of. Imaginary numbers are abstract concepts that are used when you need the square root of a negative number.\nAdding & multiplying complex numbers\nBecause a complex number is a binomial — a numerical expression with two terms — arithmetic is generally done in the same way as any binomial, by combining the like terms and simplifying. For example:\n(3 + 2i) + (4 - 4i)\n(3 + 4) = 7\n(2i - 4i) = -2i\nThe result is 7-2i.\nFor multiplication, you employ the FOIL method for polynomial multiplication: multiply the First, multiply the Outer, multiply the Inner, multiply the Last, and then add. For example:\n(3 - 2i)(5 + 3i) =\n(3)(5) + (3)(3i) + (-2i)(5) + (-2i)(3i) =\n15 + 9i + -10i + -6i2 =\n15 - i - 6(-1) =\n21 - i\nThe reason that i2 simplifies to (-1) is because i is the square root of -1.\nDividing complex numbers\nDivision, however, becomes more complicated and requires using conjugates. Complex conjugates are pairs of complex numbers that have different signs, such as (a + bi) and (a - bi). Multiplying complex conjugates causes the middle term to cancel out. For example:\n(a + bi)(a - bi) = a2 - abi + abi - (bi)2\nThis simplifies to a2 - b2(i2) = a2 - b2(-1)\nThe final result is a2 + b2\nWhen dividing complex numbers, determine the conjugate of the denominator and multiply the numerator and denominator by the conjugate. For example,\n(5 + 2i) ÷ (7 + 4i)\nThe conjugate of 7 + 4i is 7 - 4i. So, multiply the numerator and denominator by the conjugate:\n(5 + 2i)(7 – 4i) ÷ (7 + 4i)(7 - 4i) =\n(35 + 14i – 20i – 8i2) ÷ (49 - 28i + 28i – 16i2 ) =\n(35 - 6i + 8) ÷ (49 + 16) =\n(43 - 6i) ÷ 65\nAbsolute value of complex numbers\nThe absolute value of a number is considered its distance from zero on the number line. Because complex numbers include imaginary numbers, they cannot be plotted on the real number line. However, they can be measured from zero on the complex number plane, which includes an x axis (for the real number) and the y axis (for the imaginary number).\nUses of complex numbers\nComplex numbers can be used to solve quadratics for zeroes. The quadratic formula solves ax2 + bx + c = 0 for the values of x. If the formula provides a negative in the square root, complex numbers can be used to simplify the zero.\nComplex numbers are used in electronics and electromagnetism. A single complex number puts together two real quantities, making the numbers easier to work with. For example, in electronics, the state of a circuit element is defined by the voltage (V) and the current (I). Circuit elements can also have a capacitance (c) and inductance (L) that describes the circuit's tendency to resist changes in V and I. Rather than describing the circuit element's state by V and I, it can be described as z = V + Ii. The laws of electricity can then be expressed using the addition and multiplication of complex umbers.\nAs mentioned before, this can also be applied to electromagnetism. Instead of being described as electric field strength and magnetic field strength, you can create a complex number where the electric and magnetic components are the real and imaginary numbers.""]"	['<urn:uuid:986c1d2d-1ebe-460c-8fcc-54d6e1b24407>', '<urn:uuid:eb7eb00d-fac4-402a-a054-333e7ad2a7cb>']	open-ended	with-premise	concise-and-natural	distant-from-document	three-doc	novice	2025-05-12T20:11:36.665104	14	77	1728
75	competitive archer interested target setup mongolian archery versus compound bow archery differences	In Mongolian archery, targets are small cylinders arranged in a pyramid formation at ground level, and archers shoot in an arching trajectory to hit these ground-level targets. This differs from compound bow archery, where archers typically shoot parallel to the ground using peep sights to aim at raised targets, maintaining their stance until the arrow hits.	"[""This post is part of a four-part series on Mogolian Naadam (Three Manly Sports). Naadam is a festival traditionally held within the first half of July every year in Mongolia. You can read the other sections here: Horse Racing, Wrestling, Ankle Bones.\n“Archery has as ancient a history as wrestling; both are mentioned in the Secret History of the Mongols, and an inscription in stone, a little older than the Secret History and also of the thirteenth century, records an arrow shot of 335 ‘spans.'”\n-Owen Lattimore (Nomads and Commissars, p. 22)\nThe three manly sports build better soldiers. Since the Mongolian armies were well known and feared for their horse-back archery skills, it should come as no surprise that archery is the last of the trio. The archery is standing, however, and not done on horseback for Naadam.\nThe most important piece of equipment for this sport is the Mongolian bow. You cannot simply buy yourself a bow from a shop; you must have one custom made for your stature. Each bow is built specifically for its archer, so you cannot borrow a friend’s bow unless you want to struggle with firing it correctly. The bow is usually made of wood (although there are some made exquisitely from the horns of animals), with a removable thread that slips into Y-shaped joints at the end of the bow. I believe the correct term is a compound bow. While I am not skilled enough to describe proper firing techniques, or how the bow and shooting differs from other regions, the more die-hard archers out there can possibly discern these differences from the photographs in this post.\nMen, women, and young people can all participate in the archery competition. Men only compete with men, women with women, and so forth. There are lines set up for each group, while a small set of cylinders is set up in a pyramid-esqué formation at the opposite end of the field. Archers take their stance and shoot an arching shot, so that the arrow rises and falls onto the ground-level targets (unlike raised bulls-eye targets you see frequently in archery competitions). You accumulate points based on the number of cylinders you can knock over.\nSome families have a long tradition of archery. One such family, in the village I lived in for training, has three generations of archers. The grandfather, now retired, set up a horn-bow factory. The father and mother both compete on the national level, competing in the prestigious Ulaan Baatar Naadam. There is also, of course, their little girl, who while still quite young, is learning the family trade.\n3 thoughts on “Naadam: Archery”\nIs the distance to the target different for each group? Great picture of the little girl: The feminine frilly dress and determined look on her face.\nThanks for the positive comment on the picture! It took me forever to get the right shot of her. As for the archery question, they all have different distances: The young girls are the closest, followed by the young boys, but then all the adult men and women shoot from the same distance.\nI’m not an archery expert by any means, but I do have a little bit of experience in the field. The bows are recurves, not compounds; a compound bow has a complicated system of pulleys and cams that increases the force with which the string propels the arrow, and the draw has a breaking point where the draw weight drops precipitously. Recurves are curved in a direction opposite the one in which the bow is bent, which increases both the draw weight and the force of propulsion. I learned to shoot on a recurve, though the shape of an Occidental recurve is a little different from that of a Mongolian one. The counter-curvature is more gradual, and the bow is shorter, so you don’t have to draw the string past your ear as the adults pictured above are doing.\nThe other main difference is the way the bow is drawn. The little girl has one finger above the arrow and two below it, which the Western hold used by Olympic-style archers. The boy at the top has all three fingers below the arrow, which is how bare-bow archery is usually taught in the West. But if you look closely at the adults, you can see that they’re holding the string between the thumb and forefinger, which is typical of Oriental archery. The way they point the bow upwards while drawing and then take aim is also an Oriental characteristic."", 'For anyone looking to master the art of archery, they would be glad to learn just how helpful the advent of the compound bow is, and how it has helped the sport develop. While compound bows do make life easier by improving let-offs, with all their features and clever contraptions, learning how to properly use one takes some skill and lots of practice.\nWith the right guidance, however, learning how to shoot a compound bow can be done in no time, leaving an archer to focus solely on their aim.\nThe first step in mastering the perfect shot is to know your bow. A compound bow has many features and knowing how each of them works is essential in getting it right. For some pieces, such as adjustable sights, simply knowing what the piece does is of no use unless you understand the degree to which that particular part works.\nTake your time familiarizing yourself with your bow, covering as much theoretical knowledge as you can before you start practicing. This will help you avoid getting discouraged by simply picking up the instrument, and as a consequence, failing miserably.\nNaturally, the more you practice, the more you get to know your particular bow as each one is different.\nFacing Your Target\nThe next step is to focus on your own body. A good archer does not have to be the bulkiest or most ripped person on the field, but knowing how to use those muscles is of some considerable importance.\nThe entire point behind a compound bow’s design is to alleviate the strain on an archer’s muscle, providing greater acceleration and steadiness with minimal effort, but knowing that you should draw an arrow back using your back muscles as opposed to your arms, for example, is crucial if you want to make the most out of your bow’s technology.\nKnowing how to stand is important as well, where facing your target at a right angle, with the leg on the opposite end of the arm you use to draw slightly ahead of the other. So if you are a leftie, place your right foot slightly forward towards the target and leave the other behind.\nTheoretical familiarization of your bow turns into practical familiarization even before you grab the arrow. The fact of the matter is that compound bows can get pretty bulky, despite generally having a light weight, but there are measures taken to make holding one easier, that is why knowing how to is important.\nBy placing your thumb through the handle, leave the other four fingers loose so as to avoid gripping the bow too hard and making it shake as a result.\nOnce you’ve handled the bow, and you can move it up and down with ease, you need to know how and where to place the arrow. Arrows come with splits at their ends and compound bows come with nocks where the bottom end of the arrow is fitted.\nOnce the arrows bottom end has clicked into the nock, an archer then angles his arm parallel to the ground, towards the target, before readying to let go and send the arrow flying.\nFinally, you need to find your anchor point. This is a fixed place where, with the arrow drawn back, the string touches on your chin and slightly on your nose, serving as an indicator for every draw.\nAssuming your compound bow has come with a peep sight, and your arm runs parallel towards the ground, use this to point at your target. Once you release, assume your entire stance as if you are still aiming until the arrow hits its target. This is important as it guarantees that you do not disrupt the arrow along its way.\nHaving followed through all of the above, you should be able to feel the full power of the compound bow, made possible by its advanced features. Without feeling the strain, and with minimal effort, your arrows should be let off with a greater speed and a better direction than you would expect, especially if you are just getting started.\nBut for all the right guidance and all the advanced technology, an archer cannot expect perfect results unless they keep on practicing until all the aforementioned steps become second nature and hitting the target becomes the norm.']"	['<urn:uuid:5797f3bd-00e4-48ea-af4f-79679c33ea8f>', '<urn:uuid:6e8ee2ad-d068-41ba-8d8d-1798e94f99b9>']	factoid	with-premise	long-search-query	similar-to-document	comparison	expert	2025-05-12T20:11:36.665104	12	56	1479
76	How do the views of abnormal behavior treatment differ between ancient Greek physicians like Hippocrates and medieval European practitioners, particularly in terms of their underlying beliefs about causes and treatment approaches?	Hippocrates believed mental illness had natural causes and treated it through methods like bleeding, purging, and proper nutrition to balance bodily fluids called 'humors.' In contrast, during the Middle Ages in Europe, abnormal behavior was primarily viewed as demonic possession, with treatment consisting of prayer and exorcism. The procedures for exorcism were often extreme, including starving, whipping, and beating the 'possessed' person to drive out evil spirits.	"['3 Historical Perspectives on Abnormal behavior The Ancient WorldChina (200 BC) Chung Ching stated that both organ pathologies & stressful psychological situations were causes of mental disorders.GreeceHippocrates ( BC) believed mental illness was the result of natural, as opposed to supernatural, causes.Galen ( AD) divided the causes of mental disorders into physical and psychological explanations.\n4 Middle Ages ( AD)Islamic countries- a. mental hospitals were established (792 AD)b. Persian doctor Sina wrote the Canon of Medicine(medications).Europe –abnormal behavior was most frequently viewed as demonic possession.treatment entailed – prayer & exorcism.\n5 The Renaissance ADSpanish nun Teresa of Avila ( ) established the conceptual framework that the mind can be sick.Both Johann Weyer ( ) of Germany and Scot ( ) of England used scientific skepticism to refute the concept of demonic possession.\n6 Humanitarian Reforms (18th-19th century) In France, Philippe Pinel ( ) pioneered a compassionate medical model for the treatment of the mentally ill & established a hospital in Paris.In England, William Tuke ( ) introduced trained nurses for the mentally ill & helped to change public attitudes regarding their treatment.In US, Benjamin rush ( ) founder of American Psychiatry, encouraged humane treatment of the mentally ill & hospitals.\n7 Scientific Advances of the 20th Century Development in technology such as MRI and PET scans have added to our knowledge of the biological bases of psychological disorder.MRI PETDevelopment in pscycho-pharmacology have provided effective treatment for many psychological disorder.\n8 Paraphrase on your own… Article – Nearly 500, mentally ill men and women are serving time in U.S. jails and prisons.Paraphrase on your own…\n10 Abnormal Behavior Definition The behavior that is disturbing (socially unacceptable), distressing, maladaptive (or self-defeating), and often the result of distorted thoughts (cognitions).\n11 Videos – Set up your notes Definitions of Disorders-What does it mean?Rosenhan’s Experiment-What did it entail?Evolution of the DSM –What is it?5 AXES – Write examples for each1. Clinical Disorders2. Intellectual Disabilities & Personality Disorders3. Medical conditions and physical disorders4. Social & Environmental Factors5. The Global Assessment of Functioning\n15 Medical Perspective Explanation: Focus on biological and physiological factors as causes of abnormal behavior .Treated as a disease, or mental illness, and is diagnosed through symptoms and cured through treatment.Treatment: Hospitalization and drugs are often preferred methods of treatment rather than psychological investigation.Example: Schizophrenia needs medication to quiet voices, hallucinations and level dopamine.\n16 Psychodynamic Perspective Explanation: Evolved from Freudian psychoanalytic theory, which contends that psychological disorders are the consequence of anxiety produced by unresolved, unconscious conflicts(childhood).Treatment: focuses on identification and resolution of the conflicts.Example: Child neglected, no love will grow up to not love him/herself or others\n17 Behavioral/Learning Perspective Explanation: Results from faulty or ineffective learning and conditioning.Treatments are designed to reshape disordered behavior and, using traditional learning procedures, to teach new, more appropriate, and more adaptive responses.For example, a behavioral analysis of a case of child abuse might suggest that a father abuses his children because he learned the abusive behavior from his father and must now learn more appropriate parenting tactics\n18 Cognitive Perspective Explanation: People engage in abnormal behavior because of particular thoughts and behaviors that are often based upon their false assumptions. This is how the information is being decoded and retrieved (interpreted or memory issues).Treatments are oriented toward helping the maladjusted individual develop new thought processes and new values.Therapy is a process of unlearning maladaptive habits and replacing them with more useful ones.Example: Anger issues from low road to high road\n19 Social-Cultural Perspective Explain: Abnormal behavior is learned within a social context ranging from the family, to the community, to the culture.Treatment: Introducing and teaching the individual about in abnormal behavior within the culture by comparing and contrasting.Example: Anorexia nervosa and bulimia are psychological disorders found mostly in Western cultures, which value the thin female body\n20 Biological Perspective Views abnormal behavior as arising from a physical cause, such as genetic inheritance, biochemical abnormalities or imbalances, structural abnormalities within the brain, and/or infectionsAgrees that physical causes are of central importance but also recognizes the influence of biological, psychological, and social factors in the study, identification, and treatment of psychological disorders\n21 Bio-Psych-Social Perspective States Psychologists contend that ALL behavior, whether called normal or disordered arises from the interaction of nature and nurture.The bio-psycho-social perspective is a contemporary perspective which assumes that biological, sociocultural, and psychological factors combine and interact to produce psychological disorders.\n23 Abnormal Behavior Disorders – pairs of 3/computer lab Wednesday-Turn in outline/present Friday to peersWhat is the disorder?Explain the disorder.What causes it? (age)SymptomsTreatmentAn example of a case with someone having the disorderCommon or not?\n24 Mood Disorders-Bipolar PET scans show that brain energy consumption rises and falls with emotional swingsDepressed stateManic state\n25 Anxiety DisordersPET Scan of brain of person with Obsessive/ Compulsive disorderHigh metabolic activity (red) in frontal lobe areas involved with directing attention\n26 Psychological Disorders- Etiology DSM-IVAmerican Psychiatric Association’s Diagnostic and Statistical Manual of Mental Disorders (Fourth Edition)a widely used system for classifying psychological disordersHand out\n27 Take out disorder sheet add Borderline Personality disorder\n29 Schizophrenia Schizophrenia literal translation “split mind” a group of severe disorders characterized by:disorganized and delusional thinkingdisturbed perceptionsinappropriate emotions and actions\n30 Schizophrenia Delusions false beliefs, often of torture or greatness, that may accompany psychotic disordersHallucinationsfalse sensory experiences such as seeing something without any external visual stimulus\n31 A few more points to consider… for the Test next class periodA few more points to consider… for the\n32 Schizophrenia Subtypes of Schizophrenia Paranoid: Preoccupation with delusions or hallucinationsDisorganized: Disorganized speech or behavior, or flat or inappropriateemotionCatatonic: Immobility (or excessive, purposeless movement),extreme negativism, and/or parrotlike repeating ofanother’s speech or movementsUndifferentiated Schizophrenia symptoms without fitting one of theor residual: above types\n33 Schizophrenia 40 30 Lifetime risk of developing schizophrenia 20 for relatives ofa schizophrenic40302010GeneralpopulationSiblingsChildrenFraternaltwinof twovictimsIdentical\n34 Psychological Disorders- Etiology Neurotic disorder (term seldom used now)usually distressing but that allows one to think rationally and function sociallyFreud saw the neurotic disorders as ways of dealing with anxietyPsychotic disorderperson loses contact with realityexperiences irrational ideas and distorted perceptions\n35 Anxiety Disorders Anxiety Disorders Generalized Anxiety Disorder distressing, persistent anxiety or maladaptive behaviors that reduce anxietyGeneralized Anxiety Disorderperson is tense, apprehensive, and in a state of autonomic nervous system arousalPhobiapersistent, irrational fear of a specific object or situation\n36 Anxiety Disorders Common and uncommon fears Percentage of people Afraid of itBothers slightlyNot at all afraid of itBeingclosed in,in asmallplacealoneIn a houseat nightPercentageof peoplesurveyed100908070605040302010Snakesin high,exposedplacesMiceFlyingon anairplaneSpidersandinsectsThunderlightningDogsDrivinga carIn acrowdCats\n37 Anxiety Disorders Obsessive-Compulsive Disorder Panic Disorder characterized by unwanted repetitive thoughts (obsessions) and/or actions (compulsions)Panic Disordermarked by a minutes-long episode of intense dread in which a person experiences terror and accompanying chest pain, choking, or other frightening sensation\n38 Anxiety Disorders Common Obsessions and Compulsions Among People With Obsessive-Compulsive DisorderThought or BehaviorPercentage*Reporting SymptomObsessions (repetitive thoughts)Concern with dirt, germs, or toxinsSomething terrible happening (fire, death, illness)Symmetry order, or exactnessExcessive hand washing, bathing, tooth brushing,or groomingCompulsions (repetitive behaviors)Repeating rituals (in/out of a door,up/down from a chair)Checking doors, locks, appliances,car brake, homework\n39 Mood Disorders Mood Disorders Major Depressive Disorder characterized by emotional extremesMajor Depressive Disordera mood disorder in which a person, for no apparent reason, experiences two or more weeks of depressed moods, feelings of worthlessness, and diminished interest or pleasure in most activities\n40 Mood Disorders Manic Episode Bipolar Disorder a mood disorder marked by a hyperactive, wildly optimistic stateBipolar Disordera mood disorder in which the person alternates between the hopelessness and lethargy of depression and the overexcited state of maniaformerly called manic-depressive disorder\n41 Mood Disorders-Depression Percentageof populationaged 18-84experiencingmajordepressionat somepoint In life2015105USA Edmonton Puerto Paris West Florence Beirut Taiwan Korea NewRico Germany ZealandAround the worldwomen are moresusceptible to\n42 Mood Disorders-Depression Age in Years10%8642PercentagedepressedFemalesMales\n43 Mood Disorders- Suicide Suicides per100,000 people70605040302010MalesFemalesThe higher suicide rateamong men greatlyincreases in lateadulthood\n44 Mood Disorders-Suicide Increasing rates of teen suicideYear12%108642Suicide rate,ages 15 to 19(per 100,000)\n45 Mood Disorders-Depression Altering any one component of the chemistry-cognition-mood circuit can alter the othersBrainchemistryCognitionMood\n46 Mood Disorders-Depression Negative Positivebehaviors behaviorsSelf-ratings35%30252015Percentage ofobservationsA happy or depressed mood strongly influences people’s ratings of their own behavior\n47 Mood Disorders-Depression The vicious cycle of depression can be broken at any point1Stressfulexperiences4Cognitive andbehavioral changes2Negativeexplanatory style3Depressedmood\n48 Dissociative Disorders conscious awareness becomes separated (dissociated) from previous memories, thoughts, and feelingsDissociative Identity Disorderrare dissociative disorder in which a person exhibits two or more distinct and alternating personalitiesformerly called multiple personality disorder\n49 Personality Disorders disorders characterized by inflexible and enduring behavior patterns that impair social functioningusually without anxiety, depression, or delusions\n50 Personality Disorders Antisocial Personality Disorderdisorder in which the person (usually man) exhibits a lack of conscience for wrongdoing, even toward friends and family membersmay be aggressive and ruthless or a clever con artist\n51 Personality Disorders PET scans illustrate reduced activation in a murderer’s frontal cortexNormalMurderer\n53 Rates of Psychological Disorders Percentage of Americans Who Have Ever Experienced Psychological DisordersDisorder White Black Hispanic Men Women TotalsEthnicity GenderAlcohol abuseor dependence % % % % % %Generalized anxietyPhobiaObsessive-compulsivedisorderMood disorderSchizophrenicdisorderAntisocial personalitydisorder', 'Distress = the experience of emotional or physical pain. Impairment involves reduction in ability to function at an optimal or even average level. Risk = danger or threat to well-being of a person. Socially and culturally unacceptable behavior.\nThroughout the book, you will see that all three explanatory domains (biological, psychological, sociocultural) have relevance to the understanding and treatment of psychological disorders.\nTrauma which took place years ago can continue to affect a person’s thoughts, behavior, and even dreams. Emotional disturbances can arise from distorted perceptions and faulty ways of thinking.\nThe term sociocultural refers to the various circles of social influence in the lives of people.\nRelated to the biopsychosocial approach is the diathesis-stress model.\nTrephining: The drilling of a hole in the skull, presumably as a way of treating psychological disorders during prehistoric times, although it also occurred all over the world through the 18 th Century. Certain African tribes still use it to relieve head wounds. The procedures involved in exorcism seem more like torture to our contemporary eyes. The “possessed” person might be starved, whipped, beaten, and treated in other extreme ways with the intention of driving evil spirits out.\nHippocrates , whom many call the founder of Western medicine, was concerned with physical and psychological diseases. He believed 4 essential bodily fluids called “humors” influenced physical and mental health, and that an excess of any could account for changes in personality and behavior. Treatment by Hippocrates consisted of ridding the body of the excess fluid through such methods as bleeding , purging (forced excretion), administering emetics (nausea-producing substances), and establishing a healthier balance through proper nutrition.\nAesclepiades rebelled against the idea that an imbalance of bodily substances caused psychological disorders. He argued they were caused by emotional disturbances. Claudius Galen revolutionized previous thinking about psychological and physical disorders. Rather than rely on philosophical speculation, he studied anatomy as the first “medical researcher.” Unfortunately, he essentially maintained Hippocrates’ beliefs that imbalances of bodily substances caused abnormality.\nDuring the Middle Ages, there was a resurgence of primitive beliefs regarding spiritual possession. The dominance of religious thinking had negative and positive effects. Negative: Treating the mentally ill as sinners and witches had harmful effects. Positive: Ideas about Christian charity led to establishment of poorhouses to shelter the indigent. The poorhouses became known as asylums . bedlam : Word derived from the name of the Hospital of St. Mary of Bethlehem in London; became synonymous with the chaotic and inhumane housing of psychologically disturbed people.\nVincenzo Chiarugi (1759-1820) led the reform movement for humane treatment of the mentally ill. The physician Philippe Pinel (1745-1826) instituted reforms in La Bicêtre, a hospital in Paris. A hospital worker, Jean-Baptiste Pussin , had begun reform at La Bicêtre and influenced Pinel. After Pinel left La Bicêtre, Pussin made the bold gesture of freeing patients from their chains, an act for which Pinel is mistakenly given credit.\nIn England, 1792, William Tuke established the York Retreat , an institution based on the religious humanitarian principles of the Quakers. Their methods became known as moral treatment . Benjamin Rush (1745-1813), known as the founder of American psychiatry, was appalled by asylum conditions, although he himself advocated methods we now regard as barbaric, even sadistic. Boston schoolteacher Dorothea Dix spent 40 years campaigning for the proper treatment of psychologically disturbed people and was a very effective champion of this cause.\nRush advocated what we now regard as barbaric psychiatric interventions but were accepted conventions at the time: bloodletting purging the “tranquilizer” chair intended to reduce blood flow to the patient’s head and limbs submerging in cold shower baths frightening patients with threats of death the “well-cure” in which a patient was placed at the bottom of a well as water was slowly poured into it\nMedical model: The view that abnormal behaviors result from physical problems and should be treated medically. Mesmerism: Derived from the name Mesmer; a process of bringing about a state of heightened suggestibility . In the picture above, Jean-Martin Charcot demonstrates a hypnotic technique during a lecture. Hypnotism: The process of inducing a trance state. Psychoanalytic model: An approach that seeks explanations of abnormal behavior in the workings of unconscious psychological processes. Psychoanalysis: A theory and system of practice that relies heavily on the concepts of the unconscious mind, inhibited sexual impulses, early development, and the use of the ""free association"" technique and dream analysis. Psychotherapy: The treatment of abnormal behavior through psychological techniques.\nIn the 1950s, scientists introduced medications that controlled some of the debilitating symptoms of severe psychological disturbance. In the 1970s, the deinstitutionalization movement reacted to deplorable conditions in mental hospitals by promoting the release of psychiatric clients into community treatment sites. Deinstitutionalization created another set of problems: Planning was inadequate, funds were insufficient, and patients were often shuttled back and forth between hospitals, halfway houses, and shabby boarding homes, never having a sense of stability or respect. Halfway house: A community treatment facility designed for deinstitutionalized clients leaving a hospital who are not yet ready for independent living. Managed health care has become the standard by which third-party payers, such as insurance companies, oversee reimbursement for health services.\nThe scientific method involves applying an objective set of methods for observing behavior, making an hypothesis about the causes of behavior, setting up proper conditions for studying the hypothesis, and drawing conclusions about its validity. Observation process: The stage of research in which the researcher watches and records the behavior of interest. Hypothesis formation process: The stage of research in which the researcher generates ideas about a cause-effect relationship between the behaviors under study.\nIn the experimental method , the researcher adjusts the level of the independent variable and observes its effects on the dependent variable. The independent variable is the experimental variable that the researcher manipulates. An experimental group receives the “treatment” thought to influence the behavior under study, and the control group does not. (A special kind of control group in a placebo condition would receive an inert substance or treatment similar in all other ways except for its lack of the independent variable’s “treatment.”) The dependent variable is measured because it is believed to depend on the manipulated changes in the independent variable. The quasi-experimental method is a variant of this procedure and is used to compare groups that differ on a predetermined characteristic.\nThe correlational method studies associations, or co-relations, between variables.\nIn the case study method , one individual is studied intensively, and a detailed and careful analysis of that individual is conducted. In the single-subject design , one person at a time is studied in both the experimental and control conditions, as treatment is applied and removed in alternating phases. Studies of genetic influence may examine concordance rates, biological markers, or genetic mapping. Concordance rate: Agreement ratios between people diagnosed as having a particular disorder and their relatives. Biological markers: Measurable characteristics or traits whose patterns parallel the inheritance of a disorder or other characteristic. Genetic mapping: The attempt by biological researchers to identify the structure of a gene and the characteristics it controls.\nPsychological disorders affect not only the people who suffer from them but also the family, community, and society.\nIndividuals with psychological disorders suffer great stigma which adds to their emotional problems.\nFamily members are affected by the distress of their loved ones and also share a sense of stigma. The degree of impact depends the nature of the problem and the dynamics of the family. Families have also banded together for support and mutual education, forming organizations like the National Alliance for the Mentally Ill (NAMI) .\nIn some programs and communities, discharged individuals are adequately cared for; however, in many areas, particularly large cities, there are a great many formerly institutionalized people who go without homes, food, or health attention. On a broader level, the social and financial costs of mental health problems are inestimable , but mental health professionals and public health experts agree psychological problems exert a tremendous toll on society: Families are often torn apart and communities are divided.']"	['<urn:uuid:e517e49c-059c-4488-b0c2-bd1430109b44>', '<urn:uuid:81d13d55-e910-4b58-8c1b-ed9da43f6a18>']	open-ended	direct	verbose-and-natural	similar-to-document	comparison	expert	2025-05-12T20:11:36.665104	31	67	2830
77	compare costs profits traditional farming vs vertical farming vs google ads ranking system	In Google's ad system, profitability is determined by both bid amounts and performance metrics, where even lower bids can win if they have better relevance and click-through rates. Similarly, in farming systems, the cost-effectiveness comparison between vertical and traditional farming shows interesting parallels. Vertical farming can potentially reduce transport costs and increase yields per area, but requires high initial investment and energy costs. Traditional farming needs more land and water resources but has lower setup costs. Like Google's Quality Score system that balances cost per click with performance, farming methods must balance initial investments against long-term yields and operational costs. Both systems demonstrate that the highest cost option isn't necessarily the most profitable - success depends on efficiency and performance metrics.	['What is Adwords Quality Score and What It means for You\nWhat is a Quality Score?\nThe general concept of a Quality Score was first introduced by Google. It is an aggregated estimate of PPC ad, keyword, and landing page quality based on past performance. Scores range from 1-10 (10 being the highest) and is influenced by these factors:\n- Click-through rate (keywords and display URL)\n- Ad relevance and performance (ad copy relevance, ad performance on specific sites and sites in the Display Network, and ad performance on targeted devices)\n- Keyword relevance (as an ad group in context with keyword search matches)\n- Landing page experience (relevant and original content, navigability, load time, etc.)\n- AdWords account historical performance (including geographic performance)\nWhy Do Ad Networks Need a Quality Score?\nGoogle and other major search engines cite user experience as the primary reason for their implementation of a Quality Score rating. The quality of ads, after all, affect how users interact with them and of course, the advertising revenues.\nTo understand how this strategy works, a better understanding of the different monetization models used by ad networks is needed. There are different ways in which advertisers bid for ads. In the nascent years of online advertising, the most popular is the CPM (cost per mille or thousand) model. Until now, ‘impression’ is still used as the primary unit of ad inventory even though the CPC (cost per click) is now the model of choice by many ad networks.\nThe shift from CPM to CPC has obvious benefits for advertisers but the implication for ad networks are much more significant. In competitive markets, there is a convergence between an ad inventory’s pre-defined price and the price that advertisers bid. Because of this, ad networks can predict the profits they can gain through the expected sale value of each ad inventory.\nIn the CPM and CPC models, the same ad auction process occurs, and the two models can even compete in the same ad auction. The difference is that viewable CPM (vCPM) campaigns only target the Display Network. Advertisers pay for every 1,000 ad appearance and viewable instances. With the CPC model, advertisers pay every time an ad is clicked. Google can set minimum bids – different amounts for different advertisers for the same keywords. Advertisers then set the maximum cost-per-click bid or the highest amount they are willing to pay for a single click.\nThere’s no assurance though that the highest bidder would win the auction. Any bidder can win the auction at a lower price if his keywords and ads are more relevant than the others. All he needs to do to keep his ad’s position is to have a slightly higher bid than the one behind him. But isn’t this counter-intuitive? Nope. It actually makes more sense.\nAd Rank determines if an ad can compete in an auction. Ad ranking signals that are taken into account are CTR (click-through rate), ad relevance, and landing page experience. But the main determinant for ROI and the ad quality is the CTR. It can determine how much Google can earn for a thousand impressions of a PPC (pay-per-click) ad. Let’s have an example.\nAdvertiser A bids for $3 for each click, advertiser B bids for $2, and lastly, advertiser C bids for only $1. The expected CTR for advertiser A’s ad is 1%, for advertiser B it’s 2%, and for advertiser C it’s 5%. Calculating this we get 10 expected CTR for advertiser A, 20 for advertiser B, and 50 for advertiser C. If we multiply this to the bid, Google would earn $30 from advertiser A; $40 for advertiser B; and for advertiser C it would be $50. This example shows that even if advertiser A and B had higher bids, their low CTR downplays the expected ROI. Google would select the ads that have the highest chance of performing better or risk running low performing ads – that’s where Quality score comes in.\nAd Rank is keyword-based and is triggered in real-time whereas Quality Score is an aggregate of past performances. Google reiterates that Quality Score isn’t used to determine auction-time Ad Rank but it can actually improve ad position. It quantifies the relevance of an ad based on its performance – performance that can’t be quantified unless the ad had been clicked by the user. This makes CTR as one of the most important signals for ad relevance. The more relevant the ad is, the more likely it is to be clicked.\nOn an economic standpoint, Quality Score aids ad networks in managing the risk of poor performance and optimizing profit from every advertiser. Without a Quality Score, advertisers can win ad auctions based on bids alone. Poor performance could mean a lesser expense since the advertiser would only pay for whatever few clicks his ad generates, yet the ad network still delivers one thousand impressions. A Quality Score is an assurance that no ad slot is wasted. This is since low performing advertisers will have to pay a higher CPC to account for the impressions delivered. If the ad’s low performance continues, the ad campaign would stop running until the Quality Score improves and the ad’s expected CPC converges with the pre-defined price or minimum bid.\nWhat is the Implication of a Quality Score for Advertisers?\nWe briefly discussed the implications of a low Quality Score and its implications on the ad’s CPC. But it is also evident from an advertiser’s – your perspective that ad relevance isn’t only about user experience, or even about the ad network’s profits, but also about the effective monetization of ad impressions. Aside from that, Quality Score has a direct (or indirect) correlation with the following:\n- Lower cost per click.\n- Lower cost per conversion. Cost per conversion is usually higher than the cost per click since not every click results into conversion.\n- Higher ad ranks.\n- Impression share.\nThe following approaches can help optimize Quality Score:\n- High CTRs should not be your main goal. The increase of CTRs to optimize the Quality Score doesn’t necessarily maximize ROI. It is erroneous to believe that more clicks equal more sales. It can happen, but not on the same ad budget. You should pay attention to key performance metrics like cost per sale and/or cost per lead.\n- Craft ads according to campaign goals. Aside from paying attention to the key performance metrics, you should know your ‘ideal prospect’, prospective customers that you want to optimize your ads for. This is to increase the response rate of these prospective customers.\n- Organize keywords in tight themes for different ads. Also, focus on testing ads for your top keywords until you see an improvement in the Quality Score. You may need to pause low-performing keywords so that your CTR history would be good.\n- Adplexity Coupon Code $80 Off and Walkthrough - September 23, 2019\n- SiteGround vs GoDaddy: WordPress Hosting Comparison - September 13, 2019\n- SiteGround vs Inmotion: WordPress Hosting Benchmark - August 31, 2019', 'Agricultural systems around the world need to adapt to the rapidly changing environmental, demographic, and socioeconomic landscapes, and new alternative practices, such as vertical agriculture, may offer new opportunities to accelerate such adaptation.\nNext Gen Farming Without Soil and 90% Less Water | GRATEFUL\nWhat is vertical farming and why is it important\nModern agricultural systems encompass an estimated 1.5 billion hectares of the world’s surface area. With a growing population and resource needs, the availability of arable land is shrinking rapidly.\nSince the agricultural revolution, conventional agriculture has focused on practices requiring considerable quantities of space, water, fertilizer, and pesticides. The past 50 years have seen an accelerating rate of increase in these requirements as modern food production aims to increase productivity in the hopes of addressing growing food insecurity.\nLooking into the future, yield production is forecasted to decrease due to widespread environmental and socioeconomic changes that will generate unpredictable consequences on food systems.\nIn response, many strategies have been developed as alternatives to conventional agricultural practices. These strategies have focused on key principles and their combination to be effective: require less space, less water, and increase yield per unit of area. Moreover, due to the negative effects of agrichemicals, modern practices have also aimed to use significantly less to avoid potentially adverse effects for humans and animals.\nOne such alternative is the development of vertical agriculture, also referred to as vertical farming. As the name implies, vertical agriculture relies on expanding production vertically and not horizontally. Vertical agriculture is a multilayer indoor plant production system that allows for precise control of growth factors, such as light, temperature, humidity, carbon dioxide concentration, water, and nutrients.\nThis allows for the growing and production of crops year-round, completely independent of solar light and other external conditions. Indeed, vertical agriculture makes use of key concepts within ecology and physiology to optimize growing and fertilization within controlled conditions. For instance, elements of photobiology, thermomorphogenesis, hydroponics, and genetic breeding, are all used commonly across systems of vertical agriculture.\nBenefits, challenges, and disadvantages moving from horizontal to vertical farming\nAs a result of tight control over crop breeding, growing, and harvesting, vertical farming provides several benefits relative to conventional methods of ‘horizontal’ food production. This was the topic of a literature review by Kalantari et al. published in 2016 in the Journal of Landscape Ecology.\nFrom a systems perspective, the enclosed design prevents pests and diseases from entering by the adoption of a high level of hygiene, continuous monitoring, and non-chemical disinfection, providing security from crops. Moreover, recent technology has also allowed for automated control over environmental conditions by using sensors and imaging techniques in combination with crop simulation models and artificial intelligence, limiting the need for physical labor.\nVertical farming also allows for flexible organization, with designs ranging from large vertical walls covered with crops to large hangars or re-used shipping containers that can be transported. Consequently, vertical agricultural systems, can comprise many varying sizes and be located within many different areas from the middle of highly urbanized cities to more suburban or rural areas.\nMoreover, the verticality element of this system also provides nutrient and water flow, helping to reuse costly resources. The reduction in space also means there is a significant increase in yield per area, holding extensive potential for a future world of urbanization.\nFrom an economic perspective, vertical farming also provides for more jobs in localized areas and is community-focused by addressing the needs of immediate areas, which in turn can provide food at a lower price. Finally, the optionality of location for vertical systems also allows producers to reduce transport costs, as consumers may access them within urban areas, or transport can be minimized to nearby areas.\nHowever, despite the design, environmental, and economic advantages, vertical farming also incorporates several issues that remain a challenge to its broader implementation as a system.\nVertical farming has a high energy requirement and needs extensive investment costs to implement and develop successfully. Moreover, indoor issues relating to excessive UV, heat, and ozone-induced plant damage may have unpredictable repercussions for plant growth.\nAdditionally, vertical systems are difficult to adapt to a larger scale. They are costly to build and maintain and have yet to demonstrate the ability to provide food for larger areas than community-scale populations. This would make it difficult to implement in areas at higher risk of food insecurity, such as developing agricultural nations.\nThe lack of empirical research on a broader scale has meant that vertical farming has yet to develop past the concept stage on community levels, as persistent issues make it difficult to break through to a larger scale.\nImage Credit: YEINISM/Shutterstock.com\nGrowing skywards - the implications of vertical farming in a rapidly populating and changing world\nAmong the development of alternative agricultural practices, vertical agriculture provides a promising solution for many of the challenges facing current agricultural policies. However, for vertical systems to be integrated on a larger scale requires further technological progress and economic investment.\nNevertheless, gradually implementing more verticality, or combining it with other practices aiming for more sustainable practices may be promising. For instance, the combination of verticality with other practices such as intercropping may be particularly beneficial for developing more sustainable food systems.\nIncorporating technological progress into vertical systems also holds promise, with automated sensors and machinery able to operate near-independently. Progress in gene editing and plant genome modifications will also allow for faster, bigger, and healthier crops, allowing vertical agriculture to produce more over time.\nThroughout agricultural history, farming systems have typically spread over large spans of land, yet the reduction in arable land, as well as the increase in demand to house growing populations, means that such strategies need to be reconsidered, and vertical agriculture may play a role looking into the future.\nContinue Reading: Benefits of Vertical Agriculture and Hydroponics\n- Beacham, A. M., Vickers, L. H., & Monaghan, J. M. (2019). Vertical farming: a summary of approaches to growing skywards. The Journal of Horticultural Science and Biotechnology, 94(3), 277–283. doi: 10.1080/14620316.2019.1574214\n- Chaudhry A. R. and Mishra V. P.,(2019) A Comparative Analysis of Vertical Agriculture Systems in Residential Apartments, Advances in Science and Engineering Technology International Conferences (ASET), 2019, pp. 1-5, doi: 10.1109/ICASET.2019.8714358\n- Sarkar, A., & Majumder, M. (2015). Opportunities and Challenges in Sustainability of Vertical Eco-Farming: A Review. Journal of Advanced Agricultural Technologies, 2(2). doi: 10.12720/joaat.2.2.98-105\n- SharathKumar, M., Heuvelink, E., & Marcelis, L. F. (2020). Vertical Farming: Moving from Genetic to Environmental Modification. Trends in Plant Science, 25(8), 724–727. doi: 10.1016/j.tplants.2020.05.012']	['<urn:uuid:646f0ebf-2df2-44f1-8352-1e86d2d4ff28>', '<urn:uuid:3566e03d-b8ac-4d36-b13c-9d56f4d7ba0e>']	open-ended	with-premise	long-search-query	similar-to-document	multi-aspect	novice	2025-05-12T20:11:36.665104	13	121	2263
78	I've been diagnosed with cancer and I'm worried about my options - can you explain in simple terms how radiation treatment works to fight cancer cells?	Radiation therapy works by using ionising radiation to induce breaks in the DNA of tumor cells, which causes these cells to die. It is a local treatment that aims to destroy targeted cancer cells while preserving the surrounding healthy tissue.	"[""Find a doctor\nbook an appointment\nABOUT THE RADIATION THERAPY UNIT\nRadiation therapy is a local treatment technique that uses radiation. It is used primarily to fight cancer. The principle involves the destruction of targeted cancer cells while preserving the surrounding healthy tissue.\nOver half of all cancers can be treated with radiation therapy. It is generally combined with other treatments such as surgery and/or chemotherapy. It often helps to avoid major surgery and preserve the damaged organ.\nRadiation therapy can be administered to the tissues in two ways:\n- Through external beam radiation therapy, the most widely used technique, which consists in aiming photon or electron beams into the tumour.\n- Through brachytherapy, a technique by which a radioactive source (or several) is brought into contact with or implanted temporarily or permanently in the area where the tumour is located. This helps to achieve very precise aim and optimal preservation of the surrounding healthy tissue. Our unit has established an international reputation in the area of prostate brachytherapy.\nThe biological effect of ionising radiation is to induce breaks in the DNA of the tumour cells, causing them to die.\nThis type of treatment requires very detailed planning, during which different specialists interact with each other (radiation oncologist, medical physicist and radiation therapist). The volumes to be irradiated are identified precisely by means of a dedicated CT scan, together with, if needed, a PET scan or an MRI, performed each time in the position in which treatment is administered. Because of these different scans, a personalised treatment plan is established that takes into account the individual specificities and the healthy organs surrounding the volumes treated.\nTo guarantee the efficacy and quality of the treatments administered, a chain of custody and of events is scrupulously followed.\nThe projected dosimetry of each patient is carefully established by a dosimetrist or a physicist, verified by a second medical physicist and validated by the radiation oncologists. All the parameters of each treatment plan are verified point by point. Systematic quality controls are conducted prior to the start of and during each treatment.\nEvery day, the machines are checked before starting them up and the patient's position on the table is carefully checked.\nDuring treatment, the patient will be monitored by his/her doctor and the nurses.\nAt Europe Hospitals, the Radiation Therapy Unit is equipped with two linear accelerators and a dedicated PET-CT simulator, all three being latest generation, allowing for the delivery of treatment to the patients using the most cutting-edge techniques.\nThe Unit also offers low dose-rate (LDR) or high dose-rate (HDR) brachytherapy for, among other conditions, the treatment of some types of prostate, gynaecological and skin cancer.\nSince 2014, the Unit is ISO 9001 certified. This guarantees our patients continuous oversight and improvement of the all the processes implemented, of their satisfaction and the quality of care delivered to them during their entire treatment period. The Unit is composed of an interdisciplinary team comprising radiation oncologists, medical physicists, nurses, radiation therapists, and secretaries.\nThe pool of equipment allows for the delivery of different types of radiation therapy treatments:\n- Intensity modulated radiation therapy\n- Rotational or arc-modulated radiation therapy\n- Stereotactic radiation therapy (small volume focal radiation with several beams)""]"	['<urn:uuid:4b8ba8e8-003b-48ed-acae-b56ab3f35d4d>']	factoid	with-premise	verbose-and-natural	distant-from-document	single-doc	novice	2025-05-12T20:11:36.665104	26	40	538
79	shark cradle purpose dangers fishing	The shark cradle is a 10-foot aluminum and netting device used to safely lift large sharks onto research vessels for measurement and tagging. While this helps protect sharks during research, they face severe threats from commercial fishing, where up to 73 million sharks are killed annually for their fins in largely unregulated fisheries, and millions more die as accidental bycatch in various fishing operations.	['NOAA Teacher at Sea\nAboard NOAA Ship Oregon II\nJuly 23 – August 10, 2018\nMission: Long Line Shark/ Red Snapper survey Leg 1\nGeographic Area: 31 41 010 N, 80 06 062 W, 30 nautical miles NE of Savannah, North Carolina\nDate: August 8, 2018\nWeather Data from Bridge:\nWind speed 11 knots,\nAir Temp: 30c,\nVisibility 10 nautical miles,\nWave height 3 ft.\nScience and Technology Log\nNormally you wouldn’t hear the words shark and cradle in the same sentence, but in our case, the cradle is one of the most important pieces of equipment we use each day. Our mission on the Oregon II is to survey sharks to provide data for further study by NOAA scientists. We use the long line fishing method where 100 hooks are put out on a mile long line for about an hour, and then slowly hauled up by a large mechanical reel. If a shark is generally three feet and weighs 30lbs or less, it is handled by hand to carefully unhook, measure and throw back. If the shark is much larger and cannot be managed safely by hand, it is then held on the line by the ships rail until it can be lifted on deck by the cradle to be quickly measured, tagged, and put back into the ocean.\nThe shark cradle is 10 ft. long, with a bed width of roughly 4 feet. It is made from thick aluminum tubing and strong synthetic netting to provide the bed for the shark to lie on. It is lifted from the ship’s deck by a large crane and lowered over the ships rail into the ocean. The shark is still on the line and is guided by a skilled fisherman into the cradle. The crane operator slowly lifts the cradle out of the water, up to the rail, so work can begin.\nA team of 3 highly skilled fishermen quickly begin to safely secure the shark to protect it, and the team of scientists collecting data. They secure the shark at 3 points, the head, body and tail. Then the scientists come in to take 3 measurements of the shark. The precaudal measurement is from the tip of nose to the start of the tail. The fork measurement is from the tip of the nose to the fork of the tail (the place where the top and bottom of the tail meet). Finally there is a total length taken from the tip of the nose to the furthest tip of the tail.\nWhen all measurements are complete, a tag is then placed at the base of the first dorsal (top) fin. First a small incision is made, and then the tagger pushes the tag just below the skin. The tag contains a tracking number and total length to be taken by the person who finds the shark next, and a phone number to call NOAA, so the data can recorded and compared to the previous time data is recorded. The yellow swivel tags, used for smaller sharks, are identical to ones used in sheep ears in the farming industry, and are placed on the front of the dorsal fin. The measurements and tag number are collected on the data sheet for each station. The data is input to a computer and uploaded to the NOAA shark database so populations and numbers can be assessed at any time by NOAA and state Departments of Natural Resources.\nThe shark is then unhooked safely by a skilled fisherman while the other two are keeping the shark still to protect both the shark and the fishermen from injury. The cradle is then slowly lowered by crane back into the ocean where the shark can easily glide back into its environment unharmed. The cradle is then raised back on deck by the crane operator, and guided by the two fishermen. All crew on deck must wear hardhats during this operation as safety for all is one of NOAA’s top priorities. This process is usually completed within 2 minutes, or the time it took you to read this post. It can happen many times during a station, as there are 100 hooks on the one mile line.\nIt is amazing for me to see and participate in the long line fishing process. I find it similar to watching medical television shows like “ER” where you see a highly skilled team of individually talented members working together quickly and efficiently to perform an operation. It can be highly stressful if the shark is not cooperating, or the conditions aren’t ideal, but each member always keeps their cool under this intense work. It’s also amazing to see the wealth of knowledge each person has so when an issue arises, someone always knows the answer to the problem, or the right tool to use to fix the situation, as they’ve done it before.\nAnimals Seen Today: Sandbar shark, Tiger shark, Sharpnose Shark, Sea Robin, Toadfish, Flying Fish', 'Sharks consistently rank near the top of lists of American’s greatest fears. In reality, they have much more to fear from us than we do from them. Because of our actions, many species of sharks are on the verge of extinction. A recent International Union for the Conservation of Nature Shark Specialist Group report shows that fully 1/3 of open-ocean species of sharks are in danger of extinction in the next few decades. Many shark species have had population declines of over 90% in the last few decades.\nThe life history strategy of sharks is very different from that of other commercially exploited fishes, and this makes them more vulnerable to overexploitation. Several species of sharks don’t reproduce until they are older than ten years old, and some only have a few young every other year (or in some cases, every three years). It’s easy to see how this inability to rapidly replace themselves could become an existential problem when modern industrial fishing techniques are involved.\nWhile few sharks are targeted for their flesh, which is considered unpalatable except for a few species, most species are targeted for their fins. The fins, which have absolutely no meat, flavor, or nutritional value whatsoever, are made into an Asian delicacy called shark fin soup. They provide only texture to the spiced chicken broth. While it is impossible to know exactly how many sharks are killed in this global, largely unregulated fishery, the best scientific estimates we have say that the number is as high as 73 million each year.\nBeing targeted isn’t the only thing problem facing sharks-bycatch is another major threat to many shark species. Millions of sharks each year are killed by fishing gear simply because they are swimming near what fishermen are trying to catch- the ultimate example of being in the wrong place at the wrong time. Though few fisheries are blameless from a bycatch perspective, particular culprits include the shrimp trawling fishery, the tuna purse seine fishery, and the billfish longline fishery.\nSince sharks serve as apex predators in most marine ecosystems, their declines pose major troubles both to the environment and to the countless humans who depend on that environment for food and to make a living.\nThe problem is a major one, and we need to solve it.\nIt’s easy to become discouraged when considering the enormity of the problem, but we must not give up. We must focus on the goal, and work towards it!\nTo paraphrase an old joke about my fellow Jews, if you ask three conservationists what their goal is, you’ll likely get four answers. This post is about my goal, which isn’t necessarily the goal of the entire ‘save the sharks’ movement. I expect that many of my conservationist friends will disagree with parts of it, and I look forward to a lively discussion.\nSharks are being harvested (or killed accidentally as bycatch) at wildly unsustainable rates. This needs to stop, and sharks need some strong legal protections to ensure this.\nThe form that these legal protections will take is the subject of much debate. A (very) few countries like Palau make fishing for sharks in any form illegal in their territorial waters. Hawaii now bans the selling, purchasing, or possession of shark fins within state boundaries. U.S. fisheries management policy presently makes it illegal to kill some species of shark and sets size limits on some other species. However, most countries have no legal protection at all for sharks, and the number of species with worldwide protection (at least on paper, since these are difficult to enforce in the middle of the ocean) can be counted on one hand.\nThough this may shock some of my readers, I do not think that global shark conservation policy needs to be as extreme as Palau’s “you can’t kill any sharks ever” law. I do not object to the sustainable harvest of sharks for food. Sustainably harvesting animals that have so few young so late in life is extremely difficult, and most times that it has been tried, the fishery has collapsed within a few decades. That doesn’t mean that it is impossible and it doesn’t mean that we should ban all shark fishing. However, for me to be satisfied, the world of commercial fishing is going to need to undergo some drastic changes.\nThe goal for finning\nWhile I can accept sustainable fishing for some shark meat, I object to the shark fin soup fishery. In most parts of the world, this fishery is brutal, wasteful, and unsustainable. Sharks of any species and size have their fins cut off, and the rest of the animal is dumped overboard to bleed to death or drown- all to provide texture to a delicacy for the rich. The few countries that have shark finning regulations at all have different strategies to manage it.\nSome, like Canada, require that fishermen land the rest of the shark in addition to the fins (not attached to each other), and they enforce this by weighing total fins and total shark carcasses. This is silly, because different shark species can have a drastically different fin-to-body weight ratio. Other countries require that fishermen land sharks with the fins still attached, which is better and is starting to become the standard.\nI would feel differently about shark finning if it provided a staple food item for the world’s poor instead of a delicacy for the rich. I would feel differently about shark finning if the shark’s meat was used, instead of just cartilage for texture. I would feel differently if fishermen targeted only certain species of a certain size instead of every shark they find. As it stands, though, my goal for the shark finning fishery is its complete abolition.\nUpdated 7/8/12 To clarify, this personal opinion applies to fins that are provided to the market via the wasteful practice of finning. Fins that are provided to the marketplace via well-regulated comprehensive shark fisheries are a separate issue.\nThe goal for bycatch\nThe threat sharks face from bycatch is harder to regulate. In some cases, simple gear modifications can minimize the amount of sharks caught without greatly influencing the catch of target species. In other cases, simply placing gear in slightly different locations or depths can greatly reduce the number of sharks caught accidentally. Some of these changes have been made already, most have not been. Conservationists who fight for long-term large-scale goals should sometimes fight for easy fixes that will still make a lot of difference. My goal is for every single known and feasible bycatch reduction strategy to be implemented. This won’t eliminate bycatch, but it will reduce it significantly.\nSome fishing gear is so destructive (to sharks and many other ocean animals) that simple fixes just won’t help. In these cases, my goal is for that gear to be banned entirely. This is not unprecedented- the U.N. banned large drift nets almost 20 years ago because of the huge amount of bycatch they caused.\nThe goal for marine protected areas\nWhile I don’t think that we need a global ban on shark fishing modeled after Palau’s policy, some small-scale areas where shark fishing is banned would be very helpful. Research performed on a marine protected area in Belize has shown that many species of shark remain in a small area for much of their lives, showing that a small region where shark fishing is illegal can have an effect. However, some species, like the Great White shark, can swim thousands of miles in a year and wouldn’t stay in a small protected area very long. Although they won’t help all shark species, marine protected areas will protect many. My goal is a large worldwide network of marine protected areas that protect sharks and other marine animals.\nMy goal for fishermen\nThe global commercial fisheries industry is at a crossroads. There are too many fisherman chasing too few fish, and overfishing is rampant. However, contrary to the claims and insinuations of some conservationists, fishermen are not evil people trying to destroy the environment. They are hardworking people who are just trying to provide for their families, and they are correct when the point out that most policies that will protect marine life will harm them financially. However, if nothing is done, there will be no fish to catch and fishermen will be harmed financially anyway. My goal is for there to be fewer commercial fishing vessels and a much lower global catch, and I am open to suggestions on how to help the fishermen that this policy would negatively impact.\nMy goal is for an end to unsustainable shark fishing, a ban on the shark fin soup fishery, the implementation of bycatch reduction policies, a global network of marine protected areas, and some form of incentive or regulation to encourage fishermen to catch fewer fish overall. Some parts of this goal would be more effective for protecting sharks than others. Some are more achievable than others. I believe that all are worth fighting for.\nHow to get there\nIt’s easy to dream big and come up with impossible goals. It’s much harder to draw a realistic map showing how to get from where we are to where we want to be.\nMost of you reading this already don’t eat shark fin soup, and many of you don’t eat foods with high shark bycatch. That’s great, but while I have a pretty high opinion of myself, even I don’t believe that I reach enough people through my writing to make a difference in a major global issue. Not directly, at least.\nThe key to achieving the goals of the shark conservation movement (and the conservation movement in general) is education. Maybe I’m too much of an optimist, but I fervently believe that sharks aren’t in trouble because no one cares what is happening to them. I believe that sharks are in trouble because no one knows that sharks are important to a healthy ecosystem, and no one knows that sharks are in trouble.\nThe absolute best thing you can do is to learn about sharks and tell others. Tell your friends, tell your family, tell your classmates or tell your co-workers. Tell them that sharks matter, and tell them that sharks are in trouble. Tell them not to eat shark fin soup, and not to eat seafood with high shark bycatch. Tell them to support shark conservation legislation by calling their elected officials.\nWhat you should NOT do is support violent groups that claim to “fight for the sharks” through “direct action”. These groups are not only ineffective, but they are counterproductive to the cause of conservation. The conservation movement is a PR war, and we will win through facts and persuasive argument- NOT through trying to hurt people who disagree with us.\nAnother common (and flawed) solution is to not eat seafood at all because of environmental concerns. If all of the people who care about the oceans stop eating seafood, it’s impossible for conservation-minded folks to “vote with their wallets” and support more environmentally friendly methods of catching fish. I instead recommend eating Marine Stewardship Council certified sustainable seafood.\nGraduate students such as myself lack the resources to donate significant amounts of money to conservation NGO’s, but if any readers are looking for my opinion on what NGO’s to trust, I have a few. Oceana doesn’t focus exclusively on sharks, but I love almost all of what they do. WildAid also has a broad focus, but their anti-finning campaigns are wonderful (they recruited Yao Ming, who is a huge celebrity in China, to be their spokesman). The Save Our Seas Foundation does a lot of inspiring work with educating children about the importance of sharks and other sea life. The Shark Research Institute is a small but great organization that focuses on both conservation and science. Sonja Fordham’s Shark Advocates International is a new organization, but Sonja is legendary within the shark conservation community and I know she’ll accomplish amazing things with SAI.\nIf you are looking for a source for shark-themed gifts that help sharks, I have a few suggestions. Iemanya Oceanica’s “Adopt a Shark” program makes a good gift, and promotes shark research. The American Elasmobranch Society student store raises money for young shark scientists to do important research. My own Southern Fried Science store sells “Sharks Matter/No finning gear”, which raises money for the charities I’ve listed above.\nMany people claim that it’s too late to save the planet. I couldn’t disagree more.\nThe problem is a big one. The goals are difficult, but they are achievable in some form. Now that you know what to do, get to it.\nTo paraphrase a famous Donella Meadows quote, we have exactly enough time to save sharks and save our oceans… starting now.\nBaum, J. (2003). Collapse and Conservation of Shark Populations in the Northwest Atlantic Science, 299 (5605), 389-392 DOI: 10.1126/science.1079777\nBonfil, R. (2005). Transoceanic Migration, Spatial Dynamics, and Population Linkages of White Sharks Science, 310 (5745), 100-103 DOI: 10.1126/science.1114898\nClarke, S., McAllister, M., Milner-Gulland, E., Kirkwood, G., Michielsens, C., Agnew, D., Pikitch, E., Nakano, H., & Shivji, M. (2006). Global estimates of shark catches using trade records from commercial markets Ecology Letters, 9 (10), 1115-1126 DOI: 10.1111/j.1461-0248.2006.00968.x\nConrath, C., & Musick, J. (2007). The Sandbar Shark Summer Nursery within Bays and Lagoons of the Eastern Shore of Virginia Transactions of the American Fisheries Society, 136 (4), 999-1007 DOI: 10.1577/T06-107.1\nCortés, E. (2000). Life History Patterns and Correlations in Sharks Reviews in Fisheries Science, 8 (4), 299-344 DOI: 10.1080/10408340308951115\nDulvy, N., Baum, J., Clarke, S., Compagno, L., Cortés, E., Domingo, A., Fordham, S., Fowler, S., Francis, M., Gibson, C., Martínez, J., Musick, J., Soldo, A., Stevens, J., & Valenti, S. (2008). You can swim but you can’t hide: the global status and conservation of oceanic pelagic sharks and rays Aquatic Conservation: Marine and Freshwater Ecosystems, 18 (5), 459-482 DOI: 10.1002/aqc.975\nMusick, JA (2000). Management of Sharks and their relatives (Elasmobranchii) Fisheries\nPikitch, E., Chapman, D., Babcock, E., & Shivji, M. (2005). Habitat use and demographic population structure of elasmobranchs at a Caribbean atoll (Glovers Reef, Belize) Marine Ecology Progress Series, 302, 187-197 DOI: 10.3354/meps302187\nTopelko, K., & Dearden, P. (2005). The Shark Watching Industry and its Potential Contribution to Shark Conservation Journal of Ecotourism, 4 (2), 108-128 DOI: 10.1080/14724040409480343\nWalker, T. (1998). Can shark resources be harvested sustainably? A question revisited with a review of shark fisheries Marine and Freshwater Research, 49 (7) DOI: 10.1071/MF98017']	['<urn:uuid:ad609b40-f049-4848-85ae-fe485d84bd4e>', '<urn:uuid:4f23ef1a-bdcc-49bd-bfee-b87cf4c0b3d9>']	factoid	direct	short-search-query	distant-from-document	multi-aspect	novice	2025-05-12T20:11:36.665104	5	64	3248
80	How many different sets of teeth do humans get throughout their lifetime, and what's the difference in the number of teeth between baby teeth and adult teeth?	Humans get two full sets of teeth over their lifetime - they are known as diphyodont. As a baby, you have 20 teeth (milk teeth), and as an adult you should have 32 teeth. The baby teeth gradually fall out to make way for the adult set of teeth. People start losing their baby teeth and getting their adult set as early as 5 years old, and you should have the full set of adult teeth by your late teens.	['Table of Contents\nWhy do people only have 32 teeth?\nYou get two full sets of teeth over your lifetime. As a baby, you have 20 teeth, and as an adult you should have 32 teeth. Among the 32 teeth, each has its own function in the chewing and eating process. Take good care of your teeth and keep your gums healthy in order to avoid cavities and other overall health issues.\nIs it rare to have all 32 teeth?\nA very few people have more than 32 teeth but this is considered an anomaly. In general, probably over decades of evolutionary time, our jaws have gotten smaller so that there is not enough room for the full compliment of 32 teeth.\nWhy some people have many teeth?\nHyperdontia is a condition that causes too many teeth to grow in your mouth. These extra teeth are sometimes called supernumerary teeth. They can grow anywhere in the curved areas where teeth attach to your jaw. This area is known as the dental arches.\nIs it rare to have all 4 wisdom teeth?\nBut while many people have one to four wisdom teeth, some people don’t have any at all. Wisdom teeth are the third set of molars in the back of your mouth. Although it’s common to get wisdom teeth, they can cause issues. You can experience pain as the teeth break through the gums.\nIs it bad to only have 28 teeth?\nThe average mouth is made to hold only 28 teeth. It can be painful when 32 teeth try to fit in a mouth that holds only 28 teeth. These four other teeth are your Third Molars, also known as “wisdom teeth.”\nCan a wisdom teeth come in at 40?\nThey usually erupt between ages 17 and 25; however, in some individuals wisdom teeth have erupted even in 40s or 50s. This is the reason why these teeth are called wisdom teeth as they appear during the phase of life called the “age of wisdom.”\nAre extra teeth lucky?\nSamudrika science says that they are a sign of prosperity also. Meanwhile, crowded teeth with gaps indicate hurdles for success. People with such teeth may find that they lose out on several opportunities to come up in life.\nDo extra teeth need to be removed?\nIf supernumerary teeth are causing no symptoms or complications, they may not require treatment. However, in most cases of hyperdontia, the extra teeth need to be removed, even if they are not causing discomfort. A dentist may recommend removal if extra teeth cause: difficulties chewing or eating.\nHow many teeth do you have in your adult set?\nPeople start losing their baby teeth and getting their adult set as early as 5 years old. Adults have 32 teeth. You should have this full set of adult teeth by your late teens. Adult teeth include incisors, canines, premolars, and molars: 8 incisors.\nHow many molars are there in a human tooth?\nMolar teeth There are a total of 12 molars in an adult set of teeth, six of which are situated on top and six in the bottom on both upper and lower jaws. These teeth are used for grinding and chewing food to make it easier to swallow and digest. Molars have a broad chewing surface which makes it easier to perform their function.\nCan humans grow multiple sets of teeth?\nHumans are known as diphyodont, which means that we only grow two sets of teeth in our lifetime. Sharks, on the other hand, are polyphyodont, which means that they can produce multiple sets of teeth (sometimes within minutes). Alligators, too, can regenerate a lost tooth up to 50 times. Could this type of tooth growth be possible in humans?\nHow many milk teeth do you need to have?\nThe answer is 20 milk teeth, which gradually fall out to make way for a set of 32 adult teeth – or perhaps a few less, depending on how many wisdom teeth you develop. Each of these teeth in both sets is equally important and therefore must be taken care of if you wish to enjoy good oral health. Take care of those chompers!']	['<urn:uuid:60234ab7-b860-411a-9da0-101a48e8ea87>']	open-ended	direct	verbose-and-natural	similar-to-document	single-doc	novice	2025-05-12T20:11:36.665104	27	80	698
81	anaerobic digestion bacteria cultivation ph requirements	Anaerobic digestion requires strictly-controlled pH conditions for effective waste digestion, as different bacteria types (acetogenic and methanogenic) have unique living conditions. For bacterial cultivation in general, most bacteria grow within a pH range of 4 to 9, with optimal growth occurring in a narrow range of 6.5-7.5. To maintain stable pH, buffer substances like KH2PO4 and K2HPO4 are added to prevent pH shifts caused by bacterial metabolic products.	['Anaerobic digestion is a biological process whereby bacteria break down organic material into more basic compounds without requiring oxygen as a component of the process. These bacteria are believed to have appeared on Earth approximately 3,800,000,000 years ago and were the dominant form of life on the planet before plants appeared. As plant life arose around 3,200,000,000 years ago, anaerobic digestion continued in natural environments where oxygen was absent such as swamps, water-logged soils, and in ground constantly covered by water such as lakes and rivers. The biological processes of anaerobic digestion require that several types of bacteria decompose organic matter in a series of four steps, including hydrolysis, fermentation, acetogenesis, and methanogenesis.\nAs of 2011, the main use for anaerobic digestion by human industry is to produce methane gas for fuel and electricity generation. This is done in waste treatment facilities that process agricultural waste such as manure or municipal waste. The brewing industry also relies on anaerobic digestion to break down organic byproducts of beer production into methane fuel which would otherwise have to be disposed of by municipal waste water treatment systems.\nThe process of anaerobic digestion in nature is also instrumental in generating a form of renewable energy known as natural gas. Though natural gas is a fossil fuel, it consists of about 80% methane along with other related gasses such as propane and butane, and is more readily generated by the earth than other fossil fuels such as petroleum. It is a fossil fuel that is often deposited alongside other fossil fuels as well such as coal and oil.\nIndustrial biomass reactors that process biomass waste like manure to generate fuel generally produce less methane gas as a percentage by volume than what is contained in natural gas. The typical output of a set volume of biogas from a digester is 50% to 80% methane with a significant amount of waste gas in the form of carbon dioxide at 20% to 50%. Other trace gasses are also generated in the process that have some commercial value such as hydrogen, nitrogen, and oxygen, and toxic gasses of which must be safely disposed are also generated including hydrogen sulfide and carbon monoxide.\nThe biological processes that are necessary for waste digestion to effectively take place can be complex and rely on strictly-controlled conditions. Temperature is a major concern in the process as the bacteria that break down the waste thrive best at different levels. Some of the bacteria are mesophilic, thriving at a moderate temperature of 98° Fahrenheit (36.7° Celsius), and some are thermophilic and thrive at a higher optimal temperature of 130° Fahrenheit (54.4° Celsius).\nConditions must be altered for temperature, pH, and other factors like the water versus solid ratio of the biomass mixture and the carbon/nitrogen ratio as the organic material is chemically degraded as well. The two main types of bacteria that are used in anaerobic digestion are acetogenic and methanogenic bacteria, and, though they are used in tandem, each has unique living conditions under which they thrive. Acetogenic bacteria produce the chemical acetate during anaerobic digestion and methanogenic bacteria produce methane.\nBiomass material is taken through four stages for effective methane recovery. The hydrolysis stage uses water to decompose solids or semi-solids into simpler compounds, and then either fermentation or acidogenesis is used to break down carbohydrate chain structures into more basic compounds like ammonia, hydrogen, and organic acids. Acetogenesis is then used as the third step in the process, where acetogenic bacteria convert the organic acids into acetic acid along with further byproducts like hydrogen and carbon dioxide. The final step of methanogenesis uses methanogenic bacteria to combine these primary end products of acetate, hydrogen, and carbon dioxide into methane, which can then be used for fuel.', 'Basic requirements for the cultivation of bacteria are: (I) Abundant Nutrients (II) Optimum Environmental Conditions\nBacteria are present universally almost everywhere; in soil, air, water and even inside mouth and intestine of all animals. ‘Cultivation of bacteria’ or ‘bacteria culture’ means growing these minute invisible bacteria in nutritionally rich substances and suitable environmental conditions, which support their rapid growth and multiplication.\nThis results in their manifestation as large population visible to naked eye (as colonies or turbid suspension). Thus, there are two basic requirements for the cultivation of bacteria, such as (I) Abundant nutrients and (II) Optimum environmental conditions.\n(I) Abundant Nutrients:\nIn nature, bacteria take up the complex nutrients available around them after degrading them into simpler forms by the enzymes secreted by them. But in laboratory, rapid growth is augmented by growing them in substances containing nutrients in simpler forms.\nThese substances containing adequate quantity of nutrients in simpler forms for rapid growth and multiplication of bacteria are called ‘culture media’. There are a number of culture media now available containing different ingredients. Culture media are obtained in the following three ‘physical forms’.\n1. Liquid Media or Broth:\nThese are clear liquids containing water and nutrients in simpler forms for growth of bacteria, which have been sterilized in autoclave. When bacteria is inoculated into them and incubated in conditions suitable for their growth, they grow profusely into thick suspensions of bacteria cells, due to which the media become turbid.\n2. Solid Media:\nThese are solidified substances, in which liquid broths have been supplemented with a solidifying agent called ‘agar’, at a level more than 1%. Agar is a powder (sometimes called agar) extracted from seaweeds and is a complex carbohydrate composed mainly of galactose.\nIt is important to note that; agar is a solidifying agent and has no nutritional value for bacteria. It serves as an excellent solidifying agent, as in aqueous solution it liquefies at 100°C and solidifies at 40°C. Most of the pathogenic bacteria of human are grown at 37°C (normal human body temperature).\nAgar does not melt at this temperature and provides a hard surface. On this nutritionally rich hard surface, each single bacterium grows, multiplies and gives rise to an isolated colony of those bacteria. Such isolation is not possible in liquid media, where bacteria grow in suspension.\nSolid agar media can be prepared in the following forms:\n(a) Agar Plates:\nAgar powder, at a level of more than 1%, is dissolved completely in liquid broth by warming. Then, it is sterilized in an autoclave. It remains liquid in hot condition and solidifies as it cools. In the warm liquid state, it is poured into sterilized petri dishes (20 ml in each petri dish) and allowed to cool, so that it forms a thick layer at the bottom of the petri dishes.\nThese petri dishes containing a thick layer of solidified agar media are called ‘agar plates’ or simply ‘plates’. Bacteria are grown as isolated colonies on these plates for their enumeration and isolation.\nThe characteristics of these colonies also help in the identification of bacteria. Petri dishes are available in different sizes to meet different experimental conditions. For routine purposes, however, petri dishes of 15 cm diameter are used.\n(b) Agar Slants:\nTo prepare ‘agar slants’ or simply ‘slants’, agar powder, at a level of more than 1%, is dissolved completely in liquid broth by warming. In the warm liquid state, it is distributed into test tubes (20 ml in each test tube), cotton-plugged and sterilized in autoclave.\nThe sterilized medium in test tubes is allowed to solidify in slanting position, so as to get the slants. The slants are used for maintenance of pure stock cultures of bacteria obtained by isolating on agar plates.\nThese are also used for maintenance of standard reference stock cultures obtained from international standard laboratories. Pure cultures and standard cultures are maintained by periodically (usually 15 days to 1 month) transferring to fresh slants aseptically. The growth characteristics on slants also help in identification of bacteria.\n(c) Agar Deep Tubes:\nAgar deep tubes are prepared following the same procedure as followed in the preparation of agar slants except that, here the media, after sterilization, are allowed to solidify in upright position. These are primarily used for study of gaseous requirements of bacteria.\n3. Semi Solid Media:\nThey contain agar at a level less than 1%, so that they remain in a semisolid state. They are used for specific studies.\n(II) Optimum Environmental Conditions:\nSeveral environmental conditions influence the growth of bacteria. To cultivate a bacteria, it should be provided with a set of optimum environmental conditions. The most important environmental conditions to be provided during cultivation of bacteria are given below.\n1. pH of the Media:\nEach species of bacteria can grow within a specific pH range. For most bacteria this range is between 4 and 9. However, the maximum growth of each species of bacteria occurs within a very narrow pH range. This optimum pH range for most bacteria is 6.5-7.5 and that for most of the fungi, molds and yeasts is 4-6, as they prefer acidic environments.\nFor maximum growth to occur, the pH of the culture media has to be adjusted to this pH during its preparation. Chemical substances that act as buffers (e.g. KH2P04, K2HP04) are also added to the media to prevent any shift in pH that may occur due to accumulation of acidic or basic metabolic end products produced by the bacteria during their growth.\n2. Temperature of Incubation:\nMesophiles are incubated at 37°C, thermophiles at 55°C and psychrophiles at 4°C.']	['<urn:uuid:ca3143b6-ff5d-49cf-9c53-4c2a2ca290e3>', '<urn:uuid:b1feeac3-ce65-4a1a-8d52-75f26240fdb7>']	open-ended	with-premise	short-search-query	similar-to-document	comparison	expert	2025-05-12T20:11:36.665104	6	68	1556
82	explain similarities differences trustee obligations bankruptcy investigation versus corporate fraud examiner duties responsibilities	Bankruptcy trustees have specific obligations to investigate and verify bankrupt's assets and affairs, including questioning the bankrupt and pursuing potential assets - as demonstrated in the Bowles case where trustees investigated framework agreements and sought discharge suspension. Similarly, fraud examiners have investigation duties but with broader scope - they must determine if fraud occurred, gather evidence, conduct interviews, write reports, testify to findings, and assist in fraud detection and prevention. Both roles involve investigative responsibilities, but fraud examiners also focus on organizational objectives like identifying improper conduct, preventing future occurrences, and strengthening internal controls.	['Postponement of discharge from bankruptcy: High Court emphasises that suspicion is not enough\nA bankrupt is discharged after one year (Insolvency Act 1986, s. 279(1)). But the court has a discretion to extend this period “if satisfied that the bankrupt has failed or is failing to comply with an obligation” imposed by virtue of the bankruptcy (subsection (4)).\nThe recent case of Bowles v Trefilov is a warning to trustees that suspicion is not enough.\nA Russian businessman, the former owner of a chain of Russian supermarkets, was made bankrupt with debts of £121m, shortly after being found liable in proceedings in London to pay 328 million roubles to a Russian bank. He was interviewed by the trustees and corresponded with them, answering various questions about his assets and affairs. There was an initial allegation of inadequate co-operation with the trustees in particular respects, which was dealt with by a short extension of the bankruptcy and undertakings to provide the desired cooperation. The trustees, however, continued to be unhappy generally at the bankrupt’s answers to their questions. They sought a further extension of the bankruptcy.\nThe Trustees’ main concern was with a “framework agreement” they had become aware of. Under this agreement, an English company owned by an acquaintance of the bankrupt had contracted with a Dutch supermarket holding-company to receive up to US $100m in commission payments from it and related Russian entities in return for services as negotiator of new leases of supermarket premises across Russia. The bankrupt had signed the agreement as attorney for the English company, and much of the negotiation had in practice been carried out by him personally.\nThe Dutch company failed to pay under the “framework agreement” and was sued in London by the English company, this litigation settling on the basis of an agreement to pay US $14m. The trustees obtained an injunction freezing that sum before it could be paid over to the English company, their underlying claim being that it actually belonged to the bankrupt, or at least that he had an interest in it, or in the English company.\nUnless the court intervened to suspend discharge, the bankruptcy was due to expire before this claim could be heard.\nThe trustees maintained that the bankrupt’s refusal to accept that he had some sort of interest, and the answers he had given in support of this denial, amounted to a breach of his obligation, as a person in bankruptcy, to give them a full account of his affairs. They relied on various alleged inconsistencies in his explanations and between those explanations and things said on his behalf in other contexts, and on what they claimed was the implausibility of his account.\nThe court (Chief Registrar Baister) first considered the meaning of the words in section 279(4), “has failed or is failing to comply with an obligation”, which are generally understood to present a jurisdictional hurdle to an application to suspend the discharge of a bankrupt (Bramston v Haut  EWCA Civ 1637,  1 W.L.R. 1720). The court held that the words should be construed entirely literally: any failure to comply with an obligation, even if purely historic and now corrected, was enough to give the court jurisdiction to suspend discharge. Thus, the fact there had been some non-cooperation initially, albeit unrelated to the complaint the trustees now made, was enough in the court’s view to give it jurisdiction to suspend the discharge further.\nThis left, however, the question of discretion. The court was not persuaded to exercise this discretion:\n“Whilst I might share and sympathise with the applicants’ suspicions about the relationship between [the bankrupt] and [the English company], as Mr Registrar Nicholls rightly observed in Chadwick v Nash  BPIR 70, suspicion is not enough. On an application of this kind the court will often give great weight to the views of a trustee, but it cannot abandon its role by simply adopting the trustee’s stance. I remind myself that interfering with a bankrupt’s expectation of discharge after the one year period provided for by the Insolvency Act is a serious interference with his rights and is not something to be done lightly” (para 51).\nAlthough the court characterised the material relied on by the trustees as “very persuasive”, it was not so much so as to warrant disbelieving the bankrupt’s account in this context. The application was dismissed with costs.\nPlease subscribe here\nOctober 8, 2021', 'center23002457459410012100center818008745855Manav [email protected] email@example.com of Fraud Examination 9410036300Principles of Fraud Examination 213360838209410012100IntroductionFraud is a very steep problem at a global level these days.\nFraud is typically considered as a white collar crime and the process of examination involves investigation and analysis of complicated financial records. Fraud examination “”refers to a process of resolving allegations of fraud from inception to disposition, and it is the primary function of the anti-fraud professional”. Fraud examinations are focused on determining whether the fraud has occurred and, if so, then to gather evidence of the crime.Fraud examination is non-recurring and scope specific. It might be required by law.\nWe Will Write a Custom Essay Specifically\nFor You For Only $13.90/page!\nAn obligation to investigate can arise from statutes, regulations, contracts, or common law duties. Fraud examination became the need of the hour in the USA afterward the mishap of 9/11. Fraud examiners were then recognized by FBI to help them in combating the frauds. For instance, an establishment’s directors and officers owe a common duty of care to their association and shareholders, and therefore, when qualms of fraud arise, it might be essential for them to conduct an examination to confirm that they have complete knowledge of such issues affecting the company.It is a practical orientation as to how to prevent, detect and investigate fraud within a business. It can be used to classify the different types of fraud as well as to construct an environment in which fraud is minimized. However, dealing with complex issues of fraud, regulatory compliances and business disputes can diminish an organization in its efforts to prosper.Categorization of Fraud ExaminationFraud is hidden: Unlike other offenses, part of the method of fraud is to conceal its existence.\nLike hiding data which is suspicious. Reverse proof: When an examiner is to prove that fraud has been occurred, the proof must include attempts to prove that the fraud is not occurred. The existence of fraud: In resolving fraud issues, the examiner must postulate a theory of guilt or innocence in order to attempt to prove his theory. 2286001949459410012100Fraud Tree:Occupational Frauds are caused by members of an organization such as an employee, manager or even an owner of such an organization that might cause severe damage to the organization. Majorly, occupational frauds can be categorized as under:-403860176530002286001949459410012100Fraud Examination – Objective:Fraud examination can be defined as the procedure of determining allegations of fraud from inception to disposition. Fraud examination is used for much more than just reckoning out who committed the crime of fraud, but more specifically, fraud examination involves procurement of pieces of evidence and captivating statements, writing reports, testifying to findings, and assisting in the detection and prevention of fraud.\nIn specific, fraud examination can address a number of organizational objectives such as, classifying improper conduct, classifying the personnel responsible for fraudulent conduct, preventing the fraud occurrence, spreading the message throughout the organization that fraud will not be tolerated, determining the amount of potential accountabilities or losses that might exist, serving to facilitate the recovery of losses, discontinuing future losses, modifying other potential consequences and consolidation of internal control weaknesses. Steps in Fraud Investigation1729740229870Identity Type of Fraud00Identity Type of Fraud2484120344170001729740134620Create an Investigation PlanPlan00Create an Investigation PlanPlan248412022606000172974016510Interview or Re-Interview Victim00Interview or Re-Interview Victim2484120147320001729740270510Securing Evidence00Securing Evidence2286001981209410012100Fraud Examination can be divided into three basic aspects:Fraud Detection: The term “Fraud Detection” is normally perceived in Banking ; Financial Sectors, insurance, government agencies and several other Law Enforcement bodies. The important aspect of the fraud detection is to protect the clients and enterprises information, their assets, accounts, and transactions, via a real-time examination of actions by the users and other defined entities. It includes connecting all the data points in order to ascertain the potential fraudulent behaviour. Fraud Investigation: A fraud investigation helps to come to a conclusion whether a fraud has taken place or not and also gathers the evidence in order to protect the victims involved.\nFraud investigations involve the occurrence of a fraud which basically involves an intention to deceive the other party. Fraud Investigation also benefits the companies’ as to how to manage the risk, measure the financial inferences of the disputes and investigate alleged misconduct. Fraud Prevention: Fraud Prevention is a process which doesn’t have a starting or the ending point. Rather, it is an ongoing cycle which involves different stages such as monitoring, detection, decisions, case management and learning. In order to prevent an array of fraud attacks, an organization must follow three key steps, they are as follows:Capture and unify all available data types across the channels and incorporate them into an analytical process. Continually monitor transactions and apply behavioural analytics to enable real-time decision making.\nEmploy layered security techniques. Fraud comes in many forms but primarily takes three form like Asset misappropriation, corruption, and financial statement fraud. In order to be safe, the organizations, whether large or small should have a plan in place. Knowing your employees, making employees aware, Implementing Internal Controls, etc. be some of the ways in which a company may be able to keep a check on different kinds of frauds. 2286002286009410012100Fraud Examination Methodology: Evaluation of the accessible data: Evaluation procedures involves inspecting, cleaning, modifying, and presenting the data with the motive of discovering useful information, apprising conclusions, and supporting the decision-making while also keeping in mind the possibility of fraud resulting from the anonymous allegations. Generating a hypothesis: The hypothesis is unvaryingly a “worst-case” scenario which is based on the allegation, and what is the worst possible outcome? Fraud examiners create hypotheses for any specific allegation (e.g.\n, a bribery or kickback scheme, embezzlement, clash of interest, or financial statement fraud). When the hypotheses is complete, fraud examiners recognize that each and every specific scheme has its own unique characteristics that constitute the badges or “red flags” of fraud.Testing the hypothesis: Testing a hypothesis involves building a “what-if” scenario. In the hypothesis testing, an analyst firstly examines the statistical sample, with the goal of accepting or rejecting a null hypothesis.\nIf it isn’t true, the analyst formulates a new hypothesis to be tested, repeating the process until data reveals a true hypothesis.Refining and amending the hypothesis: After the testing is done by the examiner, then comes to the step of Refining and amending the hypothesis. If during the testing phase, the fraud examiner discovers that all facts do not fit a particular scenario then the hypothesis is again reviewed and retested.\nVarious types of organizational frauds like:Cash Larceny which is an occupational Fraud which involves the theft of money that has been already appeared in the company’s ledger. Skimming fraud which is known as off-book frauds, meaning thereby, the removal of cash before recorded in the company’s account.2209802514609410012100Case Studies:Like the famous Enron company case, as being one of the companies which fell down too fast due to mismanagement, conflict of interest, lack of fundamental ethical values and accounting fraud.Satyam Computer Services was the biggest accounting fraud case which gained recognition to the forensic accounting as the profession. In this we can again acknowledge that company’s ethics were compromised and fraud occurred. So we need to have a check internally and externally as well from time to time in order to secure the company from occurrence any possibility of fraud.\nNirav Modi and Kingfisher cases both can be classified into financial fraud categories as in the Nirav Modi case forensic financial analysis was conducted of the books and records of the mortgagors, merchant chronicles, bank chronicles, and all other relevant information of other entities constant with the investigation, and trace the movement of monies gained under the suspected fraud circumstances against the PNB to the debtors, and connected entities and individuals. While in the Kingfisher’s case was a serious financial fraud investigation was done as Kingfisher fraudulently induced banks to convert part of debt into preference shares by deceptive projects. Further even all the corporate ethics were compromised. Conclusion:Persons who indulge in committing fraud do not discriminate.\nIt can ensue in large or small companies in several industries and geographic locations. Professional fraud can result in an enormous financial loss, legal costs, and ruined reputations that can eventually lead to the downfall of an organization. Having the proper plans in place can meaningfully reduce fraudulent activities from occurring or cut losses if a fraud already occurred. Making the company policy known to employees is one of the best ways to deter fraudulent behaviour.']	['<urn:uuid:a3e4e7ab-3770-40dd-8840-dc27fda94ab0>', '<urn:uuid:9fcba339-070f-4853-8328-3ba9fd2f5cc4>']	open-ended	with-premise	long-search-query	distant-from-document	multi-aspect	expert	2025-05-12T20:11:36.665104	13	94	2132
83	I'm a dental researcher investigating tooth regeneration techniques - what are the key advantages of using treated dentin matrix (TDM) as a scaffold for tissue engineering?	Treated dentin matrix (TDM) has several key advantages as a scaffold: it is available in abundance from human premolars, can be easily trimmed to different shapes according to patient needs, has no obvious immune rejection after implantation due to decellularization, and contains bioactive molecules and factors related to tooth formation. Additionally, TDM is non-cytotoxic, bioactive, and capable of providing a three-dimensional microenvironment that supports cell growth and odontogenic differentiation.	"[""Review Article - Biomedical Research (2017) Volume 28, Issue 3\nTreated dentin matrix combined with dental follicle cells as a promising approach for tissue engineered tooth root regenerationWen Luo#, Shaomin Wen# and Genjian Zheng*\nDepartment of Stomatology, the Affiliated Hospital of Hainan Medical University, No. 31, Longhua Road, Haikou 570102, Hainan Province, PR China\n- *Corresponding Author:\n- Genjian Zheng\nDepartment of Stomatology the Affiliated Hospital of Hainan Medical University, PR China\nAccepted on August 05, 2016\nTissue-engineered tooth regeneration is an effective treatment of tooth loss. Tissue engineering strategies to construct a tooth employ interactions between seeding cells, scaffolds, and inductive microenvironments. However, there are still no suitable scaffolds to restore the tooth and the surrounding normal tissue structure. Treated Dentin Matrix (TDM) is a calcified tissue which makes up most of the tooth and becomes a key component for tissue engineering of tooth structure. TDM is available in abundance, can easily be trimmed to a variety of shapes, and has no obvious immune rejection. TDM scaffold combined with dental follicle cells (DFCs) could be implanted into an inductive alveolar fossa microenvironment to regenerate the tooth root. We propose that TDM scaffold combined with DFCs provides a new approach for the treatment of tooth loss.\nTreated dentin matrix, Dental follicle cells, Tissue-engineering, Tooth root, Regeneration.\nLoss of permanent dentition is a common condition that has a negative effect on a patient’s overall quality of life. The common causes for tooth loss include caries, periodontal disease and accidents. Conventionally, lost teeth are replaced with dentures or synthetic dental implants. However, none of those options can restore a normal tooth structure and completely repair masticatory function.\nA tissue-engineered tooth is made possible by harvesting seeding stem cells from the patient, obtaining a suitable scaffolding material for cell growth and inducing the stem cells toward a relevant lineage, and then reintroducing cell-scaffold construct to the patient's inductive alveolar fossa microenvironment to reconstruct the tooth and the surrounding normal tissue structures (Figure 1) [1-4]. During tissueengineered tooth regeneration, the tooth root is primarily regenerated while the crown is restored with traditional prosthodontic treatment. This strategy not only restores a patient's appearance and masticatory function, but also is effective to treat tooth loss. Commonly used cell scaffolding materials include Ceramic Bovine Bone (CBB), nanostructured materials, the copolymer of oly-DL-lactide and glycolide (PLG), polyglycolic acid (PLGA), polysaccharide and hydroxyapatite/tricalcium phosphate (HA/TCP) [3,5-9]. These materials typically provide only structural support for cells but could not provide an environment conducive to cell differentiation relevant to tooth regeneration. There is an abundance of bioactive molecules, factors, and proteins related to pulp and dentin tissue formation in dentin matrix, such as dentin sialoprotein (DSP), dentin matrix protein 1 (DMP1), bone sialoprotein (BSP), type I collagen (COLI), alkaline phosphatase (ALP) and so on . Treated dentin matrix (TDM) scaffold with inductive properties may offer specific benefits in achieving simultaneous regeneration of periodontium/pulp-dentin complex and constraining the final shape of tissue engineered tooth [11-13].\nTreated Dentin Matrix Scaffold and Tooth Root Regeneration\nTDM is a calcified tissue which makes up most of the tooth and becomes a key component for tissue engineering of tooth structure. TDM is able to induce the differentiation of dental follicle cells (DFCs) into odontoblast lineage due to the signaling molecules such as bone morphogenetic protein 2 and transforming growth factor beta (TGF-β) inside TDM. TDM is non-cytotoxic, bioactive, and capable of providing a threedimensional microenvironment supporting cell growth, odontogenic differentiation, and cellular organization into the structure of the tissue. TDM scaffold is available in abundance from human premolars and can be trimmed easily to a variety of shapes according to the patient’s jaw and occlusion. Due to decellularization, there is no obvious immune rejection after TDM implantation in the host .\nNatural tooth root structures include dental pulp (DP), odontoblasts layer, dentin (DE), cementum (CE), periodontal ligament (PDL) inserting into CE and alveolar bone (AB). TDM from human or rat has recently been shown to be a suitable scaffold for dentin regeneration [11-14]. Implantation of human TDM combined with DFCs in vivo for 8 weeks in a nude mouse model led to complete regeneration of dentin tissues, which expressed the dentin markers DSP and DMP-1 (regarded as the markers of odontoblasts) . This study showed that TDM is an ideal biomaterial for human dentin regeneration. In another study murine DFCs were seeded onto immunocompetent murine TDM and further incubated for 1-2 weeks in vitro and for 2-4 weeks in vivo. In vitro, in addition to DSP and DMP1 , DFCs induced by TDM expressed osteocalcin (OCN), DSP, COLI, osteopontin (OPN), osteonectin (ON) and ALP, all of which are expressed by odontoblasts. In vivo, TDM induced the regeneration of dentin, which expresses DSP and DMP-1.\nThese results showed the successful regeneration of complete and prefabricated-shaped dentin . Upon transplantation of allogenic rat TDM combined with DFCs into immunocompetent rats for four weeks, tooth root-like tissues stained positive for markers of dental pulp and periodontal. These tissues were regenerated in the alveolar fossa, but not in the skull and omental pockets. These results confirm that the combination of TDM with DFCs in the alveolar fossa is a feasible strategy for tooth root regeneration . During implantation, the alveolar bone should be prepared appropriately to fix TDM scaffold. Fresh alveolar fossa is adopted as an inductive microenvironment for the regeneration of PDL/CE complex . TDM shows excellent biocompatibility and bioactivity due to the decellularization and reserve of an abundance of bioactive factors. Furthermore, the success of cryopreservation of TDM provides a tissue engineering scaffold that is readily available for patient treatments . Thus TDM is an ideal biomaterial for human tooth root regeneration.\nDental Follicle Cells and Tooth Root Regeneration\nNon-dental stem cells including bone marrow mesenchymal stem cells , adipose-derived stem cells , embryonic stem cells , and induced pluripotent stem cells (iPS)  have been shown to induce periodontal regeneration. These cells have the potential of differentiating into osteoblasts, cementoblasts and fibroblasts, and can form cementum/PDLlike structures under appropriate conditions. The dental follicle (DF) is a loose connective tissue surrounding the unerupted tooth and plays crucial role in tooth eruption by anchoring the tooth in its socket to the surrounding alveolar bone. DF is present in human impacted teeth, which is commonly extracted and disposed as medical waste in dental practices. DF is also easily obtained from clinically discarded third molar extractions. DFCs isolated from DF possess mesenchymal stem cell (MSC)-like qualities, including the capacity for selfrenewal and multi-lineage differentiation including adipogenesis, osteogenesis, neurogenesis, chondrogenesis, and thus form pulp-dentin complex and PDL/CE-like tissue [14,21-24].\nDFCs are the progenitors of osteoblasts and periodontal ligament cells (PDLCs) within dental follicle. Under certain conditions, DFCs can be induced to differentiate into a variety of cells such as chondrogenic, osteogenic, adipogenic and neuronal cells. In particular, DFCs have significant advantages for periodontal tissue regeneration over PDLCs or other dentalderived cells. Furthermore, human DFCs can be easily isolated from discarded wisdom tooth, and thus are the ideal cell source for tooth regeneration [14,21]. DFCs isolated from human impacted molars were combined with ceramic bovine bone and implanted into subcutaneous pockets of nude mice. The formation of a PDL/CE complex was observed six weeks after implantation, indicating that heterogeneous DFCs contribute to the formation of PDL/CE complex [21,25].\nDental follicle cell sheets (DFSCs) were harvested from the third molars of six-month-old pigs and expanded in vitro. Primary dental pulp cells, primary enamel organ epithelial cells and subcultured DFSCs were then successively added to mimic the tooth primordia. The combination was inserted into a cavity inside the shaft of the bone and later transplanted into the omentum of six-week-old immunocompromised rats. After 24 weeks, large amount of regenerated dentin was observed and odontoblasts were present along the formed dentin. A thick layer of cementum-like tissue covered the newly formed dentin and the space between the regenerated dentin and bone was filled with connective tissues. Collagen fiber-like tissues resembling PDL were perpendicularly attached to the cementum-like layer. However, collagen fiber-like tissues were not observed on the surface of the cementum with any bone anchorage . DFCSs also improved the survival rate of implanted cells for dentinogenesis and tooth root construction. In vitro, after TDM treatment DFCSs highly expressed DMP-1 and BSP, indicating the potential for odontogenesis and osteogenesis. In vivo, TDM could induce and support DFCs to develop a new pulp-dentin complex and PDL/CE-like tissues that were positive for markers such as DSP, nestin, VIII factors, COLI and cementum attachment protein (CAP), indicating successful tooth root regeneration . Taken together, these results suggest that DFCs are suitable seeding cell for pulp-dentin complex and PDL/CE-like tissue regeneration which are essential components of tooth root.\nIn conclusion, the combination of TDM with DFCs in the induced alveolar fossa may be a promising approach for tissueengineered tooth root regeneration, which provides an effective treatment for tooth loss.\n- Flynn LE. The use of decellularized adipose tissue to provide an inductive microenvironment for the adipogenic differentiation of human adipose-derived stem cells. Biomaterials 2010; 31: 4715-4724.\n- Angelova-Volponi A, Kawasaki M, Sharpe PT. Adult human gingival epithelial cells as a source for whole-tooth bioengineering. J Dent Res 2013; 92: 329-334.\n- Mitsiadis TA, Woloszyk A, Jiménez-Rojo L. Nanodentistry: combining nanostructured materials and stem cells for dental tissue regeneration, Nanomedicine (Lond) 2012; 7: 1743-1753.\n- Horst OV, Chavez MG, Jheon AH, Desai T, Klein OD. Stem cell and biomaterials research in dental tissue engineering and regeneration. Dent Clin North Am 2012; 56: 495-520.\n- Holtgrave EA, Donath K. Response of odontoblast-like cells to hydroxyapatite ceramic granules. Biomaterials 1995; 16: 155-159.\n- Honda MJ, Tsuchiya S, Sumita Y, Sagara H, Ueda M. The sequential seeding of epithelial and mesenchymal cells for tissue-engineered tooth regeneration. Biomaterials 2007; 28: 680-689.\n- Batouli S, Miura M, Brahim J, Tsutsui TW, Fisher LW, Gronthos S. Comparison of stem-cell-mediated osteogenesis and dentinogenesis. J Dent Res 2003; 82: 976-981.\n- Duailibi SE, Duailibi MT, Zhang W, Asrican R, Vacanti JP, Yelick PC. Bioengineered dental tissues grown in the rat jaw. J Dent Res 2008; 87: 745-750.\n- Gronthos S, Brahim J, Li W, Fisher LW, Cherman N, Boyde A. Stem cell properties of human dental pulp stem cells. J Dent Res 2002; 81: 531-535.\n- Park ES, Cho HS, Kwon TG, Jang SN, Lee SH, An CH. Proteomics analysis of human dentin reveals distinct protein expression profiles. J Proteome Res 2009; 8: 1338-1346.\n- Yao S, Pan F, Prpic V, Wise GE. Differentiation of stem cell in the dental follicle. J Dent Res 2008; 87: 767-771.\n- Li R, Guo W, Yang B, Guo L, Sheng L, Chen G. Human treated dentin matrix as a natural scaffold for complete human dentin tissue regeneration. Biomaterials 2011; 32: 4525-4538.\n- Guo W, He Y, Zhang X, Lu W, Wang C, Yu H. The use of dentin matrix scaffold and dental follicle cells for dentin regeneration. Biomaterials 2009; 30: 6708-6723.\n- Guo W, Gong K, Shi H, Zhu G, He Y, Ding B. Dental Follicle Cells and Treated Dentin Matrix Scaffold for Tissue Engineering the Tooth Root. Biomaterials 2012; 33: 1291-1302.\n- Ikeda E, Morita R, Nakao K, Ishida K, Nakamura T, Takano-Yamamoto T. Fully functional bioengineered tooth replacement as an organ replacement therapy. ProcNatlAcadSci USA 2009; 106: 13475-13480.\n- Jiao L, Xie L, Yang B, Yu M, Jiang Z, Feng L.Cryopreserved dentin matrix as a scaffold material for dentin-pulp tissue regeneration. Biomaterials 2014; 35: 4929-4939.\n- Kawaguchi H, Hirachi A, Hasegawa N, Iwata T, Hamaguchi H, Shiba H. Enhancement of periodontal tissue regeneration by transplantation of bone marrow mesenchymal stem cells. JPeriodonto 2004; l75: 1281-1287.\n- Tobita M, Uysal AC, Ogawa R, Hyakusoku H, Mizuno H. Periodontal tissue regeneration with adipose-derived stem cells. Tissue Eng Part A 2008; 14: 945-953.\n- Inanç B, Elçin AE, Unsal E, Balos K, Parlar A, Elçin YM. Differentiation of human embryonic stem cells on periodontal ligament fibroblasts in vitro. Artif Organs 2008; 32: 100-109.\n- Duan X, Tu Q, Zhang J, Ye J, Sommer C, Mostoslavsky G. Application of induced pluripotent stem (iPS) cells in periodontal tissue regeneration. J Cell Physiol 2011; 226: 150-157.\n- Guo W, Chen L, Gong K, Ding B, Duan Y, Jin Y. Heterogeneous dental follicle cells and the regeneration of complex periodontal tissues. Tissue Eng Part A 2012; 18: 459-470.\n- Sujesh M, Rangarajan V, Ravi Kumar C, Sunil Kumar G. Stem cell mediated tooth regeneration: new vistas in dentistry. J Indian ProsthodontSoc 2012; 12: 1-7.\n- Zivkovic P, Petrovic V, Najman S, Stefanovic V. Stem cell-based dental tissue engineering. ciWorldJ 2010; 10: 901-916.\n- Rodríguez-Lozano FJ, Insausti CL, Iniesta F, Blanquer M, Ramírez MD, Meseguer L. Mesenchymal dental stem cells in regenerative dentistry. Med Oral Patol Oral Cir Bucal2012; 17: e1062-e1067.\n- Han C, Yang Z, Zhou W, Jin F, Song Y, Wang Y. Periapical follicle stem cell: a promising candidate for cementum/periodontal ligament regeneration and bio-root engineering. Stem Cells Dev 2010; 19: 1405-1415.\n- Honda MJ, Tsuchiya S, Shinohara Y, Shinmura Y, Sumita Y. Recent advance in engineering of tooth and tooth structure using postnatal dental cells. Jpn Dent Sci Rev 2010; 46: 54-66.\n- Yang B, Chen G, Li J, Zou Q, Xie D, Chen Y. Tooth root regeneration using dental follicle cell sheets in combination with a dentin matrix-based scaffold. Biomaterials 2012; 33: 2449-2461.""]"	['<urn:uuid:c742ee51-dda3-412b-a19d-1e22422b6bc6>']	factoid	with-premise	verbose-and-natural	similar-to-document	single-doc	expert	2025-05-12T20:11:36.665104	26	69	2218
84	depth sensing technology smartphone adoption advantages and security risks biometric authentication	Depth sensing technology is being widely adopted in smartphones through integrated 3D cameras for features like face recognition and enhanced picture quality. While this drives market growth and enables convenient biometric authentication, there are significant security risks. Hackers can easily exploit facial recognition by using photos from social media or 3D-printed heads, and compromised biometric data, unlike passwords, cannot be changed and could persist as a security risk for a person's lifetime.	"['Depth Sensing Market Size And Forecast\nDepth Sensing Market size was valued at USD 9.8 Billion in 2021 and is projected to reach USD 16.39 Billion by 2030, growing at a CAGR of 5.9% from 2023 to 2030.\nMajor factors which drive the market growth include The rising adoption of dual-camera smartphones along with accelerating the smartphone market is expected to drive the Depth Sensing Market. The Global Depth Sensing Market report provides a holistic evaluation of the market. Additionally, the market is anticipated to develop as a result of rising consumer electronics sales and increased use of augmented reality in video games. The Global Depth Sensing Market report provides a holistic evaluation of the market. The report offers a comprehensive analysis of key segments, trends, drivers, restraints, competitive landscape, and factors that are playing a substantial role in the market.\n>>> Get | Download Sample Report @ – https://www.verifiedmarketresearch.com/download-sample/?rid=5576\nGlobal Depth Sensing Market Definition\nDepth sensing technology means it is a process of using a depth sensor device in order to measure the distance to an object. And also with the help of this depth-sensing technology, any target that can be a line, dots, or even a collection of dots making an area can be measured easily. 3D depth sensors, also known as 3D scanners cameras or 3D cameras are the devices that are used for generating depth information from a certain scene by measuring the distance between the sensor and objects in front of it.\nDepth sensing technology has so far been used mainly for industrial applications, but many smartphone companies such as Apple are on the cusp of bringing it to the consumer market which will be a potential factor for driving significant changes in how people interact with devices in their daily life.\n>>> Ask For Discount @ – https://www.verifiedmarketresearch.com/ask-for-discount/?rid=5576\nGlobal Depth Sensing Market Overview\nThe Depth Sensing Market is experiencing a scaled level of attractiveness in the Asia Pacific region. The Asia Pacific area has a prominent presence and will account for a significant market share by 2030. China and India are Asia Pacific’s main markets. China. is growing with a substantial CAGR over the forecast period. At present, the latest smartphones are coming up with integrated depth-sensing technology in the form of 3D cameras. This feature of smartphones can sense movements while taking pictures and enhance picture quality. The integration of a 3D camera could be the first step toward a new generation of enhanced HMI.\nFace recognition features will likely be a significant application driving the adoption of 3D sensing in smartphones. Thus, the increasing adoption of dual-camera smartphones along with the accelerating smartphone market is expected to drive the Depth Sensing Market. Moreover, the upgrading application of depth-sensing technology in AR-VR gaming in order to maintain a sense of presence in reality, as well as virtually, will also contribute to boosting the market for depth sensing.\nGlobal Depth Sensing Market: Segmentation Analysis\nThe Global Depth Sensing Market is Segmented on the basis of Type, Technology, Components, And Geography.\nDepth Sensing Market, By Type\n- Active Depth Sensing\n- Passive Depth Sensing\nBased on Type, the market is bifurcated into Active Depth Sensing and Passive Depth Sensing. Owing to the increasing need for precise depth calculations in a variety of industries, including consumer electronics, industrial, automotive, etc., it is anticipated that the Active Depth Sensing Market will expand over the anticipated period.\nDepth Sensing Market, By Technology\n- Stereo Vision\n- Structured Light\n- Time of Flight\nBased on Technology, the market is bifurcated into Stereo Vision, Structured Light, and Time of Flight. Depth sensing Time-to-flight technology is anticipated to experience the highest CAGR growth. Low power consumption, precise depth data, and very straightforward software and hardware requirements can all be credited for this industry expansion. The rise of the global time-of-flight Depth Sensing Market can be attributed to low power consumption, accurate depth data, and relatively straightforward software and hardware requirements.\nDepth Sensing Market, By Components\n- Camera/Lens Module\nBased on Components, the market is bifurcated into Camera/Lens Module, Sensor, and Illuminator. The market for the sensor is expected to grow at the highest CAGR over the forecast period. The sensor contains a monochrome CMOS sensor and infrared projector that help in creating 3D imagery. The sensor is able to determine the depth of the subject through the processing of the displacement of the dots and creates a depth map. This component of the depth sensor has the most scope for variation in terms of capability and is a key factor in the determination of the overall quality of a depth sensor as it plays a large role in the accuracy of the end product.\nDepth Sensing Market, By Geography\n- North America\n- Asia Pacific\n- Rest of the world\nOn the basis of Geography, the Global Depth Sensing Market is classified into North America, Europe, Asia Pacific, and the Rest of the world. The Asia pacific is anticipated to dominate the market in this forecast period. This growth is due to rising consumer electronic and automotive plants in countries in China, Japan, and India. In North America, the Depth Sensing Market is expected to grow with a decent CAGR during the forecast period.\nThe “Global Depth Sensing Market” study report will provide valuable insight with an emphasis on the global market including some of the major players such as Texas Instruments (US), Infineon Technologies (Germany), Qualcom (US), Creative (Singapore), Occipital (US), Stereolabs (US), Pmdtechnologies (Germany), Sony Depthsensing (Belgium), Intel (US), Tower Semiconductor (Israel), Vermagic (Germany), New Vision Technologies (Germany), Espros (Switzerland), Sunny Optical Technology (China), PrimeSense (Israel).\nOur market analysis also entails a section solely dedicated to such major players wherein our analysts provide an insight into the financial statements of all the major players, along with product benchmarking and SWOT analysis. The competitive landscape section also includes key development strategies, market share, and market ranking analysis of the players mentioned above globally.\n- In 2018, The Qualcomm Spectra Module Program has been expanded by Qualcomm Technologies Inc., enabling enhanced biometric authentication.\nAce Matrix Analysis\nThe Ace Matrix provided in the report would help to understand how the major key players involved in this industry are performing as we provide a ranking for these companies based on various factors such as service features & innovations, scalability, innovation of services, industry coverage, industry reach, and growth roadmap. Based on these factors, we rank the companies into four categories as Active, Cutting Edge, Emerging, and Innovators.\nThe image of market attractiveness provided would further help to get information about the region that is majorly leading in the Global Depth Sensing Market. We cover the major impacting factors that are responsible for driving the industry growth in the given region.\nPorter’s Five Forces\nThe image provided would further help to get information about Porter’s five forces framework providing a blueprint for understanding the behavior of competitors and a player’s strategic positioning in the respective industry. Porter’s five forces model can be used to assess the competitive landscape in Global Depth Sensing Market, gauge the attractiveness of a certain sector, and assess investment possibilities.\nValue (USD Billion)\n|Key Companies Profiled|\nTexas Instruments (US), Infineon Technologies (Germany), Qualcom (US), Creative (Singapore), Occipital (US), Stereolabs(US), pmdtechnologies (Germany).\nBy Components, By Technology, By Type, and By Geography.\nFree report customization (equivalent to up to 4 analyst working days) with purchase. Addition or alteration to country, regional & segment scope\nTop Trending Reports:\nResearch Methodology of Verified Market Research:\nTo know more about the Research Methodology and other aspects of the research study, kindly get in touch with our Sales Team at Verified Market Research.\nReasons to Purchase this Report\n• Qualitative and quantitative analysis of the market based on segmentation involving both economic as well as non-economic factors\n• Provision of market value (USD Billion) data for each segment and sub-segment\n• Indicates the region and segment that is expected to witness the fastest growth as well as to dominate the market\n• Analysis by geography highlighting the consumption of the product/service in the region as well as indicating the factors that are affecting the market within each region\n• Competitive landscape which incorporates the market ranking of the major players, along with new service/product launches, partnerships, business expansions, and acquisitions in the past five years of companies profiled\n• Extensive company profiles comprising of company overview, company insights, product benchmarking, and SWOT analysis for the major market players\n• The current as well as the future market outlook of the industry with respect to recent developments which involve growth opportunities and drivers as well as challenges and restraints of both emerging as well as developed regions\n• Includes in-depth analysis of the market from various perspectives through Porter’s five forces analysis\n• Provides insight into the market through Value Chain\n• Market dynamics scenario, along with growth opportunities of the market in the years to come\n• 6-month post-sales analyst support\nCustomization of the Report\n• In case of any Queries or Customization Requirements please connect with our sales team, who will ensure that your requirements are met.\nFrequently Asked Questions\n1 INTRODUCTION OF GLOBAL DEPTH SENSING MARKET\n1.1 Overview of the Market\n1.2 Scope of Report\n2 EXECUTIVE SUMMARY\n3 RESEARCH METHODOLOGY OF VERIFIED MARKET RESEARCH\n3.1 Data Mining\n3.3 Primary Interviews\n3.4 List of Data Sources\n4 GLOBAL DEPTH SENSING MARKET OUTLOOK\n4.2 Market Dynamics\n4.3 Porters Five Force Model\n4.4 Value Chain Analysis\n5 GLOBAL DEPTH SENSING MARKET, BY TYPE\n5.2 Active Depth Sensing\n5.3 Passive Depth Sensing\n6 GLOBAL DEPTH SENSING MARKET, BY TECHNOLOGY\n6.2 Stereo vision\n6.3 Structured Light\n6.4 Time of flight\n7 GLOBAL DEPTH SENSING MARKET, BY COMPONENTS\n7.2 Camera/Lens Module\n8 GLOBAL DEPTH SENSING MARKET, BY GEOGRAPHY\n8.2 North America\n8.3.4 Rest of Europe\n8.4 Asia Pacific\n8.4.4 Rest of Asia Pacific\n8.5 Rest of the World\n8.5.1 Latin America\n8.5.2 Middle East and Africa\n9 GLOBAL DEPTH SENSING MARKET COMPETITIVE LANDSCAPE\n9.2 Company Market Ranking\n9.3 Key Development Strategies\n10 COMPANY PROFILES\n10.1 Texas Instruments (US)\n10.1.2 Financial Performance\n10.1.3 Product Outlook\n10.1.4 Key Developments\n10.2 Infineon Technologies (Germany)\n10.2.2 Financial Performance\n10.2.3 Product Outlook\n10.2.4 Key Developments\n10.3 Qualcom (US)\n10.3.2 Financial Performance\n10.3.3 Product Outlook\n10.3.4 Key Developments\n10.4 Creative (Singapore)\n10.4.2 Financial Performance\n10.4.3 Product Outlook\n10.4.4 Key Developments\n10.5 Occipital (US)\n10.5.2 Financial Performance\n10.5.3 Product Outlook\n10.5.4 Key Developments\n10.6.2 Financial Performance\n10.6.3 Product Outlook\n10.6.4 Key Developments\n10.7 Pmdtechnologies (Germany)\n10.7.2 Financial Performance\n10.7.3 Product Outlook\n10.7.4 Key Developments\n10.8 Sony Depthsensing (Belgium)\n10.8.2 Financial Performance\n10.8.3 Product Outlook\n10.8.4 Key Developments\n10.9 Intel (US)\n10.9.2 Financial Performance\n10.9.3 Product Outlook\n10.9.4 Key Developments\n10.10 Tower Semiconductor (Israel)\n10.10.2 Financial Performance\n10.10.3 Product Outlook\n10.10.4 Key Developments\n11.1 Related Research\nReport Research Methodology\nVerified Market Research uses the latest researching tools to offer accurate data insights. Our experts deliver the best research reports that have revenue generating recommendations. Analysts carry out extensive research using both top-down and bottom up methods. This helps in exploring the market from different dimensions.\nThis additionally supports the market researchers in segmenting different segments of the market for analysing them individually.\nWe appoint data triangulation strategies to explore different areas of the market. This way, we ensure that all our clients get reliable insights associated with the market. Different elements of research methodology appointed by our experts include:\nExploratory data mining\nMarket is filled with data. All the data is collected in raw format that undergoes a strict filtering system to ensure that only the required data is left behind. The leftover data is properly validated and its authenticity (of source) is checked before using it further. We also collect and mix the data from our previous market research reports.\nAll the previous reports are stored in our large in-house data repository. Also, the experts gather reliable information from the paid databases.\nFor understanding the entire market landscape, we need to get details about the past and ongoing trends also. To achieve this, we collect data from different members of the market (distributors and suppliers) along with government websites.\nLast piece of the ‘market research’ puzzle is done by going through the data collected from questionnaires, journals and surveys. VMR analysts also give emphasis to different industry dynamics such as market drivers, restraints and monetary trends. As a result, the final set of collected data is a combination of different forms of raw statistics. All of this data is carved into usable information by putting it through authentication procedures and by using best in-class cross-validation techniques.\nData Collection Matrix\n|Perspective||Primary Research||Secondary Research|\nEconometrics and data visualization model\nOur analysts offer market evaluations and forecasts using the industry-first simulation models. They utilize the BI-enabled dashboard to deliver real-time market statistics. With the help of embedded analytics, the clients can get details associated with brand analysis. They can also use the online reporting software to understand the different key performance indicators.\nAll the research models are customized to the prerequisites shared by the global clients.\nThe collected data includes market dynamics, technology landscape, application development and pricing trends. All of this is fed to the research model which then churns out the relevant data for market study.\nOur market research experts offer both short-term (econometric models) and long-term analysis (technology market model) of the market in the same report. This way, the clients can achieve all their goals along with jumping on the emerging opportunities. Technological advancements, new product launches and money flow of the market is compared in different cases to showcase their impacts over the forecasted period.\nAnalysts use correlation, regression and time series analysis to deliver reliable business insights. Our experienced team of professionals diffuse the technology landscape, regulatory frameworks, economic outlook and business principles to share the details of external factors on the market under investigation.\nDifferent demographics are analyzed individually to give appropriate details about the market. After this, all the region-wise data is joined together to serve the clients with glo-cal perspective. We ensure that all the data is accurate and all the actionable recommendations can be achieved in record time. We work with our clients in every step of the work, from exploring the market to implementing business plans. We largely focus on the following parameters for forecasting about the market under lens:\n- Market drivers and restraints, along with their current and expected impact\n- Raw material scenario and supply v/s price trends\n- Regulatory scenario and expected developments\n- Current capacity and expected capacity additions up to 2027\nWe assign different weights to the above parameters. This way, we are empowered to quantify their impact on the market’s momentum. Further, it helps us in delivering the evidence related to market growth rates.\nThe last step of the report making revolves around forecasting of the market. Exhaustive interviews of the industry experts and decision makers of the esteemed organizations are taken to validate the findings of our experts.\nThe assumptions that are made to obtain the statistics and data elements are cross-checked by interviewing managers over F2F discussions as well as over phone calls.\nDifferent members of the market’s value chain such as suppliers, distributors, vendors and end consumers are also approached to deliver an unbiased market picture. All the interviews are conducted across the globe. There is no language barrier due to our experienced and multi-lingual team of professionals. Interviews have the capability to offer critical insights about the market. Current business scenarios and future market expectations escalate the quality of our five-star rated market research reports. Our highly trained team use the primary research with Key Industry Participants (KIPs) for validating the market forecasts:\n- Established market players\n- Raw data suppliers\n- Network participants such as distributors\n- End consumers\nThe aims of doing primary research are:\n- Verifying the collected data in terms of accuracy and reliability.\n- To understand the ongoing market trends and to foresee the future market growth patterns.\nIndustry Analysis Matrix\n|Qualitative analysis||Quantitative analysis|\nSince the COVID-19 virus outbreak in December 2019, the epidemic has spread to nearly every country across the globe with the World Health Organization (WHO) announced coronavirus disease 2019 (COVID-19) as a pandemic. Our research shows that outperformers seek growth in every dimension which is core expansion, geographic, up and down the value chain, and in adjacent spaces.\nThe COVID-19 pandemic has impacted every industry such as Aerospace & Defence, Agriculture, Food & Beverages, Automobile & Transportation, Chemical & Material, Consumer Goods, Retail & eCommerce, Energy & Power, Pharma & Healthcare, Packaging, Construction, Mining & Gases, Electronics & Semiconductor, Banking Financial Services & Insurance,ICT and many more.\nThe population around the globe had restricted themselves going out of their home and edge towards confining themselves to their homes which is impacting all the market negatively or positively.According to the current market situation, the report further assesses the present and future effects of the COVID-19 pandemic on the overall market, giving more reliable and authentic projections\nThe spread of coronavirus has crippled the entire world. Nearly all countries have imposed lockdowns and strict social distancing measures. This has resulted in disruptions of supply chains. The pandemic has changed common systems around the world.\nAs the effect of COVID-19 spreads, the overall market has been impacted by COVID-19 and the growth rate has also been impacted in 2019-2020. Our latest research, perspectives, and insights on the management issues that matter most to the companies and organization about the market, which is leading through the COVID-19 crisis to managing risk and digitizing operations to deliver trusted information and experiences to the decision makers.\nMarket Forecast Related Considerations\n- Impact on each country and various region\n- Change in supply chain related operation\n- Positive and negative scenarios of the market during the ongoing pandemic\n- Impact on various sectors facing the greatest drawbacks are manufacturing, transportation and logistics, and retail and consumer goods', 'Understanding Biometric Security: The Growing Threats and How to Beat Them\nBiometric authentication is convenient, but privacy advocates fear biometric security erodes your privacy.\nPeople prefer biometric security authentication to passwords because PINS and passwords readily get hacked and are challenging to remember. While passwords are the current “what you know” method, your physical characteristics are ""what you are.""\nAnd there\'s only one you. (Read New Advances in Biometrics: A More Secure Password.)\nBut, picture this: a four-year-old child noticed that Amazon dropped gifts on their doorstep after her mother swiped her pinkie on the iPad\'s touchpad. So, the child used her sleeping mother\'s pinkie to unlock the device and, going to Amazon.com, one-clicked that beautiful pink bike.\nTrue story. Incidents like that happen all the time. (Read How Passive Biometrics Can Help in IT Data Security.)\nGartner, a leading research and advisory company, claims certain physical and behavioral characteristics, like your facial features or the way you type, are more secure than your password.\nIn contrast, critics count millions of data breaches and they\'re growing every day.\nWhat is Biometric Security and How Is It Used?\nOver the last decade, scientists unleashed various biometric verification identifiers to dramatically improve enterprise security.\nThe most common biometric identifiers are:\nUsed to unlock door panels, devices or computers of approved users, among other user cases.\nMore specifically the iris, sclera or retina, where devices equipped with cameras scan the unique patterns of your eyes.\nFor example, prompt server room doors to swing open automatically when cameras recognize the faces of trusted system administrators.\nMore recently, researchers at the University of Buffalo developed a way that you can use heartbeats for your new pass-code, while, at the same time, a $1,000 pocket-sized scanner hit the market for scanning DNA.\nAccording to a recent Ping Identity survey, 92% of IT and security respondents rated biometric authentication as two of the top five most effective security controls, and 80% said it is effective for protecting data stored in a public cloud.\nAround the same time, a Spiceworks survey reported that 62% of companies are already using biometric authentication, and another 24% plan to deploy it within the next two years.\nHow Reliable is Biometric Authentication?\nThe Amazon-grubbing child is one of scores of incidents that plays havoc with biometrics authentication. Two years ago, on a Qatar Airways flight a woman used her husband’s fingerprint to unlock his phone while he was asleep, to divulge his infidelity.\nFor well over a decade, I have been outspoken against the widespread use of fingerprints and most other forms of biometric authentication as a means for authenticating people – among the serious problems with such schemes are the fact that biometric information is not secret (you leave your fingerprints on everything that you touch, and often show them in pictures, for example).\nYou want to know the cheapest simplest fastest way to crack into your boss’ iPad? Use play-dough.\nAnd look for high-definition photos where your boss high-fives, makes the Vulcan peace sign or raises his hand to ask a question — just like the hacker who recreated a German minister’s fingerprints using photos of her hands in 2014.\nThere are bundles of other tricks that include researchers using voice scanners to impersonate your voice, iris scanners that match your retinas and face scanners that trick facial recognition login with photos from, say, Facebook — even 3D-printed heads.\nAside from that, facial recognition devices can readily be fooled by false positives, such as if your voice is hoarse, you switch hair-styles, you wear sunglasses, or don a mask for Halloween.\nSo, fingerprints, voices and faces are out, but so, too, are heart-beats, DNA, body odors, and eyes. If they get compromised, you can\'t just roll out your eyeball and replace it with another…\nHow Secure is Biometric Authentication Data?\nBiometric authentication is convenient, but privacy advocates fear biometric security erodes your privacy. Companies could easily collect and exploit your data on, say, where and when you typically use your phones.\nHackers could replicate and sell these biometrics for tracking and marketing your behavior and movements. As Robert Capps, VP of Business Development at NuData Security warns, “Once biometric data is stolen and resold on the Dark Web, the risk of inappropriate access to a user’s accounts and identity will persist for that person’s lifetime.”\nHad you been interested, you could have bought the personal data of more than one million citizens of India on WhatsApp for less than $10.\nSo How Can Your Business Make Biometrics Authentication More Secure?\nSusan Rebner, CEO of Cyleron, national security company, said she believes that\'s the next step and something her company\'s working on.\nFor example: devices analyze the way typists slide their fingers across desktops while sliding doors discern the person\'s stride; computers kick up at a person\'s finger impact on the keyboard, while mobiles recognize a user\'s hand tremor when punching numbers, among other items.\nOther methods include speech recognition (used, for example, by USAA’s mobile app) , well as signature verification (used, for instance, by banks on letterheads and other documents).\nAny user behavior that veers from their norms and the device or system locks those users out.\nYou can protect passwords by hashing them into chains of digits and letters. Scientists say you can do the same with biometrics, encrypting them on a secure server.\nIn an interview for Biometric Update, Infinity’s CEO Alfred Chan said their company\'s Quantum-Crypt technology developed hashed solutions for iris, fingerprints, and 2D face modalities, and is now exploring 3D modalities.\nYou can combine biometrics authentication with blockchain technology, or the decentralized ledger, where platforms are open-ended and shared by other participants. (Read Can the Blockchain Be Hacked?)\nThis means, any attempt to modify the data is detected by other users who subscribe to the platform.\nWhile behavioral biometrics seems the most secure by far, analysts warn that the system needs to be regulated for data privacy and security and that the method needs broader testing to screen out false positives or false negatives.\nOn blockchain technology and hashed biometrics, MIT researchers recently showed how hackers could breach the allegedly ""unhackable"" blockchains.\nCertainly, the same goes for cracking your hashed password to retrieve those biometrics.\nWhat About Regulations for Data Privacy?\nEuropeans have the General Data Protection Regulation (GDPR) that gives consumers protection over their personal data — including biometrics. (Read How Cybercriminals Use GDPR as Leverage to Extort Companies.)\nThe U.S., to date, only has a hodgepodge of overlapping and contradictory laws from industry groups and federal as well as local government agencies - and that\'s despite its June 2015 hack of the US Office of Personnel Management where cybercriminals pilfered more than 5.6 million fingerprints of government officials.\nThe Best Solution for a Foolproof Biometrics System\nIf you\'re a business that wants to use biometrics authentication to shield your data, you\'re likely to benefit from this 1-2-3 proactive approach.\nFix #1: Practice simple workplace security\nRegularly educate your staff on the biometrics security system you use and on how to ensure data privacy. You would also want to use strong passwords and store your biometrics in three places at best. Further, keep your operating system and Internet security software current so hackers can\'t crack it.\nFix #2: Use multi-factor security\nFor greater security, use a combo of identifiers, so, for example, add fingerprints to facial recognition, like the new LG V30 smartphone that combines facial and voice recognition with fingerprint scanning. Some security systems also include additional features, such as age, gender, and height, in biometric data to thwart hackers.\nFix #3: Put a human in the loop.\nHumans can dupe facial scanners by wearing a mask or makeup. Add a human to your security checkpoint for ultimate security.\nOh, and by the way...\nYou may want to observe the Illinois 2008 Biometric Information Privacy Act, where a company that collects its employees’ data must notify them on how the data will be used and stored and get their consent. Doing so saves you from privacy lawsuits from employees and customers whose biometric data you store.\nHackers are always going to be one step ahead of you.\nBeat them to the trick by combining passwords with biometrics authentication systems and putting humans in the loop to improve security.\nAlso remember those privacy concerns.\nWhile biometrics authentication technology is not foolproof, you may find it gives you less problems than passwords - as long as you keep on top of the system.']"	['<urn:uuid:57ff2e6d-c6b9-4769-a45c-0243806ed70c>', '<urn:uuid:d9bf65f5-d903-4605-968a-9c3ee852f466>']	factoid	direct	long-search-query	similar-to-document	multi-aspect	expert	2025-05-12T20:11:36.665104	11	72	4411
85	mohs hardness diamond vs knife comparison	On the Mohs hardness scale, a knife has a hardness of 5.5 (similar to glass), while diamond ranks as 15 (10 on the old scale), making diamond significantly harder than a knife.	['Platinum is one of the most sought-after precious metals in the world, as much for its aesthetic appeal as for its durability, which makes it ideal for a number of industrial applications. Although platinum is extremely rare — more so than both gold and silver — it’s used to make around 20 percent of all consumer products either in a component of the product itself or as a part of the manufacturing process.\nThe term “platinum” is sometimes used to refer to all of a group of six metallic elements that appear together on the periodic table and have similar physical and chemical properties. These platinum-group metals, or PGMs, include:\nThe durability of PGMs is one of their most attractive properties. As noble metals, they resist corrosion and tarnishing and can stand up to high temperatures as well as stand the test of time. Because of platinum’s strength, it often serves as a protective coating for other less-sturdy metal surfaces through a process known as electroplating.\nIn the platinum electroplating process, workers attach the substrate — the object that needs to be plated — to a rack and dip it into an electrolyte solution in which platinum salts and ions are dissolved. The platinum connects to a positively charged electrode while the substrate attaches to a negatively charged electrode.\nA DC electric current then flows through the rack to the platinum, oxidizing it, dissolving the platinum ions and depositing the ions onto the substrate. At the end of the process, you’ll have a platinum coating with a thickness of 0.5 to 5 microns.\nThis electroplating process allows equipment and components made of materials like silver or titanium to also have some of the beneficial characteristics of platinum. The durability of industrial platinum plating makes it an ideal choice for applications in a variety of industries, including the automotive, energy, medical and manufacturing sectors.\nWhat Is Durability?\nAll these different industries use platinum plating because it’s highly durable and allows the equipment it coats to operate more efficiently over a longer period of time. But in what ways, exactly, is platinum plating so durable?\nAdding a platinum coating to a softer material will increase its hardness and make it less susceptible to damage such as scratching or denting. You can measure the hardness of a material with the Mohs hardness scale, which ranks materials from one to 10 from softest to hardest. Diamond is a 10 on the scale while plastic is a one.\nPGMs fall toward the high end of the scale. Platinum is a four to 4.5, palladium is a 4.75 and rhodium is a six. Although tungsten, titanium and hardened steel are harder than PGMs, few other metals are. Gold and silver both rank as a 2.5 to a three while tin is just a 1.5.\nAdding a layer of platinum to a base material makes it thicker. This added thickness provides additional protection from damage, increases the quality and strength of the component and helps it last longer because there is more material to protect the substrate.\nYou can adjust the plating process to achieve your desired thickness, but the typical thickness is between 0.5 and five microns. It is possible, however, to achieve a thickness of 10 microns. A micron, also known as a micrometer, is one millionth of a meter. Although this may not seem like a large difference, an additional 0.5 to 10 microns of platinum adds a substantial amount of durability.\nEvery component is subjected to the wear and tear that comes with regular use. As a material comes into contact with other objects, it will eventually wear down due to friction. It will also suffer deterioration caused by abrasion, general material fatigue and other processes.\nAdding an extra layer increases the amount of time that this process takes to cause significant damage. Using a durable material for plating, such as platinum, will increase an object’s lifespan by even more. This saves you money by reducing the amount of maintenance an item needs and the frequency with which you need to replace it.\nCorrosion isn’t just unsightly — it also signals serious damage to equipment, since it is the gradual destruction of a material. Corrosion occurs when a metal interacts with something in its environment that causes it to break down. The most well-known form of corrosion is electrochemical oxidation, which occurs when a metal reacts with an oxidant. Rust is one common example of this.\nBecause corrosion is so destructive, it’s imperative that industrial operations protect their equipment from it as much as possible. This is one of the most valuable uses of platinum plating. It offers excellent corrosion resistance and often acts as a sacrificial barrier that slows or stops rust from forming on the surface below it.\nPrevention of Tarnishing\nAlthough tarnishing is essentially a minor form of corrosion, it’s often identified as a separate issue. Tarnishing reveals itself as a thin film that is black or gray in color on the surface of a material. Silver is especially susceptible to becoming tarnished, but the issue can occur on other metals, too, such as aluminum, brass and copper.\nAlthough tarnishing is common with these metals, platinum is immune to it. For this reason, platinum plating is frequently used to prevent tarnishing, which is more cost-effective than attempting to remove it at afterwards with hand-sanding, abrasive blasting or another method.\nIn some applications, materials need to be able to withstand high temperatures. This is especially critical in industries such as the automotive, aerospace and energy generation sectors, but resistance to heat is also important for many other applications as well.\nAn extremely high temperature can, of course, melt a metal and destroy a component completely. Heating can also cause chemical deterioration known as high-temperature corrosion if a material encounters a hot environment that contains oxidizing compounds. Heat can also alter the properties of a metal. It can cause it to expand, become less hard and become more resistant to the passing of electrical current, which can cause electrical components to not work as intended.\nPlatinum plating can protect products from damage caused by high temperatures. Platinum has a high melting point of 3,224 degrees Fahrenheit, which is greater than other metals including titanium, iron, silver and gold. Because of this high heat resistance, coating an object in platinum can help protect it from heat-related damage.\nIndustrial Applications That Benefit From Platinum Plating’s Durability\nWhen manufacturers need an item to withstand harsh conditions as well as the wear and tear of long-term use, they often turn to platinum. Its durability makes it ideal for a wide range of components in a number of different industries and applications, including some emerging technologies. Electroplating transfers platinum’s valuable properties to other materials as well.\nHere are a few examples of uses for platinum plating that demonstrate its supreme durability.\nThe equipment used to manufacture chemicals and synthetic materials must be able to withstand high temperatures and resist corrosion. Platinum, rhodium and iridium are perfect for this application because of their high melting points and because they remain stable when subjected to heat. They also resist damage caused by the other materials used in these industrial processes.\nOne example is the production of crystals. The procedure for growing crystals requires the crucible to contain molten salts, so the crucible must be highly heat-resistant. The resulting crystals have uses in laser technology, electronics and medicine as well as uses such as drilling, cutting and engraving.\nHigh-quality glass and glass fibers are manufactured via similar processes. The production of the glass used in plasma screens and Liquid Crystal Displays (LCDs) requires the crucible to withstand temperatures of almost 3,000 degrees Fahrenheit. Platinum also won’t react with glass, as some base metal alloys do, which maintains the purity of the glass.\nResearchers are currently looking for ways to efficiently store energy created with solar technology. They’re essentially attempting to create an artificial leaf, or photoelectrochemical cell. Platinum, it seems, will play a central role in these energy storage solutions.\nThese cells use a process called water electrolysis to split water into oxygen and hydrogen, which can then be used as fuel. Platinum is the most commonly used catalyst for this hydrogen evolution reaction. The metal’s durability and ability to function in acidic conditions allow it to consistently perform even over extended use in this application.\nOne group of researchers also found that copper plated with platinum performed better than the more expensive indium tin oxide (ITO) typically used as transparent electrodes in photoelectrolysis cells. In the study, the copper-platinum nanowires retained their conductivity despite bending and maintained high transmittance under all of the circumstances required for the process.\nThermal Barrier Coating for Gas Turbine Engines\nIn both commercial and military gas turbine engines, a thermal barrier coating, or TBC, protects the engine from heat damage and oxidation. This coating improves engine performance and prolongs the life of its components. Platinum plays an important role in forming the bond coat layer of the TBC.\nGas turbines employ platinum because it is non-reactive, resistant to corrosion and able to withstand high temperatures. Using gas to run an engine creates harsh conditions for the components inside of it. The materials used must continue to perform despite these circumstances. If they don’t, the results could be damaging to the equipment, dangerous to workers and cause power outages.\nOne group of researchers added a thin extra layer of platinum to the TBC to address the issue of molten droplets of calcium magnesium aluminum silicate (CMAS) and increase the effectiveness of the barrier.\nImplantable Micro-Wire Multi-Electrode Arrays\nMulti-electrode arrays are medical devices that essentially connect neurons in the brain to electronic circuitry in order to stimulate brain activity. Doctors have used these arrays in the treatment of neuropsychiatric disorders such as Parkinson’s disease, epilepsy and depression.\nFor implantable micro-wire multi-electrode arrays, coating the electrodes within platinum black has been shown to reduce the impedance of electrical signals, leading to the activation of more than twice the neurons of unplated microelectrodes. Using electroplating under ultrasonic agitation, known as “ultrasonic electroplating” or “sonicoplating,” has been shown to increase the durability of platinum black even further.\nThis means that implantable multi-electrode arrays could be more effective and longer-lasting thanks to the use of a specialized kind of platinum plating.\nThe automotive industry frequently uses platinum plating to strengthen parts of vehicles and protect them from corrosion, wear and scratching. The PGM palladium is especially common because of its use in catalytic converters, which change toxic gases and pollutants created by internal combustion engines into less harmful gases. In fact, more than half of all palladium used in manufacturing ends up in catalytic converters.\nPalladium’s hardness and resistance to corrosion and wear help make it the perfect material for use in catalytic converters. It will not oxidize when exposed to air and lasts much longer than other metals would under similar conditions. It also absorbs excess hydrogen in exhaust gas, which prevents the formation of hydrogen sulfide gas.\nMedial Device Coating\nPlating medical devices with platinum can help to increase their durability. This applies both to tools such as medical clamps and to devices implanted into the body, such as those used in hip and knee replacements. These implanted devices were traditionally made of steel, but today they might also be made of titanium or magnesium.\nAny medical device must be able to come into contact with the human body without injury. Because of this, the plating must be smooth. It also needs to prevent the growth of harmful bacteria. Platinum fits these criteria and also increase the strength of a device and helps it to resist corrosion, which is crucial for creating a long-lasting solution to medical conditions.\nCoating medical tools such as clamps and scalpels with platinum can also increase their strength and lifespan, improving their effectiveness and reducing the frequency with which medical facilities need to replace them, potentially lowering the cost of medical treatment.\nPlatinum Coatings from Sharretts Plating\nThese are just a few of the most impressive and exciting examples of how platinum plating improves equipment across various industries. You can find platinum in many other applications across a wide range of sectors.\nIndustry has used platinum group metals for years because of their many valuable attributes — especially their durability. These noble, precious metals have excellent strength and are resistant to wear, corrosion and heat. To attain these advantages for a larger group of materials and applications, today’s industries use platinum plating for both established and emerging devices.\nAt SPC, our experienced scientists and engineers use our perfected plating process to achieve high-quality results that will help your devices perform better and last longer through a variety of conditions and uses. To find out more about how we can help you with your commercial or industrial metal finishing needs, request a free quote through our website.', 'Moh’s Scale of Hardness\nThere are two methods to measure the hardness of materials: scratch hardness and static load indentation hardness. Scratch hardness, also known as Mohs hardness, is a relative hardness and is rather rough.\nIt uses ten natural minerals as standards. The hardness order does not represent the absolute size of a particular mineral’s hardness, but indicates that a mineral of higher hardness order can scratch a mineral of lower order. The hardness of other minerals is determined by comparison with these standard minerals.\nThe unit of Mohs hardness is kilogram-force per square centimeter (kgf/cm²), denoted as [Pa]. It’s a standard for expressing a mineral’s hardness, first proposed in 1824 by German mineralogist Frederich Mohs. The hardness is represented by the depth of the scratch made on the surface of the tested mineral using the scratch method with a pyramid-shaped diamond needle.\nThe hardness scale is as follows: talc 1 (softest), gypsum 2, calcite 3, fluorite 4, apatite 5, orthoclase (also known as feldspar or periclase) 6, quartz 7, topaz 8, corundum 9, diamond 10 (hardest). Mohs hardness is also used to express the hardness of other solid materials.\nFor a more specific method: one would scratch the mineral to be tested against the standard hardness on the Mohs hardness scale to determine the hardness of the tested mineral.\nFor example, if a mineral can scratch calcite and be scratched by fluorite, then the hardness of that mineral is between 3 and 4. Alternatively, one can use a fingernail (hardness 2-2.5), a coin (hardness 3.5), or a small knife (hardness 5.5) to scratch the mineral in order to broadly determine its hardness.\n|Representative Mineral Names||Common Uses||Hardness Scale|\n|Talc, Graphite||Talc is the softest known mineral, commonly used in the form of talc powder.||1|\n|Skin, Natural Arsenic||1.5|\n|Nails, Amber, Ivory||2.5|\n|Gold, Silver, Aluminum||2.5~3|\n|Calcite, Copper, Pearls||Calcite can be used as carving material and industrial raw material.||3|\n|Fluorite (also known as Fluorspar)||Carving, Metallurgy, Building Materials||4|\n|Phosphorite||Phosphorus is an important component of biological cells; it is used as raw material in feed, fertilizer, and chemical production.||5|\n|Glass, Stainless Steel||5.5|\n|Orthoclase, Tanzanite, Pure Titanium||6|\n|Teeth (outer layer of crown)||The main component is hydroxyapatite.||6~7|\n|Soft Jade – Xinjiang Hetian Jade||6~6.5|\n|Pyrite||It is used as raw material for the production of sulfuric acid; gold refining; and can also be used in medicinal purposes.||6.5|\n|Hard Jade – Burmese Jadeite and Jade||6.5~7|\n|Quartz Glass, Amethyst||7|\n|Electric Stone, Zircon||7.5|\n|Quartz||According to the old hardness scale, quartz is rated as 7.||8|\n|Topaz, Chromium, Tungsten Steel||On the old hardness scale, topaz is rated as 8.||9|\n|Moissanite||Synthetic gems are 2.5 times brighter than diamonds and cost 1/10th of the price.||9.5|\n|Corundum||Corundum is rated as 9 on the old hardness scale. Natural gems such as rubies and sapphires are now considered types of corundum, as is the hardness of synthetic sapphire crystals.||12|\n|Diamond||Diamonds are rated as 10 on the old hardness scale, making them the hardest natural gem on earth.||15|\nWhat is Mohs Hardness?\nMohs Hardness is a standard that indicates the hardness of minerals, first proposed in 1824 by German mineralogist Friedrich Mohs. This standard is established by using a pyramid-shaped diamond drill to scratch the surface of a mineral, with the depth of the scratch indicating the hardness.\nThe hardness of a mineral refers to its ability to resist certain external mechanical forces such as scratching, indentation, or grinding. In mineralogy, the hardness often referred to is Mohs hardness, which is the scratch hardness compared to the Mohs hardness scale.\nThe Mohs hardness scale is based on ten minerals of different hardness, divided into ten levels from low to high: 1. Talc; 2. Gypsum; 3. Calcite; 4. Fluorite; 5. Apatite; 6. Orthoclase; 7. Quartz; 8. Topaz; 9. Corundum; 10. Diamond.\nIn use, standard minerals are scratched against minerals of unknown hardness. If the mineral can be scratched by apatite but not by fluorite, its hardness is determined to be between 4 and 5.\nThis method was established and named by German mineralogy professor Friedrich Mohs (1773-1839). However, accurate measurement of mineral hardness still requires a microhardness tester or hardness tester. Mineral hardness is also one of the physical properties of minerals. Minerals with high hardness have been widely used in industrial technology.\nDiamonds, corundum, and other minerals are not only used in industry, but also become precious gemstones. As gemstones, they usually have a high hardness.\nFor example, the hardness of opal is 5.5-6.5, quartz is 6.5-7, sphalerite is 7.5-8. Tsavorite is 8.5, and the hardness of sapphires and rubies is 9, second only to diamonds. People choose high-hardness minerals as gemstones, probably because they are wear-resistant, symbolizing their timeless value!\nAccording to needs, people have also developed a gem hardness scale to identify the mineral hardness of gemstones, from the softest to the hardest minerals: talc, gypsum, calcite, fluorite, apatite, zircon, corundum, silicon carbide, boron carbide, diamond, etc.\nWhen there is no standard hardness mineral, the simplest way to measure hardness is with a fingernail or a small knife. The hardness of a fingernail is 2.5, a copper coin is 3, and glass and a small knife are both 5. Those above 6 are almost all gemstone-like minerals.']	['<urn:uuid:41fe4d44-5451-42b6-b04e-849d99e4665f>', '<urn:uuid:6f5dff6d-ddef-43d0-937c-cbff7f05f328>']	factoid	direct	short-search-query	similar-to-document	comparison	expert	2025-05-12T20:11:36.665104	6	32	3012
86	What makes the robot dog Canine-E special?	Canine-E is a unique robotic pet that features over 1 million possible combinations of sounds, lights and character traits, giving each one its own distinct personality.	['options the most recent and most revolutionary know-how, however robots usually find yourself stealing the present. From cute pet robots to actually to these revolutionizing farming and business, we won’t get sufficient and we’re right here to introduce you to all of them.\nYearly, huge firms like LG and Samsung and small startups showcase the most recent batch of cutting-edge robotics on the client electronics convention in Las Vegas. We’re right here with a heads-up on some thrilling robots which can be making their public debut at CES 2023.\nRobots already play an element in lots of our lives, due to Roomba and even more-experimental makes an attempt like, however CES provides a glimpse of the place the know-how is headed. It is at CES that we first noticed and as creepy actual imitations of individuals, and this yr will not be any completely different with regards to a glance ahead.\nThat is the primary time since 2020 that CNET has been again on the bottom in Vegas in drive, so count on our staffers to be assembly loads of robots on the present ground — hopefully ones that may make them a, or , as we have seen in earlier years.\nAmid all of the noise of the present, it may be onerous to maintain monitor of what is taking place at CES, however don’t fret — we have got you. Contemplate this our working record of all the very best and most fascinating robots at CES 2023.\nCute pet substitute Canine-E\nCanine-E is not simply one other robo-dog. Due to over 1 million potential combos of sounds, lights and character traits, this robotic pet comes with a novel character all of its personal. In case your children are pestering for a canine, however you do not have the time or area,.\nJohn Deere’s robotic planter\nJohn Deere is an everyday exhibitor at CES, and annually the tractor-maker turns up with the most recent and biggest tech to make farming extra sustainable and environment friendly. This yr, it is introduced alongside the ExactShot,, reducing down on the waste related to conventional scattershot fertilization strategies.\nAeo the humanoid helper\nAeo is a robotic— notably in well being and social care settings. It is each sturdy and delicate, permitting it carry out the heavy lifting whereas dealing with delicate items akin to remedy or electronics.\nPyxel the coding pup\nCNET Senior Reporter Bree Fowler met Pyxel the coding canine on the CES present ground. It is a toy designed for teenagers age 8 and up, and you’ll, together with sitting, talking and shaking paws. It might additionally run round and light-weight up in varied colours.\nCES Innovation Award Honorees\nDifferent robots on our radar at this yr’s CES are these which were picked by the Client Know-how Affiliation, which runs CES. Forward of the present, the commerce group handpicks the merchandise it thinks are most forward-thinking and thrilling and crown them CES Innovation Award Honorees. This yr, a bunch of thrilling robots have grabbed their (and our) consideration.\nAmong the many honorees, the one we are able to maybe most think about having in our own residence is the Yardo, a yard care robotic that is extra than simply one other sensible garden mower. With interchangeable heads, it might probably additionally clear snow and blow leaves or particles, making it an all-weather gardening companion.\nDifferent robots within the awards won’t be for the house, however that does not make them any much less intriguing. The Agrist L Robotic can choose harvest-ready peppers with millimeter precision via layered leaves. Why solely inexperienced peppers? We have but to search out out, however once we do, we’ll let .\nHello-Bot, a robotic from Korea Superior Institute of Science & Know-how and Hills Robots is a self-driving information robotic that can be utilized in varied eventualities from conferences to hospitals. It distinguishes itself by the 360-degree omnidirectional stereoscopic hologram on prime of its head, permitting for immersive face-to-face conferences.\nA possible god-send for EV homeowners, Parky is an autonomous EV recharging robotic that may present 50 miles of driving vary. It permits you to park anyplace within the parking zone, not simply the designated charging spots, and Parky will come to you.\nOn the medical finish of the robotics spectrum, Godesign’s BIOT Korean Stem Cell Navigator robotic can administer drug remedies to the affected areas of the physique in a minimally invasive method.\nWithin the Health and Sport Award class is the iVolve Professional, a tennis-training robotic that permits you to prepare alone with out one other participant. Utilizing a mixture of laptop imaginative and prescient and AI, the robotic can transfer across the courtroom to fireplace balls at you from completely different positions.\nWithin the mobility class is Neubie, the most recent in an extended line of supply robots. This one, from South Korea-based Neubility, depends on a multicamera system known as V-Slam as an alternative of lidar sensors, which optimizes it for navigating high-density city environments. It additionally has adorably giant eyes.\nInterested by seeing extra robots? Preserve checking again all through the week as we pound the CES present ground in Las Vegas. We’ll remember to present you all the most recent, cutest and weirdest bots as and once we meet them.']	['<urn:uuid:455947aa-4f22-46ed-9eed-95c8cc4a0212>']	open-ended	direct	concise-and-natural	distant-from-document	single-doc	novice	2025-05-12T20:11:36.665104	7	26	880
87	where do wagner recruits train before syria	Prior to their departure to Syria, new Wagner recruits were trained at the base of the Main Intelligence Directorate (GRU) 10th Special Mission's brigade in Molokino, near the southern Russian city of Krasnodar.	['By Kiril Avramov and Ruslan Trad\nLife comes at you fast.\nOne minute in 2015 Kiril Shadrin was a loyal Russian infantryman; the next, he was a ‘volunteer’ in the Donbass; and then in 2017 he suddenly left for Syria, where he re-emerged as a member of a Syrian paramilitary pro-government organization under the supervision of the Syrian Department of General Intelligence. In conflicts across the world, there are now many Kirils – Russian soldiers occupying a grey zone between serviceman, mercenary and spook.\nRegardless of the Kremlin’s official denial of their existence, the presence of such personnel becomes painfully evident as incidents unfold in the prolonged and bloody Syrian conflict. There, Russia is experimenting with hybrid proxy warfare, blurring the lines between official foreign policy and private groups, Russian action and that of independent parties. It is already including what it has learned in its playbook for gray zone conflict.\n— ISIS Hunters (@ISIS_Hunters) September 18, 2017\nAs the armed conflict in Syria transformed from a full-blown civil war to a brutal proxy war, the main actors involved began to experiment with hybrid approaches to proxy warfare. Russia has taken advantage of the chaos on the ground to test combining official, direct military involvement with opaque private military companies (PMCs). Syria has proven to be an excellent laboratory for testing the operational potential and “deniability” of a mix of official and unofficial efforts.\nAs Russia is a relative newcomer at using PMCs abroad for deniability and force augmentation, the results of this “cooperation” are likely to have a deep impact both in Syria and elsewhere. This experiment has strategic, tactical and legal implications both on the battlefield and abroad, for Russia and for the potential targets of hybridized proxy war.\nShifting Russian attitudes towards PMCs\nWhile mercenary activity has a long history in Russia, the use of modern PMCs is a new and novel development, emerging both from internal political and public pressure, and legal and strategic concerns. Prior to 2014, mercenary activity was suppressed by the various state security organs, due to fears of rivalry between the official security apparatus and the PMCs, the loss of supervision and control over PMC activities, and a possible competition over state funding.\nHowever, the expansionist appetite of Russian foreign policy in the Middle East and military involvement abroad in general, coupled with lobbying pressure from oil and gas sector operators, created a situation where the legal void needed to be filled even by partial means. But another driving factor was the Kremlin’s evolving understanding of PMCs’ flexibility and deployability in the course of hybrid warfare.\nAfter a bill legalizing PMCs failed in 2014, amendments to the federal law concerning conscription and military service rapidly passed in 2016. The new bill struck a tricky balance, legalizing the Russian PMCs’ personnel operating abroad without necessarily settling the legality of the PMCs themselves, as the amendments allow for “participation in activities to maintain or restore international peace and security” (limited to a year) and “suppression of international terrorist activities outside the territory of the Russian Federation.”\nThis shift in the official Russian administration’s attitude towards contractors resulted from several different factors, namely President Vladimir Putin’s desire for more flexible military policy options, domestic economic interests (of both the PMCs themselves and their customers), and the relative ease and efficiency of modernizing private-sector units, compared to Russia’s troubled military modernization project. Providing legal cover for PMCs also gives Russian authorities more latitude to spin information for the public and generally to reduce domestic pressure in crises involving captured soldiers and casualties.\nOut of the post-Soviet chaos …\nIn the past several years dozens of Russian PMCs have popped up, many of them short-lived. When a PMC goes defunct, its personnel often splinter into new groups and proliferate, as in the case of Tigr Top-Rent Security that was established in 2005 and went defunct the very next year.\nIn Ukraine, PMC personnel (a large portion of them ex-military) can be easily traced in groups such as RSB-Group, E.N.O.T Corp. and Moran Security Group. Members of these groups are often veterans of Russian efforts in Yugoslavia, the Caucasus, Iraq and Afghanistan, and they have been spotted not only in Ukraine but also in Syria. These groups, born in the chaos of the post-Soviet transition and early Russian efforts abroad, were the early beginnings of a Russian PMC foreign business outreach.\nThroughout the 2000s, Russian entities such as Tigr Top-Rent Security and Redut-Antiterror have used Iraq, Afghanistan and certain African countries to test the waters of the modern PMC business. Some of the more established PMCs (alongside other Russian militias) used Eastern Ukraine as a training ground before deployment to Syria. Volunteers from the so-called Novorossiya with close connections to Russian PMCs were tested in Ukraine before moving on to Syria.\nResults of these early efforts were mixed, as can been seen in the stories of two of the most notorious PMCs: The Slavonic Corps and Wagner. The so-called Slavonic Corps, a 267-strong unit founded under the auspices of Moran Security Group’s head – Vyacheslav Kalashnikov, a lieutenant colonel in the Federal Security Service (FSB) reserves – was sent to Syria after signing a contract with the government and began to provide security for key Syrian energy sites. As it turned out upon arrival in Syria, the Corps were probably contracted by a local kingpin, possibly on direct orders from Bashar al-Assad’s government, and were used as an offensive force to recapture the very oil fields that they were supposed to guard.\nTheir story – while one of utter fiasco, as they were inadequately equipped, poorly managed and able to execute only a single ill-fated combat operation – still illustrates the dynamics and direction of Russian efforts.\nPMC Wagner and the ISIS Hunters\nWhereas the Slavonic Corps met disaster, Wagner – started by a former lieutenant colonel of the 2nd Spetznaz (Special Forces) Brigade of the Russian Main Intelligence Directorate (GRU) who was working for Moran and then the Corps – began as a small contractor but is now effectively a private army. Wagner makes heavy use of mercenaries and probably has about 2,500 people distributed in different operational units that provide security for key Syrian infrastructure and energy sites.\nPrior to their departure to Syria, the new Wagner recruits were trained at the base of the Main Intelligence Directorate (GRU) 10th Special Mission’s brigade in Molokino, near the southern Russian city of Krasnodar. Publicly, their designation was solely security provision, however as it became evident in 2018, for the past two years, the company has been very actively involved in training, intelligence collection and forward operations on behalf of Assad’s army. On paper, no official links between the Russian forces in Syria and Wagner exist. In essence the Wagner personnel are actively augmenting the Russian troops on the ground in execution of their “special tasks.”\nCase in point would be the successful recapture of Palmyra 2016, where Wagner personnel augmented the advance of regular troops. One might argue that the active engagement of Wagner in the Middle East is suspiciously coincidental with the mass deployment of Russian armed forces in Syria in 2015, thus advancing the idea that Wagner is a thin cover for the special operations units of the regular armed forces.\nIf not an integral part of the armed forces, then certainly the two entities act in concert, where Wagner is carefully curated by elements of GRU and FSB.\nWhile early Russian PMCs began outside of Syria and bear some resemblance to Western contractors, Russia’s efforts in Syria have spawned units tailored to fighting in the Middle East, such as the “ISIS Hunters” that serve as valuable asset on the propaganda and psychological warfare front at home and for designated foreign audiences.\nThe Hunters (a spinoff of circles close to Wagner) were used in Russian propaganda efforts at home and in the West. The unit gained visibility in March 2017, when information on a special unit augmenting the Syrian government forces and with a particular focus on operations against ISIS has appeared in Russian media. It was quickly replicated by media outlets tightly connected with Russian propaganda abroad known to reproduce and spread “controlled leaks” in “target countries,” such as Bulgaria, designated for disinformation offensives.\nAlthough the Hunters strive to portray themselves as a local phenomenon, they never managed to get rid of the halo of a Russian creation, and probably because of this, their popularity does not extend beyond the Russian media and certain Western outlets sympathetic to Russian concerns. However, the ISIS Hunters do have an additional function that is far from being portrayed as “the scourge of ISIS.”\nOne of their main tasks is security provision for the Syrian army’s strategic sites, such as gas and oil fields, overlapping with Euro Polis, a company connected to a close Putin confidant. While the details regarding the Syrian contracts of Euro Polis are protected as a commercial secret, it is clear that the ISIS Hunters are an important part of the company’s activities, as the company is linked business moguls close to Wagner. These in turn are connected directly to the Russian government and personally to Putin.\nA tested model to export\nSyria has proved to be the perfect application of a hybrid military-PMC deployment model, and it is now ready to be exported elsewhere. Russia’s deployments of its own forces abroad operating alongside PMCs are on the rise. This suggests that the hybrid operational concept has moved beyond the experimental phase and is ready for wider export outside Syria, perhaps to Libya, Egypt and possibly Sudan, where there are specific Russian geopolitical and security interests connected to the former Soviet security presence in the Middle East and Africa.\nThe footprint of Russian PMCs in Syria is significant and has demonstrated the capacity of the Kremlin to apply military pressure abroad without officially deploying regular troops. PMCs give the Kremlin a way to limit its exposure to domestic pressures while still applying force abroad. The case of Kiril Shadrin – the soldier turned paramilitary contractor – is illustrative of probably hundreds of Russian citizens and citizens of Central Asian countries who have participated in operations in Syria.\nThe widely-publicized capture of Roman Vasilievich Zabolotny and Grigorii Mihailovich Surkanov by ISIS sheds additional light on the interaction between the Russian Armed Forces and Russian PMCs abroad, where the boundary between public and private becomes blurred in the name of claiming official armed forces operational successes, while reducing the body count via PMCs.\nAs Putin triumphantly announced Russia’s withdrawal from Syria for the third time, it is obvious that Russia has no real intention to abandon the region. Certain elements of the regular Russian forces may now be withdrawing, but Russian PMCs in Syria and other locations in the Middle East and Africa are in for the long haul.\nDr. Kiril Avramov is a post-doctoral fellow at the Intelligence Studies Project at the University of Texas at Austin. He is an Assistant Professor of Political Science at the Department of Political Science and former Vice-Rector of the New Bulgarian University in Sofia and a former Senior Fulbright Visiting Researcher at CREEES, UT Austin, Texas.\nYou can follow him on Twitter: @avramovok.\nRuslan Trad is a freelance journalist and analyst with over ten years’ experience covering and analysis of MENA, Balkans and Turkey regional issues; co-founder of De Re Militari Journal; and author of the book “The Murder of a Revolution” (2017).\nYou can follow him on Twitter: @ruslantrad.\nAll views and opinions expressed in this article are those of the authors, and do not necessarily reflect the opinions or positions of The Defense Post.\nThe Defense Post aims to publish a wide range of high-quality opinion and analysis from a diverse array of people – do you want to send us yours? Click here to submit an Op-Ed.']	['<urn:uuid:262c54c8-6c82-4436-946d-a0ca346ffc8d>']	factoid	direct	short-search-query	distant-from-document	single-doc	novice	2025-05-12T20:11:36.665104	7	33	1982
88	Why is it hard to restore prairies to their original state?	The restoration of prairies has been limited by the degradation of the soil community, particularly the community of mycorrhizal fungi. Without these native mycorrhizal fungi, it's difficult to restore the original diversity that characterized the tallgrass prairie.	['Microbial Feedbacks and Plant Ecology\nDynamics within the soil community can be a major driver of dynamics within the plant community. The presence of a particular plant host changes the composition of the soil community and the change in soil community composition often decreases the growth rate of their host plant. Through this negative feedback on plant growth, the dynamics within the soil community can directly contribute to the maintenance of diversity within plant communities and populations. We have found that these negative feedbacks can result from accumulation of host-specific soil pathogens. However, host-specific shifts in the composition of mutualistic mycorrhizal fungi and of soil bacteria also generate these feedbacks.\nInvasions and Restoration Ecology\nThe original tallgrass prairie that dominated the Midwest was fantastically diverse. Unfortunately, much of this community type has been lost due to agricultural development. Efforts to replant agricultural fields into prairie have met some success, but the original diversity has not been restored. We are evaluating whether the degradation of the soil community, particularly the community of mycorrhizal fungi, limits the success of restorations. We find support for an important role of native mycorrhizal fungi and find that restoration of this native diversity can have cascading benefits for ecosystem functions, including reduced erosion.\nGenetics of AMF\nArbuscular mycorrhizal fungi are thought to reproduce solely through the asexual processes of hyphal growth and production of asexual spores. However, we have found substantial amounts of heritable variation within single populations of these fungi. In culturing these fungi, we have found evidence of an unusual mechanism of inheritance within these fungi in which variable nuclei segregate through dividing and growing hyphae. New combinations of nuclei can then be created through hyphal fusion. We are developing and testing the implications of these genetic processes.\nInvasion and Biogeography\nThere is reason to expect interactions with soil microbes will influence plant colonization of oceanic islands. Plants will likely arrive on new islands before their mycorrhizal symbionts, which have limited dispersal. Therefore, plants that successfully colonize new islands would need to be self-sufficient and not dependent on AMF. These initial plant colonizers would have also had the luxury of leaving their pathogens at home and encountering few novel pathogens on the islands. Introduced plants may have the advantage of mycorrhizal fungi because they are commonly brought with intact soil and may exhibit escape from their enemies. We are studying how both beneficial (mycorrhizal) and harmful (pathogens and oomycetes) microbes dictate the success of plant invasions. Currently, our field work is in the Galapagos Islands, Ecuador, while our overarching interest is global.\nEvolution of Mutualism and Virulence\nSymbiotic services are costly. When cheaters are well integrated with mutualists—such as when the two co-occur in the same segment of a root—cheaters have higher fitness. We use experiments and theoretical models to determine how mutualism is stabilized given this incentive to cheat. Our work addresses how soil resource availability, preferential allocation of host carbon, and the spatial structure of the interaction determine whether symbiont populations are represented by mutualists, cheaters, or mixed communities.\nPlants interact with multiple mutualists and enemies simultaneously. We have found that plant response to one interactant can depend upon the identity of their symbiotic partners. For example, association with some AMF species can increase plant tolerance to insect herbivory while association with other AMF species can increase plant defense. In late successional legumes such as Amorpha canescens simultaneous association with rhizobium and AMF can result in synergistic growth benefits.']	['<urn:uuid:970abbe8-3920-44fb-9dd9-20bfac463b88>']	factoid	direct	concise-and-natural	distant-from-document	single-doc	novice	2025-05-12T20:11:36.665104	11	37	577
89	I'm curious about how robots and computers are getting smarter - what's the key difference between how robots handle touch-based tasks compared to DARPA's brain-like SyNAPSE computers when processing information?	While modern robots are being developed to use touch and pressure sensors to find objects in cluttered environments (like the Georgia Tech robot arm), they still rely on programmed behaviors. In contrast, DARPA's SyNAPSE project aims to create computers that work more like the human brain - capable of handling complex and incomplete data, changing rules, and inferential learning without requiring pre-programmed algorithms.	['NEW YORK — Finding and recognizing objects by touch in your pocket, in the dark, or among items on a cluttered table top are distinctly human skills — ones that have been far beyond the ability of even the most dexterous robotic arms.\nRodney Brooks, a well-known roboticist, likes to demonstrate the difficulty of the challenge for modern robots by reaching into his pocket to find a particular coin.\nNow, a group of roboticists in the Department of Biomedical Engineering at the Georgia Institute of Technology in Atlanta, led by one of Brooks’s former students, has developed a robot arm that moves and finds objects by touch.\nIn a paper published this month in the International Journal of Robotics Research, the Georgia Tech group described a robot arm that was able to reach into a cluttered environment and use ‘‘touch,’’ along with computer vision, to complete exacting tasks.\nThis ability is vital if robots are to leave the world of factory automation and begin to undertake tasks in human environments, such as patient and elder care or rescue missions during emergencies.\n‘‘These environments tend to have clutter,’’ said Charles C. Kemp, the director of the Healthcare Robotics Lab at Georgia Tech and Brooks’s former student. ‘‘In a home, you can have lots of objects on a shelf, and the robot can’t see beyond that first row of objects.’’\nThe development is part of a wide range of advances in the last two years that foretell a world in which robots will move freely in human environments, to be able to work near them and with them.\nFor the safety of workers, industrial robots are kept in metal or glass cages or protected from humans by ‘‘light curtains,’’ which cause the robots to stop if a human approaches.\nThat has begun to change with a new generation of robots from such companies as Rethink Robotics in Boston and Universal Robots in Denmark. They make robot arms that can operate safely near human workers.\nRobots have also been limited by their inability to reach into spaces to pick out an object. They are, in fact, programmed to avoid contact. ‘‘We’re flipping that on its head,’’ Kemp said. ‘‘Let’s say contact with the arm is fine, as long as the forces are low.’’\nThe Georgia Tech researchers have produced a robot arm that can reach and then use software to control its sense of touch, making it possible to find specific objects in a collection or area.\nKemp said the researchers were able to achieve success, both with a robot and with digital simulations, after a small series of attempts and using a simple set of primitive robot behaviors.\nThe robot also has an artificial ‘‘skin’’ that can sense pressure or touch.\nThe researchers built their software for a simulated ‘‘cluttered’’ world. The robot’s arms were designed by Meka Robotics, a San Francisco company. The software is based on the Willow Garage Robot Operating System, or ROS, which is intended to be shared freely.\nThe Georgia researchers have made their software open source, as well, and shared instructions to make and adapt robot skin.\nThe research was financed by the Pentagon’s Defense Advanced Research Projects Agency.', 'Affiliate Disclosure: By buying the products we recommend, you help keep the lights on at MakeUseOf. Read more.\nCameras that tell you what they see. Computer chips that self-destruct. A processor that mimics the human brain’s neocortex. Authentication systems that analyze a user’s “cognitive fingerprint.” Sound like Star Trek? It’s not. It’s just another day at DARPA.\nDefense Advanced Research Projects Agency (DARPA) is one of the most fascinating and secretive parts of the US government. The motto on the front page of the DARPA website is “Creating and Preventing Strategic Surprise,” which perfectly sums up the future research that the group does. Remember the Big Dog? That large, self-balancing robotic dog was funded by DARPA? DARPA was also pretty instrumental in creating the Internet.\nSimulating the Brain: SyNAPSE\nOne of DARPA’s current projects is called “Systems of Neuromorphic Adaptive Plastic Scalable Electronics,” or SyNAPSE. The idea behind the program is to create a computer that works on the same principles as the human brain. Current computer processing is extremely inflexible compared to a brain.\nWhile machine learning has come a long way toward creating a computer that can learn independently, processors are still limited to using algorithms that are created by people.\nIn contrast, the brain can deal with complex and incomplete datasets, changing rules, inferential learning, and all of the other real-world things that make effective machine learning extremely difficult.\nAlready, DARPA has made significant progress by creating “nanometer-scale electronic synaptic components capable of adapting connection strength between two neurons in a manner analogous to that seen in biological systems,” and is now seeking to work with researchers to create more advanced hardware, architecture, simulation tools, and training environments to discover what the capabilities of this sort of system are and how it could be used in the future.\nThis sort of computing could lead to huge increases in computing power, as the brain is an extremely efficient information processor. The system will also be highly scalable, meaning that it could potentially continue to grow in size and power.\nAs you can see from the image above, DARPA predicts an extreme shift in the computing landscape because of this technology.\nNon-Digital Processing: UPSIDE\nIf you know much about computing, you’ll know that computers perform calculations in binary notation — a sequence of ones and zeroes — and that’s at the very core of computation. Right? Right. For the most part.\nDARPA is working on creating analog computers with transistors that can be in a non-one, non-zero state, performing probabilistic computation. If it’s probabilistic, there’s always a chance that it will be incorrect; so why would DARPA try to make less accurate computers?\nIn a word, power. Although processing power has become cheap, creating batteries that run those processors is still inefficient and expensive.\nProbabilistic computation uses probabilistic inference, which could potentially unlock currently impossible processing speeds and power efficiencies for video and image processing, according to a 2012 DARPA news release on Unconventional Processing of Signals for Intelligent Data Exploitation (UPSIDE).\nWith spy satellites and drones being heavily used around the world for military, industrial, and commercial purposes, it only makes sense that DARPA would be looking for ways to make them more efficient.\nUPSIDE is using some pretty high-tech gear. According to the DARPA website, the project aims to create “arrays of physics-based devices (nanoscale oscillators may be one example)” that would adapt to inputs, meaning that they wouldn’t need to be programmed in the sense of the word that we use now — they would just learn what they needed to based on the input they receive.\nSelf-Destructing Computers: VAPR\nThe Vanishing Programmable Resources (VAPR) program seeks to create electronics that can self-destruct when they receive a specific signal or experience certain environmental conditions.\nThe military benefit of a program like this should be obvious — every soldier on the battlefield now carries an impressive amount of extremely powerful electronics that the U.S. government doesn’t want falling into enemy hands. What’s the obvious solution? Self-destruction!\nDARPA has already created some tiny biocompatible electronics that dissolve in water and could be used for medicinal purposes — fighting infections, for example. Dissolvable, biodegradable devices have been created that are used to prevent infections at surgical sites; they dissolve into the body and perform the same function as more typical antibiotics.\nOf course, making an implantable device that dissolves in water and one that turns to dust on command are in totally different magnitudes of difficulty, but IBM was recently awarded a significant contract to find a way to do exactly that.\nBehavioral Analysis: DCAPS\nDetection and Computational Analysis of Psychological Signals (DCAPS) is future research that “aims to develop novel analytical tools to assess psychological status of warfighters in the hopes of improving psychological health awareness and enabling them to seek timely help.”\nIn short, DARPA hopes that an opt-in monitoring program will be able to automatically search for the signs of PTSD or other psychological issues in soldiers after they come home from deployment.\nThe program will monitor text and voice communications, sleeping and eating patterns, social interactions, online behaviors, facial expressions, body posture, and body movement to develop an overall psychological health metric. It won’t give specific diagnoses, but it will identify signals that could be indicative of risk for psychological trauma or health issues.\nSound scary? Fortunately, the plan is to make this a totally voluntary program, and the data will be stored in a highly secure framework; it’ll also be controllable by participants, presumably allowing them to monitor and delete any of their data if they so choose.\n…And a Whole Bunch of Other Cool Things\nThese four programs are just a tiny fraction of the work that DARPA is doing on advanced computing. They’re at the forefront of natural language processing, quantum science, machine learning, and massive data analysis.\nIf you ever want to read about programs that will totally blow your mind, head on over to the I2O (Information Innovation Office) or MTO (Microsystems Technology Office) pages on the DARPA website and check out their projects.\nA few of my personal favorites are Active Authentication, EXCALIBUR, MADCAT, and Mind’s Eye. Check them out! Which DARPA projects do you find most interesting or unusual?']	['<urn:uuid:7c20d555-eae9-414f-8585-0e7313e958d7>', '<urn:uuid:8128cfcc-5649-4246-8a53-202207c70700>']	factoid	with-premise	verbose-and-natural	similar-to-document	comparison	novice	2025-05-12T20:11:36.665104	30	63	1574
90	aquarium water maintenance procedures for fish health equipment cleaning common practices	Water maintenance and equipment cleaning are crucial for fish health. For water maintenance, 50% of water must be renewed every eight days, and parameters like pH (between 6.5-7.5 in freshwater), temperature, and water hardness must be measured weekly. The filter should be cleaned monthly to avoid water contamination. For equipment, cleaning and disinfection stations must be easily accessible to ensure regular use. Disinfectants should be changed promptly if discolored or dirty, even before scheduled changes, as they become inactivated by organic materials. Staff should frequently wash hands, particularly when moving between different fish holding systems, and use accessible hand washing stations with soap and disinfectants.	['There are different aquariums for saltwater and freshwater fish. The first one requires a great experience and dedication, so it´s best to start with one of fresh water. If you´re a beginner on this topic, I recommend getting a fish of easy care such as the Guppy, Molly, Goldfish or Swordtail fish, and once you get greater knowledge about the management of an aquarium, you can choose other more sophisticated species. The amount of fish that can enter in the tank will depend on factors such as the size of both the aquarium and the fish on adult state, the ventilation system and the filter.\nSteps for setting up your aquarium\nBefore you begin, you must choose where you´ll be placing the aquarium. Verify that the base on which you want to place it is perfectly stable, level and can support the weight. Between the base and the aquarium you must put a polyurethane plate to prevent external vibrations on the bottom glass and a possible cracking if not properly established. Remember that water weighs.\n– The aquarium should not be too close to a source of natural light, as it encourages the growth of algae.\n– For the bottom of the aquarium you can use sand or gravel, though it´s important to clean them before use.\nAquarium with gravel\n– Perform these three steps: a) fill the aquarium with water to a quarter of its capacity, b) place the decorations and plants, making sure not to shake the sand, and c) finish filling it with water.\n– To remove chlorine from the water you can use a conditioner or bring it to a boil.\n– Install the filter and ventilation system.\nHow do I insert the fish?\nWhen adding fish avoid drastic temperature changes when passing them from one container to the other. As fish are usually sold in plastic bags, a good system to level the temperature of both containers is to immerse the bag into the aquarium for a while.\nFish in bags\nBefore releasing the new fish in the aquarium is important to feed the rest of those who are in the tank to prevent an assault.\nWhen should I move the fish?\nIt´s a serious mistake to insert the fish in the same day you have put together the tank because they can become ill or die. The initial cycle should be previously performed, which involves the development of beneficial bacteria that are responsible for recycling ammonia and nitrite (from decaying unconsumed food, feces and fish respiration), ultimately transforming them into nitrates.\nThis process takes about 3 to 4 weeks. To do this properly you must seek advise of a professional or fish specialty store. NEVER insert the fish into the aquarium without having done this initial cycle because otherwise they´ll die choking on its waste and feces.\nWhich tools can I use for maintaining an adequate quality of life in my aquarium?\nIt´s one of the most important accessories because its function is to retain and clean the aquarium water, especially feces, food waste and other toxic food for fish health. It should be cleaned monthly to avoid contaminating the water with accumulated dirt. There are a variety of filters and your choice will depend on the aquarium´s size, fish species and budget. I recommend you not saving on the purchase of a good filter because it´s the heart of the aquarium.\nUsed to oxygenate the water and create a perfect circulation of water and oxygen, as well as favoring the temperature´s stability in the tank.\nSome points to note:\n– Special, artificial light for aquariums is preferable to natural light, since the latter unduly favors the development of algae, especially if it´s direct.\n– Lighting should be as uniform as possible and should never increase the water´s temperature.\n– The daylight fluorescent types are perfectly adequate and many aquariums have it included.\n– An average lighting of 10 hours a day is correct.\nThermal regulation in an aquarium must be maintained within acceptable limits depending on the species. When there are sudden changes in temperature, the thermostat regulates so that the fish don´t suffer. Cold water fish don´t need it because they tolerate well the ambient temperature of the house, but tropical fish do need to maintain a temperature around 80.6 °. It´s also important to monitor the temperature with a thermometer (adhered to the aquarium´s outer glass) in order to act immediately in case the thermostat suffers any damage.\nWater and its parameters\nThe water is defined by a set of values which really need to be balanced. The slightest variation of temperature, pH , or any other element affects directly the health of the fish. There are inexpensive products for measurement and control of these parameters.\nSince water features vary according to regions, cities and countries, you should seek advice from a retailer on how to treat the water that you´ll use for your aquarium. 50% of the water must be renewed every eight days in order to keep it clean.\nPH and water hardness\nThe pH should be between 6.5 and 7.5 in freshwater. If you have any doubt about it, the seller can tell you what´s the pH for your fish. Hardness on the other hand indicates the amount of calcium contained in the water. There are reactors for sale that measure and control these two values.\nOxygen reaches the water from the tank through its surface contact with the outside air; the greater the tank´s surface, the higher the oxygen exchange. A second way is through photosynthesis, which is the expulsion of oxygen by plants during the hours they´re exposed to illumination.\nEnvironmental balance is a guarantee for the health of the fish.\n- When buying new fish, you should have them for 20 days in another aquarium with the right conditions before putting them in the main one, this way you can observe them and make sure they don´t have any illness that can infect the rest of the fish. Also, when changing the aquarium´s water we must ensure to keep it with the same temperature as before.\n- To keep the water clean is better to feed the fish several times a day instead of giving a lot of food at once. Food leftovers alter the water´s qualities.\n- To maintain a constant pH we should avoid a very dense vegetation and calcareous rocks (they contain calcium) for they will harden the water and make it alkaline.\n- Get rid of algae and leaves in bad state.\n- Measure weekly the water conditions to keep them in good state: pH, water hardness, nitrite, nitrate and ammonia.', 'May 25 2011\nBiosecurity Practices – Essential Considerations for the Pet Fish Industry: Best Health Practices\nI’ve arrived in Singapore but before I post my thoughts related to this trip lets finish up the biosecurity series by discussing the development of best health practices in the context of implementing/developing a biosecurity plan. TMM\nThere are husbandry practices that promote fish health and well being, and in so doing help support the principles and goals of a biosecurity program. These help assure healthy stock that are free of stress and are in optimal condition to resist infection. We will review a few of the key aspects of a quality fish health management program.\nRoutine and reliable visual assessment of the fish is essential. This may be the first line of defense against disease outbreaks within the tanks. Staff should constantly be scanning the fish populations and looking for signs of outright disease, as well as signs of distress or abnormal behavior, both of which may be the first signs of an impending disease outbreak. Affected animals should be moved to a hospital tank for observation and treatment.\nFish holding systems should be designed so that fish can be easily viewed and captured, yet still have adequate hiding spaces. There should be no edges or objects within the tanks or ponds which might injure the fish. Further, all tanks and life support systems should be easy to maintain and service. If systems are difficult to access and clean then it is likely that staff will tend to avoid the optimal level of maintenance and cleaning.\nStaff should also be encouraged to frequently wash their hands, particularly when moving from work areas in one fish holding system to another. Hand washing stations and footbaths, soap or other appropriate disinfectants, and paper towels should be easily accessible so that they will be used regularly.\nEquipment cleaning and disinfection stations must also be easily accessible to ensure regular use. A schedule for regular changes of disinfectants must also be initiated. Most of the common disinfectants are inactivated by organic materials, so these solutions should be changed promptly if they are discolored or dirty, even if it is prior to the scheduled change.\nEnsure that staff adheres to the principles of isolation and independence of systems. Reducing the number of systems connected to a single central filtration unit, limiting movement of fish from tank to tank, and striving to prevent cross-contamination via water borne, airborne and fomite transmission will enhance the isolation of fish holding systems.\nAnything that might stress fish should be eliminated or minimized. It is the responsibility of staff to be vigilant for any signs of stress in the fish as well as any aspect of the husbandry or system design that might contribute to stress. Fish density, water quality, water flow, and tank/pond sanitation should be continually monitored to assure they remain in optimal ranges or conditions. Fish should be handled as little as possible, and when handled, all precautions must be taken to assure that the fish are minimally stressed and not injured.\nNutrition must not be ignored. Foods should not only provide a balanced diet, but they must be handled and stored appropriately to assure their continued quality. Poor quality, contaminated, or spoiled foods should be discarded. Nutrition-related diseases, whether due to nutrient deficiencies or food contamination, can impact immune function in fish, reducing their resistance to infection, and impacting the success of your biosecurity plan. Care must be taken to assure that all fish are receiving an adequate and complete diet. Items to consider include:\n- · Insufficient quantity or intake of food will lead to starvation, causing poor growth, poor survival, increased susceptibility to disease, and a loss of reproductive capacity.\n- · An imbalanced diet due to the feeding of one particular food type to the exclusion of all others may lead to deficiencies of certain essential nutrients with diminished survival.\n- · Many diets may not meet the needs of certain fish species that have specific nutritional needs or feeding behaviors. Special diets or supplementation of commercial diets may be necessary. Seek guidance from those familiar with husbandry of the species or from the literature.\n- · Be wary of poorly formulated diets. This is a rare problem when good quality, commercially prepared diets are used.\n- · Outdated, spoiled, or improperly stored diets will lose nutritional value, and dietary deficits may occur. These foods should be discarded (Yanong 2001a, Winfree 1992, Roberts 2001).\nDead or sick fish should be promptly culled and examined by trained staff or veterinarians to identify the cause of death or illness so that corrective and preventative measures and treatments can be started. Routine health monitoring of apparently healthy fish may be considered to identify emerging disease issues within a facility before they become a serious problem.\nDuring the daily work routine, staff should address the needs of the most susceptible fish and their holding facilities first, moving to the fish with the highest probability of carrying disease last. Typically this means that the display fish are cared for first, starting with the youngest and moving to the oldest fish. Then staff go to systems holding fish under quarantine. Finally, any hospitalized fish or fish undergoing treatment are attended to. These workers should not go back and work with display fish until the next day. If not possible, that individual should wash well and perhaps change clothes after working with quarantine and hospital fish. Alternatively, facilities may opt to designate one individual that only works with animals under quarantine and treatment and never works with display or holding fish.\nA quality management program (aka quality assurance, quality control) is an integral part of any biosecurity plan. It will help assure that the biosecurity practices that are established for a facility are actually put into place and followed. It is essential that biosecurity practices are used consistently and accurately, and are not just brought out when they are remembered or might help control a problem. The quality program will instill a sense of routine to the biosecurity procedures, and will assure that employees use the procedures correctly. It also provides a means of checking and verifying. Quality management concepts should permeate all aspects of the biosecurity protocols. Quality management is the key to a successful biosecurity program.\nThe basic components of the quality program include written standard operating procedures, training, record keeping, accountability, and audits. All are important, and none can be neglected.\nA biosecurity program is essential to successful fish production and husbandry. The components of the program address exclusion of pathogens, control of pathogens, and good health practices, and assemble to provide a security system that will help minimize the impact of disease on a fish facility. A successful biosecurity program requires commitment by management and staff to follow operational policies and procedures, continually assess those protocols, and modify them as necessary. A good biosecurity program protects the business by protecting the animals and the customers. Finally, remember that each biosecuity program is tailored to each specific facility. Protocols are developed based upon the diseases risks associated with a particular facility or market sector, financial and human resources and facility design limitations. Remember that implementing some biosecurity protocols is always better than doing nothing. It is always more expensive to implement a biosecuirty plan after a major disease event.\nI look forward to your comments. TMM']	['<urn:uuid:ef4907ab-1cb1-4b62-9dcd-a29095706569>', '<urn:uuid:86da5eee-8013-450f-b886-885a19ae4c91>']	open-ended	with-premise	long-search-query	distant-from-document	multi-aspect	expert	2025-05-12T20:11:36.665104	11	105	2346
91	archeological remains solomon gates hatzor	The Upper City of Hatzor contains a gate typical of Solomon's building style, featuring six chambers and two towers. This gate design was also implemented in Megiddo and Gezer, as part of Solomon's building and fortification program. Adjacent to the gate is a casement wall (a double wall with rooms), similar to the wall of Jericho mentioned in biblical accounts.	['The Canaanites who lived in Hatzor during the biblical era were pretty smug. And for good reason: Not only was Hatzor a metropolis comparable in size to the biggest cities in powerful Babylonia and Egypt, but it also towered above the Via Maris – the main trade route utilized in ancient times. They had other reasons for complacency as well, for their military capabilities were formidable and their fortifications daunting. It was obvious that soldiers daring to try an attack would shiver with fear as they anticipated the burning oil, boiling water, spears and arrows that the defenders would throw down from the walls.\nBut as King Jabin of Hatzor watched the Israelites conquer piece after piece of the Promised Land, he began to worry. To make certain that he and his people would never fall into Israelite hands, the king initiated a union that consisted of 10 kingdoms in northern Israel, who “made camp together at the Waters of Merom, to fight against Israel” [Joshua 11:5].\nDespite the consolidation of their forces, Joshua managed to carry out a vastly successful surprise assault on Hatzor. And when it was over, he commanded his soldiers to devastate the once proud city. “So Joshua and his whole army came against them suddenly at the Waters of Merom and attacked them, and the Lord gave them into the hand of Israel… Israel did not burn any of the cities built on their mounds — except Hatzor, which Joshua burned” [Joshua 11:7-13]. After putting this important city to the torch, Joshua could finally settle the land of Israel.\nIn 2005, UNESCO added Tel Hatzor to its list of World Heritage Sites of outstanding universal value, and over the last few years Hatzor has undergone an incredible facelift. Today the tel is a fascinating site with partially restored and reconstructed structures and excellent signs.\nTogether with Megiddo and Gezer, Hatzor was mentioned in Kings (9:15) as part of Solomon’s vast building, and fortification, program. The newly well-protected city now had everything it needed to survive as a settlement: fertile land, lush springs, a major thoroughfare, and hills so high that its soldiers could spot an attacking army long before it reached the city gates. But there was still one glitch: Hatzor’s water sources were located outside the city walls. Enemies who couldn’t charge up the heights, or make it through the massive gates, could simply lay siege to the city and wait until the inhabitants began dying of thirst.\nKing Ahab, ruler of the northern kingdom of Israel, ordered his engineers to find a solution. The result, executed with hammer and chisel in the 9th century B.C.E., was a monumental, sophisticated water system that kept the water supply safe inside in the city. During this period Hatzor doubled in size and became the greatest city in the land of Israel.\nFortifications, the water system, and the loftiness of Hatzor all proved useless when the Assyrians attacked in 732 B.C.E. After the battle, the people of Hatzor were led into exile. “In the time of Pekah King of Israel, Tiglath-Pileser king of Assyria came and took Ijon, Abel Beth Maacah, Janoah, Kedesh and Hatzor. He took Gilead and Galilee, including all the land of Naphtali, and deported the people to Assyria” [2 Kings 15:29].\nWhat remains from Hatzor, which never recovered even a shadow of its former glory, are ruins from the 21 cities that stood here one atop the other. In short – a veritable Disneyland for archeology buffs. Save this article – for if nature is your passion, you might want to wait to visit until next April. The gorgeous, delicate Lortet Iris, no longer visible from Highway 90, will be flowering in all its glory on the slopes across from the tel. Just now, there are large white wild carrots there, instead.\nTel Hatzor features a lookout over the Lower City, which extended all the way to the trees you see to your north. Settled during the Canaanite period, the lower city boasted about 15,000 inhabitants. Among the most important finds uncovered in the lower portion of Hatzor were remains from a Canaanite temple. Some experts believe that the Israelites, who lived in the wilderness for centuries after the Exodus, had few building skills and had to copy from what they saw around them. Thus Canaanite temples like this one, full of similarities with Solomon’s Temple, may have seem to have served as its prototype.\nVisitors enter the Upper City through a gate typical of those also built by King Solomon in Megiddo and Gezer. It had six chambers and two towers. Look for the casement wall (a double wall with rooms) to the left of the gate. It was probably just like the one in Jericho, where Rahab hid Israelite spies and later “let them down by a rope through the window, for the house she lived in was part of the city wall.” [Joshua 2:15].\nOther highlights at Tel Hatzor include the huge, Canaanite palace where two enormous column bases still stand at the entrance. The lower portions of the walls were built of heavy, decorated basalt stones that today lack their ornamentation but give you the basic idea. Above them the walls were made of mudbrick, interwoven with cedar wood: reconstruction illustrates a bit of its former splendor.\nDon’t miss the jewel on Hatzor’s crown: the monumental water system, consisting of a vertical shaft that reached through the earth for 46 meters, a 25-meter long sloping tunnel, and a small pool. Walk all the way down (200 steps) and you will understand the immensity of King Ahab’s project.\nWhen you emerge from the water system, walk over to the Israelite area, which was carefully moved from its original site to permit further excavations. Here you will find a beautifully restored 8th century B.C.E. oil press, one of about 20 similar ones that have been discovered so far. Explore a typical “four-room” Israelite house, which had a central courtyard and rooms on three sides, and a large open structure with two long rows of stone columns. This was a public storehouse, and the pillars held up the mid-portion of the roof.\nA metal “soldier”, visible from a distance, tops the Israelite Tower at the western edge of the tel. Constructed as the Assyrian threat became frightening reality, it was designed to protect this side of the city from invasion. Sadly, it was of no help to Hatzor, whose conquest signaled the beginning of the end of the independent northern kingdom of Israel.\nAviva Bar-Am is the author of seven English-language guides to Israel.\nShmuel Bar-Am is a licensed tour guide who provides private, customized tours in Israel for individuals, families and small groups.']	['<urn:uuid:770a1926-92cd-40e0-a4b8-0f85a8db9a05>']	open-ended	with-premise	short-search-query	distant-from-document	single-doc	expert	2025-05-12T20:11:36.665104	5	60	1122
92	swept wing jet aircraft high altitude aerodynamic handling differences level flight vs manoeuvring	In level flight at high altitude, swept-wing jets operate in a narrow speed range between the maximum operating Mach number (Mmo) and minimum drag speed. During maneuvering, particularly in turns beyond 15 degrees bank, the additional drag could exceed available thrust and lead to speed decay. Additionally, at high altitudes, there is less aerodynamic flight control damping due to thinner air, and the effectiveness of aerodynamic controls is reduced since it depends on indicated airspeed while momentum increases with true airspeed.	"['On my flight bag, I have a sticker that says ""Dash Trash"" in large type and then ""Silly Pilot, Jets are for Hot Tubs!"" It\'s a (now ex-) turboprop driver\'s playful rejoinder to the jet jocks\' standard jab that ""Props are for boats."" The war of words between prop pilots and jet drivers has been mostly waged in jest; just about everybody understands that however cool they look and sound, modern jets aren\'t harder to fly and don\'t confer superhuman status on their pilots. That said, there are some key differences between jets and propeller driven aircraft that anybody making the transition for the first time must keep in mind.\nThe powerplant itself is actually one of the more minor differences, at least when transitioning from a turboprop to a modern jet. Most of today\'s ""jets"" are actually turbofans and little of their thrust is true ""jet thrust"" caused by exhaust gasses being expelled from the tailpipe. Rather, the majority of the thrust is generated by the large fans on the front of the engine, very similarly to how propellers are used in turboprops. The GE90 engine used in the B777, for example, has a 9:1 bypass ratio, meaning that for every pound of air that moves through the engine core and is used in combustion, nine pounds of air are accelerated by the fan blades and bypass the engine core. Similarly, the PW150A turboprop on the Q400 gets 85% of its power from the propeller; the remaining 15% is supplied by jet thrust from the engine core. You could almost say that modern jet engines are really ducted turboprops. The main difference is that turboprops have fewer blades of much larger diameter that are geared down to slower rotational speeds. This makes them more efficient at lower speeds and altitudes but limits high speed potential because the propeller tips encounter supersonic speeds long before the aircraft is supersonic. In turbofans, the duct and fan blades/low pressure compressor cause an area of high pressure that slows incoming air enough that the aircraft can cruise at transonic or even supersonic speeds while providing steady, subsonic airflow to the engines.\nOperationally, the main difference in powerplant operation has traditionally been spool up / spool down time. A piston engine can usually go from idle to full power in very little time. Normally aspirated piston engines just need more fuel/air mixture to accelerate, and this is available instantaneously. Jet and turboprop engines need more compression, which is obtained by adding fuel to the combustion section, causing the turbines to spin faster, which drives the compression section faster, increasing the mass of air available to the engine for increased power. There\'s some lag involved. On the first generation of turbojets, it could take as long as 10 seconds to go from idle to full thrust, which caused several accidents when pilots inexperienced with jet engines found themselves at low altitude and low airspeed with ""unspooled"" engines. Adding another spool to the engines so there were separate low pressure and high pressure compressors helped considerably but lag was still a factor so long as most of the thrust was generated by accelerating hot gasses out the tailpipe. With high bypass turbofans, however, thrust increases as soon as the low pressure spool accelerates. Spool-up time can still be somewhat lengthy - it varies by design - but the engine doesn\'t need to be fully spooled up to produce significant power. Turboprops still hold an edge in providing near-instantaneous power but the difference isn\'t as much as it used to be.\nNow this is not to say that going from turboprops to jets isn\'t a significant transition. It is, but generally for reasons not directly related to the powerplant. Most of the differences are in aircraft systems, high speed/high altitude aerodynamics, and low speed swept-wing characteristics.\nCompared to older turboprops like the King Air, Metroliner, or J31, most jets are more capable, more automated, and designed with greater redundancy. Over the long run this makes life a lot easier on the pilot, but during the transition there are significant systems differences to be learned. Some systems like the hydraulics or pneumatics may be more complex, unfamiliar technologies like fly-by-wire may be incorporated, and the avionics (and the electrical system that powers them) will be more important than anything else in the airplane. This really isn\'t a prop vs jet issue; it\'s an old plane vs new plane issue. Recent turboprop designs like the Q400 and Piaggio Avanti utilize technology to about the same degree as jets of the same vintage.\nLike I said, the aerodynamic limitations inherent to propeller-driven engines mean that they typically operate at lower altitudes and slower airspeeds than jets. Therefore, the transition to jet aircraft also involves learning about high speed and high altitude aerodynamics. Most transport-category jets regularly cruise at speeds of .75 to .85 Mach (75%-85% the speed of sound). This is within the transonic speed range, meaning that while the aircraft itself is below the speed of sound, airflow over some parts of the aircraft will be accelerated to supersonic speeds. This typically isn\'t much of a problem until airflow over the wing reaches supersonic speeds, which is known as Critical Mach or Mcr. At Mcr, a shock wave begins to form on the wing; as speed is increased beyond Mcr, the shock wave intensifies and moves aft, which has several undesirable side effects. The shock wave moves the wing\'s center of pressure aft, which requires increasing amounts of elevator to counteract. Eventually, the elevator may run out of authority and the aircraft will pitch down, which increases airspeed and exacerbates the situation, a phenomenon known as mach tuck. Additionally, the shock wave will cause flow separation which decreases aileron effectiveness and, depending on aircraft configuration, may blanket the horizontal stabilizer and contribute to the tendency towards mach tuck.\nThe problems associated with transonic flight have been well known for 60 years and aircraft designers have a number of weapons in their arsenal to combat them. The use of swept wings is universal among aircraft designed to cruise above .70 Mach; it delays critical mach by artificially increasing the wing\'s fineness ratio, since the relative wind travels a greater distance across the wing than its actual chord. Supercritical wings, which have a relatively flat upper surface and an unusually curved lower surface, decrease the strength of the shock wave that forms on the upper wing at speeds beyond Mcr. Vortex generators add energy to the boundary layer and increase resistance to flow separation in both high speed and low speed flight. Trimable horizontal stabilizers provide the elevator authority necessary to counteract the aftward shift in center of pressure. All these design features can decrease or delay undesirable effects of supersonic flow but they do not eliminate them. For this reason, new designs are thoroughly flight tested and a maximum operating mach limit (Mmo) is established. This is in addition to Vmo, the maximum operating airspeed limit, which provides structural rather than aerodynamic protection. Typically Vmo is limiting below FL250-FL300 and Mmo is limiting above those altitudes.\nIf high-speed aerodynamics were your only worry at high altitude, staying out of harms way would be a snap: don\'t exceed Mmo. However, low-speed aerodynamics also come into play. It\'s somewhat counterintuitive, because you\'re zooming over the ground at 400+ knots, so you\'d think you wouldn\'t have to worry about stall speed. Keep in mind, however, your indicated airspeed will be quite low compared to true airspeed when at high altitudes. Angle of attack varies with indicated airspeed, meaning you\'re much closer to a stalling AoA in high-altitude cruise. If you plot maximum and minimum airspeed versus altitude on a chart, you\'ll see the two lines eventually merge as altitude increases. The area where there is little margin between high speed and low speed danger zones is colloquially known as coffin corner. Keep in mind that the danger of stalling comes not from low airspeed but high angle of attack; therefore, an altitude that\'s perfectly safe for level flight at a light weight in smooth, cold air could be dangerous for maneuvering, at a heavier weight, in turbulence, or at higher temperature. You can avoid coffin corner by using your aircraft\'s maximum cruise altitude performance charts: they\'ll show you the highest altitude you should cruise at given a certain aircraft weight and air temperature.\nOne final consideration is the handling characteristics of a swept-wing jet. Sweeping a wing introduces significant spanwise flow at low speeds, making the wing tips susceptible to stalling before the roots. This condition is unacceptable for a few reasons ranging from loss of lateral control to the forward shift in center of lift which causes a further pitching up moment. Aircraft designers use a few tricks like washout, cuffs, or vortex generators to keep the tips from stalling first, but the fact remains that swept-wing aircraft seldom stall as docilely as straight-wing airplanes do. The solution is to never allow the aircraft to stall, and to this end most swept-wing planes have stall protection devices like stick shakers and stick pushers. That said, you don\'t need to stall a swept-wing jet to get in a pickle; getting slow at low altitude will do nicely, as the spanwise flow also guarantees rapidly increasing induced drag below Max-L/D. When jets are lost in landing accidents, it is often due to an unarrested low-speed sink that results in impact short of the runway.\nLastly, some swept wing aircraft are susceptible to ""dutch roll,"" a phenomenon of simultaneous uncoupled roll and yaw oscillations that progressively increase in magnitude. It\'s most prevalent in aircraft with strong lateral stability; swept wings are inherently stable laterally. Basically a sideslip causes extra lift on the side of the slip, resulting in a rolling moment in the other direction at the time that directional stability is causing yawing moment in the original direction of the sideslip. If the directional stability is weak compared to lateral stability, the yawing moment lags slightly behind the rolling moment, allowing an opposite sideslip to develop, and the process repeats again, feeding on itself. In extreme cases it can render an aircraft uncontrollable; susceptible designs prevent it by installing a yaw damper. If your yaw damper can be deferred (MEL\'d), your aircraft probably isn\'t extremely prone to dutch roll, but you\'ll want to make a mental note to stay coordinated with an inop yaw damper rather than flying with your feet on the floor like usual!\nNone of this is very hard stuff to grasp and the procedures for dealing with differences between props and jets are pretty transparent given all the other things you have to learn when transitioning to a new aircraft. I personally found the transition from prop to jet very easy, at least in the simulator training phase. I\'ll fly the real thing for the first time in a few days when IOE starts.', 'A large number of modern jet aircraft, of all sizes and including Very Light Jets (VLJs)s, routinely cruise at high altitudes.\nThe record of Accidents and Serious Incidents which have accompanied this increase in high altitude flight has suggested that pilot understanding of the aerodynamic principles which apply to safe high-altitude flight may not always have been sufficient. This applies particularly to attempts to recover from an unexpected loss of control. The subject is introduced in this article and covered in comprehensive detail in the references provided.\nFrom a practical point of view, ‘high altitude’ operations are taken to be those above FL250, which is the altitude at above which aircraft certification requires that a passenger cabin overhead panel oxygen mask drop-down system has to be installed. Above this altitude a number of features begin to take on progressively more significance as altitude continues to increase:\n- There is a continued reduction in the range of airspeed over which an aircraft remains controllable;\n- True airspeed (TAS) (and therefore aircraft momentum) increases with altitude. However, the effectiveness of the aerodynamic controls and natural aerodynamic damping are both dependant upon indicated airspeed (IAS) and remain largely unchanged. Therefore, the ability of the aerodynamic flight controls to influence flight path or to recover from an upset is progressively reduced as altitude increases;\n- In the event of depressurisation, the time of useful consciousness for occupants deprived of oxygen reduces dramatically - see the separate articles on Loss of Cabin Pressurisation, and Hypoxia.\n- At very high altitude, occupants are exposed to slightly increased cosmic radiation. This is covered by the separate article ""Cosmic Radiation"".\nThis article focuses on aerodynamics and aircraft handling.\nTotal Drag and Going Too Slowly\nTotal Drag Curve\nThe key to an understanding of the practical implications of high altitude flight is an understanding of the Total Drag curve and the relationship between its two primary components, Induced Drag and Parasitic Drag. Induced drag is directly related to lift production and is greatest at low speeds and high angle of attack. Conversely, parasitic drag increases in proportion to the square of the aircraft speed. Total drag, at any given speed, is the sum of its two components and, as can be demonstrated graphically, its minimum value occurs where the induced drag and parasitic drag curves intersect. The speed corresponding to the point of minimum total drag is known as the minimum drag speed or Vimd (sometimes Vmd) and will vary as a function of aircraft weight. Obviously, level flight at speeds greater than or less than Vimd is possible.\nTo fly slower than Vimd, a greater angle of attack is necessary and an increase in thrust is required to compensate for the increase in induced drag caused by the increased angle of attack. Angle of attack can be increased to the point that there is insufficient thrust available to maintain level flight or until reaching the wing\'s stall angle of attack, whichever occurs first. Conversely, increasing speed above Vimd requires a reduction in the angle of attack to maintain level flight. Additional thrust will be required to offset the increase in parasitic drag produced due to the additional speed that is required to enable the airfoil to generate the equivalent amount of lift at a lower angle of attack. At speeds above Vimd, the aeroplane is in stable flight whereas at speeds below Vimd, an aeroplane is in unstable flight.\nThe question of stability is illustrated by the effect of any encounter with air turbulence. If this occurs when an aircraft is in the stable flight regime and the power/ thrust setting is not altered, it will result in increased drag and reduced aircraft airspeed; this reduces drag so that airspeed eventually returns to the previous value. If an aircraft is in the unstable fight regime, a similar disturbance would cause a decrease in airspeed and so increased drag; this would result in a further decrease in airspeed unless the power/thrust setting were increased; the lower speeds would mean increased drag which would result in a further decrease in airspeed. This is referred to as being on the ""back side of the drag curve"". Since the applicable True Airspeed for a ‘low speed’ stall increases as altitude increases, and the reference speed for higher altitude flight is Mach Number rather than Indicated Airspeed, the minimum cruise speed as altitude increases begins to approach the Mmo (the maximum operating Mach Number).\nMmo and Risk of Mach Stall\nCertification of aircraft types includes the setting of Mmo. This is based upon setting a suitable margin from the Critical Mach Number (Mcrit), at which airflow over a wing becomes transonic, that is, reaches the local speed of sound, and forms shock waves. These shock waves induce Wave Drag and disturb previously smooth airflow leading to a loss of lift and. potentially, to Mach stall. The margin between this upper limit and the prevailing TAS for a low speed stall, which increases as altitude increases and air density decreases, narrows with an increase in altitude resulting in a flight regime often referred to as Coffin Corner. It also means that the normal cruise speed at high altitude will be nearer to Mmo. Any exceedence of Mmo at high altitude will bring the aircraft closer to the critical Mach Number and to the risk of a Mach stall.\nVariation in Cruise Speed\nSlower cruising speeds are often used as a means to save fuel, but this will mean routinely flying closer to the minimum drag speed (Vimd); this gives less time to recognise and respond to any speed loss and eventual risk of a stalled wing condition.\nSmall changes in either ‘External Factors’, such as variable winds, increased drag in turns, turbulence from any source, ice accretion or ‘Internal Factors’ such as use of anti-icing, un-commanded thrust rollback or engine malfunction can lead to loss of airspeed. Heavily damped autothrottles, designed for passenger comfort, may not always apply thrust aggressively enough to prevent a slowdown which places the aircraft on the back side of the drag curve. Close monitoring is essential.\nOptimum Cruise Altitude\nThe optimum cruise altitude is that at which a given thrust setting results in the corresponding maximum range speed. The optimum altitude is not constant and changes over the period of a long flight as atmospheric conditions and the weight of the aircraft change. A large change in temperature will significantly alter the optimum altitude with a decrease in temperature corresponding to an increase in altitude. At the optimum altitude, operating costs will be minimum when operating in the most economical (ECON) mode; it is also the cruise altitude for minimum fuel burn when in the Long Range Cruise (LRC) mode. In both cases, optimum altitude increases with reducing aircraft weight. If the aircraft is at its maximum certified level or the altitude is operationally capped, speed reduction as weight decreases will help to maintain a minimum fuel burn profile.\nMaximum operating altitude is determined by reference to three basic characteristics which are unique to each aircraft type. It is the lowest of:\n- Maximum Certified Altitude as stated in the Aircraft Flight Manual (AFM) (This is usually structural and is most often defined by pressurisation load limits on the fuselage. However, a component maximum operation envelope may impose lower limits, especially when dispatching under Minimum Equipment List (MEL) relief).\n- Thrust Limited Altitude at the prevailing aircraft operating weight and environmental conditions - the altitude at which sufficient thrust is available to provide a specific minimum rate of climb (this is the usual controlling limit especially when turning and available thrust may be very small).\n- Buffet limited maximum altitude at the prevailing aircraft operating weight and environmental conditions - the altitude at which a 1.3g loading from turning, manoeuvring or turbulence may be experienced without encountering buffet associated with either low speed stall or Mach stall (low speed pre-stall buffet occurs at increasingly high IAS as altitude increases whereas the pre-Mach stall buffet occurs at a decreasing IAS so that the margin between the two is progressively reduced with increase in altitude).\nMass and Balance Effects on Handling Characteristics\nFor conventional airplanes, a C of G towards the aft limit of the mass and balance envelope means less longitudinal stability whereas an aircraft with a C of G near the forward limit means greater longitudinal stability. Since an airplane is dependent on the elevator to provide pitch control, the forward C of G limit occurs at a point where the increase in stability will not exceed the ability of the elevator to provide this control. If the C of G moves forward, additional force is required on the elevator to raise the nose up causing the stall speed to increase. The relative longitudinal instability which comes when the C of G is near the aft limit means that the inherent susceptibility to loss of control is greater. Less effort is required by the tailplane to counteract the nose down pitch moment of the wing and this results in less induced drag on the entire aircraft and thus maximises efficient flight.\nThe wing can be stalled at any airspeed, true or indicated, and at any altitude, and aircraft attitude has no absolute relationship to the onset of an aerodynamic stall. If the wing angle of attack exceeds the stalling angle of attack, the wing will stall. Successful recovery from a full stall often involves a very different technique to that required for the recovery from the approach to one. Training for recovery from an incipient or near-stalled condition has in the past often emphasised minimum altitude loss as a goal by focusing on the application of a rapid increase in thrust without consideration of the overiding imperative to reduce aerofoil angle of attack to ensure that a fully stalled condition is avoided. For the less frequently trained fully stalled condition, the overriding imperative is to un-stall the wing by reducing the angle of attack and this is likely to involve a reduction in aircraft pitch well into the negative with the concomitant loss of altitude that this implies. Engine thrust may be significantly lost during an aerodynamic full stall of the wing for as long as the air intake to the engines is disrupted by the high angle of attack which is implied by the condition. In all cases, it is vital that pilots understand the differences between the actions required to recover from an incipient stall and those required to recover from a full stall and that they are able to apply them correctly.\nHigh Altitude Handling Considerations\n- Stay alert! The high altitude environment in not a place for complacency. The available flight envelope between low and high speed buffet may be quite limited and manoeuvering induced loads or external forces such as windshear or turbulence can result in an exceedence or, potentially, an upset.\n- Avoid flight at speeds at or below minimum drag speed (Vimd). Speeds below Vimd are referred to as on the ""back side of the drag curve"" and are inherrently unstable. In this regime, an external force, such as turbulence, which causes a speed reduction will result in further speed decay if additional thrust is not immediately applied. If not immediately arrested, the speed decay will continue and could result in stall or the necessity to initiate a descent to regain airspeed.\n- Use of V/S mode in climb can lead to a critical loss of airspeed and potentially to a stall. Be aware of the climb capabilities of the aircraft - trying to comply with a restrictive Air Traffic clearance such as ""be level in two minutes"" could also result in a critical reduction in airspeed. This is because in V/S mode, the vertical speed selected has priority over speed. If too high a rate of climb is selected, when the aircraft thrust limit (usually CLIMB - CL, CL1 or CL2) is reached, rate of climb will continue to take priority and speed will decay as the autopilot attempts to sustain climb rate, running the risk of a stall.\n- Monitor the outside air temperature (OAT). Optimum altitude is reduced by an increase in OAT. If an increase in temperature is encountered while in level flight, buffet margins will be reduced and a descent may be necessary.\n- Be aware that although an aft centre of gravity improves fuel economy, it also reduces aircraft longitudinal stability and increases susceptibility to upset.\n- Be smooth. When assuming manual control at high altitude be aware that there is less aerodynamic flight control damping due to the thinner air. Avoid over controlling as it could potentially lead to an upset. Avoid bank angles beyond 15 degrees as the additional drag could exceed the thrust available and lead to a speed decay. Conversely, be aware of the fact that in a confirmed upset or stall situation, full control deflection may be required to regain control of the aircraft.\n- Understand and be able to recognise the difference between an incipient (approaching) stall and a full stall.\n- Know the difference in the actions required to recover from an incipient stall as compared to those required to recover from a full stall.\n- Recognise that, in the event of a full stall, stall recovery is the priority. Altitude recovery is secondary and can only be achieved after successful stall recovery.\n- ^ Whenever a limiting speed is expressed in terms of Mach number, it is expressed as an ""M speed"", e.g. Mmo is maximum operating limit Mach number whereas Vmo is maximum operating limit speed in knots,.']"	['<urn:uuid:7fd2c73c-a74a-49c8-92a0-d57dfe3eb818>', '<urn:uuid:f252cc77-1259-4f26-828e-4fee14e6bf34>']	factoid	direct	long-search-query	similar-to-document	multi-aspect	expert	2025-05-12T20:11:36.665104	13	81	4095
93	What happens if a trucking company forgets to respond to a job offer?	If the carrier does not respond by the specified 'Must Respond By Date' and 'Must Respond By Time', the load is subject to be reassigned to another carrier.	['What is EDI 204 Motor Carrier Load Tender?\nThe EDI X12 204 Motor Carrier Load Tender is an electronic document originating from a paper Motor Carrier Load Tender. The EDI 204 transaction set is used to communicate an offer for a shipment to a full load motor carrier.\nThe EDI X12 204 Tender is typically sent from a retailer, a manufacturer, or a distribution center to the carrier. However, EDI 204 transaction could be sent by any company that is interested in the delivery service.\nThe 204 can be used to create a new shipment or to update, replace, or cancel a shipment.\nGenerally, companies cannot use an EDI X12 204 Motor Carrier Load Tender to transfer information related to a less-than-truckload bill of lading, manifest, or pick-up notification.\nFull Truck Load or FTL is a type of transportation that takes up at least one full cargo trailer or does not allow other loads to be carried at the same time.\nThe carrier is the party responsible for transporting the goods or freight. Generally, the carrier owns and manages transport assets. Alternative names for the carrier are: transportation provider, or 2PL (Second-Party Logistics).\nTypical workflow of an EDI 204 Document\nThe EDI 204 transaction is partnered with additional EDI transaction, the Load Tender Acknowledgement – EDI 990, and the EDI 214 -Shipment Status Message.\n1. An EDI 204 is sent to the carrier by the shipper.\n204 transaction set contains critical shipment information including load reference identification, window required for shipping, pickup, drop destination, equipment requirements, and, commodities.\n2. In return, the transportation provider responds with an EDI 990 – Load Tender Acknowledgement to inform acceptance or rejection of the shipment order.\nUnless agreed to beforehand with the shipper, a response with EDI 990 to accept or decline the load tender is required for each EDI 204. The response time for EDI 990 sent within the EDI 204 is specified as: “Must Respond By Date” and “Must Respond By Time”. If the EDI 990 is not received by indicated in EDI 204 time the load is subject to be reassigned to another carrier.\nOften, timing is critical for an EDI 990 document. For example, Amazon.com requires that EDI 990, which accepts the load request, must be generated within 90 minutes after Amazon.com sends the 204 transaction.\nCertain companies could make some exclusion, for example, Amazon.com could allow EDI 214 document as acceptance of the load if the carrier cannot generate an EDI 990, but the carrier has to inform about this, as it will require Amazon’s approval.\n3. In case of acceptance, the carrier sends back to the trading partner a Transportation Carrier Shipment Status Message- EDI 214\nEDI 214 will provide specific information about the shipment including pro-number, shipment codes, date and time of shipment/delivery.\n4. After delivery is complete, the carrier submits to the shipper an EDI 210 transaction set – Motor Carrier Freight Details and Invoice\nAn EDI 210 transaction is used to provide the full information about the cost of services provided by the transportation provider. This transaction set used both as an invoice for a payment request and as information relating to the freight charges.\n5. Once the EDI 210 document is received by the shipper, an EDI 997 Functional Acknowledgement is sent back from the shipper to the carrier to indicating that the 210 Motor Carrier Freight Details and Invoice transaction, was received.\nMany companies integrate EDI 210 transaction set into their ERP, business or accounting software applications. EDI2XML can deliver quick and efficient EDI integration. We provide certified connectors for leading IT systems such as SAP, Salesforce, JD Edwards, Dynamics CRM, Dynamics AX, and more. Contact us for a free consultation and for demo.\nThe Structure of an Electronic Document\nAn EDI transaction set contains one or more “envelopes” that identify the sender and recipient of the document.\nThe ISA segment denotes the beginning and the IEA end of the envelope. In an envelope, the transaction sets are formed into one or more functional groups that are limited to the GS (Functional Group Header) and GE (Functional Group Trailer) segments. The figure below shows the format of an EDI transmission with several tender transactions.\nEDI 204 Motor Carrier Load Tender Transaction Set Sample\nThe EDI 204 transaction set is composited from functional groups that describe the content of the deal. The following example details the various data elements and segments of EDI 204\nBelow is a sample of EDI 204 document.\nIf you need to integrate X12 EDI 204 (Motor Carrier Load Tender) transaction set into your existing flows, by converting the X12 document to XML (and vice versa) you can use our REST HTTP EDI Web Service (API). It is a cost-effective solution meant for companies who got their own technical resources to work with REST API. It easy and simple to connect, there is no contract, moreover, you can set up a 15 days free trial account, without any commitment from you.\nANSI ASC X12 EDI Transactions Set for Motor Transportation Logistics\nFollowing, is the list of the most frequently used EDI documents in the Transportation and logistics vertical, typically exchanged between the shipper, the transportation provider, and the consignee:\n- EDI 204 – Motor Carrier Load Tender\n- EDI 210 – Motor Carrier Freight Details and Invoice\n- EDI 211 – Motor Carrier Bill of Lading\n- EDI 212 – Motor Carrier Delivery Trailer Manifest\n- EDI 214 – Transportation Carrier Shipment Status Message\n- EDI 990 – Response to a Load Tender\n- EDI 997 – Functional Acknowledgment\n- EDI 106 – Motor Carrier Rate Proposal\n- EDI 107 – Request for Motor Carrier Rate Proposal\n- EDI 108 – Response to a Motor Carrier Rate Proposal\n- EDI 213 – Motor Carrier Shipment Status Inquiry\n- EDI 215 – Motor Carrier Pick-up Manifest\n- EDI 216 – Motor Carrier Shipment Pick-up Notification\n- EDI 217 – Motor Carrier Loading and Route Guide\n- EDI 218 – Motor Carrier Tariff Information\n- EDI 240 – Motor Carrier Package Status\n- EDI 250 – Purchase Order Shipment Management Document\n- EDI 251 – Pricing Support\n- EDI 601 – US Customs Export Shipment Information\n- EDI 602 – Transportation Services Tender\n- EDI 753 – Request for Routing Instruction\n- EDI 754 – Routing Instructions\n- EDI 920 – Loss or Damage Claim – General Commodities\nBenefits of using EDI in the transportation industry.\nIn recent years, haulage and transportation industry faced increasing pressure to shorten delivery times, but with EDI, the carriage is significantly easier and faster to manage. It’s no secret that logistics necessitates a lot of documentation and EDI allowed automated flows between all logistics partners. In fact, there are many benefits of EDI for transportation providers:\n- No need for manual processing from the dispatch operational and billing system, therefore saving time and money.\n- Eliminating any costly data entry mistakes\n- Employees have more time to focus on more productive tasks and provide better customer service.\n- Ability to communicate electronically and seamlessly, with all parties in the supply chain process.\n- Integrating EDI with transportation management systems or direct carrier systems will help to tender loads\nFind the best EDI solution\nAre you looking to exchange EDI 204 document with one of your business partners? Do you need a cost-effective solution? Or maybe you would like to integrate your EDI 204 transaction set directly into your ERP CRM or TMS system? We provide flexible and reliable EDI solutions to help make your businesses more profitable.\nRead this blog to compare key features of our Fully managed EDI service versus our REST HTTP EDI Web Service, to find out which is better for your business.\nPlease contact us to discuss your EDI challenges with one of our EDI experts.']	['<urn:uuid:58591865-e53d-4e1b-a9ac-b4ddc6dacf96>']	factoid	with-premise	concise-and-natural	distant-from-document	single-doc	novice	2025-05-12T20:11:36.665104	13	28	1304
94	What is required before contacting a potential internship site?	Students must be cleared by the internship coordinator before contacting a potential internship site.	['Welcome! The following is a brief description of how and why we do internships. Please keep in mind that there is more to learn once your student is enrolled:\nAt the Met, we work with students to help them explore their interests through “real-world” experiences. A key part of their school experience is to go on shadow days at local organizations and businesses. A shadow day is a one day visit to learn about the profession. If the shadow day is a match for the student and mentor (host), an internship is secured.\nSince 2003 our students have gone on thousands of shadow days and have had thousands of internships. During these experiences, they are fully covered by the Sacramento City Unified School District’s insurance policy.\nEach student requests shadow day sites based on individual interests. By learning first-hand from an adult who has the same interest and is engaged in a related career, the student has the opportunity to experience learning in a meaningful way. Students will arrive at the shadow site prepared with questions or ideas of what they’d like to learn that day.\nAll career pathways are offered at the Met. While many times students stay on the same pathway throughout their high school career, many decide to explore other pathways. Typically, a student will have one internship per year, sometimes two. As for *shadow days, students on average explore eight sites per year. That is a total of up to 40 careers a Met student would have possibly explored through hands-on experience by the end of his/her academic career.\nIt is important to note that with the privilege of being trusted to attend an internship every Tuesday and Thursday comes great responsibility. All students are required to go through the proper channels when interested in contacting a site for a shadow day.\nCONTACTING A PROSPECTIVE INTERNSHIP SITE\nStudents are allowed to contact a potential internship site WHEN CLEARED by the internship coordinator. We have students currently pursuing sites, as well as students who already have an internship. These sites are NOT to be contacted, because they are TAKEN. In addition, the Met keeps a log of all sites who do not want to be contacted. For these reasons, and others, we cannot stress enough that current students and prospective students not contact sites without permission. This includes parents and legal guardians.\nSEARCHING FOR AN INTERNSHIP\nStudents are provided with a database of over 3,000 sites, some of which are current internships, sites that offer shadow days, and also sites that do not want an intern or that are closed. We find that it is important to keep track of those who do not want us to make contact, just as much as those who do. The Met does not find your student an internship. The student is provided with tools and support to find a shadow day, and potentially secure an internship. The student is in the “driver’s seat” so to speak. Most sites students request are found on the database. However, if not on the database, students are allowed to ADD a site to the database. Students are cleared for sites when 1) the sites is not being pursued by another student; 2) the site does not currently have an intern; 3) the site has not been marked as CLOSED by the internship coordinator; 4) the site has not reported in the last year that they do not want an intern; and 5) NO FAMILY MEMBER of the student is an employee or owner of the site. Students are NOT allowed to intern where there is family.\nSECURING A SHADOW DAY/INTERNSHIP\nOnce a student has been cleared to pursue a site, he/she may contact the site to request a one day shadow. After said shadow day, if the site would like to have the student return as an intern, the student may return. FIRST the student must notify his/her advisor that an internship has been secured. SECOND the advisor contacts the site to schedule a time to meet. THIS PART IS VERY IMPORTANT! The advisor is who meets with the student and the mentor to discuss rules and expectations. The mandatory district paperwork is filled out at this meeting. After the meeting, the student is free to begin the internship.\nAn internship lasts anywhere from one semester to a full academic year. . The days work consist of daily tasks, small projects, “grunt work,” and project work. Students are to complete a project that is not only of interest to them, but also benefits the internship site. The hours are 6.5 every Tuesday and Thursday, excluding holidays and summer break.\nThere is much to learn about our internship process and expectations. The above is a brief summary. Please, when in doubt, always ask first. We have a process and system that works.']	['<urn:uuid:39ae539a-f0b5-4a43-97a9-699de02c4591>']	factoid	direct	concise-and-natural	similar-to-document	single-doc	expert	2025-05-12T20:11:36.665104	9	14	807
95	How do automatic feature generation methods compare between text data and image classification tasks in terms of their approaches?	For text data, automatic feature generation uses methods like Word2vec, which employs shallow two-layer neural networks to create vector representations based on linguistic contexts. For image classification, there's a text-based representation approach that measures dissimilarity between image regions and prototypes, converting images to text strings and using LZ-complexity for distance calculations. While Word2vec focuses on learning word contexts, the image approach transforms the visual data into a text format that can be processed without traditional image analysis techniques.	"['A Quick Guide to Feature Engineering\nFeature engineering plays a key role in machine learning, data mining, and data analytics. This article provides a general definition for feature engineering, together with an overview of the major issues, approaches, and challenges of the field.\nBy Guozhu Dong, Wright State University\nFeature engineering plays a key role in big data analytics. Machine learning and data mining algorithms cannot work without data. Little can be achieved if there are few features to represent the underlying data objects, and the quality of results of those algorithms largely depends on the quality of the available features. Data often exist in various forms such as image, text, graph, sequence, and time-series. A common way to represent data objects for data analytics is to use feature vectors. Data in raw forms such as those discussed above are not descried by informative features. Even data represented by feature vectors may still be in need of new effective features. Feature engineering is concerned with meeting the needs in generating and selecting an effective feature-vector based representation of data.\nFigure 1: Feature Engineering Makes Data Analytics Possible and More Powerful\nWhat is Feature Engineering: The feature engineering field contains a variety of issues and tasks. The most representative issues and tasks are feature transformation, feature generation and extraction, feature selection, automatic feature engineering, and feature analysis and evaluation.\n- Feature transformation is about constructing new features from existing features; this is often achieved using mathematical mappings. For example, the BMI index is a feature obtained through feature transformation using a mathematic formula.\n- Feature generation is about generating new features that are often not the result of feature transformation. For example, one generates new usable features for images from the pixels of the images (as the pixels are not usable features). Many domain specific ways for defining features also belong in the feature generation category. Feature generation methods can be generic automatic ones, in addition to domain specific ones. Patterns mined from given data can also be used to generate new features . Sometimes the terms ``feature extraction” and “feature construction” are used for feature generation.\n- Feature selection is about selecting a small set of features from a very large pool of features. The reduced feature set size makes it computationally feasible to use certain machine learning and data analytic algorithms. Feature selection may also lead to improved quality on the result of those algorithms. Feature selection has traditionally been focused on the classification problem , but it is also needed for other data analytic problems.\n- Automatic feature engineering is about generic approaches for automatically generating a large number of features and selecting an effective subset of the generated features in the process.\n- Feature analysis and evaluation is about evaluating the usefulness of features and feature sets. This is sometimes included as part of feature selection.\nIt should be emphasized that feature engineering is not just feature selection, and it is more than feature transformation.\nA better sense on feature engineering can be gained by seeing what have been done on perhaps two of the most frequently used types of data, namely text data and image data. For text data, Reference  discussed the following topics: text as strings, the sequence of words representation, the bag of words representation, term weighting, beyond single words, structural representation of text, semantic structure features, latent semantic representation, explicit semantic representation, word embeddings for text representation, and context-sensitive text representation. For image data, Reference  discussed the following topics: classical visual feature representations (including color features, texture features, shape features), latent-feature extraction (including principal component analysis, kernel principal component analysis, multidimensional scaling, isomap, Laplacian eigenmaps), and deep image features (including convolutional neural networks).\nAutomatic feature construction is a popular approach to feature generation, with Word2vec  and DeepLearning based methods  being representative methods. We describe each briefly below.\n(a) Word2vec is a group of methods that produce numerical feature vectors to represent words. They use shallow, two-layer neural networks to reconstruct linguistic contexts of words. Word2vec takes as its input a large corpus of text and produces a vector representation, typically of several hundred dimensions, for each unique word in the corpus. When words share common contexts, their word vectors are similar to each other. Figure 2 (from ) describes two architectures of Word2vec. The CBOW architecture trains a model to predict the current word based on the context described by Bag of Words, and the Skip-gram architecture trains a model to predict surrounding words given the current word. Word2vec has been generalized to image data, graph data and so on.\n(b) DeepLearning methods convert high-dimensional data into a low-dimensional representation, and it does so by training a multilayer neural network with a small central layer to reconstruct high-dimensional input vectors . The small central layer is used as low-dimensional representation. Many types of neural networks have been studied, including the autoencoder networks . Figure 3 gives a high-level view of the autoencoder architecture .\nNew Challenges for Feature Engineering: Many effective feature generation methods, and many automatic feature engineering methods, remain to be discovered. Even traditional research topics such as feature selection and feature analysis need to be re-visited from new perspectives. Indeed, there are millions of ways to generate features, and hence feature selection and feature evaluation methods need to handle this high dimensionality. More importantly, traditional feature selection and evaluation methods are mostly limited to classification and regression problems. Clearly there is a need to develop new feature selection and evaluation methods for other data analytic tasks such as clustering, outlier detection, pattern mining, feature ranking, recommendation, and so on. Finally, much remains to be done to transform feature engineering from an art to a mature engineering discipline.\nFigure 2: Two Architectures for Word2vec (from )\nFigure 3: The Autoencoder Architecture\nA Recent Book on Feature Engineering: In 2018 a new edited book on feature engineering  was published, with 12 contributed chapters from leading experts on different aspects of feature engineering.\nAs feature engineering is often data type specific and application dependent, this book contains chapters devoted to feature engineering for major data types such as text data, image data, sequence data, time series data, graph data, streaming data, software engineering data, Twitter data, and social media data. These chapters present methods for generating tried-and-tested, hand-crafted, domain-specific features, as well as automatic generic feature generation methods such as Word2Vec.\nThe book also contains chapters on feature selection, automatic approaches for feature transformation based feature engineering, automatic feature generation using deep learning approaches, and feature generation using frequent and contrast patterns. Several chapters are about using feature engineering in specific applications.\nThe book contains many useful feature engineering concepts and techniques, which are useful for multiple scenarios: (a) generate features to represent the data when there are no features, (b) generate effective features when (one may be concerned that) existing features are not good/competitive enough, (c) select features when there are too many features, (d) generate and select effective features for specific types of applications, and (e) understand the challenges associated with, and the needed approaches to handle, various data types.\nIn 2018 Guozhu Dong used the book to teach a well-received graduate level Feature Engineering Course, which was perhaps the first of its kind in the world (based on a search of the web). The course covered fundamental aspects of feature engineering, including feature generation for major dataset types, feature extraction, feature transformation, feature selection for different data analytic tasks, feature analysis and evaluation, and applications.\n Guozhu Dong and Huan Liu. Feature Engineering for Machine Learning and Data Analytics. CRC Press, 2018.\n Yunzhe Jia, James Bailey, Ramamohanarao Kotagiri, and Christopher Leckie. Pattern based Feature Generation. Chapter in Feature Engineering for Machine Learning and Data Analytics (edited by Dong and Liu). CRC Press, 2018.\n Yun Li and Tao Li. Feature Selection and Evaluation. Chapter in Feature Engineering for Machine Learning and Data Analytics (edited by Dong and Liu). CRC Press, 2018.\n Chase Geigle, Qiaozhu Mei, and ChengXiang Zhai. Feature Engineering for Text Data. Chapter in Feature Engineering for Machine Learning and Data Analytics (edited by Dong and Liu). CRC Press, 2018.\n Parag S. Chandakkar, Ragav Venkatesan, and Baoxin Li. Feature Extraction and Learning for Visual Data. Chapter in Feature Engineering for Machine Learning and Data Analytics (edited by Dong and Liu). CRC Press, 2018.\n Suhang Wang and Huan Liu. Deep Learning for Feature Representation. Chapter in Feature Engineering for Machine Learning and Data Analytics (edited by Dong and Liu). CRC Press, 2018.\n Tomas Mikolov, Kai Chen, Greg Corrado, Jeffrey Dean. Efficient Estimation of Word Representations in Vector Space. arXiv. 2013.\n Hinton, Geoffrey E., and Ruslan R. Salakhutdinov. Reducing the dimensionality of data with neural networks. Science 313.5786: 504-507. 2006.\nBio: Guozhu Dong is a professor at the Department of Computer Science and Engineering and the Knoesis Center at Wright State University.\n- On-line and web-based: Analytics, Data Mining, Data Science, Machine Learning education\n- Software for Analytics, Data Science, Data Mining, and Machine Learning\n- Good Feature Building Techniques and Tricks for Kaggle\n- Feature engineering, Explained\n- Implementing Automated Machine Learning Systems with Open Source Tools', ""AbstractWe present a new method for image feature-extraction for learning image classification. An image is represented by a feature vector of distances that measure the dissimilarity between regions of the image and a set of fixed image prototypes. The method uses a text-based representation of images where the texture of an image corresponds to patterns of symbols in the text string. The distance between two images is based on the LZ-complexity of their corresponding strings. Given a set of input images, the algorithm produces cases that can be used by any supervised or unsupervised learning algorithm to learn image classification or clustering. A main advantage in this approach is the lack of need for any image processing or image analysis. A non-expert user can define the image-features by selecting a few small images that serve as prototypes for each class category. The algorithm is designed to run on a parallel processing platform. Results on the classification accuracy and processing speed are reported for several image classification problems including aerial imaging.\nBelousov A. Massively parallel computations for image classification. Master's thesis, Ariel University, http://www.ariel.ac.il/sites/ratsaby/Theses/alex.pdf, 2015.\nBelousov A, Ratsaby J. Massively parallel computations of the LZ-complexity of strings,. In Proc. of the 28th IEEE Convention of Electrical and Electronics Engineers in Israel (IEEEI'14), pages pp. 1-5, Eilat, Dec. 3-5 2014. https://doi.org/10.1109/EEEI.2014.7005885\nBelousov A, Ratsaby J. A parallel distributed processing algorithm for image feature extraction. In Advances in Intelligent Data Analysis XIV - 14th International Symposium, IDA 2015, Saint-Etienne, France, October 22-24, 2015. Proceedings, volume 9385 of Lecture Notes in Computer Science. Springer, 2015.\nCheng H. D. Shan J, Ju W, Guo Y, Zhang L. Automated breast cancer detection and classification using ultrasound images: A survey. Pattern Recognition 2010; 43(1): 299-317. https://doi.org/10.1016/j.patcog.2009.05.012\nChester U, Ratsaby J. Universal distance measure for images. In Proceedings of the 27th IEEE Convention of Electrical Electronics Engineers in Israel (IEEEI’12), pages 1- 4, Eilat, Israel, November 14-17, 2012. https://doi.org/10.1109/EEEI.2012.6377115\nChester U, Ratsaby J. Machine learning for image classification and clustering using a universal distance measure. In Brisaboa N, Pedreira O, Zezula P, Eds., Proceedings of the 6th International Conference on Similarity Search and Applications (SISAP’13), volume 8199 of Springer Lecture Notes in Computer Science 2013; pp. 59- 72. https://doi.org/10.1007/978-3-642-41062-8_7\nCilibrasi R, Vitanyi P. Clustering by compression. IEEE Transactions on Information Theory 2005; 51(4): 1523-1545. https://doi.org/10.1109/TIT.2005.844059\nFei-Fei L, Fergus R, Perona P. Learning generative visual models from few training examples: An incremental bayesian approach tested on 101 object categories. Computer Vision and Image Understanding 2007; 106(1): 59-70. Special issue on Generative Model Based Vision. https://doi.org/10.1016/j.cviu.2005.09.012\nGalar M, Derrac J, Peralta D, Triguero I, Paternain D, Lopez- Molina C, Garc´ıa S, Ben´ıtez J. M. Pagola M, Barrenechea E, Bustince H, Herrera F. A survey of fingerprint classification part i: Taxonomies on feature extraction methods and learning models. Knowledge-Based Systems 2015; 81: 76- 97. https://doi.org/10.1016/j.knosys.2015.02.008\nGonzalez RC, Woods R. E. Digital Image Processing (3rd Edition). Prentice-Hall, Inc., Upper Saddle River, NJ, USA, 2006.\nHall M, Frank E, Holmes G, Pfahringer B, Reutemann P, Witten I. H. The WEKA data mining software: An update. SIGKDD Explorations 2009; 11(1): 10-18. https://doi.org/10.1145/1656274.1656278\nHaralick R. M. Shanmugam K, Dinstein I. Textural features for image classification. Systems, Man and Cybernetics, IEEE Transactions on, SMC 1973; 3(6): 610-621. https://doi.org/10.1109/TSMC.1973.4309314\nLazebnik S, Schmid C, Ponce J. A sparse texture representation using local affine regions. IEEE Trans Pattern Anal Mach Intell 2005; 27(8): 1265-1278. https://doi.org/10.1109/TPAMI.2005.151\nLu D, Weng Q. A survey of image classification methods and techniques for improving classification performance. Int J Remote Sens 2007; 28(5): 823-870. https://doi.org/10.1080/01431160600746456\nOjala T, Pietikainen M, Harwood D. A comparative study of texture measures with classification based on featured distributions. Pattern Recognition 1996; 29(1): 51-59. https://doi.org/10.1016/0031-3203(95)00067-4\nPham D.T. Alcock RJ. Chapter 5 -classification. In D.T. PhamR.J. Alcock, editor, Smart Inspection Systems, Academic Press, London 2003; pp. 129-155. https://doi.org/10.1016/B978-012554157-2/50005-5\nRaju J, Durai C. A. D. A survey on texture classification techniques. In Information Communication and Embedded Systems (ICICES), 2013 International Conference on, 2013; pp. 180-184. https://doi.org/10.1109/ICICES.2013.6508183\nRisojevic V, Babic Z. Aerial image classification using structural texture similarity. In Proceedings of the IEEE International Symposium on Signal Processing and Information Technology (ISSPIT) 2011; pp. 190-195. https://doi.org/10.1109/ISSPIT.2011.6151558\nSayood K, Otu H. H. A new sequence distance measure for phylogenetic tree construction. Bioinformatics 2003; 19(16): 2122-2130. https://doi.org/10.1093/bioinformatics/btg295\nZiv J, Lempel A. On the complexity of finite sequences. IEEE Transactions on Information Theory 1976; 22(3): 75-81. https://doi.org/10.1109/TIT.1976.1055501""]"	['<urn:uuid:48a26cb5-19bf-44d4-bc03-369856b7be23>', '<urn:uuid:9084db49-4e6d-43d8-b5a8-fdee81e92119>']	open-ended	direct	verbose-and-natural	similar-to-document	comparison	novice	2025-05-12T20:11:36.665104	19	78	2255
96	Which had a more recent public display - Valdés' Butterflies sculpture or Pittsburgh's Three Sisters Bridges?	Manolo Valdés' Butterflies sculpture had a more recent public display, being exhibited at Berkeley Square in Mayfair until October 30th, 2019. The Three Sisters Bridges are permanent fixtures in Pittsburgh, with their most recent construction being in 1927 when the Roberto Clemente (6th Street) Bridge was rebuilt.	"[""What's on: event details\n- This event has passed.\nFriday, 25th October - Saturday, 16th NovemberManolo Valdés, Works from 2006-2019 exhibition\nOpera Gallery are delighted to announce a new exhibition by the acclaimed Spanish pop art pioneer Manolo Valdés at their New Bond Street gallery in London on 25 October – 15 November. The exhibition will showcase 30 works, spanning between 2006 and 2019, ranging from sculpture to painting with a spotlight on the artist’s masterful craftmanship.\nValdés’ flair for playful and tactile aesthetics have made him one of the most sought-after artists for monumental public installations around the world, with works at Place Vendrôme in Paris, Park Avenue in New York and Chateau de Chambord in the Loire Valley. Valdés’ works are included in some of the most prestigious collections and museums around the world including Metropolitan Museum of Art in New York, Guggenheim Bilbao in Spain, and the de Young Museum in San Francisco and in 1999 Valdes represented Spain at the 48th Venice Biennale. Valdés’ works are sold for record breaking prices and in June 2019, Christie’s auctioned off his mixed media work Matisse como Pretexto con Ocre y Espejo for £431,250 – over three times the estimated price of £120,000.\nArguably one of the most significant contemporary artists living and working in Spain, Manolo Valdés has a longstanding career as one of the most influential creative forces in his home country. Valdés was one of the founding members of Spanish pop art movement Equipo Cronica (1964-81) that distinctively illustrated the horrors of Francoist Spain and used their art to rebel against the fascist state. Celebrating its 55th anniversary this year, the Equipo Cronica movement introduced a whole new visual expression that combined social and political ideas with humour and irony, with Valdés central to of the most visionary chapters in modern Spanish art history. Using a figurative style that was informed by press photographs and comics from 1960s and 1980s, as well as art historical symbolism of Spanish masters like Diego Velasquez and Pablo Picasso, Valdés and his collective chronicled the political turbulence in their native land by using the aesthetics of pop art as a vehicle for combative political discussion as well as connecting the unsettling political present with other global events like Vietnam War.\nHaving reinvented his practice after the glory days of Equipo Cronica and left visual politics in the past, art historical imagery has remained core to Valdés’ art practice. Like a magpie, the Valencian artist has become famous for handpicking fragments of his favourite artworks and reframing these elements in a contemporary manner. At Opera Gallery, he will showcase a series of new unseen paintings amongst some of his most beloved motifs, like his ongoing Reina Mariana series that find its inspiration from Velasquez’ iconic portrait of the Queen Mariana of Austria. The exhibition will accentuate the artist’s masterful craftmanship through a versatile use of materials, including glass, alabaster, quartz, burlap, iron and bronze amongst others and will undoubtedly be one of London’s unmissable exhibitions of this Autumn.\nThe exhibition will also coincide with the final weeks of Valdés’ monumental public sculpture Butterflies (2013) located at the Berkeley Square in Mayfair erected as a part of Westminster City Council’s City of Sculpture initiative for public art. The monumental installation will be visible to public till the end of 30 October.\nOpera Gallery London director Federica Beretta: “We are thrilled to host Manolo’s new show at Opera Gallery London and showcase some of his new unseen works as a celebration of the gallery’s 25th anniversary this year. Manolo is one of the very first artists Opera Gallery acquired so it’s wonderful to be able to simultaneously celebrate the gallery’s anniversary and his spectacular career this Autumn.”\nAbout the artist\nManolo Valdés (b. 1942) is one of Spain’s most famous living artists. He is known for his art historically inspired imagery and monumental installations. The artist takes his inspiration from old masters like Velasquez, Rembrandt and Goya and uses them as a pretext for creating a new aesthetic. Manolo Valdés’ works are included in several private and public collections around the world, including but not limited to Peggy Guggenheim collection in Venice, Metropolitan Museum of Art in New York and Centre Georges Pompidou in Paris.\nAbout Opera Gallery\nFounded by Gilles Dyan in 1994, Opera Gallery is one of the leading international dealers and representatives of Modern and Contemporary Art. Opera Gallery is established worldwide with galleries in locations including New York, Miami, Aspen, Paris, Monaco, Geneva, Zurich, Singapore, Hong Kong, Seoul, Beirut and Dubai. Opera Gallery offers museums, foundations and private international art collectors’ unique access to a diversity of Modern and Contemporary artists through an exciting programme of curated exhibitions and high-profile art fairs. Notable exhibitions include Pablo Picasso in Monaco, Marc Chagall in London, Jean Michel Basquiat & Andy Warhol in Geneva, Jean Dubuffet & Alexander Calder in Geneva and two major Manolo Valdés exhibitions in Paris and Singapore. http://www.operagallery.com/london/\nPlease contact Kasimiira Kontio at Midas Public Relations | Kasimiira.email@example.com | 020 7361 7860\nOpera Gallery opening hours\nMon-Sat: 10am – 7pm\nSun: 12pm – 5pm"", 'You’ve seen the bridges around the city. You’ve heard the names. But you may not know the story behind some of Pittsburgh’s most iconic bridges. Here’s a bit of the history behind the names.\nThree Sisters Bridges\nPittsburgh’s Three Sisters Bridges span the Allegheny River and connect the North Shore to downtown. With their bright yellow construction, these three bridges are synonymous with the Pittsburgh skyline. They are self-anchored eye-bar suspension bridges, connecting at 6th, 7th and 9th streets, and are named after three famous Pittsburghers.\nRachel Carson Bridge\nRachel Carson was born in Springdale, PA, near the Allegheny River in 1907. She attended Chatham University (at the time known as the Pennsylvania College for Women) and majored in biology. Carson then attended graduate school at Johns Hopkins University researching zoology.\nAn environmental activist, she published several critically acclaimed books including “Under the Sea-Wind” and “The Sea Around Us.” She was outspoken about agricultural science and the long-term consequences of using pesticides. In 1963, she testified in front of Congress, calling for new policies to protect the environment and public health.\nThe Rachel Carson Bridge, also known as the 9th Street Bridge, was opened in 1927. It was named after Carson on Earth Day in 2006.\nRoberto Clemente Bridge\nWith over 18 big league seasons under his belt, Roberto Clemente is considered one of the greatest professional baseball players of all time. In 1955, he made his major league debut with the Pittsburgh Pirates. Clemente received 12 Gold Glove Awards, 12 All-Star Game selections, two World Series Championships, and had over 3,000 hits during his major league career. In 1972, Clemente passed away on Dec. 31 in a plane crash headed to Nicaragua to provide aid to earthquake victims. He was inducted into the Baseball Hall of Fame in 1973.\nThe Roberto Clemente Bridge, also known as the 6th Street Bridge, was constructed in 1859. It was rebuilt most recently in 1927 to allow for more traffic needs. If you’re headed to PNC Park for a Pirate’s game, the roads are usually closed to cars to allow foot traffic into the park. This is a great spot for the perfect view of PNC Park and downtown Pittsburgh.\nAndy Warhol Bridge\nAndy Warhol is a renowned artist credited with pioneering the Pop Art movement. Born in Pittsburgh in 1928, Warhol made a career for himself in New York City. Famous for his silkscreens of cultural icons, such as Elvis, Campbell’s soup cans, and Marilyn Monroe, Warhol’s work explored the relationship between art and consumer culture, mass media and celebrity news. His work mediums include painting, sculpture, print, photography, wallpaper and film.\nThe Andy Warhol Museum, one of the four Carnegie Museums of Pittsburgh, is a seven-floor art collection that spans Warhol’s artistic output from the 1940s to 1980s.\nThe Andy Warhol Bridge, also known as the Seventh Street Bridge, was built in 1926. It was named for Warhol on March 18, 2005, as part of the 10th anniversary of the Andy Warhol Museum. As an Oakland native, Warhol supposedly walked across this bridge with his family in his youth.\nThis content was provided by a local, independent contributor to Made in PGH, a lifestyle blog.']"	['<urn:uuid:45fb8d08-f774-4ffd-a138-bd43fb2d7a0e>', '<urn:uuid:231d9028-ddd3-405d-a8a8-b0a97f4da613>']	factoid	with-premise	concise-and-natural	distant-from-document	comparison	expert	2025-05-12T20:11:36.665104	16	47	1390
97	compare indigenous environmental management practices guugu yimithirr maori interactions cook crew knowledge sharing	The Guugu Yimithirr people demonstrated strong environmental management practices, particularly regarding turtle population conservation, understanding the importance of taking only what was needed to ensure sustainable food sources. This knowledge led to conflict when Cook's crew overharvested turtles, showing a lack of understanding of Indigenous conservation practices. In contrast, the Maori account shows environmental knowledge sharing occurring more peacefully, with the Indigenous people gathering and sharing various types of grass and stones with Cook's crew when they showed interest in collecting these items. While both encounters involved Indigenous environmental knowledge, the Guugu Yimithirr interaction led to conflict over resource management, while the Maori interaction resulted in collaborative exploration of local resources.	"[""According to the sacred stories of the Guugu Yimithirr people, the Rainbow Serpent resides in the twists and bends of the Wabalumbaal Birri, whose mouth opens onto the coral reef where, on the evening of June 10 1770, the HMS Endeavour ran aground.\nThe Waalumbaal Birri has been renamed the Endeavour River and it was on the south bank of that river, on the Waymbuurr clanland, in the tribal nation of the Guugu Yimithirr people, the first contact between Europeans and Australian Indigenous people came to pass.\nThe 7 weeks that James Cook and his men spent stranded has left several lasting legacies, the best known of which is the name of our national icon, “gangurru” or “kangaroo”. The encounter also brought together the threads of issues which still dominate disscussions of Indigenous-settler relations today: environmental care, reconciliation and cultural governance.\nIn recent years, the Gamba Gamba artists at the Hopevale Arts and Cultural Centre have, through their art, begun to consider their relationship with the narrative surrounding James Cook, and their post-colonial history. As well as recounting stories of their own personal and family histories, the women’s artwork also explores the relationship that the Guugu Ymithirr people have with the wildlife that surrounds them and landscape that they inhabit. Cultural practices are inherently linked with land management, and particularly sustainable fishing and population control of reef species. For the Gamba Gamba, the importance of this relationship is highlighted by the cause of the first conflicts between the sailors of the Endeavour and the Indigenous population of Cape York.\nInitially, the relationship between Indigenous Australians and Cook’s men started peacefully. The Bama wanted to be helpful to Cook and his men, and in return for assisting them with finding food, and wood with which to mend the ship the sailors attempted to trade goods – such as cloth. Gifts which, Cook noted in his diary, that while they were accepted were swiftly discarded by a people who had no use for them.\nThis peace was interrupted when the sailors caught 12 turtles and loaded them on board Cook’s ship to be butchered. Cook’s men did not understand the laws of the Indigenous people, for whom turtles are a sacred and protected species. The Indigenous people of the land were aware that they had to keep the population of the slow-breeding sea turtles preserved – to take only what they needed to ensure the continuation of the food source for the years to come. Upon finding out that the sailors had been hoarding turtles, a group of Aboriginal men boarded Cook’s ship and began to try and rescue the turtles, throwing them over the sides of the ship. This resulted in days of violence and conflict – during which time the Bama were shot at and wounded by Cook and his men, and in retaliation, they set fire to the European camp. A highlight of this retaliation, says Daisy Hamlot, is that in setting fire to the camp, some of the European’s clothes were burned, and they were chased, naked down the beach.\nFor the artists of the Hopevale Art Centre the theft of those turtles 12 years ago, and the violence that followed because of the Bama’s attempts to free them is representative of the way that Indigenous land management practices continue to be ignored today, in the interests of greed and profit-making. The artist’s work frequently features scenes of the beach, the reef and the animals that live within.\nCook and his men made mistakes because they did not listen and did not understand the laws, and the artists feel that since that time, nothing much has changed. The rules themselves are easy to understand though - if you listen, says Gertie Deeral “Us bama, love to go out to the reef. We catch turtle, fish and dugong. It's a beautiful place. But we also want to be careful. We don't like it when too many turtles and dugong are killed. We need to look after them or we won't have any left. It worries me. Take one or two but only for special times. Don't go crazy and kill too many. Don't be greedy. Don't be wasteful.”\n250 years ago, the violence escalated to the point of bloodshed, until one day a very old man – an elder and leader of the tribe, came forward holding forth a spear with a broken tip to show that his people lay down their weapons. He stood upon a rocky outcrop on the shore of the beach, and invited Cook to join him in a dialogue that for the Guugu Ymithirr represents the first reconciliation. In 1770 this moment of exchange, dialogue and understanding became the catalyst for both groups to overcome the challenges of the language and cultural divide between them. Today, the Gamba Gamba hope that similar discussion may provide an opportunity to enrich cultural and scientific understanding of the symbiotic relationship that the Guugu Yimithirr people have always shared with the sea; an understanding which may help to create an Australian nation that is equipped to face the growing challenge of climate change and a dying reef."", 'New Zealand Childhoods (18th–20th c.)\nThe Ancient History of the Maori [Literary Excerpt]\nIn this excerpt, an adult Horeta Te Taniwha recounts childhood memories of a cultural encounter with Europeans for a Pakeha researcher. Te Taniwha, as an indigenous child of Aotearoa/New Zealand, participated in one of the first meetings between coastal tribes and the British maritime explorer, Captain James Cook, and his crew. These interactions were written up contemporaneously in Cook\'s journals and recorded by the artists and gentlemen of science on board Endeavour during its first voyage to New Zealand (November 1769 – March 1770). The published accounts and images would have influenced subsequent European perceptions of Maori people, just as later indigenous responses on island communities throughout the Pacific Ocean would have probably been affected by oral stories of earlier events. Early 19th-century missionaries and linguists successfully developed Maori as a written language but narratives such as this one by Te Taniwha would still be told in Maori and then translated, sometimes with a choice of English terms that had no Maori equivalent. Hence monsters (taniwha) and gods (atua) were part of traditional Maori cosmology but goblins were not. Horeta Te Taniwha recounted his boyhood experiences several times for different researchers with very little inconsistency.\nThis source extract can invite discussion about the issues to be considered when using adult recollections of childhood, or the influence of a translator\'s choice of words. Primarily, though, it provides scope for analyzing a relatively rare glimpse of how an indigenous child of Aotearoa/New Zealand first experienced cultural encounter. His frame of reference is Polynesian, of the Pacific, not European. The norms against which he notices similarities and differences are those of his Maori culture (as it had evolved within Aotearoa) and his particular tribe. Auditory perceptions are noticeable: his was a society with no written language but great mastery of detail since the transmission of tribal knowledge and tradition was primarily through chants, oratory, and song.\nWhite, John. The Ancient History of the Maori: Tainui, Vol. V. Wellington: Government Printer, 1888, 121–24. Annotated by Jeanine Graham.\nPrimary Source Text\nWe lived at Whitianga, and a vessel came there, and when our old men saw the ship they said it was a tupua, a god (some unknown thing), and the people on board were strange beings. The ship came to anchor, and the boats pulled on shore. As our old men looked at the manner in which they came on shore, the rowers pulling with their backs to the bows of the boat, the old people said, ""Yes, it is so: these people are goblins; their eyes are at the back of their heads; they pull on shore with their backs to the land to which they are going."" When these goblins came on shore we (the children and women) took notice of them, but we ran away from them into the forest, and the warriors alone stayed in the presence of those goblins; but, as the goblins stayed some time, and did not do any evil to our braves, we came back one by one, and gazed at them, and we stroked their garments with our hands, and we were pleased with the whiteness of their skins and the blue of the eyes of some of them.\nThese goblins began to gather oysters, and we gave some kumara [sweet potato], fish, and fern-root to them. These they accepted, and we (the women and children) began to roast cockles for them; and as we saw that these goblins were eating kumara, fish, and cockles, we were startled, and said, ""Perhaps they are not goblins like the Maori goblins."" These goblins went into the forest, and also climbed up the hill to our pa (fort) at Whitianga (Mercury Bay). They collected grasses from the cliff, and kept knocking at the stones on the beach, and we said, ""Why are these acts done by these goblins?"" We and the women gathered stones and grass of all sorts, and gave to these goblins. Some of the stones they liked, and put them into their bags, the rest they threw away; and when we gave them the grass and branches of trees they stood and talked to us, or they uttered the words of their language. Perhaps they were asking questions, and, as we did not know their language, we laughed, and these goblins also laughed, so we were pleased. . . .\nAfter the ship had been lying at anchor some time, some of our warriors went on board, and saw many things there. When they came on shore, they gave our people an account of what they had seen. This made many of us desirous to go. . . . We sat on the deck of the ship, where we were looked at by the goblins, who with their hands stroked our mats and the hair of the heads of us children; at the same time they made much gabbling noise in talking, which we thought was questions regarding our mats and the shark\'s teeth we wore in our ears, and the hei-tiki we wore suspended on our chests; but as we could not understand them we laughed, and they laughed also. . . .\nThere was one supreme man in that ship. We knew that he was the lord of the whole by his perfectly gentlemanly and noble demeanor. He seldom spoke, but some of the goblins spoke much. But this man did not utter many words: all that he did was to handle our mats and hold our mere [short, flat weapon], spears, and waha-ika [weapon of bone or wood], and touched the hair of our heads. He was a very good man, and came to us—the children—and patted our cheeks, and gently touched our heads. His language was a hissing sound, and the words he spoke were not understood by us in the least. . . .\nFull text of this work is freely available through the New Zealand Electronic Text Centre.\nHow to Cite This Source\n""New Zealand Childhoods (18th–20th c.),"" in Children and Youth in History, Item #93, http://chnm.gmu.edu/cyh/teaching-modules/93 (accessed April 27, 2017).\n- Primary Sources\n- The Ancient History of the Maori [Literary Excerpt]\n- Adventure in New Zealand, from 1839 to 1844 [Book Excerpt]\n- Annual Report on Native Affairs, 1874 [Government Report]\n- Shocking Disaster at Cambridge [Newspaper Article]\n- Juvenile Depravity Suppression Bill [Political Speech]\n- Taranaki Education Office Report, 1898 [Official Document]\n- ""Dear Dot"" Children\'s Letters [Newspaper Column]\n- Colonial Childhoods Oral History Project [Oral History]\n- Code of Honour [Literary Excerpt]\n- New Zealand School Photographs, 1950 and 1964 [Photographs]\n- 1996 New Zealand Census Information [Statistical Tables]\n- Sanitarium Weet-Bix Packet [Advertisement]']"	['<urn:uuid:d6c8ba70-d96d-4423-9983-de047ac32876>', '<urn:uuid:849db166-6432-4207-9548-574cde53b7fb>']	open-ended	with-premise	long-search-query	similar-to-document	comparison	expert	2025-05-12T20:11:36.665104	13	111	1981
98	What motivated the PLO versus religious terrorists?	The PLO and religious terrorists had different motivational foundations. The PLO had an explicit territorial and political aim - the 'liberation of Palestine' from what they called 'The Zionist Entity,' operating through armed struggle and attacks launched from bases in neighboring countries. Religious terrorists, in contrast, were motivated by an interlinked combination of factors - religious beliefs were inseparably interwoven with issues like state power, foreign interference, hierarchies of power, economic resources, and individual motivations like revenge. The PLO's motivation was more singularly focused on a specific political goal, while religious terrorism involved a more complex mix of religious ideology combined with various social and political factors.	"[""10-Minute Talks: Religion and the history of terrorism\n29 Jul 2020\nSubscribe to this podcast via your chosen service\nProfessor Richard English FBA asks four questions about religion and terrorism: should religion be seen as a cause for terroristic violence or a restraining influence upon it? Is religious terrorism an existential threat? Is religious terrorism something new? And should religion, in terms of its influence on terrorism, be seen as something that is a detachable part of that mixture?\nHello. I’m Richard English, a Fellow of the British Academy. In this short talk, I want to ask four questions about religion and the history of terrorism.\nFirst question is this: should religion be seen more as a cause behind terroristic violence, or more as a restraining influence upon it? Very famously, there have been many instances historically in which religion has been one of the causal elements leading to non-state terroristic violence, which is the subject of my short talk today. Whether in the form of 21st century jihadism, or in the form of the violence of a group like Hamas, or in the form of the IRA’s violence of 1920, or in the form of more recent political violence in Northern Ireland by non-state actors, there have been famous cases when religion has been one of the elements within the causation behind such activity.\nThis has had various different kinds: sometimes it's religiously fueled nationalisms that we're seeing here; sometimes it's a more theocratic form of politics and political violence which is being practiced; sometimes there is an individual, redemptive apotheosis involved, but in their complex, various ways, there has been much attention to this. In their complex various ways, there is little doubt that religion has on occasions been a contributory element towards the production of non-state terrorist violence. Less famously, but I think perhaps more importantly, religion has also, on occasions, been an important influence restraining the turn to terrorism and this has occurred across different religious traditions. There are hints at this, in terms of scriptural injunctions towards empathy and compassion within the Christian tradition, for example, Matthew Chapter Seven, Luke Chapter Six. But also within the Christian tradition, if you look at the post-1968 political violence in Northern Ireland, very frequently there were, out of people's religious convictions, religious beliefs, religious commitments, calls for restraint, for non-retaliation, for people not to engage in more behaviour of a callous, terroristic kind. Within the Muslim tradition, many of the most authoritative voices arguing against jihadist terrorism have been voices which have been expressly, explicitly Muslim voices, justifying their hostility to the terrorist violence in explicitly Muslim terms. Equally, within the Jewish tradition, there are very powerful voices that have argued that Jewish faith, Jewish commitment, should fructuate in a reduction of politically motivated violence by non-state actors, rather than by a repetition or an increase.\nThe Stanford neuroscientist Robert Sapolsky, with a profound expertise in understanding violence, has argued that religion tends to foster both the best and the worst of our behaviours. I think in terms of religion and the history of terrorism, we have perhaps focused more on some of the worst than of the best.\nI think there is a huge irenic potential within religion in terms of restraints upon terrorism. That has been the case and I think it could be the case even further in the future and probably deserves more attention than we have tended, many of us, to give it.\nSecond question: is religious terrorism an existential threat, or more of a horrific nuisance? There are those politicians, as well as authors, who've argued the former, who've argued that religious terrorism has represented and does represent an existential threat. I think that if you look in context, on the basis of an engagement with vast numbers of mutually interrogatory sources, including first-hand sources over long pasts, this turns out not to be the case. In other words, I think religious terrorism tends to be horrific in terms of its consequences for the victims of course, but not an existential threat and indeed, if you compare it with some of the other threats that we do face – wars, genocide, hunger, disease, climate change – I think religious, as other forms of non-state, terrorism, should be seen as profound causes for concern, but very much lesser than some of the other threats we face and far below an existential level.\nThird question: is religious terrorism something which is new, or is it something which is familiar and which we have seen in the past over long experience? There are those again who have argued that it represents something profoundly new, that it represents a kind of post-fault line new development within our experience as a species. I think again that I am sceptical about that. I think that someone like Martha Crenshaw, herself a fellow of the British Academy, has pointed out that much of what people claim to be 'new' in this new religious form of terrorism turns out to be rather familiar, when you look closely at the evidence. Georgetown's Bruce Hoffman has pointed out that religion has long been connected with terrorism, within the history of this kind of non-state political violence, I think it's less new than some observers have argued.\nFourth: should religion, in terms of its engagement with terrorism or its influence upon terrorism, be seen as something which is a detachable part of that mixture? Or is it something which is inextricably and organically linked to the other elements within it? I would argue the latter. And I think arguing the latter helps us to avoid two potential errors of interpretation when we're looking at religion and the history of terrorism. One area of interpretation is to say, if only you could just extract the religion, if only people didn't believe in a deity, you'd find terrorism dissipating and in particular you might find certain forms of terrorism, suicide attacks, disappearing. I think that's wrong. I think the cases of non-state terrorism that have really mattered in terms of their historical effect, have been ones in which yes, there has been religious motivation of a profound kind, but there's also been a set of questions around other elements: state power, foreign interference politically within particular regions, hierarchies of power within particular settings, gender relations, control over economic resources, the sense of legitimacy of power and of identity for particular groups of people within society, individual motivations of revenge and so forth. So, I think it would be an error to say if only you could just detach religion and get rid of it, the terrorist threat would somehow evaporate. I think it's also important to recognise the interwovenness of religion with other elements, because some people argue that you find religion as an element in this mixture leading to terroristic violence, but it's really a kind of badge – it's a surface badge and there are more profound things going on, which are the real causes – and again I think that's something of a mistake. I think that if you're looking at groups like Hamas, for example, if you're looking at groups that have been involved in the violence associated with Al-Qaeda, associated with ISIS, for some people there is a profound sense that the religion actually does matter. Yes, it's interwoven with questions of the state; yes, it's interwoven with hostility, in some cases, to foreign powers such as US power; yes, it's interwoven with questions of identity, but religion is something which is inseparable from the rest of it and is organically interwoven.\nWhy do these four questions matter? They matter primarily because it is responses to non-state terrorism, rather than non-state terrorism itself, that change history dramatically and that changes history in the most powerful ways.\nI think there have been ways in which wrong answers to these four questions have made things significantly worse. We've tended not as states and societies to draw as profoundly as we could do, I think, upon the irenic resources that religion represents, in terms of countering terrorism. If you look at the United States’ national strategy for counter-terrorism – yes, it identifies religious terrorism as a major threat, but it overwhelmingly ignores the resources that there are in religious belief attachment and dissociation in limiting terrorism. Something has been and continues to be missed there. If we argue that religious terrorism is an existential threat, we’re far more likely to have exaggerated and counter-productive responses, often militaristic responses, whereas a more proportional way of dealing with it is much more likely to save life and limb than an overreaction to what is misdiagnosed as an existential threat. If we see religious terrorism as something completely new there's a danger that, as did happen after 9/11 on occasions, there's a danger that people forget what we know that we know about effective, measured, proportionate counterterrorism and again, things can be counterproductive as a response. If we pretend that religion can somehow be seen as a detachable element, that it's either just a superficial badge for what's really going on, that doesn't matter or that it's something which if only you could get rid of it would mean that you could get rid of terrorism, we misunderstand the complex interwovenness, the historical jaggedness and inextricability of religion with other important world historical forces in generating non-state terrorism.\nWhat I’m arguing is if we get the answers to these four questions I’ve been talking about today, about religion and the history of terrorism, if we get our answers right, we'll be able to avoid some of the more egregious areas of response to terrorism and there is the possibility that the future will, as a consequence, be less blood-stained than it otherwise would.\nThis talk originally took place on 1 September 2020, part of the series 'The British Academy 10-Minute Talks', where the world’s leading professors explain the latest thinking in the humanities and social sciences in just 10 minutes. 10-Minute Talks are screened each Wednesday, 13:00-13:10, on YouTube and available on Apple Podcasts. Subscribe to the British Academy 10-Minute Talks here.\nDoes Terrorism Work? A History, Richard English FBA.\nThe Oxford Handbook of Terrorism, edited by Erica Chenoweth, Richard English, Andreas Gofas, and Stathis N. Kalyvas.\nTerror Through Time: The origins of the word, Radio 4 programme with Richard English FBA.\nSecuring our Future: A Decade of Counter-Terrorism Strategies, article by Mike Smith in the UN Chronicle.\nRethinking US Counterterrorism Strategy, publication by the Middle East Institute.\nLead image: People lighting candles during candlelight vigil at night. © Watsamon Tri-Yasakda / EyeEm via Getty Images."", '- A Chronological Presentation\nThe New State (1964 - 1974)\n- The Establishment of the PLO\nThe Palestine Liberation Organization (PLO)\nwas established in 1964 as an umbrella organization\nfor the various Palestinian armed groups, with the\nexplicit aim of achieving, through armed struggle,\nthe ""liberation of Palestine"" from ""The\nZionist Entity."" The West Bank and Gaza, occupied\nby Jordan and Egypt respectively, were at this time\nnot targets of the PLO\'s struggle for liberation.\nIn the 1960\'s, with Egypt\'s loss of Gaza as a base\nfor guerilla operations into Israel, Jordan (the West\nBank) and the Syrian Golan Heights became the preferred\nlaunching grounds for attacks on Israel. Israel reacted\nwith attacks of retribution across the borders into\nJordan and Syria.\naircraft destroyed on the\nrunway, June 5, 1967.\n- The Six Day War\nIn 1966 the Syrian attacks on northern Israel from\nthe Golan Heights intensified, and in the spring of\n1967 the armed clashes between the two countries escalated\nfurther. Fabricated Soviet reports of alleged concentrations\nof Israeli troops near the Syrian border made the\nArab leaders step up their threats against Israel,\nand on May 15, 1967 the Egyptian president Nasser\nordered his troops across the Suez Canal, into the\nSinai Desert. In the following days Nasser expelled\nthe UN peace-keeping force and reimposed\nthe blockade of Eilat.\nIsrael\'s Defence Minister,\nMoshe Dayan (center), and\nChef of Staff, Yitzchak Rabin\n(right), arrive in Jerusalem\nafter the fall of the city i 1967.\nIsrael sought support for the lifting\nof the blockade with its Western allies, but was rejected.\nThe Israeli Prime Minister Levi Eshkol stated in a speech\nthat Israel did not seek a military confrontation with\nits Arab neighbors. With Israel completely isolated,\nJordan and Iraq joined the Egyptian-Syrian defense pact,\nwhile several other Arab countries promised support\nfor the coming war against Israel. Nasser declares that\nthe Arab\'s objective is the complete annihilation of\nSurrounded by the Arab forces, which measured\nby troops and equipment outnumbered the Israeli armed\nforces more than two to one, and with the prospect of\nattacked from all sides, the Israelis chose to strike\nfirst. On June 5, 1967 Israeli warplanes bombed Egyptian\nairfields and in a matter of a few hours eradicated\nalmost the entire Egyptian air force. Israel appealed\nto Jordan to stay out of the fighting, and promised\nthat if it did, Israel too would refrain from attacking\nand the occupied territories\nafter the Six Day War in June 1967.\nBut when the Egyptians reported of their allegedly\nsuccessful attack on Israel, Jordan that same morning\ninitiated a massive shelling of West Jerusalem and other\nIsraeli population centres. Syria bombarded northern\nIsrael from the Golan Heights, and Jordanian, Syrian\nand Iraqi planes attacked other Israeli targets.\nIsraeli jets were dispatched against Jordan and Syria,\nand quickly destroyed both countries\' entire air forces.\nLeft completely without air support, already on the\nfirst day, the Arab armies were doomed, and within only\nsix days Israel conquered the Gaza Strip, the Sinai\ndesert, the Golan Heights and the West Bank including\nthe Old City of Jerusalem.\n1967 - UN-Resolution\n242 and the ""Three Noes""\nIn the wake of Israel\'s overwhelming victory\nin June 1967, the United Nations Security Council\non November 22 the same year adopted resolution\n242, setting the guidelines for future peace negotiations.\nThe resolution called for a peaceful solution, negotiated\nbetween the parties, and based on the following principle:\nIsraeli withdrawal from an unspecified part (to be\nnegotiated) of the territory occupied in June 1967,\nin exchange for which Israel\'s neighbors must recognize\nthe Jewish state, guarantee its security and respect\nIsrael accepted resolution 242, having from the outset\nshown willingness to negotiate a withdrawal from most\nof the territories in exchange for peace. The entire\nArab World rejected the resolution. At a summit of the\nArab League in Khartoum, already in September 1967,\na resolution was adopted containing the following ""three\nnoes"": no to peace with Israel, no to recognition\nof Israel, no to negotiations with Israel. Subsequently,\na number of Arab states have accepted resolution 242,\nhowever in a somewhat liberal interpretation, according\nto which, Israel must unconditionally surrender all\nthe territories conquered in 1967.\n1969 - The War\n1970-72 - PLO and\nThe Six Day War had only just ended, when Egyptian\nforces started shelling Israeli positions along the\nSuez Canal. Israel\'s answer was to conduct air and\ncommando raids accross the canal against Egyptian\ntargets. The Soviet Union sent large amounts of advanced\nweaponry accompanied by Soviet military advisors to\nEgypt, and in the summer of 1969 Nasser declared a\n""war of attrition"", aiming to inflict on\nIsrael the highest possible amount of losses, and\nthereby breaking the will of the Israelis to maintain\nthe occupation of the Sinai Desert. But the strategy\nfailed, the crisis escalated, climaxing in the summer\nof 1970, when Israeli fighter jets downed four MiGs\nflown by Soviet pilotes. The US pushed for a cease-fire,\nwhich then came into effect on August 7, 1970. The\nWar of Attrition ended up claiming several thousand\nlives on either side of the Suez Canal.\nAfter the Six Day War various Palestinian armed groups\nunder the umbrella organization, PLO, continued their\nattacks on Israel. However, the Israelis managed fairly\nquickly to crush the PLO\'s infrastructure in the occupied\nterritories, after which the organization, under its new\nleader, Yasser Arafat, estab-lished itself in neighboring\nJordan. The PLO became an influential power in Jordan,\neven threatening King Hussein\'s regime itself, and fighting\nbroke out between the PLO and the Jordanian army. When\nPFLP, a subgroup of the PLO, hijacked four Western airliners\nand brought them to Jordan, Hussein had enough, and ordered\nhis army to attack the refugee camps that served as bases\nfor the PLO. 2000 guerrillas and many more innocent civillians\nPFLP blows up a hijacked passenger\nplane, Jordan, September 1970.\nThe surviving guerrillas escaped to Syria, where they\nreceived training and equipment from the Syrian army.\nSubsequently, the PLO established itself in neighboring\nLebanon, from where they were able to launch attacks\nagainst residential areas in northern Israel. Following\nan attack by PLO on an Israeli school bus, in which\n12 people, children and their teachers, were killed,\nIsrael in May 1970 launched a large military operation\nin southern Lebanon, creating a 3 kilometer (2 mile)\nwide buffer zone, which temporarily reduced the Palestinian\nmasked Palestinian terrorist\nduring the hostage crisis in\nMunich, September 1972.\nMeanwhile, the PLO developed a new way of placing its\ncause on the international agenda. In 1968-72 Palestinian\nterrorists directed a string of attacks against international\nof passenger planes became a specialty, but also other\ntargets related to Israel or Jews were attacked. One\nof the most spec-tacular attacks was the hostage crisis\nat the 1972 Olympics in Munich, which ended with the\nmassacre of 11 Israeli athletes.\n1973 - The Yom Kippur\nThe cease-fire agreement of August 7, 1970, which\nended the War of Attrition, was broken that very same\nday, when the Egyptians moved advanced Soviet weapons\nsystems all the way up to the Suez Canal. The preparations\nfor the next all-out war against Israel had begun, and\nwere, after Nasser\'s death the same year, taken over by\nhis successor, Anwar sadat. Syria, like Egypt, received\nenormous quantities of weaponry from the Soviet Union,\nand in January 1973 the armies of the two countries were\nplaced under joint command. The Israeli prime minister,\nGolda Meir, and her advisors chose\nto ignore warnings from the Israeli intelligence community\nthat something was in the offing. The country was therefore\ncompletely unprepared, when Egyptian and Syrian forces\non October 6, 1973, on the Jewish holiday of ""Yom\nKippur"", initiated a coordinated surprise attack\nin the Sinai and the Golan.\nIsraeli pontoon bridge crossing the\nSuez Canal, Oktober 1973.\nDuring the first days of the war, the Arabs made significant\nprogress. But as the Israeli forces were mobilized, the\nfortunes of war turned. Having halted the Arab advance,\nIsrael succeeded in breaking through enemy lines on both\nfronts. When the fighting ended on October 25, Israeli\nartillery was within firing range of both\nKairo and Damascus. Militarily, Israel emerged victorious\nfrom the war. But the Arabs had proved that they still\nposed a real threat to Israel, and therefor regarded themselves\nas victors. Almost 2700 Israelis were killed in the Yom\nKippur War, and after public protests the Golda Meir government\nin April 1974 was forces to resign.\nA lightly wounded General Ariel Sharon\nmeet with Defense Minister Moshe\nDayan, the Yom Kippur War, 1973.\nChapter 4 - Peace With The Arabs?']"	['<urn:uuid:95f90d08-1178-4214-bb0e-f5aa15d10bd9>', '<urn:uuid:59786373-ad13-48cd-a7ca-66e0ccce7be7>']	open-ended	direct	concise-and-natural	similar-to-document	comparison	novice	2025-05-12T20:11:36.665104	7	107	3215
99	what difference between art market events biennales art fairs and dewey concept experience	Art market events like biennales and art fairs are conceptualized as units of cultural experience and factors in cultural development, serving as group gatherings that impart meaning and value. Similarly, Dewey's philosophical work extensively explored the nature and function of experience, particularly how things reveal themselves to humans through participation and communication. Both perspectives emphasize experience as a transformative process - art events create cultural experiences through collective participation, while Dewey's theory focuses on how experience enables meaningful interaction and understanding between individuals and their environment.	"['National Academy of Culture and Arts Management, Kyiv.\nKeywords: art market, event, cultural experience, cultural development, art fair, biennale, art exhibition.\nAbstract. The article explores the problems of the art market through the prism of philosophical and cultural consideration of the phenomenon of the event. The attending of art fairs and biennials has become an important part of the cultural life of modern people, so cultural study of these events make it possible to identify them as a factor in the development of contemporary art and a tool to expand the understanding of current art trends in the 21st century by visitors. The author of the article investigates the phenomenon of the events in the context of the art market as a value-semantic factor of the development of contemporary culture. Justifying and developing the thesis on the meaningful aspect of the art market event, the event is considered as a unit of cultural experience and the factor of cultural development. The art has long been instrumental in imparting the meaning and the value to the group meetings of people, so the cultural study of the event as a value and semantic factor in the development of the art market will help to understand visitors’ values and meanings. The conceptualization of the events is possible thanks to cultural anthropology, which is the source of knowledge about the culture, including holidays, customs and rituals. The analysis of the biennials and the art fairs as the events will help to understand the essence of the art market, its gravitas for the development of modern society.\nSerhii Rusakov, Ph.D., Associate Professor, Doctoral Candidate, National Academy of Culture and Arts Management, Kyiv.\nArutyunova, Anna. (2015). Art-rynok v XXI veke: prostranstvo hudozhestvennogo jeksperimenta [The Art Market in the 21st Century: The Space of Artistic Experiment]. Nacional’nyj issledovatel’skij universitet «Vysshaja shkola jekonomiki». Moscow: Izdatel’skij Dom Vysshej shkoly jekonomiki. (in Russian) DOI: https://doi.org/10.7256/1811-9018.2016.10.20570\nBarabanov, Yevgeniy. (2002). Iskusstvo na rynke ili rynok iskusstva? [Art in the market or art market?]. Khudozhestvennyy zhurnal, 46. Retrieved from http://xz.gif.ru/numbers/46/rynok. (in Russian)\nBenhamou-Huet, Judith. (2008). Tsena iskusstva [Art Business 2]. Trans. from French by Julia Ginsburg. Moscow: Artmedia Group.192 p. (in Russian)\nBiennale educational 2017. (2020, February 4). Retrieved from https://www.labiennale.org/en/educational\nBuck, Louisa & Greer, Judith. (2008). Iskusstvo v sobstvennost’. Nastol’naya kniga kollektsionera sovremennogo iskusstva [Owning Art: The Contemporary Art Collector’s Handbook]. Trans. S. Shukaiev, E. Diogot (Ed.). Moscow: ZAO «FEYS FESHN». (in Russian)\nButsykina, Yevheniia. (2020, January 30). Sad tysyachi holosiv: chomu varto vidvidaty biyenale molodoho mystetstva u Kharkovi [The Garden of a Thousand Voices: Why Visit the Biennale of Young Art in Kharkiv]. Retrieved from https://elle.ua/stil-zhizni/novosty/sad-golosv-chomu-vam-sld-vdvdati-bnale-molodogo-mistectva-u-harkov/ (In Ukrainian)\nDanto, A. (2017). Mir iskusstva [World of Art]. Trans. from English A. Shestakov. Moscow: Ad Marginem Press. (in Russian)\nDewey, John. (1934). Art as Experience. New York: Minton, Balch and Company. 353 p.\nDreon, Roberta. (2013). Was Art as Experience Socially Effective? Dewey, the Federal Art Project and Abstract Expressionism. European Journal of Pragmatism and American Philosophy. Vol. 5. Issue 1. P. 279–292. DOI: https://doi.org/10.4000/ejpap.606\nEnjoying the gallery experience. (2020, January 24). Retrieved from https://www.agora-gallery.com/artgalleryblog/enjoying-the-gallery-experience/\nFlier, Andrey Yakovlevich. (2017). Interpretatsiya smysla istorii: sobytiye kak kul’turnyy artefakt [Interpretation of the meaning of history: event as a cultural artifact]. Knowledge. Understanding. Skill. No 4. P. 98–107. (in Russian) DOI: https://doi.org/10.17805/zpu.2017.4.9\nFootnote 11: Volodymyrskyi Market. (2020, February 4). Kyiv Art Week. Retrieved from https://kyivartweek.com/program/opening-footnote-11/ (In Ukrainian)\nGetz, Donald. (2012). Event Studies: Theory, Research and Policy for Planned Events (Events Management). Second Edition. New York: Routledge.\nGorbunova, Alla Glebovna (2013). Sobytiye kak filosofskaya problema: podkhody k opredeleniyu [Event as a philosophical problem: approaches to definition]. Scientific and technical statements of St. Petersburg State Polytechnical University. Humanities and Social Sciences, 1 (167), pp. 129–135. (in Russian)\nGraw, Isabelle. (2016). Vysokaya tsena: iskusstvo mezhdu rynkom i kul’turoy znamenitosti [Der Grobe Pries. Kunst Zwischen Markt Und Celebrity Kultur]. E. Kurova (trans.) Moscow: Ad Marginem Press. (in Russian)\nHaskins, C. (1999). Estetika pragmatizma [Pragmatist Aesthetics]. Collage-2: socio-philosophical and philosophical-anthropological almanac. Moscow: PFR AN. P. 109–120. (in Russian)\nHeidegger, M. (1991). Razgovor na proselochnoy doroge [A conversation on a country road]. Moscow: Vysshaya shkola. (in Russian)\nJones, Caroline. (2017). The Global Work of Art: World’s Fairs, Biennials, and the Aesthetics of Experience. Chicago: University of Chicago Press. DOI: https://doi.org/10.1515/ngs-2017-0023\nKaverina, Ye. (2014). Sobytiynyye kommunikatsii v prostranstve kul’tury XX veka. [Event communications in the cultural space of the XX century]. Society. Environment. Development (Terra Humana). 2 (31). P. 151–155.\nMcAuliffe, Chris. (2014, May 30). Explainer: what is a biennale? Retrieved from https://theconversation.com/explainer-what-is-a-biennale-26516\nPersha virtualʹna vystavka suchasnoyi skulʹptury. Ne ihramy yedynymy [The first virtual exhibition of modern sculpture. Not games are the only ones]. (2018, July 31). Retrieved from https://lenovo.ua/blog/persha-virtualna-vistavka-suchasnoyi-skulpturi-ne-igrami-yedinimi. (In Ukrainian)\nPilyugina, E. V. (2013). «Sobytiye» kak klyuchevoy kontsept postizheniya sovremennoy sotsial’noy real’nosti: semanticheskiye i mental’nyye aktsenty [“Event” as a key concept of comprehension of contemporary social reality: semantic and mental accents]. Studia Humanitatis. 3. Retrieved from www.st-hum.ru. (in Russian)\nPozdnyakov, M. V. (1999). «O sobytii» («Vom Ereignis») M. Khaydeggera [ “On the Event” (“Vom Ereignis”) by M. Heidegger]. Questions of Philosophy, No. 7. pp. 140–157. (in Russian)\nShabes, V. Yа. (1989). Sobytiye i tekst [Event and text]. Moscow: Vyssh. shk. (in Russian)\nPublished: March 10, 2020.\nSection: APPLIED CULTURAL STUDIES AND CULTURAL PRACTICES.', 'The Essential Dewey, Volume 2\nEthics, Logic, Psychology\nPublication Year: 2009\nIn addition to being one of the greatest technical philosophers of the twentieth century, John Dewey (1859-1952) was an educational innovator, a Progressive Era reformer, and one of America\'s last great public intellectuals. Dewey\'s insights into the problems of public education, immigration, the prospects for democratic government, and the relation of religious faith to science are as fresh today as when they were first published. His penetrating treatments of the nature and function of philosophy, the ethical and aesthetic dimensions of life, and the role of inquiry in human experience are of increasing relevance at the turn of the 21st century.\nBased on the award-winning 37-volume critical edition of Dewey\'s work, The Essential Dewey presents for the first time a collection of Dewey\'s writings that is both manageable and comprehensive. The volume includes essays and book chapters that exhibit Dewey\'s intellectual development over time; the selection represents his mature thinking on every major issue to which he turned his attention. Eleven part divisions cover: Dewey in Context; Reconstructing Philosophy; Evolutionary Naturalism; Pragmatic Metaphysics; Habit, Conduct, and Language; Meaning, Truth, and Inquiry; Valuation and Ethics; The Aims of Education; The Individual, the Community, and Democracy; Pragmatism and Culture: Science and Technology, Art and Religion; and Interpretations and Critiques. Taken as a whole, this collection provides unique access to Dewey\'s understanding of the problems and prospects of human existence and of the philosophical enterprise.\nPublished by: Indiana University Press\nTitle Page, Copyright\nDownload PDF (16.0 KB)\nThese volumes were prepared at the Center for Dewey Studies during 1996 and 1997. They were produced from the text of The Collected Works of John Dewey, 1882-1953: The Electronic Edition, edited by Larry A. Hickman (Charlottesville, Virginia: InteLex Corporation, 1996), which is in...\nDownload PDF (94.1 KB)\nIn addition to being one of the greatest technical philosophers of the twentieth century; John Dewey (1859-1952) was also an educational innovator, a Progressive Era reformer, and one of his country\'s last great public intellectuals. In Henry Commager\'s trenchant appraisal, he...\nDownload PDF (19.7 KB)\nPart 1: Habit, Conduct, and Language\nThe Reflex Arc Concept in Psychology (1896)\nDownload PDF (169.2 KB)\nThat the greater demand for a unifying principle and controlling working hypothesis in psychology should come at just the time when all generalizations and classifications are most questioned and questionable is natural enough. It is the very cumulation of...\nInterpretation of Savage Mind (1902)\nDownload PDF (160.3 KB)\nThe psychical attitudes and traits of the savage are more than stages through which mind has passed, leaving them behind. They are outgrowths which have entered decisively into further evolution, and as such form an integral part of the framework of present...\nIntroduction: From Human Nature and Conduct (1922)\nDownload PDF (95.8 KB)\n""Give a dog a bad name and hang him."" Human nature has been the dog of professional moralists, and consequences accord with the proverb. Man\'s nature has been regarded with suspicion, with fear, with sour looks, sometimes with enthusiasm for its possibilities...\nThe Place of Habit in Conduct: From Human Nature and Conduct (1922)\nDownload PDF (570.9 KB)\nHabits may be profitably compared to physiological functions, like breathing, digesting. The latter are, to be sure, involuntary, while habits are acquired. But important as is this difference for many purposes it should not conceal the fact that habits are like functions...\nNature, Communication and Meaning: From Experience and Nature (1925)\nDownload PDF (370.0 KB)\nOf all affairs, communication is the most wonderful. That things should be able to pass from the plane of external pushing and pulling to that of revealing themselves to man, and thereby to themselves; and that the fruit of communication should be participation...\nConduct and Experience (1930)\nDownload PDF (225.8 KB)\n""Conduct,"" as it appears in the title, obviously links itself with the position taken by behaviorists; ""experience,"" with that of the introspectionists. If the result of the analysis herein undertaken turns out to involve a revision of the meaning of both concepts, it will...\nThe Existential Matrix of Inquiry: Cultural: From Logic: The Theory of Inquiry (1938)\nDownload PDF (220.3 KB)\nThe environment in which human beings live, act and inquire, is not simply physical. It is cultural as well. Problems which induce inquiry grow out of the relations of fellow beings to one another, and the organs for dealing with these relations are not only the...\nPart 2: Meaning, Truth, and Inquiry\nThe Superstition of Necessity (1893)\nDownload PDF (224.0 KB)\nLest my title give such offense as to prejudice unduly my contention, I may say that I use the term in the way indicated by its etymology: as a standing-still on the part of thought; a clinging to old ideas after those ideas have lost their use, and hence, like all...\nThe Problem of Truth (1911)\nDownload PDF (670.4 KB)\nTo the lay mind it is a perplexing thing that the nature of truth should be a vexed problem. That such is the case seems another illustration of Berkeley\'s remark about the proneness of philosophers to throw dust in their own eyes and then complain that they...\nLogical Objects (1916)\nDownload PDF (112.6 KB)\nThe exigencies which produced this paper will, I hope, render apologies unnecessary. I am only too conscious that it is not a paper for discussion, but a memorandum of certain positions which might be developed. I would suggest, however, that the following...\nAnalysis of Reflective Thinking: From How We Think (1933)\nDownload PDF (167.2 KB)\nWhen a situation arises containing a difficulty or perplexity; the person who finds himself in it may take one of a number of courses. He may dodge it, dropping the activity that brought it about, turning to something else. He may indulge in a flight of fancy...\nThe Place of Judgment in Reflective Activity: From How We Think (1933)\nDownload PDF (128.9 KB)\nFrom one point of view the whole process of thinking consists of making a series of judgments that are so related as to support one another in leading to a final judgment-the conclusion. In spite of this fact, we have treated reflective activity as a whole, first, because...\nGeneral Propositions, Kinds, and Classes (1936)\nDownload PDF (123.7 KB)\nIn an earlier article I called attention to the fact that Mill stated that since abstract terms are sometimes singular and sometimes general, it might be better to put them in a ""class apart."" I argued that this class apart was that of universal if-then propositions; abstract terms...\nThe Problem of Logical Subject-Matter: From Logic: The Theory of Inquiry (1938)\nDownload PDF (271.2 KB)\nContemporary logical theory is marked by an apparent paradox. There is general agreement as to its proximate subject-matter. With respect to this proximate subject-matter no period shows a more confident advance. Its ultimate subject-matter, on the other hand, is...\nThe Pattern of Inquiry: From Logic: The Theory of Inquiry (1938)\nDownload PDF (237.9 KB)\nThe first chapter set forth the fundamental thesis of this volume: Logical forms accrue to subject-matter when the latter is subjected to controlled inquiry. It also set forth some of the implications of this thesis for the nature of logical theory. The second and third...\nMathematical Discourse: From Logic: The Theory of Inquiry (1938)\nDownload PDF (298.0 KB)\nThe ability of any logical theory to account for the distinguishing logical characteristics of mathematical conceptions and relations is a stringent test of its claims. A theory such as the one presented in this treatise is especially bound to meet and pass this test. For it has the...\nThe Construction of Judgment: From Logic: The Theory of Inquiry (1938)\nDownload PDF (60.1 KB)\nIn terms of the ideas set forth in the last chapter, judgment may be identified as the settled outcome of inquiry. It is concerned with the concluding objects that emerge from inquiry in their status of being conclusive. Judgment in this sense is distinguished from...\nGeneral Theory of Propositions: From Logic: The Theory of Inquiry (1938)\nDownload PDF (79.4 KB)\nJudgment has been analyzed to show that it is a continuous process of resolving an indeterminate, unsettled situation into a determinately unified one, through operations which transform subject-matter originally given. Judgment, in distinction from propositions...\nPropositions, Warranted Assertibility, and Truth (1941)\nDownload PDF (283.0 KB)\nI propose in what follows to restate some features of the theories I have previously advanced on the topics mentioned above. I shall shape this restatement on the basis of ascriptions and criticisms of my views found in Mr. Russell\'s An Inquiry into Meaning and Truth. I am...\nImportance, Significance, and Meaning (1949)\nDownload PDF (196.3 KB)\nThis essay marks an attempt to develop a number of considerations which are reasonably fundamental in the theory of knowing and of what it is to be known, which form the substance of a recently published collection of articles by A. F. Bentley and the present writer...\nPart 3: Valuation and Ethics\nEvolution and Ethics (1898)\nDownload PDF (239.9 KB)\nTo a strictly logical mind the method of the development of thought must be a perplexing, even irritating matter. Its course is not so much like the simple curve described by a bullet as it speeds its way to a mark, as it is like the devious tacking of a sail boat upon a heavy...\nThe Logic of Judgments of Practice (1915)\nDownload PDF (815.6 KB)\nIn introducing the discussion, I shall first say a word to avoid possible misunderstandings. It may be objected that such a term as ""practical judgment"" is misleading; that the term ""practical judgment"" is a misnomer, and a dangerous one, since all judgments by their...\nValuation and Experimental Knowledge (1922)\nDownload PDF (351.5 KB)\nPlato long ago called notice to the disadvantage of written discussion as compared with oral. The printed page does not respond to questions addressed it. It will not share in conversation. But there is a disadvantage for the writer as well as for the reader. He is never...\nValue, Objective Reference, and Criticism (1925)\nDownload PDF (243.2 KB)\nIn some writings of mine on judgments of value considered as evaluations, there was no attempt to reach or state any conclusion as to the nature of value itself. The position taken was virtually this: No matter what value is or is taken to be, certain traits of evaluative...\nThe Ethics of Animal Experimentation (1926)\nDownload PDF (68.9 KB)\nDifferent moralists give different reasons as to why cruelty to animals is wrong. But about the fact of its immorality there is no question, and hence no need for argument. Whether the reason is some inherent right of the animal, or a reflex bad effect upon the...\nPhilosophies of Freedom (1928)\nDownload PDF (286.4 KB)\nA recent book on Sovereignty concludes a survey of various theories on that subject with the following words: ""The career of the notion of sovereignty illustrates the general characteristics of political thinking. The various forms of the notion have been apologies...\nThree Independent Factors in Morals (1930)\nDownload PDF (123.9 KB)\nThere is a fact which from all the evidence is an integral part of moral action which has not received the attention it deserves in moral theory: that is the element of uncertainty and of conflict in any situation which can properly be called moral. The conventional...\nThe Good of Activity: From Human Nature and Conduct (1922)\nDownload PDF (128.3 KB)\nConduct when distributed under heads like habit, impulse and intelligence gets artificially shredded. In discussing each of these topics we have run into the others. We conclude, then, with an attempt to gather together some outstanding considerations about...\nMoral Judgment and Knowledge: From Ethics (1932)\nDownload PDF (274.2 KB)\nThat reflective morality, since it is reflective, involves thought and knowledge is a truism. The truism raises, however, important problems of theory. What is the nature of knowledge in its moral sense? What is its function? How does it originate and operate? To...\nThe Moral Self: From Ethics (1932)\nDownload PDF (308.8 KB)\nThe self has occupied a central place in the previous discussions, in which important aspects of the good self have been brought out. The self should be wise or prudent, looking to an inclusive satisfaction and hence subordinating the satisfaction of an immediately urgent...\nPart 4: Interpretations and Critiques\nDemocracy and America: From Freedom and Culture (1939) (On Thomas Jefferson)\nDownload PDF (192.3 KB)\nI make no apology for linking what is said in this chapter with the name of Thomas Jefferson. For he was the first modern to state in human terms the principles of democracy: Were I to make an apology, it would be that in the past I have concerned myself unduly if a...\nEmerson—The Philosopher of Democracy (1903) (On Ralph Waldo Emerson)\nDownload PDF (102.5 KB)\nIt is said that Emerson is not a philosopher. I find this denegation false or true according as it is said in blame or praise-according to the reasons proffered. When the critic writes of lack of method, of the absence of continuity, of coherent logic, and, with the old story...\nPeirce\'s Theory of Quality (1935) (On Charles S. Peirce)\nDownload PDF (126.9 KB)\nThe questions raised in Mr. Goudge\'s criticism of Peirce on the nature of the ""given,"" are of high importance in the contemporary state of philosophy in which the problems of the given, on one hand, and of universals and essences, on the other, bulk so large...\nWhat Pragmatism Means by ""Practical"" (1907) (On William James)\nDownload PDF (229.3 KB)\nPragmatism, according to Mr. James, is a temper of mind, an attitude; it is also a theory of the nature of ideas and truth; and, finally, it is a theory about reality. It is pragmatism as method which is emphasized, I take it, in the subtitle, ""a new name for some old ways...\nVoluntarism and the Roycean Philosophy (1916) (On Josiah Royce)\nDownload PDF (125.2 KB)\nI am not about to inflict upon you a belated discovery that voluntarism is an integral factor in the Roycean theory of knowledge. Were it not obvious of itself, we have the emphatic utterances of Professor Royce himself in his address to this Association twelve years ago...\nPerception and Organic Action (1912) (On Henri Bergson)\nDownload PDF (345.3 KB)\nEvery reader of Bergson-and who to-day is not reading Bergson-is aware of a twofold strain in his doctrine. On the one hand, the defining traits of perception, of commonsense knowledge and science are explained on the ground of their intimate connection with...\nThe Existence of the World as a Logical Problem (1915) (On Bertrand Russell)\nDownload PDF (174.5 KB)\nOf the two parts of this paper the first is a study in formal analysis. It attempts to show that there is no problem, logically speaking, of the existence of an external world. Its point is to show that the very attempt to state the problem involves a self-contradiction: that...\nWhitehead\'s Philosophy (1937) (On Alfred North Whitehead)\nDownload PDF (103.3 KB)\nMr. Whitehead\'s philosophy is so comprehensive that it invites discussion from a number of points of view. One may consider one of the many special topics he has treated with so much illumination or one may choose for discussion his basic method. Since the latter...\nDownload PDF (115.1 KB)\nAbout the Author\nDownload PDF (7.0 KB)\nPage Count: 448\nPublication Year: 2009']"	['<urn:uuid:246bcfcb-21a8-46c4-a035-6b3bd4239795>', '<urn:uuid:3f08e6e0-17d1-4c03-85d5-e8072a0c973b>']	open-ended	direct	long-search-query	distant-from-document	comparison	novice	2025-05-12T20:11:36.665104	13	86	3438
100	I have leg pain when walking. What tests can check my blood flow?	To check blood flow in your legs, doctors can perform several tests. They can do simple, painless, and inexpensive non-invasive tests in the office. For a more detailed look, they may use extremity angiography, which is a hospital test that uses x-rays and special dye to see inside your arteries. The test involves inserting a thin tube called a catheter through your groin to inject the dye. Advanced imaging like CT or MRA, or catheter-based angiography might also be needed to determine the extent and severity of any blood flow problems.	"[""Extremity angiographyAngiography of the extremity; Peripheral angiography; Lower extremity angiogram; Peripheral angiogram; Arteriography of the extremity; PAD - angiography; Peripheral artery disease - angiography\nExtremity angiography is a test used to see the arteries in the hands, arms, feet, or legs. It is also called peripheral angiography.\nAngiography uses x-rays and a special dye to see inside the arteries. Arteries are blood vessels that carry blood away from the heart.\nHow the Test is Performed\nThis test is done in a hospital. You will lie on an x-ray table. You may ask for some medicine to make you sleep and relax (sedative).\n- The health care provider will shave and clean an area, most often in the groin.\n- A numbing medicine (anesthetic) is injected into the skin over an artery.\n- A needle is placed into that artery.\n- A thin plastic tube called a catheter is passed through the needle into the artery. The doctor moves it into the area of the body being studied. The doctor can see live images of the area on a TV-like monitor, and uses them as a guide.\n- Dye flows through the catheter and into the arteries.\n- X-ray images are taken of the arteries.\nCertain treatments can be done during this procedure. These treatments include:\n- Dissolving a blood clot with medicine\n- Opening a partially blocked artery with a balloon\n- Placing a small tube called a stent into an artery to help hold it open\nThe health care team will check your pulse (heart rate), blood pressure, and breathing during the procedure.\nThe catheter is removed when the test is done. Pressure is placed on the area for 10 to 15 minutes to stop any bleeding. A bandage is then put on the wound.\nThe arm or leg where the needle was placed should be kept straight for 6 hours after the procedure. You should avoid strenuous activity, such as heavy lifting, for 24 to 48 hours.\nHow to Prepare for the Test\nYou should not eat or drink anything for 6 to 8 hours before the test.\nYou may be told to stop taking certain medicines, such as aspirin or other blood thinners for a short while before the test. Never stop taking any medicines unless told to do so by your provider.\nMake sure your provider knows about all the medicines you take, including those you bought without a prescription. This includes herbs and supplements.\nTell your provider if you:\n- Are pregnant\n- Are allergic to any medicines\n- Have ever had an allergic reaction to x-ray contrast material, shellfish, or iodine substances\n- Have ever had any bleeding problems\nHow the Test will Feel\nThe x-ray table is hard and cold. You may want to ask for a blanket or pillow. You may feel some stinging when the numbing medicine is injected. You may also feel some pressure as the catheter is moved.\nThe dye can cause a feeling of warmth and flushing. This is normal and most often goes away in a few seconds.\nYou may have tenderness and bruising at the site of the catheter insertion after the test. Seek immediate medical help if you have:\n- Bleeding that doesn't go away\n- Severe pain in an arm or leg\nWhy the Test is Performed\nYou may need this test if you have symptoms of a narrowed or blocked blood vessel in the arms, hands, legs, or feet.\nThe test may also be done to diagnose:\n- Swelling or inflammation of the blood vessels (vasculitis)\nThe x-ray shows normal structures for your age.\nWhat Abnormal Results Mean\nAn abnormal result is commonly due to narrowing and hardening of the arteries in the arms or legs from plaque buildup (hardening of the arteries) in the artery walls.\nThe x-ray may show a blockage in the vessels caused by:\n- Aneurysms (abnormal widening or ballooning of part of an artery)\n- Blood clots\n- Other diseases of the arteries\nAbnormal results may also be due to:\n- Inflammation of the blood vessels\n- Injury to the blood vessels\n- Thromboangiitis obliterans (Buerger disease)\n- Takayasu disease\nComplications may include:\n- Allergic reaction to the contrast dye\n- Blood clot that travels to the lungs\n- Damage to the blood vessel as the needle and catheter are inserted\n- Excessive bleeding or a blood clot where the catheter is inserted, which can reduce blood flow to the leg\n- Heart attack or stroke\n- Hematoma, a collection of blood at the site of the needle puncture\n- Injury to the nerves at the needle puncture site\n- Kidney damage from the dye\n- Injury to the blood vessels being tested\nThere is low-level radiation exposure. However, most experts feel that the risk of most x-rays low compared with benefits. Pregnant women and children are more sensitive to the risks of the x-ray.\nAmerican Heart Association. Peripheral angiogram. Updated October, 2016. www.heart.org/HEARTORG/Conditions/HeartAttack/SymptomsDiagnosisofHeartAttack/Peripheral-Angiogram_UCM_441649_Article.jsp#.WFkD__l97IV. Accessed December 20, 2016.\nJackson JE, Meaney JFM. Angiography: principles, techniques and complications. In: Adam A, Dixon AK, Gillard JH, Schaefer-Prokop CM, eds. Grainger & Allison's Diagnostic Radiology: A Textbook of Medical Imaging. 6th ed. Philadelphia, PA: Elsevier Churchill Livingstone; 2015:chap 84.\nKaufman JA. Fundamentals of angiography. In: Kaufman JA, Lee MJ, eds. Vascular and Interventional Radiology: The Requisites. Philadelphia, PA: Elsevier Saunders; 2014:chap 2.\nReview Date: 11/11/2016\nReviewed By: Mary C. Mancini, MD, PhD, Department of Surgery, Louisiana State University Health Sciences Center-Shreveport, Shreveport, LA. Review provided by VeriMed Healthcare Network. Also reviewed by David Zieve, MD, MHA, Medical Director, Brenda Conaway, Editorial Director, and the A.D.A.M. Editorial team."", 'Home > Conditions We Treat > Vascular Conditions\nVascular care is the medical and surgical treatment of problems affecting the circulatory system (outside of the heart and brain) including arteries and veins, especially in the legs, arms, neck, and kidneys.\nPeripheral vascular disease (PVD) refers to blockages of the arteries outside of the heart composed of cholesterol plaques. As the condition progresses, patients frequently experience intolerance to walking because of discomfort or weakness in the legs. Worsening of the disease may result in pain at rest or sores in the legs. Ultimately, a person might require amputation of the affected limb.\nPVD is more common with increasing age, in smokers, and in diabetic patients. Simple, painless and inexpensive non-invasive tests can be performed in the office to detect the condition. Further evaluation may require advanced (CT or MRA) imaging. Catheter-based angiography may also be necessary to define the extent and severity of the disease.\nPVD is typically first treated with risk factor modification: smoking cessation, cholesterol lowering therapy, aspirin and possible other medications. A search for other organs affected by cholesterol plaques may be indicated. Rehabilitation is recommended for PVD patients, and may be the only form of treatment needed.\nMedications may help alleviate symptoms related to PVD. Patients who have inadequate relief with conservative management or who are at risk for sores or limb loss are frequently evaluated with the goal of aggressively improving blood flow.\nSurgery was once the only therapy available to achieve this goal. Now, minimally invasive therapy, including balloon angioplasty and stent implantation, has great success for many patients. This is typically an outpatient procedure that offers immediate relief for many patients.\nStroke is the third-leading cause of death and the leading cause of disability in the U.S. A stroke is an obstruction of blood flow to a part of the brain that results in that part of the brain dying. Symptoms of a stroke include weakness, loss of control, or numbness in an extremity, visual disturbances, or difficultly with speech. Stroke symptoms that resolve within one day are called ""transient ischemic attacks,"" or TIAs (frequently also referred to as ""mini-strokes"").\nFollowing a TIA, there is a very high risk of a completed stroke within the next year. Strokes most frequently, however, occur suddenly with no warning symptoms.\nAbout one in four strokes are the result of significant cholesterol plaque buildup (atherosclerosis) in the carotid arteries. Patients with other atherosclerotic disease are frequently screened for carotid artery disease.\nOften, physical examination findings suggest the presence of carotid artery disease. Screening for carotid artery disease is quite simple and painless. An ultrasound examination of the carotid arteries takes about 20 minutes and can usually be completed at the time of an office visit.\nTreatment of carotid artery disease begins with risk factor management including smoking cessation, blood pressure control, treatment of diabetes, and cholesterol lowering. Medical therapy most often involves blood thinners.\nCarotid surgery may be performed to remove the cholesterol plaque. More recently, a less invasive option has become available in the form of carotid stents. This procedure has been shown to have similar results to surgery in selected patients.']"	['<urn:uuid:ada4c2b2-86f3-4854-8854-a88703106cd2>', '<urn:uuid:3be8fa2a-88e2-4fc0-ab6e-bd4e3217774c>']	open-ended	with-premise	concise-and-natural	similar-to-document	three-doc	novice	2025-05-12T20:11:36.665104	13	91	1461
