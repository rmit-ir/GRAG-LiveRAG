qid	question	answer	context	document_ids	question_factuality	question_premise	question_phrasing	question_linguistic_variation	question_multi-doc	user_expertise-categorization	generation_timestamp	question_length	answer_length	context_length
1	compare traditional land use toronto toledo	Traditional Indigenous land use in both Toronto and Toledo historically centered around water bodies and trading. The Toronto area was the traditional land of the Huron-Wendat, Seneca, and Mississaugas of the Credit River, who maintained relationships with the land through cultural and spiritual practices. Similarly, Toledo, located near the mouth of the Maumee River and Lake Erie, was an active site of Indigenous travel, trade, and meeting for multiple tribes including the Erie, Kickapoo, Odawa, and Seneca. Both areas are now working to honor these traditional uses - Toronto through initiatives like the Indigenize or Die workshop series that aims to recreate urban spaces respecting traditional land usage for ceremonies and gatherings, while the Toledo Museum of Art acknowledges the Indigenous peoples' role as stewards of the land and commits to understanding the complicated history of colonialism in their space.	['“We wish to acknowledge this land on which the University of Toronto operates. For thousands of years, it has been the traditional land of the Huron-Wendat, the Seneca, and most recently, the Mississaugas of the Credit River. Today, this meeting place is still the home to many Indigenous people from across Turtle Island and we are grateful to have the opportunity to work on this land.”\nFrom the University of Toronto Students’ Union’s Annual General Meeting to special guest speaker events, the Statement of Acknowledgement of Traditional Land has become crystallized into words that are frequently used to open functions on campus. Perhaps, it was last year’s release of the Truth and Reconciliation Commission’s final report that sparked a marked rise in our acknowledgment of how the places we call home are the original homes of Indigenous peoples: the First Nations, the Métis, and the Inuit. After over a 100 years of residential schools and the intergenerational degradation of the Indigenous sense of home, it is timely that we — the settler Canadian majority — make a change to our cultural ethos; that we enable ‘re-Indegenization’; and that we assess what home must mean to those who were here long before us.\nTo understand the Indigenous sense of home, we must first conceptualize its negation: homelessness. However, the popular understanding of homelessness as the absence of physical shelter is Western-centric and limited in the Indigenous context. Indeed, a more intersectional and nuanced definition is needed.\nJesse Thistle, a Trudeau Scholar of Métis-Cree identity who was formerly homeless and addicted to drugs, leads the development of a definition of Indigenous homelessness that is set to be finalized by 2017 on behalf of the Canadian Observatory on Homelessness. Rather than homelessness in terms of physicality, property, or possessions, which aligns with the government’s definition, this new definition consults Indigenous worldviews and methodology by centring on ‘all my relations.’\nIndigenous homelessness is “fully described and understood through Indigenous worldviews as individuals, families, and communities isolated from their relationship to land, family, kin, each other, place, cultures, languages, and identities, and who do not possess or have the ability to culturally, spiritually, emotionally, and/or physically reconnect with their Indigeneity or lost relationships.” The work further identified 10 typologies that describe the nuanced experiences of Indigenous peoples and the loss of their relationships.\nThis revolutionary conception of homelessness based on Indigeneity can help contextualize the history of colonization and its impacts on Indigenous peoples and their sense of home. This work is useful for settler Canadians seeking to address their own complicity in the disturbance of that home.\nHome is complex, layered, and multidimensional in the Indigenous experience. Consider the individual story of Darlene Necan: after three years of using donated materials, Necan finally completed the building of her home this year in the unorganized township of Savant Lake in Northern Ontario on grounds which were significant to her childhood. However, this individual story of accomplishment includes incredible struggle that speaks to the broader issue of Indigenous homelessness.\nTwenty kilometers south of Savant Lake is Necan’s reserve, belonging to the Ojibway Nation of Saugeen. Due to a shortage in housing, she was unable to find adequate shelter on-reserve. Furthermore, Necan claims that her own Indigenous government and Chief lacks the willingness and accountability to help off-reserve members like her. Comparatively, off-reserve housing is not much better. In the nearest city of Thunder Bay, she complains of high rents and a “vicious cycle of welfare” that renders impoverished Indigenous peoples “paralyzed in the Ontarian provincial system.”\nWith hostility from both the reserve government and the provincial government, she opts for her own housing at a place known to her past: Savant Lake. Even on Savant Lake, however, she met legal challenges from the government, which claimed that the Public Lands Act forbade her from building a home there. Eventually, this threat was dropped. Thus, in 2016, she was able to complete and move into her home and reconnect with her Anishinaabe, familial roots.\nBoth urban living and reserve living highlight the intersections of colonialism, patriarchy, and capitalism that uniquely hurt and displace Indigenous women. Necan considers urban living to be a source of “desperation.” She has considered selling drugs to make enough money in order to survive. On-reserve, she and other women have been attempting to reclaim ownership of the land. In both urban and on-reserve cases, Indigenous women fight the hardest for homes in societies that actively marginalize them.\nIn a historical context, the primary example of disenfranchisement and Indigenous homelessness is residential schools. This colonial assault not only physically separated children from their communities into abusive conditions but also enacted cultural genocide. Those who survived and returned to their communities were given housing, but nonetheless remained ‘homeless’ in their inability to practice their culture, speak their language, enact traditional economic methods, or connect with their families. Likewise, the is ‘Sixties Scoop’ re-enacted the displacement of Indigenous children into the child welfare system, and further degraded any sense of stable, traditional, Indigenous home.\nThe colonial targeting of Indigenous children has left a profound impact in terms of intergenerational trauma, such as family violence and drug addiction; it has exacerbated systemic gaps in terms of employment, education, and health that undermine Indigenous communities and homes.\nThe alarming rate at which Indigenous women experience violence — especially given the high numbers of missing and murdered women today — has devastated Indigenous communities, given the centrality of mothers and girls in such societies. This historical culmination of separation, isolation, and trauma have often manifested in extreme forms of hopelessness, such as the suicide crisis in Attawapiskat earlier this year. Clearly, the deprivation of relationships and culture continue to contribute to Indigenous homelessness.\nThe damage to Indigenous homes does not stem merely from historical forms of trauma but also from direct aggression upon homes that are sanctioned by government and industry. For hundreds of years, ‘nation-building’ and ‘economic development’ have rationalized colonization and unwarranted encroachment on Indigenous land. This continues today. For example, the proposed chromite mining in Northern Ontario’s Ring of Fire risks water poisoning and disables Indigenous peoples from exercising their right to fish on their traditional lands.\nNecan’s cousin, Neecha, urges that future Indigenous generations should still have access to their land and inhabit it in their traditional ways; she also criticizes chief leaderships that permit corporate extractions of the land in the name of development.\nPerhaps the most urgent threat to Indigenous homes is the continued pursuit of pipelines to transport oil and natural gas. The inherent risk of ruptured pipelines disproportionately concerns Indigenous communities and their attachment to local ecosystems and the environment. Furthermore, a continued reliance on a fossil fuel economy contributes to severe climate change, which deeper undermines the Indigenous connection to environment and, at its worst, causes natural disasters that create ‘emergency crisis homelessness.’\nIndigenous struggles for home concerns the city life too. With up to 37,000 Indigenous Torontonians, who are eight times more likely than non-Indigenous Canadians to endure physical homelessness, the latter have a responsibility to bridge the gap between their isolated urban consciousness and urban Indigenous issues.\nTo this end, the Indigenize or Die workshop series informs the City of Toronto’s Park and Public Realm Plan — a plan that intends to enrich public spaces. The inclusion of an Indigenous lens aims to recreate urban space in such a way that respects traditional usage of the land — for ceremonies and gatherings around fires, as an example — and include Indigenous voices in urban planning.\nWith regard to such momentum for urban re-Indigenization, the Minister of Indigenous and Northern Affairs Carolyn Bennett acknowledges the invisibility of Indigenous Torontonians. She concedes that there must be more ‘on-the-land’ space for further exposure. For example, the Native Canadian Centre of Toronto and the First Nations House at the University of Toronto serve as prime educational hubs for settler Canadians. Street signs near campus are also examples of reclaimed Indigeneity: for example, the Anishnaabe name ‘Isphadinaa’ for ‘Spadina.’ It is crucial that we cede the urban space, in which public consciousness is most impressionable, to Indigenous heritages and the knowledge that we are settlers on their home lands.\nThe Truth and Reconciliation Commission demands that Canadians address the dark history of residential schools and the impact of intergenerational trauma on Indigenous homes. Of course, our apologies are insufficient; there must be action. The systemic gaps in education, employment, and health are stark; they illustrate the greater problem of Indigenous homelessness by which Indigenous peoples are deprived not only of physical shelter but also of their connection to land, culture, and spirituality.\nBennett also affirms that beyond just economic resources as the basis for successful Indigenous self-determination, there must be “secure, personal cultural identity.” In other words, only a reconnection to language and culture can empower an Indigenous sense of confidence and home.\nHowever, cultural vibrancy cannot exist when we Canadians continue to commit aggression, settlement, and displacement without the consent of Indigenous peoples. As we extract resources, pursue economic development, and contribute to climate change, we continue colonization and sanction Indigenous homelessness.\nIt is our responsibility, then, to act upon the knowledge that we are fundamentally settlers on lands that do not belong to us. The struggles, leaderships, and triumphs of Thistle and Necan accentuate the issue of Indigenous homelessness and our complicity in its continued existence today.\nUltimately, home is more than just housing: it is about preserving our core selves and relationships. If we are to reconcile our relationship with Indigenous peoples, we must enable the self-determination of their most valued relationships — with communities, cultures, spirit, and land.', 'The Toledo Museum of Art is located on ancestral homelands of the Erie, Kickapoo, Odawa, and Seneca. We recognize that many other Native tribes have also conducted trade on, and called this region their home, including the Lenape, Miami, Ojibway, Peoria, Potawatomi, Sauk, Shawnee, and the Wyandotte. We at the Museum acknowledge and honor the past, present, and future lives of Indigenous peoples in the Toledo area and thank them for their resilience as stewards of the land on which the Museum’s campus now resides.\nAs a cultural institution, it is our responsibility and aim to understand and continually share knowledge about the complicated history of colonialism in the space we now occupy, the peoples who were here before us, and the enduring presence and diversity of Indigenous peoples in our region. In keeping with the Museum’s vision of creating a culture of belonging, we commit to building meaningful relationships with Indigenous communities and working to dismantle the systemic invisibility of Indigenous peoples. In devising this statement and writing the history of this land, we have learned a lot, but this is not the end of the process. Through shared knowledge and exchange, we will continue to learn and enrich this history as we come to know more.\nIndigenous Ohio and The Toledo Museum of Art\nThe area that is now Ohio (named for the Iroquois word for the Ohio River: “it is beautiful”) has seen Indigenous cultures going back to at least 7500 BCE. Mound Building Woodland peoples like the sophisticated Fort Ancient and Adena cultures built earthen effigy and burial mounds in southern Ohio beginning around 2,000 years ago. Two prehistoric mounds were known to exist in the Toledo vicinity: on the south bank of Swan Creek at the intersection of Oliver and Clayton streets, and on the right bank of the Maumee River at the intersection of Fort and Fassett streets (“Maumee” is the French colonial name for the Miami Nation and the Miami River). Later Native cultures in the state consisted of four major cultural divisions: the Lenape in the Muskingum Valley in eastern Ohio; the Miami in the Miami Valley, western Ohio; the Odawa and Wyandotte in the Maumee River Valley of northwest Ohio; and the Shawnee along the Scioto River in southern Ohio.\nEuropeans took over Indian lands using the ancient Roman concept of terra nullius—a term meaning “nobody’s land”—in which the land was assumed to be unoccupied. When it became clear the land was, in fact, inhabited, colonists justified its seizure by declaring the land as “uncultivated.” With the growing numbers of European settlers in the 18th century, Ohio was deemed “Indian Territory” and many Indigenous peoples from the eastern part of the country were displaced to Ohio lands. After the Revolutionary War (1765–1783), the U.S. Government entered into treaties with Indian Nations—treaties that it later violated—in order to “lawfully” shrink this territory. With the 1830 Indian Removal Act, most Indigenous peoples living east of the Mississippi River, including Ohio, were forcibly removed by the U.S. Government to reservations in the West during the 19th century. As a result, while more than a dozen tribes have ties to Ohio lands, there are now no federally recognized tribes in the state.\nLocated near the mouth of the Maumee River, at the edge of Lake Erie, present-day Toledo was an active site of Indigenous travel, trade, and meeting. The Museum was built on land ceded to the United States in the 1795 Treaty of Greenville. In 1826, the land—which at that time formed part of the Michigan Territory—was sold by the U.S. at the direction of President John Quincy Adams to John G. Forbes. Nearly a decade later, during the property dispute that was the Toledo War, Forbes sold the land to Edward Wade on August 8, 1835. Shortly thereafter, on November 14, 1835, Wade sold the land to Maurice Wakeman and Jesup W. Scott, grandfather to one of the Museum’s founders, Florence Scott Libbey. In 1908, Florence and her husband, Edward Drummond Libbey, donated the land as the site for the present Toledo Museum of Art. Additional parcels of land were added in the ensuing years to form our nearly 40-acre campus. While the known history of the land on which the Toledo Museum of Art resides is complicated and incomplete, we are committed to further understanding this history and the role museums have played in colonization with the goal of dismantling the systemic invisibility of Indigenous peoples.\nLand Acknowledgment FAQ\nWhat is a Land Acknowledgment?\nA land acknowledgment is a formal statement that recognizes and honors the long-standing relationship between Indigenous peoples and their ancestral or traditional territories.\nWhy do we acknowledge the land?\nIndigenous communities have practiced acknowledgment for generations as signs of reverence and respect. As an institution dedicated to telling the artistic histories of different cultures and times, it is important that we at the Museum understand our own history and that of the lands we now occupy. Acknowledging the land brings awareness to this complex history and to the ongoing impact of colonialism, prompting greater understanding and reflection, as well as providing a space to restore and create relationships with Indigenous peoples connected to these lands.\nHow was the TMA land acknowledgment developed?\nCreated in accordance with Indigenous protocol, this land acknowledgment is part of a larger initiative at the Toledo Museum of Art to foster a culture of belonging and to cultivate awareness of and respect for Indigenous communities and culture. After conducting research into the history of Toledo and the Museum’s land formerly owned by the Scott and Libbey families, we shared our statement with the Black Swamp InterTribal Foundation for feedback. With the establishment of this statement, the Toledo Museum of Art acknowledges the history of the land on which it resides and reaffirms its commitment to working with the Indigenous community—both local and from afar—to broaden the culture and knowledge that we share in our galleries.\nAdditional Resources on Land Acknowledgment\nHonoring Original Indigenous Inhabitants: Land Acknowledgment. National Museum of the American Indian. Accessed on 17 November 2020.\n“Honor Native Land: A Guide & Call to Acknowledgment.” U.S. Department of Arts & Culture. Accessed on 8 October 2020.\nNative Land: Territory Acknowledgment. Accessed on 8 October 2020.\nNative Governance Center. Accessed on 8 October 2020.\nBlack Swamp InterTribal Foundation. Accessed on 8 October 2020\nOhio History Central. Ohio History Connection. Accessed on 8 October 2020\nThe Forgotten History of Ohio’s Indigenous Peoples. Midstory. Accessed 10 November 2020.']	['<urn:uuid:86b5da66-d0f2-4147-a143-20a0661c0aa8>', '<urn:uuid:87650a51-1822-4d7c-84f6-80ab90077b96>']	open-ended	with-premise	short-search-query	distant-from-document	comparison	expert	2025-05-12T21:31:42.699391	6	140	2691
2	I'm studying basic science and wondered - what's the main difference between how regular objects move compared to tiny things like electrons in atoms?	For regular objects, Newton's simple equation F = ma (force equals mass times acceleration) was used for about 200 years to describe their motion. However, for subatomic particles like electrons, a completely different approach was needed because they behave as both waves and particles. This required Schrödinger's complex wave equation, which describes how electron waves change over time and predicts probability regions where electrons might be found, rather than exact positions like with regular objects. The Schrödinger equation bridges the gap between classical Newtonian physics, which breaks down at the atomic level, and quantum mechanics, which governs the behavior of tiny particles.	"['The Schrödinger Equation1. The Schrōdinger Equation. The fundamental equation of motion in quantum mechanics is Erwin Schrōdinger\'s famous wave equation that describes the evolution in time of his wave function ψ.\niℏ δψ / δt = H ψ (1)Max Born interpreted the square of the absolute value of Schrōdinger\'s wave function |ψn |2 (or < ψn | ψn > in Dirac notation) as providing the probability of finding a quantum system in a particular state n. As long as this absolute value (in Dirac bra-ket notation) is finite,\n< ψn | ψn > ≡ ∫ ψ* (q) ψ (q) dq < ∞, (2)then ψ can be normalized, so that the probability of finding a particle somewhere < ψ | ψ > = 1, which is necessary for its interpretation as a probability. The normalized wave function can then be used to calculate ""observables"" like the energy, momentum, etc. For example, the probable or expectation value for the position r of the system, in configuration space q, is\n< ψ | r | ψ > = ∫ ψ* (q) r ψ (q) dq. (3)2. The Principle of Superposition. The Schrōdinger equation (1) is a linear equation. It has no quadratic or higher power terms, and this introduces a profound - and for many scientists and philosophers a disturbing - feature of quantum mechanics, one that is impossible in classical physics, namely the principle of superposition of quantum states. If ψa and ψb are both solutions of equation (1), then an arbitrary linear combination of these,\n| ψ > = ca | ψa > + cb | ψb >, (4)with complex coefficients ca and cb, is also a solution. Together with Born\'s probabilistic (statistical) interpretation of the wave function, the principle of superposition accounts for the major mysteries of quantum theory, some of which we hope to resolve, or at least reduce, with an objective (observer-independent) explanation of irreversible information creation during quantum processes. Observable information is critically necessary for measurements, though observers can come along anytime after the information comes into existence as a consequence of the interaction of a quantum system and a measuring apparatus. The quantum (discrete) nature of physical systems results from there generally being a large number of solutions ψn (called eigenfunctions) of equation (1) in its time independent form, with energy eigenvalues En.\nH ψn = En ψn, (5)The discrete spectrum energy eigenvalues En limit interactions (for example, with photons) to specific energy differences En - Em. In the old quantum theory, Bohr postulated that electrons in atoms would be in ""stationary states"" of energy En, and that energy differences would be of the form En - Em = hν, where ν is the frequency of the observed spectral line. Einstein, in 1916, derived these two Bohr postulates from basic physical principles in his paper on the emission and absorption processes of atoms. What for Bohr were assumptions, Einstein grounded in quantum physics, though virtually no one appreciated his foundational work at the time, and few appreciate it today, his work eclipsed by the Copenhagen physicists. The eigenfunctions ψn are orthogonal to each other\n< ψn | ψm > = δnm (6)where the ""delta function""\nδnm = 1, if n = m, and = 0, if n ≠ m. (7)Once they are normalized, the ψn form an orthonormal set of functions (or vectors) which can serve as a basis for the expansion of an arbitrary wave function φ\n| φ > = ∑ n = 0 n = ∞ cn | ψn >. (8)The expansion coefficients are\ncn = < ψn | φ >. (9)In the abstract Hilbert space, < ψn | φ > is the ""projection"" of the vector φ onto the orthogonal axes ψn of the ψn ""basis"" vector set. 2.1 An example of superposition. Dirac tells us that a diagonally polarized photon can be represented as a superposition of vertical and horizontal states, with complex number coefficients that represent ""probability amplitudes."" Horizontal and vertical polarization eigenstates are the only ""possibilities,"" if the measurement apparatus is designed to measure for horizontal or vertical polarization. Thus,\n| d > = ( 1/√2) | v > + ( 1/√2) | h > (10)The vectors (wave functions) v and h are the appropriate choice of basis vectors, the vector lengths are normalized to unity, and the sum of the squares of the probability amplitudes is also unity. This is the orthonormality condition needed to interpret the (squares of the) wave functions as probabilities. When these (in general complex) number coefficients (1/√2) are squared (actually when they are multiplied by their complex conjugates to produce positive real numbers), the numbers (1/2) represent the probabilities of finding the photon in one or the other state, should a measurement be made on an initial state that is diagonally polarized. Note that if the initial state of the photon had been vertical, its projection along the vertical basis vector would be unity, its projection along the horizontal vector would be zero. Our probability predictions then would be - vertical = 1 (certainty), and horizontal = 0 (also certainty). Quantum physics is not always uncertain, despite its reputation. 3. The Axiom of Measurement. The axiom of measurement depends on the idea of ""observables,"" physical quantities that can be measured in experiments. A physical observable is represented as an operator A that is ""Hermitean"" (one that is ""self-adjoint"" - equal to its complex conjugate, A* = A). The diagonal n, n elements of the operator\'s matrix,\n< ψn | A | ψn > = ∫ ∫ ψ* (q) A (q) ψ (q) dq, (11)are interpreted as giving the expectation value for An (when we make a measurement). The off-diagonal n, m elements describe the uniquely quantum property of interference between wave functions and provide a measure of the probabilities for transitions between states n and m. It is the intrinsic quantum probabilities that provide the ultimate source of indeterminism, and consequently of irreducible irreversibility, as we shall see. Transitions between states are irreducibly random, like the decay of a radioactive nucleus (discovered by Rutherford in 1901) or the emission of a photon by an electron transitioning to a lower energy level in an atom (explained by Einstein in 1916). The axiom of measurement is the formalization of Bohr\'s 1913 postulate that atomic electrons will be found in stationary states with energies En. In 1913, Bohr visualized them as orbiting the nucleus. Later, he said they could not be visualized, but chemists routinely visualize them as clouds of probability amplitude with easily calculated shapes that correctly predict chemical bonding. The off-diagonal transition probabilities are the formalism of Bohr\'s ""quantum jumps"" between his stationary states, emitting or absorbing energy hν = En - Em. Einstein explained clearly in 1916 that the jumps are accompanied by his discrete light quanta (photons), but Bohr continued to insist that the radiation was classical for another ten years, deliberately ignoring Einstein\'s foundational efforts in what Bohr might have felt was his area of expertise (quantum mechanics). The axiom of measurement asserts that a large number of measurements of the observable A, known to have eigenvalues An, will result in the number of measurements with value An that is proportional to the probability of finding the system in eigenstate ψn. Quantum mechanics is a probabilistic and statistical theory. The probabilities are theories about what experiments will show. Experiments provide the statistics (the frequency of outcomes) that confirm the predictions of quantum theory - with the highest accuracy of any theory ever discovered! 4. The Projection Postulate. The third novel idea of quantum theory is often considered the most radical. It has certainly produced some of the most radical ideas ever to appear in physics, in attempts by various ""interpretations"" to deny it. The projection postulate is actually very simple, and arguably intuitive as well. It says that when a measurement is made, the system of interest will be found in (will instantly ""collapse"" into) one of the possible eigenstates of the measured observable. We have several possibilities for eigenvalues. We can calculate the probabilities for each eigenvalue. Measurement simply makes one of these actual, and it does so, said Max Born, in proportion to the absolute square of the probability amplitude wave function ψn. In this way, ontological chance enters physics, and it is partly this fact of quantum randomness that bothered Einstein (""God does not play dice"") and Schrōdinger (whose equation of motion for the probability-amplitude wave function is deterministic). The projection postulate, or collapse of the wave function, is the element of quantum mechanics most often denied by various ""interpretations."" The sudden discrete and discontinuous ""quantum jumps"" are considered so non-intuitive that interpreters have replaced them with the most outlandish (literally) alternatives. The famous ""many-worlds interpretation"" substitutes a ""splitting"" of the entire universe into two equally large universes, massively violating the most fundamental conservation principles of physics, rather than allow a diagonal photon arriving at a polarizer to suddenly ""collapse"" into a horizontal or vertical state. 4.1 An example of projection. Given a quantum system in an initial state | φ >, we can expand it in a linear combination of the eigenstates of our measurement apparatus, the | ψn >.\n| φ > = ∑ n = 0 n = ∞ cn | ψn >. (8)In the case of Dirac\'s polarized photons, the diagonal state | d > is a linear combination of the horizontal and vertical states of the measurement apparatus, | v > and | h >. When we square the (1/√2) coefficients, we see there is a 50% chance of measuring the photon as either horizontal or vertically polarized.\n| d > = ( 1/√2) | v > + ( 1/√2) | h > (10)4.2 Visualizing projection. When a photon is prepared in a vertically polarized state | v >, its interaction with a vertical polarizer is easy to visualize. We can picture the state vector of the whole photon simply passing through the polarizer unchanged. The same is true of a photon prepared in a horizontally polarized state | h > going through a horizontal polarizer. And the interaction of a horizontal photon with a vertical polarizer is easy to understand. The vertical polarizer will absorb the horizontal photon completely. The diagonally polarized photon | d >, however, fully reveals the non-intuitive nature of quantum physics. We can visualize quantum indeterminacy, its statistical nature, and we can dramatically visualize the process of collapse, as a state vector aligned in one direction must rotate instantaneously into another vector direction.', 'As discussed in our Atomic Theory II module, at the end of 1913 Niels Bohr facilitated the leap to a new paradigm of atomic theory – quantum mechanics. Bohr’s new idea that electrons could only be found in specified, quantized orbits was revolutionary (Bohr, 1913). As is consistent with all new scientific discoveries, a fresh way of thinking about the universe at the atomic level would only lead to more questions, the need for additional experimentation and collection of evidence, and the development of expanded theories. As such, at the beginning of the second decade of the 20th century, another rich vein of scientific work was about to be mined.\nPeriodic trends lead to the distribution of electrons\nIn the late 19th century, the father of the periodic table, Russian chemist Dmitri Mendeleev, had already determined that the elements could be grouped together in a manner that showed gradual changes in their observed properties. (This is discussed in more detail in our module The Periodic Table of Elements.) By the early 1920s, other periodic trends, such as atomic volume and ionization energy, were also well established.\nThe German physicist Wolfgang Pauli made a quantum leap by realizing that in order for there to be differences in ionization energies and atomic volumes among atoms with many electrons, there had to be a way that the electrons were not all placed in the lowest energy levels. If multi-electron atoms did have all of their electrons placed in the lowest energy levels, then very different periodic patterns would have resulted from what was actually observed. However, before we reach Pauli and his work, we need to establish a number of more fundamental ideas.\nThe development of early quantum theory leaned heavily on the concept of wave-particle duality. This simultaneously simple and complex idea is that light (as well as other particles) has properties that are consistent with both waves and particles. The idea had been first seriously hinted at in relation to light in the late 17th century. Two camps formed over the nature of light: one in favor of light as a particle and one in favor of light as a wave. (See our Light I: Particle or Wave? module for more details.) Although both groups presented effective arguments supported by data, it wasn’t until some two hundred years later that the debate was settled.\nAt the end of the 19th century the wave-particle debate continued. James Clerk Maxwell, a Scottish physicist, developed a series of equations that accurately described the behavior of light as an electromagnetic wave, seemingly tipping the debate in favor of waves. However, at the beginning of the 20th century, both Max Planck and Albert Einstein conceived of experiments which demonstrated that light exhibited behavior that was consistent with it being a particle. In fact, they developed theories that suggested that light was a wave-particle – a hybrid of the two properties. By the time of Bohr’s watershed papers, the time was right for the expansion of this new idea of wave–particle duality in the context of quantum theory, and in stepped French physicist Louis de Broglie.\nde Broglie says electrons can act like waves\nIn 1924, de Broglie published his PhD thesis (de Broglie, 1924). He proposed the extension of the wave-particle duality of light to all matter, but in particular to electrons. The starting point for de Broglie was Einstein’s equation that described the dual nature of photons, and he used an analogy, backed up by mathematics, to derive an equation that came to be known as the “de Broglie wavelength” (see Figure 1 for a visual representation of the wavelength).\nThe de Broglie wavelength equation is, in the grand scheme of things, a profoundly simple one that relates two variables and a constant: momentum, wavelength, and Planck\'s constant. There was support for de Broglie’s idea since it made theoretical sense, but the very nature of science demands that good ideas be tested and ultimately demonstrated by experiment. Unfortunately, de Broglie did not have any experimental data, so his idea remained unconfirmed for a number of years.\nIt wasn’t until 1927 that de Broglie’s hypothesis was demonstrated via the Davisson-Germer experiment (Davisson, 1928). In their experiment, Clinton Davisson and Lester Germer fired electrons at a piece of nickel metal and collected data on the diffraction patterns observed (Figure 2). The diffraction pattern of the electrons was entirely consistent with the pattern already measured for X-rays and, since X-rays were known to be electromagnetic radiation (i.e., waves), the experiment confirmed that electrons had a wave component. This confirmation meant that de Broglie’s hypothesis was correct.\nInterestingly, it was the (experimental) efforts of others (Davisson and Germer), that led to de Broglie winning the Nobel Prize in Physics in 1929 for his theoretical discovery of the wave-nature of electrons. Without the proof that the Davisson-Germer experiment provided, de Broglie’s 1924 hypothesis would have remained just that – a hypothesis. This sequence of events is a quintessential example of a theory being corroborated by experimental data.\nPunto de Comprensión\nTheories must be backed up by\nSchrödinger does the math\nIn 1926, Erwin Schrödinger derived his now famous equation (Schrödinger, 1926). For approximately 200 years prior to Schrödinger’s work, the infinitely simpler F = ma (Newton’s second law) had been used to describe the motion of particles in classical mechanics. With the advent of quantum mechanics, a completely new equation was required to describe the properties of subatomic particles. Since these particles were no longer thought of as classical particles but as particle-waves, Schrödinger’s partial differential equation was the answer. In the simplest terms, just as Newton’s second law describes how the motion of physical objects changes with changing conditions, the Schrödinger equation describes how the wave function (Ψ) of a quantum system changes over time (Equation 1). The Schrödinger equation was found to be consistent with the description of the electron as a wave, and to correctly predict the parameters of the energy levels of the hydrogen atom that Bohr had proposed.\nSchrödinger’s equation is perhaps most commonly used to define a three-dimensional area of space where a given electron is most likely to be found. Each area of space is known as an atomic orbital and is characterized by a set of three quantum numbers. These numbers represent values that describe the coordinates of the atomic orbital: including its size (n, the principal quantum number), shape (l, the angular or azimuthal quantum number), and orientation in space (m, the magnetic quantum number). There is also a fourth quantum number that is exclusive to a particular electron rather than a particular orbital (s, the spin quantum number; see below for more information).\nSchrödinger’s equation allows the calculation of each of these three quantum numbers. This equation was a critical piece in the quantum mechanics puzzle, since it brought quantum theory into sharp focus via what amounted to a mathematical demonstration of Bohr’s fundamental quantum idea. The Schrödinger wave equation is important since it bridges the gap between classical Newtonian physics (which breaks down at the atomic level) and quantum mechanics.\nThe Schrödinger equation is rightfully considered to be a monumental contribution to the advancement and understanding of quantum theory, but there are three additional considerations, detailed below, that must also be understood. Without these, we would have an incomplete picture of our non-relativistic understanding of electrons in atoms.\nPunto de Comprensión\nMax Born further interprets the Schrödinger equation\nGerman mathematician and physicist Max Born made a very specific and crucially important contribution to quantum mechanics relating to the Schrödinger equation. Born took the wave functions that Schrödinger produced, and said that the solutions to the equation could be interpreted as three-dimensional probability “maps” of where an electron may most likely be found around an atom (Born, 1926). These maps have come to be known as the s, p, d, and f orbitals (Figure 3).\nPunto de Comprensión\nWerner Heisenberg’s Uncertainty Principle\nIn the year following the publication of Schrödinger’s work, the German physicist Werner Heisenberg published a paper that outlined his uncertainty principle (Heisenberg, 1927). He realized that there were limitations on the extent to which the momentum of an electron and its position could be described. The Heisenberg Uncertainty Principle places a limit on the accuracy of simultaneously knowing the position and momentum of a particle: As the certainty of one increases, then the uncertainty of other also increases.\nThe crucial thing about the uncertainty principle is that it fits with the quantum mechanical model in which electrons are not found in very specific, planetary-like orbits – the original Bohr model – and it also dovetails with Born’s probability maps. The two contributions (Born and Heisenberg’s) taken together with the solution to the Schrödinger equation, reveal that the position of the electron in an atom can only be accurately predicted in a statistical way. That is to say, we know where the electron is most likely to be found in the atom, but we can never be absolutely sure of its exact position.\nPunto de Comprensión\nThe Heisenberg uncertainty principle concerning the position and momentum of a particle states that as the certainty of one increases, the _____ of the other increases.\nAngular momentum, or ""Spin""\nIn 1922 German physicists Otto Stern, an assistant of Born’s, and Walther Gerlach conducted an experiment in which they passed silver atoms through a magnetic field and observed the deflection pattern. In simple terms, the results yielded two distinct possibilities related to the single, 5s valence electron in each atom. This was an unexpected observation, and implied that a single electron could take on two, very distinct states. At the time, nobody could explain the phenomena that the experiment had demonstrated, and it took a number of scientists, working both independently and in unison with earlier experimental observations, to work it out over a period of several years.\nIn the early 1920s, Bohr’s quantum model and various spectra that had been produced could be adequately described by the use of only three quantum numbers. However, there were experimental observations that could not be explained via only three mathematical parameters. In particular, as far back as 1896, the Dutch physicist Pieter Zeeman noted that the single valence electron present in the sodium atom could yield two different spectral lines in the presence of a magnetic field. This same phenomenon was observed with other atoms with odd numbers of valence electrons. These observations were problematic since they failed to fit the working model.\nIn 1925, Dutch physicist George Uhlenbeck and his graduate student Samuel Goudsmit proposed that these odd observations could be explained if electrons possessed angular momentum, a concept that Wolfgang Pauli later called “spin.” As a result, the existence of a fourth quantum number was revealed, one that was independent of the orbital in which the electron resides, but unique to an individual electron.\nBy considering spin, the observations by Stern and Gerlach made sense. If an electron could be thought of as a rotating, electrically-charged body, it would create its own magnetic moment. If the electron had two different orientations (one right-handed and one left-handed), it would produce two different ‘spins,’ and these two different states would explain the anomalous behavior noted by Zeeman. This observation meant that there was a need for a fourth quantum number, ultimately known as the “spin quantum number,” to fully describe electrons. Later it was determined that the spin number was indeed needed, but for a different reason – either way, a fourth quantum number was required.\nPunto de Comprensión\nSome experimental observations could not be explained mathematically using three parameters because\nSpin and the Pauli exclusion principle\nIn 1922, Niels Bohr visited his colleague Wolfgang Pauli at Göttingen where he was working. At the time, Bohr was still wrestling with the idea that there was something important about the number of electrons that were found in ‘closed shells’ (shells that had been filled).\nIn his own later account (1946), Pauli describes how building upon Bohr’s ideas and drawing inspiration from others’ work, he proposed the idea that only two electrons (with opposite spins) should be allowed in any one quantum state. He called this ‘two-valuedness’ – a somewhat inelegant translation of the German zweideutigkeit (Pauli, 1925). The consequence was that once a pair of electrons occupies a low energy quantum state (orbitals), any subsequent electrons would have to enter higher energy quantum states, also restricted to pairs at each level.\nUsing this idea, Bohr and Pauli were able to construct models of all of the electronic structures of the atoms from hydrogen to uranium, and they found that their predicted electronic structures matched the periodic trends that were known to exist from the periodic table – theory met experimental evidence once again.\nPauli ultimately formed what came to be known as the exclusion principle (1925), which used a fourth quantum number (introduced by others) to distinguish between the two electrons that make up the maximum number of electrons that could be in any given quantum level. In its simplest form, the Pauli exclusion principle states that no two electrons in an atom can have the same set of four quantum numbers. The first three quantum numbers for any two electrons can be the same (which places them in the same orbital), but the fourth number must be either +½ or -½, i.e., they must have different ‘spins’ (Figure 4). This is what Uhlenbeck and Goudsmit’s research suggested, following Pauli’s original publication of his theories.\nThe period described here was rich in the development of the quantum theory of atomic structure. Literally dozens of individuals, some mentioned throughout this module and others not, contributed to this process by providing theoretical insights or experimental results that helped shape our understanding of the atom. Many of the individuals worked in the same laboratories, collaborated together, or communicated with one another during the period, allowing the rapid transfer of ideas and refinements that would shape modern physics. All these contributions can certainly been seen as an incremental building process, where one idea leads to the next, each adding to the refinement of thinking and understanding, and advancing the science of the field.\nThe 20th century was a period rich in advancing our knowledge of quantum mechanics, shaping modern physics. Tracing developments during this time, this module covers ideas and refinements that built on Bohr’s groundbreaking work in quantum theory. Contributions by many scientists highlight how theoretical insights and experimental results revolutionized our understanding of the atom. Concepts include the Schrödinger equation, Born’s three-dimensional probability maps, the Heisenberg uncertainty principle, and electron spin.\nElectrons, like light, have been shown to be wave-particles, exhibiting the behavior of both waves and particles.\nThe Schrödinger equation describes how the wave function of a wave-particle changes with time in a similar fashion to the way Newton’s second law describes the motion of a classic particle. Using quantum numbers, one can write the wave function, and find a solution to the equation that helps to define the most likely position of an electron within an atom.\nMax Born’s interpretation of the Schrödinger equation allows for the construction of three-dimensional probability maps of where electrons may be found around an atom. These ‘maps’ have come to be known as the s, p, d, and f orbitals.\nThe Heisenberg Uncertainty Principle establishes that an electron’s position and momentum cannot be precisely known together, instead we can only calculate statistical likelihood of an electron’s location.\nThe discovery of electron spin defines a fourth quantum number independent of the electron orbital but unique to an electron. The Pauli exclusion principle states that no two electrons with the same spin can occupy the same orbital.']"	['<urn:uuid:fd2321d4-139a-468e-bdf6-8e22af8de92f>', '<urn:uuid:492098bb-e18e-4cda-ac5f-373b7c3c108a>']	open-ended	with-premise	verbose-and-natural	distant-from-document	comparison	novice	2025-05-12T21:31:42.699391	24	102	4371
3	I work with quantum technologies and I'd like to know what specific materials research is being conducted to advance quantum computing capabilities?	Researchers led by Professor Tom Kuech and Professor Paul Evans are working on fabricating thin films from complex oxides. These films often have unusual crystal structures with surprising electronic, magnetic and optical properties that could advance quantum information technologies, including quantum computing.	['Major NSF-sponsored materials research collaboration receives $15.6M grant\nThe University of Wisconsin–Madison center for research on materials like semiconductors, glass and liquid crystals will continue its work with $15.6 million in support from the National Science Foundation (NSF).\nThe grant provides six years of funding for the Materials Research Science and Engineering Center, based in UW–Madison’s College of Engineering and including 30 affiliated faculty from nine departments across the university.\nResearchers at MRSEC, which was established in 1996, investigate fundamental, large-scale and complex questions in materials science.\n“These questions require large groups comprised of researchers with skills in synthesis and characterization of materials, materials processing and in theory and computation,” says Nick Abbott, director of MRSEC and professor of chemical and biological engineering. “Only with these interdisciplinary groups can we go after problems that are really difficult and that involve fundamental questions in materials-related areas. If we can address them, they will lead to breakthroughs that will transform our understanding of materials and lead to a new slate of technologies. It’s high-risk, high-reward research.”\nIt’s the high-risk nature of the research that makes NSF’s broad and flexible funding commitment so important, Abbott says. MRSEC-affiliated faculty enjoy the flexibility to exhaustively pursue fundamental problems — a painstaking process that can often conclude without an elusive “eureka!” moment of discovery.\nBut NSF’s long-term commitment has also produced pioneering research: the use of liquid crystals for sensing, with applications in wearable technologies that detect airborne toxic gases; semiconductor synthesis from new materials, with broad implications for electronics; and the extremely efficient energy transfer capacity of carbon nanotubes. The property makes them a promising candidate for next-generation solar energy harvesting.\nAnother could come by way of better understanding glasses, a major thrust of MRSEC research. Most people understand glass as a brittle, transparent material useful for making windowpanes, storage containers and smartphone screens. But there are plastic and metal glasses, and MRSEC researchers may establish entirely new categories of glasses, according to chemistry Professor Mark Ediger, co-investigator with Paul Voyles, professor of materials science and engineering.\n“Our project seeks a fundamental understanding of glassy materials that enables the development of new glasses that might extend the life of machine tools or enable advances in quantum computing or lead to even better cell phone displays,” Ediger says.\nMRSEC researchers led by chemical and biological engineering Professor Tom Kuech and materials science and engineering Professor Paul Evans hope to advance quantum information technologies, including quantum computing, by fabricating thin films from complex oxides.\nThe films often have unusual crystal structures exhibiting surprising electronic, magnetic and optical properties. Understanding them will only come about through sustained investment in fundamental research, the type of investment that NSF’s MRSEC funding provides.\n“NSF support really goes a long way at UW–Madison and has a significant and broad impact,” Evans says. “And that impact is underpinned by the basic science happening at MRSEC. The basic scientific research provides the foundation for highly valuable education and outreach activities.”\nMRSEC also supports internationally recognized educational and outreach programs, developing and distributing research-inspired educational kits and activities to teachers who incorporate basic materials science concepts in thousands of high school classrooms.\n“It’s high-risk, high-reward research.”\nAbbott says the new funding will help MRSEC transition from physical kits to developing digital educational materials in the form of educational games, which could reach millions of schoolchildren.\n“The UW–Madison MRSEC has been a national leader in education for decades, with measureable impact worldwide,” says engineering physics Professor Wendy Crone, who co-leads MRSEC educational and outreach programs with Anne Lynn Gillian-Daniel, director of the Interdisciplinary Education Group. “We have created science kits, teaching laboratories, web-based content for K-12 teachers, museum exhibits, outreach activities and now digital games.”\nMRSEC’s impact includes millions of dollars of scientific equipment available for use by industry and researchers both on and off the UW–Madison campus, according to Abbott, and the Advanced Materials Industrial Consortium — a group of companies that, together with MRSEC faculty and staff, provides invaluable professional development opportunities for UW–Madison students and researchers.']	['<urn:uuid:abd7ad6b-0839-4184-b506-a1fa3d3de72b>']	open-ended	with-premise	verbose-and-natural	distant-from-document	single-doc	expert	2025-05-12T21:31:42.699391	22	42	673
4	Which era had more books and publications discussing it - the period when Jessica J. Lee explored Taiwan's nature or the Anglo-Saxon Wars period in Britain?	Based on the information provided, the Taiwan nature exploration period had more published works, including Jessica J. Lee's 'Two Trees Make a Forest', 'Turning: A Year in the Water', and multiple reviews and coverage in publications like The Guardian, Ms. Magazine, Cosmopolitan, Refinery29, Outside, BOMB, Book Riot, Electric Literature, BookPage, and the Los Angeles Review of Books. In contrast, for the Anglo-Saxon Wars period, only Osprey's Men-at-Arms series book 'Arthur and the Anglo-Saxon Wars' is mentioned.	"['You are here\nTwo Trees Make a Forest: In Search of My Family\'s Past Among Taiwan\'s Mountains and Coasts (Paperback)\nThis ""stunning journey through a country that is home to exhilarating natural wonders, and a scarring colonial past . . . makes breathtakingly clear the connection between nature and humanity, and offers a singular portrait of the complexities inherent to our ideas of identity, family, and love"" (Refinery29).\nA chance discovery of letters written by her immigrant grandfather leads Jessica J. Lee to her ancestral homeland, Taiwan. There, she seeks his story while growing closer to the land he knew.\nLee hikes mountains home to Formosan flamecrests, birds found nowhere else on earth, and swims in a lake of drowned cedars. She bikes flatlands where spoonbills alight by fish farms, and learns about a tree whose fruit can float in the ocean for years, awaiting landfall. Throughout, Lee unearths surprising parallels between the natural and human stories that have shaped her family and their beloved island. Joyously attentive to the natural world, Lee also turns a critical gaze upon colonialist explorers who mapped the land and named plants, relying on and often effacing the labor and knowledge of local communities.\nTwo Trees Make a Forest is a genre–shattering book encompassing history, travel, nature, and memoir, an extraordinary narrative showing how geographical forces are interlaced with our family stories.\nAbout the Author\nJessica J. Lee is a British–Canadian–Taiwanese author and environmental historian, and winner of the 2019 RBC Taylor Prize Emerging Author Award. She received a doctorate in environmental history and aesthetics in 2016, and her first book, Turning: A Year in the Water, was published in 2017. Jessica is the founding editor of The Willowherb Review. She lives in Berlin.\nOne of The Guardian\'s Best Books of the Year\n""Part travel guide, part memoir, part history, the new book by environmental historian Jessica J. Lee takes us on a journey through her ancestral home of Taiwan as she examines the landscape, the wildlife, the legacy of colonialism and her own roots. The book beautifully captures the deep connections between the natural world and family history."" —Karla Strand, Ms. Magazine\n""When Jessica J. Lee discovers letters written by her immigrant grandfather, she decides to head to her ancestral home of Thailand to connect with his experiences . . . Through this time, she learns more about her family\'s past, including how colonialism shaped their fate."" —Laura Hanrahan, Cosmopolitan, A Best Nonfiction Book of the Year\n""Jessica J. Lee\'s stirringly beautiful book, which mingles elements of memoir, travel writing, and history lesson, is intensely personal and profoundly generous, offering readers a glimpse into the rich legacies of both Lee\'s family and their homeland, Taiwan . . . A stunning journey through a country that is home to exhilarating natural wonders, and a scarring colonial past. Lee makes breathtakingly clear the connection between nature and humanity, and offers a singular portrait of the complexities inherent to our ideas of identity, family, and love."" —Kristin Iversen, Refinery29\n""Lee offers a divergent model for a travel memoir, in which the land is the lead character in the work, itself an ancestor that she longs to know . . . Her perspective provides a refreshing departure from the norm: for Lee, it is not the landscape that is foreign but the author herself. Although she is a descendant of this land, she makes sure to establish herself as a respectful visitor, giving the island room to reveal itself to her as it wishes to be seen . . . Ultimately, she finds that her motherland is a place of perpetual migration, and at long last, she feels less adrift."" —Frances Nguyen, Outside\n""Love is attention, as the saying goes, and in this, Lee’s memoir truly shines. A remarkable exercise in careful attention, be it to the nuances of language, the turns of colonial history, or a grandfather’s difficult–to–read handwriting, Two Trees Makes A Forest is a moving treatise on how to look closely and see truthfully, even as the fog rolls in."" —Rachel Heng, BOMB\n""After unearthing her grandfather’s hidden memoir, Lee becomes determined to piece together her family’s history as they moved from China to Taiwan to Canada. Her search brings her home to Taiwan where she explores the language, history, and memories of her family’s homeland. Her end product is a beautifully written book that is equal parts personal narrative, history of Taiwan, and meditations on nature."" —Sophia LeFevre, Book Riot, One of the Best New Memoirs Written by Asian Authors\n""After discovering her grandfather’s fragmented autobiographical writings, Lee—who has a doctorate in environmental history—travels to the island of Taiwan to hunt down lost parts of his story and attempt to reconnect with distant relatives. She offers a poetic tour and anti–colonial reclamation of the island through her descriptions of its flora, fauna, natural disasters, and political history."" —Electric Literature\n""Lee finds her own ways of imprinting her rediscovered homeland on her spirit . . . As Taiwan reveals itself, Lee comes to a kind of peace. Gong’s past and her present, so evocatively examined, suggest the forest she needed to find."" —Priscilla Kipp, BookPage\n""In the plants, history, and landscape of Taiwan, [Lee] comes to terms with her own identity . . . poetic and emotive, unfurling to reveal passages about her family, her pain, and her exploration of Taiwan’s myriad habitats . . . While we don’t know what will come next on her journey, we are left with hope that she will continue searching—not to fill a void or patch a past with which she will never be able to fully make amends, but that she will define herself by growing into her heritage. And that is the best any of us can do.""—Kristen Schott, Los Angeles Review of Books\n""As an environmental history scholar and nature writer, Lee brings a fascinating perspective to Taiwan based on an immersive connection to the land. Lee eloquently describes Taiwan’s landscapes and natural history from Qing times, when modern scientific methods were first developed, and impresses upon readers the magnificence of its mountains that climb to almost 4,000 meters from sea level."" —Hilton Yip, Asian Review of Books\n""[Jessica J. Lee] combines a botanist’s precision with a poet’s eye and ear . . . One of the most exciting voices exploring landscape and identity today. In Two Trees Make a Forest, she has created a powerful, beautifully written account of the connections between people and the places they call home.""—Elizabeth Dearnley, Times Literary Supplement\n""[An] elegiac book, which smoothly incorporates historical and travel threads . . . A beautiful and personal view of an island—and an author—shaped by environment and history.""—Kirkus Reviews\n""I want to go to Taiwan to experience the woodlands, the wetlands, the highlands, the lowlands, and the creatures in, above, and underneath, as Jessica J. Lee does with all her senses, including that sense too many of us ignore—the inner self. Then again, she has taken me there with this splendid book."" —Jack E. Davis, author of The Gulf, winner of the 2018 Pulitzer Prize', '8 edition of Arthur and the Anglo Saxon Wars found in the catalog.\nSeptember 25, 2000\nby Osprey Publishing\nWritten in English\n|Contributions||Angus Mcbride (Illustrator)|\n|The Physical Object|\n|Number of Pages||52|\nFind many great new & used options and get the best deals for Men-At-Arms Ser.: Arthur and the Anglo-Saxon Wars by Angus McBride and David Nicolle (, Trade Paperback) at the best online prices at eBay! Free shipping for many products! Arthur and the Anglo-Saxon Wars Osprey Men-at-Arms series No. (; numerous reprints) Text by David Nicolle; colour plates by Angus McBride Review by Rob Morgan When Rome left Britain, the following centuries created a fascinating chapter in history. Often regarded as little more than a period wrongly known as “The Dark.\nBuy Arthur and the Anglo-Saxon War By D. Nicolle, in Good condition. Our cheap used books come with free delivery in the UK. ISBN: ISBN Pages: Pris: kr. Häftad, Skickas inom vardagar. Köp Arthur and the Anglo-Saxon Wars av David Nicolle på\nArthur and the Anglo-Saxon Wars.-(Men at Arms series) I. Military art anct science-Great Britain-History 2. Great Britain-History, Military-Anglo-Saxon period, I. Title II. Series \'\' UG7 ISBN 1 Tourist Information by Martin Marix Evans Back cover Cartography by The Map Studio Filmset in Great Britain. Click to read more about Arthur and the Anglo-Saxon wars by David Nicolle. LibraryThing is a cataloging and social networking site for booklovers. All about Arthur and the Anglo-Saxon wars by David Nicolle. LibraryThing is a cataloging and social networking site for booklovers This book was originally written and published in - and /5(3).\nTraditional Irish Recipes\nCheryl Welchs Artistry in Bloom 2002 Calendar\nBatt upon Batt\nLife in Extreme Environments\nReport of the Board of Fire Commissioners, Chief Engineer and Fire Alarm Superintendent of the San Francisco Fire Department\nFrançois Xavier Garneau.\nMr. Frank, [interviews]\nTrust funds, State Department. Message from the President of the United States, transmitting report of the Secretary of State in relation to trust funds.\nQuakers in North-West England\nThe 2000 Import and Export Market for Iron and Steel Chain and Chain Parts in New Caledonia\nThe lessonmaker Bible.\ndevelopment of an information booklet for hormone/treatment of osteoporosis\nout of 5 stars It\'s more Anglo Saxon war and less King Arthur. Reviewed in the United States on Febru Verified Purchase. There\'s a little bit on King Arthur but the name itself isn\'t entirely accurate. It\'s more about the historic time period of the Anglo-Saxon wars, and less about who King Arthur was, or if he was/5(28).\nArthur and the Anglo-Saxon Wars book. Read 8 reviews from the world\'s largest community for readers. The Arthurian Age; the Celtic Twilight; the Dark Age /5. Stenton, and a book in the Men-at-Arms series that I bought when I was a teenager living in England called Arthur and the Anglo-Saxon Wars, and I think it is an interesting period.\nThis book had the additional charm of mentioning places I have been in England - my dad was stationed at RAF Lakenheath and RAF Upper Heyford in the s, flying F /5(51).\nThe Arthurian Age; the Celtic Twilight; the Dark Ages; the Birth of England; these are the powerfully romantic names often given to one of the most confused yet vital periods in British history.\nIt is an era upon which rival Celtic and English nationalisms frequently fought. It was also a period of settlement, and of the sword. This absorbing volume by David Nicolle transports us to an England. Arthur and the Anglo Saxon Wars book and the Anglo-Saxon Wars by David Nicolle,available at Book Depository with free delivery worldwide.\nThis book looks at the realities behind the myth of king Arthur against the backdrop of invasion as a nation was forged on the bloody anvil of war. Download Arthur and the Anglo Saxon Wars Share this. Buy Arthur and the Anglo-Saxon Wars: Anglo-Celtic Warfare, A.D (Men-at-Arms) Illustrated by Nicolle, Dr David, McBride, Angus (ISBN: ) from Amazon\'s Book Store.\nEveryday low prices and free delivery on eligible orders/5(24). Buy Arthur and the Anglo-Saxon Wars: Anglo-Celtic Warfare, A.D By David Nicolle. Available in used condition with free delivery in the UK. ISBN: ISBN Buy Britain AD: A Quest for Arthur, England and the Anglo-Saxons TV Tie in Ed by Pryor, Francis (ISBN: ) from Amazon\'s Book Store.\nEveryday low prices and free delivery on Reviews: The Anglo-Saxons These Germanic tribes are the ""Anglo-Saxons"" to whom we owe much of our tradition, language, and physical heritage.\nThey poured in upon the Romanized Celts of Britain much as the Normans would do to them in later times, pushing the inhabitants of the island back into the hills of Wales and Cornwall, creating pockets of Celtic.\nBook Review: King Arthur’s Wars: The Anglo-Saxon Conquest of England Keith Dear RUSI Journal, 16 June UK, History, Europe. Keith Dear reviews King Arthur’s Wars: The Anglo-Saxon Conquest of England by Jim Storr.\nContinue Reading. Log in. Become A Member. To access the full text of this article and many other benefits, become a RUSI member. Arthur and the Anglo-Saxon Wars (Men-at-Arms) (Book) Book Details. ISBN. Title. Arthur and the Anglo-Saxon Wars (Men-at-Arms) Buy This Book.\n$ plus shipping $ free shipping worldwide. By purchasing books through this website, you support our non-profit organization. Ancient History Encyclopedia receives a. Arthur and the Anglo-Saxon Wars by David Nicolle, Angus McBride (Illustrator) Paperback $ Ship This Item — Temporarily Out of Stock Online.\nBuy Online, Pick up in Store is currently unavailable, but this item may be available for in-store purchase. Publish your book Pages: Enquiries should be addressed to the Publishers.\nBritish Library Cataloguingin Publication Data Nicolle, David Arthur and the Anglo-Saxon Wars.-(Men at Arms series) I. Military art anct science-Great Britain- History 2. Great Britain-History, Military-Anglo-Saxon period, I. Title II.  David Nicolle was born inthe son of the illustrator Pat Nicolle.\nHe worked in the BBC Arabic service for a number of years, before going \'back to. Buy on Amazon King Arthur and the Knights of the Round Table Marcia Williams Delve into the fascinating Anglo-Saxon legends of King Arthur and his knights. The tales are retold in a comic book style, with speech bubbles and many humorous details in.\nArthur and the Anglo-Saxon wars. [David Nicolle; Angus McBride] Home. WorldCat Home About WorldCat Help. Search. Search for Library Items Search for Lists Search for Part of the ""Men-At-Arms"" series, this book looks at Anglo-Celtic warfare betweenillustrating equipment, weapons and military encounters.\nRating: (not yet rated). This narrative is based largely on Anglo-Saxon sources, the writings of Bede and in the Anglo-Saxon Chronicle, which are far from perfect but still basically informative and credible. Certainly the process was accompanied by the creation of numerous linear earthworks at sites of obvious military utility, cutting roads, barring valleys, and.\nThis is an incomplete list of the wars and battles between the Anglo-Saxons who later formed into the Kingdom of England and the Britons (the pre-existing Brythonic population of Britain south of the Antonine Wall who came to be known later by the English as the Welsh), as well as the conflicts between the English and Welsh in subsequent centuries.\nThe list begins after the Adventus Saxonum in. Get this from a library! Arthur and the Anglo-Saxon wars: Anglo-Celtic warfare, AD [David Nicolle; Angus McBride] -- Anglo-Celtic warfare, AD Discover the best Anglo Saxons books and audiobooks.\nLearn from Anglo Saxons experts like William of Malmesbury and Henrietta Leyser. Read Anglo Saxons books like Chronicle of the Kings of England and Beda with a free trial King Arthur\'s Wars: The Anglo-Saxon Conquest Of England.\nAuthor Jim Storr. Lost Gold of the Dark Ages: War, Treasure.The historicity of King Arthur has been debated both by academics and popular writers.\nWhile there have been many suggestions that the character of Arthur may be based on one or more real historical figures, academic historians today consider King Arthur to be a mythological or folkloric figure.\nThe first mention of Arthur appears inwhere he is presented as a leader fighting against the.']"	['<urn:uuid:4a3f073b-7e1f-47f1-8a87-1324453f6eef>', '<urn:uuid:e209baa6-ca07-4a9e-ab1a-5e3e28bb3da1>']	factoid	direct	verbose-and-natural	similar-to-document	comparison	novice	2025-05-12T21:31:42.699391	26	76	2522
5	How do both documents suggest improving home energy efficiency through purchases and certifications?	Both documents outline multiple approaches to improving home energy efficiency through purchases and certifications. The first document highlights certification programs like Energy Star, Ecologo, and Green Seal that help identify energy-efficient products, while also providing resources like the Federal Energy Management Program for procurement advice. The second document emphasizes purchasing energy-efficient doors, windows, and appliances, using water conservation fixtures, and mentions that homes can qualify for Energy Star certification by achieving 30% energy reductions over standard codes. Both documents also note that these improvements can lead to significant cost savings and environmental benefits.	"['Environmental Preferable Products (EPP)\nThe following Internet sites provide information on environmentally preferable products and services.\n- Energy Star Healthcare Program\n- Federal Energy Management Program\n- Recycled Content Products\n- Environmentally Preferable Purchasing\n- Labeling and Certification Programs\n- Other Resources\nThe Energy Star® Healthcare Program sponsored by the Environmental Protection Agency and Department of Energy, includes a database of energy efficient products, energy savings calculators, purchasing specifications, and a vendor locator. To find information tailored to the healthcare industry, select the “Business Improvement” button and then a ""healthcare"" link located on the right side of the screen.\nConnecticut hospitals that have a Energy Star rating of 75% or higher include the VA Hospital located in West Haven and St. Francis Hospital and Medical Center in Hartford. For additional information, please contact Bill White at EPA at (617) 918-1333.\nThe Federal Energy Management Program (FEMP) offers advice about procurement, construction, maintenance, and utility management that the private sector will find useful. As the largest energy consumer in the United States, the federal government developed this site to promote energy efficiency and the use of renewable energy resources at federal facilities.\n- DEEP\'s Buy Recycled webpage has has extensive information purchasing products with recycled content.\n- Comprehensive Procurement Guidelines (CPG),EPA designates over 50 recycled content product categories and suggests recycled content ranges based on studies of market availability and product performance. This website includes a database of suppliers searchable by state.\n- The EPA\'s Environmentally Preferable Purchasing Program site features a database of “green” procurement language, EPP publications, case studies, policy documents, tip sheets, and federal EPP links on the Internet.\n- EPPNet is a listserv dedicated to the EPP and offers participants access to a national network of “green” purchasing practitioners and advocates. Vendors are prohibited from participating in the listserv.\n- Ecologo is Canada\'s environmental product certification program. This site features an extensive catalogue of certified products, mostly manufactured by Canadian companies.\n- Green Seal is a nonprofit environmental product certification organization promoting the manufacture and sale of environmentally responsible consumer products. Their site includes a listing of certified products and “green” product specifications.\n- The Forest Stewardship Council is an international labeling program for sustainable forest products that uses third-party certifiers to verify internationally recognized standards.\n- The Center for a New America Dream is a nonprofit organization dedicated to reducing North America’s pattern of over-consumption. Their EPP Program assists state and local governments with ""green"" purchasing efforts, including a national list of EPP Policies.\n- Co-op America is a nonprofit organization that publishes the National Green Pages, a comprehensive listing of socially and environmentally responsible businesses in America.\n- Environmental Building News provides subscribers access to articles and reports on ""green"" building practices and materials, including paints, carpeting, and plastic lumber.\n- Sustainable Hospital Project is an EPA funded program which provides information on alternatives to hospital products that reduce occupational and environmental hazards.\n- INFORM is a nonprofit organization that publishes environmental reports on cleaning products, extended producer responsibility, and waste prevention.\nContent Last Updated October 2019', 'Energy efficient design and construction can substantially reduce costs and pollution because nonrenewable energy sources of energy are expensive, potentially unstable, and create toxic wastes through their development and combustion. The long-term costs of energy to the homeowner are large, in dollars as well as in health and safety.\nReasons to Change\n- The generation and use of energy is the single largest contributor to air pollution. It has also been linked to climate change. Source: Green Facts Each year the average house releases more than twice as much greenhouse gas as the typical car (more than 22,000 pounds of carbon dioxide). Source: Energy Star Home Advisor\n- Air leakage from poorly sealed ductwork and building envelope can waste over 50% of a homes heating and cooling energy. Duct leakage (estimated at 30% of waste in some homes) can also create pressure imbalances in a home, endangering health and safety by drawing in outside air and causing backdrafting of combustion appliances. Air leakage also contributes to problems with moisture, noise, dust and entry of pollutants, insects, rodents and reduced fire safety. Source: Southface Institute\n- The typical US family spends more than $1300 per year on energy costs. Ten to fifty percent of that energy is wasted and does not contribute to the comfort or convenience of the family.\nBuilding energy efficient and renewable energy homes offers contractors advantages in the marketplace. The reputation of builders with satisfied customers passes among homeowners and potential buyers. Greater customer satisfaction will result from less noise, increased durability and comfort, reduced maintenance and operating costs, and the reduced reliance on pollution causing energy consumption. Energy-efficient homes and those with renewable energy systems qualify homeowners for resource efficient mortgages, and incentives for renewable energy technologies. The more skilled a builder is at conveying these advantages to home buyers, the more profitable the builder will be.\nEnergy efficiency is one of the most direct ways a person can realize the benefits of a green built home. By implementing energy efficient techniques, energy bill savings can reach 65%. Energy-efficiency can also improve the comfort, health, and safety of home occupants through improved indoor air quality, fire protection, noise levels, reduced maintenance, and increased durability.\nP2 in Action\nWhole-building design or a systems approach considers the interaction of all elements of the building site, building envelope, mechanical systems, and occupants to help achieve optimal energy performance. The key is to reduce the house load (energy use) using the best combination of:\n- Conservation (insulation, efficient lighting and appliances, house orientation),\n- Insolation (solar gain), and\n- Thermal Storage (mass in walls and floors which helps keep the house a more constant temperature).\nThe emphasis on each should vary on a site-by-site basis.\nMost energy-efficient homes have four basic elements in common:\n- A well-constructed and tightly sealed thermal envelope with appropriate ventilation.\n- Proper design and installation of heating and cooling systems (properly sized, high-efficiency, energy source, ventilation and ductwork).\n- Energy-efficient doors, windows, and appliances.\n- Home orientation and placement of building elements to maximize natural heating and cooling efficiency.\nSource: Energy & Environmental Building Association\nSpecific ways to achieve energy efficiency through a systems approach include:\n- Use of passive solar advantages on the building site (and appropriate overhangs),\n- Efficient lighting,\n- Water conservation fixtures and appropriate placement of water heating equipment,\n- Design for appropriate insulation levels and reduce labor costs with Optimum Value Engineering (OVE) framing techniques,\n- Incorporation of wind, hydrothermal and/or active solar technologies,\n- Landscaping (trees, bushes, earthberms provide shading, block a prevailing wind),\n- Thermostat with automatic setback for night and work times,\n- Increased insulation in exterior wall, ceiling, floors and foundation,\n- Moisture control within the building envelope to control moisture buildup,\n- Selection of energy efficient appliances,\n- Dampers on all vents, fans and chimneys\n- Use of appropriate colored materials and coatings on exterior or roof (i.e. light colored where summer cooling climates dominate).\nAssistance with Energy Efficiency\nGuidelines, standards and incentive programs abound for energy efficiency in residential construction. Here are some suggestions:\n- Organizations working to promote alternative energy strive to provide tax incentives and guidance to help builders and consumers more effectively incorporate renewable energy and energy efficiency into their homes.\n- Affordable housing and economic development advocates support legislative mandates and establish requirements to include energy efficiency and renewable energy into housing assisted by their programs.\n- Utility companies provide audits and incentives for energy efficiency, as well as renewable energy technology development.\n- Certification and ratings programs create partnerships between builders and federal, state, local and nonprofit organizations to enhance communities and marketability one home at a time.\n- Energy efficient construction may also qualify consumers for higher than normal debt-to-income ratio when calculating loan potentials, enlarging the market of potential home-buyers.\nOne rating program that deserves special mention is the Energy Star home labeling program. Energy Star standards are nationally recognized and commonly adopted as criteria for other incentive programs. This program of the Environmental Protection Agency requires homes to achieve 30 percent reductions in energy over the Model Energy Code and obtain a third party verification. Energy Star for New Homes Improvements to Existing Homes\nCheck with the following sites for tools and resources, news, product reviews and current events in Renewable Energy and Efficiency:']"	['<urn:uuid:7e47d04e-aa2a-48d9-b3a0-5468fa3e486b>', '<urn:uuid:5b33f3c4-0a1d-4e6d-b054-1fc298f445d4>']	open-ended	direct	verbose-and-natural	similar-to-document	comparison	novice	2025-05-12T21:31:42.699391	13	93	1400
6	election system most votes comparison	In First Past the Post and Two-Round Systems, candidates win by getting the most votes. However, in List Proportional Representation systems, if a party gets 30% of votes they receive 30% of seats in the representative body. Mixed-Member Proportional combines both approaches with electorate MPs elected by most votes while maintaining overall proportional party representation.	"['Mixed-Member Proportional (MMP)\nThe most important feature of MMP is that it is proportional: seats in the parliament are assigned to parties in proportion to the percentage of the national vote that each party receives. The actual way that is worked out is by the Sainte-Laguë formula, and the final result is, indeed, pretty proportional. There are, however, some twists and turns.\nThere are approximately (see below) 120 Members of Parliament, which works out to 0.83% of the vote per MP. The first twist is that there is a threshold of 5%, i.e., in order for a party to get any Members of Parliament, it must get at least five percent of the vote. This stops the parliament from being fragmented into many small parties, Italian style, but whether it\'s a fair barrier to participation is questionable, especially as preferential voting is not used. As we\'ll see, this barrier exists for some, but not for others.\nThe next twist is that rather than being just a party-centred proportional system, there is also a level of regional representation built into MMP. Seventy MPs are, in fact, ""Electorate"" MPs: they are elected to represent a particular electorate by a first past the post vote. This doesn\'t mess with the proportionality of the system (mostly: see below!), because each Electorate MP counts against the party\'s total allocation. The rest is made up from ""List"" MPs.\nSo at the ballot box, each voter has two choices to make: the party they want to vote for and the local electorate candidate they want to vote for; one voter, two votes. To first approximation, the electorate vote is irrelevant: the total number of MPs the party gets is determined by the party vote, exactly which electorates they represent -- or whether they come in on the List -- is a matter of shuffling the chairs. But for small minor parties the electorate vote can be very important: winning an Electorate seat means dodging the 5% threshold.\nIn the 2008 election, for example, the ACT party received 3.65% of the party vote, less than the threshold; but one of its candidates, Rodney Hide, won the seat of Epsom, thus defeating the threshold. ACT\'s party vote was enough to elect 4 additional MPs. Another party, New Zealand First, received 4.07% of the party vote, but won no Electorates, and therefore received no MPs. The Maori Party only got 2.39% of the party vote, but won 5 Electorates, so its representation in the parliament was much larger than its party vote required (3 seats)! This, by the way, is why the number of MPs is not fixed. The Maori Party excess means there are currently 122 MPs; an ""overhang"" of two. So once again we see that concentrated regional support trumps broad, thin support.\nThe main problem with this is not so much the discrimination between parties (though I\'m sure Winston Peters would disagree), as the discrimination between voters. As we\'ve seen, the ""one voter, two votes"" of MMP is really just ""two ticks, one vote"" for most voters. But some voters -- those in the Maori Seats and the Good People of Epsom -- actually do get two votes!\nRenowned Aussie psephologist Malcolm MacKerras explained this a worthy rant some time ago, and this election this exploit is very much in the public mind. However, the complaint being made by the SM proponents is very different. What upsets them about MMP is that it is difficult to get rid of a particular parliamentarian. If they lose their Electorate, because they\'re out of favour with their constituents, but are still in favour with their party then they come into parliament as a List MP. This is a consequence of the party-centred philosophy of MMP, as opposed to the candidate-centred philosophy of STV.\nMMP is designed with the expectation that voters vote for a party, whilst STV allows voters to vote for an individual. While it is true that modern politics in much of the world, and certainly in New Zealand, is based on the party, objections to particular members are not that uncommon. The example that springs to mind is Larry Anthony, turfed from the seat held by his father and his grand-father despite the overall swing to his party. This would be almost impossible in MMP. The voters\' inability to rid themselves of unpopular politicians is a fair complaint, however the correct solution is to opt for STV, not SM.\nFirst Past the Post\n- Pro: Voters feel they understand it.\n- Con: They don\'t.\n- Pro: Can vote for the candidate you want.\n- Con: Overwhelmingly favours the largest parties.\n- Pro: Better than FPP.\n- Con: Not better enough.\n- Pro: Preferential and (approximately) Proportional.\n- Con: Complicated.\n- Pro: Proportional representation.\n- Con: Not preferential, favours voters in some electorates.', 'Electoral systems are methods and rules used to determine the outcome of an election. The system used and how it is used varies in each country. What voters know about the system can have an impact on how they vote or who they vote for. We can categorise these into four “families”, namely Proportional Representation, Mixed, Majority, as well as Plurality systems. In this article, we’ll look at Proportional Representation and Majority Systems.\nProportional Representation (PR) Systems\nThere are two types of PR Systems namely, List PR and Single Transferable Vote (STV) PR. Under List PR, voters choose from a list of candidates. Candidates are ranked on this list and become elected representatives in order of this ranking and parties are allocated seats in direct proportion to the number of votes they receive.\nList PR aims to create a representative body that more or less reflects the interests of the citizens. If a party gets 30% of the vote, they receive 30% of the seats in that representative body. South Africa, Mozambique, Namibia and other countries, use List PR. In South Africa, the party chooses its own candidates – this is a Closed List system. With Open List systems, voters choose the candidates instead.\nWith List PR, all votes have equal value and are accurately translated into the seats won by each party. There is a more representative legislature because minority parties and women in politics are given a platform. Even so, it often leads to coalition governments which are often indecisive due to each party involved trying to get their way. These systems can also lead to situations where the majority party continues to rule even when they have lost the favour of many voters (Reily&Reynolds, 2019).\nThe Single Transferable Vote (STV) used in Scotland and Ireland have voters rank candidates on the ballot based on preference. Candidates must reach a certain number of votes and anyone who has more first-preference votes than the quota is elected. STV wastes fewer votes because the extra votes received by the winning first-preference candidate, automatically go to each voter’s second favourite candidate. STV also gives voters more choice, facilitates voters’ influence in who makes up possible coalitions, and provides higher chances of the election of independent candidates.\nHowever, STV requires a certain degree of literacy and numeracy because ballot papers can get long-winded and confusing, with a complicated and long vote-counting process. STV may cause internal fragmentation within parties due to its candidate-centred approach, and possibly lead to parties with majority votes winning fewer seats than their rivals.\nMajority systems are candidate-centred with the winner being the candidate with the most votes. Candidates must win with an “absolute majority” — more than 50% of the votes. There are two types of majority systems, the Alternative Vote (AV), and the Two-Round System (TRS).\nWith AV, voters rank candidates based on preference on the ballot paper. Used in Australia and the United States of America (often called Instant Run-off Voting), it gives voters more options. AV allows the votes of the various candidates to accumulate, combining diverse but related interests to win representation (International Institute for Democracy and Electoral Assistance, 2005). Supporters of candidates who have fewer chances of winning are able to influence the election of a major candidate through their second preferences.\nAV needs a degree of literacy and numeracy for effective use. It can also produce disproportional results because it operates in single-member districts. It may lead to the election of members that voters are indifferent to or actively dislike, and it does not work well in multi-member districts.\nThe Two-Round System has two voting rounds — the first where the candidate or party must win by more than 50% of the vote and the second only happening when there is no absolute majority in the first round. How the second round is conducted differs in each country, but the most common method is a run-off contest between the top two candidates from round one.\nTRS gives voters a second chance to elect their preferred candidate (International Institute for Democracy and Electoral Assistance, 2005), or even change their minds, while enabling candidates to react to the changing political landscape in-between rounds. This system is easy to understand and lessens problems of vote-splitting. However, two elections are expensive with much electoral administration and results may be disproportionate. TRS fragments party systems in new democracies.\nElections are a defining moment in the history of any country. The electoral system used in a country matters because it influences who gets to govern a country and how it is governed. Unless a government chooses a system that works for its electorate, elections can never be free, nor fair.']"	['<urn:uuid:2cb2c77e-026b-4212-9d6c-495ef9aa6f6b>', '<urn:uuid:ce9fcfe2-08bd-4b3e-b1d3-5b8d09cfb097>']	factoid	with-premise	short-search-query	distant-from-document	three-doc	novice	2025-05-12T21:31:42.699391	5	55	1588
7	I'm going snorkeling soon - what should I do if I get stung by a jellyfish?	If you get stung by a jellyfish, avoid moving the affected limb and don't rub the area. Remove any tentacles using gloves and forceps. For most jellyfish stings, rinse with seawater and apply ice packs. However, if stung by a box jellyfish, apply vinegar for at least 30 seconds. If vinegar isn't available, use seawater and carefully remove tentacles with tweezers. Hot water (40°C) can also help with pain. Seek immediate medical attention, especially if symptoms are severe, include pain not relieved by ice, or if the stung area is large. Be particularly careful in northern Australian waters during 'stinger season' (October to March) and consider wearing protective clothing like a wet suit or lycra stinger suit.	"['Marine wounds and stings\nHazards from the marine environment include injuries, poisonous stings and secondary infection. These may result in severe pain, illness and even death. The main culprits are coelenterates, mollusca, echinoderms and poisonous fish.\nThe two most important mollusca are octopus and cone shell.\nVenomous fish include some species of stingrays and Scorpaenidae. There are many different species in the Scorpaenidae family. They vary in the size of their spines and the potency of the venom released through these spines.\nJellyfish commonly have stinging nematocysts. They are sometimes known simply as sea nettles, though often this name is reserved for the less dangerous varieties. Structurally jellyfish comprise a bell-shaped body with tentacles, some up to 30 metres in length. The nematocysts or cnidoblasts reside within theses tentacles. Stings are delivered when contact is made with the tentacles and nematocysts discharge into the skin.\nDeserving special mention are:\n- Box jellyfish or sea wasp (Chironex fleckeri) (Northern Australian waters, where they are known as ""stingers"")\n- Portuguese man of war (Physalia physalis) (Atlantic, Pacific, Indian Ocean)\n- Irukandji (Carukia barnesi); these have small box-shaped transparent bodies with 4 tentacles\nJellyfish are most prevalent in calm warm seawater, sandy beaches and harbours during the summer months. However, they are also seen in other circumstances.\nClinical presentation of jellyfish stings\nThe first symptom is of pain, often so severe as to lead to loss of consciousness. Some victims of the box jellyfish are unable to make it to the beach alive. Other symptoms include:\n- Paraesthesia (tingling and prickling) and itching\n- Red bumps and patches, often in a ladder-like pattern\n- Sweating in affected area (irukandji)\n- Nausea and vomiting\n- Muscle pain and cramps.\nDepending on the amount of stinging and the type of jellyfish encountered, shock and cardiac arrest can follow.\nAfter recovering from the initial sting, the wound may later blister and become necrotic (tissue death).\nTreatment of jellyfish stings\nIf you or a companion is stung by a jellyfish:\n- Avoid moving the affected limb. Do not rub the sting area.\n- Remove nematocysts and tentacles from the skin using gloves and forceps. Razor blades can be used to shave off nematocysts.\n- If stung by box jellyfish apply liberal amounts of vinegar (5% acetic acid), or in its absence, salt water or hot water (40C) for 30 minutes. Seek immediate medical attention.\n- If stung by the Portuguese man of war, thoroughly rinse exposed areas with seawater (vinegar is not helpful).\n- Ice or an ice pack can help with pain while seeking medical attention.\n- Antivenin injections are available in some medical centres for specific jellyfish, and are particularly recommended for box jellyfish stings.\n- Topical corticosteroid creams and oral antihistamines may be of benefit for persistent dermatitis\nDo not rub with a towel or use fresh water as this may cause further damage by causing further nematocyst discharge. Contrary to popular belief urine and alcohol can exacerbate injury and are not recommended.\nCoral cuts and abrasions are common and may become infected. Coral may also harbour stinging nematocysts.\nTreatment of coral wounds involves removing visible debris and cleaning the wound thoroughly and applying antiseptic or antibiotic cream as infection is common (watch out for redness, fever, foul smell or pus).\nTetanus immunisation is recommended.\nMost species of octopus are harmless, except for the blue-ringed octopus largely found around the coast of Australia. This is identified by iridescent blue rings that appear on its body when disturbed. Large quantities of its venom may cause paralysis but mortality is rare.\nTreatment is supportive, and the wound should be irrigated heavily. Artificial ventilation may be required in cases of paralysis.\nThe cone shell is a carnivore, living just beneath the sandy surface of shallow and deep water areas. Eighteen species of cone shell may sting humans, via a lightening-fast barb injection. The toxins target an array of ion channels, which cause paralysis in small creatures. Human fatalities are rare. Typically the sting results in local pain and numbness, and may be accompanied by general malaise.\nTreatment of cone shell stings may include:\n- Immobilisation of affected area in a dependent position (hanging down)\n- Pain relief (soaking the area in hot water may help)\n- Emergency resuscitation and respiratory support\nNo antivenin is available for cone shell stings.\nThe sea urchin is an echinoderm. It is a common source of puncture wounds because the individual often inadvertently steps on it. Local irritation is caused by embedded spines, which are often hidden by pigment release in the skin. Sea urchins are usually non-venomous, though some are capable of delivering toxin.\nTreatment should be directed at removing as many visible spines as possible.\n- Waxing and stripping the area may help remove residual spines\n- Ammonia solution can dissolve small spine fragments\n- Most thin embedded spines come out by themselves within a few weeks\nAn injury from a venomous fish results in very painful and inflamed puncture site. The venom of a stingray is found at the base of its tail.\nComplications of an injury from venomous fish may include:\n- Secondary infection leading to wound breakdown\n- Nausea, vomiting, shortness of breath and weakness\n- Death (rare).\nTreatment of injury by venomous fish\n- Carefully remove visible spines\n- Apply direct pressure to bleeding areas\n- Bathe the affected area in hot water of up to 45 degrees Celsius.\n- Tetanus prophylaxis should be considered.\n- Stonefish antivenin is available in some regions but may cause anaphylaxis (allergic reactions) so a test dose is recommended.\nSecondary infection after venomous fish injury\nSecondary bacterial infection is common after penetrating or abrasive marine injuries. Signs of secondary infection include redness, swelling and warmth.\nVibrio vulnificus infection is peculiar to marine injuries and may cause intense rapidly progressive cellulitis and death. Treatment should include tetracycline and a broad spectrum cephalosporin or aminoglycoside.\n- Bolognia JL, Jorizzo JL, Rapini RP. Dermatology 2003. Pg1346-8\n- Sivaprakasam K. Jellyfish dermatitis. Indian J Dermatol Venereol Leprol [serial online] 2015 [cited 2015 Jul 3];81:389-90. Available from: http://www.ijdvl.com/text.asp?2015/81/4/389/156198\nOn DermNet NZ:\n- Medsdcape Reference:\nBooks about skin diseases:\nSee the DermNet NZ bookstore', 'First Aid for Boating Adventures\nSummer has arrived and school holidays are here. With the warmer weather and warmer water, many families will be heading out in their boat to their favourite fishing or snorkelling spot. No matter how careful you are on your recreational boating adventures, injuries can happen. Having a well-equipped First Aid kit, and being prepared to treat injuries, are essential for a safe day out. What are some of the injuries that boaties need to be prepared for?\nFishing and Snorkelling Injuries\nFishing hook wounds\nGetting a fishing hook embedded in a hand or foot is a relatively common injury according to the British Journal of Sports Medicine. Because of their irregular shape, hooks should only be removed if they have not penetrated deeply and it is certain that there are no tendons or joints involved in the injury. Never attempt to remove a fishing hook from an eye.\nOnly if it is safe to do so, remove the hook. Then clean and cover the wound to help prevent infection. If it is unsafe to remove the hook, seek prompt medical attention.\nCoral cuts and scrapes\nAccording to the Divers Alert Network, coral cuts and scrapes are perhaps the most common injury experienced by divers and snorkelers in the tropics. Coral injuries can get infected very easily and take a long time to heal. Clean and cover coral wounds promptly.\nInjuries from marine life\nMany marine animals use toxins for catching prey or for defence and their venom may also be life-threatening to humans. Envenomation can occur through a bite, sting or puncture wound so the best thing is to prevent injury by leaving all marine life well alone. Fortunately, with care, the risk of injury from marine life is low.\nThe Royal Life Saving Society, in its Swimming and Lifesaving manual, recommend the following emergency care for envenomation. In all cases, watch for shock, monitor breathing and circulation, and be prepared to administer cardiopulmonary resuscitation (CPR).\nBlue-ringed octopus bite and cone shell sting\nTreat as for snakebite with pressure immobilisation. Apply a firm bandage over the bitten area then bandage the entire limb. Apply a rigid splint and keep still. Seek medical aid urgently.\nScorpion fish and stonefish spines\nPlace the affected area in water that is as hot as can be tolerated. Seek medical assistance immediately.\nDo not remove the barb. Apply water that is as hot as can be tolerated over the stung area. Be careful not to burn as the affected area may be numb from the venom. Seek medical aid urgently.\nJellyfish and bluebottle stings (NON-TROPICAL)\nUsing tweezers, gently remove any tentacles adhering to the skin or rinse off with sea water. In the case of non-tropical bluebottles, apply hot water over the stung area. For non-tropical jellyfish stings, apply cold packs or wrapped ice. Seek medical aid in severe cases, if pain is not relieved, or if the stung area is large.\nBox jellyfish and Irukandji stings\nIn northern Australian waters always wear protective clothing such as a wet suit or lycra stinger suit during ‘stinger season’ (October to March). If stung, seek medical aid urgently.\nFlood the affected area with copious quantities of vinegar for at least 30 seconds. If vinegar is not available, rinse well with sea water, gently detaching the tentacles with tweezers. Apply ice packs to assist in pain relief.\nShark bite and serious bleeding\nThe number of fatal shark attacks in Australia in 2020 has been worrying for many swimmers and snorkellers.\nThere is much you can do to minimise your risk of a dangerous shark encounter, however, and the Queensland Government’s Shark Smart site (https://www.daf.qld.gov.au/sharksmart) has good advice. This includes staying out of the water at dusk and dawn and keeping fish waste and food scraps away from where people swim.\nWhile the risk of an unprovoked shark attack is small, it is sensible to be prepared to treat serious bleeding. The Australian Resuscitation Council (ARC) recommends firm pressure on or around the wound as the most effective way to stop bleeding. Elevating the bleeding part and restricting movement will also help to control bleeding. Only as a last resort for life-threatening limb bleeding, the ARC recommends using an arterial tourniquet. A tourniquet is applied to the limb above the bleeding point when arterial bleeding is not able to be controlled by direct pressure.\nReady to Respond\nTo make sure you are ready to respond to injuries when out boating:\n• Get First Aid training and stay up to date\n• Make sure you have downloaded the Rescue Swag or St John’s First Aid app to your phone. They work anywhere and will give you immediate advice in any emergency, saving vital minutes and keeping you calm.\n• Know your emergency service numbers and how to use your radio\n• Have the basic safety gear that is required by local law\n• If you swim or snorkel, take hot water (a vacuum flask will stay hot for a day trip), icepacks (instant cold packs or freezer bricks from the esky) and vinegar in the tropics\n• Take a well-equipped first aid kit such as Rescue Swag’s Adventurer.\nAdventurer First Aid Kit for Boating Adventures\nLike all the Rescue Swag First Aid kits, the durable Adventurer is water and dust-proof so its contents stay clean and dry.\nThe three modules inside – for minor injuries, major injuries and snake bites and burns - ensure you have easy access to what you need, when you need it.\nFor example, the minor injuries module with cleansing wipes, splinter probes and band aids would be your go to for most coral cuts and scrapes. The snake bite module has a compression bandage suitable for pressure immobilisation in the case of octopus bites or cone shell stings.\nThe Adventurer’s outer cover also converts to a sling or splint which is useful for immobilising a bitten or stung limb. When moored, we use the cover’s straps to attach the Adventurer kit to our bimini where it is plainly visible and easily accessible.\nStop the Bleed\nThe Adventurer comes with everything that you need in a standard recreational boating First Aid kit. Because we love swimming and snorkelling on our boating adventures, we add additional wound dressings and a tourniquet. Rescue Swag stock the Rescue Bandage TQ, a lightweight and compact inflatable tourniquet that conforms to ARC guidelines.\nThis article is for information only and is not meant to take the place of skilled medical advice and care. Whether you go fishing, snorkelling on the reef, or just enjoy a day on the water, take care and enjoy safe summer boating.\nAmanda Freeman is an ecologist and writer who enjoys boating, snorkelling and diving on the Great Barrier Reef near her home in far north Queensland. She is also a qualified snorkelling instructor. She can be found at www.naturenorth.com.au']"	['<urn:uuid:e1b864d9-3fc6-4b90-b8e7-670b60bade04>', '<urn:uuid:6606f268-b01b-478e-ad1d-e6d62ba6583b>']	open-ended	with-premise	concise-and-natural	similar-to-document	three-doc	novice	2025-05-12T21:31:42.699391	16	117	2176
8	how to clean diamond jewelry at home	To clean diamond jewelry, create a warm solution of water and mild detergent. Dip the jewelry in this solution for 2-5 minutes. Then gently scrub the diamond with a soft toothbrush or jewelry brush. Rinse the diamond in warm water to remove any remaining detergent, and buff it dry with a cloth. While ammonia solutions are effective at cleaning diamonds, they should be avoided as they can remove color from most metals.	['How to clean Jewelry\nOver time the natural oils from your skin, air born contaminants, moisture and corrosion will tarnish the shine and luster of your jewelry. There are a few simple cleaning tips which can drastically improve the condition of your favorite pieces. Different metals and stones require different cleaning techniques so please ensure you are following the relevant instructions. If your gold or silver jewelry contains softer porous gemstones you should follow the cleaning instructions for the gemstones and not the metal, these gemstones are quite sensitive and can be damaged easily.\nA must before you start cleaning\nCheck the setting of your jewelry to ensure it is secure and well fastened. If you begin scrubbing a loose stone you may damage or even lose a piece. Remove any dust from both the jewelry and cleaning clothes. The dust may cause light scratching during the cleaning process. If you donít own a jewelry cloth, use a cotton or flannel cloth and shake it well to remove loose dust.\nHow to clean Gold Jewelry\nSoak your gold jewelry in a mild detergent and warm water solution for 1-3 minutes. You should avoid chlorine and ammonia as both can discolor the metal. Remove and gently brush the jewelry with a soft toothbrush or jewelry brush to remove any remaining grime. Now carefully rinse the gold jewelry in warm water to remove the detergent solution. Next thoroughly dry the gold jewelry with your cloth, any remaining moisture will dull the shine of the gold.\nHow to clean Silver Jewelry\nThere are two ways to easily remove the buildup of tarnish from your silver jewelry. If you live in a humid climate your silver jewelry will tarnish quickly and youíll need to clean it quite regularly. We are going to create an inexpensive silver dip solution. The silver dip works by creating a chemical reaction which removes tarnish. It is a great technique for cleaning silver chains which are difficult to clean by hand. You must not under any circumstances place any type of gemstones in the solution, it may permanently damage the stone. Firstly find a bowl or large cup and line the sides with aluminum foil. Next place your jewelry inside the lined cup or bowl. Cover the jewelry with a small amount of baking soda, just enough so you can barely see the jewelry. Next pour hot water over the jewelry and wait for the bubbling to stop. After the solution has stopped bubbling rinse and buff the silver jewelry dry with your cloth. You may need to repeat the process up to 5-6 times for badly tarnished pieces. Please also be careful to keep yourself away from any fumes and ensure the room is well ventilated before starting. To clean silver jewelry which contains gemstones create a solution of mild detergent and warm water. Now gently buff small sections of the jewelry taking care not to touch the gemstone. Rinse and dry to complete the process. If the jewelry is heavily tarnished or still requires cleaning you may require a specialized silver cleaning paste or professional cleaning. Keep in mind that the harsher the cleaner the more likely your jewelry will be damaged.\nHow to clean Diamonds\nOver time diamonds begin to look cloudy and lose the natural sparkle and shine that makes them so attractive. The good news is that a quick clean on all but the dirtiest diamonds will restore the original shine and luster. Ammonia solutions are effective at cleaning diamonds but will eventually remove the color from most metals so unless your diamonds are not set in any metals we suggest you avoid cleaning in an Ammonia solution. Dip the diamonds jewelry in a warm solution of water and mild detergent for approximately 2-5 minutes. Next gently scrub the diamond with a soft toothbrush or jewelry brush if you have one. After scrubbing rinse the diamond in warm water to remove any remaining detergent. Buff the diamond dry with your cloth.\nHow to clean Opals\nOpals are relatively soft, porous and usually quite thin (and fragile). The following cleaning tips are only useful for removing small amounts of grime. If your opal has lost its original shine and luster you should definitely visit an opal specialist and have your opals professional cleaned. To remove light amounts of grim with a mild warm soap water solution gently scrub your opal with a soft brush. Rinse the opal and buff dry with your cloth.\nHow to clean Pearls\nIf youíre cleaning a pearl necklace please ensure the pearls are secure before cleaning. Create a warm solution of soapy water, use small amount of baby soap. Baby soap is gentle and more importantly PH neutral, abrasive soaps will easily damage your pearls. Place the peals in the solution for a minute or so and then rub the pearl dry with your cloth. When cleaning pearls you really need to avoid any form of abrasion. Use only soft cotton or jewelry cloth and gentle soaps.\nHow to clean Gemstones\nThe majority of gemstones are best cleaned using the same method as Pearls. You may also use a soft tooth brush on hard gemstones to gently scrub away additional grime. You should avoid soaking porous (softer opaque) gemstones instead just wet another cotton or jewelry cloth and clean the gemstone. Itís important to dry the gemstone completely to remove any remaining chemicals from the cleaning process. Ultrasonic cleaners will work well for harder gemstones but may cause cracking on softer gemstones, for this reason we suggest taking your jewelry to a professional for heavier cleaning.\nThings to avoid when cleaning\n- Many metals and softer gemstones can be easily damaged by abrasive chemicals in certain cleaners. This is why it is always best to start your cleaning with relatively gentle methods before tackling any remaining grime and tarnish.\n- Do not clean your jewelry over a sink or you may find yourself frantically sourcing a plumber.\n- Rough cloths may scratch metals and softer stones, use soft lint free cloths only.\n- Do not under any circumstances use harsh chemicals on gemstones which are porous. Porous stones will absorb the chemicals and damage your stones. Porous gemstones are opaque gemstones like Pearls, Turquoise and Opals.\n- Cleaning expensive vintage jewelry is probably a job best left to the professionals.\n- Do not use pins, tweezers or other sharp objects to remove small particles of grime, you will only scratch the metal setting of your jewelry.\nWhen jewelry should be professionally cleaned\nAntique or delicate jewelry should nearly always be cleaned by a professional jeweler. Certain metals will naturally change color over time and so your antique jewelry may not be as dirty as you think. Silvers in particular tend to become slightly darker over time, this is not always tarnish which requires cleaning. Professional jewelry cleaning is often complimentary and if not is very inexpensive. A professional jeweler will also inspect and repair any loose settings which may cause problems down the track. Professional jewelers also have access and knowledge of a wide range of dedicated and specialized cleaning methods and chemicals which are much more effective than the methods described above.']	['<urn:uuid:dffe2083-da36-4cc7-b90e-7e1f906996c7>']	open-ended	direct	short-search-query	similar-to-document	single-doc	novice	2025-05-12T21:31:42.699391	7	72	1203
9	What's the main difference between how infrared waves and electromagnetic waves store their energy?	Infrared waves store energy through the motion of atoms and molecules in an object, with higher temperatures causing more molecular motion and thus more infrared radiation. In contrast, electromagnetic waves store their energy in perpendicular electric and magnetic fields that vibrate together.	"['Teachers Guide to the Infrared\nOur eyes are detectors which are designed to detect visible light waves (or visible radiation). There are forms of light (or radiation) which we cannot see. Actually we can only see a very small part of the entire range of radiation called the electromagnetic spectrum .\nThe electromagnetic spectrum includes gamma rays, X-rays, ultraviolet, visible, infrared, microwaves, and radio waves. The only difference between these different types of radiation is their wavelength or frequency. Wavelength increases and frequency (as well as energy and temperature) decreases from gamma rays to radio waves. All of these forms of radiation travel at the speed of light (186,000 miles or 300,000,000 meters per second in a vacuum).\nInfrared lies between the visible and microwave portions of the electromagnetic spectrum. Infrared waves have wavelengths longer than visible and shorter than microwaves, and have frequencies which are lower than visible and higher than microwaves.\nInfrared can be used as a way to measure the heat radiated by an object. This is the radiation produced by the motion of atoms and molecules in an object. The higher the temperature, the more the atoms and molecules move and the more infrared they produce. Any object which has a temperature i.e. anything above absolute zero (-459.67 degrees Fahrenheit or -273.15 degrees Celsius or 0 degrees Kelvin), radiates in the infrared. Absolute zero is the temperature at which all atomic and molecular motion ceases. Even objects that we think of as being very cold, such as an ice cube, emit infrared. When an object is not quite hot enough to radiate visible light, it will emit most of its energy in the infrared. For example, hot charcoal may not give off light but it does emit infrared which we feel as heat. The warmer the object, the more infrared it emits. We experience infrared radiation every day. The heat that we feel from sunlight, a fire, a radiator or a warm sidewalk is infrared. Although our eyes cannot see it, the nerves in our skin can feel it as heat. The temperature-sensitive nerve endings in your skin can detect the difference between your inside body temperature and your outside skin temperature. We also commonly use infrared rays when we operate a television remote.\nNote: To help your students understand that the Sun does indeed put out infrared light, you might want to have them perform The Herschel Experiment, in which they will have the opportunity to discover the existence of infrared light in sunlight for themselves.\nHow do infrared cameras work?\nThermal infrared imagers are detector and lens combinations that give a visual representation of infrared energy emitted by objects. Thermal infrared images let you see heat and how it is distributed. A thermal infrared camera detects infrared energy and converts it into an electronic signal, which is then processed to produce a thermal image and perform temperature calculations. Thermal imaging cameras have lenses, just like visible light cameras. But in this case the lens focuses waves from infrared energy onto an infrared sensor array. Thousands of sensors on the array convert the infrared energy into electrical signals, which are then converted into a false-color image.\nWhat is false color?\nSince we cannot see infrared light with our eyes, infrared cameras create visible ""false color"" representations in which different temperature ranges are assigned a different color. Infrared cameras often have several color scales to choose from. A false color image is one in which the colors are not the ""true colors"" of the object being imaged. The colors in the image do accurately represent variations in the brightness of the object at the observed wavelengths, but they are often dubbed ""false"" since they do not represent the visible light, or naked eye, appearance.\nWhat do the colors in an infrared image represent?\nAnything which has a temperature puts out infrared light. In the infrared images shown in these lesson plans, different colors are used to represent different temperatures. You can find out which temperature a color represents by using the color-temperature scale show to the right of most of the images. The temperatures are in degrees Fahrenheit.\nWhat specifically do infrared images reveal?\nInfrared is a type of light that we cannot see with our eyes. Our eyes can only see what we call visible light. Infrared light brings us special information that we do not get from visible light. It shows us how much heat something has and gives us information about an object\'s temperature. Everything has some heat and puts out infrared light. Even things that we think of as being very cold, like an ice cube, put out some heat. Cold objects just put out less heat than warm objects. The warmer something is the more heat it puts out and the colder something is the less heat it puts out. Hot objects glow more brightly in the infrared because they put out more heat and more infrared light. Cold objects put out less heat or infrared light and appear less bright in the infrared.\n||By using special infrared cameras, we can get a view of the infrared world. These cameras are very useful and have even helped save people\'s lives. In the infrared, you can ""see"" in the dark. Even if the Sun is down and the lights are off, the world around us still puts out some heat. The infrared picture to the right shows deer in a forest during a dark night. Notice how we can clearly see the heat from the deer, especially from areas not covered with thick fur like the ears, face and legs. The trees and the ground put out less heat than the deer, but can still be seen through an infrared camera.|\nWarm-blooded animals, like people, try to keep the same body temperature during both the day and the night. Their body temperatures do not change when it gets dark or cold outside and their heat remains about the same. This makes infrared cameras very useful for finding people who are lost at night or lost at sea. The warm body heat from a person will cause them to glow brightly in the infrared, even in the dark or floating in a cold sea. Police can use infrared cameras to find criminals hiding in the dark and firefighters also use infrared cameras to find the hot spots in a fire.\nInfrared cameras are also a good way to study warm-blooded animals at night, and are used to study how animals use fur, feathers and blubber to keep themselves warm. They are also useful for showing the difference between warm and cold-blooded animals. To learn more about warm and cold-blooded animals visit our Infrared Zoo website.\nAnother interesting fact about infrared light is that it can travel through thick smoke, dust or fog, and even some materials.\nBecause infrared light can travel through thick smoke and visible light cannot, infrared cameras are used by firefighters to find people and animals in smoke filled buildings. The infrared body heat from people and warm-blooded animals can travel through the smoke and cause them to show up clearly through an infrared camera. Many people and their pets have been saved by firefighters using infrared cameras. Also, because infrared light can travel through thick fog, it is very useful to have infrared cameras on ships and airplanes to help in navigation.\nInfrared cameras are also used by satellites in space to measure the temperature of the oceans, to study the Earth\'s weather during both the day and night, and to study the infrared light from outer space.\nTo learn more about the everyday applications of infrared imaging visit our web site Seeing Our World in a Different Light\nInfrared light is only one of the types of light that we cannot see with our eyes. There are many more, such as X-rays, gamma-rays, ultraviolet light and radio waves. Each of these different types of light brings us new information that we cannot get by using our eyes alone. We are very lucky that we live in a time when we have technology that allows us to ""see"" all of these types of light.', 'In electromagnetic waves, the amplitude is the maximum field strength of the electric and magnetic fields ((Figure)). The wave energy is determined by the wave amplitude. Energy carried by a wave depends on its amplitude.\nWhat does the energy of electromagnetic wave depend on?\nThe energy of an electromagnetic wave depends on its frequency and wavelength. The higher the frequency and the shorter the wavelength, the stronger the energy propagated.\nWhat is the most important source of electromagnetic wave?\nThe most important source of electromagnetic waves on Earth is the sun. Electromagnetic waves travel from the sun to Earth across space and provide virtually all the energy that supports life on our planet.\nWhat determines the type of electromagnetic wave?\nRadiation and Temperature\nWhat determines the type of electromagnetic radiation emitted by the Sun, stars, and other dense astronomical objects? The answer often turns out to be their temperature. At the microscopic level, everything in nature is in motion.\nWhat interferes with electromagnetic waves?\nThe electromagnetic energy from the source propagates through the path and interferes with the operation of the receptor. All three must exist to have an EMI problem. … The three most common EMI problems are radio frequency interference, electrostatic discharge, and power disturbances.\nWhere is energy stored in electromagnetic waves?\nThe E and B fields, along with being perpendicular to each other, are perpendicular to the direction the wave travels, meaning that an electromagnetic wave is a transverse wave. The energy of the wave is stored in the electric and magnetic fields.\nWhat is energy carried by electromagnetic waves called?\nElectromagnetic waves are waves that consist of vibrating electric and magnetic fields. They transfer energy through matter or across space. The transfer of energy by electromagnetic waves is called electromagnetic radiation. … The two vibrating fields together form an electromagnetic wave.\nWhat are three sources of electromagnetic waves on earth?\nSources of Electromagnetic Radiation\n- solar radiation, in other words natural radiation that originates from the sun.\n- terrestrial radiation, in other words natural radiation emitted by the Earth’s surface.\n- artificial radiation originating from a remote sensing system.\nWhat is the ultimate source of electromagnetic waves?\nThe ultimate source of electromagnetic waves is moving charged particles. These travel at the speed of light and are composed of electric and magnetic…\nHow is electromagnetic waves created?\nElectromagnetic waves are formed when an electric field (shown in red arrows) couples with a magnetic field (shown in blue arrows). Magnetic and electric fields of an electromagnetic wave are perpendicular to each other and to the direction of the wave.\nWhat are the 4 main properties of electromagnetic waves?\nLike other waves, electromagnetic waves have properties of speed, wavelength, and frequency.23 мая 2019 г.\nWhat are the 7 types of waves?\nThough the sciences generally classify EM waves into seven basic types, all are manifestations of the same phenomenon.\n- Radio Waves: Instant Communication. …\n- Microwaves: Data and Heat. …\n- Infrared Waves: Invisible Heat. …\n- Visible Light Rays. …\n- Ultraviolet Waves: Energetic Light. …\n- X-rays: Penetrating Radiation. …\n- Gamma Rays: Nuclear Energy.\nWhat are the 7 types of electromagnetic waves?\nThe electromagnetic spectrum includes, from longest wavelength to shortest: radio waves, microwaves, infrared, optical, ultraviolet, X-rays, and gamma-rays.\nDo Humans give off electromagnetic waves?\nHumans give off mostly infrared radiation, which is electromagnetic radiation with a frequency lower than visible light. … “Thermal radiation” is all the electromagnetic waves given off by an object because of its temperature, and includes radio waves, infrared waves, and even visible light.\nWhat can stop electromagnetic waves?\n5 Tips to Safeguard Against Electromagnetic Radiation\n- Disable Wireless Functions. Wireless devices — including routers, printers, tablets, and laptops — all emit a Wi-Fi signal. …\n- Replace Wireless With Wired Devices. …\n- Keep EMF Sources at a Distance. …\n- Use Your Smartphone Safely. …\n- Prioritize Sleeping Areas.\nCan electromagnetic waves affect the brain?\nElectromagnetic waves, particularly RF-EMFs emitted by mobile phones are absorbed into the brain to such an extent that it can affect the activity of neurons (Kleinlogel et al., 2008; Hinrikus et al., 2018). … Glucose metabolism in the brains that were exposed to RF-EMF increased rapidly.']"	['<urn:uuid:b00d9ff2-21de-416d-a93b-3e7eddfec969>', '<urn:uuid:ab115dc6-72f5-46a2-b5ba-7df49c3fa0ac>']	factoid	direct	verbose-and-natural	similar-to-document	comparison	novice	2025-05-12T21:31:42.699391	14	42	2069
10	symbolic religious food types examples	Different religions use specific foods as powerful symbols in their traditions. In Judaism, the Passover Seder includes haroset (a mixture of apples, wine and spices) symbolizing mortar used by slaves, while horseradish represents bitterness of bondage. The unleavened matzah bread symbolizes both the hasty escape from slavery and humility. In Islam, dates hold special significance during Ramadan's Iftar meals. Hindu temples practice prasadam, offering food to deities, while Buddhist temples use rice and oil offerings to honor deceased loved ones. Christians use bread and wine in Holy Communion to remember Christ's sacrifice. These symbolic foods combine physical nourishment with spiritual meaning across different faiths.	['The God Squad: Passover\nThis week’s column is about what it means to eat a meal for God. Next week, we will focus on Easter and ask what it means to eat a meal of God.\nWhen Father Tom Hartman was alive, he would write the Passover column and I would write the Easter column. It was our way of proving and sustaining the mission statement of the God Squad that, “We know enough about how we are different but not nearly enough about how we are all the same.”\nNow I am all alone to do his work and sustain his memory. We met at News 12 Long Island on Passover/Easter 1987 and this time is filled with his soul and his smile for me.\nBoth the similarities and the differences of Passover and Easter are profound, and we must understand them not only to understand each other’s way up the same mountain to God, which is critical for both Christians and Jews, but to understand what it means to be human in a broken world. This is important for all human beings living with or without organized religion as best we can here on planet earth.\nBoth Passover and Easter bring our families together. This was always important, but never more so than right now. COVID-19 has robbed us of family holiday dinners. Last year, I connected our family for the Passover Seder meal via Zoom. It was great to include far-flung family that never could have made it to our home in the old normal times, but it was painful to not be able to touch and hug and kiss and clean the kitchen together. Families cannot thrive electronically. Families thrive around the holiday table. I pray that this year we will be able to touch each other again.\nThe Seder meal, which precedes the actual meal, is a form of teaching and praying, as well as eating. Each of the ritualized foods of the Passover Seder is a teaching.\nDipping greens called karpas into salt water combines the despairing symbolism of the tears of slaves with the hopeful symbolism of the new growth of springtime.\nThe mixed apples and wine and spices called haroset symbolizes the mortar used to cement the bricks of pharaoh’s monuments and the horseradish root includes once again the bitter taste of being owned.\nTogether these ritual foods of the Seder meal teach us that in life bitter and sweet, good and evil, despair and hope, burdens and blessings are always mixed.\nBread is life but matzah, the Passover bread, is different. It is unleavened and flat and unseasoned. Matzah was the bread of escape from bondage that did not have enough time to rise. Leavening is also a symbol of arrogance (puffery) and so the unleavened matzah is a symbol of humility. We take for granted so much and so eating this bread reminds us to give thanks for the least of our blessings. This is the only way to train ourselves to give thanks for the greatest of our blessings.', 'The summer season is high time for good eats. Whether it’s a good barbecue, a tasty cold treat or tempting your palate with adventurous delicacies from your latest travel destination, summer time is a fantastic season for succulent spreads.\nWhile many here in Houston, and throughout the nation, are preparing for an August of barbecued burgers and brats many Muslims are preparing for Ramadan, a month long dawn to dusk religious fast beginning today, August 1 and ending with Eid al-Fitr on August 31. The Ramadan fast is held in honor of the prophet Muhammed’s reception of the first parts of the Quran in 610 C.E. when, according to Muslim teachers, he did not eat or drink from sunrise to sunset. While the vast majority of Muslims beyond puberty will refrain from consuming food and drink during the day they will enjoy special night-time mini-meals (Iftar) that will sustain them through the long hours of abstention during daylight.\nOf course, Ramadan is not only about what Muslims are not eating, but the significant meaning of items they do eat. Ramadan is a rich time of spiritual formation for Muslims in which food such as dates at the beginning of each Iftar and the many delights of Eid al-Fitr are sweet not only to the tongue, but to the soul of the faithful who devoutly deviate from their normal eating routine to learn patience, continue to submit to Allah and remember to give generously to others.\nThis reminds religious students and careful observers just how important food is in religion. Religious eats (and don’t eats) help us learn more about a religion. All religion is more than belief, religion is also very much about ritual, and food often features in important religious rites. As Corrie Norman points out in a Harvard Pluralism Project forum paper from 2003 religion, and its ritual, is experienced in the body, experienced in community and often experienced beyond the bounds of a temple, synagogue or mosque. Food is oft involved with this religious experience outside of confessions, creeds and churches. Indeed, by using our taste buds and our tongues we can stand to learn a lot about other religions.\nHoustonians, and other Americans, may be surprised at the diversity of religious eats available for them to experience and learn from. Below are examples of religious eats from several traditions, each with links and opportunities for foodies and interested religious adventurers to learn more about each religion and its culinary cultus:\n+When I think of religious eats and the importance of food for religious ritual, my first thoughts are of the many foods of Judaism. Whether it’s grabbing a chunk of braided challah bread on Shabbat, enjoying some Hamentaschen on Purim, chomping on some Kosher chips and salsa with a good Jewish friend or solemnly remembering the night through the ritual foods of the Passover Seder, Judaism is rich with religious eats. The next time you’re in the store, check the Kosher section and enjoy a “set apart” snack or try making your own braided challah for a relaxing Friday evening Shabbat meal with friends. Not only will you be blessed by the company and the time for rest, you will learn a little more of the rhythm and ritual of Jewish life.\n+Although many are familiar with the concept of Kosher foods in Judaism, others are either ignorant or oblivious to the Halal practices of Islam. As discussed above, there are many important symbolic foods (such as dates) that are eaten during the Iftar of Ramadan or at Eid al-Fitr. However, Muslims continue to pay attention to what they eat throughout the year as they live out their lives in submission to Allah by following the Halal rules. Halal regulations most often appertain to meat and its preparation, but also apply to many other foods. To learn more, and live a little, check out one of the many Halal markets or restaurants in Houston or your metro area.\n+Ever since my wife and I moved to Houston we have a new less-than-guilty pleasure – the delights of our local Udipi Cafe. Offering an all vegetarian spread, Udipi has become our Indian joint of choice. While it is easy to forget the religious significance of the baingan bharta I am devouring along with a healthy dose of malai kofta, Udipi cafes owe their cuisine to a Dvaita school of Hinduism that highlights ahimsa (non-violence) as a prime practice of Hinduism. Thus, devout Udipi Hindus eat, and serve (to our delight) only vegetarian dishes. Of course, Hinduism’s religious eats aren’t all vegetarian, there are many meat-eating Hindus hailing from places like Andra-Pradesh and Punjab province in India. A good way to learn more about Hindu eats is to visit a local Hindu temple and observe the practice of prasadam, where Hindus offer food to their deities and enjoy the leftovers.\n+A very similar devotional practice can be observed in Buddhist temples where oil and rice offerings are given for the monks or left in honor of lost loved ones. Such practices are alive and well in the Vietnamese Buddhist community, whose active Quan-Am temple in Southwest Houston can regularly be seen filled with fruits and oils left to honor the departed. Certainly, any discussion of Buddhist eats is not complete without reference to the intricate and spiritually significant Zen tea ceremony. There are local opportunities in Houston for interested individuals to learn about Zen tea ceremonies or experience a demonstration.\n+Coming from a Lutheran-Christian background, the food I think of when I think “church” is often found at a potluck. Usually potato “themed,” the dishes of a Christian potluck are not exactly culinary “delights” (although you’ve not lived until you’ve tried green jello with carrots). Jokes aside, Christians have important religious eating traditions as well. Foremost among them is the sacrament of Holy Communion where Christians partake of bread and wine in remembrance of Christ’s sacrifice and his final meal with his disciples. In addition to this practice, many pagan cultural practices were translated into Christian traditions by various cultures. Whether its the lamb cakes of the Czechs, the hot cross buns of the Brits, the Tarta de Santiago of Spain or the rich-with-meaning Slovak Christmas meal (velija) that my family enjoys each year, Christianity has its spiritually significant meals.\nAs evidenced above, when it comes to religious eats there are thousands of delicious avenues for the inquisitive to enjoy. So dive in, get your fill and along the way learn a little more about ancient faith and practice in the modern world through religious eats. All the while, as mom would say, remember you manners, be respectful and always learn the hows, whos, whys and whats. But otherwise, bon apetit!']	['<urn:uuid:3d97d8c4-ed84-4611-825d-ec2c0beada57>', '<urn:uuid:9ea7c1bf-c4e4-48d1-8c7c-22dc93997e7d>']	open-ended	direct	short-search-query	distant-from-document	three-doc	novice	2025-05-12T21:31:42.699391	5	104	1633
11	How do Miami's museums and galleries approach gender and cultural representation, and what initiatives do they take to showcase diverse artistic voices?	Miami's art institutions actively promote diverse artistic voices through various approaches. The Perez Art Museum Miami (PAMM) specifically aims to represent Miami's culturally diverse population through its contemporary art collections. In the gallery scene, spaces like N'Namdi Contemporary focus on education, culture, and aesthetics, while the David Castillo Gallery showcases artists who explore historical, cultural, and personal identities. The Fredric Snitzer Gallery has long championed Latin American artists. This commitment to diversity is also reflected in exhibitions like Norwegian Crafts' showcase at Design Miami, which presented feminist craftspeople Elisabeth von Krogh and Ellen Grieg, whose works challenge cultural stigmas about craft as a 'feminine' activity and address political and historical issues through their respective mediums.	"['Norwegian Crafts at Design Miami\nAt Design Miami, Norwegian Crafts showcases key pieces by the ever-more-relevant and politically-charged oeuvres of feminist craftspeople Elisabeth von Krogh and Ellen Grieg.\nVenerated platform Norwegian Crafts joins forces with Galleri Format Oslo to mount a special Curio showcase at this week’s Design Miami (6-10 December). Juxtaposing conceptual yet palpable works by two of the Scandinavian country’s most respected craftspeople, the Insubordinate Creatures exhibition presents anthropomorphised minimalist forms that reflexively rebel against the cultural stigma of craft production as a purely ‘frivolous’ and feminine activity. Visually-impactful ceramic vessels by Elisabeth von Krogh and vibrant fibre rope samplings by Ellen Grieg are emblematic examples of both master’s half-decade-long careers and a shared foundation in Second Wave Feminism.\nAfter having both graduated from The National College of Art and Design Norway in Oslo (today known as the Oslo National Academy of the Arts) in the late 1970s, von Krogh and Grieg embarked on separate paths while remaining true to the craft methodology. Whereas von Krogh became a studio ceramicist – creating works for numerous commissions and exhibitions – Grieg dedicated her time to teaching – running the textile department at her alma mater.\nProducing her own pieces, Grieg’s main focus has been the rich Norwegian tradition of rug-making, knitting, and weaving; techniques that have historically resulted in folk costumery and cultural tapestries. Exhaustive research into different dyes and fibre variants has allowed her to explore themes like Norway’s naval heritage and especially mooring ropes. For Insubordinate Creatures, Grieg began a new series of abstract and suspended helical sculptures that shed light on this history.\nEmerging on the scene just as Norwegian craft pottery resurged in the late 1960s, von Krogh established her practice on both artistic and artisanal terms. Her ceramics play with optical effects to highlight political and historical issues. For von Krogh, the medium is an almost archaeological means of making statements about mankind’s shared experience. Her works manipulate perspective and scale while embracing the vessel as a symbol of the female body.\nBoth employ Postmodern treatments: semiotics, elements of style, ornamentation, colour, and shape to create cohesive creations. “The works of Grieg and von Krogh activate the space they are in,” Norwegian Crafts director Lars Sture describes, “at the same time as they highlight the tension between two and three dimensions, space and surface, reality and illusion. Often their pieces take on a size that gives them a bodily presence – they appear as sculptures expressing themselves as purely aesthetic objects, ideas, and metaphors.”\nCrossing paths again, the craftspeoples’ respective oeuvres are brought together for this showcase by the scenographic design of British architect Sean Griffiths. His interventionary concept plays on the two- and three-dimensional qualities of both master’s work. Painted outlines suggests the presence of rooms-within-rooms, interacting with the formal illusions the exhibited works project. “When viewed from certain specific positions, the coloured stripes coalesce into three dimensional illusions of rectangular shapes that frame and support the art objects,” Sture adds. “The frames can either seem to be floating in response to the hanging textile sculptures by Ellen Grieg, or they sit on the ground framing Elizabeth von Krogh’s vessels.”', 'Must-See Galleries, Museums and Cultural Events in Miami\nWith its design district, notable museums and countless art galleries, Miami is one of the most culturally-rich cities in the country. Check out our picks for top artsy destinations.\nPhoto By: Armando Colls\nPhoto By: Getty Images/John Parra\nPhoto By: Bill Sumner\nPhoto By: Getty Images/Sean Drakes\nPhoto By: Getty Images/Jeff Greenberg\nPhoto By: Jumaane N\'Namdi\nPhoto By: Andres Conde\nPhoto By: Mariano BNS\nPhoto By: Calle Ocho Festival\nPhoto By: Gene Schiavone\nPhoto By: Food Network and Cooking Channel South Beach Wine and Food Festival\nPerez Art Museum Miami (PAMM)\nIt\'s hard to resist the critically-heralded, distinctive architecture by Pritzker Prize-winning Swiss firm Herzog & de Meuron that welcomes visitors to the Perez Art Museum Miami (PAMM). Originally known as the Miami Art Museum (MAM) when it opened to the public in 1984, the museum relocated to its current location in 2013 and was renamed PAMM to recognize art collector and philanthropist Jorge M. Perez\'s financial and collection gifts to the museum. PAMM focuses on contemporary art from the 20th and 21st century and aims to represent Miami’s culturally diverse population. With an impressive view of Biscayne Bay from the museum\'s in-house waterfront restaurants Verde and Cucuyo, the museum offers plenty of things to keep visitors busy beyond the galleries, including the PAMM Shop gift shop and regular outdoor music events. And art fans visiting Miami should also be sure to check out the Rubell Family Collection a contemporary art museum whose collection includes Kara Walker, Keith Haring, Jeff Koons and Yayoi Kusama, among many others, along with the Institute of Contemporary Art, Miami, which promotes experimentation in the world of contemporary art.\nIf you really want to experience the unique ambiance of Miami’s art and culture scene, the Wynwood district should definitely be at the top of your list. This edgy urban district full of restored historic buildings is also home to Wynwood Walls, a vibrant, inspiring display of graffiti and street art by artists from around the world, with new murals added continually. Wynwood is also chockfull of local restaurants, art galleries and monthly art walks and has become known as a must-visit destination for fashion enthusiasts, artists and foodies.\nVizcaya Museum and Gardens in Miami\nTake a trip to 1900s Italy by heading over to Vizcaya Museum and Gardens. In 1910, retired millionaire James Deering was told he needed to reside in a warm, sunny climate to assist with treating his anemia. He found his perfect respite in South Florida, where he built an Italian-inspired vacation home for himself. After Deering’s death in 1925, the home became a historical landmark that is now open to the public. During your time at Vizcaya, choose a self-guided tour, an audio tour, or a group tour with a Vizcaya guide. You can roam throughout the mansion to observe Deering\'s original furnishings and artwork. When you complete your tour, make sure you take some time to wander through the 10 acres of European-inspired gardens.\nArt Basel, hosted in Miami, Basel and Hong Kong, is a world-class contemporary art fair that draws collectors and artists from all over the world. Galleries from North America, Europe, Latin America, Asia and Africa participate in Miami\'s Art Basel show, showcasing paintings, sculptures, photographs, films and exceptional installations. Pictured here are Understopped by Esteban Jefferson and Gimnasio de Boxeo by Michael Vasquez. Alongside Art Basel, the design fair Design Miami brings together a global mix of renowned designers, collectors, curators and critics to showcase exhibitions of 20th and 21st-century furniture, lighting and fixtures. The cutting-edge Pulse Art Fair held in Miami every December hosts both emerging and established galleries and artists. They recently launched Pulse360, which is a year-round community initiative focusing on different conversations and topics within the arts.\nArt Deco Weekend\nThe Miami Design Preservation League (MDPL) created the Art Deco Weekend in 1976, and it is now the longest running free cultural festival in Miami Beach. The MDPL is a not-for-profit organization, and it preserves, protects and promotes the architectural community of Miami Beach. Each January, Art Deco Weekend includes live performances, film, crafts, art exhibitions, dog shows, food and more. With over 85 events, Art Deco Weekend is a great way to enjoy Miami\'s diverse culture and architecture.\nAt the age of six, Jumaane N’Namdi’s parents introduced him to the world of art appreciation and art collecting. According to N’Namdi, when it comes to his diverse art collection, he focuses on ""three concepts: education, culture and aesthetics."" If you’re in Miami and enjoy contemporary art, be sure to stop by N\'Namdi Contemporary for his rotating shows or check out some of the other contemporary art galleries throughout the city. The David Castillo Gallery showcases the work of emerging and mid-career artists who often reference historical, cultural and personal identities. Founded in 1977, the Fredric Snitzer Gallery has long championed the work of contemporary Latin American artists as well as emerging and mid-career artists.\nStacy Goodman-Conde first opened her gallery in 1998 in the Miami Design District. She eventually moved to Chicago, and then moved back to Miami to open up and establish Conde Contemporary. Conde says the gallery was born from two artists, herself and her husband, Andres. ""I love and respect our artists and recognize them as a special group of people, who in many ways speak a different language than the general population,"" Conde said. ""I\'m sort of their translator."" Conde Contemporary specializes in narrative realism, photorealistic portraiture, surrealism and international contemporary art. Pictured here is Andres Conde\'s ""First Blush.""\nNow in its 11th year, Avant Gallery presents the work of early and mid-career contemporary artists, as well as established artists, and their collections include art, photography and sculptures. They operate from three locations, which are located in Downtown Miami, the Aventura Mall and the Four Seasons Jumeirah Resort in Dubai. Pictured here is Mariano BNS\'s ""All Day Breakfast."" Avant Gallery also recently opened a restaurant in their downtown Miami location, LaMuse Café, whose menu is inspired by iconic muses in art history. Their official slogan is ""Eat More Art."" The de la Cruz gallery consists of an extensive contemporary collection, and organizes lectures, educational scholarships in New York for high school students and educational trips to Europe for college students.\nCalle Ocho Festival\nThe largest Hispanic festival in the country, Calle Ocho held every March has introduced South Florida to a taste of true Cuban culture since 1978. Wander through the 20 blocks of Little Havana where the festival is held to experience to find the streets filled with music, live entertainment, international food, dancing and more. Have you ever tried arepas or tamales? If not, Calle Ocho is the place to sample not only new foods, but new Latin artists. Some of the most recognized Latin artists have made their artistic debut at Calle Ocho, including Pitbull and Niki Jam. If you’re in the area during Calle Ocho and want to experience a truly culturally-enriching experience, make sure you head over to the festival. Held in early October every year, the Miami Broward Carnival fully embraces the Caribbean culture, including Caribbean music and dance, including dancers decked out in impressive, traditional costumes.\nMiami City Ballet\nIf you’re into all things classical, you’re in the right place. Miami is filled with orchestras, operas and one of the best ballet companies in the country. Miami City Ballet is one of the most diverse classical ballet companies in the nation, spotlighting dancers from Central and South America. When in Miami, symphony fans should check out a performance by the New World Symphony, which consists of 87 young musicians, and opera fans will love the Florida Grand Opera, one of the oldest performering arts organizations in Florida.\nSouth Beach Wine and Food Festival\nArtistry of a different sort, the South Beach Wine and Food Festival (SOBEWFF) held in Miami every February is a nationally renowned five-day event, featuring the talents of the world’s most celebrated wine and spirits creators, chefs and culinary personalities. All proceeds from the festival benefit the Florida International University’s school of hospitality. This will be the festival’s 18th year and will feature over 100 events. Stop by for the best food, drinks and educational culinary classes and seminars.']"	['<urn:uuid:40de05eb-8d08-4acf-93b6-2cf882476d90>', '<urn:uuid:ee09b6af-bef7-47d9-b219-aa4a65744a14>']	open-ended	direct	verbose-and-natural	distant-from-document	multi-aspect	novice	2025-05-12T21:31:42.699391	22	115	1906
12	What are the similarities between how an artist creates in Runway ML and how a player progresses in Dark Souls when it comes to learning from failures?	In both Runway ML and Dark Souls, learning from failures is essential to progress. In Runway ML, artists experiment with machine learning models, learning through trial and error to effectively use AI tools for their creative work. Similarly, in Dark Souls, players must learn from their deaths, as the game is designed around the concept that characters will 'irresistibly die a lot' while learning enemy strategies and skills to defeat them. Both systems require users to observe, adapt, and improve their approach through repeated attempts.	['Machine learning can be a fantastic tool for creators, but integrating AI into your workflow is a challenge for those who can’t code. A new program called Runway ML aims to make this process easier by providing artists, designers, filmmakers, and others with an “app store” of machine learning applications that can be activated with a few clicks.\nSay you’re an animator on a budget who wants to turn a video of a human actor into a 3D model. Instead of hiring expensive motion capture equipment, you could use Runway to apply a neural network called “PosetNet” to your footage, creating wireframe models of your actor that can then be exported for animation.\nAn easy way to experiment with machine learning\nOr say you need to remove a coffee cup that was accidentally left in a shot on your high-budget fantasy TV drama. You could edit it out the traditional way, painting over the cup by hand, or you could run your footage through a machine learning segmentation model, which would automatically highlight different objects in each frame to make your job easier.\nExamples like these are just the tip of the iceberg for Runway, which co-founder Cristóbal Valenzuela describes as radically egalitarian tool. “Machine learning is a very exclusive technology,” Valenzuela tells The Verge. “But I want to make things more inclusive; to get people from different backgrounds sitting around the table and using these models.”\nRunway began as Valenzuela’s thesis project at the Tisch School of the Arts at New York University. After getting enthusiastic feedback from the AI art community, he decided to take the program mainstream, asking two school friends to come on board as co-founders and gathering seed money from NYC and Silicon Valley backers. The company was incorporated last December with a beta launch following this January.\nValenzuela straddles the fields of art and code and says he wants to bridge these two worlds, empowering non-coders to use machine learning models and, in turn, connecting researchers to the people who will benefit directly from their work.\nIn a blog post Valenzuela wrote last May, he compares the current AI art scene to painting in the 16th and 17th centuries. At that time, the act of simply storing and using paint was something of a craft secret, with painters relying on esoteric techniques involving pigs’ bladders and string. But with the invention of the paint tube in 1841, the craft became more accessible. It was also easier to conduct outdoors, leading to new styles and movements.\nDoing for AI what paint tubes did for portraiture\nAs 19th century painter Pierre-Auguste Renoir told his son: “Without colors in tubes, there would have been no Cézanne, no Monet, no Sisley or Pissarro, nothing of what the journalists were to call Impressionism.” In other words: accessibility begets creativity.\nBut what is the “paint tube” for modern artists? Valenzuela makes a convincing argument that it might be Runway — or, at least, a program that looks a lot like it.\nThe pigs’ bladders holding back progress in this case is the skill-set currently needed to use machine learning models. That means learning to use software like TensorFlow or PyTorch; it might mean buying a few pricey GPUs (because your current computer won’t run these systems), or connecting to an AWS instance instead. None of these tasks are beyond the reach of non-coders, but they certainly take time and create a bottleneck for users. By comparison, Runway’s model is the perfect paint tube: just click and go.\nThe company is not the first to make AI models easier to use of course. But earlier examples — like Lobe, which let users train AI systems using a visual interface before it was bought by Microsoft — have focused on business use cases, rather than creative ones.\nRunway’s target market is obvious when you load up its store front, which lets you browse a range of models that run the gamut from text generation to motion tracking.\nYou click to see the details of each model, click to add it to your workspace, set up your inputs and outputs, then start the system running. There are hooks to connect these outputs to other apps (so you can send ML-processed images to Photoshop, for example), and users can import new models directly from GitHub with just a few lines of code.\nThis latter point is one of the most important for Runway. It’s hard to understate just how fast-paced and collaborative the current AI art scene is, and how much individuals benefit from one another’s work and professional research. No sooner does a new model get released than it’s pounced on by the masses who use it in all sorts of unexpected ways.\nTake the AI text generation system called GPT-2, unveiled in February by research lab OpenAI. In the months since it was launched, GPT-2 has been turned into accessible web apps, it’s been used to help write a novel, and someone even created a subreddit populated entirely by chatbots mimicking other subreddits using it.\nIn short: this is a bubbling and energetic scene, and Runway wants to stay as connected to it as possible. Valenzuela says his team is constantly responding to users’ feedback, adding new models to the program and updating the software’s interface every month.\nIt’s this connection to the community and speed of updating that he says will also stop Runway from getting overrun by the industry’s established players. “We’re just four people on our team,” he says. “We like to ship things fast, to get people excited and get them using things.” And although corporate giants like Adobe certainly have a lot of interest in similar AI applications, they’re necessarily going to be slower to integrate them into products.\nThe reception Runway has received so far gives credence to Valenzuela’s comments.\n“I personally find it useful for trying out some models quickly,” says Mario Klingemann, an AI artist who recently sold his work at Sotheby’s. He says the program will be particularly useful for workshops and classes since it allows teachers to get a new machine learning model up and running in minutes.\nKlingemann has reservations too. He tells The Verge that the software is “too limited” for making his own art because it only offers pretrained models. That means if someone wants to customize the output of a generator system, like a generative adversarial network (GAN), they can’t train it on their own data. Valenzuela says this functionality will be added soon to the program, along with the ability to use the output of one model as the input of another.\nRobbie Barrat, another AI artist who’s renowned for his work with GANs, also praised the program, singling out its potential to democratize the AI art scene as particularly positive.\n“If Runway can lower the barrier for entry ... I’m all for it.”\n“Right now, it’s easiest for rich tech bros who work at Google or something, went to a fancy computer science school, and have access to huge GPU clusters at work to make artwork with neural networks,” Barrat tells The Verge. “If Runway can lower the barrier for entry and let a more diverse group of people easily create art with AI, I’m all for it.”\nBut Barrat agrees that the program’s simplicity is a limiting factor. People who want to use AI to do more than just simplify an existing step in their process will need more powerful programs than Runway, he says. “No matter how good it is as a tool, people who code will still be able to make a bigger range of things and ultimately have more control over what they produce.”\nBarrat and Klingemann also say price is a potential problem when it comes to training ML models on Runway. The software is free to download and comes with $10 of cloud credit, which is more than enough to get to know its potential. But after that, it costs users 5 cents per minute of computation. For artists like Klingemann and Barrat who need to train their own models, it’s likely cheaper to buy their own GPUs or connect to a cloud service. “I am not sure it will financially make sense to me,” says Klingemann.\nBut saying that Runway isn’t a perfect fit for all users is hardly a huge criticism. No one is surprised when their Swiss army knife can’t be used to dig a trench. And for Valenzuela, the primary mission isn’t catering to pros; it’s expanding the borders of AI art world.\n“These technologies are going to radically change the way we create content because algorithms are understanding text, video, and sound in the same way we do,” he says. “And if we put these tools in the hands of people who have never accessed these before, they’re going to start thinking of new ways to use them.”', 'You are here\nDark Souls Review\nDark Souls is an action fantasy RPG developed by FromSoftware Studios and published by Namco Bandai Games in 2011.\n- Dark fantasy setting\n- High skill orientation\n- Large open world\n- Unique multiplayer\nDark Souls Gameplay\nDark Souls is the second episode of the series that started with a Demon’s Souls game developed by FromSoftware as well. The game received overall positive reviews for the depth and uniqueness of the combat system, although the difficulty of the fighting is quite complicated to master. Still, the gameplay complexity stays relevant to this day; it became the primary feature of the Souls series and migrated into the other projects later in the years. All of the similar games earned a name of “souls-likes”, as a praise for the genre origins.\nThe plot of the game tells a story about the kingdom of Lordran, where the player takes up a role of the cursed undead creature who begins an adventure to uncover the mysteries that gave birth to their kind. The protagonist collects the souls of the most powerful monsters which allows to fulfil the prophecy and open up the truth. The main character travels throughout the worlds using the central hub called Firelink Shrine. Each unique world has specific monsters and traps that require the player to learn, explore and think out how the adventure would continue and how to evade death.\nThe primary concept and feature of the Dark Souls RPG are based on death itself; the characters will irresistibly die a lot. The protagonist will get stabbed with swords, crushed with axes, hammers and boulders that fall from above; they will be burned, poisoned and electrocuted, as well as pushed off the cliff ledges and accidentally jumping off themselves. Dying will cause you to lose the souls of the previously defeated enemies; however, they can be obtained back by killing the foe that eliminated you, although this chance is given only once and in case of failing before reaching the particular enemy, the souls get permanently lost.\nAfter death, the main character gets resurrected at the bonfire - a checkpoint that is unique for the whole series of souls-likes, but all of the enemies that roamed along the way will be brought back to life as well, except bosses and friendly NPCs. Each enemy possesses unique strategy and skills which require the watch the unfriendly mobs to learn how to evade their attacks and defeat them. Souls that are gained for assassinations can be spent on bettering the physical statistics of the player, and at the vendor’s shops to buy weapons, armour or additional items.\nThe combat of the game includes a wide variety of melee-oriented attacks. In the world of this RPG, there is an extensive range of highly-detailed medieval weapons, such as swords, greatswords, axes, maces and clubs. Aside from conventional weaponry, the player can obtain specific magical wields that can put additional harmful effects on the enemies. The armour can be found, bought and upgraded throughout the game progress. As the project doesn’t require to choose a specific skill, players can decide themselves who they want to be, what kind of gear they want to wear and whether or not they want to use magic. Archetypes can be created to immerse in the RPG aspect of Dark Souls fully. However, some classes might find it more challenging to complete the game, as they do not have a needed level of protection and armour.\nAnother unique feature of Dark Souls is the multiplayer system, connected with a humanity scheme. The protagonist can travel the worlds in two forms: human and hollow. The human form allows to summon other players and NPC’s to help them in battle but brings additional dangers, such as aggressive players’ invasions who seek to restore their humanity. Hollow form grants the game progress but takes away the summoning and multiplayer ability. It can be restored by seeking out the last dying point or by consuming a particular item.\nThe communication between players is restricted, which allows them only to use limited messaging in certain places and a small number of common phrases and gestures. This feature creates an immersive and dark atmosphere in the game, granting a feeling of danger, insecurity and fear. The game has several different versions, expanded Prepare to Die Edition released in 2012 and Dark Souls Remastered with improved graphics and controls. Pay attention that different editions have different system requirements.\nDark Souls System Requirements\n- Memory: 6 GB RAM\n- Graphics Card: GeForce GTX 460, 1 GB\n- CPU: Intel Core i5-2300 2.8 GHz\n- File Size: 8 GB\n- OS: Windows 7 (64-bit)\n- Memory: 8 GB RAM\n- Graphics Card: GeForce GTX 660, 2 GB\n- CPU: Intel Core i5-4570 3.2 GHz\n- File Size: 8 GB\n- OS: Windows 10 (64-bit)']	['<urn:uuid:96c36f3d-ca7e-4252-841d-b433ce71d1c4>', '<urn:uuid:734c2ea0-dc14-4a96-b054-24ce09f23fa4>']	factoid	direct	verbose-and-natural	distant-from-document	comparison	novice	2025-05-12T21:31:42.699391	27	85	2295
13	I'm researching my Russian and Eastern European family history - what's the key difference between getting help from the Slavic Reference Service versus the East European Genealogical Society?	The Slavic Reference Service and East European Genealogical Society serve different purposes. The Slavic Reference Service offers free, personalized reference assistance with bibliographic queries and finding materials related to Russian and Eastern European studies during business hours (8 AM - 5 PM, Monday-Friday), including help with locating resources and verifying citations. However, they explicitly do not handle genealogical requests. In contrast, the East European Genealogical Society specifically focuses on providing genealogical information for people researching their German, Ukrainian, Polish, Czech, Slovakian, Jewish, Mennonite, and Romanian ancestry.	"['Do you need help with locating items or have difficult reference questions? If so, you may want to try this free year-round service at the University of Illinois. The Slavic Reference Service handles bibliographic and reference questions in all subject areas connected to Russian, East European, and Eurasian Studies. In addition to contacting the Slavic Reference Service directly, researchers can browse our collection of research guides which contain information on vernacular language print and electronic resources for Russian, East European, and Eurasian Studies, see our research guides. Moreover, the reference staff have a blog, which highlights recent online resources they have found useful in answering requests.\nHave a quick question or need assistance navigating online resources? SRS staff provides chat services for all Russian, East European, and Eurasian Studies related inquires during our regular service hours (8 AM – 5 PM), Monday through Friday. Simply join our open chat session via Blackboard Ultra and a SRS staff member will be available to assist you!\nSchedule a Research Consultation\nSRS staff is available to consult with faculty, students, and scholars in Russian, East European, and Eurasian Studies during our regular service hours (8 AM – 5 PM), Monday through Friday. Students and scholars located at the University of Illinois can schedule in-person sessions. Scholars at other institutions may schedule an appointment and SRS staff will meet you via Blackboard Ultra. To schedule a meeting with a member of our staff please use the online scheduler and indicate your location and affiliation.\nWhat We Do\nAnswer bibliographic and reference queries. If you are in need of a specific piece of information, we will be glad to try to answer your query. We will use print and online resources in our searches. We also assist patrons with their questions on the use of fonts and keyboards for languages in the service areas.\nIdentify materials in our collection. Identify libraries that hold materials you need. We will search to find libraries in the U.S., European Union, and libraries from the service areas that can lend or copy the title you need if it is not in our collection.\nVerify and correct citations. Oftentimes, titles are not fully or correctly cited in the existing bibliographic literature. Our staff are trained to search for the correct form of the citation, ensuring that you get the material you need.\nSuggest reference materials you might find relevant for your research. We cannot be held responsible for the completeness or accuracy of online information.\nWhat We Don’t Do\nCharge a fee. The Slavic Reference Service is a federally funded agency, there is no charge for our reference services. However, we are not a document delivery service. Should you need a copy of some material from our collection you will be directed to our InterLibrary Loan Department [See: http://www.library.illinois.edu/interlibrary-loan/]\nObtain certain kinds of materials, specifically archival materials and the full text of dissertations.\nRecommend particular books on a topic. We are all reference librarians with subject expertise in a variety of areas. None of us are experts in all areas. Our expertise lies in our familiarity with bibliographic materials. Therefore, we are not qualified to suggest a specific book in a subject area over some other work.\nSupply genealogical information. Our service is not equipped to handle genealogical requests.\nAdvise on copyright related issues.\nWhat We Need\nDetails. The more specific your explanation of what you are looking for, the better we are able to serve you.\nDeadlines. All requests are handled on a first come, first served basis, although requests that have a firm upcoming deadline can usually be worked in. If you must have an answer by a certain date, please indicate that in your request.\nLanguages. In order for us to locate information that will be useful we must know if there are materials that would not be helpful because of language constraints you might have.\nAffiliation. While we welcome queries from everyone, if your question will require the use of InterLibrary Loan, it is important for us to know if you are not affiliated with an academic institution. We may need to make arrangements with your public library to supply you with the material you need for your research.\nBackground. In order to make the search as efficient as possible it is important that we not duplicate the work you have already done.\nSources. All users are asked to supply the original source in which they found the citation to the material they are seeking. This saves us a great deal of time in locating and/or ordering material.\nSlavic Reference Service Staff\n- Joseph Lenkart, International Reference Librarian / Manager, Slavic Reference Service\n- Jan Adamczyk, Senior Library Specialist\n- Annabella Irvine, Visiting Research Specialist\n- Erika Weir, SRS Graduate Assistant\nPhone: (217) 333-1349\nIf you have a more involved question, please e-mail us at or use our FORM.\nYou can also reach us by mail at:\nSlavic Reference Service\nInternational & Area Studies Library\n319a Main Library, University of Illinois\n1408 West Gregory Drive\nUrbana, IL 61801\nFor some of our favorite resources, visit the Slavic Reference Service blog.', 'Genealogy: Ethnic Heritage Links\nA selection of links to websites that are useful for ethnic genealogical research.\n- Australia and New Zealand\n- British Isles\n- Eastern European and Russian\n- Hispanic: Central and South America, Mexico, Caribbean, and the West Indies\n- Native American\n- Western European\n15 Free Sites for African American Genealogy\nOutstanding African American genealogy websites compiled by Kimberly Powell.\nAfrican American Cemeteries Online\nListing of African American cemeteries arranged by state.\nAfrican American Genealogy\nResearch guidelines from the Enoch Pratt Free Library.\nAfrican American Genealogy Records\nResources for African American genealogists provided by FamilySearch.\nAfrican American History & Federal Records\nA special edition of NARAs magazine, Prologue that focuses on the use of federal records in African American historical research. Many of the articles are of genealogical interest.\nA site for researching genealogy from Africa to the Americas.\nAfro-American Historical and Genealogical Society\n""The Afro-American Historical and Genealogical Society, Inc. (AAHGS) strives to preserve African-ancestored family history, genealogy, and cultural diversity by teaching research techniques and disseminating information throughout the community. Our primary goals are to promote scholarly research, provide resources for historical and genealogical studies, create a network of persons with similar interests, and assist members in documenting their histories.""\nAfro-Louisiana History and Genealogy: 1719-1820\nThis web site provides a searchable database for African-Americans from the Louisiana area.\nAlabama African American Genealogy Research\nA comprehensive list of all Alabama genealogy resources. This web site provides a complete and easily navigable list of important records and databases.\nBlack Family Research: Using the Records of Post-Civil War Federal Agencies at the National Archives\nNARA Reference Information Paper 108, by Reginald Washington.\nThis website by Access Genealogy provides links to information valuable to the study of African American genealogy, including records by state, slave owners by state, online books, and more.\nChristines Genealogy Website\nThis site provides genealogy links, a genealogy forum and informative blog posts.\nCivil War Soldiers and Sailors System\nThis National Park Service web site includes information on the more than 230,000 African-American troops who served in the Civil War.\nLiving with the Hydra: Documentation of Slavery and the Slave Trade in Federal Records\nAn article by Walter Hill in the Winter 2000 issue of NARAs magazine, Prologue.\nMarriage Registers of Freedmen\nElaine Everly, NARA staff member, wrote this article on Freedman\'s marriage registers, an invaluable source of family history, for the Fall 1973 issue of Prologue.\nRegister of Free Blacks: Augusta County, Virginia\nA complete register of the sightings of black men freed in Augusta County, Virginia. Each registration entry includes proof of the man\'s freedom, where he was spotted, and a thorough physical description.\nThe Rost Home Colony, St. Charles Parish, Louisiana\nThis article by Michael Knight on the Freedmen\'s Bureau appeared in the Fall 2001 issue of NARA\'s magazine Prologue.\nPart of the WorldGenWeb, this site links to genealogical information sources in Asian countries from Turkmenistan in the west to East Timor in the east.\nChinese Immigration and Chinese in the United States\nNational Archives Reference Information Paper No. 99. A good starting point for Chinese-American researchers.\nChinese in California, 1850-1925\nThis collection from the Bancroft Library of the University of California Berkeley illustrates 19th and early 20th century Chinese immigration to California through about 8000 images and pages of primary source materials.\nChinese Roots Wiki\n""This wiki is designed to help people of Chinese heritage discover their ancestral roots.""\nThis site contains links found in the Bai Jia Xing (the Hundred Families Surnames). There is also a list in alphabetical order, in whatever form the names were Anglicized. The surname pages contain links and references to genealogies, family name associations, generation names, home pages, and other references to persons of particular surnames.\nGenealogy in Asia & the Pacific\n""Explore your ancestry with these genealogy and family history databases and resources covering Asia and the Pacific, including China, Japan, Manila, Cambodia, Thailand, the Philippines, Singapore, Bangladesh, India, Nepal, Pakistan, and Vietnam.""\nThis web site contains links to Japanese genealogical web sites in English, Spanish and Japanese. Research on Japanese family heritage can also be done according to Japanese prefecture.\nOnline Databases & Records for Research in British India\nResources compiled by Kimberly Powell.\nArchives New Zealand Research Resources\nThe National Archives of New Zealand provides information and research advice on important genealogical resources. Arranged by category, including Citizenship, Personal Identity, Migration, and Deeds Indexes.\nAustralian family history and genealogy selected websites\nThe National Library of Australia has compiled this listing of websites containing a variety of information relating to Australian family history and genealogy.\nThe National Archives of Australia has created this web site, which provides guidance on how to do family history research. It also provides guides to researching immigration, armed services, and indigenous records.\nA blog presented by the Auckland City Libraries. It describes resources available at the libraries and beyond.\nSociety of Australian Genealogists\n""The SAG has been helping people trace family history for over 80 years.""\nLinks to web sites with information on census records, cemeteries, military records, and other useful sources for English genealogy.\nElectric Scotland: Scottish and Irish Clans & Families\nA guide to Scottish clans and families.\nGENUKI: UK & Ireland Genealogical Information Service\nThis hubsite for accessing records in the British Isles is a quick and handy resource for genealogists.\nIrish Genealogical Society International\nNon-profit organization claims over 800 members in six countries. Offers resources and assistance to members researching their Irish heritage.\nNational Archives of Ireland: Genealogy\nA guide to using the records of the Irish National Archives, including census returns, parish records and marriage licences, and wills and testamentary records.\nNational Archives UK - Looking for a Person?\nGuides to family history research at the National Archives of the United Kingdom. Covers a wide range of topics.\nAllows searches of census and other data for Irish, British, and Scottish genealogical research.\nScots Family assists in searches within originial Scottish records for information on ancestors.\nScottish Emigration Database\nThis database contains the records of over 21,000 passengers who embarked at Glasgow and Greenock for non-European ports between 1 January and 30 April 1923, and at other Scottish ports between 1890 and 1960.\nSources for Research in Scottish Genealogy\nThe Library of Congress lists sources available there and at other large libraries for researching Scottish ancestry.\nLinks to resources that focus specifically on Welsh genealogy.\nWiltshire and Swindon Archives\nIncludes information on beginning family history research in Britain, and about the records held in the Wiltshire and Swindon Archives.\nEast European Genealogical Society\nThe East European Genealogical Society provides genealogical information for people of German, Ukrainian, Polish, Czech, Slovakian, Jewish, Mennonite, and Romanian descent.\nEurope Genealogy Links\nThis site provides lists of links based on country of origin. Also includes links to European passenger lists, cemeteries, and census data.\nFederation of East European Family History Societies\nGenealogists can find information on Eastern Europe from Armenia to the Ukraine.\nOdessa: A German-Russian Genealogical Library\nWell-researched and useful library of genealogical information.\nPolish Genealogical Society of America\nThe Societys mission is to collect, disseminate and preserve information on Polish and Polish-American family history and to help its members use that information in their own research.\nPolish Genealogical Society of Texas: Genealogy Resources\nResources include Surname Database, Maps and Geographic Locators, Organizations, PGSA Resource Center, Polish Resources, Texas Resources, and Discussion Lists.\nSGGEE: The Society for German Genealogy in Eastern Europe\n""This web site is devoted to the study of those people with German ancestry (most often of the Lutheran, Baptist, or Moravian Brethren faiths) who lived in present-day Poland and northwestern Ukraine.""\nAmerican-French Genealogical Society\nThis is the home site of the non-profit genealogical and historical organization devoted to people of French-Canadian extraction.\nCounting Heads Around the World: The Genealogy of International Census Databases (Canada)\nThis Searcher article by David Mattison, Access Services Archivist, British Columbia Archives, provides a wealth of online Canadian family history research resources. Included on the site are links to general census guides, databases, and population statistics.\n""Your gateway to Franco-American and French-Canadian Genealogy on the Internet.""\nFrench Canadian Heritage Society of California\nFosters interest in French-Canadian heritage through genealogical research. Affiliated with the Southern California Genealogical Society.\nLibrary and Archives Canada - Genealogy and Family History\nSuggestions for using the genealogical records of the Canadian National Archives.\nProgramme de Recherche en Démographie Historique\nInformation on Quebec\'s population in the 17th and 18th centuries. Hosted by the University of Montreal.\nQuebec Family History Society\nNon-profit organization is dedicated to Quebec genealogy. Site offers membership information and resources for research.\nAn online collection of genealogy data. This project is part of the Canada GenWeb.\nVermont French-Canadian Genealogical Society\nHomepage provides details about the society as well as assistance with research.\nCaribbean Genealogy Library\nResources provided by the Caribbean Genealogy Library located in St. Thomas.\nCuban Genealogy Center\nA genealogical web site dedicated to people of Cuban descent. This site provides valuable pointers for getting started, cemetery records, church records, military records, passenger lists, Cuban phone books, and indices to the history of Cuban families the country. A list of published resources is also provided.\nCyndis List: Central and South America\nOne of the most respected genealogy resources on the Internet. Cyndis List provides general Hispanic genealogy resources as well as information for individual countries.\nHispanic Genealogical Society\nThis organization offers information about families of Northern Mexico, South Texas, California, and New Mexico\nAmerican Jewish Historical Society Genealogy\nThe Society holds many records of genealogical interest, such as orphanage records, immigrant aid organizations, military records, and more.\nThis company publishes journals, books, videos, and other sources to assist in Jewish genealogical research.\nCenter for Jewish History Ackman and Ziff Family Genealogy Institute\nThe CJH Ackman and Ziff Family Genealogy Institute offers help in Jewish genealogical research, and occasionally presents workshops in conducting effective genealogical searches.\nInternational Association of Jewish Genealogical Societies\nThe International Association of Jewish Genealogical Societies (IAJGS) is a non-profit organization dedicated to the coordination of the activities of ""more than 70 national and local Jewish genealogical societies around the world.""\nInternational Jewish Cemetery Project\nThe project describes its mission as an attempt ""to catalogue every Jewish burial site in the world.""\nJewish Genealogical Society, Inc.\nNon-profit society based in New York City provides information on meetings, conferences, and resources.\nJewish Genealogy & Surnames\nThis site provides guidance for conducting Jewish genealogy.\nThe primary internet source connecting researchers of Jewish genealogy worldwide.\nLeo Baeck Institute\nThe Leo Baeck Institute is a research, study, and lecture center whose library and archives offer the most comprehensive documentation for the study of German Jewish history.\nLouis Kesslers Jewish Genealogy Links\nGathers links to the most useful Jewish genealogy web sites.\nFocuses specifically on Jews of Sephardic ancestry, with articles, links, and book descriptions available.\nStammbaum: Journal of German-Jewish Genealogical Research\nAn English-language publication that supports research and publication of reliable German-Jewish family histories.\nFrance Genealogy Links\nResearchers seeking information on French ancestry can search by region or throughout France on this web site. Includes current French phone books and an atlas of French heraldry on line.\nThis web site provide links to German genealogy in German and English. Provides: sample letters to German resources, regional resources, German societies, emigration information, place lookups, and local heritage bibliographies. This web site also offers German unit conversion charts and definitions of German terminology and symbols.\nGerman Roots: German Genealogy\nProvides passenger list from pre-1820 to the 1940s, census clues from 1790 to 1930, online databases, and more.\nThe Italian Genealogical Society of America\nProvides information on Italian heritage, how to find ancestors and even living relatives on both sides of the Atlantic, how to get started with the basic methods and resources of genealogy, and advanced research techniques from professionals in the field.\nThis page contains a wealth of information for those Italian-Americans who would like to trace their familys roots.\nThis web site provides Italian genealogy resources searchable by surname, province, or region of origin. Guidance for successful genealogy web searches are also provided.\nSpain & the Basque Country\nLinks from Cyndi\'s List to help genealogists trace their Spanish ancestry.\nSwiss Anabaptist Genealogical Association\nThis organization provides databases to members interested in Swiss Anabaptist heritage.\nUstica Genealogy Homepage\nThis web site represents an ongoing effort to translate, record, and link genealogic information for the families of Ustica, Italy, and their descendants in America. Provides indices of family surnames in Ustica and America, a list of the first inhabitants of Ustica, and references to Usticesi in the South.\nSee also additional genealogical material:\nFor information on how to do genealogical research at the National Archives, please see the Genealogy page on the NARA web site.']"	['<urn:uuid:73262ecf-34a9-4153-b53b-ba56abf87ad6>', '<urn:uuid:6f2fde4a-8a28-4d09-89c7-bfe3b90d4f28>']	open-ended	with-premise	verbose-and-natural	similar-to-document	comparison	novice	2025-05-12T21:31:42.699391	28	86	2972
14	Hi, I'm studying environmental conservation and economics, and I'd like to know what similarities exist between how natural river systems maintain balance and how businesses organize themselves in unstable times?	Both natural river systems and business organizations demonstrate similar patterns of self-organization and balance restoration. In river ecosystems, wetlands act as natural stabilizers by regulating water flow, reducing floods and droughts, and filtering pollutants. Similarly, in economic systems, particularly during the post-Soviet transition period, business associations emerged spontaneously as stabilizing forces to coordinate activities and reduce chaos. Just as wetlands provide essential ecosystem services to maintain river health, these business associations provided crucial services to their members, including stability, coordination, and information sharing. Both cases represent examples of complex systems naturally creating order from disorder - wetlands emerging as nature's solution to river system instability, and business associations forming as an economic solution to market disorganization.	"['Disorganization or self-organization : the emergence of business associations in a transition economy\nThe transition from plan to market provides a rare opportunity for insight into the endogenous development of economic institutions. Economic activities under the Soviet regime were coordinated by a central authority. Soviet coordinating mechanisms were disrupted during the transition period, leading to an increase in firms\'transactions costs. Blanchard and Kremer (1997), among others, emphasize the negative impact of this""disorganization""on output behavior at the beginning of the transition. Although their argument is correct, the authors believe that their work and similar analyses stop short of fully characterizing the transition by concentrating only on reform\'s disruptive effects. The authors start where the earlier work ends, examining the business associations that emerged spontaneously in response to the transition\'s challenges. They provide empirical evidence that institutions that help coordinate production and trade emerge spontaneously in a widely""disorganized""environment. Using a largely unexplored set of firm-level data, they document the emergence of business associations at the beginning of the transition and provide evidence that these new coordinating institutions mitigated the initial decline in output. Building on the growing literature on complexity and transaction costs, they interpret the emergence of these informal institutions as the firms\'rational attempt to coordinate activities in a decentralized economy. In other words, the creation of complex organizations such as associations is the spontaneous result of a natural tendency in every system to create order at the edge of chaos. Business associations are more likely to emerge where there is disorder to provide their members with stability, coordination, and the information needed to improve performance.\n|Date of creation:||31 Jan 2001|\n|Date of revision:|\n|Contact details of provider:|| Postal: |\nPhone: (202) 477-1234\nWeb page: http://www.worldbank.org/\nMore information through EDIRC\nPlease report citation or reference errors to , or , if you are the registered author of the cited work, log in to your RePEc Author Service profile, click on ""citations"" and make appropriate adjustments.:\n- Greif, Avner & Milgrom, Paul & Weingast, Barry R, 1994. ""Coordination, Commitment, and Enforcement: The Case of the Merchant Guild,"" Journal of Political Economy, University of Chicago Press, vol. 102(4), pages 745-76, August.\n- Diamond, Peter A, 1982.\n""Aggregate Demand Management in Search Equilibrium,""\nJournal of Political Economy,\nUniversity of Chicago Press, vol. 90(5), pages 881-94, October.\n- Alberto Alesina & Eliana La Ferrara, 1999.\n""Participation in Heterogeneous Communities,""\nNBER Working Papers\n7155, National Bureau of Economic Research, Inc.\n- Commander, Simon & Coricelli, Fabrizio, 1992. ""Output decline in Hungary and Poland in 1990-91 : structural change and aggregate shocks,"" Policy Research Working Paper Series 1036, The World Bank.\n- Ickes, B.W., 1995. ""The Organization of Markets and Its Role in Macroeconomic Stabilization During Transition,"" Papers 11-95-8, Pennsylvania State - Department of Economics.\n- Olivier Blanchard & Michael Kremer, 1997.\nWilliam Davidson Institute Working Papers Series\n38, William Davidson Institute at the University of Michigan.\n- Haltiwanger, John C & Waldman, Michael, 1989.\n""Rational Expectations in the Aggregate,""\nWestern Economic Association International, vol. 27(4), pages 619-36, October.\n- John McMillan & Christopher Woodruff, 1999.\n""Interfirm Relationships And Informal Credit In Vietnam,""\nThe Quarterly Journal of Economics,\nMIT Press, vol. 114(4), pages 1285-1320, November.\n- John McMillan & Christopher Woodruff, 1998. ""Interfirm Relationships and Informal Credit in Vietnam,"" William Davidson Institute Working Papers Series 132, William Davidson Institute at the University of Michigan.\n- McMillan, John & Woodruff, Christopher, 1998. ""Inter-Firm Relationships and Informal Credit in Vietnam,"" CEPR Discussion Papers 2036, C.E.P.R. Discussion Papers.\n- Johnson, Simon & McMillan, John & Woodruff, Christopher, 1999.\n""Contract Enforcement in Transition,""\nCEPR Discussion Papers\n2081, C.E.P.R. Discussion Papers.\n- Anders Åslund & Peter Boone & Simon Johnson, 1996. ""How to Stabilize: Lessons from Post -communist Countries,"" Brookings Papers on Economic Activity, Economic Studies Program, The Brookings Institution, vol. 27(1), pages 217-314.\n- Clay, Karen, 1997. ""Trade, Institutions, and Credit,"" Explorations in Economic History, Elsevier, vol. 34(4), pages 495-521, October.\n- Evans, William N & Schwab, Robert M, 1995. ""Finishing High School and Starting College: Do Catholic Schools Make a Difference?,"" The Quarterly Journal of Economics, MIT Press, vol. 110(4), pages 941-74, November.\n- Ickes, B. & Ryterman, R. & Tenev, S., 1995. ""On Your Marx, Get Set, Go: The Role of Competition in Enterprise Adjustment,"" Papers 11-95-9, Pennsylvania State - Department of Economics.\n- Greif, Avner, 1994. ""Cultural Beliefs and the Organization of Society: A Historical and Theoretical Reflection on Collectivist and Individualist Societies,"" Journal of Political Economy, University of Chicago Press, vol. 102(5), pages 912-50, October.\n- de Melo, Martha & Denizer, Cevdet & Gelb, Alan, 1996. ""From plan to market : patterns of transition,"" Policy Research Working Paper Series 1564, The World Bank.\n- repec:dgr:uvatin:19980053 is not listed on IDEAS\n- Paul R. Milgrom & Douglass C. North & Barry R. Weingast, 1990. ""The Role Of Institutions In The Revival Of Trade: The Law Merchant, Private Judges, And The Champagne Fairs,"" Economics and Politics, Wiley Blackwell, vol. 2(1), pages 1-23, 03.\n- Enrico C. Perotti & Stanislav Gelfer, 1998. ""Investment Financing in Russian Financial-Industrial Groups,"" William Davidson Institute Working Papers Series 242, William Davidson Institute at the University of Michigan.\nWhen requesting a correction, please mention this item\'s handle: RePEc:wbk:wbrwps:2539. See general information about how to correct material in RePEc.\nFor technical questions regarding this item, or to correct its authors, title, abstract, bibliographic or download information, contact: (Roula I. Yazigi)\nIf references are entirely missing, you can add them using this form.', ""Removing Weirs and Restoring Wetlands Can Aid Ailing Rivers\nMar 23 2023\nRivers play a critical role in sustaining life on Earth. They support an array of ecosystems, from the fish and aquatic plants that inhabit them to the animals and humans that depend on them for drinking water, irrigation, and recreation. However, human activity has had a profound impact on rivers, from the construction of dams and weirs to the pollution of waterways. Some experts suggest that two effective means of river conservation are the removal of weirs and the restoration of wetlands.\nWeirs are structures built across rivers to manage water flow, often to provide hydroelectric power or for irrigation purposes. While weirs have benefits, they can also harm rivers by disrupting natural flows and impeding fish migration. Weirs also create artificial pools and alter the physical structure of rivers, which can have negative effects on water quality, habitat, and biodiversity.\nRemoving weirs can have a significant impact on river health. By restoring natural flows, rivers can regain their ecological functions, which include supporting fish populations and maintaining healthy habitats. For example, the River Chew in Somerset, UK, saw an improvement in its health after weirs were removed. The natural flow was restored, and connectivity for fish populations improved. This resulted in an increase in the number of fish species and individual fish, which in turn had a positive impact on the entire river ecosystem.\nFurthermore, removing weirs can also benefit recreational activities such as canoeing, kayaking, and fishing. Restoring the natural flow and habitat of the river provides a more diverse and exciting experience for water enthusiasts. This can have an economic benefit for local communities, as more people are likely to visit and spend money in the area.\nAnother approach to improve river health is the restoration of wetlands. Wetlands are highly productive ecosystems that provide many essential ecosystem services, including water purification, flood control, recreation, carbon sequestration, and biodiversity restoration. They play a critical role in supporting wildlife, particularly birds, fish, and insects.\nWetlands act as natural water filters by trapping pollutants and excess nutrients, preventing them from entering rivers and causing harmful algal blooms. They also help to regulate water flow, reducing the risk of floods and droughts. During heavy rainfalls, wetlands act as a sponge, absorbing excess water and releasing it slowly, reducing the impact of floods downstream.\nWetlands store up to 30% of the world's terrestrial carbon, making them essential in mitigating climate change. They are also essential habitats for biodiversity, supporting many plant and animal species. The restoration of wetlands can help to recreate or enhance habitats for species that have been lost due to human activities such as land-use change, pollution, and fragmentation.\nIn addition, wetlands can provide excellent recreational opportunities for the people who live nearby. Wetlands are great places for bird-watching, fishing, and other outdoor activities, which can have a positive impact on mental and physical health. This can result in economic benefits for local communities as well, as more people are likely to visit and spend money in the area.\nThe combination of removing weirs and restoring wetlands can have an even greater impact on river health. By restoring natural flows and habitats, rivers can regain their ecological functions and support a diverse range of plant and animal species. This, in turn, can have a positive impact on human communities that depend on rivers for drinking water, irrigation, and recreation.\nFurthermore, the restoration of wetlands can also help to reduce greenhouse gas emissions, mitigate climate change, and provide economic benefits to local communities. The restoration of wetlands can also help to reduce the risk of flooding and droughts, making them an essential tool for water management.\nIn conclusion, removing weirs and restoring wetlands can have a significant impact on river health. By restoring natural flows and habitats, rivers can regain their ecological functions and support a diverse range of plant and animal species, as well as providing many essential ecosystem services. Wetlands are particularly important in this regard, as they help to purify water, regulate water flow, mitigate climate change, and support biodiversity. The restoration of wetlands can also provide excellent recreational opportunities for local communities, which can have positive impacts on mental and physical health.\nTherefore, it is essential to prioritize the removal of weirs and restoration of wetlands as part of river management strategies. This requires a collaborative effort between government agencies, local communities, and conservation organizations. Through careful planning and implementation, we can ensure that our rivers and wetlands are healthy, resilient, and sustainable for generations to come.\nIn This Edition STA Annual Guide - Read it Here Water/Wastewater - Continuous remote water quality monitoring networks Environmental Laboratory - The Important Role of ICP-MS in Unde...\nView all digital editions\nDec 03 2023 Budva, Montenegro\nDec 04 2023 Kuala Lumpur, Malaysia\nDec 12 2023 Nuremberg, Germany\nJan 17 2024 New Delhi, India\nJan 22 2024 Port of Spain, Trinidad""]"	['<urn:uuid:dd6d758a-fbb1-4961-97b2-6f05a547bb5b>', '<urn:uuid:8ec34fb6-d88a-4978-8fbc-ba8692526dfe>']	open-ended	with-premise	verbose-and-natural	distant-from-document	multi-aspect	novice	2025-05-12T21:31:42.699391	30	116	1731
15	What tools are available for collecting family health history?	The U.S. Surgeon General's Family Health Portrait Tool helps collect family health information, and the Genetic Alliance offers a 'Does it Run in the Family?' toolkit that allows users to create tailored family health history tools to promote conversations about health within families.	"['ASHG, along with partner organizations, develops educational materials for members and other human genetics specialists. These include webinars, online courses, infographics, and other resources. By sharing the latest information and advances, the Society hopes to provide the genetics community with opportunities to connect with and learn from one another, as well as build upon findings to spur continued progress in the field.\nTake Initiative: DNA Day Engagement and You (February 21, 2019)\nFrom Gene to Therapy (December 4, 2018)\nMatchmaker Exchange: Improving Diagnosis to Identify Novel Genes (July 24, 2018)\nTechnical and Validity Measures for NGS Data - Community Tools and Standards (February 24, 2016)\nStandardization for NGS Testing - Issues of Clinical Validity and Utility (September 1, 2015)\nPrenatal Cell-free DNA Screening Program (August 7, 2017):\nSeries: DNA Day \'15 for 15\' (April 25, 2018)\nSeries: Prenatal cfDNA Screening (August 7, 2017)\nSeries: Pediatric Genetic Testing (March 24, 2015)\nASHG has partnered with ReachMD to develop the interview series Genetically Speaking. These 15-minute interviews feature leading experts in clinical genetics care and research.\nASHG thanks its corporate sponsors for supporting its educational programming for health professionals.\nGuide to Interpreting Genomic Reports: A Genomics Toolkit (CSER Consortium; February 2017)\nA guide to genomic test results for non-genetics providers.\nClick here for a downloadable pdf.\nThe American College of Medical Genetics and Genomics (ACMG)\nACMG is responsible for providing education, resources and a voice for the medical genetics profession. ACMG also strives to make genetic services available to the public to improve health.\nNational Society of Genetic Counselors (NSGC)\nThe NSGC has patient-focused information about genetic counseling, genetic testing, and genetic conditions.\nAssociation of Professors of Human and Medical Genetics (APHMG)\nAPHMG is a group of organizations that promotes health and medical genetics educational programs in North American medical and graduate schools. More than ninety institutions are now official members of the Association.\nCenters for Disease Control and Prevention (CDC)- Public Health Genomics\nThe CDC website provides a variety of materials and resources on topics in genomics for health professionals and the general public, including information on genomics and disease, family history tools, and a public health genomics knowledge base.\nThe American Medical Association (AMA)- Genetics and Personalized Medicine\nThe AMA website provides a wealth of information and resources related to genetics and personalized medicine. These include CME programs, information about the precision medicine initiative, and tools for gathering family history information.\nThe American Academy of Family Physicians (AAFP)- AFP by topic, Genetics\nThe American Family Physicians Genetics page has a collection of the best content from AFP, as identified by AFP editors on genetics and related issues.\nGenetic Alliance: ""Understanding Genetics: A Guide for Patients and Professionals""\nThe Genetic Alliance (a non-profit organization that supports genetics policy and advocacy efforts) created an online guide to understanding genetics that covers basic scientific concepts and provides information about diagnoses of genetic conditions, family-history gathering, newborn screening, genetic counseling and genetic testing types and applications.\nThe Jackson Laboratory Clinical and Continuing Education in Genomics\nA collection of educational programs addressing specific knowledge gaps or barriers to integrating genomics into clinical practice, including courses, education on specific clinical topics and educational materials from the National Coalition of Health Professional Education in Genetics (NCHPEG).\nGenetics/Genomics Competency Center (G2C2)\nThis website contains genomics educational materials for clinical practice or the classroom. The resources are organized by discipline and are curated. The mission of G2C2 is to provide high quality educational resources for group instruction or self-directed learning in genetics/genomics by health care educators and practitioners.\nGlobal Genetics and Genomics Community (G3C)\nA collection of interactive cases that demonstrate how genetics and genomics link to health and illness. CME and CNE credits are available.\nGenetic and Rare Diseases Information Center (GARD)\nEstablished by the National Human Genome Research Institute (NHGRI) and the Office of Rare Diseases (ORD), GARD employs experienced information specialists to answer questions from the general public – including patients and their families, health care professionals, and biomedical researchers.\nGenetics Home Reference at the National Library of Medicine\nAn online guide to understanding genetic conditions that provides consumer-friendly information about the effects of genetic variations on human health.\nAn international point-of-care resource for clinicians providing clinically relevant and medically actionable information for inherited conditions covering diagnosis, management, and genetic counseling information.\nMarch of Dimes: Medical resources\nThe resources on this website provide information about preconception health, prematurity, newborn screening, and other topics related to newborn health, as well as information about prenatal testing and specific birth defects.\nPharmacogenetics: Clinical Pharmacogenetics Implementation Consortium Guidelines\nPractice Guidelines and Policy Statements from the American College of Medical Genetics and Genomics (ACMG)\nNational Comprehensive Cancer Network (NCCN) - Hereditary cancer syndromes\nGenetics in Healthcare Practice - Policy Statements\nAmerican Society of Human Genetics (ASHG)\nGenome Statute and Legislation Database at the National Human Genome Research Institute\nUpdated monthly and searchable by year, topic and/or state\nNational Society of Genetic Counselors (NSGC)\nLink to locating a genetic counselor\nAmerican College of Medical Genetics and Genomics (ACMG)\nA search engine for finding medical genetics clinical services\nPersonalized Medicine Coalition (PMC)\nThis independent, non-profit group works to advance the understanding and adoption of personalized medicine for the ultimate benefit of patients.\nFDA: Table of Pharmacogenomic Biomarkers in Drug Labeling\nA list of FDA-approved drugs with pharmacogenomic information in their labeling\nThe Pharmacogenomics Knowledgebase (PharmGKB)\nA comprehensive resource the curates knowledge about the impact of genetic variation on drug response for clinicians and researchers.\nOnline Mendelian Inheritance in Man (OMIM)\nA comprehensive compendium of human genes and genetic phenotypes.\nGTR: Genetic test registry\nHoused at NCBI, a central location for voluntary submission of genetic test information by providers. Includes test\'s purpose, methodology, validity, evidence of test\'s usefulness and laboratory contacts and credentials.\nU.S. Surgeon General\'s Family History Initiative\nThis site features disease-specific health risk fact sheets written specifically for primary care physicians, which encourage physicians to discuss family history information with their patients, as well as the Surgeon General\'s Family Health Portrait Tool to collect information from patients.\nGenetic Alliance: ""Does it Run in the Family?"" Toolkit\nThis toolkit allows users to create tailored family health history tools, with the goal of promoting conversations about health within the family and translating knowledge of family health history into healthy choices.\nGenetics and Health Impact Blog (CDC)\nComments from a public health perspective on genomic research and programs conducted by the CDC and other institutions around the world\nDNA Science Blog: Genetics in Context (PLOS)\nGeneticist blogs from the cutting edge in genomics, including genetic testing, stem cells, gene therapy and genetic disorders\nNational Society of Genetic Counselors Blog\nThis blog provides easy-to-understand information about current topics in genetic testing\nGenetics in Medicine GENEPOD\nPodcast from the official journal of the American College of Medical Genetics and Genomics\nPodcasts from the Naked Scientists (University of Cambridge)\nDNA Today Podcast\nA genetics radio show and podcast to educate the public on genetic and public health topics']"	['<urn:uuid:2d0eb103-4e1c-4ec7-85e3-23b1a9be1380>']	open-ended	with-premise	concise-and-natural	similar-to-document	single-doc	expert	2025-05-12T21:31:42.699391	9	43	1160
16	research program digital salmon purpose	The Digital Salmon project aims to create mathematical descriptions of salmon physiology, combining mathematics, data analysis, computer science, and measurement technology with genomics and experimental biology. This framework will help adapt salmon breeding and nutrition strategies to modern feedstuffs, enabling a shift from reactive to pre-emptive research and development strategies in aquaculture.	"[""Systems biology will aid sustainability in salmon farming. Scarcity of fish oil has forced development of novel feedstuffs, challenging the salmon's metabolism as well as our understanding of it.\nEnglish subtitles available. Hover over the video and click the CC or gear icons.\nSalmon farming in the future must navigate conflicting and shifting demands of sustainability, shifting feed prices, disease, and product quality. The industry needs to develop a flexible, integrated basis of knowledge for rapid response to new challenges. Project DigiSal will lay the foundations for a Digital Salmon: an ensemble of mathematical descriptions of salmon physiology, combining mathematics, high-dimensional data analysis, computer science and measurement technology with genomics and experimental biology into a concerted whole.\nDigiSal will focus on challenges of novel feedstuffs, collaborating with the Foods of Norway centre for research-based innovation at NBMU. Salmon are carnivores but today aquaculture provides more than half their fat and protein from plants, challenging the metabolic system and affecting fish health and nutritional value of salmon meat. The newly sequenced salmon genome and related resources will enable a tightly integrated theoretical-experimental study of mechanistic interactions among genetic and feed factors.\nSystems-oriented mathematical and statistical modelling will be central, using existing and novel knowledge e.g. on metabolic reaction networks to guide design of experiments through multiple iterations. Metabolic function of fish will be characterized via multiple omics technologies in feeding trials and in vitro tissue-slice culture. Gut microbiota will receive particular attention. The resulting massive data will be summarized via multivariate models to deliver a predictive understanding of a whole range of possible diets, much more efficiently than by traditional feeding trials alone. Data and models will be annotated using bio-relevant ontologies, so that new knowledge automatically connects to that which already exists. Future challenges will be met by quickly reanalysing existing information and understanding of salmon biology, identifying knowledge gaps, acquiring new data and incorporating it into a unified whole. Thus, we begin a shift from a reactive to a pre-emptive R&D strategy in aquaculture.\nProject objective: Establish a systems biology framework for adapting salmon breeding and nutrition strategies to modern feedstuffs, blazing the trail for a Digital Salmon endeavour.\nDigiSal is part of the Digital Life project by the BIOTEK2021 biotechnology programme of the Research Council of Norway.\nScientific coordinator. Leader of WP3: Omics and imaging. RNA sequencing expert.\nData and model manager.\nMathematical modeller. Biochemist.\nBiochemist, metabolic modelling.\nMSc student, biotechnology/metabolic modelling.\nModeller, systems geneticist.\nLeader of WP1: Feeding trials. Heads the nutrition part of Foods of Norway, with which DigiSal interfaces closely.\nLeader of WP4: Genomics. Salmon genome guru.\nMolecular biologist, bioinformatician.\nHead of the ELIXIR node at NMBU, focusing on the salmon genome.\nHead of Foods of Norway.\nAquaGen (salmon breeding company)\nDr. Jacob Torgersen, leader of WP2: In vitro studies. Molecular biologist.\nDr. Nina Santi, research director of AquaGen.\nNorwegian University of Science and Technology\nProf. Per Bruheim, metabolomicist.\nDr. Zdenka Bartosova, metabolomicist.\nDr. Marit Hallvardsdotter Stafsnes, metabolomicist.\nDr. Per Winge, gene editor.\nProf. Trygve Brautaset, systems biologist.\nProf. Stig Omholt, visionary.\nUniversity of Bergen\nDr. Eivind Valen, bioinformatician.\nProf. Inge Jonassen, bioinformatician.\nInstitute for marine research\nDr. Anna Wargelius, functional genomicist.\nDr. Rolf Edvardsen, molecular biologist.\nUniversity of Tromsø\nProf. Nils Peder Willassen, head of the ELIXIR node at UiT, focusing on marine metagenomics.\nUniversity of Stirling\nProf. Michael Leaver, fish-nutritional geneticist.\nWageningen University and Research Centre\nSystems and synthetic biology group:\nProf. Vítor Martins dos Santos.\nProf. Peter Schaap.\nDr. María Suárez-Diez.\nJesse van Dam.\nDr. Jasper Koehorst.\nEWOS (feed producer)\nDr. Dominic Nanton, senior scientist.\nFAIRDOM (consortium for findable, accessible, interoperable and reusable data, operations and models)\nDr. Natalie Stanford, director of community communications.\nDr. Stuart Owen, director of technical development.\nProject leader and leader of WP5 (Modelling) and WP6 (Biocuration). Systems biologist.\nClick above to findspecific contact info.""]"	['<urn:uuid:925753d8-bd28-4081-ad65-68961db975e2>']	open-ended	with-premise	short-search-query	distant-from-document	single-doc	novice	2025-05-12T21:31:42.699391	5	52	637
17	What are the benefits of different cloud computing models for businesses, and what potential risks do these service types bring to data protection?	Cloud computing offers three main models with distinct benefits: Infrastructure as a Service (IaaS) provides the highest flexibility and control over IT resources; Platform as a Service (PaaS) removes infrastructure management burden, allowing focus on application deployment; and Software as a Service (SaaS) delivers complete managed products requiring minimal user oversight. However, these models also present security challenges. The virtualization technology enabling these services can expose data to breaches since physical servers are shared between organizations. Additionally, the loss of control over server configurations and upgrades can compromise security. Organizations must protect themselves through measures such as data encryption, secure transfer protocols, maintaining local backups, implementing DDoS protections, and establishing strict access controls for employees.	"[""Types of Cloud Computing\nCloud computing is providing developers and IT departments with the ability to focus on what matters most and avoid undifferentiated work like procurement, maintenance, and capacity planning. As cloud computing has grown in popularity, several different models and deployment strategies have emerged to help meet specific needs of different users. Each type of cloud service, and deployment method, provides you with different levels of control, flexibility, and management. Understanding the differences between Infrastructure as a Service, Platform as a Service, and Software as a Service, as well as what deployment strategies you can use, can help you decide what set of services is right for your needs.\nCloud Computing Models\nThere are three main models for cloud computing. Each model represents a different part of the cloud computing stack.\nInfrastructure as a Service (IaaS)\nInfrastructure as a Service, sometimes abbreviated as IaaS, contains the basic building blocks for cloud IT and typically provide access to networking features, computers (virtual or on dedicated hardware), and data storage space. Infrastructure as a Service provides you with the highest level of flexibility and management control over your IT resources and is most similar to existing IT resources that many IT departments and developers are familiar with today.\nPlatform as a Service (PaaS)\nPlatforms as a service remove the need for organizations to manage the underlying infrastructure (usually hardware and operating systems) and allow you to focus on the deployment and management of your applications. This helps you be more efficient as you don’t need to worry about resource procurement, capacity planning, software maintenance, patching, or any of the other undifferentiated heavy lifting involved in running your application.\nSoftware as a Service (SaaS)\nSoftware as a Service provides you with a completed product that is run and managed by the service provider. In most cases, people referring to Software as a Service are referring to end-user applications. With a SaaS offering you do not have to think about how the service is maintained or how the underlying infrastructure is managed; you only need to think about how you will use that particular piece software. A common example of a SaaS application is web-based email where you can send and receive email without having to manage feature additions to the email product or maintaining the servers and operating systems that the email program is running on.\nCloud Computing Deployment Models\nA cloud-based application is fully deployed in the cloud and all parts of the application run in the cloud. Applications in the cloud have either been created in the cloud or have been migrated from an existing infrastructure to take advantage of the benefits of cloud computing. Cloud-based applications can be built on low-level infrastructure pieces or can use higher level services that provide abstraction from the management, architecting, and scaling requirements of core infrastructure.\nA hybrid deployment is a way to connect infrastructure and applications between cloud-based resources and existing resources that are not located in the cloud. The most common method of hybrid deployment is between the cloud and existing on-premises infrastructure to extend, and grow, an organization's infrastructure into the cloud while connecting cloud resources to internal system. For more information on how AWS can help you with your hybrid deployment, please visit our hybrid page.\nDeploying resources on-premises, using virtualization and resource management tools, is sometimes called “private cloud”. On-premises deployment does not provide many of the benefits of cloud computing but is sometimes sought for its ability to provide dedicated resources. In most cases this deployment model is the same as legacy IT infrastructure while using application management and virtualization technologies to try and increase resource utilization.\nReady to take the next step? AWS can help you learn more about AWS, find the right solution for your needs, or connect you with an AWS salesperson who can help you find the right solution for you."", 'There are two sides to every coin, and nowhere is that more evident than on your cloud server. The exact same attributes that make cloud storage servers so cost-effective and convenient also make them much more difficult to secure.\nIn the cloud, maintenance and upgrade management falls entirely to your cloud hosting provider. On one hand, this frees up internal resources, meaning less time (and money) is spent on routine maintenance. On the other hand, you don’t have control over server configurations and upgrades, which could spell trouble where your business security is concerned.\nSimilarly, the same virtualization technology that allows you to rapidly deploy new cloud servers also puts you at risk for data breaches and other cybercriminal events. In cloud computing, physical servers are split into virtual machines, which means other organizations could theoretically access the data stored on them—especially if your provider is lax with security controls.\nThese issues highlight just how diligently business IT support teams must work to secure cloud storage servers and databases. Beating hackers at their own game requires robust, thorough security strategies like those described below.\nEnd-to-end data encryption\nImplementing encryption solutions is one of the most crucial moves you can make to protect your data from leaks and ransomware attacks. Most cloud storage services automatically encrypt data while in transit; however, once that data is saved to your cloud server, there’s no guarantee that it’s secure. And even if a third-party cloud storage service does encrypt the data stored on its servers, the company ultimately holds the key to that information, not you. In other words, if that third party is compromised, your data could go down with it.\nThese facts make a good case for implementing some kind of encryption method before you store data in the cloud, but regardless of whether you use a cloud storage service or have a separate cloud environment, protecting your data with encryption software offers security against brute-force attacks, data leaks and ransomware. Your business IT support provider should be able to help you design an encryption technique for your cloud storage solutions—for instance, at MyITpros, we offer whole-disk encryption for our cloud services.\nSecure data transfers\nKeep in mind that data is not only at risk when it’s sitting on cloud storage servers, it’s also vulnerable when in transit (i.e. while being uploaded, downloaded or moved on your server). Although most cloud service providers encrypt data transfers as a rule, this is not always a given.\nTo ensure data is protected while on the move, make certain that transfers go through secure HTTP access and are encrypted using SSL. Your business IT support provider should be able to help you obtain an SSL certificate and configure your cloud service to use it. You may also want to install HTTPS Everywhere on all devices that connect to your cloud.\nLocal data backups\nThe cloud often lulls business owners into a false sense of security where data integrity is concerned. After all, if one of the main cloud benefits is that your data is backed up automatically, there’s no need to save it locally, right?\nNot necessarily. Hackers know that many businesses don’t save data locally, and they exploit this to their advantage when they launch ransomware attacks. Without local backups, you might feel pressure to surrender large sums of money to get your data back.\nHowever, the FBI recommends that you don’t pay ransoms. Not only is there no guarantee that hackers will play fair and return your data (these are criminals, after all), paying up could brand you as an easy target and make you more likely to be hacked in the future. Backing up your data locally will give you the confidence you need to refute a ransomer’s fee.\nDistributed denial-of-service protections\nYou may already be familiar with distributed denial-of-service (DDoS) attacks in which a hacker drowns your server, website or application with multiple requests for data, essentially rendering it useless. Cybercriminals have been launching denial-of-service attacks for at least 20 years, but today’s access to bots and IoT devices makes it even easier for hackers to coordinate the attacks through multiple systems, hence the term “distributed.”\nYou can fight back by building multi-layered protections into your networks and systems. Using web application firewalls, intrusion protection systems (IPS), load balancers and other tools, you’ll be able to better detect and prevent DDoS attacks and handle high-volume requests that might otherwise paralyze your network. A good managed service provider will be intimately familiar with these controls and able to add them to your infrastructure and servers.\nHackers are constantly at work, which means your IT support provider must be, too. To keep your company’s data, applications, websites and networks secure, you’ll need to stay one step ahead of cybercriminals.\nEssentially, this boils down to identifying system weaknesses before they cost you. You may want to engage your business IT support team in performing a vulnerability assessment, which will involve your provider testing your cloud storage networks to locate weaknesses that may allow hackers in and then developing a plan to address these. Performing these assessments regularly keeps your networks like a shark: always swimming.\nAn unfortunate truth about security threats is that they don’t necessarily come from outside a business. Internal team members with access to sensitive data on the cloud can wreak havoc in their own right, whether they take the form of disgruntled employees stealing confidential data for malicious purposes or negligent personnel accidentally exposing information housed in the cloud.\nRole-based access controls (RBACs) help you regulate the access given to various employees, allowing you to designate which servers and files individual users can open, edit or copy. Administrators can assign roles and privileges to users based on need, authority and the employee’s position within the company. Implementing these controls may also be required to stay compliant with statutory and regulatory requirements for your industry.\nSo what if there are two sides to the cloud computing coin? With the right cloud security controls, the odds will be in your favor every time!']"	['<urn:uuid:e454ccb4-761c-4cc3-97fc-8fc7300131cd>', '<urn:uuid:7485cb1f-5a98-4332-a04e-ffa03d3c8686>']	open-ended	direct	verbose-and-natural	distant-from-document	multi-aspect	novice	2025-05-12T21:31:42.699391	23	115	1660
18	emergency items driving snow vs regular road signs essential comparison	In snowy conditions, drivers need specific emergency items like de-icer, ice scraper, warm clothes, blankets and boots with good grip, while for regular driving the essential items are regulatory signs-related like reflective warning signs, a road atlas, and high-visibility clothing.	"[""Essex is once again buried in snow, with commuters left stranded and children unable to attend school.\nSome of the county's remote areas and villages are more affected than others.\nThe wintry weather can be harsh on your car's health and performance, with the cold temperatures making it more likely to break down, therefore it's vital you are prepared for the journey and the possibilities which could happen.\nMotoring experts at the RAC have conjured up a helpful list of essentials drivers need to pack in their car for the long winter journeys, the Nottingham Post reports.\nIf your car does break down, these emergency items will ensure that you can stay safe until help arrives:\n1. High-visibility jacket\nAlthough it may not keep you warm, high-visibility clothing will make you more visible to other motorists, especially if you have to leave your vehicle when it is dark.\nThe snow and rain can increase the danger if you've broken down on the side of the road and a high-visibility jacket could save your life. It should be included as one of the top priority emergency items to keep in the car.\nAny high-visibility clothing will provide extra protection, and in some European countries it is required by law, such as France and Spain.\n2. Boots with a good grip\nDrivers should have suitable and comfortable footwear behind the wheel, but it is just as important to keep boots with a good grip in the car during this time of year for safety.\nIn the heavy snow, as well as slippery conditions, wearing suitable boots with a strop grip on the bottom will help you manoeuvre when out the car.\nAnother alternative is to purchase snow grips that can slide over the top of your shoes to add extra grip. Both options help prevent you from slipping over.\nThe low winter sun can affect the driver's visibility and creates a glare making it difficult to see the road or other drivers ahead.\nKeep a pair of sunnies in your glovebox or in the car so they are easily to hand when needed.\nSalting routes in Essex\nEssex Highways have a total of 57 salting routes and operate 62 machines with ploughs plus two mini gritters.\nAll routes on the salting network are treatable in a maximum of 3 hours.\nThe routes consist of:\n- ’A’ roads that Essex Highways are responsible for - this excludes the A12, the A120 and the A130 which are administered by other organisations;\n- ‘B’ roads;\n- The access route to the main settlement of a parish of 50 or more households not already on the precautionary salting route;\n- Roads served by at least four public bus services per day for five days per week or more;\n- Roads that link sites of national importance (e.g. airports, ports, fuel refineries etc.) to the main salting network; and\n- Roads which allow ambulance and fire stations access to the main salting network.\n4. De-icer and ice scraper\nOne of the basic necessities for snow and ice but definitely worth mentioning.\nIt is a legal requirement for drivers to keep their front and rear windscreen clear of ice and snow before setting off, and not all cars or models have the extra of heated windscreens to speed up the process.\nA can or spray bottle of de-icer aids the manual effort it takes to clear the windscreen, and an ice scraper still requires you to physically scrape the car, but the two together can get you driving in no time.\nIn bad weather conditions, this should be done before driving away, but both items should be kept in the car ahead of each journey, or if becoming stranded somewhere occurs.\n5. Warm clothes and blankets\nWhen driving it is important to not be too hot in the car, but a thick winter coat is essential in the car. If you do break down you could be waiting a long time without heating from your car or outside and it's sensible to pack a big coat, a spare jumper, hat and gloves with you.\nAlso, if you have passengers along for the journey a blanket or extra clothing can help them out too, especially if you are travelling with the elderly or children.\n6. Torch and spare batteries\nIt is a very real possibility to be stuck on the side of a country lane in the dark winter evening and a flat car battery, and without any light it can be a dangerous but also frightening situation.\nA decent large torch, with sufficient spare batteries, or even a wind-up torch, should definitely keep you covered. It's vital to have it in your breakdown kit essentials, and a wind-up torch doesn't require battery power.\n7. First aid kit\nAnother top priority item for the essential checklist is the first aid kit, which can make or break a scary situation.\nThere is a national standard for first aid supplies in vehicles, which was devised by the British Standards Institution (BSI).\nA small first aid kit should include: wash proof plasters in assorted sizes, cleansing wipes, scissors, dressings, nitrile powder-free gloves, and a Revive-Aid resuscitation face shield, or something similar.\nAlthough, having a first aid kit is good practice any time of the year.\n8. Empty fuel can\nNot all breakdowns happen because of a flat battery, a mechanical fault or engine failure. Sometimes your car can come to a stop because simply, and annoyingly, it has run out of petrol.\nThis can be easily fixed providing you can make your way to a petrol station, safely, and when there fill up the can with fuel. However, you can't do this if you don't have the empty can prepped in the first place.\nUK snow facts\nThe UK gets on average 23.7 days of snowfall or sleet each year, and much of the snow does not settle, as there are on average 15.6 days a year when the snow does lay on the ground.\n- The snowiest winter of the UK took place in 1947, between January 22 and March 17, where snow fell every day somewhere in the country.\n- On September 25, 1885, snow was reported to have fallen in London, making it the earliest snowfall on the capital.\n- It doesn't have to be at or below freezing for snow to fall. However, if it is raining, with temperatures as high as 6C, it can cause the air temperature to fall low enough turning the rain into snow as it reaches the ground.\n- Snow is not white: they may appear to be white but they are in fact totally clear, however ice is translucent and lets light pass through indirectly. The many sides of the ice crystals in the snowflake diffuse the light spectrum making them appear to be white.\n9. Jump start cables\nDead or flat batteries can happen to any car regardless of its age and make and at any time, usually the most inconvenient, but in cold weather this problem is most likely to occur.\nWhen driving in the snow, always have a set of jump start cables or jump leads in the car as they will help to get the battery going again and you on your way. Whether you get assistance from a passing motorist, or a house nearby, or even the recovery and breakdown cover - they're important.\nIn deep snow, such as if you're driving on a smaller road which isn't gritted or treated, it can be really easy for the vehicle's wheels to get stuck.\nA shovel can help you to literally dig yourself out of the problem, and if your boot space is limited, an alternative is a foldable shovel.\n11. Food and drink\nThe idea of being abandoned at the roadside for hours on end, maybe in the dark and the cold and in poor weather conditions is a gloomy prospect. If you get hungry or thirsty it can make it that much worse, and having children in the car will make the situation even more terrible.\nFood and drink is important on the breakdown essential kit, such as a flask of soup tea or hot chocolate which you can make up before a long journey in the snow - as not everyone will make them up for regular journeys.\nAlso, keeping a few snacks, such as crisps and water in the car is good for any breakdown situation and they do not perish quickly.\nTop essentials every driver needs in their car\n- In-car phone charger/portable power bank\n- Food and drink for emergencies\n- High-visibility jacket\n- Road atlas\n- Reflective warning signs\n- Empty fuel can\n- Jump start cables\n- Spare batteries\n- First aid kit\n- De-icer and ice scraper\n- Warm clothes and blankets\n- Boots with a strong grip\n12. Reflective warning signs\nA reflective warning sign is a legal requirement in many European countries. It usually comes in the form of a triangle and is used to warn other drivers that your car has broken down to help avoid collisions.\nEach driver should have two, one to place in front of the car, and the other at the rear. Ideally, they should stand and not be blown over and be visible in the dark - so invest well to have a decent pair.\nThe signs also need to be at least 45 metres, or 147 feet, behind the car, however, you should never use them on motorways.\n13. Road atlas\nYes, even though most motorists use a sat-nav either built into the car, an attachment, or their mobile phones, a real road atlas could save you and your journey.\nThere is the possibility that if you get a flat battery, any electric device won't be able to work or be connected, and the retro paper-version could be the only way you get to your destination.\n14. In-car phone charger\nIf you break down in the snow and find yourself on the side of the road and then find your mobile phone is running out of power, an in-car charger can save a lot of time and misery. Also, a portable charger that you keep on you will help.\nIf you break down somewhere where you can't receive signal you'll have to try and wait for someone to stop and help, or if on a motorway locate the nearest emergency phone.\nOn the quieter roads, if there isn't a emergency phone, you should assess the situtation. It may mean you will have to walk to the nearest house, or sit tight and wait for someone to offer help passing by.\nIf driving out in snowy or icy conditions, ensure you leave enough space between you and the car in front, as well as allowing extra time for your journey as there may be delays."", '3 What Are Regulatory Signs? Regulatory signs regulate or control the movement of traffic. These signs tell you and other drivers what you must do and what you must not do when you drive. Regulatory signs are red, white, black, green on white, or white on black\n6 What Actions Should You Take at Regulatory Signs? Regulatory signs give commands or set limits. When you see a stop sign, you must stop. When you see a yield sign, you must slow and yield right of way to traffic on the cross road or the road into which you are merging. A speed limit sign indicates the maximum speed you may drive under ideal conditions.\n8 What Actions Should You Take at Warning Signs? When you see a warning sign, in-crease your level of alertness to changes in the roadway, in traffic, or in environmental conditions. Always proceed with caution.\n9 What actions should you take at a Stop Sign? You must come to a full stop at a stop sign. Often a white stop line is painted on the pavement in line with the sign. There may be two white lines indicating a pedestrian cross-walk just beyond the stop line, or there may be walk lines and no stop line.\n10 What should you do at a Yield Sign? A Yield Sign assigns right-of-way to traffic on certain approaches to an intersection. Vehicles controlled by a YIELD sign need stop only when necessary to avoid interference with other traffic that is given the right-of- way.\n11 What does a Rail-road Crossbuck sign tell us? This sign is located where railroad tracks cross the roadway. Regardless of whether or not lights or gates are present, if a train is coming you must stop.\n12 How do you know what signs are Warning Signs? All warning signs are either yellow or orange with black symbols or letters\n13 School Area SignsWhen you see a school zone or school crossing sign, you must slow down and proceed with caution. Children may be playing nearby and may dart into the street. At a school crossing sign, give right of way to children crossing the roadway.\n14 How do you proceed when you see this sign When you see a school zone or school crossing sign, you must slow down and proceed with caution. Children may be playing nearby and may dart into the street. At a school crossing sign, give right of way to children crossing the roadway.\n15 Guide SignsAs you travel along the roadways, you’ll see four kinds of guide signs. These signs give information about roadways and routes; the mileage to certain destinations; roadside ser-vices such as rest stops, service stations, and campsites; and recreational areas and nearby points of interest.\n16 Route MakersRoutes are the numbered road-ways that crisscross the continent. Interstate routes that lead into cities have three digits and begin with an odd digit. If a three-digit route begins with an even digit, the route goes around a city\n17 Destination and Mileage Signs You’ll often see destination and mileage signs mounted over highway lanes. They tell you where you are, which lane to take to get to your destination, what exits are coming up, and how far away the exits are.\n18 International SignsInternational signs are those that you can understand without knowing another language. The meaning is conveyed by their color, shape, symbols, and numbers.\n19 Yellow or White Lines on the Roadway Yellow and white roadway lines provide directions or warnings for drivers. Yellow lines divide traffic traveling in opposite directions. White lines parallel to the roadway separate same-direction traffic into lanes. White lines perpendicular to the roadway indicate crosswalks, railroad crossings, and stop signs at intersections.\n22 Broken White LineA broken white line separates two lanes traveling in the same direction. Once you have signaled, and if it is safe to do so, you may cross this line when changing lanes.\n23 White Lane LinesWhite lane lines separate lanes of traffic moving in the same direction. Single white lines may also mark the right edge of the pavement.\n24 Solid with Turn Lane Arrow Solid white lines are used for turn lanes and to discourage lane changes near intersections. Arrows are often used with the white lines to show which turn may be made from the lane.\n25 Two-Way Roadway with Center Lane Two-way roadway with a center lane for left turns in either direction of travel. The specially marked center turn lane is intended for slowing down and for sheltering turning vehicles and may not be used for passing.\n27 RedCome to a complete stop at the marked stop line or before moving into the crosswalk or intersection. At most intersections, after stopping, you may turn right on red if the way is clear. Some intersections display a ""NO TURN ON RED"" sign, which you must obey. Left turns on a red light from a one-way street into a one-way street are also allowed\n28 YellowStop if you can. The light will soon be red.GreenGo - but only if the intersection is clear. Yield to pedestrians and vehicles still in the intersection. If turning left, wait for gap in oncoming traffic to complete a turn.\n29 Flashing SignalsA flashing red light means the same thing as a stop sign. It is used at dangerous intersections.A flashing yellow light means you may move forward with caution. It is used at, or just before, dangerous intersections, or to alert you to a warning sign such as a school crossing or sharp curve.\n30 Many street crossings have pedestrian signals that show the words ""WALK"" and ""DON\'T WALK"" or show a ""WALKING PERSON"" in white and a ""RAISED HAND"" in orange. Pedestrian signals direct pedestrians while crossing the street. The ""WALK"" or ""WALKING PERSON"" appears when it is legal to start crossing.When the ""DON\'T WALK"" or ""RAISED HAND"" appears, you may not start across the street. The flashing signal means you should not begin to cross because you may not have enough time to make it to the other side before vehicles start moving across your path. If the flashing starts after you have already started\n31 YELLOW ARROW arrow.- A yellow arrow means the ""protected"" turning time period is ending. Be prepared to obey the next signal that could be the green or red light or the red\n32 Now turn to the page 95, in Chapter 5, and do the questions on that page and turn them in!']"	['<urn:uuid:0ca1778a-5d91-40b5-a044-68da623ba9c1>', '<urn:uuid:e9153aa8-d6d2-4962-82d2-b3520f44168e>']	factoid	direct	long-search-query	similar-to-document	comparison	novice	2025-05-12T21:31:42.699391	10	40	2898
19	telescope magnification for space photos	The AG Optical Telescope (Dall Kirkham) provides approximately 75 times magnification with a focal length of 1650 mm and aperture of 250 mm at focal ratio f/6.7. The Stellarvue SV80S Refractor provides approximately 21 times magnification with a focal length of 480 mm at focal ratio f/6.	['Astrophotography is a unique hobby, that is at once science, art & craft. It’s a very satisfying hobby that utilizes all your brain-cells and makes you fulfilled from a creative standpoint as well. See individual sections in the menu as well.\nI have created a website with links of all astrophotography related tips and tricks. See www.astrophotolinks.com.\nProcess of Astrophotography\nThe planets are usually bright enough that normal exposures are enough to capture the object. For example, to take picture of Moon, an exposure of 1/250 s. is sufficient, Jupiter can be captured with exposure of 1/50 or so. Such short exposure only require a large aperture and correct magnification.\nDeep sky objects are a different story. These objects are usually very dim. Except for couple of very bright nebulae, most objects require a minimum of 30 second exposure. Most objects require exposure of 5 minutes or more. When taking long exposures, we are confronted with 4 issues:\n1. The longer the exposure, more is the ‘noise’ i.e. heat within the camera that gets recorded as false signal. That creates unwanted grains.\n2. There is so much man-made light pollution that the ambient light starts overpowering the light emitted from your target. This necessitates that individual picture cannot be shot over for more than a few minutes. But, in those few minutes, we do not get enough data! We require more data!\n3. Most of these objects are so far away that the photons (light is made of photon, individual packets of energy) come intermittently i.e. signal may be received at a specified pixel in some images only, not all images. For example, a galaxy’s spiral arms’ outer edges may be so dim that the light emitted by the stars in that area may send one photon our way every few minutes. Therefore, multiple pictures have to be taken to get at least one photon in one of your pictures.\n4. Earth moves! Every second! Even at the shortest focal lengths, say, 120 mm, the Earth moves enough that the target as well as all the neighborhood stars move in the picture when shot over 15 seconds, creating ugly star trails.\nTracking & Autoguiding\nTo overcome all the constraints, astrophotographers use high quality mounts that track movement of the objects as they move from East to West in Earth’s sky. Even then, the tracking is not perfect. At higher focal lengths, these mounts give proper tracking of up to 1 minute or so. To overcome this, astronomers use a process called ‘autoguiding’. Autoguiding entails using another camera to lock on to a bright star close to the object being shot. A computer is used to receive a picture of that star every few seconds. A software checks if the star is moving from the locked position and gives instructions to the mount to correct its movement so that the star returns back to the locked position. This process ensures that we are able to take pictures of few minutes without any unwanted star trails.\nAfter multiple pictures are taken, we use softwares that help us combine data from multiple pictures, remove extra noise and extract fainter portions of the picture without overexposing the brighter portions.\nEquipments & Software that I use\nQSI 683wsg. A CCD Camera that can cool the sensor by about 40 degrees Celsius than ambient temperature. This reduce the noise substantially due to lesser heat. Most of my earlier pictures were shot by a Canon T2i DSLR camera, but, I am using QSI 683 since last 6 months.\n1. AG Optical Telescope (Dall Kirkham) – Focal Length 1650 mm. Aperture 250 mm. Focal Ratio f/6.7. For my camera, the magnification comes to approx 75 times.\n3. Stellarvue SV80S Refractor – Focal length 480 mm. Focal Ration f/6. For my camera, the magnification comes to approx 21 times.\nI now use Paramount MX mount, which is definitely better than the Losmandy G11 mount that I used couple of years back. As I have said earlier, proper tracking is way more important than a better scope. It’s all about guiding.\nFor Autoguiding – Maxim DL\nFor Stacking – Maxim DL\nFor Processing – Pixinsight\nFor Final touches – Photoshop Essentials']	['<urn:uuid:189d9b44-61bd-4594-b2f5-435ae0971214>']	factoid	with-premise	short-search-query	distant-from-document	single-doc	novice	2025-05-12T21:31:42.699391	5	47	701
20	What are the different ways that the Pacific Islander fishing program in Oregon and the Alaska Native art mentorship program address the issue of dying traditional practices?	The Pacific Islander fishing program addresses dying traditional practices by helping community members adapt their traditional fishing and crabbing practices to Oregon's regulations through cultural education and storytelling, using community ambassadors to bridge the gap between traditional island fishing cultures and state requirements. The Alaska Native art mentorship program tackles dying traditions through direct skill transmission in specific art forms, such as Bobby Itta teaching traditional parka-making, which she notes is a dying art with very few practitioners, and Marlene Nielsen teaching rare salmon skin art techniques that were once considered lost. Both programs aim to preserve cultural practices but use different methods - community-wide education versus one-on-one mentorship.	['GoFishPI Ambassadors Camp\nSupporting food security and expanding seafood access through inclusive and culturally appropriate experiential education for the Pacific Islander community in Oregon\nThis project aims to address the significant cultural disconnect between members of the Native Hawaiian and Pacific Islander communities living in Oregon and their understanding & compliance with the rules and regulations surrounding access to fishing and crabbing in the region.\nHistorically, this population has represented a disproportionate incidence of infractions and citations for littering, fishing without a proper license, and catching fish out of season. Illegal fishing is an obvious problem for marine and aquatic conservation and effective fisheries management. It is also a significant problem for the individuals within the NHPI community who receive citations. Some community members accrue repeated citations which leads directly to financial hardship. Many rely on fishing for subsistence and are unfamiliar with navigating legal channels to deal with tickets they lack the means to pay.\nParticipation in fishing and crabbing are essential pillars of these island cultures. However, much of this proud tradition exists and flourishes in a context with few restrictions on where, when, or how recreational and subsistence fishing takes place, unlike the complex requirements imposed by Oregon state fishing regulations. There is a clear opportunity for targeted and culturally competent outreach and education to help ensure that Native Hawaiian and Pacific Islander Oregonians have access to recreational and subsistence fishing and crabbing, and accompanying food security benefits, without running afoul of the law.\nThrough previous work with this community, Living Islands has found success in education and disseminating information through direct personal connections and engagement with storytelling. Living Islands also leverages key community members’ personal and familial networks to efficiently bring vital information and resources to this traditionally underserved group. This approach was critical to their success in coordinating events during the pandemic, such as food and COVID vaccination drives targeted at this population.\nAcknowledging that there is a need to improve understanding of the differences between Pacific Islander native cultures and the regulatory expectations in the state of Oregon, Living Islands has arranged to bring together professionals from the Oregon Department of Fish and Wildlife, Oregon State Police, with outreach and educational materials from the Oregon Health Authority to engage with a select group of community leaders representing the 14 distinct Micronesian cultural communities living in Oregon. With the help of the Oregon Sea Grant, our coalition will provide these ambassadors with a workshop packed with a specifically tailored curriculum designed to foster cultural exchange and connection through storytelling and experiential education. The courses and presentations have been closely coordinated with affiliate organizations and arranged with our Palauan community leader to ensure as much of the material offered is directly pertinent to and presented in ways designed to connect with the attendees. We are also proud to include presenters from Oregon Sea Grant to share some of the vital work they do and how their scope of research impacts many of the nations our Pacific Islanders hail from.\nWe have selected a key subset of Oregon’s Native Hawaiian and Pacific Islander community leaders to act as ambassadors for this program, through whom we can connect their broader individual cultural networks with the people and resources available in the Oregon Department of Fish and Wildlife and Oregon State Police. Workshop participants represent a wide range of linguistic and cultural diversity within the Native Hawaiian and Pacific Islander communities. They will be charged with sharing the information they gain during the workshop with fellow members of their cultural groups in Oregon and, when appropriate, in their countries of origin who might be considering coming to Oregon. By cultivating and educating this group of trusted ambassadors, we can affect a shift in attitudes across disparate regions.\nThe intention is to continue and expand this project in years to come, thereby increasing the scope of included individuals and improving the penetration of resources and information provided. This means a significant component of our goal for this year’s inaugural program is to develop a higher degree of passion and interest in the benefits of learning about and subscribing to the rules and regulations surrounding their access to seafood and the natural resources we all enjoy here in Oregon.\nThis project’s inspiration was partly derived from the incidence of fishing citations among NHPI communities. Subsequently, the benefits we hope to affect directly are decreases in these occurrences through attitudinal shifts and cultural education among our ambassadors and their community networks. We hope to encourage a better understanding of and compliance with existing fishing regulations by helping build relationships between representatives from the regulatory and enforcement agencies and our GoFish PI Ambassadors. We hope these personal connections will motivate the GoFish PI Ambassadors to help their fellow community members understand why the NHPI community should care about following fishing and crabbing regulations, ultimately resulting in benefits for the NHPI community and for the marine, coastal, and aquatic ecosystems of Oregon.\nThis project will improve NHPI understanding, perception of, and attitudes toward Oregon Department of Fish and Wildlife and Oregon State Police personnel, smoothing personal interactions between them and the NHPI community. We anticipate slight improvements in rates of citation quickly after completing this year’s project and intend to nurture those changes in the long term with continued liaising between regulatory and law enforcement staff and our NHPI citizens.\nThis project would not be possible without the contributions of our partners at the Oregon Department of Fish and Wildlife, Oregon SeaGrant, Oregon Health Authority, and Oregon State Police. It is also made possible by sponsorships through the Oregon Food Bank, Oregon Pacific Islander Coalition, West Marine, ShrimpDaddy’s Bait & Tackle, and others.', 'Four self-directed, six-month-long mentorship opportunities pair Alaska Native artists with mentees in traditional, physically tangible art forms. Artists were selected by Bunnell from the pool of artists participating in and working on projects for Bunnell’s national touring exhibition, Protection: Adaptation and Resistance. The mentorships explore construction of traditional regalia and garments with support from The CIRI Foundation.\nTCF Mentorship Descriptions:\nMentor artist Bobby Lynn Itta is teaching skin sewing and regalia to Bonnie Scheele. Bonnie lives in Anchorage and owns a small business named Twenty Mile Trading Company where she creates jewelry out of reindeer antler. Bonnie is a reindeer herder and is Inupiaq from Nome. She wil learn to make a parka. Bobby Itta is Inupiaq from Utqiagvik and has been sewing parkas for 18 years. She is the designer and owner of Arctic Luxe and Alaska Fur Cache. Arctic Luxe is a brand that promotes and sells handmade indigenous clothing and accessories. Bobby believes it is very important to teach these skills because traditional skin sewing is a dying art. There are very few that know how to make parkas, and it is important to teach and share this knowledge.\nJennifer Younger is a Lingit artist and designer who is learning the skills of Lingit regalia construction from Carol Hughey, an apparel designer with extensive textile skills who was adopted into the Kiks.ádi Clan. Hughey has worked in collaboration with Lingit tribal citizens to create contemporary at.óow (ceremonial sacred objects). Younger and Hughey will create a protest robe in the Lingit tradition of the shaming totem pole. The robe will express outrage over the destructive commercial Sacro Herring fishery that violates the Sovereign rights of the tribal citizens. They will unveil this robe at the Alaska Board of Fisheries hearings in Ketchikan Alaska in January 2022. The robe will be dedicated and danced at herring celebrations for many generations to come. The early years of the robe will protes extracted fishing practices. The artists hope the robe’s later years will celebrate their respect for all things over commercial exploitation\nMarlene Nielson, a Yup’ik artist from from Kokhanok, on the south shore of Lake Iliamna, is self-taught in the art of making baskets, wallets, and jewelry with sockeye salmon skin. Salmon skin art was seen as a lost skill in her area which inspired her to reintroduce the rare technique by teaching students. Nielsen has been creating art with fish skin since 2002 and has completed a 2017 IAIA Artist-in-Residence. Nielson mentors Yup’ik artist Peter Williams, a Yup’ik culture bearer, artist, designer, filmmaker, and educator originally from Akiak currently based in Sheet’ká (Sitka, Alaska). His hand-sewn works repurpose skin from self-harvested traditional foods, bridging worlds of Indigenous art, fashion, and subsistence. The mentorship focuses on researching and documenting traditional Yup’ik fish skin tanning and sewing techniques using natural materials, stitches and methodologies in the creation of a fish skin hood. Through collaborating with Elders, other artists and museums where there are information gaps, the project aims to disseminate traditional knowledge to future generations of Alaska Natives through video, photography, audio recording, written text, experimentation, process-oriented research, bark tanning and hand-sewing object creation.\nMentor artist Lily Hope leads her apprentice Sydney Akagi through weaving a child-sized Chilkat dancing blanket, inspired by one in the Alaska State Museum. The time together will focus on technical mastery of ﬁber spinning and dyeing, warping up the ﬁrst rows, weaving black and yellow borders, inserting drawstrings, negotiating design adaptations, interlocking colors, braid insertion, dropping warps, properly curving the bottom border, and weaving side braids. The skills learned by the apprentice ensure she retains all skills needed to weave a full-size Chilkat dancing blanket. In addition to techniques, Hope and Akagi will focus on spiritual aspects including gratitude prayers, daily family and home care practices, and energetic surrender to the work. In this way, the apprentice can teach her daughter(s) and nieces to weave with the Chilkat teachings fully intact.\nThank you to The CIRI Foundation for funding these mentorships.\nSydney Akagi (Tlingit/Japanese)\nIrene Lampe (Tlingit)\nPhotographs courtesy of\nSydney Akagi and Lily Hope']	['<urn:uuid:674a6fbf-83b8-4e59-a89c-a2e08fc80db2>', '<urn:uuid:a7eaebcd-3fd1-4d18-a74e-f2430cd4139a>']	open-ended	direct	verbose-and-natural	similar-to-document	comparison	novice	2025-05-12T21:31:42.699391	27	109	1628
21	I'm intrigued by the various identities of Rosh Hashanah in rabbinic literature. What were all the different names that the Talmudic rabbis used for this holiday?	The rabbis of the Talmud knew Rosh Hashanah by several names: Yom Teruah (the day of trumpet blasts), Yom Hazikaron (the day of remembering), Yom Hadin (the day of judgment), and Yom Harat Olam (the day the universe was conceived), in addition to its common name Rosh Hashanah, which means 'the head of the year.'	['Erev Rosh Hashanah\nLike a professional spy, Rosh Hashanah has many names and identities. We know it best as the Jewish New Year, or Rosh Hashanah literally meaning “the head of the year.” But the Torah refers to this holiday as either Yom Teruah (the day of trumpet blasts) or Yom Hazikaron (the day of remembering), depending on the passage. The rabbis of the Talmud couldn’t seem to nail down Rosh Hashanah’s identity either. They knew Rosh Hashanah by all of these aliases, plus two more: Yom Hadin (the day of judgment) and finally Yom Harat Olam, meaning “the day the universe was conceived.” I am particularly drawn to this last alias.\nIf today is Yom Harat Olam, or “the day the universe was conceived,” that means we are celebrating the anniversary of creation. And when we celebrate an anniversary, we are not just commemorating some past event. We are celebrating its ongoing influence in our lives. For example, when a married couple celebrates their anniversary, they are ideally celebrating their ongoing relationship and continued love, rather than simply commemorating the date of their wedding ceremony. And when we celebrate the conception of the universe, or the birthday of the world as the cakes at tomorrow’s Family Service will say, we are not just celebrating the moment when our mustard-seed of a universe began to expand. We are celebrating the ongoing process and power of creation.\nOur community includes a number of professional artists and parents, who have a profound understanding of what it is to create. The artists and parents among us know what it is to conceive and birth something into existence. But we do not all have to be professional artists or parents in order to participate in the ongoing process of creation. Every person here is a creator, fashioned in the image of the ultimate Creator. Every single one of us is a creator of life, as we create our own lives day by day, year by year.\nYom Harat Olam is the day for us to decide what it is exactly that we wish to create this year. Today is the day we must plant the seeds for the lives and growth we would like to see. But before we start planting seeds willy-nilly, we have to take a step back from our daily lives and consider the bigger picture. We need to reflect and come up with a rough sketch of the garden we wish to create. We must ask and answer for ourselves one of the most common and often dreaded job interview questions: “Where do you see yourself in 5, 10, or 20 years?”\nMany people find their palms becoming moist with anxiety while trying to describe their long-term professional goals to an interviewer. But this question need not cause us anxiety on Rosh Hashanah, when it’s just us asking the question of ourselves and when we’ve already got the job. Yes, God already hired us to be the landscape artist of our own lives. God knows we all have what it takes to make something beautiful.\nMost of us have been doing a great job already, even if we have doubted some of the decisions we have made along the way. Was it really a good idea to plant squash over here? It seems to have taken over the yard. Was it wise for me to go to medical school? I’m not sure that’s what I want to do for the rest of my life. Should I have waited longer to get married? Would marriage have been easier five years later? We all have these questions and doubts sometimes.\nEven though these questions can sometimes become a source of anxiety, it is still a blessing that we have such capacity for self-reflection. It is this capacity for examination that enables us to turn our lives into beautiful and unique gardens. To share a personal example of self-reflection and transformation, let me tell you that although I am a rabbi now, I did not grow up in a religious household. My weekends were spent watching television with my family and playing outside with my friends, rather than attending Shabbat services or Religious School. But that all changed in 7th grade, when I had to write a speech for my English class. This middle school assignment led to perhaps more self-reflection than the teacher could have ever anticipated. It was then that a seed was planted. I don’t know if it was a comment my grandmother made about her parents hiding their Jewish identity in Nazi Germany, or the fact that one of my favorite TV shows at the time stared a Jewish character. But something inspired me to pick Judaism as the topic for my 7th grade speech.\nAs part of my research, I started attending Shabbat services with my grandmother. There was something about the music, the intergenerational community, and the ancient wisdom I encountered at Shabbat services that suddenly piqued my interest. It was like nothing I had ever experienced before. It led me to come back week after week, excited to learn something new and to connect with a newfound spiritual side of myself. I realized then that something had been missing in my life. There was this rich spiritual and communal experience available the whole time, and I was only now becoming aware of it. As soon as I encountered this warm and inviting Jewish community, I realized that it was something I wanted to plant in the garden of my life.\nSo I got to work. I grabbed a metaphorical shovel and started digging. I signed up for an adult Hebrew class with my grandmother and became a regular at both Shabbat services and Torah Study at age 12. I asked my mom if I could enroll in the Religious School, and I read as many books and websites about Judaism as I could find. I knew the more work I put into it, the more lush my garden would be.\nBut that turned out to be only one part of my garden. As I got more involved with the synagogue, I grew closer to the rabbi. He really took me under his wing and graciously allowed me to shadow him in order to see what rabbinic life was like, when I said that the idea of becoming a rabbi had crossed my mind, but I wasn’t sure exactly what rabbis did when they weren’t leading services. While shadowing him, I saw how this man was able to help countless families through all sorts of challenging circumstances. I saw how he shared in the many joys of their lives, too. And I saw how community and Torah were at the center of his life. In other words, I peeked into the garden of his life and realized that I wanted the garden of my life to have the same beauty. At age 13, I decided that I was going to become a rabbi, and I started planting more seeds.\nI asked my family to enroll me in Jewish schools, including a Jewish boarding high school in North Carolina, so that I could get an even more substantial Jewish education. Then in college, I became a student leader in Hillel and began to teach Religious School at a local synagogue. I also made multiple visits to Hebrew Union College, the Reform rabbinical school, to get myself on their radar before I applied to them my senior year of college. I wasn’t going to leave the fulfillment of my dream to chance. I wasn’t going to let it sit in a stack of ideas for that vague “someday.” I knew if I wanted this to happen, I had to plant the seeds, water the soil, and pull out the weeds day after day, year after year. I was determined to create this beautiful garden of a life.\nBut I wouldn’t have known what to do, if it were not for my first rabbi and other mentors who guided me along the way. I would not have known how to make this garden, had I not seen the beautiful gardens that they created first. I would have wandered around lost if I had not asked them what I could do to create a life like the ones they had created. Of course, sometimes we all feel a bit lost. We may not know what exactly we want to do with our lives, until we encounter someone who is already doing it. This ambiguity about purpose and direction in life is very common. Even the first human beings, Adam and Eve, had this experience.\nIn the story of creation, which we will be reading in the next couple of weeks, God models for Adam and Eve what a garden can look like. God creates the garden for them to enjoy and then instructs them to work the land, so they can get a strong sense of how to take care of a garden before it is time for them to go off and create their own gardens. The first human beings needed this sort of modeling and guidance. They needed to be exposed to someone else’s creations before they could figure out how to create a good life for themselves. This is true for most people. That is why we have parents, teachers, rabbis, friends, and sacred texts to guide us. We all need artistic inspiration and guidance in life.\nSometimes we can also benefit from the fresh perspective of others. Good friends, loved ones, and professional advisers including therapists can help us to see when weeds may have grown in our gardens. They can often help us to remove those weeds before the weeds take over our garden. And they can recommend other things to plant in their place. Everyone around us is filled with some kind of wisdom. Everyone around us has made their own garden with at least one beautiful aspect that we can imitate. If you are feeling unsure about the garden you have created so far, you can change it. Research how you can remove the weeds from your life. Maybe those weeds are grudges, addictions, or negative self-talk. There are known ways to remove all of these weeds from the garden of your life. Other people can help you find out how.\nIf you are feeling lost or aimless in life, do not despair. Rather look to other gardens for inspiration. No garden is perfect, but many gardens are quite beautiful and worthy of imitation. If you are at a point where you love your garden, share it with others. Take other people under your wing and become a guide to the creation of beauty and meaning.\nToday is Yom Harat Olam, the day we must decide what world we wish to create. Think about the seeds you have planted in the past, and if they seem worth planting again or nourishing further. You have already planted some beautiful things. But your garden is not full yet. There is still time in your life to create something new, no matter if you are 6 or 96 years old. So I ask: What seeds are you going to plant this year? And I pray that you will find the support you need to tend your garden and create the luscious and beautiful life you deserve. Shanah tovah!']	['<urn:uuid:a6e73d1e-a4e5-43c7-b0c3-ca2a9ebc6d15>']	factoid	with-premise	verbose-and-natural	similar-to-document	single-doc	expert	2025-05-12T21:31:42.699391	26	55	1901
22	What role do agreements and security policies play in business operations?	Agreements and security policies serve crucial functions in business operations. On the international level, agreements like the Egypt-Iraq oil-for-reconstruction deal facilitate economic cooperation, with 15 new agreements signed and implementation mechanisms being established. On the security side, organizations need comprehensive cyber security policies that address remote working risks, including specific procedures for device protection, password requirements, public WiFi usage, and email security. These policies must include clear guidelines on actions to take if security is compromised and potential disciplinary measures.	"['CAIRO: The oil-for-reconstruction agreement signed between Egypt and Iraq will enter into bilateral implementation as soon as constitutional procedures are complete, Egyptian Prime Minister Mostafa Madbouly has said.\nLast month, the Egyptian government said it had reached a preliminary consensus with Iraq on establishing the mechanism.\nThe prime minister added that there is a presidential directive to begin a number of joint projects, pending the completion of ratification procedures.\nMadbouly said Egypt is ready to provide any raw materials, supplies or equipment to the Iraqi side. He added that Egypt is keen to overcome any routine measures that may hinder the activation of the agreement, and prepare a presidential directive to form a permanent follow-up committee for the projects.\nThe agreement will see Egyptian companies implement development projects in Iraq in return for quantities of oil imported by Egypt.\nEgyptian Minister of Petroleum Tarek El-Molla recently said that support for petroleum products in Egypt fell by 46 percent in the first quarter of the 2020-2021 fiscal year.\nThe Egyptian prime minister also said that a tripartite summit between Egypt, Iraq and Jordan is scheduled to be held during the first quarter of next year, in order to announce a number of joint projects between the three countries.\nMadbouly stressed the importance of setting up implementation mechanisms, so that preparations for the upcoming tripartite summit can be completed.\nIraqi Minister of Planning Khaled Battal said that Iraq is keen to arrive at a specific list of development projects, which will be implemented in cooperation with the Egyptian side.\n“In the next few years, a specific list of projects that will be implemented will be reached, provided that it includes one or two projects nominated by each of the relevant ministries, and that there will be continuous follow-up by the relevant ministries on both sides, bearing in mind that a specific timetable will be set for the implementation of these projects, which has been agreed upon between the two sides,” Battal said.\nEgyptian-Iraqi relations have developed significantly in recent months, culminating in 15 new agreements signed during Madbouly’s visit to Baghdad last month.\nEgyptian President Abdel Fattah El-Sisi ordered the rapid implementation of joint projects with Iraq and gave his support for security and stability in the cooperation.\nDuring his meeting with Iraq’s deputy prime minister and minister of planning, El-Sisi stressed the fundamentals of the Egyptian policy in support of Iraq. He said Egypt will help Iraq boost its “Arab national role” and overcome challenges.\nA large group of Egyptian factories recently laid out an ambitious plan to benefit from the oil-for-reconstruction agreement.\nLast week saw meetings with Iraqi delegations to examine business opportunities for the factories.\nSamir Aref, head of the Tenth of Ramadan Investors Association, said that factories have a “great desire” to take advantage of the Iraqi market, which will play a major role in increasing sales.\nAref added that more meetings will take place between Egypt and Iraq in the coming period to learn how to activate and benefit from trade opportunities.', ""Everyone's working remotely these days, yet security risks remain. Here are 10 ways you can combat online security threats.\nShare this article\nThere can’t be many businesses today that don’t use remote working to some extent throughout the working day. Even those without a culture or need to offer remote working will have employees or directors taking work home, or working from hotel suites, conference venues and public transport at times.\nThis more casual form of remote working, one that may not be accounted for when analysing how business IT networks are used, is often missed in cyber security policies and procedures. However, it is one important factor that can put organisations at risk of cyber attacks and data breaches.\nRemote working, whether a formalised arrangement between a business and an employee, or an ad hoc ‘needs must’ requirement to get work done, can leave your business IT network, systems and devices vulnerable.\nThe first step for managing security and remote workers is to understand where your business is at risk. This should be followed with an awareness raising campaign within the organisation so that all employees understand how their actions may compromise security and what steps they must take to protect company networks and systems.\nCyber security policies need to include the specific risks associated with remote working, with procedures and guidance in place for working away from the office. This will also need to explain what actions need to take place if a remote worker believes they have exposed the company to a cyber attack, and any disciplinary measures that may be taken.\nThe following top tips provide an excellent starting point:\n1. Keep mobile devices and laptops safe\nLost and stolen mobile devices and laptops are easy pickings for cyber criminals if insufficient security measures are in place. The first line of defence is to look after these business assets: keep them with you and in sight at all times, and never leave them in hotel safes, cars etc.\nNext up is securing the devices themselves with good password hygiene and encryption on laptops. Finally, installing mobile device management apps such as AirWatch and MaaS360 give employees a chance of securing and recovering lost mobiles or tablets.\nRemember: it's not just cyber crime that can disrupt your IT\n2. Excellent password hygiene\nStrong passwords will not only protect your devices and systems being accessed if a mobile or laptop is lost or stolen, they also protect businesses from hackers. Good password hygiene includes using long passwords with multi-characters, two-step authentication processes, and unique passwords for different systems and logins.\n3. Ensure up-to-date security protection is in place\nAny devices that are owned by the organisation should be properly protected with antivirus, web filtering, firewalls, device encryption and other preventative software, but so too should your employees’ own devices if they are using them for remote working.\nThis can be a difficult area to negotiate as your employee may feel this impinges on the personal use of their device: Your cyber security policies will need to address issues like these, either restricting staff from using their own devices for certain business critical activities, providing secure company owned devices, or making your cyber security protection mandatory.\n4. Use of public wifi\nPublic wifi can be vulnerable to malicious attack, presenting issues for those employees who may need to work from a hotel or conference. While it is good advice to only connect to trusted networks this is not always feasible.\nTherefore, your remote working / cyber security policy should stipulate that employees should not use public wifi for any sensitive, business critical activities. It is advisable to draw up some guidelines that explain what systems and activities staff can and cannot access when using public wifi.\n'Quick question Dave: where's all our money gone?\n5. Email encryption and best practice\nEmail is perhaps the most used digital technology by staff members who are away from the office, and one that can open a backdoor to cyber criminals. Encryption and robust management of corporate email is a must.\nThe installation of applications such as Mimecast is a no brainer, but raising awareness of the vulnerabilities of email will also help embed best practice in your organisation. This can include training in spotting cyber threats like phishing emails, and also policies on what information should not be communicated in an email – for example logins and passwords.\n6. Using public computers\nWhile the majority of people will have their own laptop or mobile device that they use for remote working, occasionally someone may need to use a public computer such as in a business suite in an airport.\nEmployees should be aware of the security implications of this and adhere to the following guidance: keep screens private (position them away from other people), don’t use public computers for any sensitive information, use ‘private browsing’ where possible, never use ‘remember me’ or ‘save information’, and clear your browsing history and delete any downloads before closing the browser.\n7. Using devices when out and about\nEmployees should also be aware of physical threats when using devices when in public places like cafes, hotels, airports etc. Just as you would hide your PIN when using an ATM, employees should be discreet when keying in passwords and logging into systems.\nThey should also be aware of the risk of snooping and eavesdropping, not just online, but also from other people in the vicinity. Can someone see and potentially grab a discreet photo of company sensitive information while they work in a public space?\n8. Removable devices\nUSB sticks and other removable devices can be a source of malware and should be checked first. Many conferences hand out USB sticks that may be infected, often unbeknown to the organisers. Also don’t allow anyone to plug in a USB device into your computer, for example to share information in a meeting. Always get your IT department to security check removable devices.\n9. Monitoring and policy enforcement\nTwenty four-seven network monitoring and security will help your organisation identify threats and monitor users on your networks. Remote workers and their mobile devices can be monitored using this solution to protect your organisation’s network.\n10. Negligence and accidental risks in the home\nEven when your employees are working from home using your secure VPN, VDI or remote desktop, there can be other risks that need to be considered. Children and pets can be a surprising threat.\nCats have a habit of jumping on computer keyboards and inquisitive minds might press a few keys when a laptop is unattended. These kinds of risks should be addressed in your remote working / security policies to ensure that your staff take every feasible step to protect your systems at all times.""]"	['<urn:uuid:ef9bbe34-59a7-47da-858d-3ed64898169b>', '<urn:uuid:02598588-2ea6-4379-b2e0-6b47df581446>']	open-ended	direct	concise-and-natural	similar-to-document	multi-aspect	novice	2025-05-12T21:31:42.699391	11	80	1633
23	What causes autism in children?	Scientists don't know the exact cause of autism, but research suggests multiple factors are involved. Genes appear to play a role - if a child has autism, their siblings are more likely to have it too. Scientists have found hundreds of genes potentially linked to autism. Some researchers think differences in a baby's immune system might be involved, and they're also studying factors during pregnancy, including the mother's immune system activity. Recent brain tissue studies have found disorganized brain regions lacking specific genes, suggesting autism might begin before birth.	"[""This is the first of a two-part series on autism spectrum disorders.\nLaura Shumaker first realized that Matthew was different from other toddlers when he was about a year old. During a group play date, other kids stayed busy exploring a variety of toys — and each other. Her son instead peered at the wheels of one toy the whole time. Although he had begun to talk like other 1-year-olds by then, Matthew mysteriously stopped soon thereafter.\nOver the next few years, instead of answering questions, Matthew just repeated them. He focused intently on lights and other objects, yet had trouble looking his parents in the eye. The Shumakers, who live near San Francisco, took Matthew to doctor after doctor. They were anxious to figure out what was wrong. Eventually, when the boy was 5, an expert diagnosed Matthew with a brain disorder called autism.\n“It was really hard because I truly didn’t know anyone else who had a child with autism,” Laura Shumaker says. (Her 2008 book — A Regular Guy: Growing Up with Autism — describes her family’s experience.)\nAutism changes how the brain develops. As a result, affected kids often have trouble interacting with other people. They can struggle to communicate. They may repeat over and over the same behavior or action —such as rocking back and forth.\nAs a term, “autism” can refer to any of many different (but closely related) disabilities. Experts refer to the group as “autism spectrum disorders.”\nIndeed, researchers use the word “spectrum” because autism’s symptoms can vary so much from one person to another. “It is such a broad-spectrum disorder: You can have a little bit of autism or a lot of autism,” Laura Shumaker says. “You can be a genius with autism but have no ability to interact with people. You can be non-verbal and very bright, or just like Matthew: He’s a nice, earnest guy who’s very quirky. Everyone’s different.”\nEven though autism diagnoses are becoming more common, no one knows exactly why. “If you have 100 children in your school, you’re probably going to have one or two kids with autism,” says Zachary Warren. He directs a major autism institute known as TRIAD at Vanderbilt University in Nashville, Tenn. (TRIAD stands for Treatment and Research Institute for Autism Spectrum Disorders.) Warren says that many autistic kids struggle to do things that might come naturally to their others their age.\nMore than 20 years after Matthew’s diagnosis, researchers still don’t know what causes autism. Many scientists, however, are excited by new studies that suggest it can be spotted quite early in a child’s life. That’s important because other research shows that early treatment can make a big difference in reducing autism’s impacts.\nAutism can be socially isolating\n“Imagine waking up in the morning and wanting friends but just not being able to connect with them. Or being on the playground and being the last person that anybody wants to play with,” Laura Shumaker says. Kids with autism may have a hard time showing emotion. That makes it harder for others to know if they’re happy or sad. Similarly, children with autism have a hard time reading facial expressions. So they may not be able to figure out how others are feeling.\n“Certainly, they enjoy things and are happy and get frustrated and are sad, just like all of us,” says Kathy Angkustsiri. “But they may not show it the same way.” Angkustsiri, a doctor and autism expert, studies child development and behavior at a research center. Located at the University of California, Davis, it’s called the MIND Institute. (MIND stands for Medical Investigation of Neurodevelopmental Disorders.) With autism impairing someone’s ability to show or read emotions, “You can see how that can get you into a lot of trouble as you get older and have to hold a job.” But even on the playground it can create social challenges, she says.\nPeople with autism “don’t have the same ability to pick out the most important social cues,” Angkustsiri says. “It’s like the volume is just turned up all the way.” And if everything is at the same volume, she says, “you can’t sense the things that should stand out more.” Some scientists like to say that the brain is “wired differently” in people with autism. That means they often see, hear and feel the world in ways that the rest of us don’t.\nIndeed, some children with autism can be extremely sensitive to loud noises, bright lights and even the textures of food. They may overreact to these sensations, yet still not respond when a parent calls their name.\nWhen Matthew was young, Laura Shumaker says, he heard every noise. What’s more, “It was like a very loud noise all the time.” He’d be sitting in school trying to complete a workbook assignment. But “he couldn’t get the work done,” she says, because every noise was so distracting and bothersome.\nWhen he was in second grade, someone pulled a fire alarm at school. For the next three months, Matthew refused to take his hands off of his ears. It’s one example of the exaggerated reactions that people with autism may have to events that upset them.\nThe Shumakers learned that Matthew couldn’t filter out sounds and sights like they could. So they adapted. When they drove in the car, for example, they learned that it was much easier for Matthew if no one talked. Piling on extra noise to the sounds and motion he was already sensing was simply too much for him.\nBecause people with autism can react to events and stimuli differently, many myths have cropped up about the disease, Laura Shumaker says. Many people, for instance, believe that children with autism are in their own world and don’t like to be touched. “Nothing could be farther from the truth for my son,” she says. “He’s hungry for friendship.”\nMany possible causes\nScientists don’t know the exact cause of autism. But they are finding more and more clues to suggest that the varied brain disorders seen across the autism spectrum are due in part to the genes that babies inherit from their parents. A gene is a segment of DNA that codes for —or holds instructions for —producing a protein. Those proteins ultimately influence what an organism looks like and how it behaves.\nIf a child has autism, researchers know that the child’s siblings are more likely to have it too. So far, scientists have found hundreds of genes that might be linked to autism. But genes alone can’t fully explain why some kids develop autism and others don’t.\n“The reality is that we don’t fully understand most of the causes of autism spectrum disorders,” says Warren at Vanderbilt University. “The reality is that there are likely many, many causes.”\nSome researchers think that differences in a baby’s immune system —how well it fights off infections —might play a role. Angkustsiri says scientists are also looking at what happens during pregnancy, including the activity of a mother’s own immune system. Researchers hope to find out whether any changes before birth boost a baby’s risk of developing an autism spectrum disorder.\nNew research has added to the evidence that many changes in the brain may be linked to genetic problems. In one small study published March 27 in The New England Journal of Medicine, scientists looked at brain tissue from 11 autistic kids who had died. They focused on a type of tissue that normally is organized like a cake — with six different layers. In all but one of the youngsters, the scientists found patches here and there where some of the brain’s layers were totally disorganized. What’s more, the affected patches lacked specific genes that should have been there.\nBecause they weren’t, cells in the affected regions lacked the instructions needed to assemble properly. Among 11 kids without autism, the researchers saw similarly disorganized brain tissue in only one child.\nThe brain tissue that became disorganized is important for learning and memory, some studies suggest. It also has been linked to some of autism’s symptoms.\nSuch research suggests that autism might begin before birth, when this part of the brain begins organizing itself. The new study also supports the idea that treating autism early in life may rewire brain connections, allowing the circuitry to form detours around damaged areas.\nOne thing that doesn’t appear to trigger autism: childhood vaccines. Yet for years, many parents have heard rumors that these shots could be a cause.\nIn fact, there is no link between autism and vaccines, says Peter Jay Hotez. He’s a pediatrician and vaccine expert at Baylor College of Medicine in Houston. (He also directs the Sabin Vaccine Institute at Texas Children’s Hospital. The institute makes vaccines for people who live in poor countries.) “There’s no evidence that any of these vaccines are linked with autism,” Hotez argues.\nAutism leads to so many complicated changes in the brain that it couldn’t be the result of giving kids a vaccine, he says. Plus, vaccines save lives. So parents who don’t let their kids get vaccines could be putting them at high risk for life-threatening diseases, such as measles and whooping cough.\nFor Hotez, understanding the real cause of autism is personal. He’s not only a pediatrician and a vaccine expert, but also the father of a 21-year-old with autism. Like the Shumakers, Hotez and his wife knew their daughter Rachel was different when she was very young.\nLooking for early symptoms\nIn 2013, the U.S. government launched a big new project called BRAIN (short for Brain Research through Advancing Innovative Neurotechnologies). Researchers want to map out how the 86 billion neurons —the nerve cells that send signals zipping through the brain —interact. The project will likely take 10 years to finish. But scientists say it could help them understand how the “wiring” of neurons differs in people with autism.\nThat research could be important because autism can be hard to spot at first. “Typically, we can’t tell the difference at birth between a child who’s going to develop normally and a child who’s going to develop autism,” Angkustsiri says. “Some of the symptoms take time to develop.” Most kids aren’t diagnosed until they are 2 or 3. Many researchers, though, are trying to detect the disorder much earlier.\nOne group at the MIND Institute, in fact, has come up with a way to help tell whether the younger brother or sister of a child with autism also might have the disability. The test can spot signs in children as young as 6 months old. If a parent or other adult stands behind the child and repeatedly calls his or her name, does the child respond? If not, that could be a warning sign.\nResearchers at Emory University in Atlanta have found that children who would later be diagnosed with autism spent less and less time, as babies, looking people in the eye. That behavior seemed to start as early as 2 to 6 months old. Babies who didn’t go on to develop autism, by contrast, paid more and more attention to a person’s eyes as they got older.\nThis difference is so subtle that people tend to miss it. But the Emory researchers used an experimental device to help them track each baby’s eye movements. It helped them spot this behavior very early. If this technology proves itself in bigger tests, it could give doctors another tool to help them diagnose the disorder.\nOther studies are likewise finding early warning signs in the younger brothers and sisters of kids with autism. Some signs might point to their risk of developing autism too. Others might simply signal they may face some risk of other problems affecting brain development or behavior.\nOne recent study, for instance, suggests that doctors can begin to detect symptoms in children as young as 12 months old by looking at how they communicate, interact and move. That’s the conclusion of Sally Ozonoff of the MIND Institute and her colleagues.\nThey studied nearly 300 infants with older siblings who have autism. On March 6 in the Journal of the American Academy of Child and Adolescent Psychiatry, the researchers reported that 17 percent of the younger brothers and sisters eventually developed autism too. Another 28 percent showed early delays in their language, thinking skills and movement —and especially in making eye contact, pointing and communicating with unfamiliar people.\nThose delays would easily have been missed if doctors had not been looking carefully, the study found.\nLaura Shumaker finds such data “really exciting.” If a parent gets such early clues about a second child, she says, “You can say, ‘OK, I see something similar to my other child with autism, and I am going to get some intervention going right away.’”\nAnd that early help, she says, can make a world of difference.\nautism spectrum disorders A set of developmental disorders that interfere with how certain parts of the brain develop. Affected regions of the brain control how people behave, interact and communicate with others and the world around them. Autism disorders can range from being very mild to being very severe. And even a fairly mild form can limit an individual’s ability to interact socially or communicate effectively.\ngene A segment of DNA that contains the instructions for making a protein. Those proteins govern the behavior of a cell — or large groups of cells. Offspring inherit genes from their parents. Genes influence how an organism looks and behaves.\nimmune system The collection of cells and their responses that help the body fight off infections.\nnerve cell orneuron Any of the impulse-conducting cells that make up the brain, spinal cord and nervous system. These specialized cells send information to other neurons in the form of electrical signals.\nvaccine A biological mixture that resembles a disease-causing agent, given to help create immunity to a particular disease.\nWord Find (click here to enlarge for printing)\nS. Ornes.“Lip-reading babies.” Science News for Students. Feb. 1, 2012.\nS. Gaidos. “Giving the brain a hand.” Science News for Students. Sept. 10, 2010.\nL. Sanders. “One problem, many paths.” Science News. July 29, 2011.\nKids' Quest: Autism, a website of the U.S. Centers for Disease Control and Prevention.\nTeachers' questions: Autism unlocked""]"	['<urn:uuid:d4bc17f3-b2cb-402d-b3c2-261072cdc20c>']	open-ended	with-premise	concise-and-natural	similar-to-document	single-doc	novice	2025-05-12T21:31:42.699391	5	89	2379
24	what scientific objects can astronomers study with muse telescope high resolution imaging	With MUSE's new high-resolution imaging capability, astronomers can study various fascinating objects in unprecedented detail. These include supermassive black holes at the centers of distant galaxies, jets from young stars, globular clusters, supernovae, and planets and their satellites in the Solar System.	"['The MUSE (Multi Unit Spectroscopic Explorer - https:/\nThe MUSE Wide Field Mode coupled to GALACSI in ground-layer mode corrects for the effects of atmospheric turbulence up to one kilometre above the telescope over a comparatively wide field of view. But the new Narrow Field Mode using laser tomography corrects for almost all of the atmospheric turbulence above the telescope to create much sharper images, but over a smaller region of the sky .\nWith this new capability, the 8-metre UT4 reaches the theoretical limit of image sharpness and is no longer limited by atmospheric blur. This is extremely difficult to attain in the visible and gives images comparable in sharpness to those from the NASA/ESA Hubble Space Telescope. It will enable astronomers to study in unprecedented detail fascinating objects such as supermassive black holes at the centres of distant galaxies, jets from young stars, globular clusters, supernovae, planets and their satellites in the Solar System and much more.\nAdaptive optics is a technique to compensate for the blurring effect of the Earth\'s atmosphere, also known as astronomical seeing, which is a big problem faced by all ground-based telescopes. The same turbulence in the atmosphere that causes stars to twinkle to the naked eye results in blurred images of the Universe for large telescopes. Light from stars and galaxies becomes distorted as it passes through our atmosphere, and astronomers must use clever technology to improve image quality artificially.\nTo achieve this four brilliant lasers are fixed to UT4 that project columns of intense orange light 30 centimetres in diameter into the sky, stimulating sodium atoms high in the atmosphere and creating artificial Laser Guide Stars. Adaptive optics systems use the light from these ""stars"" to determine the turbulence in the atmosphere and calculate corrections one thousand times per second, commanding the thin, deformable secondary mirror of UT4 to constantly alter its shape, correcting for the distorted light.\nMUSE is not the only instrument to benefit from the Adaptive Optics Facility. Another adaptive optics system, GRAAL, is already in use with the infrared camera HAWK-I. This will be followed in a few years by the powerful new instrument ERIS. Together these major developments in adaptive optics are enhancing the already powerful fleet of ESO telescopes, bringing the Universe into focus.\nThis new mode also constitutes a major step forward for the ESO\'s Extremely Large Telescope, which will need Laser Tomography to reach its science goals. These results on UT4 with the AOF will help to bring ELT\'s engineers and scientists closer to implementing similar adaptive optics technology on the 39-metre giant.\n MUSE and GALACSI in Wide-Field Mode already provides a correction over a 1.0-arcminute-wide field of view, with pixels 0.2 by 0.2 arcseconds in size. This new Narrow-Field Mode from GALACSI covers a much smaller 7.5-arcsecond field of view, but with much smaller pixels just 0.025 by 0.025 arcseconds to fully exploit the exquisite resolution.\n Atmospheric turbulence varies with altitude; some layers cause more degradation to the light beam from stars than others. The complex adaptive optics technique of Laser Tomography aims to correct mainly the turbulence of these atmospheric layers. A set of pre-defined layers are selected for the MUSE/GALACSI Narrow Field Mode at 0 km (ground layer; always an important contributor), 3, 9 and 14 km altitude. The correction algorithm is then optimised for these layers to enable astronomers to reach an image quality almost as good as with a natural guide star and matching the theoretical limit of the telescope.\nESO is the foremost intergovernmental astronomy organisation in Europe and the world\'s most productive ground-based astronomical observatory by far. It has 15 Member States: Austria, Belgium, the Czech Republic, Denmark, France, Finland, Germany, Italy, the Netherlands, Poland, Portugal, Spain, Sweden, Switzerland and the United Kingdom, along with the host state of Chile and with Australia as a strategic partner. ESO carries out an ambitious programme focused on the design, construction and operation of powerful ground-based observing facilities enabling astronomers to make important scientific discoveries. ESO also plays a leading role in promoting and organising cooperation in astronomical research. ESO operates three unique world-class observing sites in Chile: La Silla, Paranal and Chajnantor. At Paranal, ESO operates the Very Large Telescope and its world-leading Very Large Telescope Interferometer as well as two survey telescopes, VISTA working in the infrared and the visible-light VLT Survey Telescope. ESO is also a major partner in two facilities on Chajnantor, APEX and ALMA, the largest astronomical project in existence. And on Cerro Armazones, close to Paranal, ESO is building the 39-metre Extremely Large Telescope, the ELT, which will become ""the world\'s biggest eye on the sky"".\n* MUSE first light - https:/\n* First light of MUSE AOF Wide-Field Mode - http://www.\nESO MUSE and GALACSI Project Scientist\nGarching bei München, Germany\nTel: +49 89 3200 6579\nMUSE Principal Investigator / Lyon Centre for Astrophysics Research (CRAL)France\nCell: +33 6 08 09 14 27\nESO Public Information Officer\nGarching bei München, Germany\nTel: +49 89 3200 6655\nCell: +49 151 1537 3591']"	['<urn:uuid:6db74250-a55d-4ea8-bc45-4f2985005fd3>']	open-ended	direct	long-search-query	similar-to-document	single-doc	novice	2025-05-12T21:31:42.699391	12	42	839
25	protected areas mountain conservation development challenges	Mountain protected areas face significant conservation challenges due to development pressures. Kon Ka Kinh National Park, which contains 33,565 ha of natural forest, must balance conservation with local resource use and hydroelectric development. Similarly, snow leopard landscapes across Asia face unprecedented pressures from mining, road construction, hydroelectric projects, and intensified livestock grazing. These development projects typically use economic cost-benefit analysis, but often fail to account for the value of ecosystem services, making it difficult to achieve balance between economic development and ecological sustainability.	['Kon Ka Kinh National Park is located in Kon Pne, Dakrong and Krong communes, K’Bang district, Ha Dong commune, Dak Doa district, and Ayun commune, Mang Yang district, Gia Lai province. The national park is situated on the Kon Tum plateau in the central Annamite mountains. To the north of the national park, elevations gradually increase towards Mount Ngoc Linh, the highest point in the central Annamites. To the south and west, the topography is flatter, and altitudes are below 500 m. Altitudes within the national park range from 570 m in the Ba river valley, to 1,748 m at the summit of Mount Kon Ka Kinh.\nThe national park straddles two catchments: streams originating in the east of the national park feed the Ba river, which flows east, into the South China Sea; whilst the west of the national park lies within the catchment of the Mekong River. Due to the steep topography, rivers and streams originating in the national park are often short, narrow and fast flowing, with many waterfalls.\nKon Ka Kinh National Park contains 33,565 ha of natural forest, equivalent to 80% of the total area of the national park. The national park supports a range of montane habitat types. Of particular importance are 2,000 ha of mixed coniferous and broadleaf forest containing Fokienia hodginsii.\nKon Ka Kinh National Park forms part of a contiguous landscape of natural habitats in north-eastern Gia Lai province, and adjacent Binh Dinh and Kon Tum provinces, which supports some of the most intact faunal and floral communities in the central Annamites. Notably, the national park supports a number of globally threatened mammal species, including Grey-shanked Douc Pygathrix cinerea, Yellow-cheeked Crested Gibbon Hylobates gabriellae and Tiger Panthera tigris.\nKon Ka Kinh is situated within the Kon Tum Plateau Endemic Bird Area, and supports six restricted-range bird species. One of these species, the globally vulnerable Chestnut-eared Laughingthrush Garrulax konkakinhensis, which is endemic to the Kon Tum plateau, was discovered at Kon Ka Kinh in 1999. For these reasons, Kon Ka Kinh qualifies as an Important Bird Area.\nKon Ka Kinh National Park is also a globally important site for the conservation of amphibian diversity. Kon Ka Kinh supports a number of amphibian species endemic to the Annamite mountains, including four species assessed as globally threatened during the Global Amphibian Assessment: Leptobrachium banae, L. xanthospilum, Rana attigua and Rhacophorus baliogaster.\nKon Ka Kinh National Park has an important role in upstream watershed protection for several large rivers that provide water for irrigation and domestic use for a number of districts in Gia Lai and Kon Tum provinces. Additionally, the west of the national park forms part of the catchment of the Yaly hydroelectric power station.\nThe national park is of importance to local people as a source of natural resources, such as firewood honey and rattans. Also, the national park supports at least 110 plant species with known medicinal uses. Use of plants in traditional medicine is widespread amongst local people, although the commercial potential of these species remains largely unrealised.\nThe national park contains scenic landscapes and other natural features of interest to visitors.', 'Ecosystem services and human well-being\nApproaching snow leopard conservation through the lens of ecosystem service use can contribute significantly towards the protection of snow leopard habitats, or towards finding options that can better balance the need for economic development with ecological sustainability.\nEcosystem services and human well-being\nThe snow leopard, an endangered species, is a flagship for conservation in Asia’s highlands. Today, snow leopard landscapes across Asia face unprecedented developmental pressures, be it mining for minerals, building of roads and development of hydro-electricity, and the intensification of livestock grazing in response to global demands. As choices are made between various development projects and land uses, it is critical that more information is available to local communities, politicians, and bureaucrats about the value of the ecosystems being altered. Currently, there is little formal understanding or quantification of the value of ecosystem services derived from these mountains.\nEcosystem services is defined as the benefits people derive from nature. Given the current socio-economic trends in these regions, approaching snow leopard conservation through the lens of ecosystem service use can contribute significantly towards the protection of snow leopard habitats, or towards finding options that can better balance the need for economic development with ecological sustainability.\nFor my project I propose to:\n1) Identify and economically value the ecosystem services from snow leopard habitats.\n2) Understand the distribution of ecosystem service benefits within the community, across a rural-urban gradient.\n3) Identify the factors that drive socio-cultural valuation of ecosystem services.\n4) Identify areas of overlap between biodiversity and ecosystem services.\n- Report2017Valuation of Ecosystem Services in Snow Leopard Landscapes of AsiaMurali, R., Lkhagvajav, P., Saeed, U., Kizi, V.A., Zhumbai-Uulu, K., Nawaz, M.A., Bhatnagar, Y.V., Sharma, K., Mishra, C. 2017. Valuation of ecosystem services in snow leopard landscapes of Asia. Snow Leopard Trust, Nature Conservation Foundation, Snow Leopard Conservation Foundation, Snow Leopard Foundation Kyrgyzstan, and Snow Leopard Foundation Pakistan. Report Submitted to the Global Environment Facility (GEF) funded United Nations Development Program (UNDP) project on Transboundary Cooperation for Snow Leopard and Ecosystem Conservation.\n- Journal Article2017The value of ecosystem services in the high altitude Spiti Valley, Indian Trans-Himalayahttps://doi.org/10.1016/j.ecoser.2017.10.018Download\nPDF, 645 KB\nThe high mountain ranges of South and Central Asia are increasingly being exposed to large-scale development projects. These areas are home to traditional pastoralist communities and internationally important biodiversity including the endangered snow leopard Panthera uncia. Development projects rely on economic cost-benefit analysis, but the ecosystem services in the high Himalayas are poorly understood and are rarely accounted for. As a first step to fill this gap, we identified the main ecosystem services used by local people in the Trans-Himalayan Spiti Valley (7591 km2), a region important for conservation of snow leopards and high mountain biodiversity, and undertook an economic valuation. Stakeholders identified a range of services, though these were dominated by provisioning services identified by 90% of respondents. Only 5.4% of the respondents recognised regulatory services and 4.8% recognised cultural services. The mean economic value of provisioning services was estimated at US$ 3622 ± 149 HH−1 yr−1, which was 3.8 times higher than the average annual household income. Our results underscore the need to account for ecosystem services in the cost-benefit analyses of large-scale development projects in addition to assessments of their environmental and social impact.']	['<urn:uuid:e44a50d6-7684-4efd-9e06-90b15fd70712>', '<urn:uuid:11ef78bd-2699-4650-9eda-d1dc308e1680>']	open-ended	direct	short-search-query	similar-to-document	three-doc	novice	2025-05-12T21:31:42.699391	6	83	1064
26	What's the best way to improve sky in landscape photos?	There are multiple effective techniques to enhance the sky in landscape photos. The most common approach is using the Graduated Filter Tool in Lightroom, applying it across the top portion of your photo to adjust only the sky while leaving the rest unedited. This is particularly useful when dealing with overexposed skies. You can adjust exposure, brightness, contrast, and other settings, and even add color tints to emphasize warm golden tones in sunset shots. In black and white photography, you can further enhance sky effects using filters - particularly a circular polarizer which adds contrast to skies. Additionally, when shooting in black and white, you can alter the tonal range of individual colors later, giving you greater control over how the sky appears in the final image.	['The Graduated Filter Tool in Lightroom is a very versatile one, and one that everyone should know how to use. Its purpose is to apply a range of effects on a certain section of your photo, in a gradual manner, as the name indicates. The effects of the graduated filter take place in a linear fashion, and are strongest where the filter begins, and then gradually taper off.\nHow to use the Graduated Filter Tool\nThe Graduated Filter Tool is found in the Develop module, just below the histogram. You can also switch to the Graduated Filter Tool by pressing “M” on your keyboard.\nOnce you have selected the tool, you can apply the filter to your photo by starting at one edge of your photo, or wherever you want the filter to begin, and dragging it across your photo. To lock your filter perfectly vertical or horizontal, hold down “Shift” while you drag. When you have covered the area you want the filter to be applied to, release your mouse.\nThere is now a “pin” that shows where the centre of your filter is. If you want to shift the position of your filter, or change any settings at any point, simply select this pin. If you want to change the width of your filter, you can drag either of the outside lines on your filter. Changing the width of your filter will also change the hardness or softness of the graduation of your filter.\nKeep track of where your filter is being applied by selecting “Show Selected Mask Overlay” below your photo. This will show your graduated filter in red! (You can change the colour of the overlay if the default red blends in with your photo too much!\nAlthough the default is to apply the filter in a straight line, this does not always have to be the case! If you want to remove the filter from certain areas, click the brush at the top of the panel while your filter is active, select “Erase” in the brush panel, then with the mask visible, brush away the areas you don’t want your filter to be applied. You can also apply the filter to extra areas of you image by using the brush tool, but selecting “A” or “B” instead of “Erase”!\nAs you play with the hardness or softness of your filter, keep in mind that light will naturally scatter, which means there will not necessarily be a hard line. Let your effects transition over the ground/foreground a little bit for a softer line and more realistic look, rather than stopping the effect harshly at the horizon line.\nAfter you have applied your filter, you can begin to change the settings. The Graduated Filter Tool offers the same adjustment tools as the Lightroom Brush Tool: Exposure, Brightness, Contrast, Saturation, Clarity, Sharpness, and Add Colour.\nAs you adjust these settings, your changes will only be applied to the section that the Graduated Filter is applied to!\nKeep adjusting settings until you have achieved the look you want! You can also apply multiple Graduated Filters to the same photo, if needed. For example, you may want to darken the sky, but lighten the foreground. In this case apply one filter to the top portion of your photo and adjust that one to get the sky effect you want, and then apply one to the bottom portion of your photo and adjust the settings to get the foreground effect you want!\nIf you find that you are applying the same settings to photos over and over, you can save your Graduated Filter settings as presets, so you can easily access and apply them. To do this, go down to the “Filters” drop-down menu in the bottom right corner of the screen.\nFrom this menu, select “Save Current Settings as New Presets.” Simply your preset and click “Create”, and you are all set!\nWhat is the Graduated Filter Tool used for?\nThe most common application is to enhance the sky in landscape photos. In this case, you would apply a graduated filter across the top portion of your photo, which would then allow you to make adjustments to only the sky portions, leaving the rest unedited. This is helpful in situations when you may want a different exposure or different settings across various sections of your photo!\nJust like you may want to darken the sky because it is overexposed, you may want to lighten the foreground because it is underexposed! To do this, simply apply the graduated filter to the bottom of your photo and make the adjustments in the same way you would for the sky. You can also darken the foreground, which can direct the viewer’s gaze to the horizon, which is generally the focal point of a landscape photo.\nMaking the Edges of Your Image Darker\nMaking the edges of your image darker may help to highlight your subject and draw your viewer’s eyes in towards your focal point. You can achieve this effect by using multiple graduated filters along the edges of your photo. Depending on the photo, you may choose to apply the filters vertically, horizontally, or both in order to outline your subject!\nAdding a Colour Wash\nThe colour setting at the bottom of the graduated filter panel will allow you to add a tint to your filter. This colour will then be laid over your photo along with whatever other settings you choose to apply to your filter. One way you can use this is on photos that were shot at golden hour, as it will help you emphasize the warm golden tones that are already present in the photo. If you apply a graduated filter with a golden tint vertically across your photo, for example, you may be able to replicate the golden glow of the sunset, and create a really beautiful effect.\nOf course, these are not the only ways you can use the Graduated Filter Tool in Lightroom! Once you have mastered how to apply the filter and know exactly what applying one will do to a photo, you can apply them to all different types of photos and get really creative with how you use them.', 'Black & white photography enables a more personal interpretation of the landscape. Lee Acaster, winner of multiple photo awards, explains\nWe all see the world around us in colour, but as photographers we have the opportunity to look beyond this and view it as form, tone and texture. Shooting in black & white opens up a whole new landscape hidden beneath the surface.\nColour landscape photography relies very heavily on light and weather conditions, and we are almost bound by the scene that nature presents to us. However, black and white landscape photography offers the opportunity to create a more personal interpretation.\nThere is much more to creating a successful monochrome image than simply looking back through the hard drive to salvage a shot by converting it to greyscale (although this can bring about desirable results, particularly with scenes that were too contrasty to be successful in colour). To make the most of shooting in black & white, you’ll need to develop a whole new mindset before you even press the shutter. Whereas colour photography is dependent on the relationship between the various hues in a scene, when this is taken out of the equation other factors take on much greater prominence.\nThe tonal values in the image become more important, and although it’s not always easy to visualise these, with a little practice you can become much more adept at picturing the landscape as areas of light and shadow.\nShooting digitally gives us an added advantage in this regard. By switching your camera’s picture style to monochrome, you get an instant preview of how the image could look in black & white, although it is worth noting that a raw file will also retain the colour information and give you much greater control in post-production.\nThis monochrome preview shouldn’t be seen as the final image, though, as one of the most liberating aspects of black and white landscape photography is that you have the ability to alter the tonal range of individual colours later, as well as having much more control over the highlights and shadows. This allows you to recreate the image you are visualising out in the field and exercise a degree of creative influence over it.\nShape and form\nTo create striking landscape images, it pays to look for strong lines and simple compositions. Larger areas of fine detail can easily lose prominence in the overall image in black & white, so it becomes much more about the shapes in the landscape. Looking at the scene as a whole can help you to create a stunning and effective composition, and building your image around one or two key focal points will help with this. Texture also plays an important role; with no colour to focus on, it adds depth and substance to a scene. As with colour, it can be particularly useful to have interesting foreground detail when shooting wideangle landscapes, but with black & white, contrast is the important element to consider, be it in ripples of sand with deep shadows, or backlit leaves or grasses.\nCreating a route through the image is a great technique to engage the viewer, and black & white is the perfect medium for doing this. Light and shadow can become key compositional elements. The eye is naturally drawn to the contrast when very dark and very light areas meet in a black & white image, so use these to create focal points and lead-in lines. A shadow falling across a field or hill, or a patch of sunlight on the sea, can be just as important an element within a shot as physical objects such as a fence or building.\nBy using light and shade in this way, the viewer is given a ‘starting point’. Typically, this is introduced around the edges of the frame, and then the eye is led into the key focal points of the image. You can enhance this effect even further in processing, which, whether in the darkroom or digitally, has always played an important role in creating great mono images. Gradient and Radial filters provide a simple and effective method of concentrating the viewer’s attention on the important parts of the image.\nCreating a vignette effect by darkening the edges of the frame, and increasing the contrast around the key components of the shot, will naturally lead the viewer’s eye towards these lighter areas and make it linger there. The same approach can be applied in reverse to a high-key image, where the darker areas become the focal point.\nIt’s this ability to control the overall tonal range of the image that makes digital black & white photography so rewarding. This process and technique is so much more than just simply converting an image to greyscale.\nCanon EOS 5D Mark II\nI had my Canon EOS 5D Mark II converted to infrared (using a 720nm filter) purely for mono work. I love the extra contrast and slightly surreal effects I can achieve with it.\nSony Alpha 7R\nI’ve recently replaced my main kit with a Sony Alpha 7R mirrorless system. It’s lightweight, and the dynamic range and resolution are superb.\nZeiss Distagon T* 18mm f/3.5\nI use a Zeiss Distagon T* 18mm f/3.5 lens more than any other. It’s robust and beautifully built, and performs well in colour and on my infrared body.\nSonnar T* FE 55mm f/1.8 ZA\nLately I’ve found myself increasingly using a 55mm lens, and the Sonnar T* FE 55mm f/1.8 ZA is light, fast and pin-sharp.\nI like a very stable tripod, and they don’t come much sturdier than a Gitzo. A geared head is a must for me; my favourite is the Manfrotto XPRO Geared 3 Way Head.\nLee Filters system\nI still use filters for my black & white photography, in particular a Lee Filters circular polariser, which is useful for adding contrast to skies.\nAlternative technique: Infrared\nShooting in infrared opens up a whole new dimension to black & white landscape photography. When using an infrared filter or IR-converted camera, most of the visible light is blocked, allowing only the infrared spectrum to reach the sensor. In practice, this can transform images and add a surreal, dreamlike quality to them.\nFoliage becomes lighter, blue skies become much darker and atmospheric haze is reduced. This is often used to create the ghostly looking infrared shots most of us are familiar with, but it can also be used in a much subtler way to enhance contrast and alter the expected dynamics of light and shade within an image.\nOn a budget\nConverting a camera to infrared can be quite costly, and as it’s usually irreversible, it requires a second camera body. The advantages of a converted body are that you can preview the effect in live view, and the camera will operate at its usual shutter speeds. A much cheaper alternative is to use a screw-in filter such as a Hoya R72 Infrared.\nUnfortunately, you will need to compose your picture before attaching the almost opaque filter to the lens, and shutter speeds are significantly increased to one or two minutes. This can prove problematic when shooting subjects that are prone to movement, such as foliage, but it’s also a good way to experiment with the technique.\n5 steps to processing in B&W\nProcessing your black & white images well is vital to creating an impressive final image; it is also where you really get to add your creative input. Simply removing the colour and converting the image to greyscale will often lead to disappointing results, and can leave you with a rather flat and lifeless photograph.\nIncreasing the tonal range by adjusting contrast, highlights and shadows will add more punch to your images, and the use of dodging and burning alters the dynamics of a shot. A quick and easy way to do this in Lightroom is by adopting Graduated and Radial filters, which you can use to concentrate attention onto the parts of the image that are most important to your composition. The soft-feathered edges of these filters allow you to increase contrast, sharpness, light and shadow within specific areas in a subtle manner.\n1. Original raw file\nWhen shooting in raw, even if your camera was set to monochrome the file includes all colour information. Although the source file might look quite lifeless, this gives you better control over your final image and the ability to adjust tones more accurately.\n2. Convert to B&W\nThe first step is to switch the image to black & white in the Develop sidebar. Adjusting the Contrast, Highlights, Shadows and Clarity will start to lift the image, giving you a wider tonal range. You can also alter the tone of individual colours using the Black and White Mix sliders.\n3. Graduated filters\nWith the Graduated tool, start darkening or lightening specific areas of the image. Using several of these and coming in from different directions enables you to create a vignette effect, to draw the viewer’s eye to the important areas of the image. Hence, you can adjust the exposure and contrast.\n4. Adding Radial filters\nWhereas Graduated filters give you a sweeping effect, Radial filters are perfect for making adjustments to specific areas. They can be used to accentuate highlights and shadows, detail and textures, and are a good way to add depth and interest to the important areas of the photograph.\n5. Final image\nAfter some final tonal adjustments and finishing touches, such as sharpening, you can create stimulating and original images that will enable you to see the landscape in a whole new light. It’s well worth printing your best work – the tones and textures of mono can look wonderful in a nice frame.\nAbout Lee Acaster\nBased in East Anglia, Lee is an amateur photographer who has a love of landscapes. He is widely published and has won numerous national awards, including AP’s Amateur Photographer of the Year 2015 competition.']	['<urn:uuid:19824e64-ec68-488c-b3ca-ef91a936ca81>', '<urn:uuid:d056b63a-5abf-4b40-b2dc-82a2a311d288>']	open-ended	direct	concise-and-natural	distant-from-document	three-doc	novice	2025-05-12T21:31:42.699391	10	127	2694
27	virtual reality building design benefits construction worker safety considerations	Virtual reality (VR) technology in construction offers significant benefits for both design visualization and safety planning. VR enables thorough quality assurance by allowing detailed inspection of mechanical systems and precise alignment of components before installation, preventing potential hazardous rework scenarios. Teams can use VR to virtually walk through spaces, examine installations from all angles, and identify potential safety issues before actual construction begins. This technology integration supports essential safety practices, as the construction industry faces significant risks including falls from height (49% of fatalities), being trapped by collapsing structures (14% of fatalities), and being struck by objects (10% of fatalities). By identifying these hazards virtually first, appropriate safety measures can be implemented during actual construction.	['Large construction companies aren’t generally known for being leaders in adopting new technologies. But Gilbane Building Company is different. With an aggressive deadline looming to complete a new building for the Wentworth Institute of Technology in Boston, Gilbane turned to prefabricated construction and virtual reality to get the job done. As an added bonus, this building project also serves as a living learning lab for current Wentworth students studying construction management.\nJohn Myers, Director of Virtual design and Construction, Gilbane Building Company: Industrywide, construction is one of those industries that’s always hesitant to want to be the first one to adopt any new technology.\nJack Duggan, Chair of Department of Civil Engineering, Wentworth Institute of Technology: Well, the new building is our first standalone academic building in a generation.\nMyers: It’s a four-story lab and science academic building for Wentworth Institute of Technology. Wentworth is the local engineering, architecture, polytech institute in downtown Boston.\nDuggan: The faculty here in the program have been intimately involved with the design of the space. This building gives us a chance to design space for not only what we anticipate our current graduates will need but also a generation of graduates.\nMyers: We gave a kind of holistic vision to Wentworth. We said, “Look, we’re going to make all of our subcontractors draw in [Autodesk] Revit.” Drawing in Revit means that they’re drawing in the same software that the designers are so we can exchange models back and forth. Knowing that, though, we knew that we were going to be able to do a very, very intensive VR turnover to the owner at various life-cycle stages.\nWe used VR to sit with the higher-up dignitaries and you get their buy-in for the visuals of the building. The building is going to be this tall. Well, what does “this tall” actually mean? It’s going to be this tall, and then we can walk you down the street and show you really, truly what the height is.\nKevin Cooke, Senior Project Executive, Gilbane Building Company: This project is about 10 months into construction. We drove our first pile last August. We have approximately five months left to complete. As accelerated as it is, [the project] begged for the highest level of prefabrication as well as design technology.\nMyers: Prefab is everything to us. In order to do prefab properly, you have to have a 3D model because you have to give your contractors the operational surety and the dimensional surety that what they’re going to prefabricate is not going to be disassembled on the job site and put in stick wide. They have to be able to say, “Yes, okay, I see it. This is great. This is exactly what I need,” so all of those tools that we’re using, including VR, are used to get the buy-in from not just the owner, not just the architect, but also our subs who are going to be taking an enormous amount of risk building assemblies ahead of time in a warehouse, putting them on a truck, and then bringing them here.\nMichael Harris, Project Manager, Gilbane Building Company: On this project, we had two major prefab components, the first being the prefabricated penthouse. That’s where all our air-handling equipment and how we cool the building and exchange air in the building, exhaust the building. Then all of the plumbing, piping, and mechanical piping, all those came in prefab racks, about 20 feet long.\nThere were a couple unique challenges with this building. One, we’re in a very tight urban site, so some of the sizing that we would benefit in the prefab, we had to determine what could fit on trucks, fit in the neighborhood, and then get in the building—because we have limited access, both on the front and the back of the building.\nPaul Fitzgerald, Plumbing Foreman, TG Gallagher: It’s probably my guys up there. They probably know that I’m giving an interview, so they’re making some noise. In a lab situation, it probably quadruples the amount of piping that you have to put in on the job: Water, drainage, gases, because of that increased amount of work, it creates more prefabrication that has to be done.\nWhat’s changed with the business is the pace of the job, and prefab kind of matches that pace. I would say in terms of man power, it probably allows you to do the job in a third of the time that it would take compared to normal construction.\nCooke: The penthouse up at the top of this building was actually fabricated offsite in nine sections and brought and put together in a weekend. Something like that in the old days would have taken over a month to assemble.\nMyers: So because this is such a high-performing building, we actually have to ensure that if we’re going to prefabricate all of this mechanical ahead of time, that when we bring in all the casework, everything’s going to line up perfectly—because we’re not going to get another shot at this. The schedule doesn’t have the time in it to get another shot at it. We had to throw those extra layers of quality assurance, quality control, and so we used all the tools available, including VR. It’s one thing to fly through the building in [Autodesk] Navisworks and check that everything is above the ceiling, not below the ceiling. It’s another thing to actually go through like you’re a person and turn your head around and look at things and use your controllers to touch things and get into the wall where the fume hood is.\nDuggan: I think oftentimes when you’re teaching, you look at what resources are available to you, and you adapt to that. Our curriculum depends and relies on co-op. It depends and relies on students getting some fundamental knowledge and then going out and getting some real-life world experience.\nShannon Sturtz, Construction Management Student, Wentworth Institute of Technology: The learning living lab is an opportunity for Wentworth students to come out of the classroom and see what they’ve been learning in the classroom in a real-life situation. I’ll be sitting in a class and then my professor will be like, “Oh, look outside. They’re putting piles in right now.” We’ll turn around, and we’ll see exactly what we were just talking about on the board, and it will be right on the job site. I walk past it every day.\nDuggan: When we see this building go up, every element of our curriculum is on display.\nObie Rankin, Project Engineer, Gilbane Building Company: I live, breathe construction. I graduated from Wentworth in 2014. Now, I’m on the campus working on their new academic building. This project in itself has so many rewards to it: having the living learning environment with all the students and having them come through with the program, having the interns that we have on this project go through multiple rotations, teaching them different things.\nDuggan: This is a profession; this is an industry. You’re a part of it and hopefully contributing to that; you’re part of a lasting legacy. This building will be around for 100 years, and we’ve all had some part in the making of it.\nMyers: My greatest reward for my work is being able to actually partially change the industry that I work in. We are helping drive the industry to new heights, and not everybody in every industry gets a chance to do that. It’s just every day is different, and it’s a great experience.', 'Health and safety in the construction industry: what are the major risks?\nThe construction industry has the second-highest rate of fatalities across all industries in the UK according to the Health and Safety Executive (HSE) website statistics for 2019. Sadly, 30 people lost their lives during 2019 due to incidents on the construction site. With another 54,000 non-fatal injuries reported across the year, there is clearly a way to go when managing health and safety in construction to prevent further deaths and long-term conditions.\nThe construction industry is the European Union’s (EU) largest industry employer. A workforce of 18 million contributes to 9% of the EU’s gross domestic profit (GDP). This large and diverse workforce provides an invaluable service to society and the prosperity of each nation. So, why are there still so many incidents on construction sites and how can health and safety regulations work to protect our workers?\nTargeting the big five\nFive main categories of incidents are most common on construction sites:\n- Falls from height – which account for 49% of fatalities and 18% of non-fatal injuries (a number that is 10% higher than the statistics for all other industries)\n- Slips, trips or falls on the same level – accounting for 25% of non-fatal injuries\n- Being trapped by something collapsing or overturning – responsible for 14% of fatalities\n- Being struck by a moving, falling or flying object – the cause of 10% of fatalities and 12% of non-fatal injuries\n- Injured while handling, lifting or carrying – making up 20% of non-fatal injuries\nBesides these main issues, construction workers are at an increased risk from other serious and life-changing conditions. Conditions such as chronic obstructive pulmonary disease (COPD), occupational deafness, hand-arm vibration syndrome (HAVS) and asbestosis are serious and life-altering conditions that can be caused, or aggravated, by conditions on-site.\nHealth and safety legislation should be at the core of all construction businesses and every project, it should be a top priority for senior managers and it should be embedded into every plan, decision and activity. Read on for ideas on how to manage health and safety in construction, to ensure that your staff are safe, give confidence to your clients, maintain a flawless health and safety record and save time and money through introducing safety measures.\nAssess and avoid\nRisk assessments should be the first stage of all construction projects. Health and Safety teams should be involved in early planning to ensure that the health of workers is embedded in the project.\nAvoidance is the first key step. Is an activity deemed as dangerous? Can it be carried out in an alternative way that would avoid the danger? Can working at height be removed altogether or can fabrication be carried out on the ground and the structure lifted to height for fixing? Are hazardous chemicals essential to work, or can alternatives be found?\nIf time constraints or pressure from third parties will impact on the health and safety of your staff, these issues should be flagged to all stakeholders. If the risk can’t be removed, then safe working conditions should be made available through the correct equipment or protocols. If the situation can’t be made safe, then the group must work to change the working parameters. Time or client pressures should never impact the safety of workers on site. No project is that important.\nHealth & Safety: Do you have the skills on-site?\nOnce all stakeholders have understood and accepted the risk parameters for a project, plans must be put in place to reduce these risks. The main aim, and the measure of success for all health and safety teams, is for a project to run with zero reported incidents.\nChecking the qualifications and experience of all workers and supervisors is key. Workers must be able to perform tasks confidently and competently to reduce the risk to themselves and others. Construction Skills Certification Scheme (CSCS) card checks are a must in the UK and a full log of each card, every contractor and their competency to carry out a task must be documented. Where gaps in knowledge or experience are identified, training should be provided or new staff must be brought in to complete the task. If this can’t be done, then the task must be changed to accommodate the experience of the workers on-site, even if this does add extra time or cost.\nEnsure that qualified Health, Safety and Environment (HSE) officers are known to all workers and that all staff know how and when to report a dangerous situation. Whether you have a dedicated HSE officer on-site, site office staff logging incidents or an app-based solution everyone can access, it should be easy for anyone to report an incident.\nMake protective equipment a habit\nOnce you have the right workers with the correct experience, they must have access to the correct personal protective equipment (PPE) to carry out the job safely. Every person on site should understand the risks to themselves and those around them and know which equipment they need to complete the task safely. Wearing the necessary equipment should become second nature and this only happens if everyone adheres to the rules, if the equipment is missing, there should be a quick and easy way for staff to request it.\nManagers should model the behaviours expected of their teams. Everyone should be free to stop work and demand equipment if it is not available and to request that others in their team do the same.\nHealth and safety regulations become a habit when the culture enables team members to express their views openly. This open and honest approach rewards workers who show behaviours that protect their own safety and those of their colleagues.\nThe right tools for the job\nThis theme carries through to the suitability of equipment too. All equipment, from pneumatic drills and saws to large earthmoving machinery, should be assessed for its suitability and safety. Does the equipment pose a risk to the worker? Can a different piece of equipment give the same result with lower risk? If we do have to accept a risk, does the equipment minimise the risk in the best way? What other checks do we need to ensure zero incidents whilst using this equipment?\nChecks don’t stop when the equipment is in use. Maintenance schedules are essential to ensuring the safety of equipment and safeguarding those using it. A robust service and maintenance routine should be developed, assigned to an individual or team and monitored throughout the project, with any checks or maintenance work documented.\nIdentify issues, make changes and demonstrate the impact\nSoftware really can be a health and safety professional’s best friend. A good software package will allow easy data collection from multiple team members in multiple locations. Everyone should be able to easily report a health and safety concern, record standard health and safety data and complete assessments. The best software allows for collection through app-based systems, enabling any smartphone to become a data collection device.\nOnce data is recorded, software should help you to easily identify hazards and other issues, show where changes were made, demonstrate the impact of these changes and allow for easy reporting to senior management. This comprehensive and closed-loop data flow is essential for demonstrating regulatory compliance and for providing evidence during inspections.\nBy moving to a continuous improvement culture, issues that are flagged and dealt with on one project can be carried through as health and safety requirements for the next project. This process ensures that the same problem isn’t repeated and that everyone learns from previous mistakes.\nThe PlanRadar app is the perfect tool for completing comprehensive digital health and safety assessments through straightforward templates. Fire assessments and other specific templates can be designed, saved and adapted for future projects. And, most importantly, where plants require no transmitting appliances, PlanRadar can be used in an offline mode, automatically syncing information as soon as the device is reconnected to a network. This functionality can be essential to COSHH assessments.\nWork hard, but rest hard too\nWhen time pressures are mounting and the threat of late fines are looming, it’s tempting to cut corners and push to get a project done, but this should never be at the expense of health and safety. If changing plans impact on people, equipment or procedures, then risk assessments should be re-evaluated.\nAlthough contractors should expect high standards of work and output from their workers, rest is important too. Workers need to be highly alert to their surroundings and to help keep everyone safe; this is especially important when working at height, with hazardous material or in confined spaces. Workers should be given adequate time to rest and relax and not be pushed to work overly long hours, even if they request the overtime. Distraction and tiredness lead to mistakes and they can be serious and even life-threatening.\nIf in doubt, shout\nBusinesses that excel at managing health and safety in construction have safety at the very core of their work and everyone on-site sees the benefit of the precautions, follows the rules and enforces them within their teams.\nCommunication is key here. When an individual understands why a rule is in place or why something has changed, they are more likely to comply and enforce compliance from others. When you have zero incidents during a project, reward the team for achieving this. If a near-miss has occurred, communicate this to the team, help them to understand why it happened and how further incidents can be avoided. Be clear about the changes you’ve made to make ensure the incident doesn’t occur again and how others can contribute to this adjustment.\nHealth and safety in the construction industry is the responsibility of everyone on site. Put simple procedures in place so that construction workers can quickly and easily report an issue and instil a culture of responsibility. If a trip hazard is identified, encourage workers to remove it. If someone is in danger, encourage others to act. Dealing with an issue when it arises can help reduce immediate incidents that may follow.\nHealth and safety rules should be repeated at daily briefings and reinforced throughout the day. There are many ways to remind workers of their health and safety obligations, perhaps through posters displayed around the site or reminders on software tools.\nSeveral of the bigger construction companies have powerful slogans to help educate and engage all team members on-site.\nBalfour Beatty’s award-winning golden rules highlight core expectations and they are simple and easy to remember:\nMeanwhile, Amey’s Target Zero encourages workers to speak out and talk about health and safety issues so that problems can be identified and dealt with quickly.\nWe have all the tools to achieve zero incidents and to make the construction site a safe place for all workers. By taking a thorough approach to identifying risk, developing a culture of open and honest communication and ensuring workers have access to the safest tools for the job, we can reduce injuries on-site and achieve zero incidents.']	['<urn:uuid:b5662dde-d5ac-4eb8-a2d6-84e0711255e4>', '<urn:uuid:2aab1c69-8b42-4bc3-b9a1-8f308e254433>']	open-ended	direct	long-search-query	distant-from-document	multi-aspect	expert	2025-05-12T21:31:42.699391	9	115	3102
28	How has the water crisis affected farming communities in Colorado, and what solutions have been proposed to manage water resources?	The water crisis has severely impacted farming communities, particularly in areas like the San Luis Valley, where in 2002, water pumps stopped working and created sunken pits around them due to aquifer depletion. The region, which receives only seven inches of rain annually, produces $320 million in crops and is Colorado's main potato-growing region. To address these challenges, two main solutions have emerged: In the Colorado River basin, the federal government is offering financial incentives, including up to $400 per acre-foot of conserved water, while in the San Luis Valley, farmers created a unique self-governing system of subdistricts that impose fees on water use, leading to more efficient irrigation practices and a 30% reduction in groundwater pumping.	['New US plan could lead to federal action on Colorado River\nFLAGSTAFF, Ariz. (AP) — The Interior Department announced Friday that it will consider revising a set of guidelines for operating two major dams on the Colorado River in the first sign of what could lead to federal action to protect the once-massive but shrinking reservoirs behind them.\nThe public has until Dec. 20 to weigh in on three options that seek to keep Lake Mead and Lake Powell from dropping so low they couldn’t produce power or provide the water that seven Western states, Mexico and tribes have relied on for decades.\nOne of the options would allow the Interior Department’s U.S. Bureau of Reclamation to take unilateral action, as it threatened this summer when it asked states to come up with ways to significantly reduce their use beyond what they’ve already volunteered and were mandated to cut.\n“The Interior Department continues to pursue a collaborative and consensus-based approach to addressing the drought crisis afflicting the West,” Interior Secretary Deb Haaland said in a statement. “At the same time, we are committed to taking prompt and decisive action necessary to protect the Colorado River System and all those who depend on it.”\nThe agency’s announcement comes more than four months after Reclamation Commissioner Camille Touton first told Congress that water use must be cut back dramatically as drought and overuse tax the river — an essential supply of water for farmers, cities and tribes in the U.S. West, as well as Mexico.\nThe seven states that tap the river failed to reach Touton’s initial August deadline and have been working ever since to reach a compromise. It now appears unlikely a grand deal will be reached. In the meantime, the bureau has offered up billions in federal money to pay farmers and cities to cut back.\nBut Interior’s new action marks the first time it’s taking a clear step toward imposing its own, mandatory cuts. The agency anticipates changes to the conditions at which water shortages are declared in the river’s lower basin. Lake Mead and Lake Powell were about half full when the 2007 guidelines were approved and are now about one-quarter full.\nThe other two options under the Bureau of Reclamation’s plan are to let states, tribes, and non-governmental organizations reach consensus, or do nothing, which is a standard alternative in environmental impact statements.\nThe Bureau of Reclamation expects to produce a draft next spring based on public input. A final decision could come in late summer of 2023 around the time the bureau announces any water cuts for the following year.\nThe 2007 guidelines and an overlapping drought contingency plan approved in 2019 were meant to give states more certainty in their water supply. For the lower basin states — California, Arizona and Nevada — the agreements set elevation levels at Lake Mead on the Arizona-Nevada border at which they are subjected to mandatory and voluntary reductions. Mexico also shoulders cuts.\nWater users have been delayed in renegotiating the agreements that expire in 2026 because the drought and climate change have forced quicker action.\nNevada, Arizona and Mexico will have to cut their water use in 2023 for a second year in a row under existing agreements. California is looped in at lower elevations in Lake Mead. Arizona was forced to give up 21% of its total Colorado River supply. Farmers in central Arizona, tribes and growing cities like Scottsdale are feeling the impacts.\nTom Buschatzke, director of the Arizona Department of Water Resources, said he’s been hoping the bureau would require cuts from water users rather than rely on voluntary action. But he also warned the prospect of mandatory cuts could make it less likely that farms or cities will choose to give up some of their water, calling it a “zero-sum game” of sorts.\nStill, anything that results in savings is a worthwhile action, he said.\n“The situation in my mind is so dire, we’re so close to the edge,” he said in a recent interview.\nThe Bureau of Reclamation also said it would publish a report next year to address ways to account for evaporation, seepage and other loss as water makes its way to states, cities and tribes.\nIn mid-October, the bureau announced a process for paying farmers and cities in Arizona, California and Nevada to conserve Colorado River water. Under one option, they can be paid up to $400 per acre foot of water (325,850 gallons) left in Lake Mead. So far only the Gila River Indian Community in Arizona has publicly expressed interest in that option, saying it will conserve up to 125,000 acre feet of water (40.7 billion gallons) on its reservation and offer another 125,000 acre feet of water stored underground to cities annually for the next three years.\nBut farmers in California and Arizona say that’s not enough money to account to cover losses if they leave fields unplanted or to pay for things like installing more efficient irrigation systems. Instead, they plan to apply for money through a different option that allows them to name their price — and justify why they deserve it.\nThe Imperial Irrigation District, which supplies water to farms in southeast California, has said it can conserve up to 250,000 acre feet of water (81.5 billion gallons). But officials haven’t said the price tag they plan to put on those savings. Farmers in Arizona’s Yuma County also want more money.\nA third pot of money would offer payment for larger projects aimed at achieving long-term water savings, like ripping up decorative grass in urban areas or building small, on-farm storage systems that make it easier for farmers to bank water rather than lose it to runoff.\nThe bureau says water users who take the $400 payments may be prioritized for that money over users who want more for short-term conservation.\nCalifornia farmers’ commitment is part of a broad offer by the state’s water users to conserve up to 9% of its river water. That’s contingent on adequate payment and help for the Salton Sea, a drying lake bed fed by farm runoff.\nRonayne reported from Sacramento, Calif.\nCopyright 2022 The Associated Press. All rights reserved.', 'In the summer of 2002, water pumps in Colorado’s San Luis Valley stopped working.\nThe center pivot sprinklers that coax shoots from the dry soil and turn the valley into one of the state’s most productive agricultural regions strained so hard to pull water from an underground aquifer that they created sunken pits around them.\n“This one right over here,” says potato farmer Doug Messick as he walks toward one of his sprinklers in the valley, near the town of Center. “I came up to it one day and I could’ve driven my pickup in that hole.”\nStreams that flow from the nearby San Juan and Sangre de Cristo Mountains slowed to a trickle, some of them before the normal irrigation season had even begun. Rushing water created by snow from the previous winter failed to materialize. That left the ditches, creeks and rivers that recharge the valley’s aquifers dry. The precious groundwater had plenty of demands, and no supply.\n“The aquifer was declining,” Messick says. “But nobody really started noticing until they started sucking air instead of water.”\nFarmers began to cast blame as to who caused the problem. Fingers pointed at the state, water managers, Mother Nature, and among the farmers themselves, divided into camps depending on where they got their water. All the while, many farmers kept pumping whatever water they could find and the aquifer continued its unprecedented decline.\nInstead of giving in to the divisions that could have so easily fractured the rural valley of about 47,000 residents, a group of farmers decided to embark on a risky experiment — the first of its kind in the United States. They agreed to pay more money for the water they pump out of the ground by imposing fees, a kind of tax, per acre-foot of water. To get to that point, family farmers had to put aside old grudges and recognize their shared fate in the aquifer under their feet.\nSeeing hope in the farmers’ efforts, researchers are studying the risky gambit to see if it is working.\nThe rural towns and homesteads in the San Luis Valley are intrinsically tied to the quantity and quality of groundwater found there.\nArtesian water pressure — a hydrological marvel made possible by the valley’s bowl-like geology — was accidentally discovered in the valley in the fall of 1887, according to a 1891 report by professor Louis George Carpenter of Colorado Agricultural College, now Colorado State University. A dug well could spurt water from deep underground several feet into the air and irrigate hundreds of acres.\nA photograph taken at the time, now-famous among water policy wonks in the West, shows a man neatly dressed in a suit and wide-brimmed hat, his hands tucked behind his back. He is dwarfed by the gushing water fountain next to him shooting up from the valley floor. The San Luis Valley’s water, and the farming opportunities it brought, quickly became the stuff of legend.\nLike an oil boom, word of the plentiful water spread quickly and development followed. By 1904, a U.S. Geological Survey inventory in the valley put the number of water wells at 3,234. For generations, the valley’s groundwater was plentiful, able to sustain growing communities and legacy farms.Standing in the valley’s center, near the town of Center, you are surrounded by towering mountain peaks in nearly every direction. At your feet, greasewood bushes and straw-colored native grasses spread out on the valley’s flat expanse. In land area, it’s roughly the same size as the state of New Jersey.\nIn the valley’s eastern flank, wetlands and marshes are common, temporary homes to sandhill cranes as they make their spring migration. In its western portion, the country’s highest sand dunes rise hundreds of feet to form the main attraction of Great Sand Dunes National Park. In between lie thousands of acres of potatoes, barley, hay, vegetables and grazing cattle — all supported by a sprawling network of ditches and canals and water pumps.\n“The San Luis Valley is — I’ve been told — the most intensely irrigated agriculture region in the world,” says Heather Dutton, manager of the San Luis Valley Water Conservancy District. She grew up on her father Doug Messick’s potato farm.\nLooking at the valley from above, it is easy to see what she means. The flat expanse of the valley floor is a grid of circular farms with lush, green fields. The corners between fields, the sections not watered by irrigation systems, are dusty and brown.\n“So we get — especially here in Center — we get an average of seven inches of rain a year,” Dutton says. “So it’s a high mountain desert.”\nThe area sees less rain than Phoenix, Arizona, which receives an average of eight inches every year, according to the National Oceanic and Atmospheric Administration.\nWhile it’s intensely dry, the desert is also one of Colorado’s most productive and valuable agricultural regions. In 2012, the valley’s farmers sold nearly $320 million in crops alone. That same year, ranchers sold more than $61 million in livestock.\nColorado is the fifth largest state for potato production and nearly all of the state’s potatoes are grown in the San Luis Valley. In 2013, valley farmers produced more than 18 million pounds of potatoes. All of the farm production is only possible because of complex systems to manage, store and distribute water. Without that, agriculture simply couldn’t survive.\nCommunity At A Crossroads\nBy the end of 2002, it was clear the valley’s farmers were fast approaching a crossroads. Colorado’s top water enforcer, the state engineer, made clear that if the farmers continued to pump from the underground aquifer he would be forced to shut them down.\nThey were running afoul of the state’s frontier-era water laws, which prioritize water rights based on their effective date. Some farmers who held rights to divert water from streams dating back to the late 1800s were seeing their supplies drop, partially thanks to water wells dug decades later in the 1950s and ‘60s. In Colorado, when a younger water right is curtailing an older one, it is a serious problem.\nIn the years that followed the 2002 drought, scientists did enough research and monitoring to link the reduction of the aquifer to the limited availability of surface streams.\nFor the farmers that depend on the aquifer, the choice was simple: keep pumping until everyone’s supplies ran out and risk the ire of state water officials; or, find a way to curb their pumping.\n“It was really the first effort here in a recognition that if they didn’t do something that the consequences would be pretty grave,” says Cleave Simpson, director of the Rio Grande Water Conservation District, the valley’s main water management authority.\nAfter years of litigation, court cases and a round of state legislation, the farmers formed a plan. A majority made a painful decision. They agreed that it was in everyone’s best interest to pay more money for water, hoping that the higher cost would cause them to think twice when turning on their pump.\n“We couldn’t sit back and just pump to the bottom of the aquifer,” Simpson says. “The schools, the hospitals, the communities would just not survive.”Communities formed a network of subdistricts that could levy fees on water use, self-governed by the farmers themselves. Subdistrict one, the largest and most heavily irrigated in the valley, was the first.\n“This is kind of a classic ‘tragedy of the commons’ situation,” says Kelsey Cody, a doctoral student at the University of Colorado-Boulder who is part of a research team that studies groundwater pumping in the valley. “As an individual, I have no incentive to leave any water in the ground because any water I leave in the ground I know my neighbor is going to take out. And he knows the same thing.”\nToday, farmers in subdistrict one pay $75 for each acre-foot of water they pump and another $8 for every acre of crops where that water is used. An acre-foot is the standard unit of measurement when talking about vast amounts of water, and easy enough to visualize. It’s the amount of water spread out over an acre at a depth of one foot.\nIf those same farmers are recharging the aquifer by applying surface water to their crops, they’re given a credit for that added water. Some farmers who pump end up paying nothing at all if their water use finds a balance between the amounts pumped and recharged.\nFor some bigger farms without surface water rights, that is not the case. Their annual water use fees can total tens of thousands, sometimes hundreds of thousands, of dollars. That money is then invested in a fallowing program that pays farmers not to plant or to purchase farmland outright. While the fees have been tough for some farmers to swallow, at least a majority have internalized the goal.\n“The alternative is to shut these wells off until this aquifer comes back by itself,” says Jamie Hart, a barley and potato grower in the valley who sits on the subdistrict one board. “We can’t survive that. There are land payments to make and equipment payments to make. And we can’t continue without producing something.”\nBut it’s not easy. Subdistrict one’s early meetings were contentious and tense. For months farmers argued about the fairness of the fee structure and the right dollar amount to collect from those who pump from the aquifer. Conspiracy theories popped up, calling the subdistrict idea a nefarious attempt to make agriculture so costly and difficult that farmers would begin selling off their water rights to the highest bidders.\n“Working together, that’s hard for farmers to do,” Hart says. “Farmers, they’re pretty independent people.”\nA Success In The Making?\nThis was the first time a group of U.S. farmers decided to tax themselves to pump groundwater, fallow tens of thousands of acres of farmland, and try to save an aquifer, according to research from the University of Colorado Boulder. Because of that, no one really knew if it would work.\nThe valley’s farmers have been paying for groundwater now for nearly six years. The wells in subdistrict one all have meters that measure how much water they pump. Researchers were able to compare pumping figures collected before the fees went into effect, and after.\n“From our analysis of the data, we were able to determine that yes, they’ve been able to reduce their groundwater extraction pretty substantially,” says researcher Kelsey Cody.\nFarmers have cut back their pumping by an average of 30 percent since they started paying fees. Since 2013, the aquifer has risen about 250,000 acre-feet. They still need to recharge at least another roughly 600,000 to 700,000 acre-feet to have the aquifer be considered somewhat recovered and stabilized.Many farmers say they have updated irrigation equipment and switched to less thirsty crops, Cody says. Some of the water savings comes from pinpoint timing of when to water crops in order to waste less. Meanwhile, the number of irrigated acres hasn’t dropped substantially. That suggests farmers are irrigating smartly and efficiently, not closing up shop.\n“It’s not a fee that so far has caused people to go out of business,” says Craig Cotten, the Colorado Division of Water Resources engineer based in the valley’s most populous city, Alamosa. “But it is a fee that they actually think about what they’re doing when they go push a button on a well.”\nThe farmers who are part of subdistricts are under a court mandate to recharge the aquifer even more over the next decade-and-a-half. At least within subdistrict one, there are early discussions taking place about raising the pumping fee higher in order to curb pumping even more and make additional progress in aquifer recharge.\n“Can we fill up the bath tub that we created, the void that we created for $75 [per acre-foot]? Probably not,” Hart says. “It may cost us a little bit more to ever get that filled back up. But once we get there we can back that fee off. And we can survive.”\nThere is the fickleness of Mother Nature to contend with as well. In each of the last four years, snowpack in the nearby mountains has hovered at average to above average levels. When it melts, it yields higher streams, which recharge the aquifer even further. Historical weather records are pessimistic about a fifth consecutive year with above average snowpack. Drought is a constant specter hanging overhead with the possibility of undoing all the progress that’s been made.\nWith aquifers in danger of running dry in many communities in the Midwest and West, like the Ogallala Aquifer under the Great Plains or the aquifer system in the central valley of California, the San Luis Valley could provide a model, says CU’s Kelsey Cody.\n“The reason people should care about this is that anywhere there is groundwater there is almost always going to be challenges in managing it,” Cody says, “because it’s difficult to see, it’s difficult to monitor, it’s easy to think that it’s everlasting.”\nAfter a brush with danger, and a series of hard lessons, the farmers in the San Luis Valley now know they need to care for their water supply if it, and their communities, are to survive.']	['<urn:uuid:d800c496-0a85-4226-ab68-ddc9272bcbf8>', '<urn:uuid:d900ed2d-d263-43b9-bda8-5e2e52e704ca>']	open-ended	direct	verbose-and-natural	similar-to-document	multi-aspect	novice	2025-05-12T21:31:42.699391	20	117	3245
29	what are different ways seeds get dispersed in nature and how does seed banking help preserve endangered plant species	In nature, seeds are dispersed through various mechanisms to reduce competition with parent plants. Seeds can be scattered by wind, carried on animal hair or bird feathers, distributed through animal droppings, or float in water. Some plants, like lupins, have pods that explosively split open to scatter seeds. To preserve plant species, especially endangered ones, seed banking plays a crucial role. The Millennium Seed Bank Partnership has collected over 2.3 billion seeds, representing more than 12.5% of wild plant species. These stored seeds serve as a safety net against extinction and can be actively used to restore degraded habitats, increase ecosystem resilience, and recover species in the wild. Additionally, banking wild relatives of crop species can help develop more resilient food systems that can adapt to climate change.	['The process by which seeds are formed is almost unbelievable. The flowers in this picture are grown to make a garden look attractive. However, their main purpose is to contain the sex organs for the plant.\nThis is a fuchsia plant in flower.\nLet’s look more closely at flowers, as this is where all the action happens for sexual reproduction in plants.\nHere is a cross-section diagram of a fuchsia flower.\n- For seeds to be produced, the male sex cells in the pollen and female sex cells in the ovary need to get together. The first part in this process is pollination. Pollination is when the pollen lands on the female part of a flower. In most plants the stigma needs to receive pollen from another flower.\n- Cross pollination occurs when pollen from a flower lands on the stigma of a different flower of the same species.\n- Self-pollination occurs when the pollen from a flower lands on the stigma of the same flower.\nThere are two main ways that pollen gets from one flower to another:\nWhen insects like bees or flies visit flowers and pollen sticks onto their hairy legs. When they visit another flower the pollen rubs off on its stigma.\nInsect pollinated flowers may have large brightly coloured petals, large pollen grains, and be scented.\nThe wind carries pollen from one plant to another. Wind pollinated flowers may have small petals, no scent, a lot of small light pollen grains on long, droopy stamens.\nGrass pollen is carried by wind. Grass has feathery stigmas to catch the pollen.\nA few plants, for example, cucumbers, have separate male and female flowers on one plant.\nKiwifruit have male and female flowers on separate plants. These are called dioecious plants. This means a male plant must be grown alongside female plants for pollination to occur.\nOnce pollination has taken place fertilisation can occur. Fertilisation is the joining of a male and female sex cell. This is what happens:\n- a pollen grain grows a tube down the middle of the pistil (via the style) to the egg inside the ovary\n- when the tube reaches the female sex cell (egg), the tip dissolves. The male sex cell (sperm) is released\n- the sperm and the egg join in the ovule\n- once these cells are joined, the ovules turn into seeds inside the ovary\n- if there is more than one egg in the ovule then more than one seed forms as in a pea pod if each egg is fertilised by a separate sperm\n- the ovary develops into a fruit and surrounds the seed.\nThe seeds will have characteristics from the two different parent plants. Just like you, they will be individuals and in some way different from their parents.\nThe development of seeds\nSeeds contain a food store and a tiny plant inside a protective coat.\nThe food store is called the cotyledon.\nMonocotyledon seeds, such as corn, have one cotyledon to store food for the developing seed. Some seeds such as bean seeds have two cotyledons. These are called dicotyledons.\nThe tiny plant is called the embryo, and the protective coat covering the seed is called the testa.\nUnderstanding the structure of seeds will help you propagate plants more successfully from seed.\nThis is a diagram of the inside of a split lupin seed.\nHere is a cross-section diagram of a corn seed.\nIt has one cotyledon. It is called a monocotyledon seed. The cotyledon’s role in a monocotyledon is slightly different from that of a dicotyledon. The cotyledon transfers food to the embryo.\nWhen fertilisation is complete, the ovary surrounding the seeds develops into a fruit. The fruit helps to protect the seed. Some fruit is dry like gorse or lupin pods and some are juicy like an apple.\nThis is the scattering of seeds away from the parent plant. Seeds need to be dispersed to reduce the competition from the parent plant and germinating seedlings.\nWhen the pod of the lupin dries it splits open explosively to scatter the seeds away from the plant.\nSeeds can be dispersed by the wind, on animal hair or bird feathers, in bird and animal faeces or they can float in water.\n- The process of pollination and fertilisation in plants explains why plants grown by seed show some variation.\n- Pollination takes place when the pollen grains are transferred from the male anther to the female stigma.\n- Fertilisation is the joining of the female ovule and male pollen.\n- The new plants will be slightly different because all ovules and pollen grains are slightly different genetically.\n- A monocotyledon is a plant with one cotyledon in its seed.\n- A dicotyledon is a plant with two cotyledons in its seed.\nGo to: 3 Seed germination.', '23 September 2019\nPlants and policy: Tackling global challenges\nBiodiversity loss, climate change and sustainable development are urgent global issues. 2020 is a critical year to address all three - and plants are at the core of many solutions.\nIn October 2020, governments from 196 countries will gather at the UN Convention on Biological Diversity (CBD) in China, to set ambitious new global targets that aim to reverse biodiversity loss. This Post-2020 Global Biodiversity Framework will provide a roadmap for conservation efforts everywhere, going forward.\nThis is occurring at a time when the scale of biodiversity loss is staggering. Plants are intricately linked to human life, yet one in five plants are threatened with extinction. Drawing on over 250 years of plant and fungal expertise, Kew’s scientific research is at the forefront of informing policy - and contributing to biodiversity targets. From accelerating assessments of plants most at risk of extinction, to increasing food security in a changing climate, to protecting vulnerable ecosystems, Kew’s work is contributing plant and fungal based solutions to some of the world’s most pressing challenges.\nWhere to start in a biodiversity crisis?\nWith at least 390,000 plant species across the planet, and new species being discovered every year, there is a serious shortage of conservation resources to understand and monitor every single species. There is a risk that many species may go extinct before scientists can assess them, or before they are even discovered – potentially losing irreplaceable medicines, food sources, habitats, and climate regulators.\nIUCN Red List Assessments\nOver the past three years, Kew has been accelerating the number of plants being assessed for the IUCN Red List – the global source of information on species extinction risk. From 126 species of coffee (Coffea), to a plant genus that forms the basis of the Amazon rainforest (Myrcia), the status of many plant species has been captured for the first time. As well as continuing to assess new species, Kew’s Plant Assessment Unit will also be conducting further assessments – creating more ‘snapshots’ for species, so that not only the threat status of species can be determined, but also trends over time.\nThis is a particularly important indicator for measuring progress towards conservation targets - it allows us to answer key questions such as how many species are threatened with extinction, and are conservation efforts reversing these trends? These form an important component of our understanding of the current biodiversity crisis and help us measure progress towards global conservation targets.\nKew is working to champion plants and fungi, and increase their representation in these metrics, which have historically been under-represented in these assessments.\nSeeds as back-ups, and solutions\nWith land use change and exploitation of species accelerating, and climate change amplifying the challenges for species, the list of species in the biodiversity crisis is growing. Plants are fundamental to all life on Earth and seed banking is playing an essential role in creating a safety-net against extinction.\nThe Millennium Seed Bank\nWith a focus on endangered, endemic and economically important species, Kew is working with a network of partners to bank over a tenth of all wild plants in the Millennium Seed Bank Partnership. More than 12.5 per cent of wild plant species have been banked so far, with over 2.3 billion seeds currently housed at Kew’s purpose-built facility in Wakehurst Place.\nYet these seeds are not just passive ‘reserves’, locked away in a vault - they are also active solutions. The genetics of these seeds contain tools that can be applied to some of the world’s most urgent global challenges. They can be actively used in projects to restore degraded habitats, increase the resilience of ecosystems and recover species in the wild. Banking the wild relatives of crop species can reveal a suite of adaptations that can be bred into crops – enabling food systems around the world to adapt to a changing climate.\nWith a growing network of partners across the globe, this project meets all three objectives of the CBD: to conserve, sustainably use, and equitably share the benefits arising from the use of biodiversity.\nThe multiple value(s) of plants\nConservation work always requires a combination of approaches – and conserving species in their native habitats is crucial. An important limitation of seed banks is that not all seeds can be banked. This is particularly true of many tropical plant species – the very same species that form the basis of ecosystems that are globally relevant for climate regulation, and critical for millions upon millions of people locally, who depend on forests for food, medicine and livelihoods.\nThe tropics are therefore a major priority for biodiversity conservation at a global level - nowhere are the intricate links between biodiversity, sustainable development and climate change clearer.\nTropical Important Plant Areas (TIPAs)\nBuilding on an approach that has secured habitats across Europe, the Mediterranean and North Africa, Kew has carefully adapted Important Plant Area (IPA) criteria to suit the paradox of the tropics: a relative lack of plant data, but urgent need for action.\nWorking closely with partners in-country, Tropical Important Plant Areas (TIPAs) are identified within each country according to a range of criteria – from areas of unrivalled botanical richness, to concentrations of threatened species, to areas that are important locally because of plants that are used.\nFormally recognising these areas can help make a compelling case for protecting habitats, in regions that are trying to balance development with preserving irreplaceable biodiversity. TIPAs can help with the planning of protected areas, contributing to global goals to protect 17 per cent of land, or lead to legal recognition for community areas, contributing to the UN Sustainable Development Goals (SDGs). They can also put plants on the radar in the planning stages of development projects, allowing short-term and long-term trade-offs to be more accurately considered.\nTIPAs therefore act as a tool to empower local decision making, whilst simultaneously highlighting how national plant conservation fits within an international picture – bridging gaps between global policy and national context.\nA critical opportunity\nBending the curve of biodiversity loss is a momentous task. It requires a combination of approaches that include all aspects of biodiversity: from species, to genetics, to ecosystems.\nThe upcoming UN Convention on Biological Diversity represents a critical opportunity to galvanise the support and resources needed at a global level. The decisions made in these rooms will filter down to plants, people and habitats in every corner of the planet.\nKew will be striving to ensure plants and fungi are at the forefront of this. And our science continues to reveal the same message loud and clear - biodiversity loss is a major global problem, but conserving biodiversity offers major global solutions too.\nCorlett, R.T. (2016). Plant diversity in a changing world: status, trends, and conservation needs. Plant Diversity, 38(1), 10-16.\nDarbyshire, I., Anderson, S., Asatryan, A., Byfield, A., Cheek, M., Clubbe, C., Ghrabi, Z., Harris, T., Heatubun, C.D., Kalema, J. & Magassouba, S. (2017). Important Plant Areas: revised selection criteria for a global approach to plant conservation. Biodiversity and Conservation, 26(8), 1767-1800.\nMace, G.M., Barrett, M., Burgess, N.D., Cornell, S.E., Freeman, R., Grooten, M. & Purvis, A. (2018). Aiming higher to bend the curve of biodiversity loss. Nature Sustainability, 1(9), 448-451.\nWillis, K. J. (ed.) 2017. State of the World’s Plants 2017. Report. Royal Botanic Gardens, Kew.\nRead & watch\n16 January 2019\nA high extinction threat for wild coffee could rattle the sector\n6 November 2018\nPlant cryobiotechnology at Kew: past, present and future\n6 July 2018\nThe critically endangered Madagascar Banana\n22 May 2019\nOur food, our health\n17 May 2019\nSaving endangered species\n23 July 2019\nWe can see beyond the trees']	['<urn:uuid:0537322e-e7d0-4039-878b-9ac7e37b3d22>', '<urn:uuid:2e4dd975-72b3-4fb8-b7c8-d0ba8f0a9056>']	open-ended	with-premise	long-search-query	similar-to-document	multi-aspect	novice	2025-05-12T21:31:42.699391	19	128	2084
30	halloween trick or treat history worldwide changes	Trick-or-treating originated from the medieval practice of 'souling' in Europe, where poor people would visit wealthy homes asking for 'soul cakes' in exchange for prayers for the dead. In Scotland and Ireland, it was known as 'guising,' where children would perform songs or poems for food or money. The practice was later brought to North America in the early 20th century, though it was temporarily suspended during WWII due to sugar rationing. Today, Americans purchase almost 600 million pounds of Halloween candy for trick-or-treaters. The tradition has spread globally, with places like the Philippines adapting it - where children traditionally sang hymns about souls in Purgatory for alms, they now also dress in costumes and ask for sweets.	"['From the millions spent on candy each year to Halloween’s global influences and celebrations, here are interesting facts about one of our favorite holidays.\n1. It All Began in Ireland\nThe ancient Celtic Festival called Samhain was first celebrated more than 2,000 years ago in County Meath. The Celts believed it was a time of transition, when the veil between this world and the next came down, and the spirits of all who had died that year moved on to the next life. But if the boundaries between the worlds of the living and the dead overlapped, the deceased could come back to life and wreak havoc among the living. Not a good thing.\nToday the ancient past and the twenty-first century come together at the annual Spirits of Meath Halloween Festival, where a re-enactment of the Celtic celebration kicks off with a torchlit procession through town. The Irish welcome Halloween with bonfires, party games and traditional food, including a fruitcake that contains coins, buttons, rings and other fortunetelling objects. In ancient times, it was believed that if a young woman found a ring in her slice, she’d be married within the next year.\n2. Globetrotting for Halloween\nHalloween festivals in New Orleans, Bangkok, Los Angeles, New York and Limoges, France have become must-see stops on the grown up global party scene. Folks looking for a spookier way to celebrate head to historical haunts in Salem, Massachusetts and the Dracula sights in Romania, while daredevlls can get their spook on at one of the 2,500 haunted attractions worldwide, which has become a $300 million industry.\n3. Day of the Dead Isn’t Halloween\nDespite outward similarities — sweet treats, skeletons, people in costumes, and other graveyard and death imagery — Halloween and the Day of the Dead (Día de los Muertos) are very different holidays. While Halloween promotes fear of the dead, Día de los Muertos celebrates the dead. The Nov. 2 holiday is a day for people in Mexico, parts of Central and South America, and increasingly throughout the United States to honor their ancestors and loved ones who have passed away and invite those spirits back into their homes to be part of the family once more.\nIt\'s a tradition that dates back thousands of years, to the Aztec people. But when the Catholic faith became entrenched in South America, the timing of festival of the dead was changed to coincide with All Saints Day and All Souls Day. This connection is part of the reason why people confuse the holidays.\n4. The Timing Is Important\nThe name Halloween is a contraction of All Hallow’s Evening, also known as the night before All Saints Day (“hallow” is the Old English word for saint). Christians around the world have marked the Triduum of All Hallows since the 8th century AD. The three-day observation includes All Hallows\' Eve (Hallowe\'en), All Saints\' Day (All Hallows\') and All Soul\'s Day, and lasts from October 31 to November 2. It is a time to remember the dead, including martyrs and saints.\n5. Trick or Treating Was Put On Hold During WWII\nTrick or treating has been part of North American Halloween festivities since the early 20th century but like so many aspects of this holiday, it evolved from an ancient European custom. On All Souls Day, poor people would visit the houses of their wealthier neighbors for a ""soul cake"" — a form of shortbread — in return for which the beggars promised to pray for the dead of the household. Known as “souling,” the practice was later taken up by children, who would go from door to door asking for gifts such as food and money. Irish and Scottish communities in the US revived the tradition, although it was put on hold for several years during WW II due to sugar rationing. But in the U.S., instead of buying shortbread, Americans will purchase almost 600 million pounds – or $2.8 million worth — of Halloween candy for trick or treaters.\n6. Philippines: Singing Hymns For Tortured Souls\nPeople in the Pampanga province of the Philippines observe Pangangaluluwà from October 29 till 31, leading up to All Souls’ Day on November 2. Traditionally, Filipino children have gone from house to house singing hymns about the souls in Purgatory and asking for alms to pay for special masses, but today Filipino children have begun to embrace the ‘trick-or-treat’ tradition and add dressing in costume and asking for sweets to the ritual.\n7. People Wore Masks so Ghosts Couldn’t Recognize Them\nBack in the day, the Celts worried about bumping into the ghosts they believed came back to the earth on Halloween. To avoid being recognized, people would don masks when they left their homes after dark in hopes the ghosts would mistake them for fellow spirits. These days folks are willing to spend a bundle in their attempts to be mistaken for pop culture icons and internet memes: In total, Americans will spend $2.6 billion on costumes with $1 billion on children\'s costumes, $1.2 billion on adult costumes and $330 million on costumes for pets, according to the National Retail Federation (NRF).\nPrincess, animal and superhero costumes are the top three choices for kids, while pet owners say the top costumes are pumpkin and hot dog with the devil and cat tied at third. One major new trend in 2013 is an increase in DIY costumes, which the NRF attributes in part to the rise of Pinterest, which 9.3 percent of people surveyed say they plan to scour for inspiration. The average person will spend $27.85 on a costume, whether from the store or homemade, down slightly from the $28.65 spent in 2012.\n8. Halloween in Hong Kong\nThe city’s East meets West history and a rich folklore filled with ghosts and demons make Hong Kong a natural for Halloween celebrations. For the month of October, there are various celebrations around town including the Scream-No-More Challenge at Hong Kong Disneyland (where you get points for not reacting to scary things) and Tim Burton-esque spooky decorations and foods. There are also events at various malls and attractions around town, even a Halloween themed playground.\n9. Jack O\' Lanterns Scare Off Evil Wandering Spirits\nCarved pumpkins are another old school tradition, originating in Ireland, Scotland and England, where people carved menacing faces into vegetables and placed a candle inside to scare away the evil wandering spirits. Using pumpkins is an American spin on the ritual – the first lanterns were made from hollowed out turnips, rutabagas, gourds, potatoes and beets.\n10. Halloween Sales Skyrocket in the UK\nThe popularity of Halloween continues to grow worldwide. Look a the UK market where 10 years ago, consumer spending on Halloween totaled £12m, now it is a £300m+ industry. One popular British grocery chain saw sales of spider cupcakes and chocolate brownies rise 19 percent from 2011-2102, with sales of Halloween-themed chocolates jumping 86 percent in the same time.\nWhile Halloween celebrations are on the rise in Britain and across the Commonwealth, many lament that it’s at the expense of the centuries-old celebration of Guy Fawkes Day, a night of bonfires and fireworks that commemorate a 1605 Catholic plot to blow up England\'s Parliament and bring down King James I. Those festivities begin with children making an effigy of ""Gunpowder Plot"" conspirator Guy Fawkes and parading him down streets, asking passers-by to ""spare a penny for the Guy."" They would then use the money to buy fireworks and burn Guy on a bonfire. But while older Brits dismiss Halloween as an American holiday (one 2006 survey found that over half of British homeowners turn off their lights and pretend not to be home on Halloween), younger generations now enjoys a veritable witches’ cauldron of Halloween fun, bubbling over with hundreds of ghoulish things to do, from tricks and treats to huge all night warehouse raves and Halloween ghost hunts in haunted houses.', 'Halloween is celebrated on 31st October every year, and is also known as All Hallows’ Eve or All Saints’ Eve. We delved into the history of this spooky celebration and its associated traditions, from the traditional pumpkin carving to the more modern dressing up of pets…\nWhere does it come from? The true origins of Halloween are widely disputed, but the general consensus is that it originated from the ancient Celtic festival of Samhain, which means ‘summer’s end’.\nThis was to mark the end of the warmer weather and the harvest, and the beginning of the cold, dark winter. People believed that on this night, the boundary between the worlds of the living and the dead became blurred, allowing ghosts to walk the earth. People would light bonfires and wear costumes to ward off monsters and ghosts.\nDecorative carved pumpkins are synonymous with Halloween. They are also known as ‘jack-o’-lanterns’, especially in America. This name is thought to originate from an Irish folktale about a man called Stingy Jack, who tricked the devil and wasn’t allowed into heaven or hell, so wandered the earth carrying a lantern. The jack-o’-lantern would originally be carved from turnips or potatoes, but as the tradition grew in popularity in America, pumpkins became the preferred choice as they were so readily available.\nTrick-or-treating has also been around since medieval times in Scotland and Ireland, where it was known as ‘guising’. Children would dress up in costumes and ask for food or money in exchange for songs, poems or other tricks. And black cats? In the Middle Ages, people believed that witches could turn themselves into black cats to hide themselves, so they (rather unfairly) became associated with bad luck.\nThe contemporary Halloween is perhaps spooky in a different way: the spending associated with it. In America, Halloween is second only to Christmas in terms of money spent (which is estimated to be approximately $8 billion in 2020, according to the National Research Federation). Nearly half of those surveyed said they would dress up in costumes for Halloween, with the most popular costumes being princesses and superheroes. And dressing up isn’t just for the humans anymore. Spending on pet costumes was estimated at $490 million in 2019, with the most popular choices being pumpkin and hot dog. In the UK, 10 to 15 million pumpkins are grown each year, with 25% of the British public saying they tend to buy a pumpkin for Halloween. This amounts to £29.7 million spent on pumpkins alone!\nIs Halloween also an opportunity for science? That depends on what you’re researching! A few scientific studies have used Halloween in their methodology, one of which took place in 1976. It looked into the conditions behind uninhibited behaviour by presenting trick-or-treating children with the opportunity to steal; children were told to take one sweet and left unattended with a pile of sweets and money. Children that had not been asked their name or address (so were anonymous), children in groups and children not accompanied by an adult were all more likely to take extra sweets or money (Diener et al., 1976). Another study in 2004 used Halloween to present children with an imaginary character, the Candy Witch, to investigate the factors affecting belief in fantasy beings in young children (Woolley et al., 2004).\nWill you be carving out some time for pumpkins or donning your witch’s hat? This year, Halloween falls on a full moon for the first time in 19 years, and this one’s a blue moon too! So watch out for werewolves, and have a spook-tacular time!\nDiener, E., et al. Effects of deindividuation variables on stealing among Halloween trick-or-treaters. Journal of Personality and Social Psychology. 1976;33(2):178–183.\nWoolley, J. D., et al. A visit from the Candy Witch: Factors influencing young children’s belief in a novel fantastical being. Developmental Science. 2004;7.4:456-468.']"	['<urn:uuid:b2e84afc-2606-47ce-8ceb-1d5ac7491041>', '<urn:uuid:733cb9ce-5fc6-4581-b0cf-b1419a86e4ea>']	open-ended	direct	short-search-query	similar-to-document	multi-aspect	novice	2025-05-12T21:31:42.699391	7	118	1962
31	I've heard about the evolution of accounting systems and their regulatory oversight - what's the connection between early accounting software development and the implementation of corporate accountability laws?	Early accounting information systems were developed in the 1970s primarily for payroll functions and were initially created as expensive in-house legacy systems. Following corporate scandals at companies like Tyco, Enron, and WorldCom, the Sarbanes-Oxley Act of 2002 was implemented, requiring strong internal controls in transaction-based systems and making company leaders legally responsible for financial statement accuracy.	"['Accounting information system\nAn accounting information system (AIS) is a system of collecting, storing and processing financial and accounting data that are used by decision makers. An accounting information system is generally a computer-based method for tracking accounting activity in conjunction with information technology resources. The resulting financial reports can be used internally by management or externally by other interested parties including investors, creditors and tax authorities. Accounting information systems are designed to support all accounting functions and activities including auditing, financial accounting & reporting, managerial/ management accounting and tax. The most widely adopted accounting information systems are auditing and financial reporting modules.\nTraditionally, accounting is purely based on manual approach. Experience and skilfulness of an individual accountant are critical in accounting processes. Even using the manual approach can be ineffective and inefficient. Accounting information systems resolve many of above issues. AISs can support an automation of processing large amount of data and produce timely and accuracy of information.\nEarly accounting information systems were designed for payroll functions in 1970s. Initially, accounting information systems were predominantly developed ""in-house"" as legacy systems. Such solutions were expensive to develop and difficult to maintain. Therefore, many accounting practitioners preferred the manual approach rather than computer-based. Today, accounting information systems are more commonly sold as prebuilt software packages from large vendors such as Microsoft, Sage Group, SAP AG|SAP and Oracle Corporation|Oracle where it is configured and customized to match the organization’s business processes. Small businesses often use accounting lower costs software packages such as MYOB and Quickbooks. Large organisations would often choose ERP systems. As the need for connectivity and consolidation between other business systems increased, accounting information systems were merged with larger, more centralized systems known as enterprise resource planning (ERP). Before, with separate applications to manage different business functions, organizations had to develop complex interfaces for the systems to communicate with each other. In ERP, a system such as accounting information system is built as a module integrated into a suite of applications that can include manufacturing, supply chain, human resources. These modules are integrated together and are able to access the same data and execute complex business processes. Today, Cloud-based accounting information systems are increasingly popular for both SMEs and large organisations for lower costs. With adoption of accounting information systems, many businesses have removed low skills, transactional and operational accounting roles.\nAn example of architecture\nAn AIS typically follows a multitier architecture separating the presentation to the user, application processing and data management in distinct layers. The presentation layer manages how the information is displayed to and viewed by functional users of the system (through mobile devices, web browsers or client application). The entire system is backed by a centralized database that stores all of the data. This can include transactional data generated from the core business processes (purchasing, inventory, accounting) or static, master data that is referenced when processing data (employee and customer account records and configuration settings). As transactions occur, the data is collected from the business events and stored into the system’s database where it can be retrieved and processed into information that is useful for is a making decisions. The application layer retrieves the raw data held in the log database layer, processes it based on the configured business logic and passes it onto the presentation layer to display to the users. For example, consider the accounts payable department when processing an invoice. With an accounting information system, an accounts payable clerk enters the invoice, provided by a vendor, into the system where it is then stored in the database. When goods from the vendor are received, a receipt is created and also entered into the AIS. Before the accounts payable department pays the vendor, the system’s application processing tier performs a three-way matching where it automatically matches the amounts on the invoice against the amounts on the receipt and the initial purchase order. Once the match is complete, an email is sent to an accounts payable manager for approval. From here a voucher can be created and the vendor can ultimately be paid.\nAdvantages and implications\nA big advantage of computer-based accounting information systems is that they automate and streamline reporting, develop advanced modelling and support data mining. Reporting is major tool for organizations to accurately see summarized, timely information used for decision-making and financial reporting. The accounting information system pulls data from the centralized database, processes and transforms it and ultimately generates a summary of that data as information that can now be easily consumed and analyzed by business analysts, managers or other decision makers. These systems must ensure that the reports are timely so that decision-makers are not acting on old, irrelevant information and, rather, able to act quickly and effectively based on report results. Consolidation is one of the hallmarks of reporting as people do not have to look through an enormous number of transactions. For instance, at the end of the month, a financial accountant consolidates all the paid vouchers by running a report on the system. The system’s application layer provides a report with the total amount paid to its vendors for that particular month. With large corporations that generate large volumes of transactional data, running reports with even an AIS can take days or even weeks.\nAfter the wave of corporate scandals from large companies such as Tyco International, Enron and WorldCom, major emphasis was put on enforcing public companies to implement strong internal controls into their transaction-based systems. This was made into law with the passage of the Sarbanes–Oxley Act of 2002 which stipulated that companies must generate an internal control report stating who is responsible for an organization’s internal control structure and outlines the overall effectiveness of these controls. Since most of these scandals were rooted in the companies\' accounting practices, much of the emphasis of Sarbanes Oxley was put on computer-based accounting information systems. Today, AIS vendors tout their governance, risk management, and compliance features to ensure business processes are robust and protected and the organization\'s assets (including data) are secured.\nMany large and SMEs are now adopting cost effective cloud-based accounting information system in recent years.\nLooking back years ago, most organizations, even larger ones, hire outside consultants, either from the software publisher or consultants who understand the organization and who work to help select and implement the ideal configuration, taking all components into consideration.\nThe steps to implement an accounting information system are as follows:\n- Detailed Requirements Analysis\n- where all individuals involved in the system are interviewed. The current system is thoroughly understood, including problems, and complete documentation of the system—transactions, reports, and questions that need to be answered—are gathered. User needs that are not in the current system are outlined and documented. Users include everyone, from top management to data entry. The requirements analysis not only provides the developer with the specific needs, it also helps users accept the change. Users who have the opportunity to ask questions and provide input are much more confident and receptive of the change, than those who sit back and don\'t express their concerns.\n- Systems Design (synthesis)\n- The analysis is thoroughly reviewed and a new system is created. The system that surrounds the system is often the most important. What data needs to go into the system and how is this going to be handled? What information needs to come out of the system how is it going to be formatted? If we know what needs to come out, we know what we need to put into the system. The program we select will need to appropriately handle the process. The system is built with control files, sample master records, and the ability to perform processes on a test basis. The system is designed to include appropriate internal controls and to provide management with the information needed to make decisions. It is a goal of an accounting information system to provide information that is relevant, meaningful, reliable, useful, and current. To achieve this, the system is designed so that transactions are entered as they occur (either manually or electronically) and information is immediately available online for management.\n- Once the system is designed, an RFP is created detailing the requirements and fundamental design. Vendors are asked to respond to the proposal, to provide demonstrations of the product, and to specifically respond to the needs of the organization. Ideally, the vendor will input control files, sample master records, and be able to show how transactions are processed that result in the information that management needs to make decisions. An RFP for the information technology infrastructure follows the selection of the software product because the software product generally has specific requirements for infrastructure. Sometimes, the software and the infrastructure is selected from the same vendor. If not, the organization must ensure that vendors will work together without ""pointing fingers"" when there is an issue with either the software or the infrastructure.\n- As the system is being designed, it is documented. The documentation includes vendor documentation of the system and, more importantly, the procedures or detailed instructions that help users handle each process specific to the organization. Most documentation and procedures are online and it is helpful if organizations can add to the help instructions provided by the software vendor. Documentation and procedures tend to be an afterthought but is the insurance policy and the tool used during testing and training—before launch. The documentation is tested during the training so that when the system is launched, there is no question that it works and that the users are confident with the change.\n- Before launch, all processes are tested from input through output, using the documentation as a tool to ensure that all processes are thoroughly documented and that users can easily follow the procedures: They know it works and that the procedures will be followed consistently. The reports are reviewed and verified, so that there’s no garbage in-garbage out. This is done in a test system not yet fully populated with live data. Unfortunately, most organizations launch systems before thorough testing, adding to end-user frustration when processes don\'t work. The documentation and procedures may be modified during this process. All identified transactions must be tested during this step. All reports and online information must be verified and traced through the audit trail so that management is ensured that transactions will be handled consistently and that the information can be relied upon to make decisions.\n- Before launch, all users need to be trained, with procedures. This means a trainer using the procedures to show each end user how to handle a procedures. The procedures often need to be updated during training as users describe their unique circumstances and the ""design"" is modified with this additional information. The end user then performs the procedure with the trainer and the documentation. The end user then performs the procedure with the documentation alone. The end user is then on his or her own with the support, either in person or by phone, of the trainer or other support person. This is before data conversion.\n- Data Conversion\n- Tools are developed to convert the data from the current system (which was documented in the requirements analysis) to the new system. The data is mapped from one system to the other and data files are created that will work with the tools that are developed. The conversion is thoroughly tested and verified before final conversion. There’s a backup so it can be restarted, if necessary.\n- The system is implemented only after all of the above is completed. The entire organization is aware of the launch date. Ideally, the current system is retained and often run in ""parallel"" until the new system is in full operation and working properly. With the current mass-market software used by thousands of companies and fundamentally proven to work, the ""parallel"" run that is mandatory with software tailor-made to a company is generally not done. This is only true, however, when the above process is followed, the system is thoroughly documented and tested, and users are trained before launch.\n- Online resources are available to assist with strategic planning of accounting information systems. Information systems and financial forms aid in determining the specific needs of each organization, as well as assigning responsibility to principles involved.\n- The end users and managers have ongoing support available at all times. System upgrades follow a similar process and all users are thoroughly apprised of changes, upgraded in an efficient manner, and trained.\n- Many organizations chose to limit the time and money spent on the analysis, design, documentation, and training, and move right into software selection and implementation. If a detailed requirements analysis is performed with adequate time being spent on the analysis, the implementation and ongoing support will be minimal. Organizations that skip the steps to ensure the system meets their needs are often left with frustrated end users, costly support, and information that is not current or correct. Worse yet, these organizations build the system three times instead of once.\nThis section has multiple issues. Please help improve it or discuss these issues on the talk page. (Learn how and when to remove these template messages)(Learn how and when to remove this template message)\nMany AIS professionals work for consulting firms, large corporations, insurance companies, financial firms, government agencies and public accounting firms, among other types of companies. With technological advancement, traditional accounting practice will shift to accounting information systems practice. Both accounting and information technology professional bodies are working on the new directions of accounting programs and industry practices. System Auditors is one of the top choices in the past two decades, they look at the controls, data processing, data integrity, general operation, maintenance, security and other aspects of all types of information systems used by businesses. A lot of the companies will deal with software and finding a software that is right for the company, or maintaining a software for a company. If you are interested in the career, you might have the choice of working in the financial department of any type of business, or of working with a financially oriented company or a programming-oriented company that specializes in AIS. Some job titles in this field of work include financial manager, financial examiner and chief financial officer. You could also become a computer systems analyst, a computer information systems manager or a computer software engineer or programmer specializing in financial software.\nIf you are working with a financially oriented company, your job duties could range from analyzing an AIS for data integrity to managing the entire AIS. In a programming-oriented company, your focus may be directed towards developing new software in AIS or fixing bugs in an AIS. In both cases, you may also have the option of consulting, which requires travelling to different companies to provide analysis and advice concerning the company\'s AIS.\nThere are industry associations offer certificates that related to AIS area include CISA, AIS, CISSP, CIA, AFE, CFE, and CITP.\n- ""Archived copy"". Archived from the original on October 18, 2011. Retrieved November 3, 2010.\n- Accounting Information Systems: Information on Collection, Storage and Processing of Financial and Accounting Data. Accounting Information Systems. Retrieved 7 December 2012.', 'SOX Compliance – A Continuing Challenge\nSOX Compliance in Recent News\nAlthough the Sarbanes-Oxley Act has been around since 2002, compliance remains a challenge. In a recent example, a Forbes article from this month explores reasons for Plantronics’ under-performance in the stock market and brings up concerns that Plantronics may be facing a Sarbanes-Oxley violation. The article mentions that the senior vice president of sales at Plantronics, “…was instructing employees who worked under him to delete e-mails that were clearly relevant and responsive to pending discovery having to do with the distributors that are at issue in this case.”\nIn our last several posts we’ve focused on HIPAA compliance. But of course HIPAA is only one of several areas companies need to keep in mind when it comes to compliance issues. Here’s a quick refresher on the Sarbanes-Oxley Act.\nIntent of the Sarbanes-Oxley Act\nThe intent of the Sarbanes-Oxley Act (also known as Sarbox or SOX) was to protect investors by improving the reliability and accuracy of corporate disclosures. It was enacted in 2002 and while it applies primarily to public companies, the act also contains provisions for private companies. The provisions have to do with the willful destruction of evidence to impede a Federal investigation.\nA major feature of the SOX Act is that it is designed to specify financial reporting responsibilities. This means that it should no longer be possible for CEOs and CFOs to claim that ignorance of financial issues means that they should not be held accountable for the accuracy of financial statements.\nAs a result of the SOX Act, leaders of an organization are held legally responsible for SOX compliance, facing possible monetary fines and imprisonment (up to twenty years) for failure to comply. Thus, even if a company’s IT department prepares SOX audit statements, those statements will need to be certified by the CEO.\nWhat are some of the ways SOX Compliance has affected companies since SOX was enacted?\n- Stronger audit committees and public companies, due to the act’s requirement that the audit committee members must be independent of top management.\n- Increased costs, especially due to Section 404 of the act. This section requires extensive internal control tests and reporting. As a result of these costs, many companies have seen a need to focus on making their financial reporting more efficient.\n- Strengthened public disclosure requirements.\n- Stricter penalties for obstructing justice and for securities fraud.\nWhat are the main IT concerns regarding SOX Compliance?\nMost aspects of IT are affected by SOX compliance. SOX regulations mean that audit trails must be retained and auditable for five years. Any IT operation that involves financial data or activity may be affected. All forms of communication regarding finance and accounting must be tracked and archived in case of compliance audits.\nWhat kinds of information does IT need to store with regard to SOX compliance, and how?\nGenerally speaking, all emails, spreadsheets, and documents used to arrive at final financial conclusions. For a more complete breakdown, see our post “What Does Your IT Team Need to Know about SOX Compliance.”\nLearn more about the SOX Act:\nOur previous posts regarding SOX Compliance:']"	['<urn:uuid:cb7e5629-2a66-47e1-818b-04323590768a>', '<urn:uuid:24265d84-e5ff-43f2-bf06-118b9361cfd9>']	factoid	with-premise	verbose-and-natural	distant-from-document	multi-aspect	expert	2025-05-12T21:31:42.699391	28	56	3067
32	home test kit cost radon repair comparison	Short-term radon test kits cost between $15-$30, while long-term kits are more expensive. In comparison, the average cost for professional radon mitigation by a contractor is about $1,200, with costs ranging from $500 to about $2,500.	"['Self-Testing for Radon in a Home Using Test-Kits\nAdd picture of a test kit…….\nHomeowners can test their home for radon in 2 ways. One is to hire professional services. Detailed information regarding this is available under the sub tab “Finding qualified professionals to fix radon in a home”.\nSecond option is to buy a test kit and perform the test on one’s own.\nRadon test kits are of two types: short-term (3-4 days) and long- term (3-12 months). The short-term kits are quite inexpensive and in the range of $15- $30. These provide quick results.The long term kits are more expensive but provide a better idea of average radon levels in a home. Short-term kits are the ones a homeowner should use when testing a home for the first time. EPA recommends “Whether you test for radon yourself or hire a state-certified tester or a privately certified tester, all radon tests should be taken for a minimum of 48 hours…”\nProper Testing Conditions (essential to getting accurate results)(EPA, 1993)\n1. Correct placement of test kit is important to get accurate results. The kit should be placed in the most often used room of a home and in the lowest occupied level of the home. This might be a bedroom, family room, living room or a play area. However, it CANNOT be a laundry room, kitchen or bathroom.\n2. The kit should be placed at least 20 inches above the ground and 4 inches away from any other object.\n3. It should be at least 3 feet away from doors, windows or any other “openings” to the outdoors.\n4. If the room has no doors or windows, the kit should be placed at least more than 1 feet away from the building’s exterior wall.\n5. If the kit is being suspended in the air, it should hang so as to be in the general breathing zone of most people i.e. 6-8 feet from the floor.\n6. The kit should not be disturbed during the entire testing period. It should also be kept away from heating and cooling vents, fan, direct sunlight, fireplace.\n3. The home must be closed 12 hrs before the test begins and for the entire duration of test. This means all windows of the home and doors should be kept closed at all times except when entering or leaving the home under normal circumstances.\n4. Minimum 48 hours testing is a must.\nShort-term or Long-term test?\nFor first time testing, short-term kits are ideal. These can provide a quick initial estimation of radon levels in a home. Long-term test provide results for average year round conditions. EPA recommends the levels to be below 4 pCi/L.\n- If the results are below 4 pCi/L no follow up action is needed.\n- If results find radon levels between 4 pCi/L to 10 pCi/L, EPA (1993) recommends a second follow-up test which could be either long-term or short-term. If the long-term test is done and its value is found at 4 pCi/L or higher, a mitigation action is recommended.\n- If results find radon to be above 10.0 pCi/l , a second short-term test is recommended by the EPA (1993). At higher levels it is desirable to take the mitigation (reduction) action quickly. Then waiting for a long term test is not seen as a good option.\nFor both steps 2 and 3 when a second short-term test is done, an average of the two short-term tests is taken and if this comes to be 4 pCi/L or higher, than mitigation actions need to be taken by the homeowner (EPA, 1993).\nProtocols for Radon and Radon Decay Product Measurements in Homes is an excellent guidance document published by EPA (1993) on how a home should be tested for radon. It can be accessed here. For quick reference, important aspects of home testing can be found here.\nWhere to get a short-term test-kit ?\n- The best option is to call one’s state radon office and find out if they provide the kits for free or at a discounted price under their radon program. January is National Radon Action Month and many state radon programs give away free test kits on first come basis to the residents.\n- If not, these can be purchased online for a discounted price under the National Radon Program Services housed in Kansas State University.\n- Lastly, the kits might be available at the local home improvement stores and purchased from there.\nNote: If a person is buying or selling a home, it is recommended that professional services be used for testing rather than the self test kits. Here is a good video by U.S EPA for home buyers and sellers providing useful information on what either party should know about radon before a home transaction', 'What are the Health Effects of Radon?\nExposure to radon is the second leading cause of lung cancer in the U.S. Radon can be inhaled into the lungs, where it undergoes radioactive decay. As it decays, radon releases tiny bursts of energy called alpha particles, which can harm sensitive lung tissue by damaging the DNA. This damaged DNA can lead to lung cancer.\nHow is Radon Measured?\nRadon is measured in picocuries per liter of air (pCi/L), a measurement of radioactivity. The U.S. EPA and the Centers for Disease Control and Prevention recommend that homes with radon levels 4 pCi/L, or greater, be fixed.\nHow do I Find Out if My House has Elevated Levels of Radon?\nRadon test kits that meet EPA guidelines can be obtained from the Mineral County Health Department at (304)788-1321 FREE OF CHARGE. If additional assistance is needed, a trained professional from our office can be requested to perform the onsite test.\nThe cost of making repairs to reduce radon depends on how your home was built and other factors. Most homes can be fixed for about the same cost as other common home repairs, like painting or having a new hot water heater installed. The average cost for a contractor to lower radon levels in a home is about $1,200, although this can range from $500 to about $2,500.\nHow Can I Fix My House if it has Elevated Levels of Radon?\nA variety of methods can be used to reduce radon in homes. Sealing cracks and other openings in the foundation is a basic part of most approaches to radon reduction. EPA does not recommend the use of sealing and caulking alone to reduce radon because, by itself, sealing has not been shown to lower radon levels significantly or consistently. In most cases, systems with pipes and fans are used to reduce radon. Such systems are called “sub-slab depressurization”. These systems prevent radon gas from entering the home from below the concrete floor and the foundation. Similar systems can also be installed in homes with crawl spaces. Radon reduction contractors may use other methods that may also work in your home, depending on its design and other factors. Look in the Yellow Pages or call your state radon office to locate radon mitigators in your area.\nDispelling Some Common Radon Myths\n“I don’t have a basement, so I probably don’t have a radon problem.”\nRadon can seep in from soil anywhere around or under a home, regardless of whether your home has a basement, a crawl space, or is built slab-on-grade. The U.S. Environmental Protection Agency and the Surgeon General recommend radon testing for all types of homes. In multi-level homes testing should be done on a level below the third floor.\n“I don’t live in an area designated as a high radon zone, so my home won’t have a problem.”\nThe U.S. EPA and the U.S. Geologic Survey conducted surveys of radon potential across the United States. They broke the country down into three zones according to their potential for high indoor radon levels, with Zone 1 having the highest radon potential. It is true that homes in Zones 1 and 2 have a statistically higher chance of having elevated levels of radon. However, the fact is that elevated levels of radon have been found in homes in all fifty states. The radon level in your home depends on the geology under and near your home. The only way to know for sure, and to protect your family from radon, is to test your home.\n“Two of my neighbors have tested their homes for radon and they don’t have high levels, so I probably don’t either.”\nRadon levels can vary considerably from house to house, even on the same street. It is nearly impossible to predict the exact nature of geologic soil deposits and the extent to which soil gasses will seep into and be retained by a specific house. The only way to know whether radon exists in elevated levels in your home, and to protect your family from radon, is to test.\n“There doesn’t seem to be much proof that radon is a serious health problem.”\nThe science on radon has been formidable over the years, but never before have we had such overwhelming scientific consensus that exposure to elevated levels of radon causes lung cancer in humans. In February of 1998, the National Academy of Sciences (NAS) presented the findings of their Biological Effects of Ionizing Radiation (BEIR) VI Report: ""The Health Effects of Exposure to Indoor Radon."" This new report by the NAS is the most definitive accumulation of scientific data on indoor radon. The report confirms that radon is the second leading cause of lung cancer in the U.S. and that it is a serious public health problem. The study fully supports U.S. EPA estimates stating that radon causes between 15,000 and 22,000 lung cancer deaths per year.\n“I don’t have time to test for radon!”\nTesting is as simple as opening a package, placing a radon detector in your home in a designated area, and, after the prescribed number of days (typically two days), sealing the detector back in the package and mailing it to a lab. The whole process only takes a few minutes of your time!\nWhat is Radon?\nRadon is a naturally occurring, invisible, odorless gas that comes from deposits of uranium in soil, rock, and water. It is harmlessly dispersed in outdoor air, but when trapped in buildings, can be harmful, especially at elevated levels. Radon is a radioactive decay product of radium, which is itself a decay product of uranium. Uranium and radium are both common elements in soil.\nWhere is Radon Found?\nThe primary source of high levels of radon in homes is the surrounding soil. Radon has been found in elevated levels in homes in every state, and the U.S. Environmental Protection Agency estimates that as many as one in 15 homes across the U.S. have elevated radon levels.\nHow Does Radon Get Into My House?\nWarm air rises. When this happens in your home, it creates a vacuum in the lower areas of the house. Nature hates a vacuum, so something must rush in to fill it. In the case of your home, air seeps in from the soil around and under the house, and some air is sucked in through openings (cracks, doors, windows) on the lower levels. Radon gas enters the same way air and other soil gases enter the home; through cracks in the foundation floor or walls, hollow-block walls, and openings around floor drains, pipes and sump pumps.']"	['<urn:uuid:34ad3dad-39e5-43d0-8835-75c2f4cdaae0>', '<urn:uuid:0b6a5ad6-ffd6-4bdd-be09-c2102fab395f>']	factoid	direct	short-search-query	distant-from-document	multi-aspect	expert	2025-05-12T21:31:42.699391	7	36	1904
33	I'm thinking of building an extension to my house, but I'm worried about accidentally building over my neighbor's property line. What should I do to avoid any legal issues?	To avoid building over property lines, you should first check the official town planning diagrams kept by the local authority. You can refer to the title deeds of your property and your neighbor's to find the plan numbers. If you and your neighbor cannot agree on the boundary line, or if you cannot find the property boundary pegs, you should hire a registered land surveyor to re-determine the boundary. It's important to know that in the absence of a servitude, you may build only on your own ground. If you do accidentally build on your neighbor's property (encroachment), they could demand demolition of the encroaching portion or payment of compensation.	"[""Know your rights: Boundary walls\nAs a property owner, you are entitled to enjoy, use, consume, convert, alter, destroy or sell your property and what the land produces in any way you please within the limits of state and local authority regulations, provided you do not interfere with the legal rights of others, including your neighbours' rights to the same enjoyment of their property.\nThe boundaries between your properties mark where your rights end and your neighbours' begin.\nHaving a knowledge of each others' rights and obligations is the first step in establishing a good relationship with your neighbours and enjoying your rights to the full.\nBoundaries between properties are recorded in official town planning diagrams kept by the local authority in control of the area. In the event of a dispute over the dividing line between two properties, the first step is to check the official property plans. Refer to the title deeds of your property and your neighbour's to ascertain the plan numbers and with which deeds the plans are filed.\nIf you and your neighbour are unable to come to an agreement, even with the aid of the plans, or if you cannot find the property boundary pegs, you should ask a registered land surveyor to re-determine the boundary.\nIn the absence of a servitude, an owner may build only on his or her own ground.\nA builder who departs from the plans when building on your property might be guilty of encroachment - the building of part of your house on the property of a neighbour. The neighbour could demand demolition of the encroaching portion or the payment of compensation for the use of part of his or her land. A court might rule that demolition would be unreasonable (for example, because the cost of removing the structure exceeds the benefit to your neighbour of removing it) and direct you to take transfer of that part of your neighbour's property on which the building has encroached. As a property owner on whose property a neighbour has encroached, your right to insist on removal may, however, be lost if you do not demand removal within one year and one day of becoming aware of the encroachment. Alternatively, the court may, in its discretion, simply award compensation to you for the encroachment.\nA further legal remedy available to a person whose land has been encroached upon is to eject the encroaching neighbour and to retain the structure erected on the land against payment of compensation. This remedy may, however, be invoked only if the encroaching structure is independent of any structure on the land of the neighbour who built it, or can easily be separated from any such structure.\nBoundary (party) walls or fences\nIn the absence of proof that a boundary wall, fence or hedge is entirely on one of two adjoining properties, it is presumed to be half on one property and half on the other. Some legal authorities state that each part is separately owned by the owner of the property on which it stands, but that there are reciprocal servitudes of support. Other authorities state that the wall is owned jointly by the owners of the adjoining properties. The law relating to such encroaching boundary walls reflects the influence of both theories. An owner who transfers his or her property, automatically transfers the joint ownership. Neither owner may, without the consent of the other remove, raise or lower the boundary wall or tamper with it in any way except in an emergency, although in terms of common law a neighbour is allowed to break down a wooden fence and replace it at own cost with a more expensive partition.\nEither owner may re-erect a boundary wall destroyed by an act of God, such as fire or flood; the other owner would have to contribute half the cost - if he or she will derive any benefit from it. Each owner is obliged to contribute to the maintenance and repair of the wall, although an owner can refuse to contribute to the cost of an unreasonably expensive new wall. Moreover, an owner is under no obligation to replace with a similar structure a boundary wall that was unreasonably expensive when it was originally erected.\nIt is best to come to an agreement with your neighbour on the amount to be spent on the wall. Although both of you would be entitled to reasonable use of the boundary wall, fence or hedge, this right does not include reducing its strength or making it unstable. It does, however, include improving and altering the appearance of the side that fronts your property. Subject to local-authority regulations, you may use your side of the boundary wall as support for a beam or for water pipes. If it is strong enough, you may even build on it.\nThe Fencing Act, 1963\nThis regulates the duties of landowners with regard to boundary fences in agricultural areas. The Act divides the country into proclaimed areas in which contributions to the cost of boundary fences are either obligatory or non-obligatory. If you are not sure in which type of area you live, ask your local authority.\nIf you farm in an obligatory area and wish to erect a boundary fence, you have a right to demand a contribution towards the costs from landowners whose properties border the land you wish to fence in. You must send your neighbours written notice of your intention to erect a fence, stating the type and purpose of the fence, the estimated cost of erecting it and the contribution required, based on the value or benefit of the fence to them. If none of them object within one month, you may erect the fence and claim a contribution from all to whom you gave the notice. (When a neighbouring owner cannot be traced or is absent from South Africa, you can hand the notice to his or her authorised representative; alternatively, you can publish it in the government gazette and, three times, in a local newspaper. Your neighbour will then have three months in which to object.\nShould your neighbour object, by expressing the wish, perhaps, to erect a different type of fence, the dispute, in the absence of agreement between neighbouring owners, will then have to be settled by a board of three people specially constituted for that purpose. Each disputing party may appoint one member to the board, and the two members so appointed nominate a third person to decide the dispute. If one of the neighbours fails to appoint a member or if the two members appointed by them fail to appoint a third member, the chief magistrate of the district in which the land in question is situated may appoint a member. The third member acts as chairperson of the board and the decision of the majority constitutes the decision of the board, which may be enforced as if it were a judgment of the magistrate's court of the area. Alternatively, if both neighbours agree, a board consisting of only one member may decide the dispute between them.\nIf a landowner in an obligatory area wishes to jackal-proof an existing fence but the neighbour does not agree to do so, the landowner who wants the jackal-proof fence will have to carry the full cost, unless the Minister of Agriculture has declared the area to be obligatory in respect of the construction of jackal-proof fences.\nIn non-obligatory areas, a contribution can only be asked from a neighbour who is going to use the fence or is going to derive some other benefit from it\nSay, for instance, you want to erect a fence to prevent your sheep from straying onto a neighbouring farm that is devoted entirely to the production of honey, your neighbour will be perfectly entitled to refuse to contribute since he or she will derive no benefit from it. If, on the other hand, there is an obligation to contribute to the cost of the fence and the amount is in dispute, a board may be appointed to decide the matter in the same way as it would a dispute between owners of adjoining land in obligatory areas.\nIf the cost of erecting or contributing towards a boundary fence is greater than R40, an advance can usually be obtained from the Land and Agricultural Bank of South Africa.\nWhere adjoining owners share a single driveway to their garages, the plans will show who owns the driveway and who, by right of servitude, may use it.\nIt is possible that the driveway may be owned either in undivided shares or that each owner owns a specified half and is entitled to the use of the other half.\nEven the sole owner of the driveway has no right to obstruct your access to a garage on your property. If an obstruction to your driveway is in a public place, ask the police to take action; if the obstruction is in the driveway itself on private land, do not, under any circumstances, take the law into your own hands by removing it. If the owner refuses to remove it, ask your attorney to write him or her a letter. If the matter still cannot be resolved, you will have to approach the court for an order to compel the owner - or to allow you - to remove the obstacle.\nPaying for repairs\nTitle deeds might make it clear who is responsible for repairing shared driveways, walls or fences. A servitude might make a single owner entirely liable for the costs of upkeep. If your neighbour refuses to make essential repairs, anyone who is entitled to enforce the servitude may obtain an estimate of the cost of repairs. It is unlikely that a contractor will undertake the work if it is known that the person responsible for payment is unwilling to pay. The servitude holders may therefore have to pay the contractor themselves and present the owner with the bill. The owner of a wall or driveway who is not bound by servitude is normally under no obligation to carry out repairs. However, in the case of deterioration that is likely to prove dangerous to the public, a local authority might order the owner of the property in question to carry out the necessary repairs. In any event, the owner will be liable for damage to your property caused by known defects (or defects that should have been obvious) - for example, if a wall collapses and damages your property.\nIf the title deeds do not specify that repairs are the responsibility of one owner, it is best for neighbours to agree to share costs. This should be done by an exchange of letters setting out what work is to be undertaken and how the costs are to be shared; alternatively, an attorney could draw up an agreement to be signed by the parties involved.\nOverhanging branches and intruding roots\nIf branches of a tree growing on an adjoining property overhang your property, you may ask your neighbour to saw them off and remove them from your property. If your request is refused, you may saw the branches off yourself to the extent that they overhang, although you may not keep them unless your neighbour refuses to collect them. You may then recover from your neighbour the reasonable expense of removing the branches. Alternatively, you may force your neighbour to remove the offending branches by obtaining an interdict, compelling him or her to do so.\nThe law with regard to fruit on overhanging branches is not clear. Some legal writers say that you are entitled to pick fruit from the overhanging branches and keep it, while others state that you may take fruit only if it has fallen off the tree by itself. The matter awaits a decision by the Supreme Court.\nUsually you can't complain if leaves from a neighbour's tree fall onto your property. The planting of trees is held to be a natural and ordinary use of land, however close to a boundary they may be. Even if damage is caused by leaves from your neighbour's trees blocking your gutters, you cannot claim damages, since it has been held in court that damage of this kind can be prevented by routine cleaning of the gutters, a normal and inexpensive act of maintenance. If leaves, branches, flowers or fruit fall from your neighbour's tree onto your property, you cannot force your neighbour to remove them. You may demand the removal of roots that encroach on your property, whether beneath the ground or on the surface, or you may remove them yourself. If the roots have damaged your property, you may insist that they be removed and you may claim compensation for the damage they caused.\nUnless restricted by a servitude, your neighbour may erect buildings or grow trees on his or her property that obstruct your view or block out the sunlight that you would otherwise receive.\nThere is, however, an important limitation upon the right of a neighbour to build structures or grow trees that block out your light or create a mess on your property: if your neighbour does so, not in the enjoyment of his or her own property, but rather with the improper motive of causing a nuisance to you, you may be entitled to claim compensation or even to obtain an interdict compelling him or her to remove the offending structures or trees.""]"	['<urn:uuid:9395d077-3447-41d9-8c07-73cbab7b6929>']	open-ended	with-premise	verbose-and-natural	similar-to-document	single-doc	novice	2025-05-12T21:31:42.699391	29	110	2245
34	What was King Songtsen Gampo's impact on Tibet?	King Songtsen Gampo made significant contributions to Tibet in the 7th century. He built the sacred Jokhang temple in 652 AD to house Buddhist statues brought by his two wives - Princess Wencheng from the Tang dynasty and Princess Bhrikuti from Nepal. As the 33rd king of Tibet, he unified Tibet, established Lhasa as the capital, created Tibetan language, developed laws, and established the Tubo regime. His marriage to Princess Wencheng particularly enhanced cultural exchange between Tibetans and Han people. His tomb, part of the Tombs of Tibetan Kings, is located in Qonggyai County and contains an underground palace with religious statues and precious artifacts, attracting many pilgrims.	['Revering by walking, prostrating, chanting, offering…\nOfferings – of objects or one’s presence – affirms the Tibetan’s reverence of sacred places and objects.\nThresholds can be the shift into a physical structure or rituals for new events in a lifetime. Thresholds are the crossing into another existence whether physical, emotional, or spiritual… or as in these sacred places, the combination of all three.\nJokhang – the heart of Tibetan pilgrimage\nTibetans think of the Jokhang as the “spiritual heart of Lhasa” and it does sit in the middle of the Barkhor, the market square of old Lhasa. More importantly, it is the most sacred and important temple in Tibet.\nKing Songtsen Gampo (traditionally the 33rd king of Tibet) began to build the temple in 652 AD to house the many Buddhist statues brought as dowry by his two brides: Princess Wencheng of the Chinese Tang dynasty and Princess Bhrikuti of Nepal, who both helped him establish Buddhism in Tibet. The most important statue is the Jowo, an image of the twelve-year-old Buddha. The Jokhang was enlarged many times and the scene of many important events in the establishment of Buddhism in Tibet.\nIn the morning there were thousands of Tibetans in a long queue to visit the inside of the Jokhang while others did prostrations and circled this most important temple.\nSpiritual and social… taking a break from prostrations\nStarting young… her mom showed her how to do the prostrations\nThe police managing entry to the Jokhang let tourists in by another door without the long queue. Most of the other tourists were Chinese… everywhere we went.\nThe queue was long but people were patient. Many people carried large thermoses full of melted butter to add to the huge butterlamps by the main Jowo statue.\nThe murals on the walls were fantastic, but no photos were allowed past this doorway.\nThis old carving on a stone block seemed old. Some of the temple is about 1,300 years old and some has been refurbished, like on the roof.\nWe could not see signs of the fire on Feb 17, the day after Losar this year. But, barriers and security guards limited how far we could wander on the rooftop.\nMost tourists were Chinese. An elderly Tibetan man I knew from past visits said that recently published statistics on visitors to the Potala in the previous month were 34 Tibetans, 5,000 Chinese, and 17 foreigners. We did not visit the Potala on this trip. While I waited for the group, a young Chinese woman started talking to me here on the Jokhang roof. I asked her what attracted her to visit here… “pure, clean land and very faithful people” It seemed that she had not heard much else about what has happened in Tibet.\nShigatse and Tashilunpo\nTashilunpo is a relatively newer monastery in Shigastse to the west of Lhasa. It was founded in 1447 and sacked by the Gorkha Kingdom of Nepal in 1791. The Nepalis were eventually driven back almost to Kathmandu. The monastery once had over 4,000 monks but we could not find our how many are there now.\nThe worn flagstones… what history has passed over them.\nThe monks gathering for evening prayers\nThe threshold into the main prayer hall.\nAvalokiteshvara or Chenrezig in Tibetan embodies the infinite compassion of all the Buddhas and completely devoted to helping others until all being achieve liberation.\nThere are four main gompas (temples) in the monastery.\nThe representation of the spiritual character of a previous lama.\nWe were visiting in Saka Dawa, the sacred month commemorating the Buddha’s birth and enlightenment. Local people filled the area around the three stupas at Tashilungpo as they walked the kora, rested, ate, and visited.\nThe kora path was like a river of devotion.\nToddler sleeping on a bench.\nMany of the entry ways just before the thresholds at Tashlungpo had diagrams of inset turpqoise and other stones perhaps as extra symbols of one’s entry into sacred spaces.\nThis is an ancient symbol represents continuity and good fortune. Unfortunately, its reverse was stolen for use by the Nazis in the 1930s.\nPhotos on the roof of the Jokhang 1990s\nPrayers and teachings on the Jokhang roof 1990s\nMonks debating on the Jokhang roof 1990s', 'The well-known dynasty-Tubo Dynasty in Tibet history emerged from the 7th century to 9th century. The leader of this time is the world-famous king Songtsen Gampo who achieved some great courses including unified Tibet and set Lhasa as the capital, took a series of measures to develop production, created Tibetan languages, made laws and built the Tubo regime of slavery. In order to introduce advanced culture of the Central Plains, he married Princess Wencheng of Tang Dynasty and sent Tibetans to learn there, which greatly pushed the economic and cultural communication between Han nationality and Tibetan nationality. Tibetan King Songtsen Gampo has made a great contribution to promoting social economic and cultural development of Tibetan areas, enhancing mutual friendships between Tibetans and Han people as well as the formation and development of multi-nationalities country of China.\nAccording to some Tibetan historical records, there were 35 generations of Tibetan kings in total during the Tubo Period in Tibetan history and the place that they were buried was known as the Tombs of Tibetan Kings today, which included the tombs of their wives and maybe some ministers as well. The Tombs of Tibetan Kings are located within the territory of Qonggyai County, Shannan Prefecture, Tibet Autonomous Region, a burial group for the 29th Tibetan King to the 40th generation, their ministers and wives also, covering a total area of 385 square meters, among which the tomb of King Songtsen Gampo attracted the most attentions of modern people. It is also the extant largest grave group till now, located in the south shore of Yarlung Zangbo River, southwest of Zongshan where Qonggyai County is situated. In the vicinity, it is in great natural condition involved expansive ground, pleasant weather, fertile land, and beautiful mountains and rivers. That’s the reason why it was the birthplace of ancient Tibetan people a long time ago. Moreover, it is where Tibetan King Songtsen Gampo originally built up his kingdom. Since setting Lhasa as the capital, this place has become the base of Tubo Dynasty, been highly appreciated as a result. In addition to the natural conditions here, King Songtsen Gampo might also have nostalgia towards his homeland, which led to the present tomb location. Since then, a lot of Tibetan kings were buried here. There wasn’t a fixed tell of how many tombs were there. Due to annual soil erosion and sediment accumulation, several tombs situated on hills have been hard to recognize from those hills. But till today, there remained 9 tombs that can be recognized in total, covered a circumference of 3 km. The existing 9 tombs are slightly different, with some of them formed circular shape and flat top in various sizes and irregular arrays.\nOn the basis of inscription records, Songsten Gampo mausoleum is located at the mouth of Cimpu Gully, square-shape in external form. The epitaph said his mausoleum was divided into nine sections while the Buddha temple lied as main chamber. In the center was placed burning coral lamp and in the four corners stood treasure storage palaces. The inner walls were constructed by square stones while the outer ones were thick soil walls with the door of the tomb towards west. Looked from the outside, the tomb was covered with gravels, coming into being an earth hummock which was about a hundred steps’ wide and more than six Zhang high.\nTibetan legend has it that under this tomb was situated a grand underground palace where preserved the statues of king Songtsen Gampo, Sakyamuni, Avalokiteshvara and a large number of household utensils inlaid with gold, silver and agate, etc. as well as armors and weapons. In the past, there was a shrine in center top of Songtsen Gampo’s grave worshipped the king, his wives together with founder of Tibetan language. However, this original construction had been mutilated because of age but has been repaired the shrine and statues inside, attracted a constant stream of pilgrims coming here to worship and show respects to this great Tibetan king who was active in the development of Tibetan and solidarity of Tibetans and Han ethnics.\nIn front of the tombs of Tibetan kings, there are two stone steles of Tubo Period. What is next to the tomb of king Songtsen Gampo is said to be the tomb of Sadnalegs where one the two stone tablets was situated nearby. This stele is 83 cm in bottom width and 78 cm in the lower width, engraved flying dragon patterns on both sides. The lower part of it had been covered under quicksand, left only 2.3 meters high on the ground. Local relevant departments once dug it deep to 3 meters but still didn’t found pedestal of the tablet. The tombs of Tibetan kings are not only a reflection of burial system and burial level over a thousand years ago, but also valuable materials for studying the flourishing and decline of Tubo Dynasty.']	['<urn:uuid:865a014e-f8c1-4f14-bf9b-44213562264b>', '<urn:uuid:07b1f983-6d71-4fa1-a52b-52a4b2977ddd>']	open-ended	direct	concise-and-natural	similar-to-document	three-doc	novice	2025-05-12T21:31:42.699391	8	108	1525
35	how tell if math limit exists or not exists	A limit exists only if three conditions are met: 1) the left-hand limit exists, 2) the right-hand limit exists, and 3) both limits are equal to each other. If any of these conditions fails, or if the function exhibits one of these three behaviors - oscillates infinitely, increases/decreases without bound, or has unequal right- and left-hand limits - then the limit does not exist.	"['Master AP Calculus AB & BC\nPart II. AP CALCULUS AB & BC REVIEW\nCHAPTER 3. Limits and Continuity\nTECHNOLOGY: EVALUATING LIMITS WITH A GRAPHING CALCULATOR\nYou can sum up the whole concept of limits in one statement: A limit is a height toward which a function is heading at a certain x value. With this in mind, a graphing calculator greatly simplifies the limit process. Many times, the function you are given is bizarre looking, and its graph is beyond the grasp of mere mortal men and women in the time allotted to answer an AP question. The majority of these limit questions appear on the non-calculator portion of the AP test, forcing you to use the substitution, factoring, and conjugate methods to reach an answer. However, evaluating limit problems will sometimes seep into the calculator-active section like a viscous, sticky goo. In these cases, limits are no match for you at all, as the calculator affords you numerous tools in your dual quests for a 5 on the AP test and peace in the universe.\nALERT! Remember that your calculator can only be used to approximate limits. Using the calculator to approximate a limit may not be acceptable on a free-response question. This method is only your last resort!\nExample 12: Evaluate using your calculator.\nSolution: You solved this problem in an earlier exercise using the conjugate method (which works just fine) and got —1/6. Graph it on your calculator—it looks almost like a straight line (but it’s definitely not linear).\nUse the [2nd] → [Trace] → “value” command on your calculator to find the value of the function at x = 9. The corresponding y value should come out blank! This makes sense, because 9 is not in the domain of the function. Therefore, substitution does not work. However, we can use the calculator to substitute a number very close to 9. This value is a good approximation of the limit for which you are looking. Again, use the [2nd] → [Trace] → “value” function of your calculator to evaluate the function at x = 8.9999.\nThe calculator gives a limit of x = —.1666671. Using other x values even closer to 9 (e.g., 8.99999999999), the approximation looks more like —.16666666667, which is approximately —1/6, the exact value we received from using the conjugate method.\nExample 13: Show that using your calculator.\nSolution: Using the rule for rational functions at infinity, the limit is clearly 3/5 as the degrees of the numerator and denominator are the same. The graph certainly appears to approach that height as x approaches both ∞ and —∞.\nTo verify this numerically, press [Trace] and repeatedly press the right arrow key. The function’s height will slowly get closer to 3/5 = .6.\nAnother option open to you is the [2nd] → graph (or [Table]) function of the calculator, which lists function values quickly.\nClearly, the function approaches a limiting height of .6 as x approaches ∞.\nDirections: Solve each of the following problems. Decide which is the best of the choices given and indicate your responses in the book.\n1. If exists, what must be true?\n2. Given find the value of b that ensures p is nonremovably discontinuous only once on (—∞,∞).\n3. What conditions must be met if f(x) is continuous at x = c?\n5. Design two functions, g(x) and h(x), such that g(x) + h(x) = 4x2 + 3x + 1 and\n6. What three function behaviors prevent a limit from existing?\n7. What value of a makes k continuous if\n9. If d(x) is defined by the graph below, answer the following questions:\n(f) List all x values where d is discontinuous.\n(g) Which of your answers to part (f) represents removable discontinuities, and why?\n(h) What value(s) of β make the following statement true?\n11. Give the equations of the horizontal and vertical asymptotes of if a and b are real numbers.\n13. The population, v, of the bacteria Makeyoucoughus hurtyourthroatus is modeled by the equation y = 50e.1013663t, where t is days and y is the number of colonies of bacteria. Use the Intermediate Value Theorem to verify that the bacteria will reach a population of 100 colonies on the time interval [4,7].\n14. If f(x) and g(x) are defined by the graphs below, evaluate the following limits (if possible):\n15. Find the values of c and d that make m (x) continuous if\n16. James’ Diabolical Challenge Problem:\nGiven j(2) = 1 and j(x) is everywhere continuous, find A and B.\nANSWERS AND EXPLANATIONS\n1. The three conditions for a limit to exist are: (1) exists, (2) exists, and (3) they are equal.\n2. The denominator of p factors to (2x — 3)(x + 5), so p will be discontinuous at x = 3/2 and —5. If the numerator contains one of these factors, our goal will be achieved. It makes good sense to force (x + 5) to be a factor, rather than (2x — 3) because the leading coefficient of the numerator is 1. (In other words, the numerator will factor into (x + A)(x + B) and (x + 5) is of this form.) To find b, we factor the numerator using (x + 5) as one of the factors:\nSubtract x2 from each side and factor to get\nThus, 5+^1 (the coefficient of x on the left side of the equation) must equal 4 (the coefficient of x on the right side of the equation).\nTherefore, (x — 1) is the remaining factor. Thus,\nBecause (x + 5) is a factor of the numerator and denominator, exists, and the discontinuity there is removable. The discontinuity at x = 3/2 has no limit because which indicates a vertical asymptote (essential discontinuity).\n3. If f(x) is continuous at x = c, then (1) exists, (2) f(c) exists, and (3) the two are equal.\n4. Multiply the binomials in the numerator to get Because this is a rational function evaluated at infinity, you can use the shortcut method of examining their degrees. Because the degree of the denominator exceeds the degree of the numerator, the function’s limit at infinity is 0. Don’t be confused because x→ —∞. Remember that a rational function approaches the same limits as x→∞ and —∞.\n5. Because is a rational function and we know that the leading coefficients of g(x) and h(x) must be in the ratio 3:1. One possible g(x) is 3x2 + 2x + 1. The matching h(x) would have to be x2 + x. Notice that the sum of the two functions is 4x2 + 3x + 1, as directed, and There are numerous possible answers, but they will work in essentially the same fashion.\n6. If a function oscillates infinitely, increases or decreases without bound, or has right- and left-hand limits that are unequal, the function will not possess a limit there.\n7. Special Limit Rule 4 gives us that Thus, and the first condition of continuity is met. Notice that k(0) = α, and (for k to be continuous) a must equal that limit as x approaches 0. Therefore, α = 0.\n8. Use Special Limit Rule 1 to simplify the fraction as follows:\nThis is a rational function being evaluated at infinity with equal degrees in the numerator and denominator, so the limit is equal to\n9. (a) 0\n(b) -1: Remember the key is that you are approaching —1 from the right, not to the right.\n(d) 2: even though f(2) = —1, the function leads up to a height of 2 when x = 2\n(f) x = —3, —2,2,4\n(g) x = 2: If you redefine d such that d(2) = 2 (instead of —1), then the limit and the function value are equal to 2 and d is continuous there. No other discontinuities can be eliminated by redefining a finite number of points.\n(h) β = —3,∞: d increases without bound as you approach 3 from the left and the right; the graph also increases without bound as x approaches ∞.\n10. You don’t have to use special limit rules here—substitution is possible. The answer is sin 2/2, or .455.\n11. Factor the fraction fully to get\nThe denominator of g will equal 0 when However, the numerator will not equal zero simultaneously. Thus, vertical asymptotes are present, and the equations for the vertical asymptotes are x = —b/3a and x = b/2a. To find the horizontal asymptotes, note that the numerator and denominator have the same degree, and find the limit at infinity.\nFrom this, you know that the horizontal asymptote is y = 1/2.\n12. It’s the revenge of the conjugate method! Multiply the numerator and denominator of the fraction by the conjugate of the expression.\nBecause the degrees of the numerator and denominator are both 1 (since √x2 = x), you find the limit by taking the coefficients of the terms of that degree and ignoring the rest of the problem (just as you’ve done in the past with rational limits at infinity).\n13. The population at t = 4 is approximately 75, and the population at t = 7 is approximately 101.655. The Intermediate Value Theorem guarantees the existence of a c ∈ [4,7] such that f(c) = 100.\n14. (a) No limit: does not exist, so f(x) — 3 g(x) cannot have a limit.\n15. The graph will begin as a semicircle of radius 2 centered at (—3,0), will become a line (since cx + d is linear) between the x values of —1 and 3, and will end as the graph of √x shifted to the right 3 and up 4. To find the first point on the linear section, plug —1 into the first rule: The line will begin at point (—1,0). The line will end at The focus of the problem, then, is to find the equation of the line that passes through (—1,0) and (3,4). Find the slope of the line and use point-slope form to get\nTherefore, the correct c and d values are both 1. The solution is further justified by the graph of m.\n16. To begin, factor the function and use the fact that j(2) = 1.\nSince the function is continuous, must be equal to f(4). So, you get f(4) = 4 — 1 = B (according to the given information); B = 3.\nSUMMING IT UP\n• Remember that the limit of a graph at x = c is the height that the graph reaches at x = c.\n• Limits answer the question “Where is a function heading?”\n• Only point discontinuities are removable, because a limit exists.\n• Rational, polynomial, radical, exponential trigonometric, and logarithmic functions are always continuous at all points in their domain. To help remember this, use the mnemonic device\n• Red Parrots continuously Repeat Everything They Learn.', ""Let's consider the following expression:\nAs n gets larger and larger, the fraction gets closer and closer to 1.\nAs n approaches infinity, the expression will evaluate to fractions where the difference between them and 1 becomes negligible. The expression itself approaches 1. As mathematicians would say, the limit of the expression as goes to infinity is 1, or in symbols: .\nAn interesting sequence is As gets bigger (in symbols , we have smaller values of , for\nand so on. Clearly, can't be smaller than zero (for if we have that is less than zero). The we may say that . Continuing with this sequence, we might want to study what happens when gets near to zero, and later what happens with negative values of going near to zero. Usually, the letter is reserved for integer values, so we are going to redefine our sequence as . If we take a sequence of values of , say\nWe see that the respective values of grows indefinitely, for\nIn this case, we might say that the limit of , in words, the limit of as goes to zero from right (as the sequence of values of goes to zero from the left in a graphic) diverge (or tends to infinity, or is unbounded, but we never say that it is infinity or equals infinity). Other case happens if we study sequences of values of such that every element of the sequence is lower than zero, the sequence is increasing but never exceeding zero. One example of such sequence is:\n- , with\n- , with\n- , with\n- , with\n- , with\nThe values of decrease without bounds. The we say that , or that tends to minus infinity when goes to zero from left.\nFor some limits (if the function is continuous at and near the limit), the variable can be replaced with its value directly:\n(with not equal to 0)\nOthers are somewhat more complicated:\nNote that in this limit, one may not immediately set equal to because this would result in the expression evaluating to\nwhich is an undefined expression. However, one may reduce the expression by separating the terms into separate fractions (in this case, and ), which can be evaluated directly.\nRight and Left Hand Limits\nSometimes, we want to calculate the limit of a function as a variable approaches a certain value from only one side; that is, from the left or right side. This is denoted, respectively, by or . If the left-hand and right-hand limits do not both exist, or are not equal to each other, then the limit does not exist.\nThe following limit does not exist:\nIt doesn't because the left and right handed limits are unequal.\nNote that if the function is undefined at the point where we are trying to find the limit, it doesn't mean that the limit of the function at that point does not exist; for an example, let's evaluate at .\n|Left-hand limit||Right-hand limit|\nSome Formal Definitions and Properties\nUntil now, limits have been discussed informally but it shouldn't be all intuition, for we need to be sure of certain assertions. For an example, the limit\nWe have seen that the function decreases as increases, but how do we guarantee that there isn't a value , say\nsuch that is never smaller than ? If there is such , we might want to say that the limit is , not zero, and we can't test every single possible value of (for there are infinite possibilities). We must then find a mathematical way of proving that there isn't such , but for that we need to define formally what a limit is.\nRight Limits, Left Limits, Limits and Continuity\nLet be a real valued function. We say that\nif for every there is a such that, for every between and\nwhere is the absolute value of .\nThis is the formal definition of convergence from the left. It means that for each possible error bigger than zero, we are able to find a interval such that for all in that interval, the distance between the value of the function and the constant is less than the error.\nTODO: Graphics illustrating this.\nIn an analogous fashion, we say that\nif for every there is a such that, for every between and , .\nAnd to finish the necessary definitions,\nThis is a assertion that must be proved. First, lets study the behavior of near zero;\nwhere the arrow pointing right means implies. So, define the function\nIf , then . We have show how to find the delta in the definition of limit, showing that the limit of as x tends to zero is zero.\nIn fact, for any real number ,\nLets see how to construct a suitable function .\n- , then\nimplying that makes for any .\nFunctions with the property that\nare called continuous, and arise very naturally in the physical sciences; Beware that, against the intuition of most people, not every function is continuous.\nProperty of Limits\nProperty one: If , then\nfor any constant .\nProof: Construct the function for . Then\nThen the limit of is , for the delta function of is\nTODO: Demonstrate main properties of limit (unicity, etc)\nL'Hôpital's Rule is used when a limit approaches an indeterminate form. The two main indeterminate forms are and . Other indeterminate forms can be algebrically manipulated, such as .\nL'Hôpital's Rule states if a limit of approaches an intederminate form as approaches , then:\nBoth the numerator and the denominator approach zero as approaches zero, therefore the limit is in indeterminate form, so l'Hôpital's rule can be applied to this limit. (note: you can also use the Sandwich Theorem.)\nNow the limit is in a usable form, which equals 1.\nIf the limit resulting from applying l'Hôpital's Rule is still one of the two mentioned indeterminates, we may apply the rule again (to the limit obtained), and again and again until a usable form is encountered.\nWhere does it come from\nTo obtain l'Hôpital's Rule for a limit of which approaches as approaches , we simply decompose both and in terms of their Taylor expansion (centered around ). The independent terms of both expansions must be (because both and approached ), so if we divide both the and by (or, equivalently, find their derivatives), our limit will stop being indeterminate.\nIt could be the case that the Taylor expansions of both the numerator and the denominator have a as coefficient of the term, thus yielding an indeterminate. This is the same case mentioned above where the trick was to repeat the process until a suitable limit was found.\nThe case of a limit which approaches can be transformed to the case above by exchanging with , which obviously approaches .""]"	['<urn:uuid:5f54c15b-02af-4fc1-95ed-421ab2e636d4>', '<urn:uuid:0951054f-5282-45e8-b43a-ea21303ef884>']	factoid	with-premise	short-search-query	distant-from-document	multi-aspect	novice	2025-05-12T21:31:42.699391	9	64	2954
36	what are main properties limitations disadvantages of using alginate alone for bioprinting applications	When used alone for bioprinting applications, alginate has several key limitations. Prior to crosslinking, alginate has poor mechanical properties and cannot hold its shape when printed. Additionally, alginate is not cell-adhesive, making it suboptimal for biological printing by itself.	['In this article we examine printing a gelatin/alginate blend with incorporated cells using the BioBot Basic.\nWhat are Gelatin/Alginate Blends?\nGelatin and alginate are used for many biomedical applications, including as scaffolds for tissue engineered constructs (1). Gelatin is a naturally occurring, water soluble protein derived from collagen, and is widely used for 3D bioprinting. Gelatin forms a gel at room temperature, but melts in a 37°C incubator, making its printability temperature dependent. Therefore, gelatin is often combined with other biomaterials, such as silk, agarose, or alginate, to enhance its printability and stability in culture. Alginate is a polysaccharide naturally found in seaweed, which is also frequently blended with other hydrogels for bioprinting applications. Alginate, prior to crosslinking, has poor mechanical properties and does not hold its shape when printed. Additionally, alginate is not cell-adhesive. For these reasons, neither gelatin nor alginate alone is optimal for biological printing. However, when blended, gelatin/alginate can be readily printed, and provides an environment conducive to 3D cell culture (2). The gelatin component enables the material to hold its shape after printing at room temperature. Alginate improves gelatin printability by altering its viscosity, creating a smoother print. After printing the alginate is crosslinked, which allows the construct to maintain its shape after the majority of gelatin has dissolved in a 37°C incubator. Residual gelatin in the construct also improves cellular adhesion.\nGelatin/alginate blends are a well-established bioink used for printing cells in a 3D environment. Concentrations ranging from 4% gelatin/3% alginate to 10% gelatin/9% alginate have been reported for various applications (1,2). Here, we used a 6% gelatin and 5% alginate blend in phosphate buffered saline (PBS). Gelatin was first dissolved in hot PBS, then sterile filtered. Alginate was added to the sterile solution in a biosafety cabinet. Once dissolved, the material was brought to 37°C before combining with human mesenchymal stem cells (Rooster Bio) at 400K cells/ml (3).\nPrints were performed on a BioBot Basic inside a biosafety cabinet with a 22GA conical needle, at a pressure of 25 PSI and speed of 15 mm/sec (pressure and speed may vary slightly between batches). Structures printed included a cube, a tube, and a rectangular box (Fig 1).\nAfter printing, constructs were submerged in a 0.75% calcium chloride solution to crosslink the alginate. After crosslinking the structures were rinsed thoroughly with PBS, flooded with culture medium containing DMEM, 10% fetal bovine serum (FBS), and 1% penicillin-streptomycin, and placed in the incubator for two days. To measure cell viability, a Calcein-AM (Thermo Fisher) stain, which stains live cells, and a Hoechst stain (Invitrogen) for total nuclei.\nResults & Discussion\nPrinted constructs ranged from between 4mm – 8mm in height, width, and depth, with a wall thickness of ~1mm. This demonstrates the printer’s ability to print fine, stable constructs and the bioinks ability to adequately adhere to itself and maintain its structure. Calcein and Hoechst staining indicated an 80% cell viability two days after printing.\nThe small footprint and utility of the BioBot™ Basic is ideal for use in a tissue culture hood for aseptic bioprinting of tissue constructs. Here, we demonstrate using the BioBot™ Basic to print a commonly used gelatin/alginate bioink containing live cells resulting in a viable cell culture.\nNotes: Other needle sizes and types may be used, although when using non-conical needles, the material printability becomes more temperature sensitive. The optimal printing temperature for non-conical needles is 28°C. Repeated heating cycles should be avoided. Autoclaving gelatin blends should be avoided, as this degrades the gelatin and reduces printability. Coating cell culture plates with alginate or other ECM proteins can be used to prevent constructs from lifting off plates during culture, if needed.\nAxpe, E.; Oyen, M.L. “Applications of Alginate-Based Bioinks in 3D Bioprinting. Int. J. Mol. Sci. 2016, 17, 1976\nGao, Teng, et al. “Optimization of Gelatin–Alginate Composite Bioink Printability Using Rheological Parameters: a Systematic Approach.” Biofabrication, 2018, 10, p. 034106.']	['<urn:uuid:34ce05ce-2c3c-4652-9864-f8aa805e5b2b>']	open-ended	direct	long-search-query	similar-to-document	single-doc	expert	2025-05-12T21:31:42.699391	13	39	648
37	Can nerve treatments help both normal and injured bladders?	Yes, nerve treatments are used for both conditions. For spinal cord injury patients, stimulated voiding techniques can trigger bladder muscles to contract. For overactive bladder, two nerve treatments are available: Tibial Nerve Stimulation (PTNS) which stimulates nerves controlling bladder function, and Sacral Nerve Stimulation (SNM) which involves implanting a device under the skin to correct nerve pathway messages.	['Spinal Cord Injury (SCI): Managing Your Bladder\nAfter an SCI, your bladder may not work the same way as before. During your rehabilitation, your healthcare team gave you a bladder program to help you adjust to and manage these changes. Going forward, it will be up to you to follow this program on a regular basis. By taking control of your body and managing your bladder, you will help prevent accidents and complications. This will allow you to stay active and social, and help keep you healthy.\nSCI affects your bladder\nSCI can damage the nerves involved with bladder function. How your bladder is affected depends on the level and severity of your injury.\nAn upper level SCI (T12 or higher) may cause a spastic (reflex) bladder. With this type of problem, a reflex is triggered when your bladder is full. It causes your bladder muscles to contract to release urine. You can’t tell or feel when this happens. As a result, accidents can happen. Some people with spastic bladder also have problems relaxing the muscles around the bladder (sphincter muscles). This means urine can’t be emptied out of the bladder properly.\nA lower level SCI (below T12) may cause a flaccid bladder. With this type of problem, your bladder muscles do not contract to release urine when your bladder is full. As a result, your bladder can become too full. This can damage your bladder and lead to other problems.\nYour bladder program\nThe goal of your bladder program is to give you greater control of your bladder function. It allows you to empty your bladder of urine using a method that is safe and effective for you. Some common methods include:\nCatheterization. This involves the use of a thin, flexible tube called a catheter. Once the tube is in place, urine drains through the tube into a bag or collection device. There are many types of catheterization. In some cases, the tube stays in place at all times. In others, the tube is only inserted when it is time to urinate. If you can afford it, getting a bladder scanner is a good idea. The bladder scanner will allow you to know how much urine is in your bladder and when to catheterize yourself (usually when there is more than 350 ml).\nStimulated voiding. This may be used if you still have some control of your bladder. Through various techniques, such as pressing down on your bladder with your fist, you may be able to trigger your bladder muscles to contract and release urine.\nSurgery. Various types of surgery may be done to improve the function of your bladder. Or surgery may be done to bypass the bladder and create a new way for urine to leave the body.\nYour healthcare provider can tell you more about your choices and help you choose the method that is best for you.\nSteps to prevent urinary complications\nWith any type of bladder problem, there is a higher risk for urinary complications. These can include urinary tract infections (UTIs) and kidney problems. To help protect your urinary tract and stay healthy:\nKnow the signs and symptoms of a UTI. See below for more information.\nEmpty your bladder completely and on a regular schedule. A too-full bladder causes leakage. It can also lead to infection or reflux (when urine backs up in the ureters and kidneys). It can trigger a problem called autonomic dysreflexia (AD) as well. This is a sudden spike in blood pressure that must be treated right away. (Ask your healthcare provider if you’re at risk for AD.)\nUse sterile techniques during catheterization. This helps prevent infection. Wash your hands before and after each catheterization. Also, check that catheters are not blocked or kinked. This may prevent proper drainage of urine.\nDrink plenty of water. Water helps flush bacteria out of your system. This can help prevent UTIs and bladder infections. This can also help prevent bladder and kidney stones.\nSee your healthcare provider and healthcare team for regular visits. They help monitor your health. If you need to change any part of your bladder care program, let them know right away. Together, you can make adjustments that work for you.\nWhen to call the healthcare provider\nCall your healthcare provider right away if you have any of the following signs of a UTI or other bladder issue:\nCloudy or bad-smelling urine\nBurning feeling in the urethra or genital area\nAching in the abdomen or back (kidney area)\nIncreased spasms in the abdomen, legs, or bladder\n100.4° F( 38.0°C) or higher\nChills or sweating\nNausea or vomiting\nFeeling very tired\nYou don’t have to limit your activities because of your injury or bladder problem. With time and practice, your bladder program will become part of your daily routine. And you can go on doing many of the things you’ve always enjoyed. This includes working, playing sports, hanging out with friends, and dating. If you need further support, let your healthcare team know. They can refer you to counseling, if needed. Also, reach out to family and friends and let them know how you’re feeling. You may find it helpful to join a support group as well. This allows you to talk with other people who are going through similar experiences as you.', 'Overactive Bladder Treatments\nIt can often be difficult to pinpoint the cause of an overactive bladder so your treatment will involve relieving the symptoms that give you the most trouble. Many people with an overactive bladder will have a mixture of symptoms including urgency and frequency. Your GP will most likely recommend conservative measures to manage your symptoms before recommending medication if these measures don’t work. As a last resort, there are surgical options available to treat very severe symptoms. Your GP or healthcare professional will be able to advise you on the best route of treatment for you.\nLifestyle, Fluids and Diet\nA few small changes to your lifestyle could help to alleviate urgency and frequency symptoms associated with an overactive bladder . It is important to make sure that you are drinking enough fluids to avoid bladder irritation and infection. It is advisable to limit your intake of fizzy drinks, caffeine and alcohol. More advice on Lifestyle, Fluids and Diet.\nTibial Nerve Stimulation\nPercutaneous Nerve Stimulation or PTNS as it is commonly known is a treatment that directly stimulates the nerves responsible for bladder control and is a method that can be used to treat frequency and a n overactive bladder.\nThere are several medications available which can help prevent spasms of the bladder muscle, which can help reduce urgency and frequency; they belong to a group of medicines known as antimuscarinic or anticholinergic drugs. Below is a list of some medications that may be prescribed and links to more information.\n- Darifenacin Hydrobromide\n- Fesoterodine Fumarate\n- Oxybutynin Transdermal Patch\n- Solifenacin Succinate\n- Trospium Chloride\nIf conservative treatments and medication have failed then you may be referred to a surgeon to discuss your options. A bladder augmentation is one surgical option available. There are several ways in which this operation can be performed and your surgeon will discuss the best option for you. This operation is considered major surgery and involves cutting the bladder open. Further details about Bladder Augmentation.\nBotulinum Toxin or Botox as it is more commonly known is a powerful neurotoxin, which works by blocking the electrical impulses to a nerve, which makes a muscle contract causing muscle paralysis. This is a relatively new treatment licensed to treat bladder overactivity. It will usually only be considered if medications have failed to improve your conditions. Y\nThe Detrusor Myectomy is a major operation used to treat an overactive bladder with symptoms of urgency and frequency. It involves removing all or part of the outer muscle layer that surrounds the bladder.\nSacral Nerve Stimulation\nSacral Nerve Stimulation or Sacral Neuromodulation (SNM) is a device that is implanted under the skin in the upper buttock. It helps by correcting the messages that run along our nerve pathways. SNM is performed in two stages, first you will undergo an evaluation, and then, depending on your results you will be offered the implant.\nAcupuncture is an ancient practice that believes that illness and pain occurs when a person’s ‘qi’ or energy flow becomes blocked. Inserting fine, sterile needles into certain points helps to unblock this energy flow. A research study has shown that having acupuncture along the sacral vertebra can suppress overactive bladders and symptoms of frequency and urgency.\nFurther information and downloads can be found in the help and information section. Living with a bladder condition can affect you emotionally and socially; sometimes it can help to speak to others who understand your situation. The Bladder & Bowel Community Forum is available 24 hours today and will allow you to connect with those who share your condition. Start your own topic today or just follow one that interests you.']	['<urn:uuid:8bbd01e4-1113-417c-be6e-b5a797c5cb2d>', '<urn:uuid:e8dc0c70-9e1a-4536-8fe6-88ea7bf53aab>']	factoid	with-premise	concise-and-natural	distant-from-document	comparison	novice	2025-05-12T21:31:42.699391	9	58	1495
38	What are the main benefits of antifungal creams for treating skin infections, and what are the risks of not completing the full course of treatment?	The main benefits of antifungal creams include providing targeted treatment directly to the affected area, enabling faster healing compared to oral medications, and being effective at killing fungus and inhibiting its growth. They are also easily available over-the-counter and allow people to manage their condition without necessarily visiting a doctor. However, not completing the full course of treatment poses significant risks, as it can cause fungal species to become increasingly resistant to antifungal treatment. This resistance is a serious concern - approximately 7% of bloodstream infections caused by yeast are already resistant to antifungals, and one in four patients hospitalized with a bloodstream yeast infection dies of the infection. Therefore, it's crucial to complete the prescribed treatment even if symptoms improve earlier.	"[""IMARC Group’s report, titled “Antifungal Cream Manufacturing Plant Project Report 2024: Industry Trends, Plant Setup, Machinery, Raw Materials, Investment Opportunities, Cost and Revenue,” provides a complete roadmap for setting up an antifungal cream manufacturing plant. It covers a comprehensive market overview to micro-level information such as unit operations involved, raw material requirements, utility requirements, infrastructure requirements, machinery and technology requirements, manpower requirements, packaging requirements, transportation requirements, etc. The antifungal cream project report provides detailed insights into project economics, including capital investments, project funding, operating expenses, income and expenditure projections, fixed costs vs. variable costs, direct and indirect costs, expected ROI and net present value (NPV), profit and loss account, financial analysis, etc.\nAntifungal cream is a topical medication designed to treat fungal infections on the skin. It contains various active ingredients, such as Clotrimazole, Ketoconazole, Miconazole, Terbinafine, and Tolnaftate. It is available as a broad-spectrum and specific antifungal cream. It provides targeted treatment for fungal infections, applying the medication directly to the area of concern. It also leads to faster healing compared to oral medications. It works by killing the fungus and inhibiting its growth, thereby controlling and eradicating the infection. It helps to reduce the occurrence of conditions, such as athlete's foot, ringworm, yeast infections, and jock itch. It soothes rashes on the skin, which can be itchy, red, or scaly. It is also used to treat infections caused by Candida, a yeast that can infect the mouth, throat, gut, and vagina. It is also utilized for treating seborrheic dermatitis, a condition causing flaky, itchy skin on the scalp or face.\nAt present, the increasing demand for over-the-counter (OTC) antifungal creams, as they are effective, easily available, and enable individuals to manage and treat their condition without necessarily visiting a doctor, represents one of the crucial factors impelling the growth of the market. Besides this, the rising awareness about the harmful impacts of waterborne diseases is supporting the growth of the market. In addition, the growing occurrence of various types of fungal infections in the geriatric population primarily due to reduced immunity is offering a favorable market outlook. Apart from this, the increasing availability of antifungal creams providing various other benefits, such as skin conditioning, moisturizing, and quick relief from itching, is propelling the growth of the market. Additionally, the rising investment in research activities by pharmaceutical companies to manufacture various effective antifungal creams is supporting the growth of the market. Moreover, the increasing number of online pharmacies selling medications online and delivering them to the doorstep of buyers within a short time span is bolstering the growth of the market. Furthermore, the rising awareness about fungal infections, their symptoms, and their potential impact on health is strengthening the growth of the market.\nThe following aspects have been covered in the report on setting up a antifungal cream manufacturing plant:\n- Market Analysis:\n- Market Trends\n- Market Breakup by Segment\n- Market Breakup by Region\n- Price Analysis\n- Impact of COVID-19\n- Market Forecast\nThe report provides insights into the landscape of the antifungal cream industry at the global level. The report also provides a segment-wise and region-wise breakup of the global antifungal cream industry. Additionally, it also provides the price analysis of feedstocks used in the manufacturing of antifungal cream, along with the industry profit margins.\n- Detailed Process Flow:\n- Product Overview\n- Unit Operations Involved\n- Mass Balance and Raw Material Requirements\n- Quality Assurance Criteria\n- Technical Tests\nThe report also provides detailed information related to the antifungal cream manufacturing process flow and various unit operations involved in a manufacturing plant. Furthermore, information related to mass balance and raw material requirements has also been provided in the report with a list of necessary quality assurance criteria and technical tests.\n- Project Details, Requirements and Costs Involved:\n- Land, Location and Site Development\n- Plant Layout\n- Machinery Requirements and Costs\n- Raw Material Requirements and Costs\n- Packaging Requirements and Costs\n- Transportation Requirements and Costs\n- Utility Requirements and Costs\n- Human Resource Requirements and Costs\nThe report provides a detailed location analysis covering insights into the land location, selection criteria, location significance, environmental impact, expenditure, and other antifungal cream manufacturing plant costs. Additionally, the report provides information related to plant layout and factors influencing the same. Furthermore, other requirements and expenditures related to machinery, raw materials, packaging, transportation, utilities, and human resources have also been covered in the report.\n- Project Economics:\n- Capital Investments\n- Operating Costs\n- Expenditure Projections\n- Revenue Projections\n- Taxation and Depreciation\n- Profit Projections\n- Financial Analysis\nThe report also covers a detailed analysis of the project economics for setting up an antifungal cream manufacturing plant. This includes the analysis and detailed understanding of capital expenditure (CapEx), operating expenditure (OpEx), income projections, taxation, depreciation, liquidity analysis, profitability analysis, payback period, NPV, uncertainty analysis, and sensitivity analysis. Furthermore, the report also provides a detailed analysis of the regulatory procedures and approvals, information related to financial assistance, along with a comprehensive list of certifications required for setting up an antifungal cream manufacturing plant.\n||Detailed Process Flow: Unit Operations Involved, Quality Assurance Criteria, Technical Tests, Mass Balance, and Raw Material Requirements\nLand, Location and Site Development: Selection Criteria and Significance, Location Analysis, Project Planning and Phasing of Development, Environmental Impact, Land Requirement and Costs\nPlant Layout: Importance and Essentials, Layout, Factors Influencing Layout\nPlant Machinery: Machinery Requirements, Machinery Costs, Machinery Suppliers (Provided on Request)\nRaw Materials: Raw Material Requirements, Raw Material Details and Procurement, Raw Material Costs, Raw Material Suppliers (Provided on Request)\nPackaging: Packaging Requirements, Packaging Material Details and Procurement, Packaging Costs, Packaging Material Suppliers (Provided on Request)\nOther Requirements and Costs: Transportation Requirements and Costs, Utility Requirements and Costs, Energy Requirements and Costs, Water Requirements and Costs, Human Resource Requirements and Costs\nProject Economics: Capital Costs, Techno-Economic Parameters, Income Projections, Expenditure Projections, Product Pricing and Margins, Taxation, Depreciation\nFinancial Analysis: Liquidity Analysis, Profitability Analysis, Payback Period, Net Present Value, Internal Rate of Return, Profit and Loss Account, Uncertainty Analysis, Sensitivity Analysis, Economic Analysis\nOther Analysis Covered in The Report: Market Trends and Analysis, Market Segmentation, Market Breakup by Region, Price Trends, Competitive Landscape, Regulatory Landscape, Strategic Recommendations, Case Study of a Successful Venture\n||US$ (Data can also be provided in the local currency)\n|Pricing and Purchase Options\n||Single User License: US$ 2799\nFive User License: US$ 3799\nCorporate License: US$ 4799\n||The report can also be customized based on the requirement of the customer\n|Post-Sale Analyst Support\n|| 10-12 Weeks\n||PDF and Excel through email (We can also provide the editable version of the report in PPT/Word format on special request)\nKey Questions Answered in This Report:\n- How has the antifungal cream market performed so far and how will it perform in the coming years?\n- What is the market segmentation of the global antifungal cream market?\n- What is the regional breakup of the global antifungal cream market?\n- What are the price trends of various feedstocks in the antifungal cream industry?\n- What is the structure of the antifungal cream industry and who are the key players?\n- What are the various unit operations involved in an antifungal cream manufacturing plant?\n- What is the total size of land required for setting up an antifungal cream manufacturing plant?\n- What is the layout of an antifungal cream manufacturing plant?\n- What are the machinery requirements for setting up an antifungal cream manufacturing plant?\n- What are the raw material requirements for setting up an antifungal cream manufacturing plant?\n- What are the packaging requirements for setting up an antifungal cream manufacturing plant?\n- What are the transportation requirements for setting up an antifungal cream manufacturing plant?\n- What are the utility requirements for setting up an antifungal cream manufacturing plant?\n- What are the human resource requirements for setting up an antifungal cream manufacturing plant?\n- What are the infrastructure costs for setting up an antifungal cream manufacturing plant?\n- What are the capital costs for setting up an antifungal cream manufacturing plant?\n- What are the operating costs for setting up an antifungal cream manufacturing plant?\n- What should be the pricing mechanism of the final product?\n- What will be the income and expenditures for an antifungal cream manufacturing plant?\n- What is the time required to break even?\n- What are the profit projections for setting up an antifungal cream manufacturing plant?\n- What are the key success and risk factors in the antifungal cream industry?\n- What are the key regulatory procedures and requirements for setting up an antifungal cream manufacturing plant?\n- What are the key certifications required for setting up an antifungal cream manufacturing plant?\nWhile we have aimed to create an all-encompassing antifungal cream plant project report, we acknowledge that individual stakeholders may have unique demands. Thus, we offer customized report options that cater to your specific requirements. Our consultants are available to discuss your business requirements, and we can tailor the report's scope accordingly. Some of the common customizations that we are frequently requested to make by our clients include:\n- The report can be customized based on the location (country/region) of your plant.\n- The plant’s capacity can be customized based on your requirements.\n- Plant machinery and costs can be customized based on your requirements.\n- Any additions to the current scope can also be provided based on your requirements.\nWhy Buy IMARC Reports?\n- The insights provided in our reports enable stakeholders to make informed business decisions by assessing the feasibility of a business venture.\n- Our extensive network of consultants, raw material suppliers, machinery suppliers and subject matter experts spans over 100+ countries across North America, Europe, Asia Pacific, South America, Africa, and the Middle East.\n- Our cost modeling team can assist you in understanding the most complex materials. With domain experts across numerous categories, we can assist you in determining how sensitive each component of the cost model is and how it can affect the final cost and prices.\n- We keep a constant track of land costs, construction costs, utility costs, and labor costs across 100+ countries and update them regularly.\n- Our client base consists of over 3000 organizations, including prominent corporations, governments, and institutions, who rely on us as their trusted business partners. Our clientele varies from small and start-up businesses to Fortune 500 companies.\n- Our strong in-house team of engineers, statisticians, modeling experts, chartered accountants, architects, etc. has played a crucial role in constructing, expanding, and optimizing sustainable manufacturing plants worldwide."", 'None of us like to think about it, but fungus is everywhere, and the potential for fungal infections is lurking around us constantly.\nWhen you think of fungal infections, you probably think of things like athlete’s foot or jock itch, both of which are common fungal infections that are uncomfortable and inconvenient.\nFungal infections can be more than uncomfortable, however – they can be deadly. What may start as a simple yeast infection of the skin, known as candidiasis, can become a severe invasive infection within the bloodstream.\nYeast infections are the cause of more than 3.6 million U.S. healthcare visits each year, resulting in an estimated 3 billion dollars in direct medical costs, according to the Centers for Disease Control and Prevention (CDC).\nSome species of fungus are becoming increasingly resistant to antifungal treatment because patients do not complete their recommended course of treatment, allowing the fungus to adapt and change to improve its immunity to the medication.\nHowever, if caught early, most fungal infections of the skin can be treated with topical medications like nystatin cream and other similar drugs.\nIf you find yourself wondering what is nystatin cream used for, you may be surprised by the applications it has to a very common problem affecting infants and adults alike.\nNystatin cream is a topical medication that belongs to a class of drugs called antifungals. Antifungal medications are used to kill a fungus that may be infecting the skin and to prevent the growth of the fungus.\nNystatin is sold under several brand names, including Mycostatin, Nystex, and Pediaderm AF.\nIt can be used on its own, but it is often combined with corticosteroid creams that are used for inflammation treatments.\nWhen sold as nystatin topical cream, it is for use as an antifungal medication only.\nCombination medications, such as nystatin and triamcinolone, are sold under brand names like Mycolog-II, Mytrex, and Quenalog.\nNystatin cream is an antifungal medication that is used to treat certain fungal infections and conditions, including Candida (Monilia), or yeast infections.\nIt can also be used to treat diaper rash in infants.\nFungal infections, called mycosis, are caused by an infestation of fungus on the skin. Fungi can be found everywhere in our environment, including on household surfaces, in dirt, on plants, and on the skin.\nNystatin cream treats a type of fungal infection called cutaneous candidiasis, which is caused by yeast.\nYeast infections aren’t contagious, but they can be uncomfortable. They are most commonly found in warm areas of the body that are creased and are regularly moist, such as the armpits or groin.\nPeople who are obese and have folds of skin that are difficult to clean may experience yeast infections within the folds.\nPeople with diabetes and those taking antibiotics are also considered to be at a higher risk of yeast infection. Signs of a yeast infection on the skin include:\n- Pimple-like bumps\n- Oozing patches\nYeast infections can also occur in the nail beds, in the mouth (called thrush), or in the vagina.\nHowever, nystatin cream is only used for the treatment of yeast infections on the skin.\nIn some cases, fungal infections can become severe and become more serious.\nYou should seek medical attention if you notice any of the following symptoms in addition to those listed above:\n- Flu-like symptoms\n- Body aches and pains\n- Night sweats\n- Respiratory issues\n- Shortness of breath\n- Symptoms of meningitis\n- Severe headache\n- Stiff neck\n- Light sensitivity\nBecause we are always surrounded by fungus, anyone can get a fungal infection at any time.\nHowever, fungal infections are most common in people with weakened immune systems or certain other characteristics. People considered at increased risk of a fungal infection include:\n- People who are currently hospitalized\n- Patients living with HIV or AIDS\n- Transplant recipients\n- People taking immunosuppressant medications\n- People with compromised immune systems\n- Patients undergoing treatment for cancer\n- People suffering from obesity\nIn addition to skin infections, fungi can also cause other types of illnesses, including asthma or allergies, lung infections, infections of the nail bed, bloodstream infections, meningitis, and more.\nHowever, most fungal infections are minor in their early stages and can be treated effectively if caught early.\nAntifungal medications like nystatin cream work by directly attacking and killing fungal cells and by preventing fungal cells from growing.\nIn order to kill the fungal cells and stop the growth of new cells, medications like nystatin cream specifically attempt to destroy cellular structures and functions that are critical to fungal cells but not to human cells.\nThis way, fungal medications do not cause additional damage to your skin or other parts of the body.\nFungal medications commonly target the cell membrane and cell wall of the fungus, both of which are responsible for the protection of the cell.\nThe cell membrane and cell wall hold the fungal cell together, so when one or both is destroyed, the fungal cell dies and cannot reproduce.\nNystatin cream belongs to a specific subclass of antifungal medications called polyenes, which work by attacking the cell wall of the fungus to make it more porous.\nOver time, the cells of the fungus burst and die.\nBecause nystatin cream is produced by a variety of manufacturers in both brand-name and generic forms and in a variety of strengths, it is relatively inexpensive.\nA 15-gram tube of nystatin cream costs anywhere from 7 to 8 dollars, while a 30-gram tube would cost approximately 13 dollars.\nExtremely large quantities of the medication at a very high strength can be expensive, but the medication is generally considered highly affordable.\nIt is covered by many insurance plans. Patients who have difficulty affording their prescriptions of nystatin cream may receive additional savings by using a pharmacy discount card like Pharmacists.org, which offers savings on all brand name and generic FDA-approved medications at more than 60,000 participating pharmacies.\nPharmacists.org is not insurance, and no personal information or medical history is needed besides your name and contact information.\nParticipants in the Pharmacists.org pharmacy discount program pay a negotiated rate that Pharmacists.org arranges which each drug manufacturer in order to offer the maximum possible discount without complicated paperwork, copays, or coverage limits.\nNystatin cream is generally well tolerated by most patients, so there are many benefits associated with its use:\n- Effective results for treating skin yeast infections\n- Simple to apply\n- Inexpensive and accessible for most patients\n- Available in generic form\n- Relatively few side effects\n- It can be used safely on children and infants for the treatment of diaper rash.\nAlthough there are many benefits associated with nystatin, as listed above, taking the medication also poses some risks. Risks associated with nystatin include:\n- The medication should not be applied anywhere besides the skin. Patients should avoid getting the medication in the eyes or mouth.\n- Nystatin cream can cause an allergic reaction. Patients should make their doctors aware of any known allergies.\n- Patients should avoid wearing clothing that is tight, made of synthetic materials, or doesn’t allow for proper air circulation. Patients should wear loose-fitting clothing with natural fibers until the infection is healed.\n- The medication should be taken for the full length of the prescription. Symptoms may improve prior to the end of the prescription, but patients should continue to use the cream as directed. Failure to do so could result in a return of the infection.\n- When used on children for diaper rash, children should not wear tight-fitting diapers or plastic pants and make sure to keep it out of the reach of children.\n- If used for longer than directed, nystatin cream may begin to cause thinning of the skin and stretch marks on areas with thin skin, such as the face, armpits, and groin. Nystatin cream should be used only as directed.\nWhat Dose of Nystatin Cream Should I Use?\nThe dosage of nystatin cream recommended for use will vary depending on the patient and the severity of the infection.\nThe amount that is prescribed to you will also vary depending on the strength of nystatin cream that is prescribed by your doctor. In general, nystatin cream should be applied in a quantity that covers the affected area two to four times per day, or as directed by your doctor.\nNo airtight bandages should be used to cover the treated area, but a sterile gauze dressing or bandage is ok.\nThe average length of treatment is between two to eight weeks and is determined by the severity and type of infection.\nIt is important that patients do not skip doses of their medication and wash their hands before and after use in order to prevent further fungal spread and increased resistance of the fungus to medication.\nApproximately seven percent of bloodstream infections caused by yeast are resistant to antifungals, and one in four patients hospitalized with a bloodstream yeast infection dies of the infection.\nTherefore, it is absolutely critical that patients apply their medication as directed, do not skip any doses, and continue their treatment until directed to stop by their doctor.\nVery few side effects are associated with nystatin cream, and the medication is well tolerated by most people.\nSkin irritation is the most common possible side effect associated with nystatin cream, and it usually does not require medical attention.\nPatients should report adverse effects like skin irritation to their doctor or healthcare professional if it continues, gets worse, or becomes bothersome. Some people may experience mild itching.\nAllergic reactions to nystatin cream are also possible; signs of allergic reactions include hives, difficulty breathing, and swelling of the face, lips, tongue, or throat.\nPatients who experience severe burning, itching, rash, pain, or other serious side effects on the application site should discontinue the use of nystatin cream immediately and contact their healthcare professional.\nCan Women Who Are Pregnant or Breastfeeding Use Nystatin Cream?\nNystatin cream is currently classified by the Food and Drug Administration as a Category C medication, meaning that it is not known whether the medication will harm an unborn baby.\nBecause the effects of nystatin cream on unborn babies are unknown, it is important that women who are pregnant or planning to become pregnant speak with their doctors before applying nystatin cream.\nIt is also unknown whether nystatin cream passes through breast milk and could affect a nursing infant. Women considering using nystatin cream should talk to their doctors before breastfeeding and using the cream.\nDo Any Other Drugs Interact With Nystatin Cream?\nNystatin cream carries a low risk of interaction with other drugs taken orally or injected. However, it is important that you give your doctor a comprehensive list of all medications that you are currently taking, including prescription drugs, over the counter medications, vitamins, supplements, and herbal products.\nYou should not start a new medication without checking with your doctor first.\nHow Do I Know If Nystatin Cream Is Right for Me?\nNystatin cream is generally well-tolerated, but it is not right for everyone, including those patients who are allergic to the medication or its ingredients.\nNystatin cream is capable of treating fungal skin infections caused by candida (yeast), including diaper rash.\nYou should get medical advice from your healthcare provider about any prescription drugs, over-the-counter medications, supplements, or herbs that you are taking prior to using nystatin cream.\nNo studies have been conducted regarding the impacts of nystatin cream on pregnant or nursing mothers and their children, so patients should speak to their doctors if they are pregnant, planning to become pregnant, or nursing prior to using nystatin cream.\nNystatin cream must be applied multiple times per day consistently, and patients should not skip any doses or end their treatment early.\nDoing so may cause the fungal infection to get worse, so patients should not use nystatin cream if they cannot commit to applying it appropriately.\nReferences, Studies and Sources:\nKate Byrd, PharmD, is a highly accomplished Medical Writer with a strong background in pharmacy and an unwavering commitment to producing accurate, informative content. After earning her Doctor of Pharmacy degree, Kate embarked on her career as a Medical Writer, where she has since gained valuable experience in developing evidence-based content that translates complex medical information into easy-to-understand articles. We are thrilled to announce that Kate is now bringing her expertise and dedication to the medical writing team at Pharmacists.org. Her passion for empowering readers with reliable and accessible health information aligns perfectly with our mission, making her a valuable addition to our team.']"	['<urn:uuid:941c15e7-4cba-41e7-bea9-f2741ffc3de6>', '<urn:uuid:3353fa4b-7b43-4584-b5ae-3b83c549733b>']	open-ended	direct	verbose-and-natural	similar-to-document	multi-aspect	novice	2025-05-12T21:31:42.699391	25	122	3847
39	curious about military medical advances what organization was created in 2008 to develop regenerative medicine treatments	The Department of Defense established the Armed Forces Institute of Regenerative Medicine (AFIRM) in 2008. AFIRM is a multi-institutional, interdisciplinary network of universities, military laboratories and investigators that works to promote regenerative medicine therapies.	"['FORT DETRICK, Md. (July 8, 2014) -- A team of scientists scans the surface of severely burned skin, creates a three-dimensional map of the wound with a laser, and then prints skin cells onto the patient using a 3-D bioprinter.Medical specialists are developing methods to transition this research from the laboratory to clinical trials.The U.S. Army is a significant proponent and investor in regenerative medicine and 3-D bioprinting, according to officials. Scientists are aiming to advance this new research area to help injured service members recover from the wounds of war.Dr. Michael Romanko, who provides science and technology management support for the Tissue Injury and Regenerative Medicine Project Management Office, or TIRM PMO, with the U.S. Army Medical Material Development Activity, said that improvements in body armor, vehicle design and advanced medical care during the past decade led to Soldiers suffering injuries that would have caused fatalities in previous conflicts.Blasts from improvised explosive devices have increased the number of Soldiers experiencing the loss of limbs, catastrophic injuries to the face and severe burns.""There was an increasing need to deliver therapies for wounded warriors. We saw a spike in the severity of the trauma that these Soldiers were receiving. As we increased the quality of battle armor, the injuries they were surviving were that much more debilitating,"" said Romanko, who holds a doctorate in molecular medicine.The Department of Defense, in turn, established the Armed Forces Institute of Regenerative Medicine, or AFIRM, in 2008. Regenerative medicine aims to replace or regenerate human cells, tissues or organs to restore or establish normal function.The AFIRM is a multi-institutional, interdisciplinary network of universities, military laboratories and investigators under the framework of a cooperative agreement, Romanko said. The network is designed to promote integration of development, from basic science research through translational and clinical research, as the best means of bringing regenerative medicine therapies to practice.The success of the first five years of the AFIRM led to the competition of a new cooperative agreement in 2013, he said.3-D bioprinting is one tool that scientists are developing in the field of regenerative medicine. It is an early discovery technology being used to address extremity injury and skin, genitourinary and facial repair by AFIRM investigators.Skin repair is the most robust focus area addressed in the regenerative medicine portfolio, Romanko said.""The scars that Soldiers develop as a result of burns constrict movement and disfigure them permanently. The initiative to restore high-quality skin that is elastic and complete with sweat glands, appropriate pigmentation and hair follicles is incredibly important,"" he said.Romanko described how researchers are currently using 3-D bioprinters for skin repair research.In translating this technology to the clinic, scientists will take healthy cells and, using a device similar to an inkjet printer, load the cartridges with two types of skin cells -- fibroblasts and keratinocytes -- instead of ink. Fibroblasts make up the deep layer of skin, and keratinocytes compose the top layer.After the team completes a scan of the burn and constructs a 3-D map of the injury, the computer tells the printer where to start printing and what type of cells to use, depending on the depth of the injury and the layer being reconstructed. The bioprinter deposits each cell precisely where it needs to go, and the cells grow to become new skin.Early research results are promising, and scientists hope this could be a viable solution in the future, Romanko said.Addressing the need for skin repair is important, because burns account for 10 percent to 30 percent of battlefield casualties. The precision available through 3-D bioprinting allows for a custom solution for each patient.""Everyone has a different type of injury, and not everyone\'s skin injury looks the same. Skin bioprinting would provide a scalable form of personalized medicine,"" Romanko said.An additional goal in regenerative medicine is bioprinting organs, limbs and vascular systems. Other early discovery 3-D bioprinting projects within AFIRM have focused on the generation of complex tissue components with bone and muscle.3-D bioprinting is still in the early stages, Romanko said. More laboratory research is necessary, and several federal regulatory steps must be completed before patients could enroll in clinical trials.About 30 universities, hospitals and additional partners, led by the Wake Forest Institute for Regenerative Medicine, make up the consortium of AFIRM researchers. Romanko works with the TIRM PMO, which supports AFIRM and other regenerative medicine programs.AFIRM receives funding from the U.S. Army, the Office of Naval Research, Air Force Surgeon General\'s Office, Veterans Health Administration, National Institutes of Health and the Office of Assistant Secretary of Defense for Health Affairs. AFIRM investigators also bring in funding from other federal, state and private sources.Leveraging experts from across academic and medical research organizations allows the Army to pull together the range of specialties needed for this type of complex technology, Romanko said.Engineers work with bioimaging experts to develop the actual bioprinters and technology to scan the topology of injury sites, like the skin, or recreate the 3-D structures of other organs. In turn, biologists and physicians who specialize in the specific organ or tissue being produced oversee the process to ensure the appropriate placement of cells and construction of tissue. They all work together as a multidisciplinary team to advance bioprinting research.Expanding the technology to a greater number of potential patients will also help to ensure its long-term viability, Romanko said.""This has very widespread use, not only to the military audience, but also to the civilian population. We need a larger commercialization audience in order to be a self-sustaining technology,"" he said.']"	['<urn:uuid:672f8f5d-34f0-45f5-a647-4a4c6f12cace>']	factoid	with-premise	long-search-query	similar-to-document	single-doc	novice	2025-05-12T21:31:42.699391	16	34	916
40	How do astronauts recycle waste in space, and what mental challenges do they face?	In space, astronauts are developing ways to recycle waste products like urine and CO2 into useful materials. They use yeast strains that can convert urine's urea and carbon dioxide from breath into essential nutrients like omega-3 fatty acids and materials for 3D-printing tools. As for mental challenges, astronauts often face depression, anxiety, and interpersonal conflicts during long missions. Some cosmonauts have experienced communication breakdowns with crewmates, and there have been cases of depression affecting missions. To address these issues, NASA has developed the Virtual Space Station software, which provides private behavioral therapy and conflict resolution training for astronauts.	"['Long distance space travel could be powered by – human waste, according to new research.\nWaste products – such as urine and carbon dioxide – could also be used to make tools, suggests the study.\nThat’s one of the ultimate goals of scientists who are developing ways to make long space trips feasible.\nAstronauts can’t take a lot of spare parts into space because every extra ounce adds to the cost of fuel needed to escape Earth’s gravity.\nDoctor Mark Blenner, of Clemson University in the United States, said: “If astronauts are going to make journeys that span several years, we’ll need to find a way to reuse and recycle everything they bring with them.\n“Atom economy will become really important.”\nHe said the solution lies in part with the astronauts themselves, who will constantly generate waste from breathing, eating and using materials.\nUnlike their friends on Earth, Dr Blenner says, the spacefarers won’t want to throw any waste molecules away.\nHe and his team are studying how to repurpose those molecules and convert them into products the astronauts need, such as polyesters and nutrients.\nDr Blenner said some essential nutrients, such as omega-3 fatty acids, have a shelf life of just a couple of years.\nThey’ll need to be made en route, beginning a few years after launch, or at the destination.\nDr Blenner said: “Having a biological system that astronauts can awaken from a dormant state to start producing what they need, when they need it, is the motivation for our project.”\nHis biological system includes a variety of strains of the yeast Yarrowia lipolytica. The organisms require both nitrogen and carbon to grow.\nThe researchers found that the yeast can obtain their nitrogen from urea in untreated urine.\nAnd the yeast obtain their carbon from CO2, which could come from astronauts’ exhaled breath, or from the Martian atmosphere.\nBut to use CO2, the yeast require a middleman to “fix” the carbon into a form they can ingest. For this purpose, the yeast rely on photosynthetic cyanobacteria or algae provided by the researchers.\nDr Blenner said one of the yeast strains produces omega-3 fatty acids, which contribute to heart, eye and brain health.\nAnother strain has been engineered to churn out monomers and link them to make polyester polymers.\nThe polymers could then be used in a 3-D printer to generate new plastic parts.\nDr Blenner’s team is continuing to engineer the yeast strain to produce a variety of monomers that can be polymerised into different types of polyesters with a range of properties.\nFor now, the engineered yeast strains can produce only small amounts of polyesters or nutrients, but the scientists are working on boosting output.\nThey’re also looking into applications here on Earth, in fish farming and human nutrition. For example, farmed fish need to be given omega-3 fatty acid supplements, which could be produced by Dr Blenner’s yeast strains.\nHe added: “We’re learning that Y. lipolytica is quite a bit different than other yeast in their genetics and biochemical nature.\n“Every new organism has some amount of quirkiness that you have to focus on and understand better.”\nThe findings were presenting at the National Meeting of the American Chemical Society in Washington, DC.', ""What happens in space, doesn't always stay in space\nValentin Lebedev sensed that something was wrong.\n“Today passed as usual, but I can feel that I am beginning to get tired and nervous, even though my overall physical condition is good,” he wrote. “Tomorrow will be even harder because of its workload.”\nSeptember 7, 1982, was the cosmonaut’s 118th day aboard the Salyut-7space station. When completed, his mission would be remarkable for its length—211 days, a record that stood for 13 years—and for the diary that recorded in intimate and unsparing detail his life on the station. The diary recounts not only thousands of research and operational tasks but also Lebedev’s frustration with ground control, the grudging silences with his lone crewmate, and his mounting anxiety and depression in the loneliness of space.\nSince the dawn of the space age, its architects have recognized the mission-critical importance of human factors—not only nutrition and ergonomics but also management theory and group dynamics. As missions grew in length and complexity, planners devoted more attention to assessing and maintaining the mental health of space explorers, especially as planning for missions to Mars and beyond anticipated ever greater isolation: When an astronaut on Mars asks mission control a question, the reply will follow at least eight minutes of silence.\nFor help with the human factors, NASA has turned to behavioral researchers. Among those is psychologist James Cartreine, an HMS instructor in psychiatry in the Program in Behavioral Informatics and eHealth at Brigham and Women’s Hospital. He has been exploring how software could be used to deliver behavioral therapy, which applies evidence-based, pragmatic techniques to problems like depression or anxiety. The conversation between behavioral therapist and patient may resemble other psychotherapeutic techniques, but it often follows a series of questions to identify a problem in a person’s life and discrete actions to address it. And in those scripts, Cartreine and other researchers saw a way to deliver behavioral therapy without the therapist.\n“I wondered how you might automate what a psychologist would do one on one with a patient,” says Cartreine, “and it turns out, NASA was interested in how to help astronauts if they developed depression while on a mission.”\nIn 2001, the National Space Biomedical Research Institute gave Cartreine the green light to develop the behavioral therapy software that came to be known as the Virtual Space Station.\nLike its namesake, the Virtual Space Station is an assemblage of modules. Astronaut training modules employ interactive videos to walk users through potential conflicts and strategies to resolve them. One scenario: Your team has made an error that will delay, but not endanger, an experiment. You want to report the cause of the delay to ground control—but your crewmate does not. You can select from a range of responses, from “They’ll spend hours trying to figure this out. We’ve got to tell them.,” to “What’s wrong with telling ground?,” to “Okay, but you’re putting me in a bad position.,” and the video shows the likely outcomes of your choice, followed by a voice-over discussing the dynamics at play. (Hint: Ultimatums may escalate the situation; questions defuse it.)\nOther modules are designed for use during space missions. Self-assessment modules help a user rate personal levels of stress, depression, and interpersonal conflict. Based on a user’s responses, the system provides feedback and recommendations. And self-guided treatments help astronauts deal with problems such as depression and conflict, if they arise.\nTo address depression, which some astronauts have confronted on past missions, Cartreine and his colleagues developed a problem-solving therapy program. As in one-on-one therapy with a counselor, the program doesn’t tell users how to solve a personal issue, such as those involving relationships, health, finance, or family. Instead, a series of questions and answers helps the user develop a plan to address it. Follow-up sessions help to gauge progress and, if necessary, to develop a plan B. Throughout the modules, short videos provide background, encouragement, and user-tailored advice on ways to improve problem-solving success. “The goal is to simulate as closely as possible a session with a master therapist,” Cartreine says. Although the software has not yet made a trip into space, it has been used in astronaut training.\nAnticipating the myriad problems that might crop up during a mission is complicated by the sensitive nature of the data. An astronaut’s behavioral health history is part of his or her confidential medical record, which presents a hurdle to researchers. So Cartreine and others have pieced together a picture of the difficulties encountered from news reports, astronauts’ memoirs, and other documented accounts, like Lebedev’s diary.\n“Today was difficult,” Lebedev wrote about crewmate Anatoly Berezovoy on their 60th day in space. “We don’t understand what is going on with us. We silently walk by each other, feeling offended. We have to find some way to make things better.”\nOther cosmonauts may have had it worse: Evidence suggests that one Russian mission was scrubbed due to depression among personnel and another due to interpersonal conflict.\nIn the United States, Buzz Aldrin has written candidly about the depression and alcoholism that followed his astronaut career. Former U.S. astronaut Charles Brady committed suicide in 2006, 10 years and two weeks after his only space flight. And in 2007, six months after returning from a shuttle mission to the International Space Station, astronaut Lisa Nowak was charged with the assault and attempted kidnapping of a romantic rival, actions she says were triggered by depression.\nIn the wake of Nowak’s arrest, NASA administrator Michael Griffin commissioned a review of the space program’s mental health care system. The review committee recommended that NASA give more systematic attention to mental and behavioral health issues. But it cautioned that “the use of any behavioral health selection and patient data is a matter of concern for astronauts, family members, and medical and behavioral health personnel.”\nThat concern drove the design of the Virtual Space Station, Cartreine says. Data are stored on a personal flash drive that the astronaut carries—not only during the mission, but also during the years of training. Nothing is transmitted to Earth or stored on shared workstations. The goal is to give astronauts the privacy to work through problems that they feel unable to raise with their flight surgeons. “If astronauts believe that what they say is confidential and that they can answer honestly, we think they will have a better chance of benefiting from the system,” Cartreine says.\nAnother obstacle for researchers has been sample size: There just aren’t enough astronauts to generate a large enough volume of data. But this problem is one the space program has confronted and solved by studying what are called analog populations, groups such as Antarctic researchers, crews of underwater research stations, or, in Cartreine’s case, firefighters.\n“Like astronauts, firefighters live together for days at a time,” Cartreine says. “There’s this interdependence for safety but also for social support. It’s an operational environment, meaning you’re working with equipment, as opposed to working in an office. And you can die on the job.” Cartreine is working with fire departments in three U.S. cities to test whether a new Virtual Space Station computer program can help these firefighters resolve workplace conflicts.\nThe Virtual Space Station has taken some ribbing. (Comedian Paula Poundstone: therapy is hard when transmissions between Earth and Mars take up to 40 minutes “and the hour only lasts 50.”) And Cartreine was amused when a New Zealand TV newsmagazine paired him with David Bowie, whose contribution to the field is the song “Space Oddity.” But Cartreine is serious about the potential of computer-based therapy, and has cofounded Cognitive Behavioral Technologies LLC, a company to promote its wide adoption. Cartreine notes that computer-based therapy is being used with wider application overseas, where it has been shown to improve conditions from post-traumatic stress to depression.\nThe potential benefit is huge: Depression is the leading cause of disability in the United States, and mental health disorders account for four of the ten leading causes of lost productivity. A 2008 study put the annual cost of lost productivity owing to mental health disorders at $198 billion. Behavioral therapy has advanced to the point, Cartreine says, that doctors can be fairly confident that patients who follow protocol will see improvement. And computer-based therapy can deliver treatment that’s cost effective and available whenever and wherever it’s needed.\nAs technology offers more solutions on Earth and in space, it underscores that the enduring problems are human ones, a conundrum that struck the cosmonaut Lebedev some 30 years ago. “If you were a construction worker or mechanic, plowing the land or building factories, it would be different,” he wrote. “You could turn a blind eye to the imperfections of others. However, in space technology the responsibilities are obviously much more serious, but people are still the same. Social defects cross all borders, and space becomes a mirror of all our terrestrial shortcomings—social, economic, and moral.”\nR. Alan Leo is senior writer and editor in the HMS Office of Communications and External Relations.""]"	['<urn:uuid:12808b49-2a3c-42e7-8aaa-d0b43edab96d>', '<urn:uuid:e9d8b0e4-8dc9-4b2b-ae7b-4e6acaae9d95>']	open-ended	direct	concise-and-natural	similar-to-document	multi-aspect	novice	2025-05-12T21:31:42.699391	14	98	2044
41	What different ways can 3D printing help people today?	3D printing serves multiple important purposes today. In historical preservation, it's being used to recreate lost artifacts like the Zeus statue, helping museums teach history and restore cultural heritage. In the pharmaceutical industry, it's revolutionizing medicine by enabling the creation of personalized drug products with customized dosages, shapes, and drug combinations. The technology also allows for on-demand production in both fields, making it more efficient and cost-effective whether producing historical replicas or pharmaceutical products.	['Zeus must be wondering if he is about to regain his stature of antiquity. For more than a thousand years he was considered the father of the gods who ruled from his throne at the top of Mount Olympus, the highest mountain on the Greek mainland located 50 miles southwest of Thessaloniki.\nHis mighty presence is about to be resurrected at the Millennium Gate Museum in Atlanta‘s Atlantic Station in celebration of the 20th anniversary of the 1996 Atlanta Centennial Olympic Games. Although Zeus must have been disappointed that those games hadn’t been hosted in Greece’s capital, Athens, the upcoming exhibition – The Games: Ancient Olympia to Atlanta to Rio — gives full-credit to the country’s historic role.\nMinnesota firm Stratasys Ltd. and Kennesaw State University‘s 3D Center. The feat is remarkable in that the statue, one of the “seven wonders” of the ancient world, was recorded only in written descriptions and illustrations on coins.\nIn an attempt to replicate the figure, which was made of wood, ivory and gold, the parts have been printed in thermoplastic, using a 3D model approximating its design. Zeus has been cut down to size, however. Originally at 42 feet tall, the 3D version has him at only 6 feet.\nBut he is at the forefront of what far-sighted museum curators see as the beginning of new teaching tools and the historic restoration of artifacts lost through the ravages of time or the destruction by militants such as Islamists who have destroyed the cultural heritage in Iraq and Syria by attacking archeological sites with bulldozers and explosives.\nThe Stratasys Fortus 900 3D printer is one of the most powerful available and recreated the Zeus statue using fused deposition modeling (FDM) technology. Its roots stretch back to 1989 when Scott Crump and his wife Lisa founded the Stratasys company in Eden Prairie, Minn., after Mr. Crump decided to make a toy frog for his young daughter using a glue gun loaded with a mixture of polethylene and candle wax.\nRandy Emert, director of Kennesaw’s Center for Additive Manufacturing and the 3D Center, told Global Atlanta that Kennesaw’s contribution to the Zeus statue includes an eagle on a scepter and a small statue of Nike which are held by Zeus. The Kennesaw students also are learning to use the technology by creating a separate head of Zeus.\nHe said that the university’s 3D Center introduces students to the vast employment opportunities that are opening up due to 3D technology, which is playing an ever growing role in new media arts, game design and mechanical engineering.\nRodney Mims Cook Jr., founder and president of the National Monuments Foundation, an organization that is responsible for the design and construction of the Millennium Gate Museum, told Global Atlanta that the Zeus project was in keeping with the spirit of the 1996 Summer Olympic Games by underscoring cooperation among local governments, universities, business and non-profits.\nThe University of Georgia, the Georgia Institute of Technology, Savannah College of Art and Design, the Carlos Museum at Emory University, Oglethorpe University in addition to Kennesaw are participating in the project. It also is scheduled to travel in Georgia to Rome, Columbus, and Athens where it will be on display at the Darlington School, The Columbus Museum and the University of Georgia.\nPerhaps to Zeus’ dismay, the entire exhibition is not to be focused on his statue alone. Also to be included are classical artifacts to be contributed from the Hearst Castle in California, which are more than 2,500 years old and tell the story of the Olympic Games and Ancient Olympia, athletic competition and Greek mythology and politics.\nMr. Cook also was enthusiastic about the participation of the Hearst Castle because of the connection between Atlanta and the Hearst family. William Randolph Hearst bought a newspaper in Atlanta in 1912 extending the reach of his publishing empire across the United States. The Atlanta connection eventually led to his donating enough land to triple the size of Oglethorpe University.\nA classicist at heart, Mr. Cook’s interest in 3D technology includes a commitment to educational and restoration projects. This exhibition rests at the intersection of art and technology, he told Global Atlanta, “Teaming with Stratasys and KSU’s 3D Center, our hope is to deliver creations far too few have even tried to attempt.”\nAgain, in the spirit of the Olympics, Mr. Cook expects the technology to “test human limits,” and as the 1996 Summer Olympic Games did, “Give us a shot in the arm and an international space that we never had. It served the city by demanding what was expected of us and met the aspirations of our forefathers.”\nMeanwhile, Athens, Ga.-based sculptor and painter Stan Mullins, along with two craftsmen from Gravity Works LLC, also in Athens, have been putting the finishing touches on the throne upon which Zeus is to be placed for the exhibition.\nThe exhibition is to open on Saturday, Aug. 20, extending until Jan. 2, 2017, and provides insights into the scope of the games in ancient times, and the values that have been brought back to life in modern times.\nThe earliest records of the Olympic Games date back to 776 B.C. when they were a religious festival held on the fertile ground of a rural sanctuary at the base of the mountain. According to ancient legend, all hostilities between the Greek city states ceased for the duration of the games, which came to represent the Olympic spirit of global brotherhood that continues to this day.\nIn stark contrast to the Rio Olympics, the games were small in scale in their earliest years. In addition to sacrifices and other religious rites, there was only one sporting event, a footrace of 200 yards, a distance which was called a “stade” from which the word “stadium” is derived.\nAs the games evolved over the centuries, however, they became the center of Greek life and Olympia became the hub of the Greek-speaking world.\nWhile the athletic contests multiplied so did grand processions and banquets honoring the gods and heroes. The sacrifices also grew exponentially including hundreds of oxen.\nDespite all of the pageantry and the proud strutting of masculinity, the most spectacular sight at Olympia was the gold and ivory cult statue of Zeus enthroned and crowned with a sculpted wreath of olive sprays. A golden robe carved with animals and lilies covered its body. Its sandals were made of gold. In its right hand it held a statue of the crowned Nike, the goddess of victory, and in its left hand was a scepter inlaid with many metals supporting an eagle.\nThe throne itself was no less impressive, decorated in gold with precious stones, ebony and ivory.\nOne Greek orator was reported to have said that a glimpse of the statue was enough to make one forget all early troubles. A Roman general said that once when he saw the statue he felt he had seen a living god.\nBut even Zeus’ command of the known world didn’t last forever.\nIts demise is cloaked in obscurity, but Christianity is generally accredited with its end. In the early fourth century A.D., Emperor Constantine decreed that gold be stripped from all pagan shrines after he converted to Christianity.\nBy the end of the century the Olympics were viewed as a pagan rite and abolished altogether. It was then that the statue perhaps was moved by a wealthy Greek to Constantinople where it became part of his private collection of classical art.\nThere is some controversy about whether its final end occurred when fires swept Constantinople in 475 A.D. and in 425 A.D.\nWhatever the immediate cause of its end, there are only few traces of this “Wonder of the World” that remain. These are limited to a few approximate versions on Greek and Roman coins and the account of a Roman geographer and traveler on which the 3D printing restoration is based..\nSynovus Financial Corp. a financial services company based in Columbus, Ga., is sponsoring the exhibition.', '3D printing has the potential to not only revolutionise core business segments like manufacturing and construction, but also the pharmaceutical industry. From pharmaceutical manufacturing to personalised dosage forms and drug delivery systems, there are several applications of this technology in the sector.\nIt has led to a major shift with a change visible from conventional medicine mass production towards personalised drug products for each individual. The concept has future potential with regard to enabling advantages for the industry, patients, and pharmacists, through offering on-demand production and design of flexible medicine formulations, complete with personalised sizes, shapes, dosages, drug releases, and combinations of multiple drugs.\nAt the same time, 3D printing may be integrated into applications like precision medicine and additive manufacturing with the technology enabling the creation of medicines personalised for therapeutic needs of patients, including drug combinations, dosage, and drug release profiles along with personal requirements in terms of the flavour, texture, shape, and size. 3D printing also ensures multiple advantages for not just the pharmaceutical industry and clinical practices, while also helping lower overall costs and speeding up development cycles alongside.\n3D printing has several avatars in terms of its usage in the pharmaceutical manufacturing space. These include the following:\nSome methods of printing include SLS (selective laser sintering), FDM (fused deposition modelling), BJ (binder jet), DPE (direct powder extrusion), and SSE (semi-solid extrusion). Every type of technology has its own specific technical attributes while enabling the production of personalised drugs with diverse attributes.\nHere are some of the biggest advantages of 3D printing in the pharmaceutical manufacturing space, right from drug delivery systems to personalised dosage forms.\nDown the line, 3D printing may be a veritable game-changer for the pharmaceutical industry, enabling personalisation and formulations, along with efficient drug delivery as well, while simultaneously lowering go-to-market timelines, costs, and many other hassles involved in the process for pharmaceutical companies.\nWhat is 3D printing and how does it work in the pharmaceutical industry?\n3D printing or additive manufacturing is the procedure of creating three-dimensional solid items from digital files. It works through the manufacturing of specific, multi-drug, and personalised formulations in the pharmaceutical industry.\nHow can 3D printing be used to create personalised dosage forms and drug delivery systems?\n3D printing can be leveraged for the creation of personalised forms of dosages and drug delivery systems, through either multi-drug combinations or formulations, or on-demand dosages with differentiated flavors, textures, sizes, and shapes.\nHow does 3D printing impact the speed and efficiency of pharmaceutical manufacturing?\n3D printing, especially on-site, lowers the time taken by pharmaceutical manufacturing entities to produce on-demand drugs. It also reduces the time between discovery and marketing of formulations by a great extent. The entire procedure becomes more efficient with lower labour, costs, and higher efficiency with minimal logistics.\nWhat are the regulatory considerations for 3D printed pharmaceuticals?\nThe medical products made by 3D printers are regulated by the FDA. The regulatory review that is needed depends on the type of product that is being manufactured, along with its intended usage, and the potential risk factors for patients.']	['<urn:uuid:954c8226-0fc7-4261-b9cc-37faaed551f1>', '<urn:uuid:0f79a051-9795-4079-9ace-c718f7cf48da>']	open-ended	direct	concise-and-natural	similar-to-document	three-doc	novice	2025-05-12T21:31:42.699391	9	74	1847
42	how participant identity protection affects honesty research responses	Participants are more likely to provide honest responses when their identity is protected, especially for topics that are taboo in society or could result in punishment, retaliation, or employment bias if their responses were exposed.	"[""Science depends on research participants to volunteer information regarding individual beliefs and actions on a host of topics. A participant or subject is more likely to provide honest responses when their identity is not going to be exposed.\nConsiderations for confidentiality should be given to the following:\nNature of the response.\nPossible punishment for the response.\nPossible punishment for participation.\nPerceived punishment for the participation.\nFear of embarrassment\nParticipants answer differently if they think that privacy may be compromised\nThe research may focus on topics that are taboo in society and it would be difficult to illicit honest responses to some of the questions posed when a participant did not feel secure in knowing that their identity is protected.\nThere also maybe some personal liability involved depending on the topic of the study. If the study involved cheating on testing at the university they attended and a guarantee of anonymity could not be provided, the likelihood of honest answers being given would be very low, for fear of retaliation.\nBias in employment although illegal is very real, if confidentiality is not upheld and if the wrong information is passed around regarding a participant it could very well effect the ability of the participant to maintain employment and be employed in the future.\nIn addition there are other negative responses that a participant may face if the research is on an unpopular topic, or a controversial issue. If the group involved in the research all works for the same organization and the study is to determine the employer's unethical practices, participants if not kept confidential could suffer great consequences for their responses.\nIn some cultures a subject's participation alone could result in public punishment, being excommunicated from the community or even death.\nThere may only be a perception of punishment for participation if confidentiality is not guaranteed. The perception alone will keep potential subjects from participating in the research, and will also prevent honest answers.\nResearch is never meant to punish anyone, it's only goal should be greater knowledge and understanding working toward a positive goal.\nPrivacy matters should be addressed from the inception of the research to the publication of the results. There should be safety nets put in place to guarantee confidentiality. The only amount of personal data that should be collected for the research is the minimal amount needed to insure a proper sampling of the population. Personal identifiable information should not be collected nor maintained unless absolutely necessary. Research staff should be properly trained in procedure to maintain confidentiality.\nInformed consent is required in all studies and research using human participants. The consent to participate should clearly outline the purpose of the study and what the information gathered will be used for.\nWhen is Confidentiality not an Issue\nConfidentiality is not an issue when observing large groups, where individual responses or actions are not considered or when participants' identifiable information is not involved. Even in cases where there are large groups being assessed it is up to the researcher to use good judgment in making decisions regarding what information should be shared.\nA Legal Breach\nThere is a sticky area when it comes to confidentiality. If a participant in a research event or a study falls into any of the below categories than the researcher is legally compelled to report the participant.\nRisk of bodily harm - if there is a potential that a violent act will occur against someone else or the participant at his/her own hand.\nCases of child abuse - sexual misconduct with a child, or there is imminent danger to a child that has been disclosed.\nTerrorism - there is a terrorist act or plan in place that the participant has disclosed.\nThere is a eminent threat to public health.\nThere is some ethical concern by being required to report someone in any of the categories above. Let us look at these few examples to review the ethical quandary.\nA study is being conducted looking at the effectiveness of a drug on depression. The participants are asked to record feelings of violence, suicide, anger and despair, both before they began taking the medication and after three weeks of being on the medication.\nEach category is assigned a numerical value to display a range of feelings from very likely to least likely. The study is dependent on honest answers, the participants have been promised confidentiality.\nIf 30 of the 100 participants report that they feel more suicidal since taking the medication, is the researcher required to report all thirty of the participants?\nAIDS patients have been asked to participate in a study to gauge sexual habits since being diagnosed. 60% of the participants report that they have adjusted their sexual habits to include safe sex practices or abstinence, but 40% report that they still sometimes do not practice safe sex.\nShould they be reported?\nThey are a definite threat to public health, but the study depends on honest answers, once that confidentiality has been breached it certainly will sour future participants from being honest. How can we ever gauge the effectiveness of programs and education within certain communities if participants have to fear potential criminal charges?\nShould Science Be Required to Act As a Step in the Penal Process?\nA Real Life Story\nA woman in Florida is expecting baby number 3. She appears at a public health department for prenatal care and agrees to participate in a study for drug use and pregnancy.\nShe confesses that she has an addiction to cocaine (a known teratogen). She also answers yes to one of the questions asking if she would accept rehabilitative services if they were available. The information is passed on to the local authorities, her existing children are turned over to the state, and her in uteri child is taken upon birth and placed in foster care while the mother is forced into treatment so she can get her children back.\nThe case goes to court; the mother sues the state of Florida and its public health system for breach of confidentiality. She loses, based on the required reporting of suspected child abuse. The case has gone to the Supreme Court.\nThere are a few ethical issues that are evident in this brief breakdown of this story.\nMom disclosed willingly the information, she evidently had not acted in a manner to bring attention to herself, she disclosed the information believing it would be kept confidential.\nThe case was brought to court and the argument was that the mother was abusing her unborn child by snorting cocaine while pregnant hence the defense in reporting based on the abuse that was purportedly to her unborn child. Can we mandate good health practices by a pregnant women or risk being charged with child abuse and neglect?\nShould willing disclosure be punished especially when it is a part of research?\nThere are many instances of ethical concerns when it comes to confidentiality and privacy matters in research. As a result of allowing a participants information to be revealed there can be insurmountable damages, it should only be done when absolutely necessary.""]"	['<urn:uuid:bf7258af-61a9-4416-8c17-f5e6de333fd8>']	factoid	direct	long-search-query	similar-to-document	single-doc	expert	2025-05-12T21:31:42.699391	8	35	1184
43	When comparing the educational impact of fiction versus legal cases, how do The Kite Runner and The Da Vinci Code case differently contribute to Western understanding of specialized topics?	The Kite Runner has contributed to Western education about life in Afghanistan and the immigrant experience, requiring no atlas or translation to engage readers, and has been incorporated into curriculum subjects including civic social and political education. Meanwhile, The Da Vinci Code legal case has educated the public about fundamental copyright principles through a high-profile court battle, demonstrating how non-literal copying claims work differently for different types of literary works, with the case receiving significant attention as evidenced by The Holy Blood and the Holy Grail climbing to the UK top 40 due to the controversy.	"[""Language, english genre historical fiction drama publisher, riverhead books publication date may 29, 2003 pages, 372 isbn 1-57322-245-3 oclc 51615359 dewey decimal 813/6 21 lc class, ps3608o832 k58 2003 the kite runner is the first novel by afghan-american author khaled hosseini published in hosseini responded in an interview, they never say i am speaking about. “he was just a hazara” : ethnic rivalry between two brothers (dr jennifer minter, english works notes) khaled hosseini's the kite runner, set in afghanistan. The kite runner by khaled hosseini, chapters 15–20 as a pause in the incessant stream of words spewing from his mouth: “ terrible what. Welcome to the second guide where you are provided with 20 the kite runner essay topics these topics will help you have a better idea on.\nKhaled hosseini the kite runner lesson plans - free english learning and teaching resources from words are presented in context and with definitions.\nThe kite runner chronicles the lives of specifically two boys, amir and hassan within the politically ask students to work in groups to brainstorm what the words or genghis khan, we defeated the armies of the british empire and we will. Read the kite runner book reviews & author details and more at amazonin he teased hassan for the words he had never heard of as he was an illiterate hassan see and discover other items: love novels, english novel, english novels. The kite runner has probably done more to educate western readers about life in curriculum subjects: civic social and political education english history. Retrouvez the kite runner et des millions de livres en stock sur amazonfr he was preparing his speech for the next day, flipping through a havoc of betrayal and salvation that requires no atlas or translation to engage and enlighten us.\nThe kite runner [khaled hosseini] on amazoncom publisher: riverhead 1st us ed edition (june 2, 2003) language: english isbn-10: 1573222453 rahum khan's words resonated so deeply with me, “there is way to be good again. Download the app and start listening to the kite runner today - free with a 30 day release date: 08-01-03 language: english publisher: simon & schuster audio there is something inherently fascinating about listening to the words. They'll also pick up some farsi words and understand more about the experience of afghani immigrants to the united states the kite runner also presents.\nThe kite runner by khaled hosseini was challenged for sexual content this addresses the concerns of those who object to certain words and national council of teachers of english, association of american publishers. The kite runner (2007) trivia on imdb: cameos, mistakes, spoilers and more said uncle saifo's lines in dari are completely different from the english subtitles he mentions in the documentary words from the kite runner also on the. For anyone studying english as a level, you may find it quite daunting to remember the message of the kite runner is didactic (intended to teach) this is direct speech as is in speech marks because it was said by the taxi driver.\nThe kite runner has sold an astonishing 125 million copies in paperback, driven by word-of-mouth at a moment when sales of fiction are. This is unlike the sort of archetypal characters i wrote in kite runner coming to america as a 15 year old, not speaking a word of english,. The kite runner is a controversial novel written by khaled hosseini – an author of the afghan-american heritage read the whole kite runner."", 'Copyright in the Courts: The Da Vinci Code\nThe Da Vinci Code is still topping sales charts, while The Holy Blood and the Holy Grail has climbed to the U.K. top 40 as a result of the controversy. (Courtesy of Random House UK)\nBy Uma Suthersanen\nA central principle of copyright is that it protects the expression of ideas and not the ideas themselves. This was at the fore of a high profile court case in the United Kingdom (U.K.) in April. In question was Dan Brown’s worldwide best-seller,\nThe following account of the case was written for WIPO Magazine by Dr. Uma Suthersanen of the Intellectual Property Law & Policy department of Queen Mary, University of London. Dr. Suthersanen is Chair of the British Literary & Artistic Copyright Association (ALAI-UK), and sits on the Legal Advisory Board of Creative Commons (England & Wales).\nThe case against the publishers of The Da Vinci Code was brought by Michael Baigent and Richard Leigh, the authors of a non-fiction work, The Holy Blood and The Holy Grail, which was first published in 1982. Messrs. Baigent and Leigh claimed that The Da Vinci Code was an infringement of their copyright in their book.\nAt the center of the dispute was a ‘hypothesis’ presented in The Holy Blood and The Holy Grail concerning the early Christian legend of the holy Grail. (The theme of the quest for the lost Grail - i.e. the cup, or chalice, used by Jesus Christ at the Last Supper - was a popular theme in medieval tales of chivalry and has inspired countless writers, film-makers and historians through the ages.) The core of the authors’ hypothesis in The Holy Blood and The Holy Grail was that references to the Grail in early manuscripts were disguised references not to the chalice, but rather to holy blood or Sang real, i.e. to the bloodline of Jesus Christ, and to the belief that this bloodline - through marriage between Jesus Christ and Mary Magdalene - had continued and merged with the French Merovingian dynasty.\nIn their book, Michael Baigent and Richard Leigh argue that the Holy Roman Church and its successors had sought to suppress this bloodline, but that a powerful secret sect, the Priory of Sion, was formed to protect this ""grail."" Baigent and Leigh used six known ‘indisputable’ historical facts, or supposed facts, though their conclusion was the result of ‘historical conjecture’ based on those facts. This quasi-historical approach was also the basis of various other published hypotheses as to the merging of Christ’s bloodline with the Merovingian bloodline.\nDan Brown is a popular fiction writer, and his book, The Da Vinci Code, has been the number one best-selling novel in Europe and U.S. for months. The Da Vinci Code is a murder mystery. It opens with the death - in Paris’ Louvre museum - of the Grand Master of the Priory of Sion, Jacques Sauniere. Seeking to solve his murder, the heroes of the story are led on a Grail quest, in which they must unravel a series of puzzles based on the history of the Priory of Sion and on the secret behind Christ’s bloodline.\nThere was no doubt that Dan Brown had drawn on The Holy Blood and The Holy Grail. Indeed, there was a clear and explicit reference to the book in The Da Vinci Code, and the name of one of the characters, Sir Leigh Teabing, was based on an anagram of the names of the two authors.\nKey legal issues\nThe court did not set out any novel legal ideas. Instead, much of the decision was based on the application of established legal principles to the facts at hand.\nBaigent and Leigh claimed copyright in the literary work, and alleged that Dan Brown had copied the way in which they had made the sequence of connections of the facts of the merging of the bloodlines. Since there was little copying of the actual text of The Holy Blood and The Holy Grail, the claim was that there had been non-literal copying of a substantial part of their literary work.\nThe general principle in copyright law is that copyright protects expression and not ideas. Moreover, The Holy Blood and The Holy Grail is an ""historical"" book, or at least it is comprised largely of historical facts which are unprotectable ideas. Baigent and Leigh based their case, therefore, on the claim that Brown had taken a substantial part of the ""manner"" in which they had expressed those ideas, as opposed to taking the ideas themselves.\nA paradox exists in relation to works based on historical facts: such a work can achieve copyright protection; but subsequent creators can draw on those facts; nevertheless, no one is allowed to appropriate the labor of the original author.\nThe court held that, while the evidence was clear that Dan Brown and his primary researcher (his wife) had drawn on The Holy Blood and The Holy Grail to a greater extent than Brown had acknowledged, this did not mean that they had infringed copyright in the book. Rather, they had used The Holy Blood and The Holy Grail, and other books, to provide general background material for the writing of The Da Vinci Code.\nThe significance of the case for copyright law relates to the fact that the lawyers acting for Baigent and Leigh attempted to make - and lost - an argument that there can be non-literal copying of a work of literature. The non-literal argument has previously been successfully used, usually in the case of computer programs or recipes or knitting patterns.\nIntriguingly, the judge, Mr. Justice Peter Smith, hid a coded message of his own in the judgment. Using a simple code, including a keyword based on the Fibonacci Sequence employed in the Da Vinci Code, the judge’s message refers to a World War I British admiral. The judge subsequently explained that the admiral was a hero of his, and that the trial coincided with the centenary of the launch of one of the admiral’s ships. Never let it be said that English judges are staid and boring!']"	['<urn:uuid:f4c3e189-b74c-4385-8527-23b7ebc18784>', '<urn:uuid:6027fcc7-3018-4e92-9f4a-054dcf4af0a3>']	open-ended	direct	verbose-and-natural	distant-from-document	comparison	expert	2025-05-12T21:31:42.699391	29	96	1603
44	research product lifecycle influence profit objectives comparison distribution channel considerations	The product lifecycle significantly impacts both profitability objectives and distribution decisions. During the introduction stage, companies face negative profits due to high costs and low sales volume, despite attempting to maximize profits over the entire lifecycle. Companies must choose between two competing objectives - increasing market share or increasing profitability, as these cannot be achieved simultaneously. Distribution channels evolve throughout the lifecycle: starting with selective distribution in the introduction phase, expanding to more intensive distribution during growth to meet increasing demand, and adding new channels during maturity to face competition. The distribution strategy is supported by various distributor compensation methods, including trade discounts for inventory holding and bulk buying, quantity discounts, promotional discounts, and cash discounts for prompt payments.	['Product Life Cycle (PLC)\nA new product passes through set of stages known as product life cycle. Product life cycle applies to both brand and category of products. Its time period vary from product to product. Modern product life cycles are becoming shorter and shorter as products in mature stages are being renewed by market segmentation and product differentiation.\nCompanies always attempt to maximize the profit and revenues over the entire life cycle of a product. In order to achieving the desired level of profit, the introduction of the new product at the proper time is crucial. If new product is appealing to consumer and no stiff competition is out there, company can charge high prices and earn high profits.\nStages of Product Life Cycle\nProduct life cycle comprises four stages:\n- Introduction stage\n- Growth stage\n- Maturity stage\n- Decline stage\nProduct is introduced in the market with intention to build a clear identity and heavy promotion is done for maximum awareness. Before actual offering of the product to customers, product passes through product development, involves prototype and market tests. Companies incur more costs in this phase and also bear additional cost for distribution. On the other hand, there are a few customers at this stage, means low sales volume. So, during introductory stage company’s profits shows a negative figure because of huge cost but low sales volume.\nAt introduction stage, the company core focus is on establishing a market and arising demand for the product. So, the impact on marketing mix is as follows:\nBranding, Quality level and intellectual property and protections are obtained to stimulate consumers for the entire product category. Product is under more consideration, as first impression is the last impression.\nHigh(skim) pricing is used for making high profits with intention to cover initial cost in a short period and low pricing is used to penetrate and gain the market share. company choice of pricing strategy depends on their goals.\nDistribution at this stage is usually selective and scattered.\nAt introductory stage, promotion is done with intention to build brand awareness. Samples/trials are provided that is fruitful in attracting early adopters and potential customers. Promotional programs are more essential in this phase. It is as much important as to produce the product because it positions the product.\nIn this stage, company’s sales and profits starts increasing and competition also begin to increase. The product becomes well recognized at this stage and some of the buyers repeat the purchase patterns. During this stage, firms focus on brand preference and gaining market share. It is market acceptance stage. But due to competition, company invest more in advertisement to convince customers so profits may decline near the end of growth stage.\nAffect on 4 P’s of marketing is as under:\nAlong with maintaining the existing quality, new features and improvements in product quality may be done. All this is done to compete and maintain the market share.\nPrice is maintained or may increase as company gets high demand at low competition or it may be reduced to grasp more customers.\nDistribution becomes more significant with the increase demand and acceptability of product. More channels are added for intensive distribution in order to meet increasing demand. On the other hand resellers start getting interested in the product, so trade discounts are also minimal.\nAt growth stage, promotion is increased. When acceptability of product increases, more efforts are made for brand preference and loyalty.\nAt maturity stage, brand awareness is strong so sale continues to grow but at a declining rate as compared to past. At this stage, there are more competitors with the same products. So, companies defend the market share and extending product life cycle, rather than making the profits, By offering sales promotions to encourage retailer to give more shelf space to the product than that of competitors. At this stage usually loyal customers make purchases.\nMarketing mix decisions include:\nAt maturity stage, companies add features and modify the product in order to compete in market and differentiate the product from competition. At this stage, it is best way to get dominance over competitors and increase market share.\nBecause of intense competition, at maturity stage, price is reduced in order to compete. It attracts the price conscious segment and retain the customers.\nNew channels are added to face intense competition and incentives are offered to retailers to get shelf preference over competitors.\nPromotion is done in order to create product differentiation and loyalty. Incentives are also offered to attract more customers.\nDecline in sales, change in trends and unfavorable economic conditions explains decline stage. At this stage market becomes saturated so sales declines. It may also be due technical obsolescence or customer taste has been changed.\nAt decline stage company has three options:\n- Maintain the product, Reduce cost and finding new uses of product.\n- Harvest the product by reducing marketing cost and continue offering the product to loyal niche until zero profit.\n- Discontinue the product when there’s no profit or a successor is available. Selling out to competitors who want to keep the product.\nAt declining stage, marketing mix decisions depends on company’s strategy. For example, if company want to harvest, the product will remain same and price will be reduced. In case of liquidation, supply will be reduced dramatically.\nLimitations of Product Life Cycle (PLC)\nProduct life cycle is criticized that it has no empirical support and it is not fruitful in special cases. Different products have different properties so their life cycle also vary. It shows that product life cycle is not best tool to predict the sales. Sometimes managerial decisions affect the life of products in this case Product Life Cycle is not playing any role. product life cycle is very fruitful for larger firms and corporations but it is not hundred percent accurate tool to predict the life cycle and sales of products in all the situations.', 'Product pricing is one of those aspects of the marketing plan that is neglected\n– in some plans it is completely absent! The reason for this stems from the\nfact that all other aspects of marketing can be easily identified and quantified\nwhereas pricing is more of an ‘art’ than a science.\nThe product pricing issues that face any company are very\ncomplex in nature – due to the numerous variables that have to be considered\nbefore understanding pricing parameters.\nA company may have two objectives in its pricing policy. One may be to increase\nmarket share – the other to increase profitability. It is NOT possible to\nachieve both objectives simultaneously.\nWe will now consider the factors that should be taken into\naccount when trying to resolve the question of whether the business should try\nto gain market share or increase profitability.\nThese factors are:\nObjective of the business and the product portfolio;\nThe product life cycle;\nCosts (own and competitors);\nChannels of distribution\nObjectives and the product portfolio\nEvery business should have a series of objectives relating to sales, profits,\nmarket share and return on capital. The business objectives might be such that\nit calls for short term profits. However, the business owner should be conscious\nthat any decision made only for short term profitability will impact the long\nterm survival of the business. This is especially true if pricing decisions are\nmade in an ad-hoc manner it is probable that the goodwill that the company has\ngenerated with its customers will be destroyed due to unreasonably high prices.\nThe setting of marketing objectives for any particular product is therefore\nwithout doubt the starting point in any consideration of pricing.\nProduct life cycle\nAccording to the product life cycle a product passes through 4\nstages, they are:\nIntroduction: Product has been just introduced to the\nmarket – so the price will typically be at the higher end;\nGrowth: this is the stage in which a products sales\nincreases rapidly – price is set high at the growth phase to skim the\nMaturity & saturation: Here the product reached\noptimal sales and plateaus out – price is gradually reduced to maintain\nmarket share and meet the threat of competition;\nDecline: The product ceases to be popular due to a\nvariety of reasons and decline sets in – price cutting is in full swing to\nmake sure that all inventory is exhausted before market demand runs out, a\ntype of harvesting the market.\nThe importance of the product life cycle in pricing cannot be\nunderstated. Obviously the pricing strategies at the different stages should\ndiffer to ensure that profits are maximized.\nThe term product positioning has already been explained. It is a\nvery important concept in setting the price of the product. It is clearly very\nfoolish to position a product as a high quality exclusive item, and then price\nit too low.\nPrice is one of the clearest signals that the customer has about\nthe value of the product being offered. So there should always be a sensible\nrelationship between the product and the price.\nCompetition and potential competition\nAlthough the product has been well positioned there will always\nbe competitors and it goes without saying that the threat of the competition\nshould be carefully considered. In a situation of high competition it is\nimportant to note that competing purely on price is counter productive. The\nbusiness should consider all elements of the marketing mix and how they interact\nto create demand and value for the product should be considered in setting the\noverall competing strategy.\nSome firms launch new products at high prices only to find that\nthey have made the market attractive to competitors who will launch similar\nproducts at much lower prices. A lower launch price might make diffusion in the\nmarket quicker and allow for greater experience and the margin for a competitor\nto enter the market will be reduced.\nAnother key variable in pricing is costing – this is not only\nthe business cost but also the cost to competitors. There are many cost concepts\nbut the two main concepts are marginal cost pricing and full absorption costing.\nThe conventional economists model of product pricing indicates\nthat pricing should be set at the point where marginal cost is equal to marginal\nrevenues i.e. where the additional cost of production is equal to the additional\nincome earned. The theory is undisputed but considers only price as variable. In\nthe real world there are many more variables than only price.\nIn practice the cost of production provides key guidelines to\nmany businesses in setting price. This is called the ‘cost plus method‘ of\npricing where a fixed mark up is added to the price.\nChannels of distribution\nThe standard product pricing theory does not provide insight to\nwhat should be one’s policy toward distributor margins. The distributor\nperforms a number of functions on behalf of the supplier which enables which\nenables the exchange transaction between the producer and the customer.\nThere are a number of devices available for compensating the\ntrade intermediaries, most of which take the form of discounts given on the\nretail selling price to the ultimate customer.\nTrade discount – This is the discount made on the\nlist price for services made available by the intermediary. e.g. holding\ninventory, buying bulk, redistribution etc.\nQuantity discount – A quantity discount is given to\nintermediaries who order in large lots\nPromotional discount – This is a discount given to\ndistributors to encoutage them to share in the promotion of the products\nCash discount - In order to encourage prompt payments\nof accounts, a small cash discount on sales price can be offered.\nGaining competitive advantage\nIt is possible to use price as a strategic marketing tool. The\naspects of competitiveness have been listed below:\nReduce the life cycle/ alter the cost mix – customers are\noften willing to pay a considerably higher initial price for a product with\nsignificantly lower post-purchase cost.\nExpand value through functional redesign. E.g. a product\nthat increases customers production capacity or throughput, product that\nimproves quality of the customers product, product that enhances end-use\nExpand incremental value by developing associated\nintangibles. For example service, financing, prestige factors etc.\nPreparing the product pricing plan\nWe have considered some of the factors that affect the pricing\ndecision. We now have to amalgamate all these decisions into one framework. It\nhas been demonstrated that as a firm develops expertise in producing a\nparticular product the cumulative cost of producing every additional unit falls.\nThis is demonstrated by the learning curve. The effect of the learning curve\nshould be considered in pricing of new products.\nThere are in principle only two main pricing policies they are\nprice skimming policy and price penetration policy. The factors that should be\nconsidered before implementing either policy are given below.\nThe factors that favour a price skimming policy are:\nDemand is likely to be price inelastic;\nThere are likely to be different price market segments,\nthereby appealing to those buyers first who have a higher rage of acceptable\nLittle is known about the cost and marketing the product\nThe variables that favour a price penetration policy are:\nDemand is likely to be price elastic;\nCompetitors are likely to enter the market quickly;\nThere no distinct price-market segments;\nThere is possibility if large savings in production and\nmarketing costs if large sales volumes can be generated.\nGo back to marketing plan guide\nDetailed articles and tools on product pricing']	['<urn:uuid:aa60928f-ab20-432c-8d85-73d3ea96cb19>', '<urn:uuid:fe4ff945-1ca6-4ec6-98f6-78aad893c305>']	open-ended	with-premise	long-search-query	distant-from-document	multi-aspect	expert	2025-05-12T21:31:42.699391	10	119	2238
45	dosage diflucan candida cns infection mortality	For internal fungal infections, Diflucan (fluconazole) is administered at 400mg on the first day followed by 200-400mg once daily for 6-8 weeks or longer if needed. In CNS Candida infections specifically, mortality approaches 100% without therapy and remains 10-30% even with treatment. Poor outcomes are associated with delayed diagnosis, low CSF glucose levels (<35 mg/dL), and development of intracranial hypertension or focal neurological symptoms. Treatment should be monitored carefully as some Candida species can develop antifungal resistance, and therapy modification may be needed based on clinical response and culture results.	"[""Diflucan is an anti-fungal antibiotic which used to treat fungal infections called candidiasis. This medication is prescribed for treatment such type of infections as vaginal, throat and fungal infections, infections of the urinary tract, peritonitis, and pneumonia. Diflucan works by killing sensitive fungi by interfering with the formation of the fungal cell membrane.\nDosage and direction\nTake it orally once a day, with or without food using the measuring spoon. Take it at the same time each day if you want to get best result with a full glass of water. Diflucan make you feel better after 24 hours. However, your symptoms go away completely just after few days. If there is no change in your health after a few days consult with your doctor immediately.\nDosage for adults various from type of infection and consist:\n1) Genital thrush - 150mg as a single dose\n2) Mucosal thrush - dose depends on where the infection is located 50mg once daily during month.\n3)Fungal skin infections - 50mg once daily for 2-4 weeks ;\n4) Internal fungal infections 400mg on the first day then 200-400mg once daily for 6-8 weeks or longer if needed.\n5) To stop you from getting fungal infections - 50-400mg once daily while you have risk to get an infection;\n6) To stop infection caused by Cryptococcus from coming back - 100-200mg once daily indefinitely\nDosage for children 4 weeks to 15 years old also various from type of infection and consist:\n1) mucosal infections - 3mg/kg once daily;\n2)internal fungal infections - 6-12mg/kg once daily;\n3)prevention of fungal infections - 3-12mg/kg once daily while at risk of getting an infection.\nFor children 2-4 weeks old it is same dose as above but given once every 2 days. A maximum dose of 12mg/kg every 2 days.\nFor children less than 2 weeks old it is same dose as above but given once every 3 days. Do not give more then 12mg/kg every 3 days.\nNote: this instruction presented here just for review. It's very necessary to consult with your doctor before using. It help you to get best results.\nFor vaginal yeast infections, doctors usually prescribe a single dose of Diflucan. If the yeast infection does not improve after 5 days, contact your doctor as soon as possible. Be careful with this medication if your medical history include liver disease or kidney disease. Although uncommon, this drug may make you dizzy, so be more attentive while you are driving or using machinery. Limit alcohol intake. Diflucan should not be used during pregnancy, becoming pregnant or lactating without doctor's advice. Do not use before breast-feeding without doctor's advice.\nDiflucan is not allowed to people which are hypersensitivity to Fluconazole or other anti fungal azole preparations, eg clotrimazole, ketoconazole, itraconazole and other. Also Diflucan contraindicated in:\nKidney and liver disorders;\nCardiovascular diseases including heart diseases(heart rate or rhythm disturbances);\nLow blood levels of calcium, magnesium or potassium.\nPossible side effect\nThey may include an allergic reaction: hives; difficulty breathing; swelling of your face, lips, tongue, or throat. Also the most possible side effects include:\nnausea, stomach pain, low fever, loss of appetite, dark urine, clay-colored stools, jaundice (yellowing of the skin or eyes);\nfever, chills, body aches, flu symptoms;\nsevere blistering, peeling, and red skin rash;\neasy bruising or bleeding, unusual weakness; or\nLess serious include:\nmild nausea, vomiting, diarrhea, or upset stomach;\nunusual or unpleasant taste in your mouth; or\nskin rash or itching.\nIf you experience one of them stop using Diflucan and tell your doctor as soon as possible. Also consult with your doctor about any side effect that seems unusual.\nDiflucan interact with such medication as:\na blood thinner such as warfarin (Coumadin)\ncyclosporine (Gengraf, Sandimmune, Neoral);\nan oral diabetes medicine such as glipizide (Glucotrol), glyburide (Diabeta, Micronase, Glynase), tolbutamide (Orinase), tolazamide (Tolinase), chlorpropamide (Diabinese), and others;\nrifabutin (Mycobutin) or rifampin (Rifadin, Rifater, Rifamate, Rimactane);\na sedative such as diazepam (Valium), lorazepam (Ativan), alprazolam (Xanax), or midazolam (Versed);\nseizure medication such as phenytoin (Dilantin) or valproic acid (Depakene);\ntheophylline (Theo-Dur, Theolair, Theochron, Elixophyllin, Slo-Phyllin, others).\nAlso note that interaction between two medications does not always mean that you\nmust stop taking one of them. As usual it affects the the effect of drugs, so consult with your doctor about how it interactions are being managed or should be managed.\nIf you forgot to take your dose in time, please do it as soon as you remember. But do not take if it is too late or almost time for your next dose. Do not take double or extra doses. Take your usually dose next day in the same regularly time.\nSymptoms of Diflucan overdose may include: confusion or unusual thoughts or behavior. If you experience one of them call your doctor immediately.\nStore at room temperature between 59-77 degrees F (15-25 degrees C) away from light and moisture, kids and pets. Do not use after expiration term.\nWe provide only general information about medications which does not cover all directions, possible drug integrations, or precautions. Information at the site cannot be used for self-treatment and self-diagnosis. Any specific instructions for a particular patient should be agreed with your health care adviser or doctor in charge of the case. We disclaim reliability of this information and mistakes it could contain. We are not responsible for any direct, indirect, special or other indirect damage as a result of any use of the information on this site and also for consequences of self-treatment."", ""Although the first case of Candida meningitis was reported in 1933, Candida species have increasingly been described as important causes of central nervous system (CNS) infections.1 The rising occurrence of these infections mirrors the growing populace of immunocompromised hosts and patients with recent neurosurgical procedures; a review at 1 institution found that Candida is the fourth leading cause of meningitis among neurosurgical patients.2 Risk factors for Candida CNS infections include neurosurgical procedures, broad-spectrum antibiotics, indwelling catheters (eg, ventricular shunts), intravenous drug use, immunosuppressive therapy, and parenteral nutrition.2-4 Specific populations at risk consist of low birth weight infants and immunocompromised patients with acquired immune deficiency syndrome, cancer, or congenital disorders including severe combined immunodeficiency, chronic granulomatous disease, and myeloperoxidase deficiency.3,5-10 Most cases are caused by Candida albicans, followed by Candida parapsilosis, Candida tropicalis, and Candida krusei.11,12 We describe a rare case of meningitis and ventriculitis caused by Candida lusitaniae and provide a review of the literature summarizing the clinical manifestations, diagnosis, and treatment of this unusual infection.\nAn 84-year-old man underwent a ventriculoperitoneal (VP) shunt for the treatment of normal pressure hydrocephalus. The patient developed acute mental status changes and seizure activity on postoperative day 1; imaging showed an intracranial hemorrhage. He was treated with 5 days of intravenous Decadron to diminish cerebral edema. His condition stabilized until hospital day 5 when he developed fevers (103°F); a cerebrospinal fluid (CSF) specimen revealed 730 white cells/μL, with 93% neutrophils, 6000 red cells/μL, protein of 356 mg/dL, and glucose of 13 mg/dL. The CSF culture grew Enterobacter aerogenes sensitive to third-generation cephalosporins, carbapenems, and aminoglycosides. A computed tomography of the abdomen was negative for a viscus perforation or an abscess at the shunt tip. The shunt was externalized, and the peritoneal catheter tip grew Enterobacter. The patient was treated with intravenous cefotaxime and intrathecal gentamicin (2 mg in 3 mL of sterile dextrose for 5 days). Cerebrospinal fluid cultures became negative, and indices improved. The VP shunt was removed on day 18, and the VP shunt tip grew Enterobacter despite 12 days of antibiotic therapy with cefotaxime. A new internal shunt was simultaneously placed because the prior shunt tip abutted a blood vessel, and there was concern for recurrent intracranial bleeding.\nThe patient developed severe Clostridium difficile colitis, and the antibiotic regimen was switched to meropenem, intravenous metronidazole, and oral vancomycin; the latter 2 drugs were administered for the treatment of C. difficile. After improvement of the diarrhea, the patient was again treated with intravenous cefotaxime and remained in a stable, but bedridden, condition, with the ability to follow some commands. All follow-up CSF cultures obtained via the VP shunt were negative.\nOn hospital day 36, the patient developed decreased mental status, and the CSF fluid sampled via the shunt showed a protein of 144 mg/dL, glucose of 51 mg/dL, and a white blood cell (WBC) count of 55 cells/μL. The VP shunt was removed, and a culture of the old ventricular shunt tip grew C. lusitaniae. The patient was initially treated with liposomal amphotericin B (AmBisome) 5 mg/kg per day as well as intrathecal amphotericin B at 0.2 mg daily via a ventriculostomy for 4 days. The regimen was then changed to liposomal amphotericin B and 5-fluorocytosine (5FC); however, the patient developed a rash necessitating discontinuation of the 5FC. Sensitivities of the organism showed a minimum inhibitory concentration to amphotericin B of 0.3 μg/mL, fluconazole of 0.5 μg/mL, and caspofungin of 0.25 μg/mL. The liposomal amphotericin B at 5 mg/kg per day was continued for 12 days and then switched to intravenous fluconazole 400 mg twice daily. Cerebrospinal fluid indices 3 weeks after the diagnosis of Candida meningitis/ventriculitis showed a WBC count of 44 cells/μL, protein of 103 mg/dL, and glucose of 65 mg/dL; cultures were sterile. A new shunt was placed after 6 weeks of antifungal therapy, at which time the CSF had normalized, with a WBC count of 2 cells/μL, protein of 46 mg/dL, and glucose of 66 mg/dL, with repeat negative cultures. Fluconazole was discontinued after a total of 42 days of therapy (7 days after the new shunt was placed). Despite successful clearance of the C. lusitaniae infection, his mental status remained poor, and he was unresponsive to verbal stimuli. He eventually died because of his underlying medical problems and ventilator-associated pneumonia.\nREVIEW OF THE LITERATURE\nWe performed search of MEDLINE (1966-2006), OVID (1950-1965), EMBASE (1974-2006), and the Parasitology Database (1972-2006) using the terms Candida or lusitaniae or yeast infections or fungi or mycotic AND meningitis or brain or neuro or ventriculitis or central nervous system infection to identify cases of C. lusitaniae CNS infection. We also searched for Clavispora lusitaniae, which is the teleomorph of C. lusitaniae, and included cases among large case series of fungal or Candida meningitis reports. We limited our search to human data in the English language.\nIncluding the current case, we found 7 cases in the literature, with most infections occurring in infants and children (Table 1)3,6,13-16; our case represented the oldest person to develop C. lusitaniae meningitis. The most common presenting signs were fevers and/or seizures. Like our patient, most published cases occurred in the setting of prior antibiotic therapy and a recent neurosurgical procedure including VP shunt placement; the likely route of these infections was direct inoculation of the organism into the CNS. The CSF indices were variable, with a normal or elevated WBC (mean, 196/μL; range, 8-8250/μL); the protein level was often increased (mean, 228; range, 61-585 mg/dL), but only 2 cases had a significantly depressed glucose level. None of the Gram stains was positive; all diagnoses were all established by CSF culture. Earlier cases were predominantly treated with amphotericin B and 5FC, whereas more recent cases were treated with fluconazole. Fifty-seven percent of cases survived.\nInfection due to C. lusitaniae was first described in 1979 in a leukemic patient with fungemia17,18; the first case of meningitis was reported in 1986.6 Our case involved a patient with a recent neurosurgical procedure with the presence of a VP shunt; the portal of entry was likely direct inoculation via this device. Whether the intrathecal gentamicin in a dextrose solution could have become contaminated is a possibility, although this is unlikely, given the numerous negative repeat CSF cultures in proximity to the time of administration. Like other Candida meningitis cases occurring in neurosurgical patients, our patient had preceding bacterial meningitis and had received broad-spectrum antibiotics; both are described risk factors.2,11 In addition, our patient had prolonged admission to the intensive care unit and parenteral nutrition and received intravenous steroids-all known risk factors for Candida infections.\nCandida CNS infections may occur in 3 primary settings: (1) among neurosurgical patients who had a recent craniotomy and/or have a VP shunt in place, as in our case; (2) as a manifestation of disseminated disease, especially among premature neonates; and (3) rarely as isolated chronic meningitis.10,19 Neurosurgical patients often present with meningitis and/or ventriculitis secondary to direct inoculation of the organism into the CNS, whereas those with disseminated disease usually have hematogenous spread of the organism into the parenchymal brain tissue (eg, microabscesses formation). Patients with isolated chronic meningitis, which likely also occurs by the hematogenous route, present similar to other cases of fungal (eg, cryptococcal) meningitis.19\nSigns and symptoms of Candida meningitis in neurosurgical patients may include headache, fever, seizures, and altered mental status; in addition, abdominal symptoms may occur in the presence of a VP shunt.11,20 Patients with microabscesses may have subtle or no neurological symptoms; hence, a high index of suspicion is necessary. Central nervous system involvement should be considered early because permanent neurological dysfunction may occur if the condition is not diagnosed and treated in a timely fashion. Rarely, patients with CNS Candida infections develop macroabscesses, vasculitis, or mycotic aneurysms, which may manifest by the development of focal neurological signs and/or seizures.10\nThe diagnostic workup of a possible Candida CNS infection should include blood cultures, CNS imaging, and a lumbar puncture. Candidemia is frequently found among patients with disseminated disease and CNS microabscesses, whereas those with neurosurgical-associated Candida meningitis often have sterile blood cultures because the source of their infection was at the local site. Patients with cerebral microabscesses are the most difficult to definitively diagnose owing to the small size of the lesions (<3 mm) and to the fact that the CSF is often unremarkable; often, these cases are diagnosed postmortem.10 Macroabscesses and vascular complications are typically detected on computed tomography or magnetic resonance imaging scans. Candida meningitis is established by obtaining CSF fluid. The CSF characteristics are typically undistinguishable from that of bacterial meningitis2; in cases of Candida meningitis, the CSF shows pleocytosis (mean, 600 WBC/μL) with an equal predominance of lymphocytes and neutrophils and an elevated protein; some cases exhibit hypoglycorrhachia (glucose, <40 mg/dL). The rate of wet mount or Gram stain positivity is reportedly 40%21 but was 0% in our series. The diagnosis usually relied on CSF culture results, which may lead to a delay in the diagnosis of 2 to 7 days. Methods, to enhance Candida isolation from the CSF, have been previously described.10 Because C. lusitaniae shares several microbiological characteristics with C. tropicalis and C. parapsilosis, misidentification may occur.13 Differentiating characteristics include C. lusitaniae's ability to assimilate and ferment cellobiose and to assimilate rhamnose.\nThe significance of a single CSF positive culture among neurosurgical patients has been questioned, as this finding may be due to colonization rather than a true infection.22 In addition, culturing the organism from a catheter tip, rather from the spinal fluid itself, also may represent colonization versus a true infection. Data that suggest that an isolate is an invasive organism rather than a contaminant includes abnormal CSF indices, the lack of mixed bacterial flora within cultures, and the presence of risk factors and symptoms of meningitis.4,22 Cases must be accessed on an individual basis; we would suggest that great care should be taken in discarding a positive CSF culture result. Our case had both risk factors for the development of Candida meningitis and clinical and laboratory signs of meningitis at the time of fungal isolation.\nTreatment of Candida meningitis and parenchymal infections are unclear, given the lack of controlled trials. Treatment guidelines suggest that the treatment of choice is a combination of intravenous amphotericin B (0.7-1.0 mg/kg daily) and 5FC (25 mg/kg 4 times daily).2,23,24 Other potential regimens include intravenous lipid formulations of amphotericin B, intrathecal amphotericin B, or intravenous fluconazole.7,25,26 Lipid formulations of amphotericin B have been used successfully in Candida meningitis among neonates.26 Intrathecal amphotericin B is usually reserved for severe cases because of its potential to cause chemical arachnoiditis or ventriculitis.9 Fluconazole has been used alone or in combination with 5FC and can be considered when amphotericin B is not an option.10,19 Therapy should be continued for a minimum of 4 weeks after the CSF is sterilized and neurological symptoms/signs have resolved or stabilized.24 For patients with CNS infections associated with a VP shunt, removal is recommended for eradication of the organism, which may adhere to the biofilm layer of the device.2\nTherapy for C. lusitaniae meningitis may be problematic considering the possibility for primary or secondary resistance to amphotericin B. The first clinical case of C. lusitaniae in the literature described the organism's ability to develop resistance to amphotericin B.17 Since this time, studies have shown that C. lusitaniae, unlike most other Candida species, can generate amphotericin B-resistant lineages from previously sensitive colonies at a rate of 1 in 104 cells in vitro.27 Emergence of resistance has been attributed to alteration in the expression of ergosterol biosynthesis genes.28 Most experts recommend performing susceptibility testing on an initial Candida isolate causing a CNS infection. If the patient clinically deteriorates, or if cultures remain positive, repeat susceptibility testing on subsequent isolates and antifungal therapy modification because of the possibility of the development of resistance have been recommended. That being said, a recent review of C. lusitaniae infections showed that susceptibility testing results for amphotericin B did not predict clinical response or outcome.29 At this time, the treatment of choice for C. lusitaniae is unknown, but fluconazole may be the preferred agent because of its efficacy against most isolates and its favorable pharmacological profile including good CNS penetration.\nThe mortality rate of Candida meningitis without therapy approaches 100%; despite the availability of antifungal agents, the death rate remains substantial at approximately 10% to 30%.3,8,11,20,22 Mortality rates for C. lusitaniae CNS infections were 43% in our review. A poor outcome among patients with Candida meningitis has been associated with a delayed diagnosis, low CSF glucose levels (<35 mg/dL), and the development of intracranial hypertension or focal neurological symptoms.11\nIn summary, Candida species are an increasingly recognized cause of meningitis, especially among neurosurgical patients and immunocompromised hosts who have received prior broad-spectrum antibacterial therapy. Nonalbicans Candida infections of the CNS will likely rise because of the use of prophylactic antifungal agents in at-risk populations. Novel diagnostic strategies are needed to achieve earlier diagnosis of these infections, which are typically only identified by culture. Prompt recognition and treatment are advocated, given the high mortality rate associated with these emerging infections.\n1. Smith LW, Sano ME. Moniliasis with meningeal involvement. J Infect Dis\n2. Nguyen MH, Yu VL. Meningitis caused by Candida\nspecies: an emerging problem in neurosurgical patients. Clin Infect Dis\n3. Huttova M, Kralinsky K, Horn J, et al. Prospective study of nosocomial fungal meningitis in children-report of 10 cases. Scand J Infect Dis\n4. Arisoy ES, Arisoy AE, Dunne WM Jr. Clinical significance of fungi isolated form cerebrospinal fluid in children. Pediatr Infect Dis J\n5. Fernandez M, Moylett EH, Noyola DE, et al. Candidal meningitis in neonates: a 10-year review. Clin Infect Dis\n6. Fleischmann J, Church JA, Lehrer RI. Case report: Candida\nmeningitis and chronic granulomatous disease. Am J Med Sci\n7. Marr B, Gross S, Cunningham C, et al. Candidal sepsis and meningitis in a very-low-birth-weight infant successfully treated with fluconazole and flucytosine. Clin Infect Dis\n8. Casado JL, Quereda C, Oliva J, et al. Candidal meningitis in HIV-infected patients: analysis of 14 cases. Clin Infect Dis\n9. McCullers JA, Vargas SL, Flynn PM, et al. Candidal meningitis in children with cancer. Clin Infect Dis\n10. Sanchez-Portocarrero J, Perez-Cecilia E, Corral O, et al. The central nervous system and infection by Candida\nspecies. Diagn Microbiol Infect Dis\n11. Bayer AS, Edwards JE, Seidel JS, et al. Candida\n12. Dorko E, Pilipcinec E, Tkacikova L. Candida\nspecies isolated from cerebrospinal fluid. Folia Microbiol (Praha)\n13. Sanchez PJ, Cooper BH. Candida lusitaniae\nsepsis and meningitis in a neonate. Pediatr Infect Dis J\n14. Leggiadro RJ, Collins T. Postneurosurgical Candida lusitaniae\nmeningitis. Pediatr Infect Dis J\n15. Sarma PSA, Durairaj P, Padhye AA. Candida lusitaniae\ncausing fatal meningitis. Postgrad Med J\n16. Kremery V Jr, Mateicka F, Grausova S, et al. Invasive infections due to Clavispora lusitaniae\n. FEMS Immunol Med Microbiol\n. 1999;23: 75-78.\n17. Pappagianis D, Collins MS, Hector R, et al. Development of resistance to amphotericin B in Candida lusitaniae\ninfecting a human. Antimicrob Agents Chemother\n18. Holzschu DL, Presley HL, Miranda M, et al. Identification of Candida lusitaniae\nas an opportunistic yeast in humans. J Clin Microbiol\n. 1979; 10:202-205.\n19. Voice RA, Bradley SF, Sangeorzan JA, et al. Chronic candidal meningitis: an uncommon manifestation of candidiasis. Clin Infect Dis\n20. Sanchez-Portocarrero J, Martin-Rabadan P, Daldana CJ, et al. Candida cerebrospinal fluid infection. Report of two new cases and review of the literature. Diagn Microbiol Infect Dis\n21. Lipton SA, Hickey VF, Morris JH, et al. Candidal infection in the central nervous system. Am J Med\n22. Geers TA, Gordon SM. Clinical significance of Candida\nspecies isolated from cerebrospinal fluid following neurosurgery. Clin Infect Dis\n23. Smego RA, Perfect JR, Durack DT. Combined therapy with amphotericin B and fluorocytosine for Candida\nmeningitis. Rev Infect Dis\n24. Pappas PG, Rex JH, Sobel JD, et al. Infectious Diseases Society of America. Guidelines for treatment of candidiasis. Clin Infect Dis\n25. Gurses N, Kalyci AG. Fluconazole monotherapy for candidal meningitis in a premature infant. Clin Infect Dis\n26. Scarcella A, Pasquariello MB, Giugliano B, et al. Liposomal amphotericin B treatment for neonatal fungal infections. Pediatr Infect Dis J\n27. Yoon SA, Vazquez JA, Steffan PE, et al. High-frequency, in vitro reversible switching of Candida lusitaniae\nclinical isolates from amphotericin B susceptibility to resistance. Antimicrob Agents Chemother\n28. Young LY, Hull CM, Heitman J. Disruption of ergosterol biosynthesis confers resistance to amphotericin B in Candida lusitaniae\n. Antimicrob Agents Chemother\n29. Hawkins JL, Baddour LM. Candida lusitaniae\ninfections in the era of fluconazole availability. Clin Infect Dis""]"	['<urn:uuid:15b14a9e-c89e-4097-976d-1fdce5b8e77b>', '<urn:uuid:d3fed38c-10b9-43ce-8b3b-a9aa3d1ee47c>']	open-ended	with-premise	short-search-query	similar-to-document	three-doc	expert	2025-05-12T21:31:42.699391	6	90	3670
46	aravind eye care system vision statement	The Aravind Eye Care System's vision is to eliminate needless blindness by providing high-quality, high-volume and compassionate eye care for all.	"[""Professor Rachel Williams FREng is a Professor of Ophthalmic Bioengineering at the University of Liverpool\n“An eye surgeon asked me to help improve their treatment of retinal detachment, a condition that occurs when the retina (the layer of tissue on which light is focused) starts to come away from the back of the eye. ”\nHow would you describe your current role to someone who knows nothing about engineering?\nEngineering is part of everyday life. Take a look at any object or structure around you: what is it made of? How is it shaped? Why? Its design and very existence are down to the creativity and work of engineers. In my case, engineering has involved designing materials and discovering ways to improve outcomes for patients with conditions that affect their vision.\nWhy did you choose to go into engineering?\nI was interested in design and making things from an early age and was always interested in science subjects at school. I studied maths, physics and chemistry at A-level. My parents were medics, and I became interested in the role of engineering in medical innovation and how it could make a real difference to people’s lives.\nPlease describe your first job.\nI have always been an academic engineer. Early in my career, I was working on understanding how materials behave when implanted in the human body. An eye surgeon asked me to help improve their treatment of retinal detachment, a condition that occurs when the retina (the layer of tissue on which light is focused) starts to come away from the back of the eye. This started a long and productive partnership with the clinicians in St Paul’s Eye Unit at the Royal Liverpool University Hospital.\nWhat do you like most about being an engineer?\nA career in engineering often involves working in multidisciplinary teams and for me this has led to many exciting opportunities. I have collaborated with clinicians and industrialists. My clinical colleagues have taught me so much about eyes and vision and how they are influenced by disease and ageing. I have learned how to best explain to them how engineering can develop solutions to their problems. The industrialists have helped to make these solutions a reality.\nTell us about an achievement that you are most proud of.\nI have recently been working on a project in South India. We heard startling statistics from our collaborators at the Aravind Eye Care System, who reported that 200 million people need eye care in India and less than 10% receive it. The Aravind Eye Care System’s vision is 'to eliminate needless blindness by providing high-quality, high-volume and compassionate eye care for all'. Our collaborative work aims to develop new ways to diagnose and treat sight-threatening conditions that can be delivered directly to patients within the community.\nHow has being a woman in engineering changed since you started working in the profession?\nAs a woman in academic engineering, and particularly in biomedical engineering, the numbers of women have, perhaps, been better than in other sectors. It is very encouraging to see the number of excellent and inspiring women engineers who are developing their careers in this field and who will be future leaders.\nWhat would you say to someone considering a career in engineering?\nAn education in engineering opens doors to such a variety of career opportunities. All it takes is your ambition and enthusiasm to apply your knowledge to the area of daily life that interests you the most. The opportunities are limited only by your imagination.\nThis year’s IWD theme is ‘Balance for Better’. How can engineers contribute to a gender-balanced world?\nWe need to inspire the younger generation of girls to embrace engineering as an exciting career and understand how their abilities can make a huge difference to society. The engineering community needs to evolve to support gender balance and demonstrate how this leads to excellence.\nThis profile was created for International Women’s Day in March 2019. All information was correct at time of publication""]"	['<urn:uuid:48df9f71-62a2-4da9-89c0-5389a7061f06>']	factoid	direct	short-search-query	similar-to-document	single-doc	expert	2025-05-12T21:31:42.699391	6	21	667
47	chinese surnames pre post immigration variation	Chinese surnames underwent significant changes before and after immigration to America. In traditional China, surnames were taught through primers like the Hundred Family Surnames text, which contained four to five hundred surnames along with their native place locations. After immigration to America, these surnames evolved differently: early immigrants (pre-1882) had their names spelled according to Cantonese dialects, while different spelling variants emerged based on various factors including different dictionaries, ports of departure, and personal preference. Some surnames were further altered through the 'paper name' system during the Exclusion Laws period, and new patronymic surnames developed when parts of fathers' Chinese names became surnames for their children.	"['As part of our ongoing work on the library’s Chinese collections (see previous blog post) we are beginning to digitise some of the most significant and interesting texts. Watch out for more soon! Here Gregory Scott, Lecturer in Chinese Studies introduces four items recently digitised. There are three widely used Chinese primers, plus one unique text that reflects a common genre of inscription used during the imperial era.\nEducation was a central concern of Chinese society from at least the Song dynasty (960 – 1279 CE), when a centralised exam system was greatly expanded and used to fill positions in an empire-wide bureaucracy of civil officials. Intended to promote meritocracy, excelling in these exams became a means of social advancement for families throughout the realm. Education started quite young, with private tutors and small schools using primers to teach the basic vocabulary of the Literary Chinese language, sometimes also called Classical Chinese. It served as the lingua franca of educated persons throughout East Asia from before the Common Era right up to the early twentieth century. Although likely originally based on the spoken languages of the Warring States period (475 – 221 BCE), Literary Chinese is quite different from any spoken Chinese dialect, and so even native speakers of Chinese have to learn its particular vocabulary, grammatical patterns, and stylistic conventions. Literary Chinese primers were designed to be memorised by students, and were normally also written so that at the same time as they learned the language, they also learned fundamental morality, geography, history, and other subjects. The content of the core texts of the Literary Chinese corpus, from basic primers to advanced classics, were common knowledge throughout the wider East Asian world.\nThree Character Classic\nSo-named because the text is made up of three-character phrases. It was the standard traditional primer for primary education, as it teaches Literary Chinese reading as well as elements of history and philosophy. The earliest editions were likely first composed in the 13th century CE, and are attributed to several authors but its original author remains unknown. This text remained widely used right up to the mid twentieth century, and it is still read today although it is no longer part of formal education. Virtually every educated person in early-modern and modern China, and elsewhere in East Asia, would have memorised this text. Printings of this text were widely available in relatively cheap woodblock printed editions.\nMuch of the middle of the text describes the dynastic history of China, which in this edition ends with the Qing dynasty (1644-1911) on p. n16. Another feature of this edition is that the upper quarter of the page is devoted to short aphorisms accompanied by illustrations.\n- Sanzi jing on Wikisource\n- Sanzi jing on the Chinese Text Project\n- A comparison of the Qing dynasty and Republican-era editions of the text\n- James T. C. Liu, “The Classical Chinese Primer: Its Three-Character Style and Authorship,” Journal of the American Oriental Society 105:22 (1985): 191-96.\nThousand Character Classic\nAnother example of a text intended for primary education, the Thousand Character Classic consists of one thousand unique characters grouped into 250 lines with four characters each. It is said to have been composed in the 6th century CE, and earliest extant copies date from the Tang dynasty (618-907 CE), but its original author or authors unknown. It was used in primary education throughout East Asia up to the twentieth century, intended to be memorized by pupils.\nThis edition also has a version of the Hundred Family Surnames printed in the uppermost register. The final three lines describe the origin of the text.\n- Qianzi wen on Wikisource\n- Qianzi wen on the Chinese Text Project\n- Barry C. Keenan, “Neo-Confucian Education,” in Neo-Confucian Self-Cultivation (Honolulu: University of Hawai`i Press, 2011), pp. 20-34.\nHundred Family Surnames\nThis text is a collection of four to five hundred single- and double-character surnames depending on the edition. The first examples of this text were originally compiled in the 10th or 11th century, and its original author is unknown. This text would be memorised as a primary education primer, and would teach pupils the essential surnames of established Chinese kin lineages. Each family name is accompanied by the name of their traditionally-recognized native place location.\nJiexiao shishi tu\nIllustrated True Stories of Chastity and Filiality\nNo other copies of this work were uncovered during our research. It is a collection of ink rubbings of stone inscriptions, pasted together in an accordian-style folding format. The original inscriptions were part of an ancestral shrine built in Shanghai in 1832, and are dated in sixth lunar month of the gengyin 庚寅 year of the Daoguang 道光 era [1830 CE]. The calligraphy was drawn by Xu Weiren 徐渭仁 (1788-1855.) The content of the inscriptions relates to values of filiality and chastity amongst women, which were important moral teachings found within didactic texts from this era.\n- Fei Siyen, “Writing for Justice: An Activist Beginning of the Cult of Female Chastity in Late Imperial China,” The Journal of Asian Studies 71:4 (2012): 991-1012.\n- Du Fangqin and Susan Mann, “Competing Claims on Womanly Virtue in Late Imperial China,” in Women and Confucian Cultures in Premodern China, Korea, and Japan, edited by Dorothy Ko, JaHyun Kim Haboush, and Joan R. Piggott (Berkeley: University of California Press, 2003), pp. 219-48.', ""Frequently Asked Questions\n1. How did Chinese immigration patterns to the United States affect Chinese American surnames?\nChinese immigration can be divided into several periods: 1850 to 1882, 1882 to 1943, 1944 to 1965, and 1965 to the present. The early immigrants who came before 1882 were almost all from Guangdong (Canton) province, speaking certain dialects of Cantonese and the Hakka language. They came when names in America were phonetically spelled according to the way they were heard or thought to sound. The particular Cantonese dialects that were encoded into names were Siyi (Seiyap) or Four Districts speech, particularly Taishan (Toihsaan), the predominant dialect; the Sanyi (Saamyap) or Three Districts speech and the Zhongshan (Jungsaan) speech. Most of the early Chinese immigrants to the Hawaiian Islands were Zhongshan and Hakka speakers. Therefore Chinese Americans, who are 5th and later generation descendants, have surnames that are reminders of the language spoken by their ancestors; surnames that are not usually found in dictionaries.From 1882 to 1944, Chinese immigration was severely restricted but merchants, teachers and students were allowed entry. During the early 1900s, dictionaries were used for name spelling according to Mandarin, standard Cantonese, Shanghai, and the dialects spoken in Fujian province. After 1950, after China became a communist country, thousands of foreign born Chinese remained as permanent residents; others emigrated as refugees from communism.\nIn 1965, when all restrictions on Chinese immigration were removed, there was the Chinese American population suddenly increased but the tremendous sharp increase came after 1980 when both People’s Republic of China and Taiwan received annual quotas of 20,000 immigrants a year. Suddenly, Mandarin sounding surnames were heard more often and this will continue to take place in the future as Mandarin replaces Cantonese as the lingua franca* of the Chinese in America.\n*a language that is adopted as a common language between speakers whose native languages are different\n2. Did the U.S. Chinese Exclusion Laws have any effect on Chinese American surnames?\nYes. Since the Cantonese predominated in the Chinese population prior to 1882, these laws (1882-1944) kept them as the dominant group for over a century of Chinese immigration. Surnames also remained predominantly Cantonese-sounding names for well over a century. The 2000 list of U.S. surnames based on Social Security records reveal that surnames of Mandarin pronunciation have surpassed those for Cantonese pronunciations.\n3. What are Wade-Giles and Pinyin, and why do they matter?\nBoth Wade-Giles and Pinyin are names of spelling methods — called romanization and latinization, respectively — to transcribe the Beijing standard for Mandarin pronunciation. The Wade-Giles system was named for the two British diplomats in China who devised it: Thomas F. Wade initiated it in mid-1800s and Herbert A. Giles who revised and published it in his Chinese-English dictionary of 1912. This occurred before Mandarin became China’s national language.\nDuring the early 1920s language reform program in the Republic of China (ROC), the Mandarin spoken in Beijing — Peking at the time — was selected as the national language. Wade-Giles became the most popular method used in the English-speaking world for transcribing names, placenames, and other information about China in articles, books, and reference materials.\nPinyin — meaning “spelled-out sounds” — was developed in the People’s Republic of China (PRC) during the 1950s. It is based on the spelling method used by the Russians to raise the literacy rate among its Chinese settlers. During the 1980s, our country and other countries officially replaced Wade-Giles with Pinyin latinization. This occurred in Taiwan in 2009 although its citizen may still use Wade-Giles for spelling their names.\nOne feature of Wade-Giles is the absence of the initial letters B, D, and G. These sounds are represented instead by P, T, and K and the usual sounds for these letters are indicated by an apostrophe: P’, T’, and K’. Because the apostrophe and other diacritic marks are not used in American names, some Chinese American surnames are terribly mispronounced; its owners having to acquiesce to the spelling pronunciation.\nSome Pinyin-spelled words can be baffling too. For example, the X in Xie should be pronounced “sh” as in “shield;” the Zh, as in Zhang, has a dj sound and the surname Qin is pronounced ch’in. Could one guess that the surname Cai is pronounced Tsai.\n4. How did you compile your lists of surnames?\nI looked for Chinese American materials that had names of Chinese Americans accompanied by their Chinese name. This is to ensure that the surname is Chinese in origin and the pronunciation is Chinese pronunciation. Resources included Chinese American business directories of various cities, organization and church membership lists, and articles and books about Chinese Americans. I visited cemeteries in various cities to search for gravestones and niches etched in English and Chinese.\n5. What is the difference among your three lists of Chinese American surnames?\nI wanted to illustrate the fact that one cannot assume the spelling of a Chinese American surname represents a particular surname character. Besides the diversity in spelling the same Chinese family name according to pronunciation, different family names could have the same spelling variations. Hence the compilation of the second list. As the Chinese began settling in America, unawareness of the differences in name traditions led to the development of new surnames; surnames that arose from a father’s Chinese name for his children, which are called patronymic surnames. Since numerous such surnames are found among 5th and later generations of Chinese and partial-Chinese Americans, a third list was created.\n6. What if my family’s surname isn’t on any of your three lists on this website?\nThere was never any intention to list every Chinese American surname that exists or existed. Also it is not possible to locate every single one. If you want to ascertain the surname character to your surname, you would have to do some researching. Some of the resources that are used for researching family genealogy include looking for documents that include signatures of names in English and Chinese, such as: Chinese records (up to 1943) in a National Archives branch, cemetery gravestones, Chinese students’ directories, and a trip to the birthplace of your ancestor.\n7. Is ascertaining my family’s correct Chinese surname difficult if I don’t read or write Chinese?\nYes, because it’s easy to mistake one Chinese character for another that looks almost similar in structure. Examples of lookalike surname characters can be seen in the surnames lists. You must look carefully to see that each stroke in the Chinese character matches up. You can also obtain help from a Chinese speaking person, such as a relative, the minister of a church, a teacher at a Chinese language school, or a professor at a university.\n8. Are cemetery gravestones really helpful in discovering or confirming accurate Chinese surname?\nIt’s helpful only when the deceased is identified by a name in English and a name in Chinese characters. Sometimes the surname character is etched separately from the Chinese given name or is placed on the back of the gravestone. You can also ask for help if the cemetery has a Chinese-speaking staff member.\nWhile doing research for the book Chinese American Names, gravestones were an excellent resource because I never found any errors in the surname characters. Unfortunately, since the year 2000, a few have been seen. The wrong character that was engraved looked almost the same as a surname character. For example, the word 鐘 (meaning “clock” or “bell”) has been mistaken for the common family name 鍾, which means “a wine container.” The two characters even have the same spelling in dictionaries: Chung in Cantonese, Zhong in pinyin. But the “clock” character is not for a family name.\n9. Why do different Chinese surname characters have the same spelling in English?\nEvery Chinese/English dictionary is filled with an abundance of words of different meanings that have the same spelling. But after the spelling for each character, there may be a number, from 1 to 4, or an accent mark, such as an umlaut. These diacritic marks indicate the tone or musical accent for pronouncing the word correctly. Every variety of spoken Chinese uses the tonal system, which helps to differentiate the meaning of spoken words when used in context with other words.\nBut accent marks or diacritic marks are not usually used in America for spelling names. Therefore the spelling cannot reveal the exact surname character. In some cases, the accent mark would not help when two different family names have the same sound and the same tone. You would still need to see the surname character.\n10. Why is there so much diversity in spelling the same Chinese surname?\nThere are many reasons because: 1) different letters can be used to convey the same sound; 2) the family settled in America when names were spelled according to the American way; 3) different Chinese-English dictionaries were used; 4) the spelling was received at a certain port of departure from China; 5) the Chinese name was spelled according to the system used in a country of former residence; 6) after settling in America, the surname was respelled to personal whim.\n11. Why was there a sudden proliferation of new Chinese American surnames?\nThe elimination of restrictions on Chinese immigration in 1965 enabled ethnic Chinese from all over the world to immigrate as long as they fulfilled U.S. requirements. The quotas granted the People’s Republic of China and the Taiwan government more than doubled the number of Chinese nationals to be admitted.\nEach wave of new immigrants brought additional spelling variants for Chinese family names already in this country. They also brought family names that hadn’t been seen before among Chinese Americans.\nThe entire Chinese American population in 1960 was less than 240,000. In 1980 the number jumped to over 800,000, just .003 percent of the total U.S. population of 240 million. Over 2 million were counted in the 2000 census. And the 2010 census listed close to 4 million, which included part-Chinese Americans and 230,000 Taiwanese Americans.\n12. What are the most common Chinese American surnames?\nThe latest information on American surnames of Chinese origin comes from the 2000 U.S. census list of surnames for Asian Americans. The surname Lee heads the list for the Chinese in America. It ranks 22 out of 1,000 common American surnames. Out of over 600,000 persons that have this surname–a surname for Americans of different ethnicity, about 230,000 were for those of Chinese ancestry. The next eleven common surnames, in the order of ranking (placed is in parenthesis) are: Chen (260), Wong (277), Yang (397), Wang (438), Chang (424), Chan (459), Li (519), Lin (624), Liu (650), Wu (683), and Huang (697).\nSome of these surnames also belong to Korean Americans. The surnames are likely spelling variants for the same family name: Chen and Chan; Wong and Huang. But this doesn’t change the ranking. Wong was the most common Chinese American surname according to the 1984 U.S. census listing of common American surnames, which was based on the 1980 census.\n13. What is a Chinese “paper name”?\nSeveral practices arose to circumvent the U.S. Chinese Exclusion Laws that limited Chinese immigration. Since certain categories of Chinese could be admitted, Chinese owners of stores enabled laborers and kinsmen to claim being merchants so they could bring their families. Some young men gained entry as “sons” of merchants. When it was discovered that foreign-born children of a citizen were able to enter this country, many Chinese were able to claim San Francisco as a birthplace by taking advantage of the destruction of legal documents and records in the 1906 San Francisco earthquake and fire. Since most Chinese had wives in China, they often made trips to see them. And it became common practice, on returning to America, to claim a son was born in China (seldom a daughter) and to have a certificate issued for him. Such certificates were later sold to ablebodied young men so they could come as a “son” of a citizen.\nThe Chinese called the owner of such a certificate a “paper son” and the Chinese name (transcribed in English) on the certificate, a “paper name.” The “paper son” had to use the name on the certificate as his legal name or face deportation if the ruse were discovered during the exclusion period. When a “paper son” married, his “paper surname” had to be handed on to his children. Nevertheless the “paper son” always kept his full Chinese name so that his children and next generations would know their actual family name. It is not unusual to hear a Chinese American say that his (or her) surname is not the real surname, but a “paper name.”\n14. Why do some Chinese surnames begin with “A” or “Ah”?\nThe type of surname that begins with the letter A or letters Ah is a reminder of the southern Chinese way of calling or referring to a person informally by name. For some unknown reason, almost all patronymics of this type arose among the early Chinese families in the Hawaiian Islands. Many surnames of this type can be seen in Hawaiian telephone directories.\n15. What is a patronym, patronymic surname, or “turned-around name”?\nMany early immigrants kept the Chinese tradition of stating and transcribing into English, the family name first in their names. As a result, in some families, part of the father’s Chinese name became the surname for his children. The Chinese called it a “turned-around name” because the last word of the father’s full name usually became the surname for his children.\nThis type of family name is called a patronym or patronymic surname, a common source from which surnames arose in most countries. Four types of patronyms occurred for the second generation and have been handed down to 5th and later generations of Chinese and partial-Chinese Americans. These types are listed in detail in the book Chinese American Names: Tradition and Transition. And the explanation is also given in the Listing of Chinese American Patronyms.\n16. What do you mean when you say that “Chinese is a tonal language” and why is that important to understanding Chinese American surnames?\nEvery variety of spoken Chinese (e.g., Mandarin, Cantonese, Hakka, etc.) uses the tonal system (or musical accent) for pronouncing the word correctly. This helps to differentiate the meaning of spoken words when used in context with other words. The same word sound spoken in two different tones means two different words; a beginning non-native Chinese speaker using the wrong tone can inadvertently create an odd or amusing sentence due to the error in tone usage.\n17. How can I find out the meaning behind my surname character?\nAlmost all Chinese family names are also common words with different definitions, which can be found in a Chinese-English dictionary. However, a Chinese speaker may say that a family name doesn’t have any meaning. The late Dr. Yuen Ren Chao, a renowned linguist, wrote in a letter to me: “Even if the character happens to be a common word with meaning, in the context in which it is used as a surname, one usually disregards the meaning except when intentionally making a pun of it.”\n18. What if my family’s surname is listed on your website, but the Chinese character you show does not match what our family uses?\nThe explanation would have to be found in your family history. The correct surname character would be in the Chinese name of the ancestor who started family life in the United States.\n19. I'd like to learn more about Chinese American surnames! Where can I buy your book, “Chinese American Names: Tradition and Transition”?\nIt can be ordered through any retail bookstore, such as Barnes & Noble or Books, Inc. Or you can buy it from an online bookstore, such as Amazon.com. Sometimes used copies are available at a lower price from an online bookstore. Click here to see it on Amazon\n© Emma Woo Louie 2015-2021 All rights reserved""]"	['<urn:uuid:b2aa173d-d915-43d7-98a3-e8eb7d7bec13>', '<urn:uuid:d5df6e3f-c676-4e87-886d-9e93c6d18a57>']	open-ended	direct	short-search-query	distant-from-document	comparison	expert	2025-05-12T21:31:42.699391	6	106	3551
48	bicep curl training benefits risks shoulder injury	Resistance band bicep curls provide benefits like increased strength, muscle size, and tone while helping balance arm muscles. However, excessive bicep training can lead to shoulder injuries, particularly affecting the long head of the bicep brachii muscles and potentially causing SLAP lesions (superior labrum antero-posterior lesions) at the bicep anchor point.	['Are you struggling to effectively target your biceps during your workouts? Are traditional bicep curls with dumbbells becoming too easy and not pushing you to your limits? Fear not, because you’re not alone. Many people have found themselves in this exact same situation. Perhaps you’ve been using weights that are too light, or you simply need to switch up your routine. Luckily, the solution is right at your fingertips: resistance band one arm bicep curls. In this post, we’ll dive into the benefits of incorporating resistance bands into your bicep routine and show you how to properly perform the one arm bicep curl with this simple yet effective tool.\nResistance Band One Arm Bicep Curls Summary\n- Primary Muscles: Biceps Brachii\n- Secondary Muscles: Brachialis, and Brachioradialis\n- Equipment: Resistance Band\n- Mechanics Type: Isolation\n- Force: Pull\n- Utility: Auxiliary\nResistance Band One Arm Bicep Curls Instructions\n- Secure the resistance bands to your anchor point down at the floor.\n- While facing away from the anchor point grab the handles with both hands.\n- The arm you are training should be extended to your side, and the other arm should pull the band tight. Now with you palm facing forward pull with your bicep to curl your arm.\n- Squeeze at the top and hold for a brief moment.\n- After then return your arm to the starting point.\n- Repeat for this arm 8-12 reps and then switch.\nResistance Band One Arm Bicep Curls Muscles\nBenefits of Resistance Band One Arm Bicep Curls\nResistance Band One Arm Bicep Curls are a great exercise for targeting the biceps brachii, one of the main muscles in the arm. This exercise strengthens the biceps brachii by using resistance bands to provide extra resistance during the curl. The resistance bands make the exercise more challenging and can help to increase strength, muscle size, and tone. Additionally, because the movement is unilateral (one arm at a time), it helps to even out any imbalances between both arms. Resistance Band One Arm Bicep Curls are a great way to add variety to your strength training routine and help build strong, toned biceps.\nTips for Performing Resistance Band One Arm Bicep Curls\nWhen you wish to attain the most beneficial results, adhere to these basic tips. Similarly, if you desire to reduce the likelihood of injuries, abide by these tips.\n- Use Multiple Bands To Help to Increase The Challenge. When you are easily performing 12 or more reps every exercise, then you need to raise the resistance. You can either do this by stretching the band further at the beginning of the movement or by using more bands. The recommended method is to add more bands given that it is easily repeatable to have a similar resistance on each workout.\n- Use A Mirror To Check Your Technique. You could think you are a little vain looking in a mirror while you are exercising, but you are not admiring yourself, you are insuring your technique is solid.\n- Slow Every Repetition Down For Improved Strength Gains. By Raising the time under tension you are exercising your muscles more and they will respond by developing stronger muscles. You can accomplish this without raising the weight by doing slow reps about 4 – 6 seconds flexing and pause then 4 – 6 seconds relaxing.\nBenefits and Tips Video\nFrequent Mistakes To Avoid\nYou ought to keep away from these typical mistakes to have ideal form and improved tonning. Furthermore\n- Don’t Allow Yourself To Cheat. In the majority of the time, cheating is applying momentum rather than the strain of your target muscle tissue. Once in a while, a little cheating on your last rep can be great to overload your muscle, although not for more than a handful of reps.\n- Stop Yourself From Using Too Little or Too Much Tension. Not enough, and you will not be sufficiently employing your target muscle, excess, and you’ll probably cheat. Ensure that you can conduct between 8-12 reps with correct technique.\nFind More Resistance Band Exercises Here\nVariations and Complementary Exercises\nIf you’re looking to vary up your routine or add more intensity, there are several other exercises you can incorporate in place of or in addition to Resistance Band One Arm Bicep Curls. Below are some variations, complementary exercises, and alternative exercises that can help you switch things up while still targeting your biceps.\nThe Concentration Curl is a great alternative or complementary exercise to Resistance Band One Arm Bicep Curls. It is an isolation exercise that specifically targets the bicep muscle, making it great for building strength and size. The exercise involves sitting on a bench with one arm resting on the inner thigh and the weight in the other hand. As you curl the weight up towards your shoulder, be sure to keep your upper arm still and keep your elbow tucked in. This will help you to really target the bicep muscle and ensure that you are getting the most out of this exercise.\nReverse Grip Cable Curls\nReverse grip cable curls are an excellent alternative or complementary exercise to Resistance Band One Arm Bicep Curls. This exercise involves holding a cable with an overhand grip and curling the weight up towards the shoulder. The use of the cable allows for continuous tension on the biceps, providing an effective way to build strength and muscle mass. The reverse grip increases the level of difficulty as it requires more control and stability of the muscles. This exercise can be adjusted to target different parts of the biceps, depending on the position of your hands. Reverse grip cable curls are a great way to add variety to your bicep workouts and help you reach your fitness goals faster.\nRope Cable Hammer Curl\nThe Rope Cable Hammer Curl is a great complementary or alternative exercise to the Resistance Band One Arm Bicep Curl. It is an isolation exercise that works the biceps and forearms, as well as the shoulder muscles. The exercise involves gripping a rope handle attached to a cable pulley and curling it up towards the shoulder. It requires more control than the Resistance Band One Arm Bicep Curl and targets the biceps from a different angle. It also allows for more resistance than the band exercise, which increases the intensity of the workout.\nCheck Out These Top Resistance Band Exercises\nCable Straight Bar Curls\nCable Straight Bar Curls are a great complementary or alternative exercise to Resistance Band One Arm Bicep Curls. This exercise is done with a cable machine, which provides a consistent amount of resistance throughout the full range of motion, making it ideal for targeting the biceps. Additionally, the cable bar allows you to use both arms at once, which increases the intensity of the exercise. Furthermore, this exercise encourages you to use proper form and can help strengthen your wrists and forearms in addition to your biceps.\nResistance Band Hammer Curls\nResistance Band Hammer Curls are an excellent alternative or complementary exercise to Resistance Band One Arm Bicep Curls. This exercise is performed by holding the band in both hands and curling your arms at the same time. This exercise targets the biceps, forearms, and shoulders, allowing you to get a full body workout. It is an effective exercise for building strength and size in the arms and upper body. The resistance bands also provide extra resistance to help you challenge your muscles even further. Resistance Band Hammer Curls are a great way to mix up your routine and add variety to your workouts.\nResistance Band Reverse Grip Curls\nResistance Band Reverse Grip Curls is a great complementary exercise to Resistance Band One Arm Bicep Curls. It works the same muscles but in a slightly different way. This exercise targets the biceps, the forearms, and the shoulder muscles. To perform this exercise, place the resistance band under both feet and hold onto the handles with an overhand grip. Keep your elbows close to your body as you curl up and down, making sure to keep your shoulders and arms relaxed. This exercise can be used as an alternative to Resistance Band One Arm Bicep Curls or as a way to increase intensity when combined with it.\nFind More Arms Exercises Here\nOpposing Complementary Exercises\nTo maximize the effectiveness of your workout, it is important to include exercises that work opposing muscle groups. Complementing the exercise Resistance Band One Arm Bicep Curls with exercises that target the opposite muscles can help create balance in your body and prevent injury. Below are a few exercises that are great for targeting the opposing muscle groups of the Resistance Band One Arm Bicep Curls:\nResistance Band Anchored Tricep Kickbacks\nResistance Band Anchored Tricep Kickbacks are a great exercise to complement Resistance Band One Arm Bicep Curls. This exercise works the opposing muscle group of the biceps, the triceps, by targeting the back of the upper arm. The tricep kickback is a great way to complete your upper arm workout since it works the opposing muscle group while helping to increase strength and definition in your arms. By completing both exercises, you will be able to get a full workout for your upper arms in a short amount of time.\nResistance Band Tricep Kickbacks\nResistance Band Tricep Kickbacks are a great way to work the opposing muscle group to Resistance Band One Arm Bicep Curls. This exercise specifically targets the triceps, working them to their full potential. To perform this exercise, stand with your feet hip-width apart and hold a resistance band in one hand. Bend forward slightly at the hips and engage your core. Keeping your elbow bent, extend your arm behind you until it is parallel with the ground. Then, slowly return your arm back to the starting position. This exercise is a great way to strengthen and tone your triceps, making it the perfect complement to Resistance Band One Arm Bicep Curls.\nResistance Band Push Downs\nResistance Band Push Downs are the perfect complement to Resistance Band One Arm Bicep Curls as they focus on the opposing muscle group. Resistance Band Push Downs work the triceps and help to create strength and definition in the back of the upper arm. To perform this exercise, stand with both feet on the resistance band and grasp it with both hands. Keeping your elbows close to your body, push down and slowly return to the starting position. This exercise will help to create balance between the biceps and triceps and give you a more toned upper arm.\nMaster Your Biceps With One Arm Resistance Band Curls!\nOne way to get the most out of your resistance band bicep curls is to focus on your form. Make sure to keep your elbow tucked in to your side and your wrist straight as you curl the band up towards your shoulder. Also, try doing one arm curls at a time to really isolate and work your biceps. Using a resistance band can provide a challenging workout and help you build stronger, toned arms. Don’t be afraid to switch up your routine and try different variations of the exercise to keep things interesting and avoid hitting a plateau.\nReferences: Wikipedia | ExRx.net | PubMed.gov | Comprehensive List of Arms Resistance Band Exercises', 'Most athletes that participate in collision and high impact sports, such as football, ice hockey, mogul skiing, and downhill mountain biking, commonly see acute and repetitive trauma injuries to the acromioclavicular joint. Other common shoulder injuries reported are, impingement syndrome, A/C problems, shoulder instability, and bicep anchor damage (SLAP lesions).\nIn these cases the sports medicine professional, Athletic Trainer, or Strength & Conditioning Specialist should be able to make correct decisions on the progression of a rehabilitation program, and the prevention of subsequent injuries from weight training.\nThe following are some modifications of the bench press, shoulder press, and lat pulldown. Many times these three movements along with other multi-joint exercises are main components in an athletes periodization based training program.\nBenching with a straight bar places a tremendous amount of stress on the humeral head stabilizers of the rotator cuff (infraspinatus, supraspinatus, subscapularis, and teres minor), and the long head of the bicep brachii muscles. One repetition, max lifts (1RM) can attribute significantly to shoulder lesions, and should be limited to 2-3 times a year or avoided all together. The risks on 1RM lifts typically outweigh the benefits.\nAnterior shoulder instability:\na) The athletes hand spacing should be no wider than 1.5 biacromial width. This will minimize peak shoulder torque in pressing motions. Biacromial with is measured from the superior portion of right acromioclavicular joint to the left acromioclavicular joint. The distance between the two acromioclavicular joints are considered 1x biacromial width.\nb) Having the trainer assist the athlete to lift the bar from bracket, is what is called a “Mandatory handoff”, and will prevent shoulder subluxion, and dislocation.\nc) Incline Bench pressing should be avoided\nd) Supine and decline bench press should be systematically alternated every other training session that involves chest work.\ne) Be certain of proper form. The barbell should be lowered slowly to sternum, 2-3 inches superior of xiphoid process. Press bar upwards, in line with shoulders, and maintain a “five point” contact throughout the entire lift.\nPosterior shoulder stability:\na) The athletes hand spacing should be increased to be 2x biacromial width. This angle will allow better structural approximation of the humeral head in the glenoid fossa, and decreases the strain on the posterior soft tissue.\nb) Perform a mandatory handoff.\nc) Perform this lying in the supine position only, no incline or decline pressing.\nSlap lesions (superior labrum antero-posterior lesions), glenoid labrum and bicep anchor damage:\na) Same guidelines as anterior stability.\nb) The athletes grip should be varied from underhand to an overhand grip on alternate days. This will decrease the micro-traumatic injuries and stress on the long head of the biceps tendon.\nc) The overhand grip removes the bicep tendon from beneath the acromion by internally rotating the humerus, but due to the full pronation of forearm, it stresses the attachment of the long head of the biceps.\nd) The underhand grip places the long head of the biceps tendon (supination) in a preferred position to decrease the stress on the tendon, but places the long head of biceps tendon beneath the acromion, allowing it to impinge.\ne) Overhand and underhand grip should not be used in same day and should follow a periodization protocol.\nThe barbell shoulder press starts with hand spacing slightly wider than shoulder width. Starting position is when bar is at the anterior portion of the deltoids, and sternoclavicular joints, and it is pressed overhead. Behind the neck shoulder press should be avoided all together due to the physiological effects of this movement, until full function is achieved. It should be replaced with posterior deltoid specific exercises, such as rows, rear deltoid raises, and dumbbell rows.\nThe barbell shoulder press starts with shoulder abduction, in the “high 5 position” and external rotation. This position places significant strain on anteroinferior glenohumeral ligaments. It also stresses the cervical region, by placing the head into excessive flexion, and stressing the rotator cuff through the impingement mechanism. The high 5 position is when the athlete has their palms pronated outward, facing away from them, at about chin level, with the arms in the saggital plane.\nModification of movement\na) A functional isometric program is implemented within the confines of a power rack, and performed by athletes that are required to perform overhead movements.\nb) Two sets of pins are set up in rack. One to support the weight, and one to limit range of motion starting at 60 degrees of shoulder flexion and one at 120 degrees. This provides minimal torque in external rotation and provides significant glonohumeral joint approximation.\nc) An isometric hold of 5-10 seconds is held per repetition at each angle, and 6-10 repetitions performed at each of the three 60, 90, 120 degrees.\nd) Be sure to watch for resistance training machines with excessive high five positions, and internal rotation. Machines that typically have safe positions are Nautilus, Hammer Strength, and Hoist Roc-it strength machines.\nLAT PULL DOWN:\nThis exercise involves a combination of movements in the saggital plane (flexion/extension), and the coronal plane (adduction/abduction). The bar is pulled from overhead to the base of the neck to the first thoracic vertebrae or just inferior of the clavicle .The behind the neck pulldown should be avoided for similar reasons as the behind the neck shoulder press. Pulling the bar behind your head and forward tilting chin to far anterior, can create excessive compression in the shoulder complex and cause transient brachial plexus nerve impingement (TBPNI). Transient upper extremity paralysis after completion of behind the neck pull downs is typically noticed in the hands first.\nModification of movement\na) Start with the upper body in a 30 degree reclined position, with bar inline with xiphoid process.\nb) Handgrip should be 1.25-1.5 biacromial width.\nc) This position prevents athlete from being in high five position, and increases the activation of the shoulder adductor and scapular retractor muscles which assist in stabilizing the humeral head throughout the movement.\nd) Raise bar slowly upward, and increase elbow flexion as much as possible, just before scapular elevation occurs. Otherwise this can cause instability in the glenohumeral joint.\ne) For the reverse grip pulldown, the hands are placed at 1x biacromial width, and the start and end positions are performed in the same manner.\nRemember these are modifications for competitive athletes for these three particular movements, and after the completion of physical therapy. They must also be done under the supervision of a sports medicine professional. If the athlete experiences acute pain while performing these exercises, then they should stop, and consult their physical therapist, due to the fact that they may not yet, be ready to perform these particular motions.\nJon Torerk, CSCS']	['<urn:uuid:bbef9ee4-433a-4194-95af-2af0489b1322>', '<urn:uuid:e1284b42-75d0-44e2-8a2e-cc517b3f480b>']	factoid	direct	short-search-query	similar-to-document	multi-aspect	novice	2025-05-12T21:31:42.699391	7	51	2997
49	How does the Family Check-up program help prevent behavioral problems in children from high-risk families during their early years?	The Family Check-up increases parents' positive behavior support in early childhood, which helps prevent problem behavior. In a randomized trial examining 2-year effects, this family-centered approach demonstrated effectiveness in preventing early conduct problems.	['Principles of Substance Abuse Prevention for Early Childhood A Research-Based Guide (In Brief)\n- Kellam SG, Brown CH, Poduska JM, et al. Effects of a universal classroom behavior management program in first and second grades on young adult behavioral, psychiatric, and social outcomes. Drug Alcohol Depend. 2008;95 Suppl 1:S5-S28. doi:10.1016/j.drugalcdep.2008.01.004.\n- Kitzman HJ, Olds DL, Cole RE, et al. Enduring effects of prenatal and infancy home visiting by nurses on children: follow-up of a randomized trial among children at age 12 years. Arch Pediatr Adolesc Med. 2010;164(5):412-418. doi:10.1001/archpediatrics.2010.76.\n- August GJ, Lee SS, Bloomquist ML, Realmuto GM, Hektner JM. Dissemination of an evidence-based prevention innovation for aggressive children living in culturally diverse, urban neighborhoods: the Early Risers effectiveness study. Prev Sci Off J Soc Prev Res. 2003;4(4):271-286.\n- Catalano RF, Mazza JJ, Harachi TW, Abbott RD, Haggerty KP, Fleming CB. Raising healthy children through enhancing social development in elementary school: Results after 1.5 years. J Sch Psychol. 2003;41(2):143-164. doi:10.1016/S0022-4405(03)00031-1.\n- DeGarmo DS, Eddy JM, Reid JB, Fetrow RA. Evaluating mediators of the impact of the Linking the Interests of Families and Teachers (LIFT) multimodal preventive intervention on substance use initiation and growth across adolescence. Prev Sci Off J Soc Prev Res. 2009;10(3):208-220. doi:10.1007/s11121-009-0126-0.\n- Shaw DS, Dishion TJ, Supplee L, Gardner F, Arnds K. Randomized trial of a family-centered approach to the prevention of early conduct problems: 2-year effects of the family check-up in early childhood. J Consult Clin Psychol. 2006;74(1):1-9. doi:10.1037/0022-006X.74.1.1.\n- Beets MW, Flay BR, Vuchinich S, et al. Use of a social and character development program to prevent substance use, violent behaviors, and sexual activity among elementary-school students in Hawaii. Am J Public Health. 2009;99(8):1438-1445. doi:10.2105/AJPH.2008.142919.\n- Hawkins JD, Kosterman R, Catalano RF, Hill KG, Abbott RD. Effects of social development intervention in childhood 15 years later. Arch Pediatr Adolesc Med. 2008;162(12):1133-1141. doi:10.1001/archpedi.162.12.1133.\n- Snyder F, Vuchinich S, Acock A, Washburn I, Beets M, Li K-K. Impact of the Positive Action program on school-level indicators of academic achievement, absenteeism, and disciplinary outcomes: A matched-pair, cluster randomized, controlled trial. J Res Educ Eff. 2010;3(1): 26-55. doi:10.1080/19345740903353436.\n- Hawkins JD, Catalano RF, Kosterman R, Abbott R, Hill KG. Preventing adolescent health-risk behaviors by strengthening protection during childhood. Arch Pediatr Adolesc Med. 1999;153(3):226-234.\n- Kellam SG, Wang W, Mackenzie ACL, et al. The impact of the Good Behavior Game, a universal classroom-based preventive intervention in first and second grades, on high-risk sexual behaviors and drug abuse and dependence disorders into young adulthood. Prev Sci Off J Soc Prev Res. 2014;15 Suppl 1:S6-S18. doi:10.1007/s11121-012-0296-z.\n- Lonczak HS, Abbott RD, Hawkins JD, Kosterman R, Catalano RF. Effects of the Seattle social development project on sexual behavior, pregnancy, birth, and sexually transmitted disease outcomes by age 21 years. Arch Pediatr Adolesc Med. 2002;156(5):438-447.\n- Bruce J, McDermott JM, Fisher PA, Fox NA. Using behavioral and electrophysiological measures to assess the effects of a preventive intervention: a preliminary study with preschool-aged foster children. Prev Sci Off J Soc Prev Res. 2009;10(2):129-140. doi:10.1007/s11121-008-0115-8.\n- Fisher PA, Stoolmiller M, Gunnar MR, Burraston BO. Effects of a therapeutic intervention for foster preschoolers on diurnal cortisol activity. Psychoneuroendocrinology. 2007;32(8-10):892-905. doi:10.1016/j.psyneuen.2007.06.008.\n- Tolan P, Gorman-Smith D, Henry D. Supporting families in a high-risk setting: proximal effects of the SAFEChildren preventive intervention. J Consult Clin Psychol. 2004;72(5):855-869. doi:10.1037/0022-006X.72.5.855.\n- Webster-Stratton C, Jamila Reid M, Stoolmiller M. Preventing conduct problems and improving school readiness: evaluation of the Incredible Years Teacher and Child Training Programs in high-risk schools. J Child Psychol Psychiatry. 2008;49(5):471-488. doi:10.1111/j.1469-7610.2007.01861.x.\n- Dishion TJ, Shaw D, Connell A, Gardner F, Weaver C, Wilson M. The family check-up with high-risk indigent families: preventing problem behavior by increasing parents’ positive behavior support in early childhood. Child Dev. 2008;79(5):1395-1414. doi:10.1111/j.1467-8624.2008.01195.x.\n- Fisher PA, Stoolmiller M, Mannering AM, Takahashi A, Chamberlain P. Foster placement disruptions associated with problem behavior: mitigating a threshold effect. J Consult Clin Psychol. 2011;79(4):481-487. doi:10.1037/a0024313.\n- Conduct Problems Prevention Research Group. Initial impact of the Fast Track prevention trial for conduct problems: II. Classroom effects. J Consult Clin Psychol. 1999;67(5):648-657.\n- Ialongo NS, Werthamer L, Kellam SG, Brown CH, Wang S, Lin Y. Proximal impact of two first-grade preventive interventions on the early risk behaviors for later substance abuse, depression, and antisocial behavior. Am J Community Psychol. 1999;27(5):599-641.\n- Fisher PA, Stoolmiller M. Intervention effects on foster parent stress: associations with child cortisol levels. Dev Psychopathol. 2008;20(3):1003-1021. doi:10.1017/S0954579408000473.\n- Shaw DS, Connell A, Dishion TJ, Wilson MN, Gardner F. Improvements in maternal depression as a mediator of intervention effects on early childhood problem behavior. Dev Psychopathol. 2009;21(2):417-439. doi:10.1017/S0954579409000236.\n- Dodge KA, Goodman WB, Murphy R, O’Donnell K, Sato J. Toward Population Impact from Home Visiting. Zero Three. 2013;33(3):17-23.\n- Mullany B, Barlow A, Neault N, et al. The Family Spirit trial for American Indian teen mothers and their children: CBPR rationale, design, methods and baseline characteristics. Prev Sci Off J Soc Prev Res. 2012;13(5):504-518. doi:10.1007/s11121-012-0277-2.\n- Olds DL. Prenatal and infancy home visiting by nurses: from randomized trials to community replication. Prev Sci Off J Soc Prev Res. 2002;3(3):153-172.\n- Fisher, PA, Chamberlain, P. Multidimensional treatment foster care: a program for intensive parent training, family support, and skill building. J Emot Behav Disord. 2000;8:155-164.\n- Battistich V, Solomon D, Watson M, Schaps E. Caring school communities. Educ Psychol. 1997;32(3):137-151. doi:10.1207/s15326985ep3203_1.\n- Kellam S, Rebok G. Building developmental and etiological theory through epidemiologically based preventive intervention trials. In: McCord J, Tremblay R, eds. Preventing Antisocial Behavior: Interventions from Birth through Adolescence. New York, NY: Guilford Press; 1992:162-195.\n- Reid JB, Eddy JM, Fetrow RA, Stoolmiller M. Description and immediate impacts of a preventive intervention for conduct problems. Am J Community Psychol. 1999;27(4):483-517.\n- August GJ, Realmuto GM, Hektner JM, Bloomquist ML. An integrated components preventive intervention for aggressive elementary school children: the early risers program. J Consult Clin Psychol. 2001;69(4):614-626.\n- Pears K, Fisher P, Heywood C, Bronz K. Promoting school readiness in foster children. In: Saracho O, Spodek B, eds. Contemporary Perspectives on Social Learning in Early Childhood Education. Charlotte, NC: Information Age Publishing; 2007.\n- Conduct Problems Prevention Research Group. Merging universal and indicated prevention programs: the Fast Track model. Conduct Problems Prevention Research Group. Addict Behav. 2000;25(6):913-927.\n- Flay BR, Allred CG, Ordway N. Effects of the Positive Action program on achievement and discipline: two matched-control comparisons. Prev Sci Off J Soc Prev Res. 2001;2(2):71-89.\n- Barrera M, Biglan A, Taylor TK, et al. Early elementary school intervention to reduce conduct problems: a randomized trial with Hispanic and non-Hispanic children. Prev Sci Off J Soc Prev Res. 2002;3(2):83-94.\nCite this article\nNIDA. (2016, September 1). Principles of Substance Abuse Prevention for Early Childhood A Research-Based Guide (In Brief). Retrieved from https://www.drugabuse.gov/principles-substance-abuse-prevention-early-childhood-research-based-guide-in-brief']	['<urn:uuid:c94c388a-de74-4f25-9bf4-bccdc57b9f85>']	open-ended	direct	verbose-and-natural	distant-from-document	single-doc	expert	2025-05-12T21:31:42.699391	19	33	1114
50	What happens if the Pulse's screen breaks?	If the screen breaks, the Pulse can be operated in a special screenless mode by holding both buttons pressed while switching from Off to Search. This puts the device in analog mode without distance or direction information, with volume adjustable using the buttons.	"['Summary: The Mammut Pulse Barryvox is a small,\nthree-antenna, avalanche transceiver\nwith a long range. It offers both analog and digital modes,\nprovides feature-rich handling of\nhandles spikes well, has a floating\nthat indicates minor changes in direction as well as points\nbehind you, and has excellent\nIf the victim is wearing a Pulse transceiver (which has\nbeen configured for use in the same region),\nthe searching Pulse can tell you if the victim is alive\nby sensing tiny movement—hence the name Pulse. With the\n3.0 firmware update,\nthe Pulse offers both a basic and advanced user interface.\nThe Pulse is the big brother of the\nThe Pulse is similar to the\nARVA Pro W (read a\nof the two transceivers).\nSearching: The Pulse has a long digital\nreception range and as\nwith the other top avalanche transceivers, it has a recommended\nsignal strip search\nwidth of 50 meters.\nThe Pulse can be toggled into analog mode after changing\na custom setting (which should be\ndone in the comfort of your home). You can then press both\nbuttons simultaneously to toggle the between analog and\ndigital. This is a nifty feature for power-users.\nA cool feature of the Pulse is that if you are headed\nin the wrong direction (which can easily happen, since transceivers\nsimply align you with the transmitting beacon\'s\nflux lines) the\nwill reverse direction and point behind you. If you\'re\nheading in the wrong direction, only the Pulse and Ortovox\nS1 will point to the victim behind you (the\nPro W will display\na U-Turn icon). On all other transceivers, you\nwill need to see that the distance indicator is increasing\nand turn 180-degrees.\nThe Pulse (and the Ortovox\nS1 and ARVA Pro W) uses an internal compass to\nhelp you locate the victim. You need to hold the Pulse level\nwhile searching or a message may appear telling you to ""Hold\nDevice Horizontally"" (the occurrence of this message\nwas significantly reduced in the\n3.0 firmware update).\nBeginning with version 3.0, the Pulse can display either\na ""landing strip"" or ""cross fire"" image\nduring the fine search.\n(You can read my initial thoughts about the landing strip\nBeginning with version 4.0, the Pulse can display a ""Guided\nFine Search"" during the fine search.\nWhen using the advanced profile,\nthe audio signal is analog during the\nThe analog tone lets you hear faint signals (as well as\nnon-beacon-generated background noise) and can be helpful\nburials. During the\nfine search, the\ndigital audio tone increases the pitch, cadence, and volume.\nLearn more about\nAs with the Element, the Pulse can emit ""directional\ntones"" which vary the audio tone based on the direction\nyou are headed.\n|Please help improve this website\nreporting typos, broken links, and spelling\nWhen in ""analog"" mode, you can turn off the\nPulse\'s display which increases the reception range dramatically.\nThis is a valuable feature for power-users. Unfortunately,\nthis technique was not used when measuring the Pulse\'s analog\nreception range as shown on the\nIt\'s a shame, because in my informal testing the Pulse received\na signal at 80 meters when in this mode.\nTo enter this long-range analog mode you\'ll first need\nto access the device\'s Settings\nand ensure that the ""advanced"" profile is selected.\nYou\'ll also want to make sure the ""Analog""\nSetting is set to ""Manual.""\nTo enter the long-range analog mode:\n- Slide the Pulse\'s switch to Search.\n- Toggle analog mode by pressing both buttons. The\nword ""Analog"" will appear on the display.\n- Press the right button to increase the volume to ""A8""\nand then increase the volume with one additional button\npress. The display will go blank.\nMammut states that if there isn\'t other\ninterference (e.g., buildings,\npower lines, chairlifts, cell phones, etc), the reception\nrange should be 90 or more meters. That\'s a huge range and\nsimilar to what I\'ve seen during informal tests.\nSpikes: Spike handling\nMultiple Burials: The handling of multiple burials\nis similar to most digital transceivers (you press a button\nto ignore the current beacon and advance to the next). However\nas with the ARVA Pro W and S1, the Pulse also allows\nyou to select the transceiver you want to locate and to\nunmark a previously marked beacon. Read the details\nwith the version 3.0 firmware,\nthe Pulse offers two user ""profiles."" The basic\nprofile simplifies the user interface by removing a few\nof the more advanced features. When using the basic profile:\n- Toggling to analog mode is not available\n- The left and right buttons perform the same task\n(except when you are modifying the internal settings)\n- During searches for multiple victims:\n- The searcher is always sent to the closest victim\n(rather than being able to select a victim by scrolling\nthrough a list)\n- The screen display is simplified (rather than\nshowing the distance and direction to each of the\n- A ""landing\nstrip"" is displayed during the\n(this is an option on the Pulse).\ntones are available as a custom setting.\nThe first time you turn on your\nPulse, you are prompted to choose a profile. After that,\nyou can change the profile via the custom settings:\n- Turn on the unit\n- Immediately press either button (""activated""\nwill appear at the bottom of the screen for several\n- Use the left button to scroll down through the list\n- Choose OK with the right button to select a setting\n- Select the desired value using the left button and\nconfirm the selection with the right button\nThe basic profile simplifies the interface by displaying\nonly one command at a time and allowing you to select commands\nby pressing either button. The basic profile still includes\nall the core features, including the ability to search for\nsingle or multiple victims, mark victims, etc, but it limits\nthe number of menus options. This is significant and goes\nstraight at the ease of use of the Ortovox 3+, the Pieps\nDSP, and the Trackers, while still offering advanced capabilities\nnot found on those beacons, such as an analog mode, an arrow\nthat points behind you, and selective marking and unmarking.\nThe user interface still has a few oddities that can\nconfuse the unwary. For example after turning on the unit\nand immediately pressing either button, ""activated""\nis displayed on the screen to let you know that you\'ve activated\nthe custom settings (and Group Check) menu. The ""activated""\nmessage appears for several seconds, then ""self test\nokay,"" and then ""Group Check."" You can use\nthe left button to move through the custom settings or press\nthe right button to enter the\nGroup Check mode. I have\nworn the Pulse on more than 100 occasions and I\'m still\noccasionally confused by the startup and Group Check messages.\nSwitch: The Pulse has a single switch that\nmoves between Off, Send, and Search. Two buttons lock the\nswitch into position. One of the buttons must be pressed\nbefore you can slide the switch from Off to Send or from\nSend to Search, and both buttons must be pressed before\nsliding the switch to the Off position.\nHarness: The Pulse\'s\nharness positions the transceiver vertically on your side.\nThe transceiver fits into a molded-plastic case that is\nsecured with a small Fastex buckle. The case is designed\nso the transceiver\'s display must be facing toward your\nbody and so the beacon cannot be switched to search mode\nwhile in the harness. Good stuff. I find the harness very\ncomfortable, but it\'s a little difficult to insert the transceiver\ninto the harness while wearing it.\nThe somewhat unusual double-threading of the straps through\nthe buckles makes the harness difficult to adjust.\nSize: The Pulse is\n25% smaller than the Tracker2, 22% smaller than the Ortovox\nS1, 4% smaller than the ARVA Pro W, and 3% smaller\nthan the Ortovox 3+.\nGroup Check: When\nthe Mammut Pulse is turned on, the message ""Group Check""\nis displayed for five seconds. Pressing either button (which\ndisplays the less-than-intuitive ""Activated"" message],\nand then the right button (to select ""OK"") enables\nthe group check mode. In this mode the Pulse will only receive\na signal when held one meter from a properly transmitting\nbeacon (if you get too close to the transmitting beacon,\nthe message ""Too close!"" is displayed). The Pulse\nautomatically exits the group check mode after a few minutes.\nThis is a quick and easy way to confirm that your partner\'s\navalanche beacon is transmitting and it\'s also a good reminder\nto perform a test each time you turn on your beacon. (The\ndistance that the group check will receive a signal is normally\none meter, but it can be changed to five meters for snowmobilers\nwho do group checks while on their sleds.)\nThere are a couple of oddities when checking your partners\'\ntransceivers with the Pulse. One, which was mentioned above,\nis that it isn\'t obvious that you need to press a button\nduring startup to activate the group check mode. The words ""Group\nCheck"" might be interpreted as the mode is automatically\nactivated on startup. Second, a countdown timer after pressing\na button when Group Check is displayed adds additional confusion\n(is the timer displaying the time until the group check\nbegins or ends?). And if you get too close to the transmitter\nduring the Group Check mode, the Pulse will display ""Too\nclose!"" It really wants to be in the one meter range.\nThese oddities are not very significant, but they can create\nYou can set owner information, such as your name and email\naddress, which will be displayed every time the Pulse is\nturned on. This is certainly handy, although the data-entry\nfelt extremely time consuming even after discovering (by\nthat holding the left button pressed moves backwards\nthrough the alphabet.\nVitals Data: If both\nthe victim and searcher are using Pulse transceivers (which\nhave been configured for use in the same region, see below),\nthe searching transceiver will report if the victim is alive\nby sensing tiny movement (hence the name Pulse). With multiple\nburials (and while using the Advanced profile), a small\nheart is displayed next to victims who have any movement\n(e.g., breathing or arterial pulses) which can presumably\nbe used to triage which victim to locate first.\nThe ""vitals data"" requires that the Pulse is\ncarried close to your body so it can sense motion. There\nis a ""Vital Sensor Test"" where you lie as still\nas possible on the ground to see if the transceiver will\nsense the motion of your body.\nAfter unburying a victim who is wearing a Pulse, the\nrescuer can press either of the buttons to display the length\nof time the beacon did not experience significant motion\n(i.e., the ""burial"" time) and the amount of time\nthe victim was still moving slightly (i.e., the ""still\nalive"" time). If the rescuer presses either button\nagain, the display will rotate through six previous burial\nperiods. The six burial periods are undeniably confusing\n(I had to contact Barryvox for an explanation), but they\nmay be necessary if the beacon isn\'t checked when the\nvictim is first unburied (so you can access information\nthat would otherwise be overwritten by movement while transporting\nThe vitals information is transmitted\nto other Pulse transceivers using a separate radio signal\n(referred to as W-Link). Unfortunately, the allowable radio\nfrequency varies between regions (e.g., Region A in Europe\nand Region B in North America). And the FCC does not allow\nbeacons that are sold in the USA to support Region A. If\nyou purchase a beacon in the USA and you will be traveling\nto Region A, you can send your transceiver to a service\ncenter to have Region A enabled. That is impractical for\nmost of us.\nThe W-Link information is also used by the Pulse and\nthe ARVA Pro W to help identify transmitters when searching\nfor multiple victims (i.e., in addition to receiving the\nstandard beep-beep from the transmitters, they transmit\nand receive serial number information on the W-Link which\nhelps distinguish the transmitters).\nCertainly Mammut is proud of the uniqueness of the vitals\ndata. I think the feature is valuable for a fleet (e.g.,\na backcountry ski service, snowmobile tours, etc) where\nall members of a party wear Pulse transceivers and everybody\nis in the same Region. And there are no downsides to this\nfeature. However, I personally see only a small benefit\nin the vitals data—the Pulse would easily get five stars\non BeaconReviews.com without this feature.\nRevert to Transmit: By default, the Pulse will\nautomatically switch from\nreceive mode to transmit mode after eight minutes to\nensure that you will be transmitting if you get buried by\na second avalanche. The Pulse also requires that there isn\'t\nany user interaction (e.g., button presses) or major\nmotion during this period. The advantage of sensing\nmotion is that it is less likely that a searcher will unknowingly\nreturn to transmit mode during a search (which can seriously\nconfuse the other searchers). The Pulse\'s auto-revert mode\ncan be changed to four minutes or be disabled. The Pulse\ngives a loud audible warning before reverting. You can prevent\nthe unit from reverting by pressing either button during\nthe warning. The Pulse also has a ""rescue send""\nmode (read about it\nCompass Calibration: The Pulse has an internal\ncompass that helps the direction arrow update. As with the\nARVA Pro W and\nOrtovox S1, the internal\ncompass should be calibrated before the start of each season\nand after replacing the batteries. This is done by selecting\na menu option and then slowly rotating the beacon 360°.\nThe Pulse automatically prompts you to calibrate the compass\nwhen you replace the batteries. The\nmanual implies that you should also calibrate the compass\nif you travel a long distance, but of course the beacon\nwill not prompt you in this case. The calibration affects\nthe smooth flow of the search arrow—without it the arrow\nwill be jerkier and less intuitive. The internal compass\ncan\'t be used for traditional navigation.\nScreenless Mode: If the screen on your Pulse is\nbroken, you can enable a special mode by holding both buttons\npressed and then switching from Off to Search. You will\nthen be in analog mode (without any distance or direction\ninformation) and can change the volume using the buttons.\nThis is explained on a sticker that you are supposed to\nadhere to the back of the transceiver.\nUpdates: The Mammut Pulse supports software updates.\nYou can read about updating the Pulse\nView the comparison table for\nmore information regarding the Mammut Pulse.']"	['<urn:uuid:cc122f38-3397-4a52-8cfb-44a5474940bd>']	factoid	direct	concise-and-natural	distant-from-document	single-doc	expert	2025-05-12T21:31:42.699391	7	43	2428
51	Where can patients receive virtual reality treatment?	The Virtual Reality Medical Center operates multiple facilities, including a 10,000 square foot facility in San Diego, California, a 1,000 square foot clinic in La Jolla, California, and a satellite clinic in Coronado, California. They also have clinics at Naval Medical Center San Diego and Camp Pendleton, California. Additionally, they maintain research facilities and clinical trials capabilities in Brussels, Belgium and Qingdao, China.	['The Virtual Reality Medical Center Facilities\nThe Virtual Reality Medical Center (VRMC) maintains a 10,000 square foot facility in San Diego, California, a 1,000 square foot clinic in La Jolla, California, and a satellite clinic in Coronado, California, with additional research facilities and clinical trials capabilities in Brussels, Belgium and Qingdao, China. The VRMC Virtual Reality Engineering Lab is located in a 1,700 square foot space in the same office complex as the San Diego headquarters. Two additional clinics are located at Naval Medical Center San Diego and Camp Pendleton, California, and future clinics are in development. The Virtual Reality Medical Institute is an affiliated European corporation located in Brussels, Belgium.\nParticipant wearing HMD and physiological sensors.\nCurrent clinical space at VRMC San Diego includes 12 VR treatment rooms with virtual reality software for the treatment of fear of flying, driving, public speaking, heights, claustrophobia, agoraphobia, panic disorder, social phobia, posttraumatic stress disorder due to motor vehicle accidents, autism, anger management, and generalized social phobia. It also includes distraction software geared toward painful medical procedures and chronic pain; a clinical trials center for conducting drug and medical device studies; 6 noninvasive physiological monitoring labs (2 rapid prototyping labs for human interface sensors and 2 brain facilities), 12 noninvasive labs that measure EEG, ECG, EMG, EOG, skin resistance, temperature, respiration, heart rate variability, blood pressure, and pulse oximetry; a human-robotics interaction lab; and a multi-user VR facility. It has been approved and certified by the State of California as a patient care facility. It has also been approved by the Food and Drug Administration (FDA) and the Centers for Medicare and Medicaid Services (CMS) for government patient-care reimbursement.\nLocated near the University of California, San Diego (UCSD), Camp Pendleton, Miramar Marine Corps Air Station, Naval Health Research Center, Naval Medical Center San Diego (Balboa Naval Hospital), and Coronado Naval Air Station, the San Diego clinic is located within a 40,000 square foot office building.\nA typical therapy room consists of a VR computer and head-mounted display (HMD), a physiological monitoring computer with noninvasive sensors, and some type of augmented reality device. In the case of driving, the patient sits in an actual automobile seat and uses a steering wheel, brake, and gas pedal. VRMC specializes in the creation of augmented reality environments that are used to increase levels of immersion and presence for patients undergoing therapy. VRMC also has significant experience in the development of noninvasive medical sensors, and has produced a number of PC-based portable medical devices that have been approved by the FDA. In particular, we have developed a variety of contact sensors based on piezoelectric formats that simultaneously record acoustic, displacement, and electrical activity.\nAll key personnel in this project have their own offices and adequate computer equipment/software/ printers/faxes to support this project. See, under Other Resources below, additional computer capabilities to support virtual environment design.\nVRMC has about 300 sq. ft. of office space, composed of a large office for the Principal Investigator and several offices for support personnel. It is located in the s\nBrenda Wiederhold, Ph.D., MBA, BCB, BCN, with participant wearing HMD (a HMD from the 1990s on permanent exhibit at the National Library of Medicine).\name building as the VRMC clinic, so the PI will be easily available to train personnel in treatment methods and oversee the clinical portion of the project. The VRMC Virtual Reality Engineering Lab is also located in the same complex as the offices for easy access and management.\nVRMC currently is involved in research, clinical services, and training. VRMC trains 20–30 graduate students each year who are working towards doctorates in clinical psychology, health psychology, industrial and organizational psychology, applied psychophysiology, biomedical engineering, bioengineering, computer science, and digital arts. Student exchange programs are in place with Hanyang University, Seoul, Korea; the Catholic University, Milan, Italy; Jaume I University, Valencia, Spain; the University of California, Los Angeles (UCLA), Stanford University, and the University of Ottawa. VRMC also trains students from UCSD, San Diego State University, and the California School of Professional Psychology at Alliant International University, San Diego.\nVRMC serves as a beta test site for virtual reality software and physiological monitoring equipment jointly developed in Canada, Italy, Korea, Spain, and the United States. We have applied standardization to our products by obtaining ISO 9001:2000 and ISO 13485:2003 certification, with CMMI (Capability Maturity Model Integration) Level 3 certification from Carnegie Mellon University’s Software Engineering Institute (SEI) under way. ISO 13485:2003 is a quality management standard for medical devices, while CMMI is focused on the quality of the software development process. In addition, VRMC is in the process of preparing for inspection as it comes into compliance with the Food and Drug Administration (FDA) Quality System Regulation (QSR) for medical devices.\nCurrent research under way at the clinic includes understanding and quantifying the physiological correlates of stress, anxiety disorders research, eating disorders (anorexia, bulimia, binge-eating, and obesity), peak performance training, executive coaching, student state learning, distraction during painful procedures, ADHD, autism, and health promotion/disease prevention of diabetes, asthma, and teen smoking. In addition, telehealth projects are being conducted using avatars, Internet-based virtual worlds, video cameras, and microphones to treat social phobia and agoraphobia.\nA low-cost HMD\nDrs. Brenda and Mark Wiederhold are on faculty at UCSD and have access to the San Diego Supercomputer Center and Brain Imaging Facilities (PET, MRI, fMRI, and CT) located on the campus. Dr. Mark Wiederhold is a member of the Scripps Clinic Medical Group and has access to the General Clinical Research Center (GCRC) at Scripps Clinic Torrey Pines. The GCRC is a 10-bed NIH-funded clinical research facility with a 1,000 square foot clinical laboratory that is capable of conducting any clinical or research protocol. In addition, the facility is staffed by registered nurses and other healthcare professionals who are specifically trained in the conduct of clinical trials. Dr. Brenda Wiederhold is on the affiliated medical staff at Scripps Memorial Hospital – La Jolla and serves on the Pain and Palliative Care Committee.\nOur current active research programs continue to attract local, state, and national funding. The facilities at VRMC meet the environmental laws and regulations of federal, State of California, and local governments for airborne emissions, waterborne effluents, external radiation levels, outdoor noise, solid and bulk waste disposal practices, and handling and storage of toxic and hazardous materials.']	['<urn:uuid:4db38440-20c4-4808-b352-3d05c1b39509>']	open-ended	direct	concise-and-natural	distant-from-document	single-doc	novice	2025-05-12T21:31:42.699391	7	63	1055
52	difference between dye and pigment ink	In inkjet printing, a dye is a colorant that is soluble in the carrier liquid, while a pigment is insoluble and is supplied as very small particles. Both types of colorants are used in water-based carrier liquid to form inkjet ink, along with other components.	"[""The name of HARMAN technology Ltd may be new, but we have a long history in the field of photography. We have a range of HARMAN PHOTO inkjet printing media.\n- Receiving Layers\n- Swelling Polymer\nDuring recent years there has been considerable growth in the use of inkjet printers for producing photographs, either as output from digital cameras or from scans of conventional photographic materials. Inkjet printers work by jetting very small droplets of ink on to a suitable receiving material to form the image. Such images are normally disappointing when printed on plain paper because they lack density, vibrancy, and sharpness, and therefore special coated papers are recommended for the purpose. Such materials comprise one or more ink-receptive recording layers coated on a supporting substrate such as resin-coated (RC) paper or baryta-coated paper that give the look, weight, and feel of a traditional photograph.\nThe electron microscope image above shows a view of a typical system: the thin ink-receiving coating and thicker substrate are visible. In general, two types of coating are in use in such image receiving systems: so-called swelling polymer types and so-called porous types.\nHARMAN's coating technology allows us to coat two or more layers simultaneously, and we can also control the layer formulations for optimum overall properties. The microscopic cross-section on the left shows a two-layer swelling-polymer type inkjet assembly coated on resin-coated (RC) paper base: the fibrous paper substrate is at the bottom, and the thick dark layer is a polyethylene resin coating that is filled with titanium dioxide white pigment. There are two ink receiving layers: to maximise the brightness and detail of the image it is important that the colorant from the ink be retained at or near the surface after printing, and therefore there is a thin upper layer to trap the dyes and a thicker lower layer to absorb the water-based fluid components of the ink.\nThe receiving layer of a swelling-polymer system comprises a continuous coating of one or more water-accepting hydrophilic polymers, together with other addenda. It is easy to prepare highly glossy ink jet coatings of this type, but they are rather slow to absorb the ink because absorption depends on the hydrophilic polymer swelling when it is printed.\nTypically, an inkjet ink comprises a water-based carrier liquid and at least one colorant (a dye or a pigment), together with other components. A dye is a colorant that is soluble in the carrier liquid whereas a pigment is insoluble and supplied as very small particles.\nThe microscopic views here show cross-sections through a two-layer swelling-polymer type ink receiving assembly. The individual layers are not very clear on the view of the unprinted material: the thicker lower layer absorbs the ink solvent and the thin surface layer traps the dyes. The second view shows a cross-section after printing with a pigmented ink: the very fine particles of pigment are clearly held on the surface of the material. The final view shows the same material after being printed with dyed inks. In this case the colorant penetrates into the assembly and is retained near the surface by the dye trapping layer for maximum colour and image brightness.\nThe ink receiving layer of a porous type recording system comprises very fine particles of one or more colourless fillers or pigments such as silica or alumina, a polymeric binder to hold the particles together, and other addenda. The ink is taken into the interstitial volume between the particles. For many applications this type is preferred because prints on such materials are quickly dry to the touch after printing as the liquid phase of the ink absorbs very rapidly into the open pores on the surface of the material.\nThe Scanning Electron Microscope view of the surface of a porous inkjet layer clearly shows the pigment particles (in this case silica) and the open pores between them. The particles are approximately 1 - 10 microns (µm, micrometres, or millionths of a metre) in size. Particles of this size scatter light and therefore the material has a bright white matt surface. HARMAN PHOTO matt inkjet materials have an attractive smooth surface finish.\nThe second SEM image shows the surface of a porous inkjet material which has very much smaller particles (alumina in this case) in the size range of approximately 10 - 20 nm (nanometres). These particles are smaller than the wavelength of light and thus do not scatter light like the larger particles do; the coating offers a smooth, sharp, and robust glossy finish.\nFor full details of the HARMAN PHOTO range of inkjet media, and availability, visit our dedicated website www.harman-inkjet.com .\nPlease do not hesitate to Contact Us for more information about any of our products and services.\nClick here to email your enquiry.\nBack to Top""]"	['<urn:uuid:bfcf4cf3-9e65-45fa-8186-a6d3e15a37d2>']	open-ended	direct	short-search-query	similar-to-document	single-doc	novice	2025-05-12T21:31:42.699391	6	45	798
53	What are scientists doing to help sea creatures survive in more acidic waters in the future?	Scientists are exploring several solutions to help marine life cope with ocean acidification. In the short term, hatcheries are testing the injection of carbonates into acidic ocean water to help organisms like clams and mussels develop hard shells. For long-term solutions, researchers are studying genetic adaptability, looking for species that are naturally more resilient to acidic conditions. For example, research in New Zealand involves cross-breeding different types of green-shelled mussels to develop new resistant strains. The goal is to identify and potentially shift to raising species that are genetically better equipped to handle more acidic waters.	['A recent NOAA study found that by 2040, Alaskan shellfish hatcheries may no longer be sustainable because of ocean acidification, unless serious mitigation efforts are put in place. We recently reported on a hatchery in Oregon that’s become a model for adapting to these different conditions. But the long term solution may actually lie in shellfish genes.\nEvolution and resiliency are the buzzwords for a sustainable mariculture industry in Alaska, a state that is particularly vulnerable.\n“And Alaska is going to be the test bed unfortunately for informing us for how the rest of the ecosystem will respond to ocean acidification,” says Jeremy Mathis, a NOAA oceanographer who worked on the recent study based at the Alutiiq Pride Shellfish Hatchery in Seward.\nOne short-term solution hatcheries are testing is injecting the acidic ocean water with carbonates that are needed for organisms like clams and mussels to develop hard shells.\nBut in the long term, Mathis says they may need to turn to genetics for answers.\n“Ideally we can start looking at species that are more resilient to ocean acidification and adapting the commercial fisheries and commercial processing to animals that have that robustness to tolerate ocean acidification as opposed to the ones that are more vulnerable to it,” says Mathis.\nThat’s where scientists like Gretchen Hofmann come in. She’s a marine biology professor at the University of California, Santa Barbara.\n“I work on marine invertebrates and sometimes fish and we study how they respond to their environment. We would call it environment-organism interactions and lately we’ve been interested in how these organisms will respond to future changes in ocean pH and ocean warming,” says Hofmann.\nShe’s a leader in what Mathis calls the emerging field of genetic adaptability.\n“So it really just started with a conversation with oceanographers who were thinking about this and from there, we started to do experiments, and then we started to ask deeper questions about whether or not organisms could adapt to these changes in the ocean and even if there are already genotypes and strains of organisms that are able to handle a low pH condition,” says Hofmann.\nShe says the first experiments they did were a bit too basic for Mother Nature. They’d take species, put them in water with different pH, and see how they’d react. That didn’t reflect natural variations in ocean conditions.\n“What we found was that there wasn’t just this straight line, no pH change, but that pH was going up and down sometimes quite dramatically,” says Hofmann.\nAlaskan waters, for example, are very cold and have shifting pH depending on the seasons, fresh water inputs, and how much CO2-rich glacial melt is present.\nFrom Washington to California, the coast is subject to a phenomenon known as upwelling, which is a cycle of seasonal winds pushing newer oxygen-rich water off the surface of the ocean and older, nutrient and CO2 rich water rising up to take its place. That means a lot of pH fluctuation.\nSo, Hofmann says they shifted their sea urchin research to take upwelling into account.\n“And we formed the hypothesis to test that the adults from the place where there was a lot of low pH exposure would be genetically different from the wimpy ones that did not experience all that pH stress,” says Hofmann.\nThey found that the urchins from areas with upwelling had a different genetic signature from those who weren’t and their progeny, or babies, were more tolerant of acidic water.\n“It was even more interesting because it looked like the trait of being able to tolerate that low pH, that was heritable,” says Hofmann.\nShe points to work being done in New Zealand, where different types of green-shelled mussels are being cross-bred to develop a new resistant and adaptive strain.\nSo, an Alaskan hatchery, for example, could choose to make the shift from some common species being raised now to ones that selectively favor that trait.\nBut it also may mean letting go of consumer preference for certain types of clams, mussels, and other shellfish that just don’t measure up.\n“These are things that we should be taking a strong look at because it could be that there are other strains of shellfish that could be used that would be more successful in a mariculture setting. But, it is a very thorny issue and one that I think science could bring a lot of daylight to, I think, if we work together on it,” says Hofmann.\nHofmann says it’s important for industry and scientists to start partnering now, to get ahead of the game as much as possible.\n“The first thing we have to do though is get carbon dioxide emission levels under control and then we can deal with the damage that has already been done through mitigation and adaptation strategies,” says Mathis.\nBecause, the problem will only get worse with each coming year.']	['<urn:uuid:688c3bdb-1781-42e7-a13b-c4e23c8c8d32>']	open-ended	direct	verbose-and-natural	distant-from-document	single-doc	novice	2025-05-12T21:31:42.699391	16	96	814
54	startup validation methods and risks of bias	Startup validation involves several key steps: defining the ideal customer profile, conducting customer development through interviews and surveys, and finding interviewees through various channels like LinkedIn, forums, and communities. However, it's crucial to be aware that bias can affect validation processes. Just as machine learning systems can be biased based on unrepresentative training data, startup validation can be skewed by biased feedback from friends/family who just say 'yes, you're awesome' or by focusing on too narrow a sample of potential users. To combat this, it's important to use numbers and data to back up validation claims, seek diverse perspectives, and critically question whether you're truly reaching your target audience.	"['Building a startup is easy, right? All you need is an idea, and the right skills, with related past experience, alongside a strong team, with just the right amount of money, a grand vision, an insane level of perseverance, people open to giving you a shot, and a bit of luck, all at the right time.\nOK fine, building a startup is hard. Well, at least about 4,000 people here agree.\nThat is why I want to spend some time sharing the importance of customer development (also used interchangeably with the terms user discovery and idea validation) – so you don’t waste months or years of your life building something no-one wants.\nBut first, let’s level-set on the average progress early-stage startups have made on their product at the pre-seed and seed stage. I’ve found that through the years, as the market has become more crowded and therefore competition has increased, expectations of having some sort of MVP earlier and earlier in a company’s lifecycle have become the norm. In fact, according to DocSend’s research, a heavy majority of companies have gone out to fundraise their first official venture-backed round with at least an alpha or beta version of their product.\nAs a founder, R&D and market research is hopefully what drives you in these early days of building a company. My love for market research came from these books: The Startup Owner’s Manual, The Lean Startup, Running Lean, Sprint, and Hacking Growth. Check them out if you find yourself resonating with the lessons within this blog and want to dig in deeper.\nThe three strategies I will focus on — in order to gather idea validation and user feedback regardless of the stage that your product is at — are defining the ideal customer profile, investing in customer development, and finding interviewees.\nStep 1: Ideal Customer Profile\nBefore you dig too much into the idea and solution, customer development, and post-launch user feedback collection, it is important to know who your target audience is. Hubspot has this great tool that I often recommend that lets you create your personas.\nUse that as the basis of everything else you do, because if you don’t know who you’re solving the problem for, you’ll end up in the same boat as I see many founders: “So I have built this thing. Does anyone in the world have a problem that this solution could help with?”. That’s the situation you want to avoid, 100% of the time. Define your target audience, and then start customer development.\nAnother common problem is trying to do too much, or solve too many things, which in-turn has too broad of an audience and in turn no real target audience. Start laser-focused on solving one problem for one target customer, and then expand later when you have proven traction or product-market fit. Know what you plan on doing well day one, while keeping in mind where this could go on day one thousand.\nStep 2. Customer Development\nOnce you’ve got the persona(s) defined, you can start doing some basic outlining: what is the problem, what is the proposed solution, what is your unfair advantage, what are current market trends, and which competitors are working on this. If you need a place to organize all of this, I suggest using Idea Validator, a workspace dedicated to tracking all your pre-launch customer development notes.\nThis tool has a whole section dedicated to conservation tracking. At this stage, you’re looking for how your potential customers speak about their current way of doing things. Look out for feature requests, pain-points, and patterns. Whether you’re getting on a phone call with your target audience, or sending out online surveys, Idea Validator provides the workspace to document it all.\nSo, when your investor asks “what type of validation or traction do you have?” you can show them how many people you have collected feedback from, and even how many show support for your proposed solution.\nThis is the key to making things believable.\nI run the other way when I hear these things:\n- We’ve spoken to so many people and they all love it – 100%, really?\n- We’re overwhelmed at all the positive feedback – is that including your best friend?\n- We’re crushing it – no, you’re failing this pitch.\nHence, use numbers and data to back up your narrative.\nSolid example: “We’ve reached out to 280 people who are within our target audience, 224 (80%) responded, and of those, 202 (90%) said they would pay $20 a month for this if it existed.\nStep 3. Finding Interviewees\nSome find this the hardest part. If it’s too hard, then really figure out if you are building something needed. For example, if it’s impossible to find your target audience, then it could mean your total addressable market (TAM) may be too small, or that people don’t self-identify as potential users of this solution. Proceed with caution. Otherwise, if you know your buyers exist and just need more, this should give you some starting points to find folks:\n- Reach out to old connections on LinkedIn that fit the persona you are targeting. The advanced search on LinkedIn is very useful here.\n- Email your friends, family and colleagues and ask if they know someone who fits the persona you are targeting. You can ask them directly too, but know that they’re likely the ones to just say “yes, you’re awesome just build it” (My thoughts on: How to tell if your friends or mum are just saying your idea is awesome…)\n- Meet people out and about, at coffee shops, co-working spaces or networking events. Be careful not to make judgment on whether they fit your persona, and always try to #GiveFirst. Some things you can offer are: free trials, office hours, coaching sessions on your area of expertise, or introductions to people in your network.\n- Post on forums like Reddit, HackerNews or Facebook Groups where people are often sharing their own ideas. Kernal is another more recent platform that I recommended checking out too.\n- With these forums, it’s important to not join all of them and “spray and pray”. The key is find subtopics, or relevant categories to follow and find people who are in your target group. I always say, it’s better to speak to 20 vetted people than 200 unvetted.\n- Join founder/creator communities like Product Hunt or Pioneer. These have recently been my top picks.\n- Product Hunt – the best of the best community where anyone can post their project and collect feedback from thousands of people.\n- Pioneer – more tournament style (if you want it, you get paired with other entrepreneurs that you work with closely once a week.\n- Explore TalkHowdy.com to find free or paid communities on Slack, Discord, and more. There is literally a community for everyone, so join them, start engaging and get them excited about what you’re building!\n- Host Twitter Spaces, Clubhouse rooms, or other audio-first platforms to randomly meet people who have the same interests as you.\n- Attend Startup Weekends and Hackathons to mingle with other entrepreneurs who are building stuff.\n- If you have the budget, you could also use User Interviews or User Testing. Note, these tools are great because they are even more useful post-launch.\nRemember though, market research doesn’t stop after launch. Continue to listen to your users. Email them, send follow-up surveys, have phone calls with your most active users, and your least active. Have video calls whenever possible.\nWhatever it takes to give them a voice in your product development roadmap.\nJust because you gain some traction, raise some money, or have some happy customers, it doesn’t mean R&D stops. This is always a critical role of being (co)founder and CEO.', 'Bias in Machine Learning\nFairness in Machine Learning\nSo far, this course showed how machine learning can enhance your work, from saving precious time on existing tasks to opening up new opportunities. ML can do a lot for you, but it comes with challenges you shouldn\'t overlook.\nTo address those challenges, a growing number of researchers and practitioners focus on the topic of ""fairness"" in machine learning. Its guiding principle is that ML should equally benefit everyone, regardless of the societal categories that structure and impact our lives.\nWhat is bias?\nWhat are the negative consequences that might derive from the use of machine learning? The short answer is: Bias.\nAs humans, we all have our biases. They are tools our brain uses to deal with the information that is thrown at it every day.\nTake this example: close your eyes and picture a shoe. Most likely you pictured a sneaker. Maybe a leather men\'s shoe. It\'s less likely that you thought of a high-heeled women\'s shoe. We may not even know why but each of us is biased toward one shoe over the others.\nNow imagine that you want to teach a computer to recognise a shoe. You may end up exposing it to your own bias. That\'s how bias happens in machine learning. Even with good intentions, it\'s impossible to separate ourselves from our own biases.\nThree types of bias\nThere are different ways in which our own biases risk to become part of the technology we create:\nTake the example before: if we train a model to recognise shoes with a dataset that includes mostly pictures of sneakers, the system won\'t learn to recognise high heels as shoes.\nIf you train a ML system on what a scientist looks like using pictures of famous scientists from the past, your algorithm will probably learn to associate scientists with men only.\nSay you\'re training a model to recognise faces. If the data you use to train it over-represents one population, it will operate better for them at the expense of others, with potentially racist consequences.\nSo what can we do to avoid these biases?\nAsking the right questions to avoid bias\nAs a journalist, a first line of defence against bias is firmly within your reach: the same values and ethical principles you apply every day in your profession should extend to assessing the fairness of any new technology that is added to your toolbox. Machine learning is no exception.\nFurthermore, in all cases you should start by considering whether the consequences might negatively impact individuals’ economic or other important life opportunities. This is critical especially if the data you use includes sensible personal information.\nOften, the unfair impact isn\'t immediately obvious, but requires asking nuanced social, political and ethical questions about how your machine learning system might allow bias to creep in.\nConsidering the main sources of bias\nWhile no training data will ever be perfectly ‘unbiased’, you can greatly improve your chances of building a fair model if you carefully consider potential sources of bias in your data, and take steps to address them.\nThe most common reason for bias creeping in is when your training data isn\'t truly representative of the population that your model is making predictions on. You must make sure to have enough data for each relevant group.\nA different kind of bias manifests itself when some groups are represented less positively than others in the training data. You should consider reviewing your data before using it to train a model, in order to verify whether it carries any prejudices that might be learned and reproduced by the algorithm.\nPreventing bias: it starts with awareness\nBias can emerge in many ways: from training datasets, because of decisions made during the development of a machine learning system, and through complex feedback loops that arise when a ML system is deployed in the real world.\nSome concrete questions you might want to ask in order to recognize potential bias include:\n- For what purpose was the data collected?\n- How was the data collected?\n- What is the goal of using this set of data and this particular algorithm?\n- How was the source of data assessed?\n- How was the process of data analysis defined before the analysis itself?\nBias is a complex issue and there is no silver bullet. The solution starts with awareness and with all of us being mindful of the risks and taking the right steps to minimise them.\nGoogle Translate: Translations on-the-go.lesson 5 minutes BeginnerSpeak the language just about anywhere in the world.\nGoogle Consumer Surveys: Gain real insights.lesson 5 minutes BeginnerGather data from real people by creating your own web surveys.\nDataset Search Quickstart Guidelesson 5 minutes BeginnerSearch across millions of datasets on the web']"	['<urn:uuid:3b59733a-c8c5-46b0-bce9-2e5cd0e791da>', '<urn:uuid:74606cbf-aac5-400f-b898-ec82b04a0f8b>']	open-ended	direct	short-search-query	similar-to-document	multi-aspect	novice	2025-05-12T21:31:42.699391	7	109	2099
55	What causes heart problems in cats and how are they diagnosed?	Hypertrophic cardiomyopathy (HCM) is the most common cardiovascular disease in cats, where heart chamber walls become progressively thicker, particularly affecting the left ventricle. In Maine Coon cats specifically, it's caused by a mutation in the MYBPC3 gene. Diagnosis typically occurs through blood tests, x-rays, and particularly doppler echocardiograms. For pulmonary hypertension, which often accompanies heart problems, diagnosis includes blood pressure screening, urinalysis, blood chemistry tests, and cardiac imaging to detect abnormalities in lung and heart function.	"['It seems you can’t turn on the news\nwithout hearing about heart disease. Everyone is aware that humans are\nat risk for cardiovascular disease, but most cat owners probably don’t\nrealize that cardiovascular diseases are common disorders in cats. In\nfact, disorders of the myocardium (heart muscle) are the major cause of\nheart failure in cats. The most commonly seen cardiovascular disease in\ncats is hypertrophic cardiomyopathy (HCM).\nThe heart has four chambers that pump\nblood. The normal thickness values of the walls of the heart have been\nmeasured and charted, and are very familiar to veterinary\ncardiologists. In cats suffering from HCM, the walls of the heart\nbecome progressively thicker, with one particular chamber, the left\nventricle, usually becoming the most affected. The left ventricle pumps\nblood out through the aorta to the rest of the body. When the left\nventricle becomes thickened, it cannot hold as much blood. Imagine the\nwalls of a coffee mug becoming thicker and thicker, growing inward. It\nwould hold less and less coffee. Such is the case with HCM.\nAs the left ventricle becomes thicker\nover time, other complications can arise. The left atrium is the\nchamber directly above the left ventricle, and it supplies blood to the\nventricle, which then sends it to the rest of the body. As the left\nventricle becomes thicker and stiffer, it becomes more difficult for the\natrium to push blood into the ventricle. As the atrium strains to do\nthis, it becomes larger and larger. Once the atrium stretches out, it\ncauses the valve between the atrium and ventricle to malfunction.\nPressure eventually builds up in the left atrium, and this pressure can\nbe transmitted to the lungs, resulting in fluid retention in the lungs\nand congestive heart failure.\nHCM can strike any breed of cat, but\nMaine Coons are predisposed to the disorder. For years, the genetic\ncause of HCM in this breed was a mystery. In 2005, however, the reason\nwas discovered: a mutation in the gene that codes for a protein, called\n“myosin-binding protein C” (abbreviated MYBPC3). Mutations in MYBPC3\nare the most common cause of hereditary HCM in people, but a genetic\nmutation causing any type of heart disease in a cat had never been\nreported until that study was published.\nThe prevalence of HCM in cats has been\nreported to be anywhere from 1.6% to 8.3%, depending on the population\nevaluated and the method of diagnosis. The prevalence of HCM in the\nMaine Coon cat population is not known, but recently, a study was\nperformed to determine the percentage of cats worldwide with this\nDNA (either blood or a cheek swab)\nsamples were submitted to the Veterinary Cardiac Genetics Laboratory at\nWashington State University. Samples came from both clinically normal\ncats as well as cats known to have HCM. A total of 3,310 samples, from\n21 different countries, were submitted. At least 17 different breeds\nwere represented. Maine Coons accounted for 100% of all of the positive\nsamples; no other breed tested positive for the MYBPC3 mutation. Of the\n3,238 samples from Maine Coons, 1089 tested positive for the mutation.\nIn other words, the overall worldwide percentage of Maine Coon cats with\nthe mutation was 34%.\nGenes come in pairs. Only one gene in\na pair may be affected by a mutation, or both genes may be affected.\nWhen one gene is affected, we say that the cat is “heterozygous” for the\nmutation. When both genes in the pair are affected, the cat is termed\n“homozygous” for the mutation. Of the 1,089 Maine Coon cats that tested\npositive, 91% were heterozygous for the mutation. Only 9% had the\nmutation in both copies of the gene.\nSo what does this mean for Maine Coon\nowners and breeders? Clearly, the MYBPC3 gene mutation appears to be\nspecific to Maine Coons only, and at 34%, is fairly prevalent within the\nMaine Coon cat population. Does testing positive for the gene mean that\nthe cat will develop HCM? In this prevalence study, the authors did not\nattempt to correlate the presence of the gene with the development of\nHCM, however, it seems clear that not all cats that test positive for\nthe MYBPC3 mutation will go on to develop HCM. Also, in the original\nstudy that identified the mutation that causes HCM in Maine Coons, cats\nthat were homozygous for the mutation (both genes in the pair affected)\ndeveloped severe disease, with most dying before the age of 4, whereas 3\nout of 10 Maine Coons that were heterozygous for the mutation (one gene\nin the pair affected) had moderate disease and were still alive at 8 to\n12 years of age.\nFor Maine Coon breeders, the\ndevelopment of the test is a major breakthrough. The mean age at\ndiagnosis of cats with HCM is 5.9 years. The data we have so far do not\nyet indicate the percentage of cats with the mutation that do go\non to develop clinical HCM, nor do they tell us what percentage of cats\nwithout the mutation go on to develop HCM. Because cats with the\nMYBPC3 gene are believed to be more likely to develop clinical HCM than\nnegative cats, these cats should be evaluated periodically by a\nveterinary cardiologist to see if HCM is developing. From a breeding\nperspective, having this genetic test affords an opportunity to detect\ncats with the mutation years before they develop HCM, and would allow\nselective breeding programs aimed at eliminating the gene from the\nMaine Coons with the MYBPC3 mutation are believed\nto be more likely to develop HCM than if they did not have the mutation.\nMaine Coons with the mutation affecting both copies\nof the gene tend to have more severe disease compared with Maine Coons\nhaving the mutation affecting only one gene in the pair\nNot all Maine Coons with the mutation will develop\nNon-Coons that develop HCM do not have the MYBPC3\nmutation; this mutation is only present in Maine Coons.\nWhether Maine Coons that test negative for the\nmutation can go on to develop HCM is not known at this time.', ""What is Pulmonary Hypertension?\nSigns of PHT include respiratory distress, coughing, and fluid buildup in the lungs or abdomen. Heart failure is closely associated with pulmonary hypertension as the function of the lungs and heart are closely related. PHT is a serious, life-threatening condition and veterinary help should be sought immediately if signs of PHT appear in your cat.\nPulmonary Hypertension (PHT) is the result of high pulmonary arterial (blood) pressure in the lungs. Cats do not appear to be prone to primary PHT that is due to congenital lung abnormalities, which occurs in other animals. When cats experience PHT it tends to be secondary to another condition that has affected lung functioning, usually by causing obstruction in the arteries or capillaries of the lungs. As capillaries are very narrow they are particularly prone to obstructions that result in oxygen not being properly taken up by the lungs. Common causes of obstruction that may result in PHT in cats are heartworms and blood clots.\nSymptoms of Pulmonary Hypertension in Cats\nBecause blood pressure is not routinely monitored by veterinarians, PHT is not usually discovered until after symptoms occur. Because of the serious, life-threatening nature of pulmonary hypertension symptoms to be alert for are:\n- Respiratory distress/breathing problems/labored breathing\n- Respiratory crackles and wheezes\n- Lethargy/reluctance to exercise/fatigue\n- Loss of consciousness/fainting\n- Abdominal distension\n- Symptoms of other condition(s) that are causing PHT e.g. vomiting, fever\n- Signs of parasitic infection\n- Jugular distention\n- Pale mucous membranes due to lack of oxygenation of tissues\n- Heart murmur\n- Symptoms of heart failure\n- Diminished pulse\n- Loss of appetite/weight loss\n- Bleeding, including coughing blood, blood in urine, eye hemorrhage or nosebleed\n- Seizures and neurological symptoms\nCauses of Pulmonary Hypertension in Cats\nIn cats, PHT is usually secondary to another condition, it does not tend to be caused by genetic abnormalities of the lung. Conditions and causes of PHT include:\n- Poisoning/exposure to toxic substances\n- Adverse reaction to drugs or medication\n- Viral infection\n- Bacterial infection\n- Chronic airway disease\n- Hypoxia from high elevations\n- Heart disease\n- Upper airway obstruction\n- Obstructive pulmonary disease (chronic pneumonia, infectious bronchitis)\n- Pulmonary thromboembolism/blood clots/vascular blockage\n- Parasitic infection - Heartworm\n- Kidney disease\nObesity may be a contributing factor, making an overweight pet more susceptible to PHT.\nDiagnosis of Pulmonary Hypertension in Cats\nRoutine blood pressure screening is not usually part of regular veterinarian monitoring of your pet's health, therefore, hypertension is usually only discovered after symptoms have occurred. Because PHT in cats is secondary to other conditions, your cat may already be under veterinary care for the underlying condition when PHT symptoms appear. Your veterinarian will perform a complete physical examination including measuring your cat’s blood pressure and taking a medical history of your cat in order to determine possible underlying conditions for pulmonary hypertension symptoms in your cat if causation is not already known from previous diagnosis and treatment.\nUrinalysis, blood chemistry and blood gas tests will be ordered to identify possible underlying conditions such as organ failure, bacterial or viral infection, or heartworm infection, and to measure oxygen and carbon dioxide in the blood to determine abnormalities in lung functioning.\nPulmonary functioning tests may be used to check for obstructions and airway disease. If fluid has built up in the sac around the lungs or in the abdomen, a sample will be taken and analyzed.\nX-rays will reveal abnormalities in the lung or heart. Cardiac catheterization is a standard method of diagnosis for PHT, but more recently a doppler echocardiogram which will reveal blood clots, pressure in vessels, and heart abnormalities has become a more effective method of diagnosis.\nTreatment of Pulmonary Hypertension in Cats\nPrognosis for PHT in cats is guarded and the outcome will be poor if an underlying condition cannot be identified and successfully treated.\nYour veterinarian will treat the underlying condition causing the pulmonary hypertension. Examples of common treatment for secondary conditions are; surgery for removal of heartworms, antibiotic for infections, blood thinners for blood clots, and medications to counteract heart failure or chronic pulmonary disease.\nMedications for lowering blood pressure and opening airways will be prescribed and supportive therapy involving oxygen therapy used if necessary. Hospitalization and cage rest during treatment is common.\nRecovery of Pulmonary Hypertension in Cats\nPrognosis, recovery, and management of PHT depends on its cause. Along with appropriate management of the underlying condition, recovery from PHT will require pet owners to provide rest and a stress-free environment. A low-sodium diet to reduce blood pressure is recommended. If your pet is overweight a weight loss diet will lower their risk of relapsing.\nPulmonary Hypertension Questions and Advice from Veterinary Professionals\nmy cat was fine one day next day took her to vet she died.\nno conclusive reason.\nthey chunk it up to hear t worm she had labored breathing.\nI think she was just stressed form the changes in our home we brought home our inlays cat because he had kittens in his home and then a dog that for 2 nights\nAdd a comment to sake's experience\nWas this experience helpful?""]"	['<urn:uuid:dd703ea1-fc27-4080-92d4-2a52f2ff49c2>', '<urn:uuid:0734f3b0-8349-489d-87f5-f74d512becc7>']	open-ended	direct	concise-and-natural	distant-from-document	multi-aspect	novice	2025-05-12T21:31:42.699391	11	76	1873
56	What kinds of punishments were given to people who violated the Sabbath in Peel Castle, and where were these prisoners kept?	Sabbath breakers were kept in the Cathedral crypt of Peel Castle, which served as the Bishop's prison. The crypt was dank, dark and cheerless, and the conditions were so harsh that long sentences were unnecessary. People were imprisoned there for activities like playing the violin, making hay, or fishing on Sabbath or special saint's days.	['The first known fortification on St Patrick’s Isle came after the arrival in 1098 of the invading Viking chieftain known as Magnus Barefoot. The Chronicles of the Kings of Mann and the Isles tells us that Magnus saw to the erection of two wooden prefabricated forts, one now believed to be in the South of the island and the other on St Patrick’s Isle. It is likely that the rampart of this fort would have been topped by some sort of wooded fence or paling (=Peel), the possible inspiration of the eventual name of both the Castle and the Town.\nThe visible stonework in the Castle includes the remains of buildings belonging to the Celtic monastery, possibly dating from the 6-8th centuries and the 13th century cathedral of St German and later buildings allied to it. Other buildings include a medieval armory and barracks. These are protected by fortifications consisting of 14th century flanking towers, linked by 15th century curtain walls. The latest military construction took place in the early 19th century during the Napoleonic wars and later in the 1860s during the time of Napoleon III. It is considered that a vestige of Magnus’s fort was found during the final phase of the excavations of 1982-87, when rough stonework of a rampart was uncovered at a depth of 5 metres beneath the surface, just inside the present curtain wall to the north of the Cathedral.\nIt is believed that Irish missionaries came here about the year 550, disciples of St Patrick and probably not, as local legend has it, led by him, but by St Carmane (or German). This would be a suitably isolated spot for monks of the Christian Celtic Church to choose. At the highest point of St Patrick’s Isle stands the 50ft (16m) Round Tower. This is of the type seen at monastic sites in Ireland, where the towers tend to taper towards the top, unlike Peel’s which has vertical sides. Though its top is battlemented, it once had a conical roof, like those in Irleand. The Tower would have acted as a belfry, as a lookout place and as a place of refuge. Nearby are as many as three keeills or chapels, the largest being St Patrick’s Church, which was enlarged in the 12th century, possibly with the intention for it to be a cathedral. Until about 1500, it acted as the Parish Church for Patrick, when St Peter’s Church in Peel became the parish church for both Patrick and German.\nIrish sagas record St Patrick’s Isle suffering a Viking attack, but modern belief is that it was another St Patrick’s Isle, that near Dublin, which is indicated. It is likely that the settlement of the pagan Vikings about the year 800 would witness the decline of sites such as this monastery, possibly followed by destruction during later invasions.\nHowever, it is likely that once the Vikings were Christianised, St Patrick’s Isle once again became a holy place when the diocese of Sodor, with its cathedral, was established here. St German’s Cathedral was built of the local red sandstone from across Peel bay about 1230, replacing a previous church. This was alongside the residence of the King and capital of the Viking Kingdom of Mann and the Isles.\nAfter the Isle of Man passed into English suzerainty in the early 14th century, the fortress we see today was created. Firstly, about 1390, the sandstone keep – the peel tower which eventually was to give its name to the town – was built, along with the so-called ‘Red Curtain’, the adjoining sandstone curtain walls. This was to compensate for the weakness of the defenses at this point, so easy of access at low tide. Flanking towers were also built and later, the rest of the curtain walls. These are nicknamed ‘The Green Curtain’ after the slate used, which was quarried from an area just outside the Castle, at the northern most part of St Patrick’s Isle.\nPeel Castle was garrisoned as the administrative centre of the Northside of the Island, with Castle Rushen controlling the Southside. In addition to its governmental role, Peel Castle was used as prison. One of the best known political prisoners was Edward Christian, the Earl of Derby’s Lieutenant Governor of the Island. Christian was one of the leaders of the 1643 protest about tithes. The Earl very cleverly calmed the large crowd of protesters who had gathered to meet him at The Green at Peel – today a public car park and grassy area along Peel Promenade. Christian, whose revolutionary reforms included election of the members of the House of Keys, was arrested and charged with treason. He died in 1661, during his second period of imprisonment in Peel Castle.\nThe Cathedral crypt became notorious as the Bishop’s prison, housing Sabbath breakers for short periods. This crypt was, and remains, dank, dark and cheerless, conditions which made long sentences unnecessary. Those incarcerated had been caught, possibly by their neighbours, desecrating the Sabbath or special saint’s days by playing the violin, making hay or by fishing. Meanwhile, just to the north of the Cathedral, in the most sheltered area of the Castle, the English Lords of Mann, the Earls of Derby, built their private apartments. The Cathedral continued its role until 1785, when it saw its last enthronement of a Bishop. Even then in a bad state of repair, its final humiliation came in 1824, when the remaining roof timbers collapsed in a violent storm, leaving the Cathedral as we see it today.\nSave for the construction of a revetment for two heavy guns, plus a guardhouse and powderhouse about 1816, the Castle was abandoned until a revival of the garrison was necessary in the 1860s because of the scare caused by the renewed predatory ambitions of France. Today’s ruinous state of the former Lord’s apartment buildings is due to their being used as a quarry for the construction of these defenses.\nThere was some refurbishment at the behest of Sir Henry Loch, the Island’s enlightened Lieutenant Governor 1863-83. His period of duty coincided with the burgeoning of both the tourist trade and the fishing industry, both of which became so important to the livelihood of Peel’s inhabitants.\nThere have been several archaeological excavations carried out in Peel Castle. By far the largest was carried out 1982-87 by Liverpool University archaeology group at the behest of the Manx Museum and the then Government Property trustees. This was a ‘teaching dig’ in which the public were invited to take part, supervised by qualified archaeologists. A large number of volunteers, mainly local residents, but people from Britain and some from abroad, including the USA, also took part.\nThe many important finds included a cache of silver coins dating from around 1030, an extensive burial ground with several hundred graves, which included that of the so called ‘Pagan lady’, buried with her beautiful necklace, signs of Magnus Barefoot’s fort of 1098 and many details of the Lord of Mann’s apartments. An unexpected bonus as a claim to fame is the finding of the British Isles’ earliest human flea. The dig progressed through the ruined buildings to the north of the Cathedral, revealing details hitherto unseen for centuries. Overall, the outcome of these excavations was a much greater understanding of the role of Peel Castle in the Island’s history.']	['<urn:uuid:c76a130f-35ea-4219-85fb-2dc53adb89fe>']	factoid	direct	verbose-and-natural	similar-to-document	single-doc	expert	2025-05-12T21:31:42.699391	21	55	1223
57	What's special about summer days in Svalbard?	Svalbard lies north of the Arctic Circle where the summer sun never sets.	['On a voyage aboard the new National Geographic Endurance, discover the remote islands and fjords of northern Norway, including the high Arctic archipelago of Svalbard. Spend several remarkable days amid the ice floes of Svalbard and follow nature’s rhythms in search of walruses, seals, and polar bears. Continue to Bear Island to encounter towering cliffs populated with thousands of seabirds. Then head further south to the spectacular isles of Søroyø and Senja, where nature flourishes amid vibrant coastal cultures.\nThis trip is offered in partnership with Lindblad Expeditions.\nDepart on an overnight flight to Oslo and check in to our hotel upon arrival. In the afternoon, embark on a tour of Norway’s capital city. Stroll through Frogner Park among the famed Vigeland sculptures—hundreds of life-size human figures by native sculptor Gustav Vigeland. Then visit the Fram Museum, dedicated to the wooden ship sailed by Norwegian pole explorers such as Nansen and Amundsen.\nDepart Oslo on a private charter flight and enjoy breathtaking vistas en route to Longyearbyen, the largest settlement in Svalbard. On arrival, visit the Svalbard Museum to learn about the history of Svalbard and the Arctic, from 17th-century whaling to modern scientific research. Explore the Svalbard Art Gallery, home to a permanent collection of historic items as well as contemporary exhibitions of local artists. Later, the embark the National Geographic Endurance, your floating base camp for the next six days.\nNational Geographic Endurance through Day 9\nSvalbard lies north of the Arctic Circle, where the summer sun never sets. Here, in true expedition style, we let nature guide our course. Our flexible itinerary and years of experience in the archipelago allow us to take the best advantage of ice and weather conditions. With our fully stabilized, ice-class ship, we probe deep into the ice in search of wildlife. Join our spotters on the bridge, scanning the edges of the pack ice for huge walruses, arctic foxes, reindeer, polar bears, and other resident species. Venture ashore to hike the tundra, abloom with wildflowers. Sail deep into beautiful fjords and kayak quiet coves surrounded by sparkling icebergs. Board Zodiacs to view tidewater glaciers and spectacular ice formations.\nWith our National Geographic photographer, experts, and seasoned naturalists, we make the most of our days here. Enjoy nightly recaps and talks about the natural wonders we encounter, and view underwater video footage taken some 1,000 feet below the surface by our remotely operated vehicle (ROV). On this extraordinary voyage, every day is different, and every day is an adventure.\nSail south to isolated Bear Island, the southernmost island of the Svalbard archipelago. Board a Zodiac to explore the coves and inlets of this mist-shrouded island. Gaze up at sheer cliffs and use your binoculars to take in the spectacular sight of tens of thousands of fulmars, kittiwakes, and murres.\nOff Norway’s northern coast lies the mountainous island of Sørøya, known for its scenic bays, pristine nature reserves, and long sandy beaches. It also is renowned for its large populations of skrei, or cod, which migrate to these waters during winter and spring. We plan to make an in-depth visit to this beautiful island, with opportunities to explore its surrounding fjords by Zodiac and kayak, and hike on shore.\nSenja, the second largest island in mainland Norway, is characterized by dramatic mountains, colorful fishing villages, and a coastline split by beautiful fjords. In the southern part of the island, you’ll find Ånderdalen National Park—more than 70 square miles of pristine wilderness that is home to reindeer, elk, red foxes, and more. Spend a day exploring the island and fjords, and, if you wish, join our naturalists on a spectacular hike.\nDisembark in Tromsø, known as the “gateway to the Arctic” for the numerous Arctic expeditions that originated here. Explore this charming coastal city, then board our charter flight to Oslo and check in to our hotel.\nAfter breakfast, transfer to the airport for your return flight.']	['<urn:uuid:5237ebc6-25f6-4cb9-b467-f9ae906488d7>']	factoid	with-premise	concise-and-natural	distant-from-document	single-doc	novice	2025-05-12T21:31:42.699391	7	13	653
58	My friend told me that doctors are using some spray called Sativex to help people quit smoking marijuana. How effective is this treatment in reducing cannabis use?	Studies have shown that nabiximols (Sativex®) was effective in reducing cannabis use by 70% in treated patients. The treatment was well tolerated and did not lead to greater incidence of abuse. Nabiximols contains both THC and CBD, with CBD counteracting the negative effects of THC abstinence.	['Fighting Fire with Fire: Using agonist replacement therapy to treat cannabis addiction\nBy: Sonja Elsaid\nPot. Weed. Skunk. Dope. Mary Jane. Call it whatever you want, cannabis, is material that contains dried leaves, flowers or fruiting tops of the plant Cannabis sativa,1 usually smoked as joints (cigarettes), or in bongs (pipes). It can also be baked into foods, such as cakes, cookies, and the ever-magical brownies. Worldwide, cannabis is the third most commonly used psychoactive substance after tobacco and alcohol.1 It is estimated that over 40% of Canadians have tried cannabis at least once in their lifetime.2 Cannabis consumers often seek feelings of relaxation or euphoria and they often experience changes in perception.3 Responsible for these effects is tetrahydrocannabinol (THC), the main psychoactive ingredient in Cannabis Sativa.\nSadly, it is expected that 8-12% of regular cannabis users will at some point in their lifetime develop cannabis use disorder (CUD), which can lead to cannabis addiction.1 According to the Diagnostic and Statistical Manual of Mental Disorders (DSM-V) used by Canadian healthcare professionals, CUD is a recognized mental health disorder.4 Furthermore, cannabis withdrawal, which is the more severe form of CUD, can develop after the abrupt reduction or cessation of prolonged cannabis use.4 Individuals in the state of withdrawal normally experience unpleasant symptoms, such as irritability, sleep difficulty, depressed mood and restlessness.4 These unpleasant conditions not only interfere with daily functioning of these individuals, but efforts to avoid them represent the driving force for continuous cannabis abuse.1 Currently, the only approved therapy for cannabis withdrawal is psychotherapy.1,5 However, only 20% of those treated achieve abstinence, indicating that better treatments are needed.5\nSince successes were observed with using agonist replacement therapies for treating other substance use disorders, such as using nicotine patch to help quit smoking, scientists started testing synthetic THC in subjects with cannabis withdrawal.1 The rationale for using the agonist replacement therapy was the potential to reduce symptoms of withdrawal by providing a similar drug—synthetic THC—through a safer route of administration. Thus, instead of smoking a joint, and inhaling chemicals, synthetic THC would be ingested.1 The other benefit of this harm reducing method was to empower patients to make the necessary lifestyle changes, while distancing them from the regular substance abuse.5\nOne of the first synthetic THC drugs tested for cannabis addiction was dronabinol, an oral and synthetic THC.1 Initially, dronabinol was approved for treatment of anorexia, nausea, and vomiting in cancer patients.1 Early clinical trials demonstrated that dronabinol was effective in reducing cannabis smoking by 50%. However, its poor bioavailability and slow onset of action prompted scientists to test nabilone (CesametTM), a synthetic analog with better bioavailability.1,5 Like dronabinol, nabilone was originally approved for the treatment of nausea and vomiting in cancer patients.1 In clinical trials, nabilone was effective in decreasing cannabis smoking and treating symptoms of withdrawal.1 Furthermore, nabilone is long-acting and did not exhibit addictive potential likely due to its slow onset of action.5 Despite these strengths, nabilone is not approved as CUD treatment, as nabilone is still composed of THC, the substance originally derived from marijuana, which led to cannabis addiction in the first place.5 Furthermore, the medical community’s fear of prescribing THC alone for any medical condition was why researchers had to look elsewhere for more sophisticated compounds to treat CUD.5,6\nThe most recent variations of agonist replacement therapy is nabiximols (Sativex®), which have been prescribed for the treatment of multiple sclerosis and cancer-related pain in 15 countries.2 Nabiximols contains THC, which aims to provide the agonist replacement therapy. However, unlike dronabinol and nabilone, nabiximols also contains cannabidiol (CBS), the other major ingredient found in marijuana.1 Importantly, CBD does not exhibit the same mind-altering properties of THC and counteracts the negative effects of THC abstinence.1 Nabiximols is administered as a buccal spray and is placed between the gums and cheek in the mouth for absorption.5 The first study was conducted on inpatients with cannabis dependence, who reported having fewer symptoms of withdrawal when treated with nabiximols.7 Furthermore, when tested in outpatients, patients taking nabiximols exhibited fewer withdrawal symptoms when abstaining from smoking cannabis.8,9 In fact, this study was conducted at the Centre of Addiction and Mental Health (CAMH) in Toronto, under the supervision of the Institute of Medical Science’s very own, Dr. Bernard Le Foll, a psychiatrist at the Addiction Division at the CAMH. Moreover, in another study at CAMH, Dr. Le Foll and his group have demonstrated that cannabis use was reduced by 70% in those treated with nabiximols. Use of nabiximols was well tolerated and did not lead to greater incidence of abuse.9,10\nYet, despite the evidence that synthetic cannabinoids have been shown their effectiveness in treatment of cannabis addictions, Canadian and American government officials remain hesitant about giving these drugs the official indication for treatment of cannabis addictions.5 It seems that much more effort is needed for agonist replacement medications to get approved on the market.5 With the recent legalization of cannabis for recreational use in Canada, cannabis use and the number of individuals developing CUD are expected to rise, with no drugs currently indicated for treatment, except for psychotherapy.2,9 It is also worthwhile to note that psychotherapy may not be accessible to those without health insurance.\nAlthough the use of synthetic THC and nabiximols may be controversial, the benefits of cannabis agonist replacement therapy demonstrated by its superior safety, efficacy, and potential for harm reduction far outreach the cost of fear by the medical community to use THC-containing compounds for treatment. Unfortunately, changing the opinion of the medical community and government officials to approve cannabis-derived treatments for CUD may require more time and effort.\n- Brezing CA, Levin FR. The Current State of Pharmacological Treatments for Cannabis Use Disorder and Withdrawal. Neuropsychopharmacology. 2018;43(1):173-94.\n- Bonner WIA, Andkhoie M, Thompson C, et al. Patterns and factors of problematic marijuana use in the Canadian population: Evidence from three cross-sectional surveys. Can J Public Health. 2017;108(2):e110-e6.\n- Canadian Centre on Substance Abuse. Canadian Drug Summary: Cannabis. 2018 June [10p]. Available from: http://ccsa.ca/Resource%20Library/CCSA-Canadian-Drug-Summary-Cannabis-2018-en.pdf\n- American Psychiatric Association. Diagnostic and statistical manual of mental disorders. 5th ed. Washington, DC; 2013.\n- Allsop DJ, Lintzeris N, Copeland J, et al. Cannabinoid replacement therapy (CRT): Nabiximols (Sativex) as a novel treatment for cannabis withdrawal. Clin Pharmacol Ther. 2015;97(6):571-4.\n- CBC Radio Quirks and Quarks. Doctors’ group wants to scrap Canada’s medical cannabis program [document on the Internet]. CBC Radio; 2018 April 30 [cited 2018 Oct 20]. Available from: https://www.cbc.ca/radio/quirks/scrap-medical-weed-women-in-space-and-more-1.4636793/doctors-group-wants-to-scrap-canada-s-medical-cannabis-program-1.4636810\n- Allshop DJ, Copeland J, LIntzeris N, et al. Nabiximols as an agonist replacement therapy during cannabis withdrawal: a randomized clinical trial. JAMA. 2014;71:281-291.\n- Trigo JM, Lagzdins D, Rehm J, et al. Effects of fixed or self-titrated dosages of Sativex on cannabis withdrawal and carvings. Drug Alcohol Depend. 2016;161:298-306.\n- Le Foll B. Oral communication 2018 Oct 17.\n- Trigo JM, Soliman A, Quality LC, et al. Nabiximols combined with motivational enhancement/cognitive behavioral therapy for the treatment of cannabis dependence: A pilot randomized clinical trial. PLoS ONE 13(1): e0190768. https://doi.org/10.1371/journal.pone.0190768']	['<urn:uuid:37c8480b-4d05-4cd8-84eb-93b06dd3af6a>']	factoid	with-premise	verbose-and-natural	distant-from-document	single-doc	novice	2025-05-12T21:31:42.699391	27	46	1161
59	bald eagle population decline ddt impact current wind turbine mortality numbers	The bald eagle population declined from 300,000-500,000 to just 412 nesting pairs in the 1950s due to DDT's egg shell-thinning effects. While the bald eagle has recovered and was removed from the threatened list in 2007, wind turbines are now expected to kill at least one million birds each year by 2030 when there will be more than 100,000 turbines in the U.S.	"['8 Endangered Species Making Epic Comebacks\nIt\'s easy to become disheartened when perusing the endangered species list. However, conservationists are making strides to keep these animals from disappearing forever. Their hard work, and dedication are making enormous progress each and every day.\n1. Bald eagle\nOne of countless victims of the egg shell-thinning effect of DDT, the bald eagle population declined from an estimated 300,000-500,000 to just 412 nesting pairs in the lower 48 states in the 1950\'s. The national bird of the U.S. was helped by the 1940 Bald Eagle Protection Act (which banned trapping and killing them), and then by the eventual ban of all uses of DDT, and the bald eagle was removed from the ""threatened"" list in 2007.\nThe American Bald Eagle Foundation and the state of Alaska have teamed up to create a 48,000 acre preserve to protect the world\'s largest population of Bald Eagles. You can read more about the Chilkat Bald Eagle Preserve and make donations\n2. Swift fox\nSwift foxes were decimated by 1930\'s predator control programs that were aimed at wiping out gray wolves and coyotes. Though populations are now stable and no longer listed as endangered in the United States, the species only inhabits 40% of its former range and are still endangered in Canada. Numerous state agencies have pooled their conservation efforts to form the Swift Fox Conservation Team with the aim of restoring their population to its former size.\nThe Endangered Wolf Center works with the Swift Fox Conservation Team as well as other groups collaborating to preserve and increase swift fox populations. You can read more about them and sponsor their efforts by ""adopting"" a swift fox here.\n3. Peregrine falcon\nLike many species, the peregrine falcon was moved to the endangered species list prior to the restriction of DDT, which weakened the calcium content of eggs and thus dramatically decreased the percentage of falcons that survived until they hatched.\nWith pesticides more closely regulated now and captive breeding operations enormously successful, the peregrine falcon was removed from the endangered species list in 1999. Captive bred falcons are fed via a chute or a peregrine hand puppet (so as to prevent them from imprinting on the human trainers). Over time, feeding is reduced and the falcons are forced to learn to hunt before being fully released into the wild.\nAs the world\'s largest member-supported raptor conservation organization, Hawk Mountain in Kempton, Penn., offers international conservation training and plays a large role in scientific research. You can join their organization by purchasing a membership here or volunteering at the preserve.\n4. American alligator\nThreatened by over-hunting and habitat loss, the United States Fishing and Wildlife Service has teamed up with state wildlife agencies to restrict and regulate legal commerce of alligator meat and products. Their efforts have resulted in the American alligator being removed from the endangered species list in 1987, and they are now listed in the ""Least Concern"" category, indicating a profound comeback.\nThe National Parks Conservation Association seeks to protect America\'s national parks, including ones with important American alligator habitats such as Biscayne National Park, Everglades National Park and others.\n5. Black-footed ferret\nThe black-footed ferret was thought extinct, their population ravaged by plague and human contact with their main food source, the prairie dog. Then a Wyoming woman\'s dog showed up with a dead black-footed ferret in its mouth in 1981, leading to the discovery of a few dozen.\nIn 1987, 18 black-footed ferrets were artificially inseminated with the hopes of reviving the species. Reintroduction began in 1991 and as of 2013, a total of 1200 is believed to live in the wild. Despite their monumental comeback, they are still listed as endangered and efforts to protect them are fought by ranchers seeking to control prairie dog populations on their land.\nThe Black-footed Ferret Recovery Implementation Team is tracking the progress of countless state and federal agencies, non-profits, universities, and conservation groups as they strive to bring back black-footed ferret populations. You can follow along with the ferret\'s recovery and learn more about how you can help here.\n6. California condor\nThe California condor was nearly wiped out by effects of the pesticide DDT, lead poisoning from feeding on carrion with bullet fragments inside, power line collisions, and human contact.\nIn 1987, the 22 remaining members of the species were captured and put into captive breeding operations. The newly bred population were trained to avoid humans and power lines, which aided in their survival. As of 2013, there are believed to be 435 California condors (237 in the wild and 198 in captivity). They are still critically endangered and extremely rare, but scientists have gathered evidence that they are breeding in the wild so there remains hope for the species, especially with groups like the Center for Biological Diversity spearheading efforts to protect this rare, slowly rebounding species.\n7. Florida panther\nBy the 1970\'s, the panther population had been reduced to an estimated 20 remaining in the wild. Conservation efforts have boosted the population to 160 as of 2013, but this species still faces countless challenges in its fight for survival.\nHuman encroachment has devastated the Florida panther\'s habitat. Each breeding unit (one male with anywhere from two to five females) needs a wide, 200 square mile expanse to establish their territory. So, our population of 160 panthers needs 32,000 square miles. We have made available an estimated 3,800 square miles for them.\nAdditionally, vehicular collisions, Feline Immunodeficiency Virus, inbreeding, and contact with chemicals that feminize male panthers and decrease their likelihood of reproducing are also contributing to this species\' struggle.\nConservation efforts have centered around maintaining their habitat in the face of a rapidly developing south Florida real estate market. Numerous conservation groups have proposed the Florida Panther Protection Program to establish a large, contiguous habitat for the species.\n8. Przewalski\'s horse\nTheir population of this never-domesticated horse declined as its habitat was lost or degraded due to human interference. The last sighting of Przewalski\'s horse in the wild was in the Gobi Desert of Mongolia in 1969.\nThe Przewalski\'s horse has endured a long road to recovery, rebounding from a captive population of only 15 in the 1970s to more than 400 in the wild today (with another 1,500 in captivity). Reintroduction programs in China, Khazakstan, and Mongolia are proving fruitful so far, but genetic diversity and inbreeding remain a serious concern.\nOverseeing efforts to restore this wild horse to the fields it once roamed is the Conservation Centers for Species Survival. Their successful conservation efforts -- and the never-ending works of others -- prove that it\'s never too late to save a population from the brink of extinction.', 'Wind Energy Frequently Asked Questions |\nWhat Is American Bird Conservancy’s (ABC) policy regarding wind energy?\nWind power can be an important part of the solution to global warming, but wind farms can also kill birds—including eagles, songbirds, and endangered species—through collisions with turbines, and also harm them through loss of habitat. By 2030, there will likely be more than 100,000 wind turbines in the U.S., and these are expected to kill at least one million birds each year—probably significantly more. Wind farms are also expected to impact almost 20,000 square miles of terrestrial habitat, and over 4,000 square miles of marine habitat by 2030, some of it critical to threatened species.\nTo be a truly green source of energy, wind power needs to be Bird-smart and that means wind power employs careful siting, operation and construction mitigation, monitoring, and compensation to reduce and redress any unavoidable bird mortality and habitat loss from wind energy development. These are issues that should be included in mandatory federal wind standards. All wind farms should employ bird-smart principles and comply with relevant state and federal wildlife protection laws.\nWhat is bird-smart wind energy?\nBird-smart wind power implements careful siting considerations, operation and construction mitigation, bird monitoring, and compensation to reduce and redress unavoidable bird mortality and habitat loss. These are issues that the federal government should include in mandatory wind standards. For terrestrial wind farms, bird-smart wind should address:\n- Siting: Bird-smart wind power (including wind farms and associated infrastructure) is sited to prevent harm to birds, ideally in already altered habitats such as farmland, and avoids sensitive areas. Examples of such areas may include migratory bottlenecks, wetlands, raptor concentration and key nesting areas, the edges of ridges used by migrants, key habitat or flight paths for endangered or declining species, breeding concentrations of species that avoid tall structures (such as some grouse species), and in or adjacent to Important Bird Areas. Maps with detailed data on wildlife are currently being developed by conservation groups for use by the wind industry. Pre-construction assessments should always be conducted to confirm whether a particular site presents an especially high risk to birds. Some areas are not going to be suitable for wind development.\n- Operation and Construction Mitigation: Bird-smart wind power uses the best technology and management practices to avoid and minimize harm to birds, such as by burying transmission lines in high risk areas, following Avian Power Line Interaction Committee standards for above-ground transmission lines, using lighting that minimizes nighttime migratory bird collision mortality (such as strobe lights), using unguyed rather than guyed meteorological towers, and restoring habitat disturbed by construction, e.g., re-compacting soils disturbed by construction and replanting native vegetation (or restoring the site if the wind farm is decommissioned).\n- Monitoring: Bird-smart wind power conducts effective, federally reviewed and approved, site-specific, pre- and post-construction studies/assessments to assist with improved siting and operation, and to properly quantify impacts. Pre-construction assessments must provide sufficient data to assist with micro-siting (e.g., by use of radar to detect local bird movements), create an annual baseline against which post-construction studies can be evaluated, use all existing available bird study data, and be conducted during months when bird use can be expected to be at its peak at the selected site. Post-construction studies must employ mathematical models that best account for variations in local conditions and the relative difficulty of locating bird carcasses in different habitats, as well as any scavenging by predators that may reduce the number of carcasses found, and run for at least two years (and long enough to determine the efficacy of, and make needed revisions to, operational mitigation measures).\n- Compensation: Bird-smart wind power redresses the loss of any birds or habitat unavoidably harmed by construction and operation to a net benefit standard. This includes bird deaths caused by collisions with turbines and their associated power lines, and lost or degraded habitat (e.g., areas of abandoned habitat) Such compensation could include acquiring additional land for the National Wildlife Refuge system or other off-site habitat conservation projects.\nAlthough offshore wind power is not yet operational in the U.S., an analogous set of siting, operating, and compensatory measures need to be developed to make it bird-smart.\nAll wind farms should have an Avian Protection Plan which includes American Bird Conservancy’s bird-smart principles, and a means of implementing them and tracking and reporting on this implementation. Wind farms should also comply with relevant state and federal wildlife protection laws such as the Endangered Species Act, Migratory Bird Treaty Act, Bald and Golden Eagle Protection Act, and National Environmental Policy Act.\nHow would you know if a potential wind farm site will have large or small impacts to birds?\nSee section above on pre-construction monitoring and post-construction monitoring.\nIsn’t the Federal Government developing wind energy regulations?\nNo, the Federal Government has released draft, voluntary wind-energy guidelines that were produced from recommendations to the Secretary of the Interior by a Federal Advisory Committee. During the public comment period on those proposed voluntary guidelines, ABC will urge that that the Department of the Interior enact mandatory standards that the industry must follow. We do not believe that energy industries should be able to choose whether or not to consider bird impacts.\nA recent lawsuit regarding the Altamont Pass Wind Farm in California illustrates that getting the right thing done voluntarily is anything but a sure thing. Recent studies say that approximately 7,600-9,300 birds were killed there each year, including 55-94 Golden Eagles, 475-477 American Kestrels, 253-433 Red-tailed Hawks, and 714-718 Burrowing Owls. After seven years of being challenged by various conservation groups, it finally took a lawsuit to get changes made at Altamont Pass. NextEra Energy Resources has only recently agreed to replace 2,400 of its old wind turbines at Altamont Pass with fewer, larger models that produce the same amount of total power. They also agreed to place the new turbines in “more environmentally friendly” locations, and to pay $2.5 million for research and raptor habitat improvement.\nWhat kinds of birds are impacted by wind farms?\nPotentially all night-migrating songbirds are at risk of colliding with wind turbines, as are raptors and other birds when wind farms are sited in areas they frequent. Habitat loss is also an issue as wind farms can degrade bird habitat or cause birds to abandon habitat.\nGolden Eagles will be especially impacted because much of the additional wind build-out planned for the western U.S. is expected to occur in areas they inhabit. The endangered Whooping Crane will be exposed to additional risk from collision with new power lines erected to service wind farms along their migratory pathways.\nTo a Greater Sage-Grouse and some other birds, any tall structure such as a wind turbine is a threat because it is a potential perch for a predatory bird. A wind turbine standing a considerable distance away has much the same effect as a small tree at a few hundred yards, causing sage-grouse to abandon traditional lekking grounds up to three miles away from a wind farm. Unfortunately for the Sage-Grouse, Wyoming, one of this bird’s last remaining strongholds, is slated for significant wind farm build-out. It is very important for the future of the Greater Sage-Grouse that this development is appropriately sited\nHow many birds are killed by wind farms each year?\nNo one knows for sure. Recent estimates of the number of birds killed by wind turbines ranges from a low of 100,000 birds/year to 440,000 birds/year (calculated by the U.S. Fish and Wildlife Service). If 20% of the nation’s electricity comes from wind power by 2030, ABC estimates that at least one million birds per year will be killed by wind turbines, probably significantly more.\nAren’t there laws that protect birds from being killed by wind farms?\nWhooping Cranes and some other birds are protected under the Endangered Species Act. Golden Eagles are protected under the Bald and Golden Eagle Protection Act (BAGEPA), and most migratory birds are protected by the Migratory Bird Treaty Act (MBTA). Unfortunately, both the BAGEPA and MBTA are not currently sufficiently enforced to prevent predicted mortality resulting from wind development. However, in 2011, the U.S. Fish and Wildlife Service is expected to begin issuing permits for wind farms that expect to kill Golden Eagles, contingent on the wind farms taking tangible steps to protect eagles. The Greater Sage-Grouse, meanwhile, currently receives no federal legal protection, though several states have stepped up to protect remaining core breeding areas.\nRead full ABC Policy Statement on Wind Energy']"	['<urn:uuid:5f764e04-aa23-4d77-9fe8-50bcd3733d02>', '<urn:uuid:ad348f5b-9b9c-419d-9045-9be2eb6e06f6>']	factoid	direct	long-search-query	distant-from-document	multi-aspect	expert	2025-05-12T21:31:42.699391	11	63	2535
60	How do the electoral outcomes compare between minor political parties and the two major parties in U.S. presidential elections?	The two major parties (Democrats and Republicans) have completely dominated presidential elections, winning every single one since 1852. In contrast, third parties have achieved limited success, with one of the best results being Ross Perot winning 19 percent of the vote as a third-party candidate, which was the best result for any third party candidate in 80 years.	"['Watching this resources will notify you when proposed changes or new versions are created so you can keep track of improvements that have been made.\nFavoriting this resource allows you to save it in the “My Resources” tab of your account. There, you can easily access this resource later when you’re ready to customize it or assign it to your students.\nA two-party system is a system in which two major political parties dominate voting in nearly all elections at every level of government and, as a result, the majority of elected offices are members of one of the two major parties.\nA two-party system is a system in which two major political parties dominate voting in nearly all elections at every level of government and the majority of elected offices are members of one of the two major parties.\nUnder a two-party system, one party typically holds a majority in the legislature and is usually referred to as the majority party while the other is the minority party.\nThe United States is an example of a two-party system in which the majority of elected officials are either Democrats or Republicans .\nThe modern political party system in the U.S. is a two-party system dominated by the Democratic Party and the Republican Party.\nThese two parties have won every United States presidential election since 1852 and have controlled the United States Congress to some extent since at least 1856.\nHowever, the political party system did not develop until tax reform.\nThe First Party System of the United States featured the Federalist Party and the Democratic-Republican Party (Anti-Federalist).\nIn 1829, the Second Party System saw a split of the Democratic-Republican Party into the Jacksonian Democrats, who grew into the modern Democratic Party, led by Andrew Jackson, and the Whig Party, led by Henry Clay.\nThe Third Party System stretched from 1854 to the mid-1890s, and was characterized by the emergence of the anti-slavery Republican Party, which adopted many of the economic policies of the Whigs, such as national banks, railroads, high tariffs, homesteads and aid to land grant colleges.\nThe Fourth Party System, 1896 to 1932, retained the same primary parties as the Third Party System, but saw major shifts in the central issues of debate.\nThis period also corresponded to the Progressive Era, and was dominated by the Republican Party.\nThe Fifth Party System emerged with the New Deal Coalition beginning in 1933.\nThe Republicans began losing support after the Great Depression, giving rise to Democratic President Franklin D. Roosevelt and the activist New Deal.\nExperts debate whether this era ended in the mid-1960s when the New Deal coalition did, the early 1980s when the Moral Majority and the Reagan coalition were formed, the mid-1990s during the Republican Revolution, or continues to the present.\nSince the 1930s, the Democrats positioned themselves more towards Liberalism while the Conservatives increasingly dominated the GOP.\nThere are several reason two major parties often dominate the political landscape in some systems.\nIn the U.S., forty-eight states have a standard winner-takes-allelectoral system for amassing presidential votes in the Electoral College system.\nThe winner–takes–all principle applies in presidential elections, thus if a presidential candidate gets the most votes in any particular state, all of the electoral votes from that state are awarded to the candidate.\nIn all but Maine and Nebraska, the presidential candidate must win a plurality of votes to wins all of the electoral votes; this practice is called the unit rule.\nThere are two main reasons winner–takes–all systems lead to a two-party system.\nFirst, the weaker parties are pressured to form an alliance, sometimes called a fusion, attempting to become big enough to challenge a large dominant party and, in so doing, gain political clout in the legislature.\nSecond, voters learn, over time, not to vote for candidates outside of one of the two large parties since their votes for third party candidates are usually ineffectual.\nTherefore, weaker parties are eliminated by the voters over time.\nThe gravitation of voters towards one of the two main parties is called polarization.\nAdvantages and Disadvantages\nOne opinion in political science is that a two-party system promotes centrism, less extremism, and that a two-party system is generally more stable and easier to govern than multi-party systems which can become a hung parliament. However, two-party systems have been criticized for ignoring alternative views and putting a damper on debate within a nation.\nMulti-party governments tend to permit wider and more diverse viewpoints in government and encourage dominant parties to make deals with weaker parties to form winning coalitions.\nCompared to the United States\' two-party system, the most common form of democracy is the British multi-party model.\nThere have been arguments that the winner-take-all mechanism discourages independent or third-party candidates from running for office or promulgating their views.\nOne analyst suggested that parliamentary systems, which typically are multi-party in nature, lead to a better ""centralization of policy expertise"" in government.\nMulti-party governments permit wider and more diverse viewpoints in government, and encourage dominant parties to make deals with weaker parties to form winning coalitions.\nWhile there is considerable debate about the relative merits of a constitutional arrangement such as that of the United States versus a parliamentary arrangement such as Britain, analysts have noted that most democracies around the world have chosen the British multi-party model.\nThe existence of political parties is mandatory as outlined in the Constitution, The existence of two parties has always been a part of the American political system, Democrats and Republicans were the first major political parties in the United States, and U.S. election rules support the creation and maintenance of a two-party system\nIt stamps out any alternative point of views in democratic debate, It leads to a centralization of decision-making in government, It promotes moderate perspectives that reflect the wider attitudes of the electorate, and It constrains the development of alternative political parties\nGeorge Washington left presidential office., male suffrage expanded during the tenure of President Andrew Jackson., powerful political machines came into prominence at the turn of the twentieth century., and right before the Civil War.\nThe United States has proportional representation rather than single-member districts, Americans’ political views are not very diverse., The founders of the Constitution were suspicious of political factions and thus established a lasting preference for few parties, The parties take extreme ideological positions to become prominent in government., and Americans have been historically averse to radical politics and desire to maintain a stable democratic political system.\nThe existence of political parties is mandatory as outlined in the Constitution., The existence of two parties has always been a part of the American political system., Democrats and Republicans were the first major political parties in the United States., The two-party system was in place by 1800, and U.S. election rules support the creation and maintenance of a two-party system.\nSource: Boundless. “The Two Party System.” Boundless Political Science. Boundless, 14 Nov. 2014. Retrieved 25 Mar. 2015 from https://www.boundless.com/political-science/textbooks/boundless-political-science-textbook/interest-groups-7/the-two-party-system-54/the-two-party-system-313-5192/', 'Third Parties Procedure To preview the lesson, students will watch a montage of photographs of Theodore Roosevelt while listening to his The Right of the People to Rule speech. Next, we will review the definition of a third party.\nFirst party to hold a convention to nominate candidates Prohibition Party Has nominated a candidate for president in every election since Progressive Party Elected a number of candidates to state legislatures, Congress, and even the U. Green Party Some members have won local elections. People who feel strongly about a particular issue might be drawn to a third party that focuses exclusively on that issue.\nThe Greenback Party focused on the monetary system, and the Prohibition Party sought to ban the consumption of alcohol. The Populist Party, meanwhile, grew out of the Populist movement, and the Republican Party developed primarily out of the abolitionist movement.\nDissatisfaction with the status quo: Some third parties form when part of a major party breaks off in protest and forms a splinter party. InTheodore Roosevelt led a group of dissidents out of the Republican Party to form the splinter Progressive Party. Third parties can be closely tied to a specific region, which can increase their appeal.\nIf the leader leaves the party, however, the party often collapses, which is what happened to the Reform Party in the mids.\nInthe party split in two over the candidacy of former Republican Pat Buchanan. Neither Buchanan nor his Reform Party rival gained many votes, and the party has largely disappeared from the national stage. The Role of Third Parties Despite their lack of success in the polls, third parties can affect American politics in a number of ways: Third parties propose many government policies and practices.\nThe Populist Party introduced ideas that influenced some economic policies of the New Deal, whereas the Anti-Masonic Party was the first party to use a convention to nominate its candidates, in the mid-nineteenth century.\nPut issues on the agenda: Third parties can force the major parties to address potentially divisive problems. Inneither Bill Clinton nor George H. Bush talked much about the budget deficit until independent candidate Ross Perot emphasized it in his campaign. Third parties can cost one party an election by playing the spoiler.\nIf a third party draws enough votes away from a major party, it can prevent that party from winning. It is impossible to know for sure what would have happened had the third-party candidate not run, but in some cases, it seems that the third party probably cost one candidate the election.\nKeep the major parties honest: A leftist party can challenge the Democratic Party, for example, on social justice issues, whereas a conservative party can pose problems for the Republican Party.\nBecause third-party candidates usually have little chance of winning, they can speak more frankly than their major party rivals, addressing facts and issues that the major parties would often prefer to ignore.Minor political parties can play an important role in a county\'s politics by virtue of their ability to influence the controlling party to adopt some of their platforms.\nBecause they are not guaranteed a good chance of winning a major election and becoming a controlling party, minor political.\nThird party is a term used in the United States for American political parties other than the Republican and Democratic parties. Contents 1 Current U.S. third parties.\nA Brief Look at the History of Third Parties in America by Gary Odom, political activist and former National Field Director for the Constitution Party Most Americans have been led to believe that that the United States has a two-party political system. Third Parties in American Politics The electoral system in the United States works against a proliferation of political parties.\nThis fact has not prevented minor parties or independents from running for office at the local, state, and national levels, however.\nPerot\'s national organization, known as ""United We Stand America"" succeeded in getting Perot on the ballot in all 50 states. Perot won 19 percent of the vote in November, the best result for a third party candidate in 80 years.\nFollowing the election, Perot and ""United We Stand America"" organized into the Reform Party. Third party is a term used in the United States for American political parties other than the Republican and Democratic parties.']"	['<urn:uuid:eed57f4c-298d-4dac-a025-a93d2a8d0dd5>', '<urn:uuid:4cb3f8ea-d7ce-40d2-8541-82e37ba3c5bf>']	factoid	direct	verbose-and-natural	similar-to-document	comparison	novice	2025-05-12T21:31:42.699391	19	58	1879
61	I'm curious about fighting styles - how does underwater sword fighting compare to archery techniques?	In underwater combat, sword fighting is heavily restricted - most melee weapons are used at disadvantage unless they're piercing weapons like daggers, javelins, shortswords, spears, or tridents. This is due to water resistance making it difficult to swing slashing or bludgeoning weapons. In contrast, 3D archery techniques focus on distance estimation and precision shooting at specific scoring rings on life-sized foam animal targets, with different shooting positions based on skill level and equipment class. Success in archery depends more on accurately judging distance rather than dealing with environmental resistance.	['While individual combat encounters in D&D 5e are dynamic and ever-changing, the rules that govern these encounters thankfully don’t change much. One of the few scenarios where some additional rules come into play are underwater combat encounters.\nDepending on the campaign or adventure module you’re playing, you may never find yourself fighting throngs of enemies below sea level. On the other hand, it’s not outside of the realm of possibility for an underwater combat encounter to come into play. Even if you hadn’t intended there to be underwater combat when planning the encounter.\nRegardless, thankfully the underwater combat rules in 5th edition are pretty easy to remember. The majority of the rules revolve around limiting the effectiveness of certain types of weapons and spells. Realistically, this makes sense. For example, it’s more difficult to swing a sword or shoot a bow underwater.\nFirst of all, moving underwater is difficult terrain. While this isn’t technically tied into the underwater combat rules, it’s a significant point to remember when running an underwater encounter.\nDifficult terrain just means that characters and creatures move at half speed unless otherwise noted. For example, difficult terrain as far as swimming underwater is concerned can be ignored if your character or creature has any amount of swim speed.\nYou could gain swim speed as a racial trait from races like tritons or sea elves do. Some classes and archetypes may also grant swimming speed such as the Path of the Storm Herald Barbarian. Another, more accessible option, is to obtain a magical item that grants the user swim speed.\nUnderstandably, not all weapons are able to work effectively underwater. The list of what weapons do work is pretty small so at least it’s easy to remember what will and will not be detrimental to use underwater.\nIf you’re using aquatic creatures like merrow as the DM you won’t have to worry about weapon disadvantages. I’ve found that most of the aquatic creatures use weaponry that isn’t hindered by being underwater.\nMelee Weapon Restrictions\nMelee weapon attacks are made at disadvantage by creatures underwater. However, there are a few exceptions to this rule. If you are using a dagger, javelin, shortsword, spear, or trident you can make a regular weapon attack.\nNotice that all of these are piercing weapons, weapons that are thrust straightforward when attacking. This makes practical sense too, there’s less resistance from the water when thrusting as opposed to swinging a slashing or bludgeoning weapon.\nHaving swim speed is a huge boon to any melee attackers. This isn’t just because of the increased speed and maneuverability, though. It’s also because having a swimming speed means that you can use any melee weapon without having disadvantage on the attack roll.\nI’m assuming that the logic behind this is that you’re clearly an expert at swimming so you’ll know how to swing a sword with enough power and accuracy to reliably deal damage.\nTo remember this simply determine if the melee weapon is a piercing weapon you thrust forward with. If it isn’t then you have disadvantage on the attack unless you have any swim speed.\nRanged Weapon Restrictions\nAs you might imagine, shooting a longbow is terribly difficult underwater. Not only that but even if you managed to pull the bowstring back the resistance from the water would slow the arrow making it extremely inaccurate.\nRanged attacks are made at disadvantage unless the weapon is a crossbow, net, or a weapon that is thrown like a javelin such as a spear, trident or dart. Essentially, ranged weapons that are manually drawn like bows or thrown in a swinging motion like a handaxe will make their attacks at disadvantage.\nIn addition to this, any ranged weapon attack must be within the weapon’s normal range otherwise it automatically misses. This means that a heavy crossbow can only be shot within 100 ft. underwater even though it has a maximum range of 400 ft.\nFirearms and explosives typically don’t work underwater, though that may depend more on your setting than anything. Most of the firearms I implement in my games use powder to fire. It’s hard to do that when the powder is wet!\nThe only real benefit to underwater combat in 5e is that everyone who is fully submerged in water has resistance to fire damage. Even then, it’s not much of a benefit for the players from my experience.\nFigure it this way, most creatures that the party will be fighting underwater are probably aquatic or have spent a lot of time in or around the water. Their statblocks, abilities, and spells generally don’t include fire damage.\nFire damage is found in some signature spells like fire bolt, burning hands, and fireball which most offensive wizards or sorcerers have at least one of. Reducing the effectiveness of these lessens the party’s damage output.\nTypically these mechanics breakdown articles are a lot lengthier, but underwater combat isn’t very challenging to interpret. I’d go as far to say that the vast majority of the rules for underwater combat can be assumed by thinking logically about the situation.\n- Ranged weapon attacks are at disadvantage\n- Unless the weapon is a crossbow, a net, or a weapon that is thrown like a javelin\n- Ranged weapons automatically miss if they are attacking a target outside of the weapon’s normal range\n- Melee weapon attacks are at disadvantage\n- Unless the weapon is a dagger, javelin, shortsword, spear, or trident\n- Unless the attacker has swim speed\n- All creatures submerged in water have fire resistance\nUnderwater combat is restrictive, but it’s certainly not detrimental, even to creatures without swim speed. It opens up some unique maneuvers as it’s also three-dimensional combat. Enemies and allies can be above and below you at all times now which is not typically the case when fighting on flat land.\nIf you enjoyed what you read be sure to check out my ongoing review for all of the official D&D 5e books!\nSign up to get e-mail updates for new articles on Dungeon Solvers using the form below!', 'RESULTS: Thanks to all who came out this weekend! Below are the results, target statistics, and pictures. We will work on processing all awards this week and have them mailed out to you ASAP. I wanted to also thank the 22 volunteers that helped put this event...\nPlease visit SCHEDULED EVENTS to see the clubs offerings!\nThe sport of 3D Archery is a form of Target Archery where archers shoot at life-sized foam animals. Here at the club we offer 3D archery both indoors and out. In January we host an indoor 3D league with 15-19 animals since the weather outside rarely cooperates. From March through August we offer 7-10 Outdoor shoots with 30 animals.What exactly is it?\nCan I do it?\nUnlike other courses in the area 3D Archery can be participated by anyone at any skill level. 3D Archery can be set in the woods, fields and sometime even indoor ranges. The goal of 3D Archery is having the ability to guess how far away the target is and know where to shoot to achieve the highest possible score.\n3D Archery is compared to golf… but with a bow\nIn Golf you generally need to know of far the hole is to choose the right club. In 3D you have to guess the distance and shoot to get the highest score. This means you stand at a designated stake in the ground and try to guess how far the target is. You then take your shot trying to score the highest possible points. As in golf, if you guess the distance wrong you will probably not score as well. In golf the lowest score wins, in 3D the highest score wins.\nTarget Scoring in 3D Archery\nEach 3D Target will have a set of scoring rings on it. New Castle 100 Archers using the IBO scoring method (except during select money shoots). The smallest circle (Usually located at the very center of the heart of the animal scores 11 points, the next ring is the heart, 10 points, the lungs and other vitals are 8 points, the body is 5 points..\nArchery Skill Levels Required\n3D Archery has many classes for different skill levels and equipment. Even beginners and children can have a lot of fun in this sport. The Archery equipment you use, is the deciding factor on how far away your targets will be. An example for bowhunter class it 35 yards and under.\nWhich 3D Archery Class do I belong in?\nHowever, there are some general classes. The closest stake to the target is for the Youth and traditional archers stake (16 and under) set between 15 & 25 yards, followed by the bowhunter stake set between 25 and 40 yards, with the open class shooting at the metal numbered stake up to 52 yards away. Indoor is a totally different story. Our indoor range only accomdates a 35 yard shot at best. It is made more difficult with very close shots, and very tight shots created by obstacles. We have a set time for adults to shoot and another for youths. Each group of animals is set up accordingly.\nWhat about guessing yardage in 3D Archery\nGuessing the distance is probably the number one skill in 3D Archery. In unmarked 3D Archery, you must guess how far the target is in order to make a high scoring shot.']	['<urn:uuid:d5bf1094-9867-496a-98f4-08b4c83aab16>', '<urn:uuid:a86f14c3-983d-4d19-8d22-ae4517977e7e>']	open-ended	with-premise	concise-and-natural	distant-from-document	comparison	novice	2025-05-12T21:31:42.699391	15	89	1577
62	How did Greek philosophy get translated into Arabic?	Greek philosophy was translated into Arabic primarily by Syriac-speaking scholars who were mostly Nestorian and Jacobite Christians, as well as Sabians from Harran. While some translations occurred during the Umayyad caliphate, most were completed during the Abbasid caliphate over approximately two centuries. The most famous translator was Hunayn Ibn Ishaq and his team, who maintained high scholarly standards. Most translations were made from Syriac, though some came directly from Greek.	"[""When preparing myself to write about medieval Islamic philosophy and the classical tradition, I felt a touch of anxiety—a fear that I would be belaboring the obvious.1 For medieval Islamic philosophy, as we know it, was a direct result of the translations of Greek philosophy and science to Arabic. It is rooted in Greek philosophy. I also felt some discomfort with the term medieval, a chronological and cultural term commonly applied to European history. To what extent and in what sense can it apply to Islamic history? This is a question which should at least be asked, even though it is beyond my scope to go into it here. For my purposes, when writing about the medieval Islamic philosophers I will confine myself to those philosophers who flourished between the 9th and 12th centuries. They include such major figures as Kindi (al-Kindi) (d. ca. 870), Alfarabi (al-Farabi) (d. 950), Avicenna (Ibn Sina) (d. 1037) and Averroes (Ibn Rushd) (d. 1198). These are also the main philosophers who influenced the development of Jewish and medieval Latin scholastic thought.\nAt this point, however, we should remind ourselves that creative Islamic philosophy did not cease in the centuries that followed the death of Averroes. The world of Islam saw a multitude of post-Averroes philosophers. These included some of the most original Islamic thinkers as, for example, the Tunisian philosopher of history Ibn Khaldun who died in 1406 and the great Persian metaphysician Mulla Sadra who died in 1641. Mulla Sadra represents a rich tradition of post-Avicennan Iranian philosophy that continues unabated to the present day. This is a philosophy that has its own native genius and originality. Nonetheless, it remains rooted in what we have termed medieval Islamic philosophy that in turn rests on the Arabic translations of Greek thought.\nThe translators of Greek philosophy and science to Arabic were Syriac speaking scholars, mostly Nestorian and Jacobite Christians, but also Sabians from the city of Harran whose religion which included star worship had a Greek philosophical base. Some translations were made during the Umayyad caliphate (661-750), but most of them were undertaken, within approximately a period of two centuries, during the caliphate of the Abbasids who came to power in 750. There were phases in the history of these translations: translations were revised, new translations of works already translated and of works hitherto untranslated were made. The achievement of these translators was quite remarkable, not only for the volume of philosophical and scientific works translated, but also, as in the case of the most famous of the translators, Hunayn Ibn Ishaq (d. 871) and his team of translators, for the high standards of scholarship they strove to maintain. Most of the translations were made from the Syriac, but sometimes directly from the Greek. These translators, many of them physicians, scientists and philosophers in their own right, were the teachers of the Islamic philosophers, whether directly as in the case of Alfarabi, or through their translations and treatises. More than anything else, It was their translations of Platonic, Aristotelian and Neoplatonic works that gave rise to Islamic philosophy.\nPlato's thought seems for the most part to have been known indirectly, although we are in the realm of uncertainty here. Medieval Arabic histories mention such works as The Republic, The Laws, the Parmenides, the Timaeus, other dialogues and even The Letters as having been translated. These, however, seem to have been translations of paraphrases and summaries, Galen's Synopsis of the Platonic Dialogues being a most likely major source. Alfarabi gives a summary of the Laws, but without Bk.10 and in his Philosophy of Plato, he gives a summary of the Dialogues. Averroes wrote a middle commentary on Plato's Republic, that is, a lengthy paraphrase, not a commentary that reproduces the original text and comments on it section by section. It is not entirely clear, however, whether Averroes based his commentary on an Arabic translation of Plato's original text or only on a translation of a prosaic summary that concentrates on the substance of some of its main political themes. Plato's theory of ideas was well known to the Islamic philosophers, partly through Aristotle's account and criticism of it. Most of the medieval Islamic philosophers accepted Aristotle's interpretation of the theory and some, like Avicenna, gave parallel arguments in refuting it. The philosopher al-Suhrawardi (d. 1191) was a notable exception: he defended it and adapted it to his philosophy. The ideas of the Timaeus left their impact on the cosmology of the physician-philosopher Razes (al-Razi) (d. ca. 925) who argued for the world's creation at a finite moment in time, but not out of nothing. Creation for him was the imposing of order on pre-existing atoms. The main influence of Plato on Islamic philosophy, however, was on the development of medieval Islamic political philosophy.\nAristotle's thought was known more directly. Most of Aristotle's major works were translated, excepting the Politics and Bk.K of the Metaphysics. The earliest Arabic version of the Metaphysics was made probably in the first half of the 9th century by a certain Astat (Eustathius), about whom little is known. Translations of Aristotle's Organon together with elements of Stoic logic formed the basis of Arabic logic. In this connection, it should be remarked that Aristotle's very influential Posterior Analytics was a relatively late comer to the Islamic world. A partial Syriac translation first made by Hunayn Ibn Ishaq was followed by a complete translation by his son Ishaq, which in turn was translated into Arabic by Abu Bishr Ibn Matta (d.940). Aristotle's works embodied his scientific writings which together with translations of Greek medical, astronomical and mathematical works formed the basis of Islamic science. In addition to the works of Aristotle, a body of Greek commentary on Aristotle was also translated.\nNeoplatonism, which had its impact on such main stream Islamic philosophers as Alfarabi and Avicenna, also influenced sectarian philosophies, notably Ismaili philosophy. Neoplatonic thought was known mainly through two works. The first was the apocryphal The Theology of Aristotle, a paraphrase of Books IV, V and VI of Plotinus' Enneads, translated in the 9th century for Kindi by Ibn Naima al-Himsi. The second was a work that seems to have been derived largely from Proclus' Elements of Theology, known in Arabic as Fi Mahd al-Khayr, On the Pure Good, and in its Latin translation as the Liber de Causis.\nAs already suggested, it was these translations, philosophical and scientific, that gave rise to both medieval Islamic philosophy and Islamic science. But they also had a determining effect on the development of other Islamic disciplines. This is particularly true of Islamic dialectical theology,2 defend and explain revelation. It differed from Islamic philosophy, falsafa, kalam. Kalam used reason—excessively according to some of its critics—to in that its starting-point was an implicit acceptance of the revealed word. The starting-point and concern of falsafa, on the other hand, is perhaps best entailed by a definition of philosophy enunciated, as we shall see, by some of the Islamic philosophers, namely, as knowledge of things in their true nature to the extent of man's capability. True enough, the theologians used premises that seemed independent of revelation, but as Maimonides (d. 1204) in his adverse criticism of the kalam noted, each school of the kalam (and there were many schools) chose those premises that suited their doctrine.3 Nonetheless, one must not underestimate the Islamic theologians' acumen and perceptiveness. The theories they developed, though in the service of religious doctrine, did not lack originality, intense probing, subtlety, and the systematic building of a world view.\nKalam also differed from falsafa in its historical origins. It had as its background seventh century religious sectarian concerns. It preceded the translation movement proper. But with time it became influenced by it, changing and adapting to itself Greek philosophical ideas, as for example, atomism. It interacted with falsafa and came into conflict with it. But it was also influenced by it. It reinterpreted philosophical theories, adapting them to its own theological views. The interaction between kalam and falsafa is at the heart of medieval Islamic intellectual history. But this is a topic all unto itself, beyond the scope of this lecture.\nIn what follows our concern will be with Islamic philosophy proper, with falsafa, more specifically with certain metaphysical outlooks engendered by the translation of Greek philosophical works. We will be confining ourselves to the thought of three representative medieval Islamic philosophers.\nWas this article helpful?""]"	['<urn:uuid:d729c4ef-c34d-4957-8645-9779aeaa046c>']	open-ended	direct	concise-and-natural	similar-to-document	single-doc	expert	2025-05-12T21:31:42.699391	8	70	1402
63	age compare carraigin warwick castle	Carraigin Castle was built in the 13th century, while Warwick Castle was built earlier, in 1068 by William the Conqueror.	"[""13th Century Lakeside Castle in 7 Acres of Park and Woodland\nThe owner has been emailed.\nThis owner doesn't offer online booking through VRBO's checkout. You won't be eligible for our Book with Confidence Guarantee.\nNever use instant money transfer services such as Western Union and MoneyGram. Learn more\n- Minimum stay:Contact owner\n- Property type:Castle\n- Pets considered:Yes\n- Wheel chair accessible:No\nSurrounded by seven acres of lawns, park and woodland, Carraigin Castle is an idyllic holiday home in a beautiful setting on the shores of Lough Corrib, one of Ireland’s biggest lakes, famous for its brown trout and its multitude of picturesque islands. From the Castle one can enjoy boating and fishing on the lake, walking, riding and sightseeing all over Galway and Mayo, or just relax by the open hearth and contemplate the charm and simple grandeur of this ancient dwelling, a rare and beautiful example of a fortified, medieval “hall house”.\nFamily groups or close friends will love the relaxed atmosphere of this authentic 13th-century manor house, which has been restored by the present owner after languishing for more than two centuries as a crumbling, roofless ruin. Carraigin's church-like structure sits on a rise reached by an avenue across the tree-lined Pleasure Ground. The ancient-looking, nail-studded front door on the ground floor, often mistaken for an authentic antiquity, was actually made by the owner during the building's restoration in the 1970s. Round the corner, an imposing stone staircase leads to another grand entrance, into the lofty, oak-beamed Great Hall featuring a wide, stone-arched fireplace that provides a comforting aroma of turf and wood-smoke.\nThe Great Hall is the central living and dining area of the castle. It features a mix of old oak and comfortable modern furniture surrounding the welcoming hearth. Its white walls are extensively decorated with art including tapestries, brass rubbing portraits of ancient kings and knights and a magnificent triptych featuring a Galway galleon (as on that city’s coat of arms). There is a tiny but well-equipped kitchen next door with a view over the tall trees of the Pleasure Ground.\nOn the same level as the Hall is an oak-beamed double bedroom with a king-size bed and bathroom. A stone staircase winds upwards over this master bedroom to a family loft room overlooking the Great Hall. Another winding stairs leads up to a little single bedroom in the corner tower. From both of these second-floor rooms you can stroll out onto the castle parapets with fabulous views of Lough Corrib and the hills of Connemara and Mayo, and even those of Clare, on the other side of Galway Bay.\nThe rest of you sleep in the four cosy 'Vaults' on the ground floor below, their walls also lined with tapestries and other artworks. The Vaults have much picturesque charm with their oak-timbered partitions, arches and vaulted ceilings, and they work if you know each other well as the rooms lead one into the other. Vault I, the largest of the four, sleeps two in bunk-beds and features a fair-sized work table and chairs for busy teenagers and a mini-sofa for one or two in the window embrasure. Vault II (off No. 1) has a double bed and a similar window seat. Vault III (also off No.1) has one double and one single bed and a window seat. Vault III in turn gives access to Vault IV, a small single room with a three-light gothic window looking out at the standing stone sundial on the lawn.\nBefore making a payment, always call the owner at the number listed on our website to make sure your payment instructions are accurate.\nNever use instant money transfer services such as Western Union and MoneyGram.Learn more\nMember since: 2010\n- Vacation Rental\n- Guests Provide Their Own Meals\n- Children Welcome\n- Non Smoking Only\n- Pets Considered\n- Wheelchair Inaccessible\nBedrooms: 7 Bedrooms, Sleeps 12, Beds for 10-12\n- Baby Crib (1) available on request\nBathrooms: 2 Bathrooms\n- CD Player\n- DVD Player\n- Satellite / Cable\nLocal Services & Businesses\n- Medical Services\n- Horseback Riding\n- Sight Seeing\nSports & Adventure Activities\n- Dining Area\n- Clothes Dryer\n- Hair Dryer\nSpecial reduced letting rates for 2013: For self-catering parties of 2-12 guests Rates quoted are for 2 guests Every additional guest is an extra €45/week Low rates (Jan and Feb; Nov-Dec) .......................... €800-€1000/week Mid-season rates (Mar-May; Sep-Oct) ....................... €800-€1200/week High season rates (Jun-Aug, Christmas, Easter, New Year) .. €1200-€1600/week\nNote: Until confirmed, rates are subject to change without notice.\nDon't forget your vacation protection! Get protected now\nAdding our Vacation Protection services can make sure your getaway goes smoothly, no matter what. We offer Cancellation Protection and Damage Protection so you can truly relax.\nYear Purchased: 1969\nAbout the owner: When Christopher bought the site of Carraigin Castle, his friends and family panicked at the idea of his attempting to restore what looked like a shapeless heap of stones. Even the builder who undertook to have a go at putting the place together thought he was 'cracked'. Chris had grown up near a ruined castle, and it had been his childhood dream to restore one. He was, therefore, quite determined to go ahead with the project. By doing a lot of research into medieval hall houses and visiting nearly every castle in Ireland, Chris lovingly designed every corner and corbel of the building, including the roof and the original door latches. I am particularly fond of the down-stairs front door. It took Christopher 7 years (!) to design it and collect the materials – hand-forged nails, oak planks etc. Finally he took one whole month off to build it, and although I am his wife and I say so myself, the result is quite delightful. In spite of its authentic medieval appearance, Carraigin is as warm and comfortable as any modern holiday home.\nWhy the Owner Chose Galway City, Co. Galway, Ireland:\nChris had grown up near a ruined castle on the other side of the lake. After a long search all across the West of Ireland he found (and fell) for the ruin.\nThe Unique Benefits at this Castle:\nWe come for the fishing, fabulous walks, fireside chats and peace and quiet aplenty. There's a boat to rent, with outboard and fishing rods.\n- Primary: +447946703539\nIrish Lakeside Castle\nGreat Hall from the Gallery\nThe Great Hall\nThe Master Bedroom\nIndoor Original Stairs\nSingle Ground Floor Bed Room\nSpiral Stairs to Tower bed room.\nThe Wood in Winter\nSunset seen from the Great Hall\nView from the Great Hall over the lake\nThe Pleasure Ground\nView from the tower\nTimbered roof over Great Hall and Gallery\nSunbeam at Sunset\nThe Gallery Bedroom\nVault 1 leading into Vault 3\nGround Floor Entrance\nView from the Pleasure Ground\nA summer evening\nBoat and lake\nThe outside stairs\nThis listing was first published here in 2010.\nDate last modified - Thursday, December 01, 2016\n- Response time Within a few days\n- Response rate 90%\n- Calendar last updated 11/28/2016"", ""England's Top 10 Castles (Part 1)\nWhen people think of England, people think castles. And the first thing a lot of people think about England is castles and medieval fortresses. Not only being historic sites, they are also attractive to visitors from all over the world.\n1. Warwick Castle, Warwick\nWarwick Castle is a medieval castle in Warwick, the county town of Warwickshire, England. It sits on a cliff overlooking a bend in the River Avon. Warwick Castle was built by William the Conqueror in 1068 within or adjacent to Anglo-Saxon burh of Warwick. It was used as a fortification until the early 17th century, when Sir Fulke Greville converted it to a country house. It was owned by the Greville family, who became earls of Warwick in 1759, until 1978.\nFrom 1088, the castle traditionally belonged to the Earl of Warwick, and it served as a symbol of his power. The castle was taken in 1153 by Henry of Anjou, later Henry II. It has been used to hold prisoners, including some from the Battle of Poitiers in the 14th century. Under the ownership of Richard Neville – also known as “Warwick the Kingmaker” – Warwick Castle was used in the 15th century to imprison the English king, Edward IV. Warwick Castle has been compared with Windsor Castle in terms of scale, cost, and status.\nSince its construction in the 11th century, the castle has undergone structural changes with additions of towers and redesigned residential buildings. Originally a wooden motte-and-bailey, it was rebuilt in stone in the 12th century. During the Hundred Years War, the facade opposite the town was refortified, resulting in one of the most recognisable examples of 14th century military architecture.\nIn the 17th century the grounds were turned into a garden. Warwick Castle was purchased by The Tussauds Group in 1978 and opened as a tourist attraction. It is protected as a Scheduled Ancient Monument and a Grade I listed building.\n2. The tower of London, London\nHer Majesty’s Royal Palace and Fortress, more commonly known as the Tower of London (and historically as The Tower), is a historic monument in central London, England, on the north bank of the River Thames. It is located within the London Borough of Tower Hamlets and is separated from the eastern edge of the City of London by the open space known as Tower Hill.\nThe Tower of London is often identified with the White Tower, the original stark square fortress built by William the Conqueror in 1078. However, the tower as a whole is a complex of several buildings set within two concentric rings of defensive walls and a moat.\nThe tower’s primary function was a fortress, a royal palace, and a prison (particularly for high status and royal prisoners, such as the Princes in the Tower and the future Queen Elizabeth I). This last use has led to the phrase “sent to the Tower” (meaning “imprisoned”). It has also served as a place of execution and torture, an armoury, a treasury, a zoo, the Royal Mint, a public records office, an observatory, and since 1303, the home of the Crown Jewels of the United Kingdom.\n3. Bodiam Castle, East Sussex\nBodiam Castle is a quadrangular castle located near Robertsbridge in East Sussex, England. It is said to be a perfect example of a late medieval moated castle. While not large enough to garrison many soldiers, the castle was ideally suited for defense against a militant rural populace after the English Peasants’ Revolt and for the entertainment of foreign merchants or dignitaries.\nIt was built in 1385 by Sir Edward Dalyngrigge, a former knight of Edward III, supposedly at the request of Richard II in order to defend the surrounding area from French invasion. By 1434 Sir Edward Dalyngrigge’s nephew Richard was living in the castle. Recent research suggests that the castle was built more for show than as an effective defence. There is evidence supporting that research, as the walls of Bodiam Castle are only a couple of feet thick.\n4. Leeds Castle, Kent\nLeeds Castle is four miles south east of Maidstone, Kent, England, dates back to 1119, though a manor house stood on the same site from the ninth century. The castle and grounds lie to the east of the village of Leeds, Kent, which should not be confused with the city of Leeds in West Yorkshire.\nAt 900 years old, Leeds Castle sits 30 miles from central London. It was the home of Henry the VIII. Despite its British location, Leeds Castle has many American connections. Two families who owned the castle also owned large tracts of land in Culpepper and Fairfax, VA. Today, the castle's connection to the United States continues.\nWilliam Randolph Hearst (Hearst Corporation) almost purchased Leeds Castle, until he discovered the missing bathrooms, lack of electricity and that the servants' quarters had served as dungeons. Another American, however, did acquire the castle. After purchasing it in 1926, Lady Olive Baillie set about refurbishing the castle and installing all the necessary items.\n5. Arundel Castle in West Sussex, England\nArundel Castle is a restored medieval castle. The castle dates from the reign of Edward the Confessor (r. 1042-1066) and was completed by Roger de Montgomery, who became the first to hold the earldom of Arundel by the graces of William the Conqueror. The castle was damaged in the English Civil War and then restored in the 18th and 19th centuries.\nFrom the 11th century onward, the castle has served as a hereditary stately home to several families (with a few and brief reversions to the Crown) and is currently the principal seat of the Duke of Norfolk and his family. It is a Grade I listed building.\nLopngoaingu.com chúc các bạn học tiếng Anh thật tốt!\nLưu ý: Trong tất cả các bài viết, các bạn muốn nghe phát âm đoạn nào thì chọn hãy tô xanh đoạn đó và bấm nút play để nghe.""]"	['<urn:uuid:3cc0edd9-2878-4909-a493-9e276d1f4bb9>', '<urn:uuid:744a66ae-024e-475e-9045-0cf546378bf9>']	factoid	with-premise	short-search-query	similar-to-document	comparison	novice	2025-05-12T21:31:42.699391	5	20	2171
64	As a wildlife conservationist interested in both raptor protection and renewable energy, I want to know how traditional falconry practices in Ontario align with concerns about bird mortality in wind farms - what's the connection between these two conservation issues?	Both topics highlight the complex balance between human activities and bird protection. In falconry, birds are typically captured live, trained, and often released back into the wild after hunting season, demonstrating a sustainable practice that has contributed to raptor conservation. In contrast, wind farms pose significant risks to birds, with projections indicating they will kill at least one million birds annually by 2030, including raptors like Golden Eagles. While falconry in Ontario has evolved to include careful regulations and permits for live capture, wind energy development requires similar careful oversight through 'Bird-smart' principles including proper siting, monitoring, and compensation for unavoidable bird mortality.	['The Ontario Ministry of Natural Resources and Forestry’s announcement to allow for the full spectrum of falconry activities to be practiced in the province is met with utter relief, joy, appreciation, and gratitude. The decision is the result of nearly four decades of collaborative effort by the OFAH and the Ontario Hawking Club (OHC) working with MNRF to see falconry flourish in Ontario. As a falconer and former director of the OHC, chairwoman of the OFAH Small Game Advisory Committee, and someone who has worked on this initiative, the announcement also has deep personal meaning.\nConsidered an art, falconry is an ancient hunting tradition that involves taking quarry in its natural state and habitat by means of a trained bird of prey, and has changed very little over thousands of years. It is one of the oldest forms of hunting still in practice around the world and transcends culture, religion, politics, age and gender. It is inherently sustainable and gave rise to the study of ornithology, captive breeding of raptors, and many other contributions to conservation.\nIn Ontario, the art of falconry was initially practiced in the late 1960s and 1970s by a handful of falconers. By the early 1980s, small numbers of falconers and a lack of falconry regulations, in combination with the political and popular opinion that reflected significant opposition to all forms of hunting, put falconry at risk of being lost entirely in Ontario. In response to this significant threat, the OHC was formed in 1984 with 11 members, becoming the first falconry-specific OFAH member club.\nThe traditional practice of falconry involves the live capture of a suitable raptor and allowing a bond to develop between bird and human through consistent, highly specialized training and hunting (sharing) of quarry captured by the bird of prey. Birds are often released back into the wild at the end of the hunting season. Beginning in the 1980s, and at the continual behest of OFAH and OHC, regulations in Ontario were implemented and improved in ways that have allowed for most of the traditional practice of falconry to occur.\nOFAH support for falconry has remained steadfast over the years, leading to a long-term collaboration with the OHC. While falconers and the OHC had unique expertise and experience, the OFAH, as the largest hunting and conservation organization in the country, was able to assist with lobbying efforts through communication platforms like Ontario OUT of DOORS magazine, providing support in negotiations with politicians, engaging with staff at the MNRF, hosting and participating in the provincial Falconry Advisory Committee, and voting on numerous falconry issues at OFAH Advisory Committee and Board of Directors levels. The collaborative relationship between the OFAH and OHC has endured through successive provincial governments, as well as multiple leadership changes in both organizations.\nThe ability to engage in live-capture and release of falconry birds is the most recent and final step to allowing for full participation in falconry. A concerted joint effort was undertaken by the OFAH and OHC beginning in 2006 in order to achieve this final and significant missing step. In 2011, MNRF approved a permit system to allow for licensed falconers to capture and train raptors in Ontario. However, the process was considerably limited because only 25 permits were available each year, the lottery system for permits lacked transparency, and goshawks were excluded for no biological reason. In 2020, with nearly 10 years of data that demonstrates success in implementation, and no concerns for biological sustainability, the OFAH and OHC jointly requested a review and expansion of live-capture of raptors for falconry. We are collectively pleased and welcome the MNRF’s decision.\nOn a personal note, falconry has and continues to be my greatest gift and inspiration. When I began to learn falconry in 2004, only captive-bred raptors were allowed. Although my first bird was a Harris’ hawk, it’s an experience I will never regret. The depth of the bond I shared with my bird is the reason I became a hunter, a devotee of falconry, and an ardent supporter of the OFAH. The MNRF’s decision goes far beyond a legal or administrative action about licensing. It’s a demonstration that the ancient art of falconry is both valid and valued in Ontario. For that, I am eternally grateful.\nThis post was written by Maya Basdeo — a member of the OFAH Board of Directors and Chair of the OFAH Small Game, Migratory Bird & Wetlands Advisory Committee', 'Wind Energy Frequently Asked Questions |\nWhat Is American Bird Conservancy’s (ABC) policy regarding wind energy?\nWind power can be an important part of the solution to global warming, but wind farms can also kill birds—including eagles, songbirds, and endangered species—through collisions with turbines, and also harm them through loss of habitat. By 2030, there will likely be more than 100,000 wind turbines in the U.S., and these are expected to kill at least one million birds each year—probably significantly more. Wind farms are also expected to impact almost 20,000 square miles of terrestrial habitat, and over 4,000 square miles of marine habitat by 2030, some of it critical to threatened species.\nTo be a truly green source of energy, wind power needs to be Bird-smart and that means wind power employs careful siting, operation and construction mitigation, monitoring, and compensation to reduce and redress any unavoidable bird mortality and habitat loss from wind energy development. These are issues that should be included in mandatory federal wind standards. All wind farms should employ bird-smart principles and comply with relevant state and federal wildlife protection laws.\nWhat is bird-smart wind energy?\nBird-smart wind power implements careful siting considerations, operation and construction mitigation, bird monitoring, and compensation to reduce and redress unavoidable bird mortality and habitat loss. These are issues that the federal government should include in mandatory wind standards. For terrestrial wind farms, bird-smart wind should address:\n- Siting: Bird-smart wind power (including wind farms and associated infrastructure) is sited to prevent harm to birds, ideally in already altered habitats such as farmland, and avoids sensitive areas. Examples of such areas may include migratory bottlenecks, wetlands, raptor concentration and key nesting areas, the edges of ridges used by migrants, key habitat or flight paths for endangered or declining species, breeding concentrations of species that avoid tall structures (such as some grouse species), and in or adjacent to Important Bird Areas. Maps with detailed data on wildlife are currently being developed by conservation groups for use by the wind industry. Pre-construction assessments should always be conducted to confirm whether a particular site presents an especially high risk to birds. Some areas are not going to be suitable for wind development.\n- Operation and Construction Mitigation: Bird-smart wind power uses the best technology and management practices to avoid and minimize harm to birds, such as by burying transmission lines in high risk areas, following Avian Power Line Interaction Committee standards for above-ground transmission lines, using lighting that minimizes nighttime migratory bird collision mortality (such as strobe lights), using unguyed rather than guyed meteorological towers, and restoring habitat disturbed by construction, e.g., re-compacting soils disturbed by construction and replanting native vegetation (or restoring the site if the wind farm is decommissioned).\n- Monitoring: Bird-smart wind power conducts effective, federally reviewed and approved, site-specific, pre- and post-construction studies/assessments to assist with improved siting and operation, and to properly quantify impacts. Pre-construction assessments must provide sufficient data to assist with micro-siting (e.g., by use of radar to detect local bird movements), create an annual baseline against which post-construction studies can be evaluated, use all existing available bird study data, and be conducted during months when bird use can be expected to be at its peak at the selected site. Post-construction studies must employ mathematical models that best account for variations in local conditions and the relative difficulty of locating bird carcasses in different habitats, as well as any scavenging by predators that may reduce the number of carcasses found, and run for at least two years (and long enough to determine the efficacy of, and make needed revisions to, operational mitigation measures).\n- Compensation: Bird-smart wind power redresses the loss of any birds or habitat unavoidably harmed by construction and operation to a net benefit standard. This includes bird deaths caused by collisions with turbines and their associated power lines, and lost or degraded habitat (e.g., areas of abandoned habitat) Such compensation could include acquiring additional land for the National Wildlife Refuge system or other off-site habitat conservation projects.\nAlthough offshore wind power is not yet operational in the U.S., an analogous set of siting, operating, and compensatory measures need to be developed to make it bird-smart.\nAll wind farms should have an Avian Protection Plan which includes American Bird Conservancy’s bird-smart principles, and a means of implementing them and tracking and reporting on this implementation. Wind farms should also comply with relevant state and federal wildlife protection laws such as the Endangered Species Act, Migratory Bird Treaty Act, Bald and Golden Eagle Protection Act, and National Environmental Policy Act.\nHow would you know if a potential wind farm site will have large or small impacts to birds?\nSee section above on pre-construction monitoring and post-construction monitoring.\nIsn’t the Federal Government developing wind energy regulations?\nNo, the Federal Government has released draft, voluntary wind-energy guidelines that were produced from recommendations to the Secretary of the Interior by a Federal Advisory Committee. During the public comment period on those proposed voluntary guidelines, ABC will urge that that the Department of the Interior enact mandatory standards that the industry must follow. We do not believe that energy industries should be able to choose whether or not to consider bird impacts.\nA recent lawsuit regarding the Altamont Pass Wind Farm in California illustrates that getting the right thing done voluntarily is anything but a sure thing. Recent studies say that approximately 7,600-9,300 birds were killed there each year, including 55-94 Golden Eagles, 475-477 American Kestrels, 253-433 Red-tailed Hawks, and 714-718 Burrowing Owls. After seven years of being challenged by various conservation groups, it finally took a lawsuit to get changes made at Altamont Pass. NextEra Energy Resources has only recently agreed to replace 2,400 of its old wind turbines at Altamont Pass with fewer, larger models that produce the same amount of total power. They also agreed to place the new turbines in “more environmentally friendly” locations, and to pay $2.5 million for research and raptor habitat improvement.\nWhat kinds of birds are impacted by wind farms?\nPotentially all night-migrating songbirds are at risk of colliding with wind turbines, as are raptors and other birds when wind farms are sited in areas they frequent. Habitat loss is also an issue as wind farms can degrade bird habitat or cause birds to abandon habitat.\nGolden Eagles will be especially impacted because much of the additional wind build-out planned for the western U.S. is expected to occur in areas they inhabit. The endangered Whooping Crane will be exposed to additional risk from collision with new power lines erected to service wind farms along their migratory pathways.\nTo a Greater Sage-Grouse and some other birds, any tall structure such as a wind turbine is a threat because it is a potential perch for a predatory bird. A wind turbine standing a considerable distance away has much the same effect as a small tree at a few hundred yards, causing sage-grouse to abandon traditional lekking grounds up to three miles away from a wind farm. Unfortunately for the Sage-Grouse, Wyoming, one of this bird’s last remaining strongholds, is slated for significant wind farm build-out. It is very important for the future of the Greater Sage-Grouse that this development is appropriately sited\nHow many birds are killed by wind farms each year?\nNo one knows for sure. Recent estimates of the number of birds killed by wind turbines ranges from a low of 100,000 birds/year to 440,000 birds/year (calculated by the U.S. Fish and Wildlife Service). If 20% of the nation’s electricity comes from wind power by 2030, ABC estimates that at least one million birds per year will be killed by wind turbines, probably significantly more.\nAren’t there laws that protect birds from being killed by wind farms?\nWhooping Cranes and some other birds are protected under the Endangered Species Act. Golden Eagles are protected under the Bald and Golden Eagle Protection Act (BAGEPA), and most migratory birds are protected by the Migratory Bird Treaty Act (MBTA). Unfortunately, both the BAGEPA and MBTA are not currently sufficiently enforced to prevent predicted mortality resulting from wind development. However, in 2011, the U.S. Fish and Wildlife Service is expected to begin issuing permits for wind farms that expect to kill Golden Eagles, contingent on the wind farms taking tangible steps to protect eagles. The Greater Sage-Grouse, meanwhile, currently receives no federal legal protection, though several states have stepped up to protect remaining core breeding areas.\nRead full ABC Policy Statement on Wind Energy']	['<urn:uuid:5551f8ce-51c2-4e06-8d23-2cfa36f68cdd>', '<urn:uuid:ad348f5b-9b9c-419d-9045-9be2eb6e06f6>']	factoid	with-premise	verbose-and-natural	distant-from-document	three-doc	expert	2025-05-12T21:31:42.699391	40	103	2157
65	I'm learning about jewelry making and came across the term 'pave setting' but I'm not sure what it means - can you explain how this type of stone setting works?	Pave is a method of setting many small stones very close together, similar to the way stones are set in a paved road.	"[""Glossary of Commonly Used Terms and Techniques in Vintage Jewelry\nChatelaine - Clip worn at the waist from which trinkets are suspended. Also can refer to a pair of coordinating brooch attached by a long chain, popular in the 1940's\nCloisonne- An enamelling technique where a pattern is formed of wire on to a goundplate, creating cells filled with enamel.\nCollet - A setting made of a short tubular band of metal that encloses a stone.\nCushion Cut - Rounded corner square faceted stone.\nCut Steel - Small steel studs, riveted closely together on a metal backing.\nDamascene - A decorative technique where gold, silver wire, or cutout sheet metal is embedded into blackened iron or steel base.\nDepose - A registered trademark or design found on French items\nDress Clip - An ornament attached to clothing by a hinged clip with a flat back with prongs on the underside.\nEmbossing - A technique that creates a raised decoration on metal with hammers on the reverse side.\nEmerald Cut - Square or rectangle cut stone with square chamfered corners and step cut sides.\nEnamel - a) Powder pigmented glass fired onto a metel groundplate. b) Painted metal surface\nEngraving - A decorative technique with the application of a design or pattern on the surface of metal or stone by removing surface material.\nEtching - The process that creates a design on metal or glass using an acid.\nFacet - Plane cut polished surface of a stone.\nFaux - French, meaning false or fake.\nFiligree - Metal decoration made with thin twisted wires.\nFob - A decorative ornament or seal suspended from a watch chain.\nFrench Jet - black glass\nFur Clip - An brooch-like ornament with a hinged double-pronged mechanism to attach to clothing.\nGilding - A process where a base metal is plated or washed with a thin layer of gold.\nGold Filled - A process where heat and pressure is used to join a layer of gold to a base metal.\nGold Plated - A layer of gold of less than 1/20th of the total weight.\nGold Wash - A very thin coating of gold over base metal.\nGuilloche - Machine-engraved metal decoration with a translucent enamel applied over it.\nHair Jewelry - Pieces made with or containing human hair.\nHallmark - The marks on gold, silver, or platinum that indicate fineness or karat.\nIntaglio - Italian for an engraved stone, opposite of a cameo, with a recessed design carved into the surface.\nInvisible Setting - A type of channel setting using specially cut square or rectangular colored gemstones that are notched to slide onto metal tracks to fit closely together in rows, this way no metal is visible from the front of the piece.\nJet - A type of fossilized coal, used primarily for mourning jewelry in the nineteenth century.\nKarat - A measure of fineness that is 1/24th of the total weight in a gold alloy, 24 karats is pure gold.\nLavalier - A chain suspending a gemstone or small pendant set sith gemstones.\nLocket - A two-part pendant or brooch with a hinge and cover containing a picture or other momento.\nLucite - Du Pont tradename for acrylic thermoplastic.Frequently generically used for a variety of vintage plastic with similar characteristics.\nMarcasite - Faceed iron pyrite with a silver luster.\nMarquise Cut - An oval or elliptical cut gymstone with pointed ends, also called Navette.\nMosaic - An object or jewel decorated with many small pieces of inlaid multicolored stone or glass and arranged into a design.\nMother of Pearl - Iridescent inside lining of mollusks.\nMourning Jewelry - Jewelry worn in memory of a deceased loved one.\nNavette - a gemstone cut that is oval or elliptical with pointed ends, also called Marquise.\nNegligee - A pendant or necklace with two suspended uneven drops.\nNiello - A form of enamelling with a powdered mixture of silver, copper, and lead, that is fused by heat into an engraved design in silver or gold, that creates a grayish-black metallic contrast with the metal background.\nParure - A complete set of jewelry, three or more matching pieces.\nPaste - A high lead content glass that has been faceted to resemble a gemstone. 'Black dot' paste referse to faceted clear leaded glass stone with a black dot painted on the culet to imitate the look of a diamond,\nPatina - A color change on the surface of metal, a type of oxidation cause by age and/or wear.\nPave' - Method of setting many small stones very close together, as in a paved road.\nPendant - Decorative element suspended from a necklace or chain.\nPlique a jour - An enamelling process that resembles stained glass where the groundplate is removed after firing\nPot metal - Base metal; tin and lead alloy used in early twentieth century costume jewelry.\nProng Set - Stones held in place by metal claws or prongs.\nRepousse - A raised design in metal.\nRhinestone - Faceted colorless or colored glass, cut like a gemstone.\nRondel, Rondelle - A thin donut shaped metal ornament that is strung on anecklace between beads, typicallty set with rhinestone around the circunference\nScarab - Egyptian symbol of immortality, carved or molded stone, clay, or glass wing bug-like creature.\nSigned - Marked with the initials, name, logo, or trademark of the designer, maker, or manufacturer.\nSlide - Movable decorative element of a chain or bracelet that adjusts its opening.\nSterling Silver - An alloy that is 925 parts pure silver and 75 parts copper.\nStrass - A high lead content glass that has been faceted to resemble a gemstone, also called a paste.\nSuite - Several pieces of jewelry similarly designed to be worn together.\nTaille d' Epargne - An enameling technique that designs a partially filled engraving that is filled with opaque enamel.\nTorsade - A twisted multi-strand necklace usually made with beads or pearls.\nTrombone Catch - A two part sliding tubular closure for brooches.\nTube Hinge - Brooch finding, an elongated tubular hinge to which pinstem is attached.\nVermeil - Gold-plated or gold washed sterling silver.\nVulcanite - Vulcanized rubber, used for mourning jewelry in the nineteenth century.\nWatch Pin - A small pin with a hook at the base of the reverse with its open end up, for suspending a small watch.\nWirework - Twised wire decoration applied to metal ground.""]"	['<urn:uuid:a75aa8ca-8f93-45d7-97bc-1a5353a2958c>']	factoid	with-premise	verbose-and-natural	similar-to-document	single-doc	novice	2025-05-12T21:31:42.699391	30	23	1063
66	transformer protector what prevents explosion	The TRANSFORMER PROTECTOR (TP) is the solution to prevent transformers from exploding, saving your company's reputation by avoiding unwanted consequences.	['Transformers meet connectivity. Allow us to use hello to label the final hidden state of the final Encoder layer for each wi. The Decoder also incorporates multiple layers – sometimes, the number is the same as that of the Encoder. This leads to the 33 kv vacuum circuit breaker with good quality, which serves as the next enter for the Encoder RNN, together with the second aspect within the enter sequence “suis”. The first layer is 4 instances the scale of the mannequin (Since GPT2 small is 768, this network would have 7684 = 3072 units). Every layer of GPT-2 has retained its own interpretation of the primary token and will use it in processing the second token (we’ll get into extra detail about this in the following section about self-consideration). I have expanded the first one so you may see its self-attention layer is the masked variant. A middle-tapped transformer is commonly used in the output stage of an audio energy amplifier in a push-pull circuit Modulation transformers in AM transmitters are very related. Concatentate the anticipated phrase to the decoder enter as pass it to the decoder. The model continues iterating till the complete context is generated (1024 tokens) or until an end-of-sequence token is produced. The context vector is the first enter to the Decoder RNN, which should then generate the primary factor of the output sequence “I” (in reality, the final layer of the Decoder is typically a softmax , but for simplicity we will simply maintain the more than likely factor on the end of every Decoder step). The place the reference voltage (V_N) is the nominal voltage at the low voltage facet of the transformer and the rated obvious energy (S_N) is defined system vast in the web object (see Unit Methods and Conventions ). The evaluation and training strings are tokenized, and the resulting knowledge is sharded, shuffled, and saved as TFRecords. Transformer is a different architecture for remodeling one sequence into another one with the help of two parts, Encoder and Decoder. There are N decoder layers in the transformer. The converter tools and traction transformers should accommodate completely different enter frequencies and voltage (starting from as excessive as 50 Hz down to sixteen.7 Hz and rated as much as 25 kV). I created it to introduce more visual language to describe self-consideration with a purpose to make describing later transformer fashions easier to look at and describe (taking a look at you, TransformerXL and XLNet). This allows the network to pay attention to relevant components of the enter sequence at completely different ranges of abstraction: the values V of the decrease Encoder layers shall be closest to the original input tokens, whereas Self-Attention of the deeper layers will contain more abstract constructions. In reality, the Encoder Self-Attention, that’s bi-directional by design, is a crucial part of BERT , the pre-educated contextual word embeddings, that we shall discuss in a while. Three-part transformers utilized in electric power systems can have a nameplate that indicate the part relationships between their terminals. First, “je” (or, most definitely, a word embedding for the token representing “je”), usually accompanied by a relentless vector hE0 which could possibly be both realized or fastened, gets fed into the Encoder RNN. That is true for Seq2Seq models and for the Transformer. The Multilin 845, a member of the Multilin eight Collection relay platform, has been designed to offer comprehensive safety, control, and administration for 2- or 3-winding energy and distribution transformers used in utility and industrial functions. The trick here is to re-feed our model for each place of the output sequence until we come across an end-of-sentence token. 9 By working at higher frequencies, transformers can be bodily more compact as a result of a given core is ready to switch more energy without reaching saturation and fewer turns are needed to attain the identical impedance. At every location in the sequence, y, the MultiHeadAttention runs all 8 consideration heads throughout all different areas in the sequence, returning a brand new vector of the identical size at each location.\nAn encoder block from the original transformer paper can take inputs up until a certain max sequence size (e.g. 512 tokens). If this seems acquainted to you, it is for a very good cause: this is the Transformer’s Encoder-Decoder Consideration, which is somewhat similar in spirit to the Consideration mechanism that we mentioned above. The token is processed successively through all of the layers, then a vector is produced alongside that path. The output of the encoder is the enter to the decoder. Transformer generates and study a particular positional vector that is added to the input embedding before it is fed into the primary encoder layer. The TRANSFORMER PROTECTOR (TP) is the solution to prevent transformers from exploding, saving your organization’s fame by avoiding unwanted consequences. 17 Conversely, frequencies used for some railway electrification programs have been a lot lower (e.g. 16.7 Hz and 25 Hz) than normal utility frequencies (50-60 Hz) for historic causes involved mainly with the constraints of early electric traction motors Consequently, the transformers used to step-down the high overhead line voltages have been a lot larger and heavier for a similar power rating than these required for the higher frequencies. In Sample Efficient Textual content Summarization Utilizing a Single Pre-Educated Transformer , a decoder-solely transformer is first pre-educated on language modeling, then finetuned to do summarization. At other occasions, you surprise why Linkin Park was included, when sequences with emotional pieces are instantly juxtaposed with the present Billboard Sizzling a hundred. For our instance with the human Encoder and Decoder, imagine that as a substitute of solely writing down the translation of the sentence in the imaginary language, the Encoder also writes down keywords which might be vital to the semantics of the sentence, and offers them to the Decoder along with the common translation. The eye mechanism learns dependencies between tokens in two sequences. Use our included mounting hardware to setup the Ring Transformer in no time. The Decoder will then take as input the encoded sentence and the weights supplied by the attention-mechanism. Power transformer over-excitation condition attributable to decreased frequency; flux (inexperienced), iron core’s magnetic characteristics (crimson) and magnetizing present (blue). No matter in case you function a transformer in a power era plant, an industrial software or within the grid: Your belongings will let you know their operational status and provides an indication when abnormalities happen. A sequence of tokens are handed to the embedding layer first, adopted by a positional encoding layer to account for the order of the phrase (see the subsequent paragraph for extra details). Air-core transformers are unsuitable for use in power distribution, 12 but are incessantly employed in radio-frequency functions. The eye output for each head is then concatenated (utilizing tf.transpose , and tf.reshape ) and put through a remaining Dense layer. Because of this the weights a are outlined by how every word of the sequence (represented by Q) is influenced by all the opposite words in the sequence (represented by Okay). Moreover, the SoftMax perform is utilized to the weights a to have a distribution between 0 and 1. These weights are then applied to all the phrases in the sequence that are launched in V (similar vectors than Q for encoder and decoder however different for the module that has encoder and decoder inputs). Enhance efficiency by understanding the true-time status of your transformers. We’d like yet another technical detail to make Transformers easier to grasp: Consideration. It’s estimated that 50% of energy transformers will survive 50 years of use, that the typical age of failure of power transformers is about 10 to 15 years, and that about 30% of power transformer failures are as a consequence of insulation and overloading failures. V (worth) and Okay (key) receive the encoder output as inputs. 20 Eddy current losses might be decreased by making the core of a stack of laminations (skinny plates) electrically insulated from each other, rather than a stable block; all transformers working at low frequencies use laminated or related cores.\nValue: Value vectors are actual word representations, as soon as we have scored how relevant each phrase is, these are the values we add up to signify the current word. Power-frequency transformers could have taps at intermediate points on the winding, often on the upper voltage winding facet, for voltage adjustment. We offer varied materials, stamped components and inductive parts akin to differential present sensors or current transformers to support you in your solution. As an example, this self-consideration layer in the prime block is listening to a robotic” when it processes the word it”. This story takes us all the way in which again to 2014 ( Ref , another Ref ), when the concept of approaching seq2seq problems through two Recurrent Neural Networks mixed into an Encoder-Decoder model, was born. Thus, getOutputProperties().getProperty(String key) will get hold of any property in that was set by setOutputProperty(.String,String) , setOutputProperties(.Properties) , in the stylesheet, or the default properties, whereas getOutputProperties().get(String key) will only retrieve properties that had been explicitly set by setOutputProperty(.String,String) , setOutputProperties(.Properties) , or within the stylesheet. As we’ve seen in The Illustrated Transformer , the unique transformer model is made up of an encoder and decoder – every is a stack of what we are able to name transformer blocks. At that point, we could use a beam search algorithm to maintain the top few predictions at each step and select the most likely output sequence at the finish, or simply maintain the best choice each time. Learning the place of each word or the distance between phrases can enhance translation, especially for a language like German, where verbs come at the very end of the sentence many instances. In The Illustrated Word2vec , we’ve checked out what a language model is – mainly a machine learning model that is in a position to take a look at a part of a sentence and predict the subsequent phrase. The Encoder takes the input sequence and maps it into the next dimensional house (n-dimensional vector). Attempt utilizing a special dataset to train the transformer. It turns out to attain higher outcomes than a pre-trained encoder-decoder transformer in limited information settings. Guidelines on ecodesign primarily cowl minimum energy effectivity ranges of transformers with a minimal power ranking of 1 kVA that are used in 50 Hz electricity networks or in industrial functions. We have to score each phrase of the enter sentence towards the current input word. As power ratings improve, transformers are often cooled by compelled-air cooling, forced-oil cooling, water-cooling, or combos of these. This concludes our journey into the GPT2, and our exploration of its father or mother mannequin, the decoder-only transformer. Back then, a typical broadcast console contained dozens, typically hundreds of audio transformers. Transformer is a neural community architecture that solves sequence to sequence problems utilizing consideration mechanisms. In addition to the proper-shifting, the Transformer applies a mask to the input within the first multi-head consideration module to keep away from seeing potential ‘future’ sequence elements. Operation of a transformer at its designed voltage however at the next frequency than supposed will result in lowered magnetizing present. Input the complete encoder sequence (French sentence) and as decoder input, we take an empty sequence with only a start-of-sentence token on the first position. The ideal transformer identification shown in eq. 5 is an inexpensive approximation for the standard industrial transformer, with voltage ratio and winding turns ratio each being inversely proportional to the corresponding current ratio. GPT-2 (from OpenAI) released with the paper Language Fashions are Unsupervised Multitask Learners by Alec Radford, Jeffrey Wu, Rewon Youngster, David Luan, Dario Amodei and Ilya Sutskever. For those who’re curious to know exactly what occurs contained in the self-attention layer, then the next bonus section is for you.']	['<urn:uuid:754286d5-2c0d-49b4-9cc0-608d3222736d>']	factoid	with-premise	short-search-query	similar-to-document	single-doc	expert	2025-05-12T21:31:42.699391	5	20	1979
67	When comparing the water infrastructure between the Mississippi River's shipping channel and Dalbeattie's factory system, which appears to be more complex in terms of water management features?	The Mississippi River's shipping infrastructure consists primarily of a maintained 9-foot shipping channel from Baton Rouge to Minneapolis, and a deeper 45-foot channel from Baton Rouge to Head of Passes. In comparison, Dalbeattie's factory had a more intricate water management system comprising multiple components: the Lochenkit Reservoir and Treatment Works, a 100,000-gallon service reservoir on Aiket Hill, an extensive network of earthenware drains and ditch drains, two ponds with pump houses, concrete canals, settling manholes, a pumping station with debris filters, sediment settling tanks, and backup diesel engines. While the Mississippi's system is larger in scale, Dalbeattie's appears more complex in terms of different types of water management features.	"[""Identification of Structures - Water Supply and Sewerage\nReliable water supply and sewerage are the little-respected essentials for any large industrial or residential development.\nThe manufacture of explosives requires water for dilution and washing chemicals and equipment during processing, also for steam\nto transfer heat where it is needed. Additional water is needed for fire-fighting and safety precautions, for staff washing,\ntoilets, showers, cooking, laundry and drinking.\nThe supply at Dalbeattie appears to have been divided into two types - a reservoir-fed drinking-quality supply from the\nLochenkit Reservoir and a cooling and firefighting supply from site drainage, local burns and ponds. The Lochenkit supply\nwas apparently sufficient for Unit 2 and some Unit 1 household needs, but process cooling and firefighting required\nadditional impure supplies.\nAn extensive system of earthenware drains and ditch drains was installed to try to reduce the groundwater levels\nin this naturally wet site. Damaged by later activities, this system's manholes and pipes are still visible in many parts\nof the site, particularly in Unit 1 (Southwick). Field surveys of the Factory have revealed two ponds at the lowest sections\nof the Factory, either side of the Kirkgunzeon Lane north of the railway viaduct. The Pumping Station on the eastern pond\nappears to have been the intake for drainage and river water used as process coolant and possibly for fire-fighting.\nReservoir-fed Drinking Quality Water :\nThe rainy climate of South West Scotland had already been harnessed by the hydro-electric scheme at Tongland north of\nKirkcudbright in the early 1930s; this may have been in the planners' minds when considering Dalbeattie. The water supply\nfor MS Factory Dalbeattie came from near Corsock at the Lochenkit Reservoir (NGR NX803757) and filtered half a mile away at the\nLochenkit Treatment Works (NGR NX792756) near Areeming, before being piped fourteen miles to a 100,000 gallon covered\nservice reservoir on Aiket Hill (NGR NX834634) overlooking the works site (NGR NX834646). A 1945 report on water supply\nindicates that Lochenkit had a capacity of up to 300,000 gallons per day, but that the Factory at Dalbeattie received\nbetween 80,000 and 100,000 gallons per day.\nAt the A711 roadside, in line between the service reservoir and the works, is a strongly-built brick building (NGR NX839631)\nwith a concrete roof. Matthew Taylor, in a telephone conversation on 29/05/2006, stated that this was the pumping station\nfor water from the service reservoir. Presumably this would have ensured that the supply had sufficient pressure to reach\nthe tops of buildings in Unit 1 (Southwick) which would have been nearly a mile from the service reservoir. Matthew recalled\nthat the empty building had been sold to a man who wanted to turn it into a filling station, but the project failed and it\nwas just used as storage. As far as is known, the pumping station did not draw any water from the Edingham Loch, a shallow\nand silted-up natural feature, merely acting as a booster for water from Aiket Hill service reservoir.\nA further and recent development came from a study of the Knock Burn. This drains the much larger Loch Fern reservoir\n(NGR NXZ864626) about a mile east of the works on the east side of Barclosh Hill. Graham Cook confirmed that Loch Fern was\noriginally an estate sporting lake with its own boathouse and a sluice to control water release into Knock Burn. He further\nconfirmed that no service reservoir existed on Barclosh Hill, the logical location, and has no evidence for any water intake\nfrom Loch Fern for any purpose.\nRiver and Burn Process Water :\nThe Lochenkit supply is impressive but relatively low to cover on its own the entire requirements of the two large Units of\nSouthwick and Edingham. The writer had guessed that the Kirkgunzeon Lane flowing through the site could have been used, as\nmight the Knock Burn flowing through Unit 1 (Southwick). Survey visits showed that two bunded surface reservoirs\nhad been constructed in the course of the Knock Burn, with a capacity assessed at around 1,000,000 gallons. Thus far, no\nintakes have been photographed there or along the course of the Kirkgunzeon Lane within the Factory site, although the Unit\n1 Pump House near the Kirkgunzeon Lane bridges may have been used.>\nWritten proof from G.M. Nicholson's document Acids Recovery Plant - Failure of Factory Services - Revised\nInstructions (15/12/42) has references to 'River Water' actually being used in the Denitration and Fortified Acid\nMixing processes and the 'Pots' used for spent acid re-concentration. Apparently the water was for cooling purposes, but\nfurther investigation will be needed. There are references to 'Gravitation Water' being used in place of river water,\npossibly tapped from the supply from the service reservoir.\nMatthew Taylor in a telephone conversation on 29/05/2006, stated that he thought Stelrad's had built and stocked the\nKnock Burn surface reservoirs as trout fishing for their clients. On discussion, he modified this by agreeing that it might\nbe the case that the pre-existing reservoirs had indeed been stocked for fishing purposes. Bill Woods later stated that the\nKnock Burrvoirs had been stocked by the Dalbeattie Angling Association, who now use the old Dalbeattie Municipal Reservoir\ninstead. Examination of the old spillway of the Knock Burn reservoirs shows that the concrete bases of at least two pipe\nsupports are set in or on it, but the Royal Commission produced 1946 aerial photographs proving that Knock Burn - although\ncanalised - had not been dammed, the seeming dams and bunding being raised Narrow Gauge bogie railway traverses.\nConversations with Graham and Sandra Cook of Barclosh Farm revealed that Loch Fern at the head of Knock Burn had been\na pre-war boating and fishing lake for a local estate. It appears that the Loch and its sluice acted as a convenient\nwartime header resevoir for the Burn and its reservoirs within Unit 1 in the Factory. For most of its course from the railway\nline up to the reservoirs, the Knock Burn was retained in a concrete canal. This may have been to prevent flooding in the\nlow-lying area occupied by the Wash Water Settlement Hills of the Nitroglycerine section. It is unclear whether Loch Fern\nwas ever used to provide process water by gravity flow, although that would seem sensible, but the previous theory of factory\nuse of Knock Burn now has to be discarded.\nThe Drainage System and its Pump House :\nThis ingenious system seems to have made use of the naturally wet site in a remarkably clever way. Field drains and\nditches, supplemented by land drains, settling manholes and glazed earthenware pipes, were used to gather and drain site\nwater. The water was channeled to ponds at the lowest part of the Factory site, either side of the Kirkgunzeon Lane and\njust north of the Kirkgunzeon Lane Viaduct. The water could then either be discharged through sluices into the Lane burn\nor pumped from a large Pump House into pressure mains to provide cooling water for site processes and probably\nto fill Emergency Water Supply (EWS) tanks.\nThe Pump House and intakes in Unit 1 survive in remarkably good condition, although the pumps and the motor to run them\nwere removed many years ago. WARNING : DO NOT APPROACH CLOSER TO THE PUMP HOUSE THAN THE NEARBY ROAD. THE POND IS AS\nTOXIC AS A SEPTIC TANK FROM CATTLE WASTES AND IS DANGEROUSLY DEEP. The photographs on this page are the safest way to\nobserve the details :-\nIt is still unclear whether the Unit 1 Pump House supplied more than Unit 1 itself. Given the water supply from Lochenkit\nReservoir it is possible that this and the Pump House near Edingham Farm were sufficient for Unit 2. Although there are a\npair of unidentified insulated steel pressure pipes under the viaduct, it is uncertain whether these are anything other than supply pipes for Auchenkit drinking-quality water.\n- Settlement tank with baffles beside Pump House. Possibly coolant tank for backup diesel engine.\n- Sluicegates on riverside and Pump House ends of pond. Presumably to control flow to and from Kirkgunzeon Lade\nand into intake housing.\n- Intake debris filters and sediment settling tank, to reduce mud content of pumped water.\n- Interior with bases for two pumps (probably centrifugal design) and later poured concrete base.\n- Chimney cowling on roof. May have been for diesel engine exhaust, as backup to electrical pumping system.\n- Large insulated intake pipe under viaduct (pipe end severed by acetylene cutter) goes towards end of (now drained) pond\non west side of Lane. It seems that this water from Unit 2 was pumped by the Pump House in Unit 1.\nDetails of Sewerage Services are given separately."", 'Mississippi River Facts\nMississippi River Overview\nThe Mississippi River is one of the world’s major river systems in size, habitat diversity and biological productivity. It is the second longest river in North America, flowing 2,350 miles from its source at Lake Itasca through the center of the continental United States to the Gulf of Mexico. The Missouri River, a tributary of the Mississippi River, is about 100 miles longer. Some describe the Mississippi River as being the third longest river system in the world, if the length of Missouri and Ohio Rivers are added to the Mississippi’s main stem.\nWhen compared to other world rivers, the Mississippi-Missouri River combination ranks fourth in length (3,710 miles/5,970km) following the Nile (4,160 miles/6,693km), the Amazon (4,000 miles/6,436km), and the Yangtze Rivers (3,964 miles/6,378km). The reported length of a river may increase or decrease as deposition or erosion occurs at its delta, or as meanders are created or cutoff. As a result, different lengths may be reported depending upon the year or measurement method.\nFor reasons mentioned above there are competing claims as to the Mississippi’s length. The staff of Itasca State Park at the Mississippi’s headwaters say the main stem of the river is 2,552 miles long. The US Geologic Survey has published a number of 2,300 miles, the EPA says it is 2,320 miles long, and the Mississippi National River and Recreation Area suggests the river’s length is 2,350 miles.\nAt Lake Itasca, the river is between 20 and 30 feet wide, the narrowest stretch for its entire length. The widest part of the Mississippi can be found at Lake Winnibigoshish near Bena, MN, where it is wider than 11 miles. The widest navigable section in the shipping channel of the Mississippi is Lake Pepin, where it is approximately 2 miles wide.\nAt the headwaters of the Mississippi, the average surface speed of the water is about 1.2 miles per hour – roughly one-third as fast as people walk. At New Orleans the river flows 3 miles per hour on average.\nMississippi River Watershed\nThe Mississippi River watershed is the fourth largest in the world, extending from the Allegheny Mountains in the east to the Rocky Mountains in the west. The watershed includes all or parts of 31 states and 2 Canadian Provinces. The watershed measures approximately 1.2 million square miles, covering about 40% of the lower 48 states.\nCommunities up and down the river use the Mississippi to obtain freshwater and to discharge their industrial and municipal waste. We don’t have good figures on water use for the whole Mississippi River Basin, but we have some clues. A January 2000 study published by the Upper Mississippi River Conservation Committee states that close to 15 million people rely on the Mississippi River or its tributaries in just the upper half of the basin (from Cairo, IL to Minneapolis, MN). A frequently cited figure of 18 million people using the Mississippi River Watershed for water supply comes from a 1982 study by the Upper Mississippi River Basin Committee. The Environmental Protection Agency simply says that more than 50 cities rely on the Mississippi for daily water supply.\nAgriculture has been the dominant land use for nearly 200 years in the Mississippi basin, and has altered the hydrologic cycle and energy budget of the region. The agricultural products and the huge agribusiness industry that has developed in the basin produce 92% of the nation’s agricultural exports, 78% of the world’s exports in feed grains and soybeans, and most of the livestock and hogs produced nationally. Sixty percent of all grain exported from the US is shipped on the Mississippi River through the Port of New Orleans and the Port of South Louisiana.\nIn measure of tonnage, the largest port district in the world is located along the Mississippi River delta in Louisiana. The Port of South Louisiana is one of the largest volume ports in the United States. Representing 500 million tons of shipped goods per year (according to the Port of New Orleans), the Mississippi River barge port system is significant to national trade.\nShipping at the lower end of the Mississippi is focused on petroleum and petroleum products, iron and steel, grain, rubber, paper, wood, coffee, coal, chemicals, and edible oils.\nTo move goods up and down the Mississippi, the U.S. Army Corps of Engineers maintains a 9-foot shipping channel from Baton Rouge, LA to Minneapolis, MN. From Baton Rouge past New Orleans to Head of Passes, a 45 foot channel is maintained to allow ocean-going vessels access to ports between New Orleans and Baton Rouge.\nAt Lake Itasca, the average flow rate is 6 cubic feet per second. At Upper St. Anthony Falls, the northern most Lock and Dam, the average flow rate is 12,000 cubic feet per second or 89,869 gallons per second. At New Orleans, the average flow rate is 600,000 cubic feet per second.\nThere are 7.489 gallons of water in a cubic foot. One cubic foot of water weighs 62.4 pounds. A 48 foot semi-truck trailer is a 3,600 cubic foot container.\nAt Lake Itasca, it would take 10 minutes for one semi-trailer of water to flow out of the lake into the Mississippi.\nAt St. Anthony Falls, the equivalent of 3 semi-trailers full of water go over the falls every second.\nAt New Orleans, the equivalent of 166 semi-trailers of water flow past Algiers Point each second.\nThe Mississippi River and its floodplain are home to a diverse population of living things:\nAt least 260 species of fish, 25% of all fish species in North America;\nForty percent of the nation’s migratory waterfowl use the river corridor during their Spring and Fall migration;\nSixty percent of all North American birds (326 species) use the Mississippi River Basin as their migratory flyway;\nFrom Cairo, IL upstream to Lake Itasca there are 38 documented species of mussel. On the Lower Mississippi, there may be as many as 60 separate species of mussel;\nThe Upper Mississippi is host to more than 50 mammal species;\nAt least 145 species of amphibians and reptiles inhabit the Upper Mississippi River environs.\nContact the Park\n111 E. Kellogg Blvd., Suite 105\nSaint Paul, MN 55101\nThis is the general phone line at the Visitor Center, which is staffed every day except Mondays. Please leave a voicemail if we miss your call and expect a return call within 1 day, often sooner.\nMississippi River Facts, Bridges, Tributaries, History and …\nDescription : Leather Boots on Mississippi-River.netExcerpt found on www.mississippi-river.netThe Mississippi River, derived from the old Ojibwe word misi-ziibi meaning.\nFactbox: Five facts about Mississippi River flooding (Reuters …\nDescription : Here are five facts about the Mississippi River: MIGHTY MISSISSIPPI: The Mississippi begins at Lake Itasca in Minnesota, curls south for more than 2300 miles through the United States and empties into the Gulf of Mexico. …\nNervous residents watch rising Mississippi River at Interesting facts\nDescription : Memphis, Tennessee The swollen Mississippi River rolled south Wednesday as communities along its delta braced for flooding, and vast farms remained under threat as it left a trail of submerged homes. …\nprofile-Facts: Mississippi River\nDescription : Mississippi River is the largest river system in North America. About 2320 miles (3730 km) long, the river originates at Lake Itasca, Minnesota, and flows slowly southwards in sweeping meanders, terminating 95 miles (153 km) by river …\nMississippi River to crest Tuesday at Interesting facts\nDescription : The crisis stems in large part from a deluge of rainfall over the past month further up the Mississippi River. Reichling said that, over one two-week stretch, there was about 600% more precipitation than usual. This contributed to a …\nIncoming search terms:\n- mississippi river\n- where is the mississippi river located']"	['<urn:uuid:2fd92328-5e19-4ef5-a85f-6b1aa6b9dcb3>', '<urn:uuid:d0b9d68e-1b7e-4a12-99b3-e7b5f62fe445>']	open-ended	direct	verbose-and-natural	distant-from-document	comparison	novice	2025-05-12T21:31:42.699391	27	109	2738
68	costume designer career paths mio guberinic irene lentz compare	Mio Guberinic and Irene Lentz took different paths to success in costume design. Guberinic started with theater education at Northwestern University, moved to NYC, and developed a diverse portfolio including theater, nightlife events, dance shows, and pop stars, while also running his own studio incorporating modern technology like 3D printing. In contrast, Lentz began as an actress in silent films, opened a small dress shop, gained recognition at Bullocks Wilshire department store, and then entered film costume design, eventually becoming MGM's leading costume supervisor and designing for major Hollywood stars like Ginger Rogers and Lana Turner.	"[""Masterful indeed, Mio Guberinic zoomed from school to designing costumes for Katy Perry in record time. He is the recipient of the 2019 TDF/Kitty Leech Young Master Award, presented to an early-career designer whose work has gone beyond mere beginner's promise and has entered a period of fruition. The award is given in recognition of Irene Sharaff’s wish to see young designers encouraged on their way to fully acknowledged success and excellence in the field. The 2019 Sharaff Awards are presented by TDF on April 26 at The Edison Ballroom in NYC. Live Design chats with Guberinic about his life in costumes.\nLive Design: In a nutshell what was your career path?\nMio Guberinic: I graduated from Northwestern University in Chicago, with two designed shows at the Steppenwolf Theater. I enjoyed Chicago, but I knew I wanted to grow more and pursue a career in NYC. One week after graduation, I moved to NYC as I was invited to design my first show, Ghost Girl, and that started my journey here. Since then I have designed for theater, special nightlife events, dance shows, avant-garde theater, pop stars, club scene, and any and all projects that came my way. I always aimed to focus on unique design projects where I could create new costumes and wearable art. With time, my responsibilities and scale of projects grew. During my first few years in New York, I also assisted Fabio Toblini and Paul Tazewell. These were both incredibly rewarding and eye-opening experiences, and helped me grow as an artist and understand better the professional world of costume designers.\nBesides designing for theater, I worked on window display projects creating specialty costumes and crafts for windows on 5th Avenue. I worked for Spaeth Design Studio and Burke & Pryde studio. My experience as costume creator/designer for windows and costume designer/maker for theater always overlapped. My goal was to push the limits of what’s considered wearable and explored new materials and ways to express myself and characters and stories I was developing.\nLD: How do you approach a project in terms of research, cues from the script, character development vis-à-vis the costumes?\nMG: I start with overall artistic emotional response and then I get more specific with each character. Sketching as I read also helps. I also like to research what was done previously with a particular show or a character, so that knowledge helps me lay the groundwork for an original artwork. I like to skip the paths that were already taken and carve my own.\nLD: Fabrics...favorites and how do you choose them?\nMG: I love to develop new textures and materials, and layer multiple fabrics to create my own. I like using non-traditional materials in development of costumes. Since I have a laser cutter and 3D printer in my studio, I try to experiment with ways to gain new textures. I’m always excited about new chrome-like materials mimicking metal but conforming to the body.\nLD: Favorite costumes shops and examples of special items made for you?\nMG: I feel very fortunate that I actually run my own costume studio, Mio Design NYC, where I develop a lot of my own design work and work for other designers for Broadway, TV, and film. My most recent ambitious undertaking was a Medusa costume I designed and made in my studio. I developed its spine from laser-cut foam components, and 3D printed components - snake skulls...all embellished in Swarovski crystals.\nBeing in Brooklyn, I have access to other 3D printing studios like Roboto NYC. They have multiple kinds of printers and some larger machines so all of the snakeheads were printed there while my team did painting and finishing with crystals in my studio. I like overlapping traditional costume making skills with contemporary technology.\nLD: Favorite or most challenging project to date, and why?\nMG: Favorite? There are many, but at the moment I would say “Court Garden,” the dance piece I designed for A Canary Torsi Dance Company. I had great support from my team, and when we opened the show, it felt that there was a moment of theater magic. Every part of the puzzle came together perfectly.\nLD: What advice would you give to design students?\nMG: Believe in your vision as an artist. Make time to create your own art as often as you can. Observe beauty in nature; nature is the best designer. Unfollow influencers and follow true artists.\nClick through the slideshow for more costume sketches and images of his designs."", 'Irene (costume designer)\n|This article needs additional citations for verification. (April 2009) (Learn how and when to remove this template message)|\nDecember 8, 1901|\nBaker, Montana, U.S.\n|Died||November 15, 1962\nLos Angeles, California, U.S.\n|Cause of death||Suicide|\n|Resting place||Forest Lawn Memorial Park, Glendale|\n|Other names||Irene Gibbons\n|Known for||Designing costumes for motion picture actors|\n|Spouse(s)||F. Richard Jones, Elliot Gibbons|\nIrene Maud Lentz (December 8, 1901 – November 15, 1962)  also known mononymously and professionally as Irene, was an American fashion designer and costume designer. Her work as a clothing designer in Los Angeles led to her career as a costume designer for films in the 1930s. Lentz also worked under the name Irene Gibbons.\nBorn in Baker, Montana, to Emil Lents and Maud Walters, Lentz started out as an actress under her birth name, appearing in secondary roles in silent films beginning with Mack Sennett in 1921. She played ingenue parts opposite Sennett\'s leading comedians, Ben Turpin and Billy Bevan. Lentz was directed in her first film by Sennett\'s production chief, F. Richard Jones; their professional relationship matured into a personal one. They had been married for less than a year when Jones succumbed to tuberculosis in 1930.\nLentz had been taught sewing as a child and with a flair for style, she decided to open a small dress shop. The success of her designs in her tiny store eventually led to an offer from the Bullocks Wilshire luxury department store to design for their Ladies Custom Salon which catered to a wealthy clientele including a number of Hollywood stars.\nLentz\'s designs at Bullocks gained her much attention in the film community and she was contracted by independent production companies to design the wardrobe for some of their productions. Billing herself simply as ""Irene,"" her first work came in 1933 on the film Goldie Gets Along featuring her designs for star Lily Damita. However, her big break came when she was hired to create the gowns for Ginger Rogers for her 1937 film Shall We Dance with Fred Astaire. This was followed by more designs in another Ginger Rogers film as well as work for other independents such as Walter Wanger Productions, Hal Roach Studios as well as majors such as RKO, Paramount Pictures and Columbia Pictures. During the 1930s, Irene Lentz designed the film wardrobe for leading ladies such as Constance Bennett, Hedy Lamarr, Joan Bennett, Claudette Colbert, Carole Lombard, Ingrid Bergman, and Loretta Young among others. She ""is generally regarded as the originator of the dressmaker suit""  that was popular in the late 1930s.\nThrough her work, Lentz met and married short story author and screenwriter Eliot Gibbons, brother of multi-Academy Award winning Cedric Gibbons, head of art direction at MGM Studios. Despite her success, working under the powerful set designer Cedric while being married to his brother Eliot was not easy. Irene confided to her close friend Doris Day that the marriage to Eliot was not a happy one. Generally regarded as the most important and influential production designer in the history of American films, Cedric Gibbons hired Lentz when gown designer Adrian left MGM in 1941 to open his own fashion house. By 1943 she was a leading costume supervisor at MGM, earning international recognition for her ""soufflé creations"" and is remembered for her avant-garde wardrobe for Lana Turner in 1946\'s The Postman Always Rings Twice. In 1948, she was nominated for the Academy Award for Best Costume Design, Black-and-White for B.F.\'s Daughter.\nIn 1950, Lentz left MGM to open her own fashion house. After being out of the film industry for nearly ten years, in 1960, Doris Day requested her talents for the Universal Studios production Midnight Lace for which Lentz earned a second Academy Award nomination. The following year she did the costume design for another Doris Day film and during 1962 worked on her last production, A Gathering of Eagles.\nIn 1962, after Doris Day noticed that Lentz seemed upset and nervous, Lentz confided in her that she was in love with actor Gary Cooper and that he was the only man that she had ever loved.[page needed] Cooper had died in 1961.\nOn November 15, 1962, three weeks short of her sixty-first birthday, Lentz took room 1129 at the Knickerbocker Hotel, checking in under an assumed name. She jumped to her death from her bathroom window.\nShe had left suicide notes for friends and family, for her ailing husband, and for the hotel residents, apologizing for any inconvenience her death might cause. Per her wishes, she is interred next to her first husband, director F. Richard Jones, at the Forest Lawn Memorial Park Cemetery in Glendale, California.\nIn 2005, Irene Lentz was inducted into the Costume Designers Guild\'s Anne Cole Hall of Fame.\n|1933||Goldie Gets Along||Costume designer, uncredited|\n|1933||Flying Down to Rio||Costume designer, uncredited|\n|1937||Shall We Dance||Gowns for Ginger Rogers|\n|1937||Vogues of 1938||Gowns for Joan Bennett|\n|1938||You Can\'t Take It With You||Gowns for Jean Arthur|\n|1938||Topper Takes a Trip||Gowns for Constance Bennett|\n|1938||Vivacious Lady||Gowns for Ginger Rogers|\n|1939||In Name Only||Gowns for Carole Lombard|\n|1939||Intermezzo: A Love Story||Costume designer for Ingrid Bergman|\n|1939||Midnight||Gowns for Claudette Colbert|\n|1940||Green Hell||Gowns for Joan Bennett|\n|1940||Seven Sinners||Gowns for Marlene Dietrich|\n|1941||That Uncertain Feelings||Gowns for Merle Oberon|\n|1941||Mr. & Mrs. Smith||Gowns for Carole Lombard|\n|1941||To Be or Not to Be||Gowns for Carole Lombard|\n|1942||Take a Letter, Darling||Gowns for Rosalind Russell|\n|1942||You Were Never Lovelier||Gowns for Rita Hayworth|\n|1943||No Time for Love||Gowns for Claudette Colbert|\n|1943||Girl Crazy||Costume supervisor|\n|1944||Meet Me in St. Louis||Costume supervisor|\n|1944||Bathing Beauty||Costume supervisor|\n|1945||The Picture of Dorian Grey||Costume supervisor|\n|1945||Week-End at the Waldorf||Costume supervisor|\n|1946||Harvey Girls, TheThe Harvey Girls||Costume supervisor|\n|1946||Ziegfeld Follies||Costume designer/supervisor, uncredited|\n|1947||Lady in the Lake||Costume supervisor|\n|1947||Cass Timberlane||Costume designer|\n|1948||Easter Parade||Costume designer (women)|\n|1948||Pirate, TheThe Pirate||Costume supervisor|\n|1949||Barkleys of Broadway, TheThe Barkleys of Broadway||Costume designer|\n|1949||Neptune\'s Daughter||Costume designer|\n|1950||Shadow on the Wall||Costume designer|\n|1960||Midnight Lace||Gowns for Doris Day|\n|1961||Lover Come Back||Gown for Doris Day|\n|1963||Gathering of Eagles, AA Gathering of Eagles||Costume designer|\n- ""Irene"", in Suicide in the Entertainment Industry: An Encyclopedia of 840 Twentieth Century Cases, by David K. Frasier (McFarland, 2005) p156-157\n- Hall, Mary (March 23, 2009). ""Angelina Jolie\'s Costumes in The Tourist Pay Homage to MGM Fashion Designer Irene Lentz"". Huffington Post. Retrieved August 7, 2011.\n- In Day\'s autobiography, she wrote that in 1962, Irene ""had an unhappy marriage to a man who lived out of the state and only occasionally came to visit her.""\n- Day, Doris; Hotchner, A.E. (Oct 1976) . Doris Day: Her Own Story (Bantam mass market paperback) (6th printing ed.). New York: William Morrow. p. 237. ISBN 0-553-02888-X.\n- Day later wrote that she got the feeling that she was the first person to whom Irene had confided this information. She also wrote: ""Thinking about it now, I cannot honestly say whether Irene\'s love was one-sided or whether she and Cooper had actually had or were having an affair.""\n- Michelle Vogel (2012). McFarland, ed. Lupe V\'Lez: The Life and Career of Hollywood\'s Mexican Spitfire. p. 47. ISBN 978-0786461394.']"	['<urn:uuid:f8e6972f-906a-4a14-9405-b879cd93d75a>', '<urn:uuid:14ba064b-395c-47f5-bbee-49b222b6be6e>']	open-ended	with-premise	short-search-query	similar-to-document	comparison	expert	2025-05-12T21:31:42.699391	9	96	1918
69	dangers centralized metadata storage system	The main dangers of centralizing metadata are related to file mapping interactions. If files are moved without updating the metadata database, the database becomes invalid. Additionally, you need mechanisms to quickly update the centralized metadata server when files are updated, and you must ensure data integrity of the database through backups and protection against read errors.	"['The amount of data we want to collect, archive, and search, is amazing. The use of metadata allows us to quickly find data files in which we are interested. However, the storage and search of the metadata alone has become a ""big data"" problem. One important aspect of this is where you store the metadata.\nThe craze of ""big data"" is upon us, and the world is adjusting to what it means, how they can use it, and how to build systems for it. Feeding the big data beast is a sea of sensors. For example, there are video cameras everywhere—outside stores, inside stores, intersections, on helicopters, dash cams, people with cell phones. There are road sensors, sensors in our cars, sensors all around wooded areas, sensors along bridges. There are domain-specific sensors, such as those for the power grid, the oil and gas industry, hospitals, ISPs, websites, weather, oceans, the military, and on and on.\nAll of this data has a common thread—the need for metadata.\nMetadata is simply data about data. For example, metadata can include information about where a sensor is located (GPS coordinates), the time period for a specific recording, the direction the sensor was sensing, the firmware of the sensor, and the model of the sensor, and more.\nYou can also ""tag"" files with new metadata about information that is usually found by post-processing the data. In the case of a camera, these metadata tags could be time stamps where there is something interesting happening (perhaps along with a note of the interesting event itself). Other metadata tags could also be pointers to other related sources of information such as other cameras or weather data.\nIt\'s obvious that the usefulness of metadata depends upon its quality. If the metadata isn\'t accurate, then using the associated raw data will result in a degrade—if not an outright failure—of the resulting analysis. Some of the metadata has to be created by people and cannot be automated, so there is always the possibility of error.\nUnderstanding which metadata is important for a specific data file and how to make it useful for researchers is an extremely important question. It\'s also probably a question that has not only a technological solution but also sociological and psychological solution.\nBut one seemingly simple question has a huge impact on the use of metadata: where do you store the metadata?\nA Place for Your Stuff\nWhen I originally investigated the question of where to place metadata I explored two options. The first option is to put the metadata in a central location for all data. The second option was to put the metadata with the data itself.\nThe first option is one that is used by many search or archive systems. The idea is fairly simple—gather metadata about a specific file and store it, typically in a database. Then you can search the database as you like, hopefully finding the files that contain the information in which you are interested (assuming that the metadata is correct—but that\'s another story).\nOne of the outputs from the search should be the location (fully qualified name which contains the full path to the file) of the file(s) of interest. Then you can copy the file(s) into some sort of working storage and have at it.\nThe dangers of centralizing the metadata mostly deal with the file mapping interaction between the metadata and the files. For example, you need a mechanism to update the centralized metadata server when the metadata for the various files is updated. Ideally, this update mechanism should be fairly fast; otherwise, the search data can be out of date. How you define ""fast"" is up to you based on your users and usage model.\nIn this update mechanism there is a buried problem. What happens if the database and the files are no longer in synch? For example, what happens if a file is moved so that its full path in the database is no longer valid?\nThe result is obvious, the database is no longer valid, at least concerning that file. Hopefully, the update mechanism can tell the database that the file has moved and to either create new metadata for the new location or to update the existing metadata to reflect the new location of the file. In either case, the update window will have an impact on updating the database.\nA third aspect that needs attention is the data integrity of the database itself. You will need to provide data protection of the database using backups, copies, or something similar. Don\'t forget that the database is primarily used for reads, requiring that you pay attention to the size of the database and the rate of read errors. Building an index from consumer SATA drives, as some manufacturers do, means that when you read as little as 100GB you are likely to hit a read error. This forces a rebuild if you built the storage with RAID controllers, and could cause further problems during the rebuild.\nThe second option, storing the metadata with the data, is a very desirable approach because now you have to worry about data integrity of just one file system and not two. If the file moves, the metadata moves with it. You can add metadata to the files at any time because they stay with the files.\nIdeally, if we had good tools for copying and moving metadata with files, you could easily copy or move the data files somewhere else. For example, if you copied the file to some work storage, the metadata would need to come with it. This also means that you could update the metadata of the file and then copy it back, taking the updated metadata with it.\nOne way to achieve this is by using extended attributes (xattr). Many file systems support extended attributes and there are ways for users to add metadata to the file via xattrs and ways to read them. Some file systems impose limitations on the extended attributes, such as the amount that can be added, but others do not. Regardless, being able to store metadata along with the data is a very attractive proposition.\nBut just like anything else, there are some downsides to storing the metadata with the data. The first one that comes to mind is how do you effectively search the metadata?\nIt basically boils down to walking the file system tree structure and searching the metadata of each file and returning the information. Depending upon the number of files and the tree structure of the files, this could take a long time, perhaps unnecessarily wasting time.\nIf you find the files you need by searching the metadata of all of the files, unfortunately, there are very few tools and techniques for copying the file, including the metadata, to some other storage (working storage). For example, NFS does not transmit the xattrs of a file, preventing you from using it to copy a file from and archive/collection to a working location. There are ways to copy the file and the metadata, but you need to pay very close attention to ensure that it works as anticipated.\nI admit that I thought the second approach was better because the metadata always stayed with the data, allowing you to always access the metadata. For the option of a centralized metadata index, you had to spend additional resources to protect that database and ensure its accuracy, in addition to paying attention to the data protection of the data itself. Given the incredible size of some data collections, coupled with the projections on the size of the new collections/archives, I now believe that either approach by itself is not sufficient.\nRather, I believe a hybrid approach is a more desirable one.\nI still fundamentally believe that metadata needs to be stored with the data. The basic reason for this is that the metadata is about the data—it is really all part of the same thing. Separating them makes no sense to me.\nBut at the same time, walking a tree with potentially billions of files is not practical. To overcome this, we need a centralized index of the metadata in a framework that is designed for searching. But the authority of all metadata is still the file itself.\nIf we create an archive and start pushing data into it, that gives us an opportunity to grab the metadata and store it a centralized index. We can gather the metadata for a file when the file is moved into the archive.\nHopefully, the archive has mechanisms that indicate when a file in the archive has changed, kicking off a metadata acquisition step which updates the central index. However, with archives that have a REST interface, there is no obvious way to ""update"" a file. If you ""get"" a file from the archive and then change it, most of the time you must ""put"" the entire file into the archive again. To the archive it is a ""new"" file. There are some archives that allow you to update files but the mechanisms are not easy to use and it\'s much easier to just ""get"" the file, modify it, and then ""put"" the changed file back into the archive. For these cases, the ""put"" operation initiates a metadata acquisition step making life much easier.\nThe authority for all metadata are the files, which have the metadata attached (using xattrs, in my opinion). If the centralized index fails for some reason, you have to spend some time walking the file system to ""re-gather"" the metadata, but you haven\'t fundamentally lost your index for good.\nWho would have thought that something as simple as where to store metadata could result in a lengthy discussion? But suddenly, in today\'s world of millions to billions of files in data individual collections and archives, the importance of where you store your metadata is a key component in making the data actually useable in a practical way.\nPutting all of the metadata in a centralized index is impractical because you now have to provide good data protection on the data and on the centralized index. Simply moving a file location without updating the index could wreak havoc with someone\'s search.\nSimilarly, putting the metadata with the data using something like xattrs is not as desirable, because each data search means that all of the metadata has to be collected via a tree-walk and searched. While keeping the metadata with the data is very natural, it could easily result in much longer searches than necessary.\nI believe the better solution is to combine aspects of both into a single solution. Leave the metadata with the data, but as the data is pushed into the collection/archive, you can extract the metadata for use in a centralized index. Then the index is used for data searching.\nWith a simple REST interface, it is fairly easy to keep the index up to date. But in the event that the index is lost or corrupted, it is possible to walk the tree structure of the collection/archive and recover the metadata.\nEven if you don\'t agree with these concepts, at the very least I hope this article has caused some people to think about where they want to store their metadata and how it is used for searching. If possible, write about your solution and the path you took to get there. I\'m sure there are many people who would like to learn from what you have done.']"	['<urn:uuid:5d6d9a6d-e200-46fb-9687-2d3b19ee8e02>']	factoid	with-premise	short-search-query	similar-to-document	single-doc	expert	2025-05-12T21:31:42.699391	5	56	1917
70	In my comparative literature studies, I've noticed different approaches to cultural authenticity in storytelling. How do the concepts of 'insider vs. outsider perspective' in multicultural literature compare to the regional and ethnic manifestations in folk literature?	In multicultural literature, the insider perspective comes from authors writing about their own culture, while the outsider perspective is from authors writing about different cultures based on association, experiences, and research. This raises questions about whether cultures can be accurately represented by outsiders. Similarly, in folk literature, regional and ethnic manifestations show that the way stories and traditions are expressed varies significantly based on cultural authenticity. For instance, folk literature reflects profound differences in interests and emphases across cultures, often deeply rooted in specific religious concepts, natural environments, and historical experiences of each group. Both frameworks emphasize that cultural representation is heavily influenced by whether the storyteller is from within the culture or viewing it from the outside, and both recognize that authentic cultural expression is closely tied to the specific characteristics and experiences of each cultural group.	"['Presentation on theme: ""Literature Representing Diverse Perspectives. Why Bother? We live in segregated neighborhoods Our first interaction with other cultures often comes from.""— Presentation transcript:\nWhy Bother? We live in segregated neighborhoods Our first interaction with other cultures often comes from movies, books, and other media People of color make up at least 25% of our population and increases each year. Knowledge of other cultures helps increase tolerance\nTypes of Diversity Gender Culture Ethnic Racial background Language Physical and mental abilities\nWhat Role Does Literature Play? Literature portrays diversity in natural ways and can spark discussion. Can enhance students’ understanding of others. Books of diverse perspectives are found in all genres.\nMulticultural Literature Defined Literature that reflects the multitude of cultural groups within the United States. Literature that reflects ethnic and regional groups whose cultures historically have been less represented than European cultures. International literature is a related body of literature about countries outside the US.\nParallel Cultures Phrase coined by Virginia Hamilton. Many “minority” cultures are no longer numerically in the minority. This phrase refers to some cultures being parallel to the mainstream culture: Asian, Afro-American, Native American, Hispanic.\nWhy Should Literature Be Multicultural? Makes students feel welcome if their people and perspective are included in the texts and literature they read. Helps those in the “mainstream” learn about others they will interact with.\nIdentifying Multicultural Lit. Full range between books that just show diverse faces to books that are entirely based on specific cultural aspects. If a person is included just to show that the book is multicultural and is not authentically represented in the text, then it a poor selection. The other end of the spectrum is when the culture is central to the book and is more than highlighted.\nTwo Ends of the Spectrum Culturally Generic Books –Generally American in theme and plot, the inclusion seems to be incidental at times while purposeful at other times, but the theme remains culturally generic. –Even though they aren’t culturally specific, they are valuable because they present characters of other cultures and universal themes, which shows that people are the same.\nCulturally Specific Books –Illuminate the experience of members of a specific cultural group –The daily life, language, attitudes, values and beliefs of members are accurately portrayed. –Helps others share the culture and universal qualities of character as well as differences.\nIssues Cultural Authenticity –Written by authors with a cultural conscientious –Make members of the culture feel they have been accurately portrayed and represented. –They engender racial and cultural pride –Does not mislead or convey incorrect images of a culture.\nInsider vs Outsider Perspective Insider - author is of the culture being written about. Outsider - author is of a different culture and writes from association, experiences, and research. Is it possible for a culture to be accurately represented by an outsider?\nStereotyping Using a single set of attributes to represent an entire culture. To determine if a stereotype is true, find out if the people of the culture embrace the attribute about themselves.\nIdentification of Cultural Groups Always included: Asian, African American, Latino, Native Americans. Others: Jewish Americans, Appalachian Americans European American are usually excluded because they are seen as the mainstream.\nCriteria for Evaluating and Selecting Do the author and illustrator present insider perspectives? Is the culture portrayed multidimensionally? Are cultural details naturally integrated? Are details accurate and is the interpretation current? Is language used authentically? Is the collection balanced?\nInternational Literature Books written outside the US and translated and books set entirely in foreign countries. Books written in other countries allow readers to experience native perceptions. Books written in US about other countries include information that may enlighten nonnative readers.\nCriteria for Evaluating Intended audience: will students in US be able to understand the story? Translation: Is there a flow of language? –Are the unique details preserved –Are foreign situations explained so that readers can understand?\nAwards General awards: Newbery and Caldecott Coretta Scott King Award - African American authors and illustrators whose books promote an understanding an dappreciation of the culture and contribution of all people to the realization of the “American Dream”\nPura Belpré Award - Latino author who best depicts the Latino culture Asian Pacific American Award for Literature - Asian Pacific American writers in three categories: literature for children and young adults is one category.\nHans Christian Andersen Award - international award to honor an author who made a significant contribution to children’s literature. Mildred Batchelder Award - books originally published in another language and translated.\nLiterature about Diverse Perspectives Gender Equity and Gender Roles –Portray males and females appropriately –Homosexuality and alternative family structures Social Diversity: poverty, low social class, homelessness, illiteracy, migrant lifestyle People with exceptionalities', '- Origins and development\n- Characteristics of folk literature\n- Techniques of folk literature\n- Regional and ethnic manifestations\n- Major forms of folk literature\n- Study, collection, and preservation\nRegional and ethnic manifestations\nIn many particulars of form and substance there will be found great variations in the ways folk literature is manifested. The interests of people in one culture may differ profoundly from those of people in another. One group may enjoy singing folk songs, another listening to romantic folktales, and a neighbouring group may even be concerned only with legends and traditions. This difference is often geographic, so that the student of folk literature in the Pacific Islands who may later investigate a Central African tribe will find a completely different emphasis in the two areas. These differences may well depend upon the varieties of religious concepts held by the group or its natural environment, whether islands or jungle or cultivated farm lands, or its stability or mobility. These characteristics are likely to become especially deep-seated in groups that have been settled in one place over a long period of history. Frequently they may correspond to national frontiers, but more often they are aspects of the general culture of an area and may well be quite independent of political or linguistic boundaries.\nThe Russian epic songs are found only in Russia, but the wonder story such as Cinderella or Snow White is a part of the folk literature of a good portion of the world. The Navaho Indians of the Southwestern United States place great emphasis on their remarkable chants and lengthy folktales. Their neighbours throughout the Great Plains tell many well-constructed unified stories but confine their rituals largely to the dance. In Europe the Irish excel in storytelling, both of legends and fictional tales, so that even today it has been possible to record a prodigious number for their national archive. But in England and Wales the folktale is little cultivated, preference having been given to legends and ballads. As expected, there is a contrast between the abundance of oral saints’ legends in Spain and Italy and their rarity in Scandinavia. Finland, meeting place of Eastern and Western tradition, shows an abundance of nearly every kind of folklore. From eastern Europe to Central Asia the folk epic flourishes.\nTales and origin legends have been collected in great numbers from various parts of Oceania, where there is a common mythological background extending over enormous distances. Except for probable early contact by way of Indonesia, these folktales seem to show little Eurasian influence. In many parts of South America the merging of Iberian, Indian, and African materials seems almost complete.\nThe folk literature of the African Americans is in a state of continual change, reflecting their history. Much certainly goes back to Africa, usually by way of the West Indies, and much was borrowed long ago. But African Americans have themselves in a truly oral fashion developed songs and stories, and particular music styles. Of very special character is the folklore of modern Israel. Jews from various lands have brought together folk literature from all these countries. Assimilation of this is a long task, and, since divergent language backgrounds are unimportant for folktales, the problem is to absorb the great variety of forms.\nTaken the world over, folk literature is found everywhere, though the emphasis differs from place to place.\nMajor forms of folk literature\nSinging of some kind is almost universal, and it is probable that where there are no reports of it the information is simply missing. Folk song implies the use of music, and the musical tradition varies greatly from one area to another. In some places the words of songs are of little importance and seem to be used primarily as support for the music. Frequently there are meaningless monosyllables and much repetition to accompany the voice or the musical instrument. In much of the world, drums and rattles, beating time by hands or feet, or the stroking of a harp give a strong rhythmic effect to folksinging. In other parts of the world, flutelike wind instruments or bowed fiddles of one kind or another affect the nature of folk song texts. In many places folk songs are of great importance, serving as excitement to war or love or as a part of religious or secular ritual. Through them the group expresses its common emotions or lightens the burden of communal labour. In certain preliterate groups and sometimes elsewhere, folk songs are used for magic effects, to defeat enemies, to attract lovers, to invoke the favour of the supernatural powers. Sometimes the magic effect of these songs is so greatly valued that actual ownership of songs is maintained and their use carefully guarded. They may come to the owner in a dream or as the result of fasting or other austerities.\nEven when folk songs are not used for such practical purposes but only for the pleasure of singing or listening, the greater part of the world uses them for the expression of ideas or emotions held in common by the group. Only in societies used to the songs of composers or poets does purely personal expression enter into the folk song. This is not frequent, and songs of this type are hardly to be distinguished from some of the simple lyrics of poets such as Robert Burns. Folk songs, essentially expressions of commonly shared ideas or feelings, are often trivial but sometimes they may be profoundly moving.\nThe lyric folk song in one form or another is found almost everywhere, but this is not true of narrative singing. Unless the reporting of the activities of preliterate cultures has been very faulty, it would seem that the combination of song and story among these peoples has been rare, in spite of a wealth of prose narrative. On the other hand, in major Western and Asian civilizations the narrative song has been important for a long time and has been cultivated by the most skillful singers. In the course of time these songs of warfare, of adventure, or of domestic life have formed local cycles, such as the byliny of Russia or the heroic songs of many of the Balkan States and Finland or the ballad tradition of western Europe and elsewhere. Each of these cycles has its own characteristics, with its distinctive metrical forms, and its formulas both of events and expression. Any reader of the Homeric poems will be aware of their essentially oral and musical nature, and all the early literary narratives of Sumer and the Middle East suggest a long previous development of narrative singing.']"	['<urn:uuid:17efdb62-66d1-4c4d-8b0f-dd3f2c68bd1e>', '<urn:uuid:11846f5d-fd2a-4a95-a606-34590ecfdd64>']	open-ended	with-premise	verbose-and-natural	similar-to-document	comparison	expert	2025-05-12T21:31:42.699391	36	138	1890
71	how long prepare implement records system rim sox time requirements compare	The implementation timeframes for RIM and SOX compliance systems differ significantly. A RIM program typically requires at least six months for creating and agreeing on policy statements, 7-8 hours per employee for record inventory, 3-6 months for researching retention periods, and 3 months for implementing electronic management systems. In contrast, SOX compliance preparation should begin 18-24 months before a company goes public, with the most successful IPO companies operating under SOX rules for at least 12 months prior to going public. Both systems require ongoing maintenance, with RIM requiring reviews every 12-18 months, while SOX requires continuous compliance and annual reporting within Form 10-K.	['Reduce Medical Record Risk through a RIM Program\nEffectively manage records by supporting compliance, audits, and litigation preparedness.\nAs custodians of patient records, providers are ethically obligated to manage records responsibly, and are bound to do so by federal laws such as HIPAA. Providers are also obligated to manage their own business records in accordance with state and federal regulations. Despite these requirements, your organization may not have a defined, systematic process for effectively managing patient records because existing systems were not initially configured to track lifecycle, retention, and security.\nTo be compliant (and to report compliance), your organization must have a comprehensive, enterprise-wide records and information management (RIM) program in place that supports client services, governance, audit response, and litigation support for patients, as well as litigation preparedness for the organization itself.\nImprove Your Organization with RIM\nA RIM program provides structure to address business concerns, helps to avoid the risk of non-responsiveness during legal discovery, and demonstrates the practice’s lawfulness.\nLitigation readiness – Organizations must maintain records that support business decisions, operations, and services for the length of time required by the regulating agencies, and as dictated by the statutes of limitations to bring a claim. A RIM program communicates these requirements clearly and concisely.\nAwareness of communication and education – RIM will help your organization:\n- Write “for the record:” no jargon, abbreviations, or slang.\n- Write a business email: to the point, one subject per email.\n- Dispose of drafts properly: When the final document is approved, delete the drafts.\n- Consistently classify and label documents: no jargon, abbreviations, or slang.\n- Dispose of records and non-records: shred paper, delete and empty the recycle bin.\nBuilt-in cost control:\n- Keeping records beyond the required time leaves your business open to discovery and high legal costs.\n- Paying for storage of (electronic or hardcopy) patient records beyond the obligation affects the bottom line.\n- Disposing of duplicates and convenience copies eliminates confusion about which is the “real” record (a favorite question of an auditor).\n- Being able to retrieve the right record quickly reduces the time the auditor will be on site.\nImproved customer service – When you keep only the most current records, retrieval time to look up a reference takes seconds or minutes, instead of hours or days. This improves productivity.\nLegal case law requires it – Safe Harbor is not available if you do not have a RIM program in place. Per a U.S. District Court ruling in Starbucks Corporation v. ADT Security Services, Inc., 2009, “Failure to adopt a compliant records retention and destruction protocol that permits cost effective access to relevant records and creates an audit trail subjects the non-compliant litigant to sanctions and constitutes spoliation.” Although this ruling came from a non-medical practice case, it pertains to any business that creates, maintains, and manages records either for internal use or as a custodian of someone else’s records.\nOrganizations are responsible for instructing their employees, contractors, and third-party vendors on how to manage their records. This, too, is part of a RIM program. Information governance, content management, disaster recovery plans, and knowledge management are related and fall under the “Say it. Do it. Prove it!” approach to reducing risk.\nBuild a RIM Program\nIf your organization has not implemented an enterprise-wide records management system, the task of identifying employee-created records, and finding how and where they are stored, will be a challenge. To be ready for an audit or litigation, however, it must be done. If you waiting until your organization is audited or subpoenaed, you’ll suffer far worse. Courts have stated that if competitors in your industry have been sued, you may expect litigation in your company’s future. The same holds true for audits.\nCreating a RIM program starts with a records and information policy statement that clearly explains how records are to be managed.\nNext, take RIM inventory: It’s important to know what you have and how the records are used. During the inventory, gather information on why the record is created in the first place, so similar business functions can be subsequently grouped together. ARMA International® provides more detailed information about this process, including inventory forms.\nAfter you get the RIM inventory results you can start grouping similar records. For instance, you might group together “correspondence,” “letters,” and “email.” Email is problematic because it is often used for topics unrelated to business. A clear statement about emails — how to write them, what is or is not a business-related record, etc. — should be part of your policy statement. Again, ARMA International® has good resources.\nA retention schedule follows RIM inventory. A good retention schedule will meet legal requirements, operational requirements, and historical value when determining how long a record should be kept. The minimum time a record needs to be kept is the legal requirement. It’s OK to keep records longer if that is the policy stated in the retention schedule. “Just in case” is not a valid operational reason to keep something longer than the legal requirement. It’s also better to use “life of the legal entity (LOE)” instead of “permanent,” for records that need to be kept that long. “Permanent” causes issues when businesses are dissolved or go bankrupt; avoid them by using “LOE” in your schedule.\nThe retention schedule can also explain how to dispose of records that have met their retention period. Any record with personal information should be either shredded or permanently deleted.\nNote: Many software programs offer a recovery period of up to 48 hours after you delete a digital record from your system, allowing the record to be discoverable in an audit or litigation. Leave deletion of digital records to the information technology department. These folks know how to permanently delete electronic files. Consider an electronic records management (ERM) or electronic content management (ECM) to maintain metadata about the record or information with a status of destroyed per schedule. (Important for audits and litigation!)\nFactor at Least Six Months to Implement\nA policy statement with related procedures may take as long as six months, by the time everyone agrees on them. Allot seven to eight hours per employee to inventory the records each creates, three to six months to research retention periods (operational being the longest) to create the schedule, and three months to source and implement an ERM/ECM application. Review and update the inventory and schedule every 12 to 18 months, and refresh training on the RIM program every 12 to 18 months.\nARMA International®: www.arma.org/\nDocumentum by EMC:\nOnBase by Hyland®: www.onbase.com\nPaperVision® Enterprise by Digitech:\nFileBound by Upland: www.filebound.com/\nCheryl Ahrens Young, CIP, APMD, CDIA+, CTT+, ecmP, ermM, has been employed over the last 30 years as trainer, records manager, image capture analyst, contracts manager, and project manager with specific expertise in RIM projects. Young is pursuing an Information Governance Professional (IGP) designation. She earned a Bachelor of Arts in Psychology from the University of California, Riverside, and has taken graduate level courses in Business Administration, Information Systems at California Polytechnic State University, Pomona. Young is the senior project manager at Western Integrated Systems for RIM projects. She is past chair for the ARMA International 2004 Conference and past Pacific Region Manager. Young is vice president of ARMA’s Orange County, Calif., chapter and is a member of the Project Management Institute, AIIM and IOFM.\nLatest posts by Guest Contributor (see all)\n- I Am AAPC: Mita Shah Morar, PA-C, MBA, CIRCC, CPC, CPMA - September 19, 2018\n- VALUE JOURNEY: Design A Roadmap for Success - September 11, 2018\n- Pathology Key Words for Correct Coding: Know Their Differences - September 11, 2018', 'Preparing to Go Public? Address SOX Compliance Early\nThe Sarbanes-Oxley Act of 2002, commonly referred to as SOX, requires public companies to report annually on their internal controls for financial reporting within their Form 10-K. The purpose is to protect stakeholders and provide assurance on the adequacy of controls in place to prevent fraud and financial misstatement. Public companies are required to comply with SOX, but compliance is often an overlooked or underestimated step for a company preparing to go public.\nAccording to many experts, the process should start about 18-24 months prior to going public, and studies have shown that some of the most successful IPO companies operated under public company rules (including SOX compliance) for 12 months prior to going public. Demonstrating SOX compliance prior to going public shows management’s commitment to protecting future stakeholders by identifying risks, gaps and weaknesses and remediating them, and implementing industry best practices for financial, operational and technological processes. This assures that the right people are in place and the company is fully aware of their risk areas.\nKey SOX Provisions for Companies Considering an IPO:\n- Section 302 – Mandates senior officers of a public company certify that they have established and maintained internal controls to ensure the accuracy of company information found within their reports.\n- Section 404 – Requires management and external auditors to report on the adequacy of the internal control over financial reporting.\n- Section 802 – Outlines rules for record keeping including a) destruction and falsification of records, b) defined retention periods for storing records, and c) specific types of business records that must be stored.\nBenefits for a SOX-compliant Company Include:\n- Prioritizing risks,\n- Strengthening internal control structures,\n- Improving performance audits, and\n- Establishing centralized, automated financial reporting.\nSOX compliance is a big undertaking for any business, and companies should consider the available resources they have for not only design and implementation of compliance, but also for maintaining future compliance. Becoming SOX-compliant can be a time-consuming and expensive endeavor, and employees may push back on procedural or system changes that require more of their time. Bringing in a qualified team of experts to help build your company’s SOX program, will expedite the process and ensure that your company is fully compliant.\nFor more information on navigating SOX and/or preparing for an IPO, please connect with us.\nMaterial discussed in this communication is meant to provide general information and should not be acted on without obtaining professional advice tailored to you or your company’s individual and specific needs. Any tax advice contained in this communication (including any attachments) is not intended or written to be used, and cannot be used by any person or entity, for the purpose of (i) avoiding penalties that may be imposed on any taxpayer or (ii) promoting, marketing or recommending to another party any transaction or matter addressed herein. This information is for general guidance only and is not a substitute for professional advice.\nThe information contained herein should not be construed as personalized investment advice. Investment in securities involves the risk of loss, and past performance is no guarantee of future results. There is no guarantee that the views and opinions expressed in this document will come to pass. Historical performance results for investment indexes and/or categories generally do not reflect the deduction of transaction and/or custodial charges or the deduction of an investment-management fee, the incurrence of which would have the effect of decreasing historical performance results. There can be no assurances that your portfolio will match or outperform any particular benchmark.\nInformation presented was obtained from sources deemed qualified and reliable; however, MFA makes no representations as to accuracy, completeness, suitability, or validity of any information within this communication and will not be liable for any errors, omissions, or delays in this information or any losses, injuries, or damages arising from its display or use. Any forward-looking statements are believed to be reasonable; however, MFA gives no assurance that such expectations will prove to be correct.']	['<urn:uuid:9a428782-bf1c-4198-8ad3-32fdbda18496>', '<urn:uuid:21ca8236-d936-4da9-89cc-e89f49ca8a57>']	open-ended	direct	long-search-query	distant-from-document	comparison	novice	2025-05-12T21:31:42.699391	11	104	1945
72	What are the main differences between nuclear deterrence during the Cold War and the challenges countries face with nuclear weapons today?	During the Cold War, nuclear deterrence was straightforward: attack the United States or its allies, and you risk devastating counterattack. The mutual assured destruction concept between the US and Soviet Union helped prevent escalation. Today, deterrence is more complex, encompassing nuclear, conventional and nonkinetic capabilities, limited missile defenses, and access to space and cyberspace. The Indo-Pacific region adds additional complexity with multiple nuclear-armed states like China, India, Pakistan and North Korea, most of which are not party to key nuclear treaties.	['Stratcom Broadens Dialogue on Changing Nature of Deterrence\nBy Donna Miles\nAmerican Forces Press Service\nOFFUTT AIR FORCE BASE, Neb., April 3, 2013 No one can accuse Air Force Gen. C. Robert Kehler, commander of U.S. Strategic Command, of seeking out “yes men.”\nThe Ohio-class ballistic missile submarine USS Alabama pulls into Naval Base Kitsap-Bremerton, Wash., after returning from a patrol. Recognizing the continuing importance of nuclear deterrence, U.S. Strategic Command is striving to increase dialogue and understanding about other forms of deterrence in light of 21st-century threats. U.S. Navy photo by Chief Petty Officer Ahron Arendes\n(Click photo for screen-resolution image);high-resolution image available.\nFor the past four years, he has hosted an annual symposium near the Stratcom headquarters here to shed new light on the whole concept of deterrence -- and how the command strives to provide it on behalf of the Defense Department.\nGetting those far-flung views was relatively easy during the Cold War, when the discourse filled lecture halls and library shelves around the world.\nDeterrence was pretty straightforward at that time: attack the United States or its allies, and you risk a devastating counterattack. Recognition by both the United States and the Soviet Union of the consequences -- mutually assured destruction -- is considered a key factor in preventing the Cold War from escalating into an outright conflict.\nBut while nuclear deterrence remains vital to U.S. security today, Kehler said, it’s not enough to protect the United States against the range of threats it now faces.\nStratcom’s primary mission objective is “to deter strategic attack on the U.S., our allies and partners by making anyone who might contemplate such an attack recognize that they will not achieve their goals and will pay an extraordinary price if they try,” he told Congress earlier this month.\nProviding that deterrence is no longer a “one-size-fits-all” proposition, he explained, and has to be tailored to each individual threat and challenge.\n“In practice, 21st-century deterrence encompasses a wider range of complementary tools”,” the general said: nuclear, conventional and nonkinetic capabilities; limited missile defenses; and unfettered access and use of space and cyberspace. And reflective of the U.S. emphasis on “whole-of-government” approaches to national security challenges, he said, it also includes “soft power” contributions from across the interagency spectrum.\nHow to bring this all together is heady stuff, and opinions vary widely in how to provide this 21st-century form of deterrence.\nIt’s a subject that Kehler and his Stratcom staff focus on every day, working in lock-step with their military and U.S. government partners to maintain strong deterrence that protects the United States and assures its allies and friends, Stratcom officials said. Yet ironically, officials here note a dearth of the kind of discussion that flowed freely when the challenge was so much less daunting.\nSo Stratcom is actively stirring up debate and soliciting diverse viewpoints as it strives to shape an approach to deterrence for now and into the future, Pat McKenna, Stratcom’s plans evaluation and research division chief, told American Forces Press Service.\nThe command sponsors an annual symposium that brings together hundreds of experts from the military, government, industry and international realms and academic leaders to delve into the subject. Stratcom also sponsors an annual writing contest to encourage critical thinking among students and professors at senior military colleges, as well as civilian colleges and universities, on the theories of strategic deterrence and how it is applied.\nThe Gen. Larry D. Welch Writing Award is named to honor the former Strategic Air Command commander who went on to become the Air Force chief of staff and, after retirement, president of the Institute of Defense Analysis.\nThe goal of these efforts is to stimulate discussion that generates a deeper understanding of the issue, McKenna said.\nThe upcoming symposium, scheduled for July, will include several keynote speakers and panel discussions that highlight the full spectrum of viewpoints.\n“What we strive to do is bring those alternative points of view together in a public forum and invigorate the debate,” McKenna said. “We want this to be a catalyst to push that debate along and get a robust discussion going on so we can examine many of the issues.”\nDelving into the issue helps to provide insight into what promotes deterrence and what doesn’t, and how seemingly positive approaches can backfire if not applied correctly, he said. The discussion enlightens leaders at Stratcom and across the Defense Department and U.S. government as they strive to achieve deterrence in a changing geopolitical environment, he added.\n“The symposium engenders and reinvigorates the debate so we can understand those issues,” McKenna said. “And understanding the diversity of opinion on the subject is important as not only Stratcom, but the U.S. government as a whole strives to implement policies and procedures to achieve their objectives.”', 'The Future of Arms Control is Global: Reconsidering Nuclear Issues in the Indo-Pacific\nOn Feb. 1, a reporter asked President Donald Trump if the announced U.S. withdrawal from the Intermediate-Range Nuclear Forces (INF) Treaty was as much about the Western Pacific as it was about Russia. In response, Trump affirmed his view that for the arms control pact to work, more countries would need to be added: “I hope that you’re able to get everybody in a big and beautiful room and do a new treaty that would be much better.”\nThe good news is that the Indo-Pacific region is flush with big, beautiful rooms where experts and officials can and should gather to discuss the future of arms control. What such gatherings might look like is a pressing but under-explored question. Rectifying this imbalance must begin with a recognition of the many ways in which Indo-Pacific nations shape the international nuclear landscape. This region is likely to play a variety of roles — with and without the United States and Russia — in shaping the future of global arms control. For the United States, it will be imperative to launch new arms control dialogues with regional allies and find productive ways to engage with China and identify potential mutual gains.\nNuclear Affairs in the Indo-Pacific\nIn the United States, discussion of Indo-Pacific nuclear weapons issues often centers on how existing and developing nuclear weapon capabilities in the region affect U.S. strategic calculations. Imagining future arms control pathways that could more deeply integrate the Indo-Pacific requires recognizing that along with the region’s mix of both nuclear weapons advances and restraint, its nations have long played important roles in arms control and nuclear risk reduction.\nIndo-Pacific countries have engaged in numerous efforts to mitigate nuclear weapons threats. All 10 members of the Association of Southeast Asian Nations (ASEAN) are party to the region’s nuclear weapon-free zone. Australia has long played a leading role in strengthening export control regimes and countering weapons of mass destruction. Japan has for decades led in international nuclear diplomacy: For example, it has for 25 straight years introduced a United Nations resolution calling for the eventual elimination of all nuclear weapons, often angering both nuclear-armed states and countries that seek faster progress toward nuclear disarmament. More than a dozen Indo-Pacific countries signed onto the 2017 nuclear weapons ban treaty. Many are partners in the Missile Technology Control Regime.\nIn spite of these strong voices and actions — and the risks stemming from the gradually expanding nuclear weapons capabilities in the region — there is insufficient exploration of how to more effectively include Indo-Pacific countries in arms control concepts. This reflects the fact that the United States and the Soviet Union have rightly long dominated the conversation because they hold tens of thousands of nuclear weapons at Cold War heights and each still deploys more than 1,000 strategic nuclear weapons today. China in particular has long held that the United States and Russia must bring their numbers of nuclear weapons far lower (closer to the approximately 300 or fewer other possessing states likely have) before it will engage in meaningful discussions on caps and reductions.\nBeyond numbers mismatch, the sheer complexity of nuclear issues in the Indo-Pacific has stalled the region’s fuller integration into arms control discourse. Most of the region’s nuclear weapon-possessing countries are not party to the Treaty on the Non-Proliferation of Nuclear Weapons (NPT), including India and Pakistan. This clouds all discussions on nuclear weapons, civil nuclear cooperation, and broader security issues. The international community constantly contends with the question of how to better bring non-NPT nuclear weapon-capable states into international norms and frameworks while avoiding steps that may encourage other countries to withdraw from the treaty or otherwise lower nuclear governance standards. Moreover, of course, the North Korean threat has loomed for decades, and its recent progress in nuclear and missile capabilities — and opening to diplomacy, however sincerely or insincerely it is being pursued — further complicates the region’s nuclear landscape.\nAmid such complexity, no singular, sweeping approach to arms control will work in the Indo-Pacific — just as U.S.-Soviet and later U.S.-Russian arms control progress resulted from a series of diverse agreements and policy shifts over many decades of effort. The international community should develop a menu of options that over time could form a similarly strong web of nuclear constraints.\nA Multitude of Options\nThe United States and Russia are both Pacific nations, so the trajectory of nuclear measures involving these two countries will certainly shape the future of Indo-Pacific arms control. Both countries must resume bilateral work on steps to reduce their nuclear arsenals, and future U.S.-Russian arms control agreements should certainly include mechanisms for additional countries joining over time. Yet this will be only one aspect; there are many options for arms control measures to extend to the Indo-Pacific. Some may include active U.S. and Russian participation, while others will not.\nIn substance, future arms control pacts may be most likely to focus first on countries abstaining from developing or deploying new types of nuclear capabilities. While China, India, and Pakistan are all striving toward full nuclear triad capabilities, beyond that, there are other, more specific capabilities that no Indo-Pacific countries currently possess or that are only present on a very small scale. These capabilities present an opportunity. Countries across the region likely share a common interest in preventing the proliferation of nuclear-armed cruise missiles, given their uniquely destabilizing characteristics. A measure limiting these missiles would not require China, India, or Pakistan to cut back their current nuclear arsenals; it would only require them to refrain from fully developing stocks of this class of nuclear weapons.\nThe same may also hold for coupling nuclear and hypersonic capabilities, and for marrying stealthy missiles to stealthy delivery platforms in air-based systems. Giving countries confidence that such systems are only armed with conventional payloads would significantly reduce the risk of nuclear war.\nNew arms control concepts may also focus on specific attributes that cross a variety of systems. For example, global awareness is growing regarding the danger of new nuclear weapon capabilities that introduce unnecessary risks of miscalculation and inadvertent escalation. International dialogue on reducing ambiguity is expanding. New arms control agreements might focus on dual-capable missile systems that may be either nuclear or conventional. Multiple countries, particularly those in close proximity, agreeing to keep specific systems conventional-only could prove invaluable. Alex Bell and Andrew Futter have recommended that new arms control concepts could emerge from discourse on “points of agreement on dangers” such as unauthorized launch of nuclear weapons.\nJust as they have in the past, future nuclear agreements will take a variety of formats. Formal treaties with strong verification are critically important, but they are not the only option. New arms control deals may begin with political commitments, and may or may not evolve over time into legally binding agreements. Future pacts may be bilateral, “minilateral,” or multilateral.\nWe’ve heard these and other ideas often in our conversations with officials and experts in many Indo-Pacific nations. From a U.S. perspective, it is important to develop tracks to explore many creative approaches, yet figuring out how to do so will require care and sequencing.\nStart With Allies\nThe United States should launch new tracks of dialogue with its Pacific allies on how the future of arms control can enhance their security.\nNotably, there are several existing deterrence-focused dialogues among U.S. and allied officials. But as we wrote recently, these dialogues often tilt toward merely justifying U.S. decisions: “Dialogues that are intended to reassure allies can easily bleed into sales pitches that seem designed to convince our partners to support new nuclear capabilities — even those that may seem redundant or excessive, or put them at greater risk.” Moreover, there is a dearth of similarly in-depth bilateral conversations about the role of arms control in bolstering regional security.\nIn many ways, the U.S.-Japan alliance remains strong and grounded in deep, respectful collaboration. However, nuclear issues are frequently thorny. In 1991, as part of his Presidential Nuclear Initiatives, President George H.W. Bush announced the withdrawal of nuclear Tomahawk sea-launched cruise missiles from the Asia-Pacific. Many Japanese officials were taken aback by the near-total lack of consultation with or advance notification to their country. This history still colors bilateral discourse and official Japanese statements regarding U.S. nuclear weapons decisions — as does, of course, the U.S. decision to drop nuclear weapons on Japan in 1945. In 2016, then-foreign minister Fumio Kishida hosted his G7 counterparts for a forum on nuclear disarmament and nonproliferation in Hiroshima, the target of the first U.S. nuclear attack on Japan. Though the final foreign ministers’ statement acknowledged the “human suffering” caused by the use of nuclear weapons, for months in advance U.S. officials pushed against acknowledging the humanitarian effects of nuclear attacks. In 2018, the United States abstained from backing Japan on its aforementioned U.N. resolution on disarmament. These past challenges show the urgency of a concerted bilateral dialogue on how future arms control agreements can advance U.S.-Japan alliance interests.\nClose coordination with South Korea on these challenges is essential as well, and not just because the denuclearization of North Korea is a shared imperative. The United States and South Korea should work together to prevent the spread of more usable nuclear capabilities and risky deterrence concepts that place the Korean Peninsula in the crossfire. This is an important lesson from the INF Treaty, which reduced the threat of nuclear warfighting in Cold War Europe by eliminating destabilizing classes of nuclear weapons that were most likely to be used on that territory.\nFinally, public debate on nuclear weapons in Australia has expanded in recent years, highlighting the importance of deep U.S. Australia dialogue on arms controls. Ideas span from the country signing onto the ban treaty, to hosting U.S. nuclear weapons, to developing an Australian nuclear arsenal.\nThe United States should also continue supporting Indo-Pacific countries in expanding their exchanges with European and NATO counterparts, and backing diplomatic leadership by Japan and Australia in minilateral fora like the Non-Proliferation and Disarmament Initiative. Deeper collaboration, building on the region’s already substantial efforts to reduce nuclear risk, would help define a strong leadership role for the United States and its allies in Indo-Pacific arms control and serve as important tool of U.S. reassurance.\nFind Ways to Talk to China\nThere is plenty of common ground that allows the United States and China to begin exploring common interests that could inform future arms control and nuclear risk reduction measures. Some contours of how such cooperation may proceed are already clear.\nThe notion that China can simply roll into existing nuclear arms control frameworks, such as the longstanding Russian proposal to multilateralize the INF Treaty, is unrealistic. Likewise, frameworks focused solely on nuclear weapons numbers will not fly so long as China remains so far below any sort of parity with the United States or Russia. On top of China’s already complex strategic calculations with Russia and the United States, it must also account for dynamics with India, Pakistan, and North Korea, among other considerations.\nStill, China may have a strong interest in preventing escalation risks along its borders, through an uptick either in the numbers of non-strategic nuclear weapons or in the types of nuclear weapons in the region. Highly focused arms control measures could help address specific challenges like these.\nIn our private discussions in Beijing since leaving government service, some of the strongest concerns we’ve heard focus on nuclear capabilities that could undermine China’s command and control or second-strike abilities. Early warning of attack seemed to play a central role in our interlocutors’ perceptions of stable deterrence. James Steinberg and Michael O’Hanlon offered similar observations in their 2014 book, Strategic Reassurance and Resolve: U.S.-China Relations in the Twenty-First Century, in which they suggested promoting mutual trust by “reducing as much as possible the ambiguity and uncertainty associated with unilateral security policies,” and providing “timely indicators and warnings of any less benign intentions to allow each side adequate time to adjust its own policies.”\nEven if it begins with unofficial or academic discussions, the United States and China must work toward better understanding of one another’s definitions of strategic stability. Tong Zhao of the Carnegie-Tsinghua Center for Global Policy recently noted how jarring to Chinese officials it was when the 2018 U.S. Nuclear Posture Review “conspicuously dropped the previous US commitment to maintaining strategic stability with China.” He rightly emphasizes the importance of dialogue to explain each side’s understanding of strategic stability. Other nuclear experts have recommended the same step as a multilateral exercise to identify nuclear risk reduction steps of common interest to countries across Asia, Europe, and the Americas.\nIn the immediate term, the region is paying attention to questionable claims that INF-violating weapons would significantly expand U.S. military options vis-a-vis China. Statements by some defense leaders and experts that intermediate-range missiles would expand U.S. optionality for saving money while helping to deter China are certainly being heard across the Indo-Pacific. This commentary unnecessarily raises tensions with China and puts allied publics on edge at a time when it is important to be building political will for meaningful engagement.\nAmerica’s current footprint of conventional and nuclear sea- and air-based systems and some elements in development, like the new B-21 long-range bomber, are sufficient for U.S. needs at this time. There are valid arguments for thought processes for examining potential U.S. military options that may be opened if poor U.S. and Russian decisions lead to the total collapse of the INF Treaty (as defense planning must include both smart and terrible policy decisions), and we agree it is long overdue that the United States diminish the separation between its trans-Atlantic and Indo-Pacific strategies. But such strategy work and military analysis can be done in ways that avoid blurring potential nuclear and conventional intentions. This planning can also be conducted in private with careful communications to both allies and countries like China to minimize the risks of Indo-Pacific countries misinterpreting U.S. actions. Most importantly, equal or greater effort must go toward developing concepts for arms control and restraint. In the near term, new U.S. moves to deploy ground-launched intermediate-range missiles as part of its response to threats from China would be a mistake. Beyond the lack of military need, this would undermine any prospect of even unofficial U.S.-China dialogue on the future of arms control.\nContinued U.S.-Russian progress in nuclear weapons reductions is imperative, but the future of arms control is global. Countries of the Indo-Pacific region will play a variety of roles in shaping that future.\nFor America’s part, it should turn back toward smarter nuclear decisions and work on enhancing its moral authority. In the Indo-Pacific in particular, the United States will have no credible voice in helping to halt rising nuclear risks so long as it pursues its own new nuclear weapons capabilities. And while we welcome the prospect of countries in this critical region advancing arms control agreements themselves, it will best serve U.S. interests to remain deeply involved.\nIncorporating the Indo-Pacific into the global future of arms control will be anything but simple. But if the INF Treaty taught us anything, it is that history-altering nuclear agreements take years of work to shape, negotiate, and implement, but the security dividends they pay can last generations.\nAndy Weber is the former Assistant Secretary of Defense for Nuclear, Chemical, and Biological Defense Programs (ASD-NCB) where he directed the Nuclear Weapons Council. Christine Parthemore was formerly the senior advisor to the (ASD-NCB). Both have lived and worked in the Indo-Pacific region and are now with the Council on Strategic Risks. You can follow Weber on Twitter @AndyWeberNCB and Parthemore @CLParthemore.\nImage: MSGT Jose Lopez Jr.']	['<urn:uuid:7ee1dced-373e-40f3-a721-29fef81dbe44>', '<urn:uuid:41464494-3a0b-4945-8183-04df42f2fedb>']	factoid	direct	verbose-and-natural	distant-from-document	three-doc	novice	2025-05-12T21:31:42.699391	21	81	3424
73	water vapor resistance breathability rating values	The breathability ratings are: less than 6 is very good/extremely breathable, 6-13 is good/very breathable, 13-20 is satisfactory/breathable, 20-30 is unsatisfactory/slightly breathable, and above 30 is unsatisfactory/not breathable.	['KNITTED FABRICS: EVALUATION OF ANTIMICROBIAL ACTIVITY\nUNI EN ISO 20743:2007 method\nAbsorption test with inoculum of the microorganism done directly on the fabric sample. The standard predicts 18-24 hours of contact but it can be adjusted to the product use (for example: if the athlete wears a shirt for 6 hours, it is possible to assess the antimicrobial effect for the actual time that the garment is used).\nThe microorganisms generally used to test are:\n- Streptococcus aureus (human)\n- Klebsiella (human)\nThe following potentially interesting microorganisms may be added (in order of importance):\n- Escherichia coli (human)\n- Pseudomonas aeruginosa (environmental)\n- Bacillus subtilis (environmental)\nUNI EN ISO 20645:2004 method\nThis test evaluates the release of the active antimicrobial (Ag) ingredient on the fabric in order to verify if there could be risks for the wearer (penetration in the organism through the skin) and/or for the environment (release during the wash). To verify if a halo forms around the fabric sample that is in contact with the selected microorganisms. This is a qualitative test.\nBoth above-mentioned antimicrobial tests may be performed on the fabric as is and/or after a number of washes.\nKNITTED FABRICS: EVALUATION OF THERMO PHYSIOLOGICAL PROPERTIES (COMFORT)\nISO 11092:2014 method\nDetermination of physiological properties. Measurement of thermal resistance and to water vapour in stationary conditions (proof of breathable hot plate). This method serves to determine both the thermal resistance (Rct) and resistance to water vapour (Ret) relating to textiles, membranes, coatings, etc. by simulating heat and mass transfers that are produced close to human skin. The equipment used in this type of test is called Sweating guarded hotplate or “Skin Model”. It is an instrument that simulates the process of heat and body humidity transfer occuring at the interface of the human body. The unit is placed within a weather chamber at a maintained constant temperature and humidity with an air flow presence.\nMeasurement of thermal Resistance\nThe thermal resistance of the apparel represents the resistance to heat flow compared to the clothing and the layer of air present between the clothes and the skin. To determine Rct, the test tube is placed directly on the perforated plate maintained at a temperature of 35°C. Air temperature is of 20°C with a relative humidity of 65%. The test consists in measuring heat flow through the surface of the test tube as soon as the state of equilibrium is reached.\nMeasurement of Water Vapour Resistance (Ret)\nThe evaporative resistance Ret (Resistance to Evaporative Heat Transfer) evaluates the breathability of a fabric, in other words, its capacity of humid air passing through. Breathability is generally linked to the porosity of the material. A more breathable material lowers the possibility of creating condensation on the surface of the material. More breathable materials ensure that body humidity is removed more easily, thus decreasing the condensation of sweat and increasing comfort. To determine RET, both the unit of measure and air are maintained at 35°C (isothermal) with relative humidity of 40%. A cellophane membrane is placed above the porous plate that is permeable to water vapour but waterproof to liquid water. Water is sent to the plate generating vapour simulating perspiration of the human body. The fabric to be tested is placed over the membrane, usually with the side in contact with the human body that is in contact with the plate. The heat flow required to maintain the plate at a constant temperature represents a measure of evaporation water flow that gathers the resistance to the test tube water vapour. The lower the resistance, the greater the breathability of the fabric.\n- < 6 Very good or extremely breathable. Comfortable during intense activity.\n- 6-13 Good or very breathable. Comfortable during moderate activity.\n- 13 – 20 Satisfactory or breathable. Not comfortable during intense activity.\n- 20 – 30 Unsatisfactory or slightly breathable. Not comfortable during moderate activity.\n- > 30 Unsatisfactory or not breathable. Not comfortable and short tolerance period.']	['<urn:uuid:cc1a4ce9-f455-417d-a43f-c1e354f7eec6>']	factoid	direct	short-search-query	similar-to-document	single-doc	expert	2025-05-12T21:31:42.699391	6	28	660
74	How do board games like Settlers of Catan and mobile games in app stores compare in terms of their influence on gaming evolution?	Settlers of Catan was a breakthrough that helped people realize there were better games than traditional ones like Monopoly or Life, breaking Euro-gaming to the world. Similarly, mobile games in app stores have created a 'Cambrian explosion' of games, leading to meteoric growth in game development and demonstrating that even simple games from 20 years ago can still be engaging and fun on modern platforms.	"['Co-founder and designer director of Mohawk Games, Soren Johnson studied both history and computer science. This gave him a perfect combination of skills and knowledge to work on the Civilization franchise at Firaxis. He served as co-designer of Civilization III and lead designer of Civilization IV. Johnson also worked at EA, serving as lead designer of Dragon Age Legends and senior designer of Spore.\nWhat will you be talking about at DICE 2016?\nI\'ll be part of a panel that discusses the history of the Civilization. The franchise turns 25 this year. The panelists are the people -- the programmers and designers -- that made Civ I, Civ II, Civ III, and Civ IV. For my part, I was the co-designer of Civ III and the lead designer of Civ IV. I\'ll be talking about how we extended the series in Civ IV from where it began.\nWhat was it like for you working on Civ III and jumping into one of the biggest PC gaming franchises of all time?\nI was far too young to feel a lot of pressure. [Laughs] It was literally my first job out of college. For a lot of people, their first experience in the real world is what they consider normal. For me, it just seemed like this was what I was supposed to do. It was the perfect job for me. Civ was always one of my favorite games. Beyond that, I\'m a huge history guy. I actually double-majored in history and computer science. I\'ve always joked that Civilization was essentially the only job for me.\nThe Civ III team was hiring right when I was getting out of school, so it worked out. It worked out really well. I knew it was an incredible opportunity, so I was ready to work as hard as I could and learn as much as I could in order to help get the game out.\nOne of the things that you\'ve always been big on is the relationship between tabletop board games and digital games. What\'s the correlation you see there?\nMaking strategy games, I think a lot about what makes them work and what people enjoy about them. When you start off as a designer on a game like Civ, there are a lot of different design ideas that you want to try. Oftentimes you come up with ideas that are complicated -- ideas that are so complicated that they\'re difficult to explain easily to the player. They might not be hard to code, but there\'s not necessarily an easy way to communicate that idea with the UI or the graphics or even outright telling the player through text. You can still implement that idea and get the game mechanic in there, but what you\'ll sometimes see is that you have something that\'s potentially fun, but people just won\'t get it. They won\'t see what\'s happening directly in front of them.\nA good example of this would be how religion was originally spread in Civ IV. Originally, we had this complicated system where religion was spread naturally -- along trade routes, rivers, contact with other civilizations. That all sounded like fun gameplay, but players weren\'t able to see the mechanic in action. We ultimately changed it to something that was much more direct. ""You want to build a religion? Fine. Build a Christian missionary. Move them to that city. Press a button."" That\'s a very direct, clear, and transparent thing. It was a tremendous improvement. Players understood what was happening and had an active role in the process. It wasn\'t just words. The player was having fun.\nThe lesson of transparency is one of the reasons I talk about board games a lot. That\'s one of the things that makes board games so compelling. It\'s something that has become more obvious as the line between board games and video games has blurred. Take Hearthstone, for example. It\'s obviously a video game; it doesn\'t exist in a physical form. Yet people still conceive of it as a card game or board game. Part of it is the graphics and the UI, but the key reason for that perception is that the mechanics of the game are transparent. People have this expectation that they\'re able to see how all the parts work. That\'s a real important aspect of what makes strategy games work and you can really see the benefits of that in board games.\nDo you have any concerns about younger gamers that haven\'t played board games or maybe haven\'t played board games physically? That you might have to adjust your design strategies to entertain them?\nI definitely believe that younger gamers are going to be different from us, for sure. Though I don\'t necessarily believe that they\'re playing less board games. If anything, kids of this generation are playing more board games than previous generations. That industry doesn\'t appear to be slowing down. That said, I definitely think a lot about the youngest generation that\'s starting to play games and what their expectations are, because at some point there\'s going to be a huge distance between me and them. They\'re going to have very different expectations of what a game does, in terms of session length, complexity, and what it means to play with other people. I spend a lot of time seeing how people are playing games and learning about games on places like Twitch or YouTube. That\'s one thing that has changed a lot in the last three or four years -- how information on games spreads around now. Those are things I think about a lot.\nWhat are some of your favorite board games of all time?\nSettlers of Catan is one that has to be mentioned. It\'s maybe not my favorite game, but it\'s such an important one. It broke Euro-gaming to the world. For many people, it\'s the game that made them look past the games they grew up with. It made them realize that there might be better things out there than Monopoly or Life.\nIn terms of games that I think are the best designed, I have an enormous amount of respect for Reiner Knizia\'s Samurai. I think that game is an incredible piece of design. Every single part of that game fits together perfectly. It\'s just so well done.\nI have a lot of respect for 7 Wonders. It\'s a game that took the ""drafting"" aspect of Magic: The Gathering and turned it into a real game. As a nice side benefit, it\'s one of the few games where you can play with seven people and it doesn\'t go any slower than if you had three. That\'s a pretty remarkable thing.\nBattle Line is another one. It\'s probably the best two-player game that I can think of.\nThere\'s an older game called History of the World that I really love, especially coming from a Civ background.\nIs there a lesser-known board game that you\'d like to see more people play?\nI don\'t know if I\'d necessarily recommend it, because it\'s an older game, but I\'ll say Belter. Offworld Trading Company is based on this game from 1978. It\'s from Game Designers\' Workshop way back then. It\'s one of those old hex-based where you\'re moving asteroid miners around an asteroid belt. You\'re prospecting asteroids, you\'re mining ore, you\'re mining gas, and you\'re bringing it all back to the market. The prices go up and down relative to supply and demand. I remember playing a lot of Belter as a kid and being really inspired by it. It was the first thing that put me on the path to making Offworld Trading Company.', 'Will Wright, the creator of blockbuster franchises like SimCity and The Sims, discussed the future of games at the Game Horizon Live event in a live webcast this week. During his Q&A session, he discussed a wide variety of topics about where games are going.\nWright said that he was inspired by the “Cambrian” explosion of games (as in the meteoric growth of life during that epoch in Earth’s history) that has come from indie game development on app stores for smartphones, tablets, and other platforms.\nWright co-founded Maxis and created games like SimCity. Electronic Arts acquired the company in 1997, and Wright went on to create titles like The Sims and Spore while at EA. He left Electronic Arts in 2009 and set up a series of startups. His latest company is Syntertainment, which focuses on creative play and the interaction between entertainment and reality.\nWright has won multiple awards and was inducted into the Academy of Interactive Arts and Sciences’ Hall of Fame, and in 2007, he became the first game designer to receive a British Academy of Film and Television Arts (BAFTA) award. For young game designers getting started, Wright advised them to study nongaming fields so they can get more creative inspiration. Here’s an edited transcript of a portion of the Q&A with Wright at Game Horizon 2013. This is part two (click here for part one).\nWright: A lot of people design games for different reasons. If you talk to different game designers, it depends on where they are, what company they’re in, if they’re in a startup situation, and things like that. Some just want to build a successful company. They don’t really care what kind of game they make. They’re trying to figure out what the hot genre is. Other people are much more into artistic expression. These are more like your indie game developers. They want to push forward the state of the art in game design and try new, weird, interesting things. They want to strike out and discover new genres. Some people are very wedded to a particular genre. “I want to do the coolest first-person shooter set in World War II that’s been done so far.” It’s much more of a craftsmanship approach.\nAt the end of the day, as a player, you want to sit back and have a good time and enjoy whatever it is. I think that Sid Meier said once that sometimes game designers design games for players, and other times they design games for themselves. I think I’ve been guilty of that a couple of times. I’ve learned from that, as I step back.\nWith SimEarth, which was a game I did many years ago, I got really into the simulation. I spent all this time getting the climate connected to geology and things like that. From the player’s point of view, they didn’t see under the hood. They didn’t see all these bits and pieces moving around and how beautifully it worked together, this wonderful clockwork. For them, all they saw were these graphs and things. Playing SimEarth was like being put into the cockpit of a 747 in a nose dive with all the gauges spinning. They were pushing the controls around, trying to come up with some association between their input and the outputs. The simulation was too complex, I think, for players to enjoy and learn from. As a designer, I got totally into the simulation and forgot about the player. That was something I learned a lot from. So that does happen a lot.\nRight now, the market is so immediate and so Darwinian. With the metrics that we’re able to collect from players, we’re able to discover within a few hours where players are getting stuck, what they’re enjoying, what things are surprising us about their activities in the game. We’re able to learn from players so much more quickly that it creates a much tighter iteration loop.\nGH: Do you engage directly with your audience? Do people contact you and ask you questions and you reply, or is that just overwhelming?\nWright: [I engage] as much as possible. One of the most important roles for any game being released today is community management. Listening to the players, getting a sense of what the consensus is from the players, and responding back to them. Part of that is just care and feeding of the community. If they understand that you’re listening, you can spin it from going bad and turning into a flame war to bringing them on to your side and making them into your evangelists. Another big part of that is understanding what they’re enjoying about your game. What are the things about your game that are most frustrating to them? How quickly can you respond to that and fix it? As soon as you can start the conversation with your community, then you’re working with them. They’re helping you develop the game. We have the technology and the metrics and the communications to do that now very effectively and very rapidly.\nGH: You mentioned earlier the idea of player states, the different psychological states that a player can be in. Are there any particular states that you’d like to explore more? If you did that, would that lead to new game ideas, possibly?\nWright: Games, in some sense, started almost from the core brain, the reptilian brain. Early games were about aggression, defense, survival. The reptilian brain is what we were initially preying on because that was easy and effective. Now games are starting to get to the outer layers of our brain and a wider palette of emotions, and that brings the player into different states. A lot of games out there right now are very reflective or meditative, very relaxing, which is the opposite of the early games that were all about adrenaline.\nGames are also more external to the player now. In other words, it’s about the player being sent into some world and dealing with things around them. One of the areas that we haven’t explored much that I’m very interested in is how we get players to get more of a sense of their internal states, their own psychology. What is it like to be inside their head? Games can extract a lot of that from individual players and allow players to see interesting reflections of themselves in the game, not just some external virtual reality, some fictitious world. These self-reflective games to me imply a certain mental state of self-awareness that I think we haven’t explored at all in games.\nGH: Do you feel that narrative can get in the way of that kind of experience?\nWright: I think that there’s always going to be this tension between narrative and freedom or agency in games,. I could sit there and tell you a story. That’s a very passive experience. The chief strength of a game, though, the thing that almost defines a game, is the fact that you’re driving it. It is interactive, and you get to steer the situation. What I’ve kept with me from gaming are the unique stories that players have told me about things that they did in a game, things nobody else did. So I think of games more as a platform for players to play out stories than as a medium for the game designer to tell a story. To me, telling a story is almost, right off the bat, putting the player in a passive role.\nAs a game designer, I think it’s much more interesting to get on the path of “how do we teach our games to recognize the story that the player is trying to play out and then respond and support that story?” The game itself can become something of a director in the background, watching the player drive the story forward, and then try to support and amplify that dramatically.\nGH: Did you play Journey, which won lots of awards in the U.K. and the U.S. last year? I mention it because it found a very interesting balance between the narrative, which it tells you very little about – you discover it yourself – and this sense of emotion in the characters.\nWright: Yeah. I think Braid captured that as well. I’m not saying there aren’t ways to do that. It’s a very tricky thing, though.\nGH: We see a lot of readers arguing among themselves about the danger of next-generation consoles delivering more advanced visuals but forgetting about the gameplay aspect – just giving us more flash. Do you feel like that’s a valid concern?\nWright: Historically, we’ve had this kind of arms race in the games industry. Graphics, pixels, polygons. If I look back to the games I was playing 20 or 30 years ago, when there were just a few little pixels on the screen that made up a character, somehow I still cared about that character. I was able to infuse that character with life. Looking at what we can do with modern-day equipment, we can build these beautifully rendered humans and creatures, but their behavioral range is still about that of an ant. We’ve gone way, way ahead on the graphics side, but on the simulation and behavioral side, we’re still many steps back.\nAt the same time, I see that people are playing these games in the app market that we could have done 20 years ago. They’re still fun, though. People engage, and they spend time playing them. You can go down this path of hyper-realistic graphics, and it’s impressive and nice to look at for a while, but at the end of the day, it’s gameplay that’s going to keep me with a game for 30 or 40 hours.\nAs we get on that more realistic path, it also makes development much more expensive. That’s where you wind up needing millions of dollars to put something on the market that’s on par with everything else. That world will continue to exist, I think, but it’s going to become a smaller and smaller chunk of the overall games market. I’m not sure that it’s even all that relevant to the industry as a whole. It’s cool that we have these advanced graphics processors to create these hyper-realistic images, but gameplay is what makes up the broad landscape. Making things realistic is just one path through it. Who knows how many different types of innovative gameplay are out there that we haven’t yet discovered. That’s the frontier.']"	['<urn:uuid:d375b587-e68b-4899-bd21-62a723e3fb82>', '<urn:uuid:4cdfe900-a37d-4b30-a7a6-fcb669d40f44>']	factoid	direct	verbose-and-natural	distant-from-document	comparison	expert	2025-05-12T21:31:42.699391	23	65	3031
75	live events backstage safety revenue protection	The backstage area in live events can be a very dangerous place, especially for those unfamiliar with show site environments, which has led to increased focus on backstage design and safety. To protect against financial risks associated with these safety concerns and other potential issues, event organizers can purchase Event Cancellation Insurance coverage. This insurance helps limit risk from various circumstances including loss of venue and other unforeseen situations, protecting the significant revenue at stake - considering that in North America, concert tickets alone generated over $8 billion in revenue in 2017.	"['The value in bringing order to the chaos of backstage.\nIn the process of delivering shows within the live events industry, the backstage has traditionally been considered this elusive and hidden space where all the ‘magic’ happens. The reality is that backstage can be a very dangerous place, especially for people unfamiliar with a show site environment. For many years, the importance of the backstage look and layout was ignored compared to the front of house and show stage. Today, a new trend has emerged where the design of the backstage has become a great concern. […]\nUnderstanding the True Value of Technical Directors to Ensuring a Successful Event\nLive events come together in many ways. Some firms bring in their own internal teams, while others employ a professional known as the technical director to facilitate the process. Essentially, the job of a technical director (TD) is to make a show run efficiently. From beginning to end, the TD plays a pivotal role in ensuring the event is executed not only successfully, but also in the most efficient way for the client. The benefit of a TD can sometimes be undervalued because it can take solving a problem for their role to be recognized, when the true value of what they do is minimizing any issues on show site. But, whether an event goes off exactly as planned, or experiences challenges, a good TD is well worth the investment and the value they can provide pays for themselves, significantly more than the cost. […]\nThe Critical Importance of Choosing the Right People for the Right Job\nIn the wild west of live events, it’s clear that the technicians rule the world. They transform black boxes of miscellaneous gadgets and gizmos into these amazing displays and moving experiences for the audience. Whether a small, medium, or large show, it is critical to have a talented crew to ensure a successful outcome. The team in place can either make or break a show, and it is extremely important to take the proper measures to place the right people in the right jobs for your next event. […]\nWhat is the Catalyst for Change When Using Legacy Technology Providers?\nMany events take place year after year using the same technical providers and crew. A successful show can often lead to the show producer developing a trusted relationship with the technology provider, and a certain comfort level is established. As a result, the same team will often end up in the fortunate position of supporting that show every year. There are benefits to having that level of trust and familiarity with your team. There is less risk involved supporting an event with a time-tested crew, technology, and staging approach that has proven to be successful in the past. Given that, why would a show producer ever change staging providers? What is the catalyst for a change? […]\nEngage the staging company early in the creative process to maximize results\nThere are many players involved in developing the creative elements for a live event. The initial idea may come from someone in executive management at a firm, and then either an internal creative services team or outside production company develops the vision and messaging to bring the idea to fruition. There are also the people doing the actual presentations at the event. […]\nPreparing to Navigate and Negotiate the Pitfalls of the Venue Contract\nWhen planning a live event and trying to budget effectively, meeting planners may not realize what they can do to prepare on the front end to save time and money. During the planning process, companies commonly talk about equipment discounts, shipment costs, providing local labor and/or using in-house audiovisual to save money for the overall event budget. Although important items to consider, these types of savings occur much later in the overall planning, after the most important step in the process has already transpired – the execution of the venue contract. […]\nHow do you choose the right mix of technology for the right audience?\nThe fundamental essence of live events is driven by a group of people coming together to receive, understand and communicate a message. Whether in a hotel ballroom, arena, convention center, or any other type of gathering place, companies that put together these meetings are trying to create an experience for the audience. […]\nEveryone Loses with Current Paradigm of In-House Hotel Audiovisual Services\nA hot button issue right now within the live event industry is the current state of in-house hotel audiovisual services. Twenty-plus years ago, audiovisual departments within hotels offered reasonable commissions to the venue, which allowed them to provide a quality service to the customer at a reasonable price, and were able to make a profit. It was synergistic relationship between the provider and hotel, and it presented value to the customer. Today, the majority of in-house hotel audiovisual companies operate on a model with highly inflated commissions going to the venue, resulting in prices that have skyrocketed to rates well beyond market pricing. The value in this proposition for the customer is virtually nonexistent. […]\nEnsuring Your Staging Provider Has a Plan to Expect the Unexpected\nA phase commonly heard in the event staging industry is, “you are only as good as your last show”. This motto originates from the idea that despite having 10, 50, 100 or whatever number of previous successes on a show, people will only remember what happened the last time, especially if there was a problem. Clients expect to experience a perfect show as recognized by the audience. When problems or failures occur, these unfortunate circumstances can make an impression and overshadow any of the other successes. In order to avoid technical issues taking center stage during the event, it is critical for the event producer or planner to work with a staging provider that has a plan in place to deal with any challenges. […]\nIdentifying and developing new talent in the challenging industry of live events.\nLet’s face it, among the current generation of millennials entering the workforce, there is most likely a very small percentage that dream of becoming an audiovisual technician. Working in dark ballrooms, unloading trucks, and pushing cases does not attract the younger generation to the live events business, as opposed to other more mainstream industries. Within a growing industry that depends on a large pool of talented technical people to drive the success of each event, it is critical to attract and invest in a new generation of human capital that will take the world of live events into the future. […]', ""Concerts. Festivals. Fairs. Sporting events. Live theater. Corporate events. Whether it is indoors or outdoors, we love to be entertained and inspired. We thrive on learning from experts and hearing from speakers we admire. And this is why the events industry is enormous. In the United States, business meetings, conferences, tradeshows and the like provided over 5.9 million jobs.\nEvents require detailed planning and organization to produce a quality experience for all those involved. To put on an event requires a financial investment in the venue, promotion, and people to make it a success. But what happens when things outside the event organizer’s control cause the event to be canceled or postponed? The financial impact can be disastrous for its organizers.\nEvent Cancellation Insurance coverage can help to limit the risk for those unforeseeable circumstances like illness or injury to a key person, adverse weather, loss of venue, and other situations that arise. This coverage is purchased in advance of the event to protect the event organizers, sponsors, and other rights-holders. Depending on the event type, policies can be purchased up to three years in advance. It is best to get a policy in place as soon as planning for the event begins.\nWHO CAN PURCHASE EVENT CANCELLATION COVERAGE?\nCarriers are looking for those who have an insurable financial involvement in an event. Concert promoters, event organizers, band managers, fairs or festivals, sporting events and venues, theater and media companies, sponsors, advertisers, tradeshows, conferences and exhibitions are all insurable. Also, personal or private affairs such parties have coverage options available.\nWHAT CAN EVENT CANCELLATION COVER?\nEvent cancellation policies can be purchased to cover two types of losses, though policies do not cover both on the same policy. The first option provides coverage for the Costs and Expenses (excluding profit) involved in putting on the event. At the least, the event organizer may want to cover their expenses. If an event is established to earn money for its backers, the other option is to cover Gross Revenues (including profit) earned from the event.\nNot all events are created to generate revenue. Corporate meetings and some conferences are examples of this. These types of events would typically look to purchase an event cancellation policy that would cover Costs and Expenses. But concerts, theaters, sporting events, tradeshows, and many other events expect to earn a profit from ticket sales, food and drink sales, and merchandise. These types of events may consider purchasing both types of policies, one to cover Costs and Expenses and also a policy to cover Gross Revenues.\nFor the majority of event creators, merchandise, food and drink sales, and sponsor revenue comprise less than 20%.1 For 48% of events, ticket sales compose 60% to 100% of revenue. The music industry is no exception. It is big business every day of the week. In 2017 concert tickets sold in North America accounted for over $8 billion in revenue while the average revenue per show generated nearly $1 million in gross sales.3,4 That’s a lot of music. And a lot of revenue on the line.2\nBroadway is also a big business. According to The Broadway League, touring Broadway shows grossed $1.63 billion in 2019-2020 and Broadway productions in New York City grossed a total of $1.83 billion.5,6 These are only two segments of the live theater industry. Whether it is the big leagues of Broadway or small regional theaters and production companies, what happens when a show is sold out but must close for a night or a week due to unforeseen circumstances?\nIf a loss does occurs, it will be necessary to provide copies of receipts, refunds, invoices, and contracts to show proof of the expenses and costs for the event. When the policy is insuring Gross Revenues, the insured must establish with reasonable certainty the amount of the loss. Discuss this in advance with the underwriter so you know what kind of documentation or other information they will expect in the event of a claim. If it is a first-time event, this may be more difficult than a recurring event with historical evidence of ticket sales, merchandise sales, and other revenue generators.\nUNFORESEEN, UNCONTROLLABLE, AND UNEXPECTED\nTicketholders shelled out big money to see their favorite pop star. Now the headliner has blown out her knee and cannot go on. A fire damaged the theater making it unavailable for the local movie festival. A huge storm blew in, and the location for the company conference has no electricity.\nWhat types of situations may trigger your coverage under an event cancellation policy? Depending on the coverage you requested to be underwritten for, the following circumstances may apply:\n- Non-Appearance of an Individual or Team (Also Extends to Non-Appearance Due to Illness or Death of a Family Member)\n- Adverse Weather\n- Accident or Illness\n- Failure of Utilities\n- Natural Perils i.e. Earthquake, Windstorm, etc.\n- Public Transport Failure\n- Third-Party Strike Action\n- Denial of Access\n- Damage to Venue\n- Death, Accident, Illness or Travel Delay of Key Speaker/Performer\n- Terrorism or Threat of Terrorism (May add via Endorsement)\n- Civil Commotion (May add via Endorsement)\n- National or State Mourning (May add via Endorsement)\n- Communicable Disease (May add via Endorsement)\nMost of these coverages are negotiable and may be added via a rider or endorsement to your policy. If you are concerned about a particular peril, be sure to discuss it with your underwriter.\nBEFORE PURCHASING COVERAGE\nWhen purchasing an event cancellation policy, be aware that once the policy period begins, it isn’t cancellable either by the insured or the carrier. Premiums are typically due prior to the policy being activated or the start of the event, and they are fully-earned, meaning there are no refunds.\nDiscuss with your underwriter exactly what and how you need the policy to be structured based on the unique aspects of your event. The policy can be constructed to cover at different points such as including set-up and teardown. For example, a golf tournament might be set up for coverage to begin when the first ball is played and end when the last ball is played or to cover weather for the whole weekend. These types of decisions will affect pricing for the policy.\nALL POLICIES ARE NOT THE SAME\nEvent cancellation coverage varies widely, even on policies offered by the same carrier. Terms and conditions are dependent upon so many factors such as event attendance, location, performances, number of days, and more. Certain perils must be added via a rider or endorsement. For instance, Communicable Disease is not typically included but can be added. When seeking out insurance, make it a priority to understand the details of the quotes you receive before purchasing the policy. Some carriers offer an ‘All Cause’ option. Depending on the size of the request, there may be sublimits or minimum losses before coverage begins. Since Event Cancellation policies are flexible, coverage can be crafted specifically for certain perils such as adverse weather, non-appearance, and terrorism.\nNon-Appearance. For many events, Non-Appearance coverage isn’t necessary. However, if your event relies on the appearance of an individual or group, make sure you consider this coverage. Wedding officiants, guest speakers, performers, players, and sports teams are examples of this. This also extends if your caterer, florist, or other major vendor fail to provide their promised services. When a major star has to cancel their event, the loss can be steep. Lady Gaga suffered a hip injury and had to cancel her tour in 2013 resulting in an estimated $30 million in losses.8 She was forced to cancel 22 tour dates after sustaining an injury to her right hip. More recently, Madonna suffered injuries during her live concerts and had to cancel shows.9\nWeather. As weather patterns continue to shift across the globe, the likelihood of weather impacting an event is increasing. Any event can be impacted if weather is significant enough. Hurricane Harvey and Hurricane Irma as well as other exposures led to $10-$15 million in losses when many college football and other sporting events had to be cancelled, postponed, relocated, or reschedule.10\nBroadcast Events. What happens if an event is being broadcast or televised in the media, and the broadcast transmission fails? This situation calls for a separate policy. It can be purchased to cover if there is an interruption to a live TV signal when a third-party is responsible for the broadcast.\nExclusions are a Part of Every Policy; event cancellation is no different. Typical exclusions depending upon the policy structure include financial failure, insolvency, or default; withdrawal of support by any party; lack of sales; and, radioactive contamination. That’s right, don’t expect there to be coverage if an event has to be cancelled or rescheduled due to the insured’s financial troubles or if ticket sales are too low; likewise if a sponsor for the event backs out. While these are difficult circumstances, they aren’t what this type of policy is designed for.\nAlthough it is unlikely that events cancelled due to the coronavirus will be covered under a typical Event Cancellation policy, insured should check their policy language for specific exclusionary language. If insureds have a current inforce policy with a Communicable Disease rider there may be coverage. If you are putting on an event big or small, the best thing you can do to protect your investment and your revenue is to have the right event cancellation policy in place. Working with a producer who specializes in Event Cancellation can ensure you are aware of all of the options available in the marketplace to ensure your event investment is protected.\nContact your CRC Group Producer for more information.\n- Chris Peterson is the President of Hanleigh, a CRC Group Company, located in the Windsor, CT office.\n- Scott Lalonde is the Chief Underwriting Officer of Hanleigh, a CRC Group Company, located in the Windsor, CT office.\nFor over 35 years, Hanleigh has designed and underwritten products for high limit disability, personal life & accident, special risk contingency, and other niche insurance needs, including event cancellation. Our clients include some of the most successful professionals, athletes, entertainers, and organizations in the world.\nHanleigh utilizes a balanced approach of discipline, creativity, and industry-leading sales support, allowing us to provide insurance solutions quickly, competitively, and accurately. As an organization, we are committed to superior service within all aspects of our business and pride ourselves on experience, market knowledge, and unique, consultative strategies.\n- Economic Significance of Meetings to the US Economy, Oxford Economics for the Events Industry Council, 2019. https:// insights.eventscouncil.org/Full-Article/ArtMID/398/ArticleID/69/Economic-Significance-of-Meetings-to-the-US-Economy\n- 2019 Event Statistics and What They Mean for Your Events, EventBrite.com, January 01, 2019. https://www. eventbrite.com/blog/event-statistics-ds00/\n- Concert ticket sales revenue in North America 1990-2017, Statista.com, August 26, 2019. https://www.statista.com/ statistics/306065/concert-ticket-sales-revenue-in-north-america/\n- 2019 Business Analysis: The State of the Concert Business is ‘F**kin’ Perfect’, Pollstar, December 16, 2019. https:// www.pollstar.com/article/2019-business-analysis-the-state-of-the-concert-business-is-fkin-perfect-143035\n- Broadway Season Statistics, Playbill, 2019. https://www.broadwayleague.com/research/statistics-broadway-nyc/\n- Statistics – Touring Broadway, Playbill, 2019. https://www.broadwayleague.com/research/statistics-touring-broadway/\n- Wedding Wire Newlywed Report 2020, WeddingWire.com, 2020. https://go.weddingwire.com/newlywed-report\n- Cost of Lady Gaga’s Tour Cancellation Could Exceed $30 Million, Rolling Stone, February 15, 2013. https://www. rollingstone.com/music/music-news/cost-of-lady-gagas-tour-cancellation-could-exceed-30-million-90849/\n- Madonna Cancels London Concert Due to Injuries, But Promises 'I Will Keep Going Until I Cannot', Billboard, January 25, 2020 https://www.billboard.com/articles/columns/pop/8549040/madonna-london-concert-canceled-injuries\n- Event Cancellation: What Is A Game Worth To Your Institution?, AthleticDirectorU. https://athleticdirectoru.com/articles/event-cancellation-what-is-one-game-worth-to-your-institution/""]"	['<urn:uuid:28e97484-7597-425c-98e4-eed204903c89>', '<urn:uuid:9f4a5408-b30c-4ac4-85fb-65062d22694e>']	open-ended	with-premise	short-search-query	similar-to-document	multi-aspect	expert	2025-05-12T21:31:42.699391	6	92	2991
76	How do German dog breeds differ in their shedding patterns?	German Shorthaired Pointers are seasonal shedders that shed heavily in fall and spring, with a short, dense coat that repels dirt. German Shepherds, nicknamed 'German Shedders', have a medium-length double coat that sheds heavily especially during spring and autumn, causing what people describe as a 'snowstorm' effect.	['Dog Pregnancy Calculator And Timeline\nHave you ever come across a versatile, enthusiastic and challenging hunting dog breed? If you haven’t had such experience, get a German Shorthaired Pointer.\nDeveloped in Germany for the sole purpose of hunting, it can be described as a medium-sized, multi-faceted, gun dog. Agile with its strong legs, the German Shorthaired Pointer can hunt different types of game.\nSuitable for both water and land (i.e. game hunters and water retrievers), it has the ability to do any job. This dog had even served in the bomb detection department of the United States Air Force. An affectionate companion, it is a wonderful choice for active families, too!\nIf you are a kind of person who spends the most time in the outdoors, this breed is the best companion. It can also accompany you for long hikes or run. This dog will be the right pet for you with its distinctive abilities and features.\n- Long, cute floppy ears and strong, broad muzzle of this breed help it to retrieve or fetch even powerful game.\n- They are good at hunting waterfowls with its purled feet.\n- It will be a great watchdog for your family. This dog will likely bark if someone’s at the door or on your property, but won’t be aggressive with strangers or other pets.\n- You can easily maintain this dog with minimal grooming needs.\n- Its size and protective nature will help for the safe dawn training.\nThe only demand from these dogs is plenty of exercises for their physical and mental stimulation. If you’re able to provide it, the German Shorthaired pointer will be your best four-legged fur baby.\nLet’s get into the details of this amazing breed in this article.\nGerman Shorthaired Pointer Breed Characteristics Sheet\n- Origin: Germany\n- Size: Medium\n- Dog Breed Group: Sporting, Hunting\n- Purebred: Yes\n- Lifespan: 10 to 12 years\n- Height: 23 to 25 inches (males); 21 to 23 inches (females)\n- Weight: 55 to 70 lbs (males); 45 to 60 lbs (females)\n- Coat Appearance: Short, dense, thick\n- Coat Colors: Black, White, Reddish-brown\n- Temperament: Intelligent, Friendly, Affectionate, Outgoing, Energetic\n- Good With Children: Yes\n- Intelligence Level: High\n- Good With Pets: Yes\n- Hypoallergenic: No\n- Grooming: Less\n- Shedding: Moderate\n- Barking: Rare\n- Suitable For Apartments: No\n- Need For Exercise: High\n- Easy To Train: Yes\n- Good For First Time Owners: Yes\n- Health Issues: Gastric dilatation volvulus, Entropion, Hip dysplasia, Lymphedema, Von Willebrand’s illness\n- Litter Size: 7 to 12 puppies\n- Average Price: $600 to $800\nGerman Shorthaired Pointer History\nAs their name indicates, the German shorthaired pointers are natives of Germany. It is believed that their ancestors date back to the 17th century. And, this breed’s forerunner (either German bird dog or German Pointer) was a cross between Spanish Pointers and Bloodhounds.\nAs the crossbreed looked like a large hound-like dog with a keen nose, hunters wanted elegant, stylish hunting dog breeds with obedient nature. So, they used pointers imported from England to add style.\nThe hunters’ efforts resulted in a dog breed that works well in both land and water. But, Prince Albrecht zu Solms-Braunfeld of the Royal House of Hanover encouraged breeders to choose early specimens on the basis of function. Finally, a lean, athletic, intelligent, affectionate and all-rounder hunting dog was developed, which is the German shorthaired pointer today.\nThe breed was initially developed for multipurpose in the mid to late 19th century.\nIn 1925, Dr. Charles Thornton of Montana imported the first known dog of this breed to the United States of America and started breeding these dogs. But, only in 1930, the first German Shorthair dog “Greif v.d. Fliegerhalde” was registered with the American Kennel Club.\nBut, people sent their German shorthaired pointers to Yugoslavia for safeguarding them near the end of World War II. This affected the breeding of German shorthaired pointers as West German breeders couldn’t access these dogs. So, they developed these dogs from a limited gene pool.\nDuring the world wartime, this breed had significant advancement in the United States of America. But, most people believe that 1968 was the zenith for this breed in the United States. Today, the American Kennel Club ranked it as 19th among the 155 breeds recognized by it.\nGerman Shorthaired Pointer Interesting Facts\n- Good in sniffing bombs and other hidden explosives, the US Air Force has employed a well-behaved German pointer called “Haus” for official duty.\n- A multi-purpose dog with all-around hunting skills, the German Shorthaired Pointers are ready for hunting expedition round the clock.\n- A German Shorthaired pointer was elected as the Pet Mayor of Montclair.\n- The American Kennel Club conducted a licensed specialty show for these dogs on March 29-30, 1941 in Chicago.\n- As of 2015, 8 German shorthaired pointers have the “Preferred Agility Champion” title of AKC.\n- The German Shorthaired Pointer Parent Club in Anoka conducted the first AKC-licensed field trial on May 21, 1944.\nGerman Shorthaired Pointer Appearance And Coat Colors\nGerman shorthaired pointer gives you a stunning look with its brown, almond-shaped eyes. Having large, dark nose in its chiseled head, this breed has flat ears that are set high.\nIt body is lengthy with a wide chest and tapers at a point close to the tail. Though this breed’s body look sounds odd, it is an athletically build dog breed with its pointy nose and slim body.\nThe most fascinating feature of this breed is the webbing between their toes. The other noticeable feature is its docked tail that is 40% of its natural length. The tail gives you the stumped appearance at the end.\nGerman Shorthaired Pointer Coat Colors\nAn attracting aspect of German shorthaired pointer is its short, dense and smooth coat. The water-resistant nature of the coat helps to regulate its body temperature.\nThis breed comes in a range of colors, including solid liver (reddish brown), black and white. The coat also comes in different patterns such as ticked or patched in white.\nGerman Shorthaired Pointer Size And Lifespan\nIt is a medium-sized dog that stands between 21 and 25 inches. Males are generally taller than females and stand anywhere between 23 and 25 inches. And, females are 21 to 23 inches tall.\nThese dogs weigh around 70 pounds. Male dogs are heavier than females and weigh 55 to 70 lbs, whereas females weigh 45 to 60 lbs.\nGerman Shorthaired Pointer Lifespan\nThis breed is generally healthy. The average lifespan of these dogs is around 10 to 12 years.\nGerman Shorthaired Pointer Temperament\nCalled as the “perfect pointer” by aristocrats, the German Shorthaired pointers are smart and intellectual dogs. Eager to do whatever tasks assigned to it, this dog is an active, bubbly dog which takes part in any activity without being nervous.\nThis breed got the name “pointer” because of its arrow-like stance when they spot the prey. It will love everyone in your family and has a special bond with its favorite family member.\nThe physical attributes, intelligence, and personality of these dogs facilitate them to excel at various activities, including agility, dock diving, field trials, and search operations. This breed also plays roles such as\n- Excellent swimmer – These dogs excel at swimming with their webbed feet and this ability makes them a perfect retriever over the water.\n- Social dog – An adult dog of this breed will get along with other pets as well.\n- Child-friendly pet – It is a good friend to children, but won’t suit for small children.\n- A watchdog – This breed does like to bark when someone approaches your home. But, it won’t behave with them in an aggressive manner.\n- Good sporting dog – These dogs perform well in agility events. Its speed and willingness to learn to make them an ideal sporting dog.\n- Great traveler – If you are an adventurer, the German shorthaired pointer will accompany you in a much better way.\nGerman Shorthaired Pointer Training\nFirst, train your dogs to follow basic commands which will be a foundation for good behavior.\nAs it is a hunting dog, manage its hunting instinct. Provide him with toys to play and fetch, which will redirect your dog from hunting instinct.\nTake dogs to a long walk and needs daily exercise like running, swimming and playing.\nTrain to control dogs barking by giving commands, so they will know when to bark and when to stop barking.\nFor example, ‘speak’ command for barking and command ‘quit’ to stop barking.\nGeneral tips for training:\n- Try to keep short training sessions maximum of 15 minutes, because they are fast learners.\n- Positive reinforcement will give energy during a training session.\n- As they are prone to distraction, repetitive and consistent will act as a key for all task.\nGerman Shorthaired Pointer Shedding And Grooming\nGerman Shorthaired Pointers do not shed much, but they are seasonal shedders. This breed sheds in fall and spring and blows its coat during these seasons.\nThat is, the shedding amount will be high in those seasons and its coat requires frequent brushing.\nBut, you can easily groom its short and smooth coat due to its tendency to repel dirt.\nAnd, consistent and regular grooming will help your dogs to prevent shedding.\n- Brush its coat using a firm bristle brush at least weekly once to remove loose hairs. As it is difficult to remove its coarse hairs from carpet and furniture, it is a wise idea to brush it outside. And, you can use a good vacuum cleaner to clean up the pesky hairs.\n- Occasional bathing is enough for this breed due to its water-resistant coat. So, bath the coat only when needed using a gentle dog shampoo. After bathing, rub its coat with a towel or chamois to keep it shining.\n- Its floppy ears have the tendency to build up moisture and warmth. So, check the ears regularly for any signs of infection such as redness or bad odor. Clean them using a soft cloth dampened with the vet-recommended cleanser. But, consult the vet if your GSP frequently scratches the ears.\n- Frequently brush its teeth to maintain good dental health.\n- Trim its nails once in every few weeks to avoid scratching and bleeding. It will also keep its feet healthy.\n- Dry him thoroughly after hunting to prevent a chill.\nGerman Shorthaired Pointer Food\nThis breed does well on high-quality dog food. A well-balanced, healthy diet will promote good health in canines and keep their coat looking better.\nSome of the top-quality German Shorthaired Pointers foods are\n- Natural Balance\n- Purina Pro\n- Nature’s Variety\n- Wellness CORE\nUsually, a healthy German Shorthaired Pointer can eat 2 or 3 bowls of dry food. But, the German shorthaired pointer is prone to bloating.\nSo, you should be vigilant on the feeding amount and the schedule. And, you should always make sure that the food you choose suits your pooch’s age, size, and activity levels.\nPuppies of 6 months age – Feeding these puppies more than twice a day is enough. But, don’t feed more times a day.\n1-year old puppy – The normal serving ratio for these puppies is twice a day.\nA few general tips will help you to avoid bloating in these dogs in addition to the feeding amount and the schedule.\n- Don’t feed a large, wholemeal to it. Divide the meals into small portions.\n- You shouldn’t feed these dogs immediately after exercise. Else, provide at least an hour gap to them after eating to run or exercise.\n- Don’t leave this dog without providing food for long periods of time.\n- Check these dogs regularly for obesity.\n- Provide clean, fresh water to them.\nYou can also get advice from the veterinarian about the alternative diets and to ensure if your pooch’s nutritional needs are met.\nGerman Shorthaired Pointer Health Problems\nSome of the common health problems that can affect a German Shorthaired Pointers are-\nTorsion or (GDV) Gastric Dilatation-Volvulus\nA life harming disorder wherein the stomach is filled with unnecessary air causing it to bloat.\nDogs suffering from this condition will find it difficult to vomit or belch, and this, in turn, affects the dog’s heart condition. The dog can slip into a shock and without immediate treatment; this disorder can also turn fatal.\nVon Willebrand’s Illness\nA deficiency affecting the clotting ability of the blood vessels, this disorder can affect both dogs and humans. The von Willebrand percentage will turn low in persons and animals affected by this disorder, thereby altering the clotting capacity.\nThis disorder can affect the dog when they are between 3 to 5 years of age.\nWithout any cure available, the disorder can be handled effectively by avoiding certain drugs, and/or transfusion before surgery (Von Willebrand element), suturing or cauterizing injuries.\nThe condition where the dog’s eyelids roll inward causing injury or irritation to the eyes. Surgical help is needed to set right this condition.\nA health condition where the fluid accumulates due to lymphatic ducts malfunction.\nA hip-joint deformity which is caused due to several factors such as environmental, genetics or improper diet. Do not breed with the dogs suffering from hip dysplasia.\nUndetected infections can cause certain diseases such as thrombocytopenia, and hemolytic anemia. Tick prevention remains very important. In most cases, they vaccinate the dog without providing any antibiotic treatment.\nVets generally suggest a dual-dose vaccine program for tick bite treatment.\nGerman Shorthaired Pointer Price And Breeders\nThis breed is quite expensive. The average price of German Shorthaired puppies ranges from $600 – $800 USD.\nAnd, a puppy with pedigree and registration costs higher in the reputable registries such as the American Kennel Club. If you buy a puppy from the non-registered breeders, you have to keep in mind the shipping costs.\nThe adoption cost of German shorthaired pointer is considerably cheap. But, shelters have varying adoption fees.\nGerman Shorthaired Pointer Breeders\n- German Shorthaired Pointers Club of America\nLynn Pettinato (Secretary)\n1292 Antrim Dr; Roseville,\n- AKC Operations Center\n8051 Arco Corporate Drive, Suite 100\nRaleigh, NC 27617-3390\n- Bona Kennels\n5735 S 500 W', 'The ever-obedient, confident, and very loyal German Shepherd is one of the planet’s most popular breeds of dogs. Aside from being the dependable members of law enforcement, search and rescue, and military organizations around the globe, they make very faithful companions. No one can ever question the loyalty of a German Shepherd. And despite its imposing size and commanding presence, these dogs are very friendly and gentle on kids, making them one of the best dogs to have in a growing family.\nSmart, confident, and courageous, German Shepherd Dogs (GSDs) are currently classified by the American Kennel Club in the Herding Group of dogs and ranks second in the organization’s most popular breeds. German Shepherds have a life expectancy of anywhere between 9 and 13 years, although it is not unusual to see some dogs outliving their breed standards.\nInstantly recognizable with its black saddler and tan- or red-colored body and limbs, the GSD has a domed forehead, black nose, strong jaws, and a long, albeit square-cut muzzle. Perhaps one of the most characteristic features of a GSD is its large ears standing erect, although these are often pulled back with movement. It has a self-assured and intelligent look in its brown, medium-sized eyes.\nThe stance of a German Shepherd dog (GSD) is one of grace and refinement. Instead of the clearly sharp angles in other dog breeds, GSDs come with easy, gentle curves when viewed from the side. It is this naturally smooth flowing of their body contours that give the GSD amazing speed and agility. Their natural gait is one of an easy-and-free trot, although they can easily flip a switch and you’d see them dashing like a greyhound.\n- Rin Tin Tin, the movie world’s first canine superstar, cemented the position of the German Shepherd as one of the world’s most easily recognizable dog breeds.\n- The average male German Shepherd can grow up to 26 inches and weigh between 65 and 90 pounds. Females can grow up to 24 inches and weigh between 50 and 70 pounds.\n- During the First World War GSDs were renamed by the Americans as Shepherd Dogs because they didn’t want the dog to be associated with Germany. At the same time, the English also renamed the GSD as Alsatian Wolf Dogs. The name Alsatian is still used by many Europeans to this day.\n- German Shepherds are also called ‘German Shedders’ because of their natural tendency to shed a lot. They have medium-length fur in a double-coat setup. It is the undercoat that sheds heavily especially during spring and autumn. They are also known to ‘blow’ – shedding a lot at once that people describe it as a snowstorm.\nConfident, fearless, and direct; these are ideal qualities of a German Shepherd. It is intelligent, devoted, fun-loving, easy to train, and a very devoted dog especially if its parents have good temperaments, too. You can ensure the best in your German Shepherd if it is trained and socialized early on to become familiar with different people, sounds, and sights.\nGSDs are working dogs and as such require physical exercise and a good dose of quality mental stimulation. While they can be successfully raised in kennel situations, they will still require plenty of interaction from their human masters. They want to work. There is nothing more fulfilling for a German Shepherd than be able to perform even the most mundane of tasks. It enjoys plenty of walks or even going on a hike. Play ball with your GSD and you’ll find out its stamina is a lot greater than yours. Involve it in dog sports and it will easily bring home the bacon every time.\nExercise and mental stimulation is a must for GSDs: lest they grow bored, lonely, and destructive. As such, they are best reserved for pet parents who are themselves smart and active and who can provide these dogs with focused attention, training, and exercise. Above all, German Shepherds love lots of one-on-one time with their human masters.\nRelated Post: Best Dog Food for German Shepherd\nGerman Shepherds are very intelligent. Studies show that they can perform a trick 95% of the time just by teaching them the trick only once. Mastery of a certain trick can be achieved in as little as 3 to 5 sessions whereas other dog breeds will typically require a few weeks to several weeks just to master a single trick alone. GSDs rank third among the world’s most intelligent dogs following Border Collies and Poodles. However, if you think that German Shepherds will perform the trick on their own because they’re intelligent, think again. They are intelligent only when they are trained early on. This makes puppy training an absolute must. Remember, being gifted with intelligence is nothing if they cannot put this intellect to work.\nOne of the most interesting aspects about German Shepherds is that they have this reputation for being aggressive. Their stance, size, and body language coupled with their instinctive protectiveness make them excellent deterrents and watchdogs. While this reputation is quite accurate, there is actually another side to it that makes the GSD such a revered member of the family. GSDs are known to be very friendly with kids as well as other pets especially when they grew up together. They’re very affectionate with their families, too.\nThings You Should Know\nGetting a German Shepherd requires a lot of careful consideration. These are very active, smart, intelligent dogs that require the same qualities from their human masters or at least have a fundamental understanding of their needs. To help you decide whether the GSD is for you or not here are a few things you need to know about this dog breed:\nGerman shepherds are highly intelligent dogs. They’re smart and can easily be trained using the correct techniques and positive reinforcement. Animal behavior experts say that GSDs can easily learn a trick in as few as 3 repetitions, with many GSDs showing 95% accuracy of the behavior with just one repetition. That being said, training a GSD should start right at puppyhood. Learning the different steps and techniques of puppy training is crucial to harnessing the full trainability of GSDs. Equally important are the use of treats and other positive reinforcers.\nGerman shepherds are primarily motivated by the ability to work. They are originally bred as sheepdogs so one can only expect that they live to work. This is the very essence of their intelligence. They need something to work on so that they can use their cognitive capabilities. As such, their training is usually designed in such a way that it looks and feels like work for GSDs. They love scent detection, herding, and other physically-demanding activities that are best reserved for dogs that have exceptional stamina.\nGerman shepherds are aptly called “German shedders” because of their predisposition to shed heavily, especially in spring and fall. During these times of the year, GSDs require frequent bathing and brushing of their coat to help remove loose hair. A dog shedding brush may also be used to facilitate the removal of the excess hair. Whether or not it is spring or fall, German shepherds still require weekly brushing to help promote healthier and cleaner coats. You may also want to get a heavy-duty pet hair vacuum cleaner to help get rid of loose hair in your home.\nWhile German shepherds are known to be very sturdy, they do present with health problems that are very common in large dog breeds. For instance, GSDs are prone to the development of hip dysplasia, a condition whereby the hip joints failed to develop in a more normal manner leading to the thigh bone not fitting properly into the joint capsule of the hips. When the condition is not managed this can lead to arthritis and can severely impact the quality of life of your German shepherd. Remember that GSDs are highly active work dogs. Problems in their joints will impair their mobility; hence, their activities will also be impaired.\nAnother major cause of concern among GSD owners is degenerative myelopathy which is a lot similar to multiple sclerosis. What happens is that the GSD develops a progressively slow, creeping paralysis of its hindquarters. Other health conditions that GSDs are known to have include heart murmurs, cardiomegaly, heart valve diseases, visual problems, epilepsy, hemangiosarcoma, bleeding disorders, exocrine pancreatic insufficiency, and immune-mediated diseases.\nA German shepherd is generally a good choice for a dog if you’re looking for a hound that is:\n- Affectionate with its family\n- Incredibly kid-friendly\n- Very easy to train\n- Smart and intelligent\n- Handsome, regal, and well-built\n- Very eager\n- Loyal and faithful\n- Obedient and protective\n- Fearless, hardworking, and strong-willed\nWhile these characteristics can easily make a German shepherd the perfect choice for many dog-lovers, they are not ideal for someone who:\n- Doesn’t know how to be the leader of the pack\n- Doesn’t have the time to train the dog\n- Cannot spend at least 30 minutes of exercise every day with their dog\n- Cannot devote at least 15 minutes of quality one-on-one bonding time with their dog\n- Doesn’t like to brush and bathe their dog regularly and frequently especially during spring and fall\n- Doesn’t like the idea of having plenty of loose hair on the carpet, furniture, and other items in the house\n- Lives in very hot temperatures since the double-coat nature of GSDs can lead to overheating\nThe modern German Shepherd has its humble beginnings from 6 different breeds of European sheep herding dogs. These included the Italian Bergamasco Shepherd, the Lupino del Gigante, the Cane de Pastore della Lessinia e del Lagorai, the Cane Paratore, the Pastore d’Oropa, and the French Berger Picard. By the middle of the 19th century, attempts were already made to standardize the different breeds. From these efforts came the characteristics that are desired from a sheep herding dog: intelligence, keen sense of smell, strength, and speed.\nBy 1891, the Phylax Society was formed to institutionalize the standards of sheep herding dogs in Germany. While the group did not last, some of its former members persevered in their quest for standardizing the breed that will become the quintessential sheep herder in Germany. One of these ex-members is Max von Stephanitz who firmly believed in the idea that dogs should be specifically bred for working. He admired Germany’s native sheepdogs for their ability, strength, and intelligence, but this did not satisfy him to consider the breed as the ideal working dog.\nIn 1899, all these changed when von Stephanitz was introduced to Hektor Linksrhein, a sheepdog that is the result of selective breeding that lasted several generations. Hektor’s beauty, intelligence, loyalty, and strength were all that von Stephanitz wanted in the perfect working dog. He bought Hektor from its owner and renamed it Horand von Grafrath. Soon after, von Stephanitz established the Verein fur Deutsche Schaferhunde or the Society for the German Shepherd Dog. This formally introduced the German Shepherd Dog to the world with Horand officially being recognized as the very first German Shepherd.\nHorand became the progenitor of all modern German Shepherds. The dog was introduced in the US towards the early parts of the 1900s. They were instantly loved by dog-lovers because of their highly-adaptable, intelligent, attractive, and hardworking nature. The very first GSD registered with the AKC was Queen of Switzerland in 1912. It was the following year that the German Shepherd Dog Club of America was formally established.\nUnfortunately, the outbreak of the First World War saw the decline in the popularity of the breed as they were inadvertently correlated with the enemy. But the war only cemented the GSD’s unique characteristics as a working dog. It braved artillery fire, tanks, and landmines just so it can bring much-needed supplies to the German frontlines.\nIt was after the war that Rin Tin Tin and Strongheart revived the popularity and favorability of the breed to the American dog-loving society. The breed’s popularity peaked with the crowing of Sieger Pfeffer von Bern as the AKC’s Grand Victor for 1937 and 1938. Unfortunately, with World War II and the growing anti-German sentiment, the breed’s popularity again took a hit.\nAfter the Second World War, their popularity steadily increased, culminating in the number 2 spot of the AKC’s most popular breeds, second only to the Labrador, in 2016.\nThe German shepherd continues to wow dog-lovers everywhere. Their proud stance, characteristic coat, and their irrefutable reputation for intelligence, speed, strength, loyalty, dedication, protectiveness, and faithfulness all make the GSD a great dog to have for pet parents with the same characteristics.\nDog Food Mat\n- German Shepherd Dog – The American Kennel Club']	['<urn:uuid:e586b38f-ebd3-4666-ace3-5f9663f67d86>', '<urn:uuid:caca882a-0890-4978-bc90-cee1a83e3fe6>']	factoid	direct	concise-and-natural	similar-to-document	three-doc	expert	2025-05-12T21:31:42.699391	10	47	4493
77	how software monitoring works challenges verifying large applications	Software monitoring involves two key aspects. First, in systems like Informatica MDM, monitoring involves tracking data quality and performance across multiple stages using different repositories to collect metrics about workflows, sessions, and job runs. Second, for cloud applications, monitoring is particularly challenging due to their concurrent and distributed nature, where components can fail independently, and the system needs to handle dynamic scaling. The monitoring process must collect information from the system with minimal instrumentation and run alongside it during testing to effectively detect and assess issues.	"[""A typical Informatica MDM implementation uses Informatica ETL tool for processing data to the Landing tables, and may have one or more stages, depending on the architecture, for processing data from various in house or third party data providers; processing from Landing to Staging to Various target tables(cross reference, and associated tables), tokenization of these records and then match merge in the Informatica MDM space; and then generating feeds to various downstream data marts, data warehouses, OLTP systems, and third parties. In my experience a majority of this work happens with the Powercenter and MDM toolsets from Informatica. Though in this realm work can be and is done using Web Services, but for the purpose of this article, I shall ignore that use case.\nOnce a system is in place, it needs to be monitored for ensuring ongoing data quality and performance. This tends to be a tedious task because not only are there multiple stages to the whole processing before, during and after MDM processing of the data, but also the performance metrics are stored in two different repositories. This article show what data is available in those repositories and how one could set some naming patterns as part of the standards while architecting such a system that would make it not only possible, but approach the whole effort in an efficient manner.\nSo let us explore what both the repositories provide, starting with Informatica Powercenter repository:-\nPowercenter repository provides and can be leveraged to get a multitude of reports as shown here. For the purpose of this blog I shall be using the following tables:-\nThe first table could be queried to get the various workflows/worklets/sessions that are available in the repository. I use the following query to get the information about the workflow and the other artifacts contained therein in a hierarchical fashion.\nAS (SELECT *\nFROM (SELECT connect_by_root workflow_id the_workflow_id\nSys_connect_by_path (instance_name, '/')\nFROM (SELECT *\nFROM opb_task_inst ta\nWHERE task_type <> 62\nAND version_number = (SELECT\nFROM opb_task_inst ti2\nti2.instance_id = ta.instance_id))\nWHERE connect_by_isleaf = 1\nSTART WITH ti.workflow_id IN (SELECT task_id\nWHERE task_type = 71)\nCONNECT BY PRIOR ti.task_id =ti.workflow_id) q\nWHERE q.task_type <> 70),\nAS (SELECT subject_id,\nFROM (SELECT subject_id,\nPARTITION BY subject_id, task_id\nORDER BY version_number DESC) trn\nWHERE task_type = 71) q\nWHERE q.trn = 1)\nSELECT f.subj_name folder_name,\nFROM session_cfg_info s\njoin workflow_cfg_info w\nON ( s.the_workflow_id = w.the_workflow_id )\njoin opb_subject f\nON ( f.subj_id = w.subject_id )\nWe shall come to the use of this result set later.\nNext we gather the run time information about various jobs that ran using the following query as a starting point.\nSELECT workflow_id, rn, workflow_run_id, worklet_run_id, child_run_id,\ntask_id, instance_id, instance_name,\nworkflow_id || SYS_CONNECT_BY_PATH (task_id, ',') full_task_id,\nSYS_CONNECT_BY_PATH (instance_name, '/') fully_qualified_name,\nFROM (SELECT workflow_id, workflow_run_id, worklet_run_id, child_run_id,\nREPLACE (run_err_msg, CHR (10), ' ') run_err_msg,\nWHEN child_run_id = 0\n* (ROW_NUMBER () OVER (ORDER BY end_time,\nFROM opb_task_inst_run t\nWHERE task_type <> 62) q\nWHERE CONNECT_BY_ISLEAF = 1\nCONNECT BY NOCYCLE PRIOR rn = worklet_run_id\nAND PRIOR workflow_run_id = workflow_run_id\nSTART WITH worklet_run_id = 0\nJoining this information with the data from the opb_sess_task_log table, the opb_wflow_run tab and opb_subject table, one can derive the information to be able to join with the data from the workflow configuration data derived in the first query above. One has to be careful as the opb_sess_task_log will have higher cardinality than the workflow run query provide immediately above. If I am trying to monitor things at each and source qualifier or target level it, we could join it directly. However, if one is just trying to get a ball park idea of the volumes processed it might as well be aggregated, and that may provide good enough information for trending the performance numbers. For this exercise let us go with the latter:-\nSUM (src_failed_rows) src_failed_rows,\nSUM (targ_success_rows) targ_success_rows,\nSUM (targ_failed_rows) targ_failed_rows,\nSUM (total_trans_errs) total_trans_errs\nFROM etl_prd_rep.opb_sess_task_log tl\nGROUP BY workflow_run_id, instance_id\nSo having discussed the Powercenter repository queries, let us review the provided results. If your implementation is like any that I have seen at multiple organizations, you would find that the sessions running MDM job stored procedures are reporting either 1 or 0 rows processed, depending on how the stored procedure is being called. This just ends up giving the run timings. If you are lucky the implementation may even record error when the MDM job fails. But, the number of rows are not reported back. So to address this deficiency, let us move on the MDM repository schema. Here the queries are straight forward to obtain information about various jobs:\nSELECT jc.rowid_job job_run_id,jc.system_name data_system,\njc.table_display_name subject_table_name,jc.object_desc job_desc,\nWHERE job_status_code = jc.run_status)run_status,status_message,\nWHERE object_function_type_code =\njc.start_run_date start_date, jc.end_run_date end_date,\njn.total_records, jn.inserted_records, jn.updated_records,\njn.no_action_records, jn.matched_records, jn.updatedxref_records,\nFROM c_repos_table t JOIN c_repos_job_control jc\nON (t.rowid_table = jc.rowid_table)\nLEFT OUTER JOIN\nWHEN metric_type_code = 1\nWHEN metric_type_code = 2\nWHEN metric_type_code = 3\nWHEN metric_type_code = 4\nWHEN metric_type_code = 5\nFROM c_repos_job_metric jm\nGROUP BY rowid_job) jn ON (jc.rowid_job = jn.rowid_job)\nAgain I chose to report the statistics in a row, but decided not to aggregate as that wonâ€™t always make sense to add these numbers. Also, I am showing only a few stats in the query, other metrics are available as well.\nNow, we come to the real challenge. The challenge lies in being able to combine the rows from the Informatica repository with those from the MDM repository. One could follow the approach whereby one could create characterize the data pulled from both the repositories with common attributes. Here we go back to the purpose of the very first query from this article. We could e.g. gather that data in a table where we could add extra attributes that would help us characterize those sessions/jobs. We would follow a similar approach in case of the MDM data. We would need to make sure that whenever we add new jobs on ETL/MDM side that we add those rows to these characterization tables. Using these characterization tables and knowing the fact that on a reasonably time synchronized platform the ETL jobs timing would be same or a little outside the MDM job timing, we could combine the rows exactly and get all the relevant stats from the MDM system.\nOn a multi-domain MDM system the following attributes would be sufficient:-\na. Domain (Customer/Product)\nb. Source System (Admin or others..)\nc. Operation (Landing, Staging, Loading, Auto Match & Merge, etc.)\nd. Table Name or MDM Target Object.\nFor joining the ETL run information with the configuration donâ€™t forget to use the fully qualified task id, as that takes care of the various shared objects use in multiple parent objects.\nWhile building one such system, I was able to use the various patterns in names to find these attributes. Since, I approached this problem after the fact of building the system in first place, this task was not trivial, but was made easy by a consistent use of naming conventions. If I was leading the build process I think this task can be further simplified, and can be more easily incorporated into the initial build itself.\nAbout Sarbjit Parmar\nA practitioner with technical and business knowledge in areas of Data Management( Online transaction processing, data modeling(relational, hierarchical, dimensional, etc.), S/M/L/XL/XXL & XML data, application design, batch processing, analytics(reporting + some statistical analysis), MBA+DBA), Project Management / Product/Software Development Life Cycle Management."", 'Research in Monitoring within ElasTest\nCésar Sánchez (IMDEA Software) and Felipe Gorostiaga (IMDEA Software)\nSoftware is becoming more and more oblivious in all aspects of our lives. Software based companies are becoming economically more dominant, opening new economic sectors with surprising innovations and making economic processes more efficient (see, for example, Marc Andreeseen “Why software is eating the world” ). Even in classical industries, like the automotive industry, software is involved in more and more functionalities, and very rapidly gaining a higher ratio of costs and resources in car development.\nThis tendency has two important consequences: (1) software is more complex to develop, deploy and maintain; and (2) our dependence on software reliability (and the complexity of the software reliability tasks) presents important challenges.\nSoftware reliability is a notoriously difficult and challenging process. This is witnessed by the increasing costs, in terms of time and money, that is invested in increasing the reliability of the software before deployment. This trend is exacerbated in complex systems, not only in the critical software domains like automation, where software bugs can have catastrophic consequences, but also in areas dominated by the time to market, like for example cloud based applications.\nSoftware reliability has received a lot of attention for decades, both in academia and in industry, and many techniques have been developed and applied. Most techniques are based on a two-step effort, where the reliability techniques are applied a-posteriori, after the software development process creates the software artifact. Even though software reliability has been improved by the invention of modern programming languages and the use of more sophisticated software development processes, verification and validation is still required. Verification techniques range from formal verification on one hand that aims at providing a definite guarantee of the correctness, to testing at the other extreme where the actual software is exercised repeatedly, aiming to find bugs.\nFormal techniques like static formal verification requires lots of expertise and human resources, and in spite of recent breakthroughs, it does not scale to large projects and complex properties. Consequently, it is typically only applied to small pieces of critical software. Testing is, by far, the dominant approach in all non-safety critical areas of software development, including cloud computing.\nThe project ElasTest aims to improve the process of testing large cloud applications, and in turn improve the quality of the resulting software and reduce the cost of this process. Cloud computing applications share some of the characteristics that make software testing difficult.\n- First, cloud application software is inherently concurrent. It is well-known that it is unfeasible to exercise all interleavings of concurrent software, and that concurrency bugs are extremely hard to find, repeat, explain and correct.\n- Second, cloud application software is inherently distributed and lousily coupled, components can fail independently, and there are complex failure dependencies between components. This is witnessed by the merge of programming and operation activities in the same group of experts, modernly known as DevOps .\n- The elasticity of cloud applications makes the problem even more difficult because concurrent and distributed components can be spawned and connected dynamically.\n- As a consequence of the previous points, the run-time system that cloud applications depend on is more sophisticated than in earlier platforms and more often less reliable itself.\n- Cloud applications are reactive, in the sense that there is an environment that the application interacts with including the cloud infrastructure, other applications and the users. Cloud applications are typically interactive so unknown users are meant to use the application.\n- The users of cloud applications can also sometimes be malicious, exercising the software in unexpected ways sometimes with the intention of breaking the functionality with some undesirable target goal.\nThe main interest of our IMDEA Software research group in ElasTest is the exploration of runtime verification techniques in improving the testing process of cloud elastic applications. Runtime Verification  is an attempt to leverage the research in static verification, and in particular in behavioral specification languages, to applicable dynamic techniques. In runtime verification a specification is used to generate a monitor that inspects the execution of a software system in order to detect violations of the specification. This technique has been used to complement testing, by facilitating the uncovering and explanation of bugs, or even be for dynamic repair.\nIn the context of the project ElasTest, we are focusing on the development of a monitoring component that can be spawned and used in a testing activity. The monitoring service is a Test Support Service that can be used in different tests independently. The ElasTest Monitoring Service (EMS) will run outline (as opposed to inlined embedded in the system under test) by collecting information from the system under test with minimal instrumentation, and also collecting events. The EMS will also run online, alongside the system under test during the testing process (as opposed to offline analyzing traces in a post-mortem manner).\nThe main research challenge of the EMS is to find a monitoring description language that can be:\n- usable for the users describing tests, to effectively to collect and correlate the necessary information to assess the outcome,\n- to facilitate the description of tests that dynamically direct the test to explore possible bugs more effectively,\n- to make use of the resources to create an efficient online evaluation language, specific to the monitoring needs for testing\nThis challenge requires combining techniques from three active areas of research in recent years: runtime verification, event correlation and complex-event processing, and cloud monitoring.\nThese techniques have potential beyond the goals of ElasTest, like better orchestrating the bootstrap of cloud applications or specific monitoring systems in production.\n Marc Andreessen, Why software is eating the world, Wall Street Journal, Oct. 2011']"	['<urn:uuid:534595bb-64f8-4082-8146-00ac872aeb8e>', '<urn:uuid:84e0b82e-1e57-447e-8e79-980c924c7ebc>']	factoid	with-premise	long-search-query	similar-to-document	multi-aspect	novice	2025-05-12T21:31:42.699391	8	86	2183
78	medical reference laboratories testing methods future developments	Medical reference laboratories, governed by ISO 15195 standards, are witnessing significant developments in testing methods. Traditional testing approaches are being complemented by emerging nanotechnology, including nano-fiber filters and Metal Organic Frameworks (MOFs) with extremely high surface areas up to 6000 m²/g. New testing technologies include ultra-high speed analytical centrifuges and nanoparticle tracking devices, capable of measuring particle size distributions down to nanometer levels. These developments are particularly relevant for reference measurement laboratories, which must maintain specific calibration standards and participate in inter-laboratory comparisons to ensure measurement accuracy and validation of uncertainty claims.	"[""OR WAIT 15 SECS\nGraham Rideal is managing Director at Whitehouse Scientific.\nGraham Rideal of Whitehouse Scientific explains the importance of filter testing and offers some considerations with regards to choosing filter test methods.\nHow important is filter testing in the pharma industry?\nFiltration in the pharmaceutical industry covers a wide range of applications from powder processing to liquid purification, such as saline or dextrose solutions. In the case of the former, the advantage of good filtration processes mainly lies in recovering valuable drugs, which can cost in excess of $1 million per kilogram. The advantage here is purely commercial.\nFor intravenous products, however, good filtration is a matter of life and death; a simple internet search on recalled drugs reveals particulate contaminants from the clearly visible, such as glass fragments, wood fibres and mould, to the invisible, and potentially lethal, bacteriological contamination.\nThe primary test of a filter begins with the filter medium itself to ensure it is fit for purpose. Thereafter the filter system must be tested in situ. Failure can occur in the filter medium or in the sealing arrangement when assembled.\nOne of the simplest methods of comparing filter media is to test porosity either by air or water flow under prescribed conditions. Lower flow rates or higher back pressures indicate better filter performance.\nThe flow rate method is refined in the so-called bubble point test. Here, the filter is saturated with a liquid and then a gas (usually air) is pressurised from below. As the pressure increases, the air finds the single largest pore, which is blown out, forming a bubble on the surface: the bubble point. The Washburn equation is then applied to convert the applied pressure into the diameter of the pore.\nPorometry takes the bubble point one step further by continuing the pressurisation of the filter beyond the bubble point. Successively smaller pores are blown clear by the air pressure until the smallest pore is finally evacuated. The applied pressure versus flow rate profile can then be interpolated to provide complete pore size distribution.\nChallenge testing, as the name implies, involves challenging the surface of a filter with actual particles, which could be solid or liquids such as oil mists. Particle sizes are measured upstream and downstream of the filter. The maximum size particle passing reflects the largest pore, while the reduction in concentration determines the efficiency of the filter.\nAlthough conceptually easy to understand, challenge testing can give varying results depending on the particles used and the method of measurement. Irregular-shaped particles have a number of dimensions determined by their shape, so it is important to specify the method of particle size analysis. Furthermore, the method of measuring particle concentration can give different results; averaging by volume, for example, will give a completely different result compared with a number average.\nHow should a filter test method be chosen?\nSelecting an appropriate methodology for testing filter performance depends very much on the application of the filter. For non-critical applications where a simple quality assurance test is required, a basic porosity or bubble point test may be all that is required. For example, in the manufacture of syringe and water filters or in belt filters for product collection, the maximum pore size or the effective cut point of the filter medium is often the process control parameter.\nIf a comprehensive analysis of the poresize distribution in a filter is needed (e.g., dialysis membranes or air vents in bodily fluid aerosol traps used in theatre, then a porometer has to be used. However, because the pore sizes obtained are derived theoretically, they do not necessarily correspond to the true geometric size of the pores. There may also be variations from instrument to instrument, and operator to operator.\nNevertheless, the porometer is one of the most powerful tools in filter testing because it can measure pores from the sub-micron level to several hundred microns. Another advantage is that the test is nondestructive. As the pore structure is only invaded by an easily cleanable liquid, the actual filter used in the test can be reused in the final filter application.\nSo long as care is taken in particle selection and the statistics of measurement, challenge testing is one of the most robust methods of filter testing. If spherical particles are used and analysed by microscopy, where any out-of-shape particles can be eliminated, it is the most unambiguous test of them all. A particle either passes or is trapped by the filter and its size can be traced back to international standards of length. The range of measurement is restricted only by the particles available, so measurements could go down to a few nanometres using gold sols.\nGenerally speaking, challenge testing is restricted to measuring the cut points or maximum pore sizes, although new developments can provide pore size distributions in some cases. The measurement technique is called the 'near mesh' method where the spherical particles become lodged in the pores during the challenge test. When released by tapping or ultrasonic energy, the diameters of the 'near mesh' microspheres reflect the diameters of the filter pores.\nIn critical applications, challenge testing is the method of choice. However, unlike porometry, the actual filters used in the test cannot be used in the final application because of particle contamination.\nWhat developments do you expect to see in the future?\nWe now seem to be living in the nanotechnology generation where science is moving to smaller and smaller sizes. Filtration is no exception and there are already several filters available that use nano fibres, which, when used in syringe filters, can turn black Indian ink into pure water with a gentle press of the thumb (see nanofibre filtration now being offered by many filter media manufacturers).\nWhere as material science was once the predominant technology, chemistry is becoming the predominant discipline as molecular cages are built, rather like nano-lobster traps, to selectively remove contaminants or harvest valuable nano products. This exciting new technology known as Metal Organic Frameworks or MOFs, chemically constructs links or cages around a pore that are reduced in thickness to that of a single chemical bond of sub-nano proportions. As a consequence, these structures have the highest known surface areas, typically up to 6000 m2 /g. Put into perspective, this is equivalent to having the surface area of six football pitches packed under the nail of your little finger!\nAt such small sizes, porometry would not be appropriate because of the small pore size and the physical structure of the filter, which is usually in powder form. The only filter testing method available in these instances is challenge testing. Particle size analysis then becomes of paramount importance. However, there are now 'new kids on the block' rising to meet the challenge, such as ultra-high speed analytical centrifuges and nanoparticle tracking devices.\nIn the case of the analytical centrifuge running at up to 24000 rpm (CPS Disc centrifuge), a 'line start' technique is used, rather like a 100 m sprint. The sedimentation is followed optically as the larger particles sediment faster and so arrive at the finishing line before the smaller ones.\nIn the nanotracking method (NanoSight) individual nano particles scintillate light as they are illuminated by a laser in a liquid. Rather like floating dust particles scintillate a beam of sunlight in a room. This Brownian motion can then be tracked by microscopy and the diffusion rates used to calculate particle size.\nIn both of these methods, particle sizes distributions down to a few nanometres can be measured. The technologies are being used in the study of gold sols in the treatment of cancer and rheumatoid arthritis.\nHowever, in the MOF porous structures, even these methods fall short and either transmission electron microscopy or calculations from chemical bonds must be used to determine the pore sizes. At the moment, this exciting technology is something of a 'solution looking for a problem', but there is no doubt that the most significant applications in the future will be found in the pharmaceutical industry, most likely as highly specific catalysts in producing new active ingredients.\nDr Graham Rideal is CEO, Whitehouse Scientific and Science Correspondent for The Filtration Society."", 'Health and Safety\nThe Health and Safety Pillar of KENAS has five accreditation schemes:\n(a) Medical Laboratory Accreditation\nThe general criteria for accreditation of medical laboratories are contained within the ISO 15189:2007 Medical Laboratories- Particular Requirements for quality and competence.\nMedical laboratory accreditation is a means of determining the technical competence of a medical laboratory to perform specific types of testing, measurement and calibration of equipment. Medical laboratory accreditation also provides a formal recognition to competent laboratories, thus providing a ready means for customers to identify and select reliable testing and measurement services able to meet the customers’ needs.\nMedical laboratory services are essential to patient care and therefore have to be available to meet the needs of all patients and the clinical personnel responsible for the care of these patients. These services include: arrangements for requisition, patient preparation, patient identification, collection of samples, transportation, storage, processing and examination of clinical samples together with subsequent validation, interpretation, reporting and advice, in addition to the considerations of safety and ethics in medical laboratory work.\nCurrently KENAS is offering accreditation to the following medical laboratories:\n- General medical/clinical laboratory accreditation\n- Research medical laboratory accreditation\n- Forensic medical laboratory accreditation\n- Blood transfusion laboratories\n- X-ray laboratory accreditation\n(b) Medical Reference Laboratories\nThe general requirements for accreditation of medical reference laboratories are laid down in ISO 15195: Laboratory medicine: Requirement for Reference Measurement Laboratories.\nISO 15195:2003 is not applicable to routine medical laboratories.\nISO 15195:2003 refers to the specific aspects of calibration laboratories in the field of laboratory medicine where such “calibration laboratories” are usually denoted as “reference measurement laboratories”. ISO 15195:2003 may form a basis for the accreditation of a reference measurement laboratory that applies for official recognition of the performance of a reference measurement procedure.\n(c)Proficiency Test Accreditation\nThe general criteria for accreditation of medical laboratories are contained within the ISO 17043:2010 conformity assessment- general requirements for proficiency testing. ISO/IEC 17043:2010 provides a consistent basis for all interested parties to determine the competence of organizations that provide proficiency testing.\n(d) Inter Laboratory Comparisons\nInter laboratory comparisons are widely used for a number of purposes and their use is increasingly internationally. Purposes for inter laboratory comparisons include: –\n- Evaluation of performance of laboratories for specific tests or measurements and monitoring laboratory’s continuing performance;\n- Identification of problems in laboratories and imitation of actions for improvement which, for example, may be related to inadequate test or measurement procedures, effectiveness of staff training and supervision;\n- Establishment of the effectiveness and comparability of test or measurement methods\n- Provision of additional confidence to laboratory customers;\n- Identification of inter laboratory differences;\n- Education of participating laboratories based on the outcomes of such comparisons;\n- Validation of uncertainty claims;\n- Evaluation of the performance characteristics of a method;\n- Assignment of values to reference materials and assessment of their suitability for use in specific test or measurement procedures;\n- Support for statements of the equivalence of measurements of national metrology institutes through “key comparisons” and supplementary comparisons conducted on behalf of the internationally bureau of weights and measurement (BIPM) and associated regional metrology organizations.\nThe need for ongoing confidence in laboratory performance is not only essential for laboratories and their customers but also for other interested parties such as regulators, laboratory accreditation bodies, and other organizations that specify requirements for laboratories.\nAll Medical, Veterinary, Pharmaceutical and General testing & calibration laboratory are required to participate in recognized/ accredited proficiency testing where applicable or possible.\n(e) Veterinary Laboratory Accreditation\nThe general criteria for accreditation of Veterinary Laboratories are contained within ISO/IEC 17025:2005.\nSpecific criteria are used to augment the general criteria applicable to a certain field of testing, testing technology, or specific test. The specific criteria for accreditation of veterinary laboratories are stated in the OIE Quality Standard and Guidelines for Veterinary: Laboratories: Infectious Diseases, 2008.\nAdditional requirements supplement ISO/IEC 17025:2005 to evaluate diagnostic and clinical veterinary laboratories that conduct commercial, government, academic, and international veterinary testing in the following areas: infectious disease diagnostics, disease surveillance, virology, pathology, microbiology, immunology, screening for growth promoters and drug/chemical residue that includes antibiotics, antihelmethetics, pesticides, metals, organics, DNA/RNA, and GMO.\nThese specific requirements also are designed to complement any applicable regulatory requirements that may affect government veterinary laboratories (state, regional, national, and international.)\n(f) Pharmaceutical Laboratory Accreditation\nThe general criteria for accreditation of pharmaceutical laboratories are contained within ISO/IEC 17025:2005. There is a need for global acceptance of this form of accreditation in Kenyan pharmaceutical laboratories to achieve national and global recognition, as well as conformity against international standard led to the establishment of an accreditation programme for pharmaceutical laboratories under the auspices of Kenya Accreditation service (KENAS).\nRequirements for accreditation\nPharmaceutical laboratories seeking accreditation need to comply with the following requirements:\n- The quality management, scientific and technical aspects of pharmaceutical laboratories as determined by ISO/IEC 17025 regulatory documents which may have been developed by the KENAS Pharmaceutical Technical Committee (KPTC).\n- Participation in proficiency testing or inter-laboratory comparisons, where possible.\n- What is Accreditation?\n- Testing Laboratories\n- Health and Safety\n- Inspection and Verification\n- Proficiency Testing Providers\n- Directory of Accredited CABS\n- Gazetted Fees\n- ISO/IEC 17021-1:2015 Conformity assessment – Requirements for bodies providing audit and certification of management systems — Part 1: Requirements\n- ISO/IEC 17024:2012 Personnel Certification\n- ISO/IEC 17065:2012 for Products,Processes and Services']"	['<urn:uuid:cac3c98c-f291-40a7-a779-f3b8d8b8eafc>', '<urn:uuid:e3634bbc-91d0-4967-bc17-da826b47141c>']	open-ended	direct	short-search-query	similar-to-document	three-doc	expert	2025-05-12T21:31:42.699391	7	92	2254
79	compare efficiency drip irrigation vs furrow irrigation water conservation farm	Drip irrigation from pipes laid on or under the ground uses much less water and does less damage to soil than water poured from above (like in furrow irrigation). Furrow irrigation systems face issues with residue dams plugging furrows and preventing uniform water distribution. Additionally, furrow irrigation requires careful management of crop residue, set times, length of run, surface conditions, and furrow size to achieve water savings.	['Hesperian Health Guides\nUse Water Wisely\nEvery day 20,000 people visit the HealthWiki for lifesaving health information. Ifwe could translate 50 more chapters.\nMake a gift to support this essential health information people depend on.\nEvery farmer needs water. If you live in a dry place, the best way to conserve water is to grow plants native to your area or plants that need water only during the rainy season. Green manure and mulch help hold water in the soil, and contour barriers save water by keeping it from running off. Other methods to save water on the farm are:\n- planting shade trees to protect plants and soil from drying out in the sun. Some trees bring water up from deep in the ground for shallow-rooted plants to use.\n- planting crops close together to shade soil so it does not dry out. The air between plants close together holds a little moisture so plants do not wilt. This can be done with green manure or by planting a variety of crops together in the same field.\n- drip irrigation from pipes laid on or under the ground, which uses much less water and does less damage to soil than water poured onto the ground from above.\n- strip-cropping (growing different crops together along contour lines) to help crops share moisture. A ground-cover crop is planted uphill from the contour line, and a crop that gives only a little ground cover is planted below it. Water collects on the ground cover and flows to the downhill crops.\n- reusing wash water to water gardens near the house.\n- protecting water catchment areas to provide more water for people and crops.\nMake planting pits\nPlanting pits collect rainwater to help plants grow even in very dry conditions. Planting several crops in the same pit makes the best use of water. The crops that need the most water grow best at the downhill end. Crops that can live with less water grow well on the higher side of the slope.\nIn the second year, plant in the same pits, or dig new pits between the old ones. If you dig new pits, over the years the whole area will be fertilized.\nStone walls prevent erosion and save waterThe central plateau of Burkina Faso is a mix of flat ground and gentle slopes. Rainfall has always been low, but there has been even less in recent years, and the land and people have suffered. To conserve water and prevent erosion, farmers build low stone walls across fields. The walls slow down the flow of water, allowing time for it to soak into the ground. The walls also prevent soil from blowing or washing away and catch soil that erodes from higher slopes.\nFarmers also dig large planting holes. They fill the holes with compost or manure to fertilize the crops and hold water.\nWhere gullies have formed, people fill them with stones. If a gully is too big to fill, they build a stone wall across it. Just as on the field, the stone wall slows water down and keeps the gully from getting worse. Over time, soil may fill the gully.\nBy using these methods, farmers in Burkina Faso are able to make the land richer and improve crop yields even with less rainfall. And with more food, people’s health has improved.', 'Limited Irrigation Crop Management\n- What is meant by limited or deficit irrigation?\n- How do conservation tillage practices aid water conservation efforts?\n- Practically, or in reality, how much water can I conserve by practicing limited or deficit irrigation?\n- What limitations to irrigation practices are caused by conservation tillage?\n- What are some other low-cost management practices that irrigators can adopt to conserve moisture when implementing deficit or limited irrigation strategies?\n- Are some crops better suited to limited or deficit irrigation than others?\n- What guidelines should be applied to selection of crops for limited or deficit irrigation?\n- Are some cropping strategies or crop rotations better-suited to limited or deficit irrigation than others?\n|Q:||What is meant by limited or deficit irrigation?|\n|A:||Limited or deficit irrigation refers to situations where irrigation water is either managed or applied out of necessity in amounts throughout the duration of the irrigation season such that the total amount of water applied is less than that required to meet the potential evapotranspiration of the crop. Limited irrigation may be achieved by one of several techniques, including: 1) reducing acres under irrigation, 2) reducing irrigation water applied to the field per application and throughout the growing season, 3) making use of the available recycled water, and 4) modifying irrigation equipment and systems through sprinkler nozzle configurations, system pressure modifications. Limited irrigation practices are appropriate where irrigation water is not readily available to meet crop-water requirements and in areas of competing demands for ground and surface water supplies.\nFactors that will determine the ultimate success of limited irrigation strategies include crop choice in relation to climate and availability of water, physical and chemical soil properties, anticipated precipitation amounts and timing throughout the year and during the growing season. Because reduced yield is often a result of limited irrigation practices, the goal is to manage crops and water use for the greatest possible return for the crop grown. For this reason, it is important to implement additional water conservation techniques in conjunction with limited or deficit irrigation strategies.\n|Q:||How do conservation tillage practices aid water conservation efforts?|\n|A:||Conservation or reduced tillage refers to the practice of utilizing tillage methods and equipment that strive to retain crop residue that provides a minimum of 30% soil surface coverage at all times, which includes both during crop growing periods and periods when agricultural land is idle (such as during fallow periods). Conservation tillage can enhance water conservation by: 1) reducing runoff, 2) increasing surface moisture detention by means of snow trapping and wind reduction, 3) reducing evaporation by providing shade and protection from wind. Conservation tillage and the resulting changes in soil physical properties often lead to increased infiltration rates, which can be a be a benefit to irrigated soils with low and medium intake rates. Conservation tillage, in conjunction with monitoring soil moisture particularly in the spring after sufficient winter precipitation, may allow irrigators to delay or eliminate one to two irrigation applications during the irrigation season.|\n|Q:||Practically, or in reality, how much water can I conserve by practicing limited or deficit irrigation?|\n|A:||It is conceivable that an irrigator could reduce the amount of irrigation water applied by 10 to 20% by practicing limited or deficit irrigation. Obviously, this depends on the crop, irrigator experience, system design and efficiency, previous irrigation practices, soil and weather conditions. It also depends on the irrigator’s management and production goals.|\n|Q:||What limitations to irrigation practices are caused by conservation tillage?|\n|A:||Concerns related to irrigation timing and uniformity of water distribution has discouraged some irrigators from adopting conservation tillage practices. Residue left on the soil surface often causes problems in furrow irrigation systems, where residue dams plug furrows and prevent uniform movement of water down furrows.|\n|Q:||What are some other low-cost management practices that irrigators can adopt to conserve moisture when implementing deficit or limited irrigation strategies?|\n|A:||Furrow firming is a practice which can be used to reduce or even infiltration rates in soils with high intake rates. Additional changes irrigators can implement include changes in furrow stream size which determines advance time. Management of the amount of crop residue, set times, length of run, surface conditions at the trough of the furrow, and furrow size together will increase water savings.|\n|Q:||Are some crops better suited to limited or deficit irrigation than others?|\n|A:||Some crops are drought and saline tolerant, while others may flourish in a cooler or warmer climate. Additionally, some crops are more sensitive to moisture deficit during early growth while others are more sensitive to moisture deficit during mid-season or late-season growth. Pasture is a good example of this, where warm-season grasses perform better in Texas while cool-season fescue grows better in high altitude and cool temperatures.\nCool-season, short-growth crops such as brown mustard, canola and camelina have become an essential component in on-farm agricultural water conservation strategies. They are known to use less water and allow more time to accumulate water prior to planting wheat, for example, to replace fallow in the High Plains of western United States. Choosing crops based on their plant-water requirements as well as crop rotation and fallow periods contribute to agricultural water conservation at the micro and macro level of farming systems.\n|Q:||What guidelines should be applied to selection of crops for limited or deficit irrigation?|\n|A:||Analyzing crop selection requires knowing the well yield and field size, the irrigation regime, the cost of irrigation, yield response to water for different crops irrigated, water requirements per crop, labor requirements, and expected climate. Before making any cropping decisions, check with the appropriate Farm Service Agency, CRC coverage, Revenue Assurance, etc. regarding any restrictions. When selecting crops, look at when the crop most needs water as well as how much water will be needed. Make a comparison between when the crop needs water and when it will be available from the irrigation district. If water is not expected to be available during a critical growth period, consider another crop. Try to make adjustments that will work in a dry year, but will not limit potential yields if the drought ends. Make sure there is an economic use for the crop being considered as well as the technical expertise and equipment necessary for growing a different crop.|\n|Q:||Are some cropping strategies or crop rotations better-suited to limited or deficit irrigation than others?|\n|A:||When implementing limited or deficit irrigation it is important to match crop rotation with local patterns of precipitation and evaporative demand. Certain crops like soybean, edible bean, winter wheat and sunflower are the major crops with lower water requirements. Splitting fields between corn and one of these crops would reduce total water requirements for the field and distribute the water requirements across a longer portion of the growing season. For example, peak water demands for wheat are during May and June, while corn uses the most water during July and soybean water needs peak in August. Splitting the field into multiple crops allows producers with low-capacity wells or limited water supplies to more completely meet the peak requirements of all crops (Scheneekloth, Determining Crop Mixes for Limited Irrigation).|']	['<urn:uuid:9cbb4705-02a0-4b46-9cef-1aadf24d5e55>', '<urn:uuid:ba6abbd0-1b0b-4ce6-86c8-68d434152fa9>']	factoid	direct	long-search-query	distant-from-document	comparison	expert	2025-05-12T21:31:42.699391	10	67	1744
80	What significant improvements and changes have been made to modernize the St. George church building over the years?	The church has undergone several renovations and improvements. They have sided the church, painted it inside and out, installed new carpeting, new pews, new Stations of the Cross, and added a new Room of Reconciliation. Additionally, following Vatican Council II (1962-1965), they implemented a renewed Liturgy of the Mass, updated the Sacrament of Penance and other Sacraments, and established new ways of relating to non-Catholic communities through ecumenism, including shared Thanksgiving Worship Services.	"['History of St. George Catholic Church\nOriginally St. George, and most of the surrounding territory, was part of the Archdiocese of Chicago and thereafter the Diocese of Peoria. In 1948, with the formation of the Diocese of Joliet, St. George Church of Melvin and Our Lady of Lourdes of Gibson City became the southern most parishes of the new Diocese.\nPrior to 1881, the Catholic priests from St. Joseph\'s church in Loda, Illinois cared for the spiritual needs of the Catholics in the Melvin area. In those days Catholics had few or no buildings for Mass or services. Mass was first celebrated in Melvin at the home of Barney McTernan, who was a railroad section boss. Thereafter, the arrival of the priest from Loda, usually once a month, was dependent on the weather and the condition of the few roads of that time.\nIn 1881 the St. George Parish of Melvin was officially founded. The Bishop (John Lancaster Spalding) and the people of the new parish purchased a church building, formerly owned by the Zion Methodist Congregation and re-located it on the present site of 131 South Hunt Street. The present frame structure of the church was actually built in 1889.\nThen in 1891 Fr. John P. Barry was named the first resident pastor of Our Lady of Lourdes in Gibson City, Illinois. It was at that time that St. George, Melvin, became the mission parish of Our Lady of Lourdes. Fr. Barry\'s parishioners consisted of 135 families scattered over 900 square miles and encompassing 22 small towns.\nFr. James T. Fitzgerald, in 1910, published a commemorative booklet called ""Annual Report of Our Lady of Lourdes and Missions"". At that time, St. George, Melvin and Immaculate Conception, Roberts were the Missions. In this report Fr. Fitzgerald states: ""(This Report) .... will serve also for the parishioners of St. George Church as a souvenir of perhaps the most epoch-making event in the history of the parish, namely, the remodeling, or I might say, the rebuilding of their parish church....""It was at this time that considerable work was done on the church building and the present stained glass windows were installed.\nPrior to 1961, when Fr. Kirk Memorial Hall was built in Gibson City, the youth of St. George attended religious classes in a variety of places: the church, the rectory, homes of parishioners both in Melvin and Gibson City, a country school half-way between Chatsworth and Melvin, and at St. Rose Church in Strawn, Illinois. The classes were taught by pastors, nuns, volunteer parishioners, and the parents themselves. Classes were held a variety of times: Saturdays, Sundays, after school, in the evening, and even during summer vacation.\nSt. George is grateful to have had one young man enter the priesthood: Fr. Alphonse M. Freehill, a member of the Benedictine Order, and the son of Michael A. and Mary (Egan) Freehill. Three of our women parishioners have entered Religious Orders: Emma Matt, who became a nun in Milwaukee, Wisconsin, and was then known as Sister Mary Maurina; Sister Linda Hatton, daughter of Mr. and Mrs. John (Ford) Hatton, professed her vows in the Order of the Holy Heart of Mary; Sister Catherine Wisthuff, with the Handmaids of the Precious Blood, was a parishioner of Our Lady of Lourdes when she entered the convent, but was formerly a member of St. George parish during her childhood years.\nOur St. George Parish has been blessed with Cooperative organizations. The CCW, since its establishment in 1942, has been a great help to the women of the parish and the parish as a whole. For years almost all of our social activities and their planning came through the generous efforts of many CCW members. Our religious education teachers were not only cooperative but had to be dedicated through some very difficult times in our history and with considerable effort. The Parish Council, which was formed soon after the Second Vatican Council, has also been a great help to a number of pastors and the people. Late in our present history, the Ford Knights of Columbus founded in 1979, is the official men\'s organization of St. George. Its present work and action in the Parish is very promising for our future.\nWe have sided the church, painted it inside and out, installed new carpeting, new pews, new Stations of the Cross, a new Room of Reconciliation and re-decorated. But the spirit of Vatican Council II (1962-1965) is alive and in our attitude! We have a renewed Liturgy of the Mass, the Sacrament of Penance, and other Sacraments. We also have new ways of relating to our non-Catholic brothers and sisters because of ecumenism, such as, Thanksgiving Worship Services together and so many ways of sharing ideas.\nSt. George Parish has been blessed by the steadfast faith, love, concern, and generosity of all those thousands who have heard The Word of the Lord in this small prairie church for the last one hundred thirty plus years. We shall not break faith with them!\nIn our own way, perhaps, we\'ve come a long way since early days. As people of faith, we see things, untold deeds of love, known only to God, countless blessings received...and so much forgiveness given and accepted. It is the same MIGHTY and LOVING Lord of the Universe who will raise up all things bright and beautiful far beyond a small prairie town and church in Illinois. We know.....and He knows!']"	['<urn:uuid:121bc19c-9e4f-4e4d-a556-546801dd3fb9>']	open-ended	direct	verbose-and-natural	distant-from-document	single-doc	novice	2025-05-12T21:31:42.699391	18	73	906
81	How did email change newspaper article submission?	Email revolutionized newspaper article submission by allowing writers to send their work at the touch of a button without leaving their seats. The key improvement was that sub-editors could now directly cut and paste the articles when they arrived in their inbox, which eliminated typographical errors that previously occurred when copy typists had to rewrite the articles.	"['|From typewrite to iPad--What an evolution and journey!|\nI\'ve been running columns in Zambian national weekly newspapers non-stop since 1990, i.e. for twenty-four years. At one time I was writing three per week. In two of them I have used a pseudonym to hide my identity but in one of them I have used my real name.\nAs I have thought about this, I have been gripped by the changes that have taken place between 1990 and today. If they had taken place overnight, they would have been shocking but they took place slowly. So, it is the panoramic view that is breathtaking.\nWhen I began writing for the National Mirror newspaper in 1990, I was using a typewriter. For my younger readers, a typewriter was a machine we once used for producing print-like characters on paper. It has since been buried with the dinosaur.\nApart from the energy one exerted to push the mechanical keys and to push back the paper roller after every line, the challenge was that you could not squeeze in thoughts into your composition that occurred to you afterwards. Often I also had to re-type the whole article because of some errors made while typing.\nAfter typing the article, I used to get on my bicycle and ride across town to take it to the newspaper offices in time for my weekly deadline. At that time I was running two columns concurrently—one in my own name and the other in a pseudonym.\nBeing an itinerant preacher meant that sometimes I would have to consider the fact that the articles would be needed while I was away. Thankfully, there was the facsimile (shortened to fax) machine. If you are wondering what that is, it was a machine that scanned documents and transmitted them. It has also been buried with the dinosaur.\nThe fax machine was wonderful if I was travelling to foreign countries that were developed enough to have them. However, often my ministry took me to rural Zambia where they were rare. In such cases, I would write and deliver a number of articles to cover the period of my absence. This really stretched my creative capacity.\nWhen all was done, the final agony was when I would read the article in the newspaper only to find serious typographical errors. This was because in those days the newspaper copy typists had to copy word-for-word and they were not impeccable. Sometimes the sentences would say the exact opposite of what I wanted to say. It was agony, I tell you!\nThe Personal Computer\nThen the personal computer came to Zambia and I bade farewell to the typewriter. What a change this brought to my life! One of our deacons was running a project at the University of Zambia and had some five or six desktop computers in his office. He offered me the use of one at any time I wanted to use it.\nThis solved the problem of energy lost pushing the mechanical keys and the paper roller. It also solved the problem of failure to squeeze in thoughts and correct errors as I typed. At least now I could print only when the whole job was done.\nHowever, it now meant that each time I needed to work on the columns I would cycle to the office of the deacon, work on my articles, and print them out. Then I would cycle to the newspaper premises to drop off the script before cycling back home. It was an agonizing triangle across town. At least it kept me healthy.\nYou can well understand how delighted I was when I bought my first personal computer in 1993. Its total hard drive was 24Mb and it cost me an arm and a leg. It took very serious master bedroom negotiations to finally clinch the deal to get a slice from our savings as a family in order to purchase one. Felistas told me afterwards that she thought I was being extravagant. A pastor with a personal computer!\nWell, one of the main reasons for my ""extravagance"" was that I wanted to do my work as a columnist from my own home. So, I was now back to the situation where I could write the articles as soon as I was inspired. Sometimes inspiration came in the middle of the night. Also, now I only needed to cycle one way when delivering the articles.\nThankfully, before long, software for computers became available that enabled one to send a document from the computer using a telephone line and it arrived at its destination through the fax machine. That spelt the end of my cycling to the newspaper offices. I would only call to find out if the article had “arrived”. At that time, I thought that this was a great technological achievement.\nThe Electronic Mail\nThe next major blessing was the introduction of the electronic mail (shortened to email) in Zambia. At the touch of a button on my computer, my article could now arrive at the newspaper offices without me even leaving my seat. Although the fax machine did this, this time the sub-editors only needed to “cut and paste” my articles when they arrived in their inbox. That spelt the end of the typographical errors caused by copy typists.\nAt one time the editor for the newspaper with whom I was writing the two columns said to me, ""You should see the difficulties we have in re-writing some of the articles we get from our columnists. But for you, even when I\'m going away, I just tell the sub-editors, \'When Pastor Mbewe\'s articles come, just cut and paste!\'""\nMy itinerant ministry still presented a challenge because up to that point my computer was a desktop. You can well understand my joy when the laptop computer became an option. I did not hesitate to begin bedroom negotiations and before long I had bought my first laptop, which weighed a ton but was at least portable.\nSince then, I have worked on reducing the weight of my laptop so that I can carry it around more easily and write my articles at the moment of inspiration. I now carry an iPad with me. My wife—yes, you read it correctly—recently urged me to buy one. It took one year of persuasion before I finally yielded in March 2012. Now, I am very glad I did.\nSo, as I pause on the eve of a quarter of a century of being a weekly columnist in my country, almost all my initial challenges are behind me. I can now compose my articles anywhere, even while on a queue in the bank. And I can now do it without exerting any physical energy and without re-writing the whole article. I can now send my articles from the comfort of my home or wherever I am on the planet at the touch of a button, and it will be published as it was written. Let’s face it, compared to the early nineties, this is paradise!']"	['<urn:uuid:317c3bdf-685d-4476-84e1-2a2fc8821adc>']	open-ended	direct	concise-and-natural	similar-to-document	single-doc	novice	2025-05-12T21:31:42.699391	7	57	1171
82	I'm interested in how our body deals with ongoing pressure. How do orexin and cortisol differ in handling long-term stress?	Orexin and cortisol show different patterns during long-term stress exposure. With repeated stress, orexin levels decrease over time, as shown in studies where rats exposed to daily stress for five days exhibited decreasing orexin levels in their cerebrospinal fluid and reduced activation of orexin neurons. This decrease appears to be part of the body's habituation to stress. In contrast, cortisol levels typically remain elevated during long-term stress, with consistently higher levels throughout the day. However, prolonged stress eventually depletes the body's ability to produce cortisol, leading to a state of burnout where hormone production becomes dysregulated, resulting in symptoms like fatigue and insomnia.	['What’s going on inside our bodies and brains when we respond to stress? Previously, we covered research into powerful little neuropeptides called orexins that may help regulate an individual’s vulnerability to stress. Now, we dug into fresh research from the lab of Seema Bhatnagar, PhD, an associate professor in the division of Stress Neurobiology at Children’s Hospital of Philadelphia, and conducted by Laura Grafe, PhD, a postdoctoral fellow. Dr. Grafe studies how orexins regulate behavioral and neuroendocrine responses to repeated stress. (Or, in other words, how animals behave when exposed to a stressor multiple times.) The exciting research of Dr. Grafe and her colleagues appeared recently in Neuroscience. When we chatted with Drs. Bhatnagar and Grafe, we learned five fascinating facts about how orexin can affect an individual’s stress response – and what that might mean about building resilience.\nOrexins increase arousal and can affect sleep, binge-eating, and habituation to stress.\nOrexin is a neuropeptide that’s been associated with sleep, appetite, and more recently, stress. It’s known to mediate arousal and alertness in the body. In the case of sleep, blocking orexins can help to combat insomnia, while too little orexin can cause narcolepsy. In the case of stress, high orexin levels have been linked with a higher stress response.\n“We know that with a single stressor, if you block orexins, you reduce the stress response,” Dr. Grafe said.\nIn the new study, however, she and her colleagues looked into a lesser-known aspect of the phenomenon: How orexins might be involved in the habituation to repeated stress – or what Bhatnagar describes as a “decreased response to a familiar stress that you have been exposed to over and over again.” A simplified example might be a freshman college student becoming acclimated to the onslaught of exams and homework and, with experience, finding they don’t get as flustered.\nOrexin levels drop over a period of repeated stress.\nDr. Grafe and her fellow researchers conducted a restraint test with rats: For 30 minutes a day, five days in a row, they limited the rat’s physical movement to study how the animals struggled in response. They then measured their orexin levels in the brain.\n“We found that by measuring levels of orexin in the cerebrospinal fluid and the activation of orexin neurons, both of these decreased over the five days of repeated stress,” Dr. Grafe said.\nThe finding shows that orexin levels do indeed fluctuate as the rats habituate or adapt to repeated stress – and that orexin may even play a key role in that habituation. While this was fascinating in itself, the researchers wanted to know how and why exactly this happens, and also what might happen under conditions of extreme stress, as seen in panic disorders. Learning the mechanisms behind a decrease in orexin can help researchers design treatments to mimic that process.\nBlocking the orexin 2 receptor decreases stress levels for just day one.\nOrexin has two receptors: orexin 1 and orexin 2. As with all receptors, different receptors can produce different effects when attached to a respective chemical. Using an orexin receptor antagonist, the researchers blocked the actions of orexin 2 over the course of the five days of stress to see whether stress levels still decreased in the absence of the chemical’s second receptor. From their observations, they found that orexin 2 receptor did appear to regulate the rat’s stress levels – but only on day one. One possible hypothesis: Orexin 1 receptor may play a larger role in the course of five days of repeated stress.\nIn a hyperactive state of stress, however, blocking the orexin 2 receptor decreases stress levels.\nAs part of the study, the researchers also wanted to create models of extremely high stress responses, such as panic disorder, to study how blocking orexin receptors might help with that.\n“What’s interesting is that with certain disease states like panic disorder, orexins are highly stimulated, and levels are much higher with those types of disorders,” Dr. Grafe said. “So what we wanted to do was model this.”\nThey used DREADDS technology (which stands for designer receptors exclusively activated by designer drugs) to stimulate the rat’s orexin neurons and release more orexins. In other words, the rats were incredibly stressed out.\n“We did this throughout the repeated stress to see what would happen in that hyperactive state of orexin activation,” Dr. Grafe said.\nThey found that orexin 2 now appeared to play a role in five days of stress – not just one – during this overstimulated state.\nWe can’t translate rat studies to human application just yet, but that’s the direction researchers hope to travel toward.\nWhile it’s tempting to make the connection between a rat’s behavior when put under restraint and our own resilience during our days of high pressure, we can’t exactly translate a rodent study into immediate human conclusions. Nevertheless, the researchers have their hypotheses.\n“What you can sort of hypothesize is that in somebody with panic disorder, where orexins are overstimulated, perhaps orexins are regulating their response to repeated stress,” Dr. Grafe said.\nFuture drugs may even be developed to block the actions of orexin 2, as Dr. Grafe did in the study, to soothe an aggressive stress response in the body. In current and upcoming research, Drs. Bhatnagar and Grafe will dig deeper into the link between orexins and vulnerability, and piece their discoveries together with other lab work on the neurobiology of stress resilience.', 'Cortisol is a steroid hormone that is responsible for a wide range of processes within the body including immune responses, the regulation of metabolism, and acting as an anti-inflammatory. It also plays an important role in the way in which the body responds to stress. It is created in the cortex of the adrenal glands and is transported and released through the body by our blood.\nThe balance of cortisol levels that are present within the body at any one time is extremely important for our overall health. Having too much and too little cortisol in your system can cause a number of issues that range from minor to serious. The effects of having too much cortisol in the body can include rapid weight gain, high blood pressure, muscle weakness, and severe mood swings that manifest in anxiety and depression. On the other hand, individuals who possess an extremely low amount of cortisol are susceptible to experiencing problems such as dizziness, fatigue, weight loss and a peculiar darkening in certain skin areas.\nFortunately, the body possesses a clever system that enables us to regulate the levels of cortisol that are present in our systems. The optimal level, interestingly, varies quite drastically at different times throughout the day.\nCortisol levels are generally high in the morning as we wake from a prolonged period of sleep, with an increase of up to fifty percent in the twenty to thirty minutes after waking. This is known as the ‘cortisol awakening response’. Then, as the day progresses, our cortisol levels naturally begin to drop in a fairly constant and regular fashion that is termed a diurnal rhythm, ending up as low in the late evening. This allows the body to keep a regular sleeping pattern, with the cortisol level dropping for periods of sleep, then replenishing during the following morning.\nThe body can also detect and change the timing and cycle of cortisol production and release for certain individuals. A great example is those individuals who work on night shifts. In these cases, the pattern and timing of the release of cortisol is reversed to allow for higher levels throughout the late evening and early morning hours. For those of you who travel long distances, a similar rearrangement occurs when we experience jetlag.\nCortisol levels are not just dependent on the time of day. Stress also plays an important role. The exact response depends on the type of stress, whether its short-term acute stress or long-term chronic stress. For short-term stressors like an argument or a fall, we will see a brief spike in cortisol. For longer term stressors like work stress or illness, we see a consistently higher level of cortisol at all times of the day.\nHow does all this relate to Adrenal Fatigue? Well, those long-term stressors can eventually deplete the nutrients and precursors that we need to produce cortisol and other hormones. In other words, chronic stress will raise your cortisol levels for a while, but eventually your body is unable to continue producing cortisol in such high amounts. At this point we start to see declines in not only cortisol, but also key hormones and neurotransmitters like aldosterone, testosterone, epinephrine and more. Additionally the diurnal rhythm of cortisol production is often disrupted, resulting in late-evening spikes that cause insomnia.\nThis dysregulation of hormone production is the ‘burnout’ that follows long periods of stress. The body is no longer able to regulate cortisol levels effectively, which leads to symptoms like fatigue, a lack of enthusiasm, insomnia, and a general lack of vitality.\nHow can we reverse the effects of long-term stress and regain our energy levels? A combination of good nutrition, supplementation, and effective stress management techniques can quickly result in some significant changes. In the longer term, eliminating the causes of stress is crucial for a full recovery.']	['<urn:uuid:4deed907-578d-4fb0-a18c-57bb0e737472>', '<urn:uuid:16bc5d31-8d70-4046-945c-c85fccfe31ae>']	open-ended	with-premise	concise-and-natural	distant-from-document	comparison	novice	2025-05-12T21:31:42.699391	20	103	1545
83	marine invertebrate classification xenoturbella dna analysis mistake	There was a significant mistake in the classification of Xenoturbella. Initially, genetic data incorrectly classified Xenoturbella as molluscs. This was later revealed to be an error because researchers had confused DNA from shellfish prey with genes from Xenoturbella itself. After this correction, Xenoturbella was recognized as its own phylum, clearly grouped with other bilaterally symmetrical animals.	['Animal Kingdom Gains Phylum But Loses Link\n- 3:57 PM\n- Categories: Animals\nA new rearrangement of the animal kingdom has expanded a little-known offshoot into a major branch, and opened a gap between relatively simple and highly complex life.\nOcean-dwelling flatworms called acoelomorphs were thought to represent one of life’s early stages, bridging primitive animals with radially symmetrical bodies, like jellyfish and sea anemones, and animals with bilaterally symmetrical bodies, from butterflies to humans.\nBut acoelomorphs now appear related to Xenoturbella, a bilaterally symmetrical ocean worm.\n“It was nice to have this intermediate group between jellyfish and the other complicated animals. We’ve lost that,” said Max Telford, a University College London geneticist and co-author of the new analysis, published Feb. 9 in Nature.\nThe identity of both worms is a matter of ongoing interpretation among scientists who map life’s tree. Until the early 1990s, both were lumped with the Platyhelmine, or “true flatworm,” phylum. Then genetic data reclassified Xenoturbella as molluscs and acoelomorphs as descended from that unknown, ur–complex animal.\nOver the last decade, the Xenoturbella interpretation was revealed as a mistake. It wasn’t a mollusk, but ate them: Researchers had confused DNA from shellfish prey with genes from Xenoturbella itself. Thus corrected, Xenoturbella was recognized as a phylum unto itself, clearly grouped with other bilaterally symmetrical animals.\nNow Telford’s team has turned a corrective lens on the acoelomorphs. Like other phylogenists, the team analyzed DNA from species in every phylum, searching for patterns of relationships and then constructing a tree to fit the data. But Telford and fellow researchers made one key adjustment, tweaking their algorithms to account for the unusually high mutation rates of acoelomorphs.\n“They’ve evolved much more quickly than other animals,” said Telford. “We suspected that this effect caused previous studies to put them a long way from animals with normal rates of evolution.”\nThis rate-adjusted analysis pulled the acoelomorphs out from the base of the tree of life, and set them down on a far branch alongside Xenoturbella. Together, the two Xenoturbella species and hundreds of acoelomorph species constitute a newly classified phylum, which the researchers named Xenacoelomorpha.\nXenacoelomorpha is on roughly the same branch of life’s tree as sea stars, sea cucumbers and acorn worms — not the most complicated animals, but far more complex than what the acoelomorphs were thought to be.\nAs for what might replace acoelomorphs as a candidate for root-animal status, Telford doesn’t know.\nAnother question raised by the findings is how and why acoelomorphs evolved in such fashion. Their ancestors were likely creatures with guts, fluid-filled cavities and gill slits. Acoelomorphs have none of these features, and are so physically primitive as to resemble researchers’ notions of how an ancient animal ought to appear.\n“Now we’ve got these very simple worms nested right in the middle of the complex animals,” said Telford. “How did they end up so simple? They must have lost a lot of complexity.”\nVideos: 1) Acoelomorph flatworm/Max Telford. 2) Xenoturbella/Max Telford.\n- Hagfish Analysis Opens Major Gap in Tree of Life\n- Animal Kingdom Gets a New Root\n- Early Life Didn’t Just Divide, It United\n- Green Sea Slug Is Part Animal, Part Plant\n- The Mass Extinction of Scientists Who Study Species\nCitation: “Acoelomorph Flatworms Are Deuterostomes Related to Xenoturbella.” By Herve Philippe, Henner Brinkmann, Richard R. Copley, Leonid L. Moroz, Hiroaki Nakano, Albert J. Poustka, Andreas Wallberg, Kevin J. Peterson & Maximilian J. Telford. Nature, Vol. 470 No. 7333, Feb. 9, 2011.']	['<urn:uuid:2380e62e-67dd-4cb5-86e4-ce922909f15d>']	open-ended	with-premise	short-search-query	distant-from-document	single-doc	expert	2025-05-12T21:31:42.699391	7	56	581
84	what causes brake fluid fluid problems and fixes disc drum brake vehicle maintenance	Brake fluid problems are primarily caused by fluid reaching high temperatures and boiling, as well as moisture absorption from air over time which lowers the boiling point. The solution involves using fresh fluid with a higher dry boiling point, ensuring proper reservoir cap sealing, and regular fluid replacement every few years during brake service. Both disc and drum brake systems rely on this hydraulic fluid, and any leaks in the master cylinder, reservoir, wheel cylinders, lines or hoses will reduce braking power.	['COMMON BRAKE QUESTIONS\nKryderacing has been\nin the “Trackside Service” business for many years. Along the way we have\nbeen asked many questions. Brake parts used under racing conditions undergo\nsevere abuse. Analyzing them is always a learning process for both ourselves\nand our clients. Among the more common brake problems we have encountered\nare the following.\nPROBLEM: “Boiling” of the brake fluid\nAssuming no leaks, the explanation is simple. The solution may be difficult. Air in the system is usually the result of the fluid reaching a temperature where it “boils”. Once that happens you will need to bleed all of the fluid’s air bubbles out of the system. To keep it from happening in the future you need to lower it’s operating temperature and/or raise the fluids boiling point. Lowering the operating temperature usually requires modifications to the vehicle and we will leave that discussion to individual vehicles.\nAs far as raising the boiling point of the fluid is concerned let’s start with some basics. First, be sure the fluid is fresh. Fluid boiling points slowly drop with time. The usual cause is moisture from the air slowly being absorbed by the fluid. This diluted fluid has a lower boiling point than fresh fluid. So make sure the fluid is fresh and the reservoir cap seals properly. If you are using fresh fluid and it still boils you might want to consider a higher boiling point fluid. The DOT rating is a good indicator, but can be misleading.\nThe DOT ratings are based on “dry” and “wet” boiling points. For example, a DOT 4 has higher boiling points than a DOT 3. But it is possible to have a DOT 3 fluid which will handle your boiling problem better than a DOT 4. Obviously this can be very confusing. The reason this is possible is because of the “wet” boiling point portion of the test. This portion of the DOT test is designed to show how good the fluid is after it has been in the car for a while and absorbed a certain level of moisture. In other words, the DOT ratings also take into account aging of the fluid. Another way of saying this is that the “dry” boiling point is for fresh fluid and the “wet” is for old fluid. What if you had a fluid which had a very high “dry” test result but didn’t age very well and tested poorly for the “wet“ portion of the test? Several racing brake fluids fit this description.\nSo what is the solution?\nIf you are going to change the fluid frequently because of your track activities,\ndon’t go solely by the DOT rating. Look for the “dry” boiling point temperature\non the labeling. That reading represents the conditions under which you\nare using the fluid. Obviously there are other factors to consider, but\nthis “dry” boiling point is one of the more critical if you are worried\nabout boiling your fluid.\nPROBLEM: Silicone brake fluids\nWe don’t see this\nproblem much anymore. Thank goodness. Most of the people we have met who\nused silicone fluid bought it because it was rated DOT 5. Sounds good,\nbut like the story above, there are other things to consider. Silicone\nfluid offers big advantages at not absorbing moisture, but racers we have\ntalked with do not like the “feel” of the pedal with silicone brake fluids.\nNote: Do not confuse “silicone” with “synthetic”. They are two very different\nthings and we have had great luck with the Motul synthetic.\nPROBLEM: Rotor Grooving (“Record” appearance)\nWhat is happening\nis the temperature has reached a level where it is boiling the metal surface\narea of the rotor. As in the case of boiling fluid the two basic approaches\nare to either use better components, or lower the temperature. Usually\npeople blame either their brake pads or rotors and they decide to replace\none or both of them with better components. In a problem talked about above,\nwe used better brake fluid to cure a boiling problem. But, using better\npads and/or rotors may not be the best approach to cure the record appearance.\nWhile there are definite differences in the quality of rotors, unless you\ngo to non-ferrous materials such as carbon (very expensive) the boiling\npoints of the metal do not vary drastically. It would be a better use of\nyour time and money to initially concentrate on lowering the temperatures\nthrough the use of air ducts, brake balancing front-to-rear, and other\nPROBLEM: Picking the proper brake pad compound\nIf you are looking for a simple answer as to what brake compound you should use, forget it. If you talk to a factory engineer from one of the leading brake pad manufacturers you are likely to end up being more confused than when you started. That’s because they want detailed information about your exact usage. That goes beyond the model of car and type of racing you do. Even the driving style of individual drivers (both drivers being very talented) can result in different pads being recommended.\nOur suggestion is to consult with several sources. We at Kryderacing will be happy to talk with you and tell you our recommendations based on what we have seen at racetracks and what we know from our industry contacts regarding the latest products.\nProblem: Improper brake fluid bleeding\nOver the years we have seen lots of different ways to bleed brakes. A lot of very successful racing mechanics say their method is the only way that works. But different mechanics have different methods. Who is right? From what we have seen of all these seemingly correct methods, they have one common factor: DO NOT MOVE THE FLUID RAPIDLY THROUGH THE LINES. If you ever see your mechanic rapidly pumping the brake pedal in order to bleed the brake, find a new mechanic. If they are slow and methodical, you are probably getting a good bleed job.\nRather than go into\na long detailed explanation of any particular procedure, we suggest you\ngo to “www.baer.com/bleeding”\nfor one of the more thorough explanations. The Baer website has an extensive\nQ&A section on brakes and many other questions are answered.', 'Complete Guide to Disc Brakes and Drum Brakes\nWhen it comes to driving safety, your brakes are critical. We’ve compiled a guide to help you understand your brakes. This includes disc and drum brakes, how they work, how they’re different and alike, why you may have both types on the same vehicle, what kind of wear to expect, and what parts will need maintenance.\nWhat you’ll find in this guide:\nWhat Is a Simple Brake System\nMaster Cylinder: Contains a piston assembly and brake fluid.\nBrake Fluid: Transfers the hydraulic pressure.\nDisc Brake Assembly: Includes caliper, pads, and rotor.\nBrake Lines and Hoses: Carries the brake fluid to the brake assemblies.\nDrum Brake Assembly: Includes shoes, wheel cylinder and drum.\nBraking System Basics\nDisc and drum brakes are based on a hydraulic pressure system. Braking starts with a mechanical force — your foot pressing the brake pedal. The end result is that your vehicle safely stops. Below is a quick explanation of the five points of the system.\nA piston compresses brake fluid inside the master cylinder located under your vehicle’s hood near your engine. This creates hydraulic pressure, generating a much bigger force than that of the small effort of pressing down on the pedal. See above image.\nThe pressure is transferred via the brake fluid through the brake lines and/or brake hoses (flexible tubes) that connect the lines with brake assemblies at each wheel.\nThere, wheel cylinders convert that hydraulic pressure back to mechanical force. Brake friction material is pushed against the brake disc or drum, slowing or stopping your vehicle.\nThe disc brake assembly is designed to safely stop your car when activated. More details about how this works below.\nThe drum brake assembly, equipped on some vehicles, is also designed to stop your vehicle when other points on the system are activated. More details below.\nBasics of Disc Brakes\nDisc brakes are found on most vehicles today. They are mounted on the front and/or rear axle. To stop a wheel (and your car), a disc brake uses a caliper fitted with brake pads to grab a spinning disc or rotor.\nWhat is a caliper?\nThe caliper is an assembly mounted to the vehicle with a bracket. It frames the rotor, looking and functions like a c-clamp. It contains the following components:\nBrake pads are a friction material bonded to a metal plate that helps stop your vehicle.\nOne or multiple pistons to push the brake pads against the rotor when you brake.\nA bleeder screw to allow for servicing the brakes and replacing the fluid.\nA rubber piston seal that prevents brake fluid leakage and retracts the piston when the brakes are released.\nA dust boot to keep contaminants out of the cylinder.\nAnti-rattle clips that add stability.\nWhat is a rotor?\nThe rotor is made of cast iron or a steel/cast iron composite. It’s attached to the wheel hub and turns with the wheel. This is the surface that the brake pads make contact with to slow your vehicle. Here’s how it works: when you step on the brakes, pressurized brake fluid pushes against the pistons inside the caliper, forcing the brake pads against the rotor. As the brake pads press against both sides of the disc, the friction stops the wheel’s rotation.\nRotors can either be solid or vented. Vented rotors have more surface area and can more easily dissipate heat.\nTwo Types of Disc Brakes: Floating & Fixed\nThere are two types of disc brakes. They are floating and fixed, named for the type of brake caliper used.\nA floating caliper (also called sliding) is the most common type. It has one or two pistons. When the brakes are applied, the inner brake pad is forced against the disc while, at the same time, the caliper body moves closer to the rotor. This action forces the outer brake pad against the rotor.\nThe fixed caliper design has one or more pistons mounted on each side of the rotor. The caliper itself doesn’t budge or move. It’s rigidly fastened to a brake caliper bracket or spindle. When the brakes are applied, only the caliper pistons move, pressing the brake pads against the disc.\nBasics of Drum Brakes\nDrum brakes are most often used on the rear axle of today’s vehicles. They are also very common on trailers. Manufacturers don’t use brake pads as the friction material in drum brakes. Instead, a drum brake system has a wheel cylinder with pistons that push brake shoes out against the inside of a spinning drum. This contact slows and stops the rotation of the brake drum and the wheel, bringing your vehicle to a stop.\nDrum vs. Disc Brakes\nMany of today’s vehicles have disc brakes on all four wheels. However, some models still have disc brakes on the front axle and drum brakes on the rear.\nWhy are discs put on the front? It’s due to weight factors. A typical, unloaded vehicle is about 10 percent heavier in front due to the engine. When you hit the brakes, the weight of the car transfers to the front. Therefore, more braking power is needed in the front, making disc brakes the perfect option for the job.\nDisc vs. Drum Brakes: Servicing\nServicing disc and drum brakes is important. When it comes to repairs, drum brakes have more hardware and can be more complex to service. But the materials typically cost less than disc brake pads, calipers, and rotors. Disc brakes generally consist of fewer components and are easier to service.\nBrake Maintenance: What Needs to Be Checked\nA lot of heat is created when you apply the brakes. When you press on the brake pedal, you’re converting the kinetic (moving) energy of the vehicle into thermal energy (heat), subjecting many parts to very high temperatures. This means a lot of wear and tear even in normal conditions. Some brake components will need to be replaced over the life of a vehicle, including:\nBrake Pad or Brake Shoe Friction Material: Disc brake pads slow the rotor through friction and wear with normal use. Eventually, they become too thin to function properly. The same applies to drum brakes. The friction material gets worn out and braking can become compromised. These components should be inspected regularly to avoid wearing them to the point that they cause damage to additional parts of your vehicle (rotor or drum).\nBrake Fluid: The brake system should be checked regularly for leaks and fluid should be replaced every few years (usually when the brakes are serviced). Any leak in the master cylinder, the brake fluid reservoir, the wheel cylinders, lines or hoses will reduce the hydraulic pressure that’s created when the brakes are activated. If the system can’t generate sufficient force needed to create braking power, you’ll notice your brake pedal needs to be pressed a lot further in order to slow or stop.\nHow often should I replace my brake fluid?: The brake fluid should be replaced every few years. This is usually done when brakes are serviced. While brake fluid is specifically formulated to prevent corrosion of the brake hydraulic components, time, heat, and moisture can lower the boiling point of the fluid. This results in less pressure in the hydraulic system forcing you to push down farther on the brake pedal to slow your vehicle. Other impurities, such as road grime, brake dust, rust, and more can get into the brake fluid, reducing braking efficiency and possibly damaging parts of the braking system.\nSeals: These rubber rings keep the hydraulic fluid from leaking and protect it from moisture and contaminants. They also cause the piston to return to its off position so the brake pads disengage properly when you release the brake pedal. If this doesn’t happen, you could experience brake drag, premature wear, and the vehicle may pull to one side when you brake.\nBrake Lines/Hoses: Brake lines and hoses are an essential part of the braking system. Rubber hoses can wear over time, while brake lines can be subject to corrosion or other damage. When this happens, it may compromise brake efficiency, loss of hydraulic pressure, pulling to one side, etc. Routine inspections, including hoses and lines, are critical to avoiding these issues.\nRotors: Rotors should be inspected regularly for even wear, physical damage, or cracking from excessive heat. These conditions can compromise braking power and efficiency. Brake rotors should be resurfaced or replaced when you have your brakes serviced (brake job or pads replaced). If you are experiencing any of the previously mentioned issues, your rotors may need to be repaired or replaced sooner.\nDust Boots: Brake components are constantly exposed to road debris and brake dust. The dust boot prevents grime from entering the caliper piston. If it fails and can’t do its job, piston damage can occur, causing brake drag, pulls and premature wear.\nMaster Cylinder: Regular fluid maintenance is important for prolonging cylinder life. Failing master cylinders can leak internally. In this case, you may get a low or soft pedal feel. You might not notice any visible fluid loss.\nNOTE: There are different approaches to brake service. See our Brake Servicing 101 and learn why it’s important to maintain more than just the brake pads or drum brake shoes.\nHow Often Do Brakes Need to Be Serviced?\nThere’s no set timeframe. The need to replace or service your brakes depends on your vehicle, driving style, climate and road conditions. To ensure proper braking when you need it, get your brakes checked regularly.\nStill have questions about your brakes? Review these frequently asked brake service questions.\nDisc and drum brakes are built differently, with somewhat different advantages. Both work as part of the hydraulic brake system. This system is constantly under high pressure, subject to heat, and can be compromised by road grime, air, brake dust and moisture. It’s important to get regular brake inspections to keep everything in proper working condition. Refer to your owner’s manual for a recommended schedule. Remember that odd brake sounds, smells or performance are indicators you need to get your vehicle to Les Schwab right away.']	['<urn:uuid:bb74bb60-ac7e-43e4-a2d2-1274447f4854>', '<urn:uuid:88e9c02a-59ac-4cb0-a6f7-64420ed04c98>']	factoid	direct	long-search-query	distant-from-document	three-doc	expert	2025-05-12T21:31:42.699391	13	82	2720
85	How do quilt dimensions change when working with different strip sizes, specifically when using 5.5-inch finished width strips to create a large-scale project?	When using 5.5-inch finished width strips, multiply this width by 16 (the number of rows) to get the quilt length of 88 inches. For the width, multiply your desired width (70 inches) by 16 rows to determine the initial strip length needed (1,120 inches). Then divide 1,120 by your strip length (42 inches) to calculate the number of strips needed (about 26.6 strips).	"[""I have made three traditional jelly roll race quilts (here's one) using the tutorial from the Missouri Star Quilt Company. I really enjoy the mindless sewing and the random placement of these type of quilts. Basically, you make one really long strip by sewing all the pieces end to end. Then you keep folding it in half and sewing down one side. The only thing I do differently is iron the strips to one side after I sew each long seam.\nI wanted to use up scraps and yardage from my friend, but had no idea how big it would end up. So I made some calculations to make an educated guess. Every time you fold the strip in half you are doubling the length of the quilt. The number of rows grows in this order: 1, 2, 4, 8, 16, 32...*\nMy scraps were all cut 4 inches wide to finish at 3.5 inches. So I multiplied 16 by 3.5 to get a length of 56 inches. I decided that I wanted to make the width about 42 inches (making the backings easier to piece). So I multiplied 16 by 42 to get 672 inches. 672 inches is about the length I want my very first super long strip to be.\nMy four inch strips ranged in length from 16 to 21 inches. I guess that the average length was about 18 inches. So I took 672 and divided it by 18. This gave me 37.3 strips. Through these calculations I estimated that I would need 37 or 38 of these four inch strips per quilt.\nHappily my estimation was very close. I made five of these quilts with 37 strips, and they all ended up between 42 and 45 inches wide. I just trimmed one side to make it the size I wanted. I was able to use what I trimmed as part of the backing.\nOne thing to note when using strips which are not all the same length is that the chunks will stagger in places and almost match in others.\nI think that is part of the charm. I also love that although you may start with nearly identical quilt kits, the quilts are not identical.\nTo see more of these quilts and the story behind them, check out this post. :)\nYou can use these calculations to make a jelly roll race type quilt using any size of scraps or yardage. I will use 6 inch cut, 5.5 inch finished width of fabric strips as an example to make a quilt top that is 70 x 88 inches.\n- Start by multiplying the finished width of your strip (5.5 inches) by the number of rows (16) and you end up with the length of your quilt (88 inches) Remember that the number of rows doubles each time: 1, 2, 4, 8, 16, 32...*\n- Multiply what you want the width of your quilt to be (70 inches) by your number of rows (16). This gives you the length of your very first strip (1,120 inches -- not too bad considering the length of the regular jelly roll race is 1600 inches)\n- Divide the length of the long strip (1,120) by the length of your strips (42 inches with straight seams) and you get the number of strips you need. (26.6). Just remember that first strip should be half the length of the others to make it stagger.\nI may just have to make one that size. I think that would be a quick, fun stash busting quilt!\n*An exception to this doubling rule for the number of rows is if you stop after a few seams and divide the strip set into thirds. Then you could multiply the number of rows by 3 instead of two. For example, it could go: 1, 2, 4, 8. Stop and cut the strips into thirds. Then sew the three sections together to get 24 rows. Dividing it into thirds can be helpful with larger quilts like queen size. You could even divide all your fabric strips into thirds before you start and sew each third individually. It would help distribute the prints more evenly if you were using repeats.\nClear as mud? Please let me know if you have any questions. I LOVE quilty math!\nP.S. I am linking up with Cynthia for Oh Scrap and Lorna for Let's Bee Social.""]"	['<urn:uuid:6e3eeba4-5a2e-4956-9c62-2ec4d5b94d10>']	factoid	direct	verbose-and-natural	distant-from-document	single-doc	expert	2025-05-12T21:31:42.699391	23	63	735
86	How long does the Level 2 background screening remain valid?	The Level 2 background screening is valid in the State of Florida for 5 years.	"[""SENIOR COMPANION TRAINING\nJFS Career & Employment Services offers a Senior Companion Training Program. The two-day program is for those who want to learn how to become a caregiver or for those who are placed in the position of taking care of a family member. The program is free, but registration is required.\nFAQs ABOUT SENIOR COMPANION TRAINING\n1. WHAT SERVICES DOES A SENIOR COMPANION PROVIDE TO CLIENTS?\n- Senior Companions are trained to work specifically with seniors to provide:\n- Assistance with Daily Tasks\n- Companionship and Daily Walks\n- Nutritious Meal Preparation\n- Encouragement to Participate in Social Activities\n- Advocacy in the Healthcare Setting\n- Hospital Visitation\n- Respite to Primary Caregivers\n- Transportation using your own vehicle to Shopping and Appointments\n2. WHAT WILL I NEED TO PROVIDE?\nAfter you complete your training you will need to provide the following to JFS at Home to be considered for employment:\n- Completed JFS at Home Employment Application\n- Copy of Driver’s License\n- Copy of Social Security Card\n- Copy of Current Auto Insurance\n- Physical Exam statement from doctor within the last year\n- Completed Chest X-Ray or TB Test (chest x-ray within last 5 years or TB test completed within last year)\n- Copy of a Voided Check\n- Complete the Drug Screening\n- Complete the Level 2 Background Screening and Fingerprinting\n3. HOW MUCH DOES THE BACKGROUND SCREENING COST?\n- The Level 2 background screening costs $85 and is valid in the State of Florida for 5 years.\n4. DOES MY DRIVING RECORD AFFECT BEING HIRED?\n- JFS at Home may check the motor vehicle records annually for all new and current employees with driving responsibilities. Any covered employee without a valid driver's license or unsatisfactory driving record will not be allowed to drive on company business.\n- JFS at Home has the right to restrict driving on company business due to driving record and/or if personal limits are lower than the minimum. Only those drivers that meet the following conditions will be allowed to drive on company business:\n- No More Than Two Moving Violations in the Past Three Years\n- No At Fault Accidents Within the Past Three Years\n- No Convictions of Driving Under the Influence (DUI), Reckless Driving, Driving While Intoxicated (DWI), Vehicular Manslaughter, Driving Dangerously, or Other Similar Type of Offence\n5. HOW LONG IS THE SENIOR COMPANION TRAINING PROGRAM?\n- The Senior Companion Training Program takes 2 days to complete. Both days start at 9:30 a.m. and finish at 4:00 p.m. You will have breaks throughout the training including a break for lunch.\n6. WHERE IS THE TRAINING LOCATED?\n- 6413 Congress Avenue\n- Suite 225\n- Boca Raton, FL 33487\n7. IF I AM ALREADY LICENSED IN THE STATE OF FLORIDA, DO I HAVE TO COMPLETE THE SENIOR COMPANION TRAINING PROGRAM?\n- If you are licensed in the State of Florida as a Home Health Aide or Certified Nursing Assistant, you do not need to complete the Senior Companion Training Program. If this applies to you, please call JFS at Home directly at 561.852.3116.\n8. WHAT ADDITIONAL REQUIREMENTS ARE THERE FOR BECOMING A SENIOR COMPANION?\n- Please be advised that reliable transportation and a smart phone are required in order to become a Senior Companion.\n9. HOW MUCH DOES A JFS AT HOME SENIOR COMPANION MAKE HOURLY?\n- Senior Companions earn a competitive hourly rate + reimbursement for mileage when clients are driven in the companion’s car (not for commuting to and from client).\n10. HOW WILL I BE PAID?\n- Senior Companions are employees, not independent contractors, and as such, they complete a W4 and taxes are taken out according to what they indicate on their W4. Employees are paid via direct deposit every two weeks on Thursdays.\n11. HOW MANY HOURS PER WEEK DOES A SENIOR COMPANION WORK?\n- We are looking for a commitment of at least 6-9 hours a week which is typically 2 or 3, 3-hour shifts.\n12. WHAT CITIES WILL I BE WORKING IN?\n- Current active cities are Boca Raton and Delray Beach.""]"	['<urn:uuid:cff0d42f-1f5b-4be2-bb72-b078259009f2>']	factoid	with-premise	concise-and-natural	similar-to-document	single-doc	expert	2025-05-12T21:31:42.699391	10	15	683
87	I'm good with my hands. What skills do metal workers need?	Metal working requires several key physical abilities and skills: Near Vision to see details at close range, Arm-Hand Steadiness to maintain steady movements, Control Precision to adjust machine controls accurately, and Manual Dexterity to quickly manipulate objects. Important skills also include Operation Monitoring (watching gauges and indicators), Operation Control of equipment, and Quality Control Analysis for evaluating product quality.	"[""Precision Metal Working\nTypes of Degrees Precision Metal Working Majors Are Getting\nThe following table lists how many precision metal working graduations there were in 2018-2019 for each degree level.\n|Education Level||Number of Grads|\nWhat Precision Metal Working Majors Need to Know\nPeople with careers related to precision metal working were asked what knowledge areas, skills, and abilities were important for their jobs. They weighted these areas on a scale of 1 to 5 with 5 being the highest.\nKnowledge Areas for Precision Metal Working Majors\nThis major prepares you for careers in which these knowledge areas are important:\n- Mechanical - Knowledge of machines and tools, including their designs, uses, repair, and maintenance.\n- Mathematics - Knowledge of arithmetic, algebra, geometry, calculus, statistics, and their applications.\n- Production and Processing - Knowledge of raw materials, production processes, quality control, costs, and other techniques for maximizing the effective manufacture and distribution of goods.\n- English Language - Knowledge of the structure and content of the English language including the meaning and spelling of words, rules of composition, and grammar.\n- Design - Knowledge of design techniques, tools, and principles involved in production of precision technical plans, blueprints, drawings, and models.\nSkills for Precision Metal Working Majors\nprecision metal working majors are found most commonly in careers in which the following skills are important:\n- Operation Monitoring - Watching gauges, dials, or other indicators to make sure a machine is working properly.\n- Operation and Control - Controlling operations of equipment or systems.\n- Monitoring - Monitoring/Assessing performance of yourself, other individuals, or organizations to make improvements or take corrective action.\n- Critical Thinking - Using logic and reasoning to identify the strengths and weaknesses of alternative solutions, conclusions or approaches to problems.\n- Quality Control Analysis - Conducting tests and inspections of products, services, or processes to evaluate quality or performance.\nAbilities for Precision Metal Working Majors\nAs a precision metal working major, you will find yourself needing the following abilities:\n- Near Vision - The ability to see details at close range (within a few feet of the observer).\n- Arm-Hand Steadiness - The ability to keep your hand and arm steady while moving your arm or while holding your arm and hand in one position.\n- Control Precision - The ability to quickly and repeatedly adjust the controls of a machine or a vehicle to exact positions.\n- Manual Dexterity - The ability to quickly move your hand, your hand together with your arm, or your two hands to grasp, manipulate, or assemble objects.\n- Problem Sensitivity - The ability to tell when something is wrong or is likely to go wrong. It does not involve solving the problem, only recognizing there is a problem.\nWhat Can You Do With a Precision Metal Working Major?\nBelow is a list of occupations associated with precision metal working:\n|Job Title||Job Growth Rate||Median Salary|\n|Computer Numerically Controlled Machine Tool Programmers, Metal and Plastic||16.3%||$53,190|\n|Computer-Controlled Machine Tool Operators, Metal and Plastic||1.1%||$40,070|\n|Sheet Metal Workers||8.6%||$48,460|\n|Solderers and Brazers||5.6%||$41,380|\n|Welders, Cutters, and Welder Fitters||5.6%||$41,380|\nWho Is Getting a Bachelor’s Degree in Precision Metal Working?\nAt the countrywide level, the racial-ethnic distribution of precision metal working majors is as follows:\n|Race/Ethnicity||Number of Grads|\n|Black or African American||0|\n|Hispanic or Latino||0|\nStudents from other countries are interested in Precision Metal Working, too. About 20.0% of those with this major are international students. The most popular countries for students from outside the country are:\n- Saudi Arabia\nHow Much Do Precision Metal Working Majors Make?\nSalaries According to BLS\nPrecision Metal Working majors often go into careers where salaries can range from $38,140 to $45,250 (25th to 75th percentile). This range includes all degree levels, so the salary for a person with just a bachelor’s degree may be a little less and the one for a person with an advanced degree may be a little more.\nTo put that into context, according to BLS data from the first quarter of 2020, the typical high school graduate makes between $30,000 and $57,900 a year (25th through 75th percentile). The average person with a bachelor’s degree (any field) makes between $45,600 and $99,000. Advanced degree holders make the most with salaries between $55,600 and $125,400.\nAmount of Education Required for Careers Related to Precision Metal Working\nSome careers associated with precision metal working require an advanced degree while some may not even require a bachelor’s. Whatever the case may be, pursuing more education usually means that more career options will be available to you.\nHow much schooling do you really need to compete in today’s job market? People currently working in careers related to precision metal working have obtained the following education levels.\n|Education Level||Percentage of Workers|\n|Less than a High School Diploma||9.7%|\n|High School Diploma - or the equivalent (for example, GED)||51.6%|\n|Post-Secondary Certificate - awarded for training completed after high school (for example, in agriculture or natural resources, computer services, personal or culinary services, engineering technologies, healthcare, construction trades, mechanic and repair technologies, or precision production)||24.6%|\n|Some College Courses||7.8%|\n|Associate’s Degree (or other 2-year degree)||4.7%|\n|Post-Baccalaureate Certificate - awarded for completion of an organized program of study; designed for people who have completed a Baccalaureate degree but do not meet the requirements of academic degrees carrying the title of Master.||0.6%|\n|Post-Master’s Certificate - awarded for completion of an organized program of study; designed for people who have completed a Master’s degree but do not meet the requirements of academic degrees at the doctoral level.||0.0%|\n|First Professional Degree - awarded for completion of a program that: requires at least 2 years of college work before entrance into the program, includes a total of at least 6 academic years of work to complete, and provides all remaining academic requirements to begin practice in a profession.||0.2%|\nOnline Precision Metal Working Programs\nIn the 2018-2019 academic year, 990 schools offered some type of precision metal working program. The following table lists the number of programs by degree level, along with how many schools offered online courses in the field.\n|Degree Level||Colleges Offering Programs||Colleges Offering Online Classes|\n|Certificate (Less Than 1 Year)||1,052||2|\n|Certificate (1-2 years)||1,179||4|\n|Certificate (2-4 Years)||83||0|\n|Doctor’s Degree (Research)||0||0|\n|Doctor’s Degree (Professional Practice)||0||0|\n|Doctor’s Degree (Other)||0||0|\nIs a Degree in Precision Metal Working Worth It?\nThe median salary for a precision metal working grad is $41,090 per year. This is based on the weighted average of the most common careers associated with the major.\nThis is 3% more than the average salary for an individual holding a high school degree. This adds up to a gain of about $23,800 after 20 years!\nMajors Related to Precision Metal Working\nYou may also be interested in one of the following majors related to precision metal working.\n|Major||Number of Grads|\n|Other Precision Production||177|\n|Leatherworking & Upholstery||112|\n|Precision Production Trades||14|\n- College Factual\n- College Scorecard\n- National Center for Education Statistics\n- O*NET Online\n- U.S. Bureau of Labor Statistics\n- Usual Weekly Earnings of Wage and Salary Workers First Quarter 2020\nMore about our data sources and methodologies.\n|Request Info||Southern New Hampshire University You have goals. Southern New Hampshire University can help you get there. Whether you need a bachelor's degree to get into a career or want a master's degree to move up in your current career, SNHU has an online program for you. Find your degree from over 200 online programs. Learn More >|""]"	['<urn:uuid:396e2777-fea8-4590-a42a-2f3132c99ad1>']	open-ended	with-premise	concise-and-natural	distant-from-document	single-doc	novice	2025-05-12T21:31:42.699391	11	59	1234
88	jal neti kriya nasal cleansing technique compared vinyasa yoga sequence breath connection differences similarities	While both Jal Neti Kriya and Vinyasa yoga involve breath work, they serve different purposes and are performed differently. Jal Neti is a purification technique that cleanses the nasal passages using warm saltwater, flowing from one nostril to the other, to remove toxins and clear respiratory blockages. In contrast, Vinyasa yoga focuses on synchronizing breath with movement, creating flowing sequences of postures where inhales and exhales are coordinated with specific movements. The main similarity is their emphasis on breathing - Jal Neti aims to improve respiratory function, while Vinyasa uses breath to guide movement through poses. However, Jal Neti is specifically a cleansing practice, while Vinyasa is a complete yoga practice that integrates breath, body movement, and mind.	['Yoga is an ancient Indian body of knowledge that not only comprises of physical exercises but also has techniques that replenish your mind and soul. Yoga and Ayurveda suggest various ways to cleanse your body so that it can function better. One of the age-old techniques that have been used to clean the respiratory system and remove toxins from the nasal passage is called Jal Neti. Jal Neti is a natural technique used by yogis to prevent respiratory ailments and most importantly, to be able to breathe well. Yogis have practiced it for centuries and still vouch for its powerful benefits.\nWhat is Jal Neti Kriya?\nJal Neti is one of the six purification procedures or ‘Shatkarmas’. According to Ayurveda Expert, Dr. Ashutosh Gautam from Manta Spa in New Delhi, “Jal Neti is a technique of cleansing your nasal path to remove the mucous and blockages, if any. It involves pouring water in one nostril and flushing it out from the other nostril helping you keep the nasal passage clean.” Jal Neti is a way to cleanse your sinuses and consequently keep the respiratory tract clear of dirt and toxins. The sinus cavities get clogged with impurities which may cause inflammation, infections and other respiratory disorders.\nThe Many Benefits of Jal Neti Kriya\n1. Regular practice can help keep your nasal passage clean making your respiratory system strong.\n2. It is effective in dealing with asthma and makes it easier for asthmatic patients to breathe.\n3. It is a great technique to keep your ears, eyes and nose away from infections.\n4. It is effective for people with bronchitis as it keeps the nostrils clear so you don’t have to breathe from your mouth.\n5. It has a cooling effect on the brain by drawing out excessive heat and even relieves you from migraine.\nWhat is the Correct Way to Practice Jal Neti Kriya?\nTo practice this technique, you must have a Jal Neti pot which has an elongated nozzle and is small enough to be inserted gently into one of the nostrils during the process. Dr. Ashutosh recommends a copper Jal Neti pot to make it more effective. Before starting the process, ensure that you have the following things with you:\n- Room temperature or lukewarm water\n- A Jal Neti pot\n- A pinch of salt\nMix the salt with water and pour it into the Jal Neti pot.\nYoga Expert Vikram Malik from Rasayana Yoga and Fitness suggests the following steps to practice Jal Neti:\n1. This process should be done while you sit as it will help you stay calm and stable. You can also stand and bend towards the front to let the water flow in the right position.\n2. Place the spout or the nozzle of the pot inside the left nostril. The cone present at the end of the nozzle should be inside your nostril.\n3. Tilt your head towards the right side and open your mouth to breathe. Adjust your head to a certain point where the water flows from the left nostril into right nostril and is finally flushed out. Continue till the water in the pot finishes.\n4. Fill the pot again and repeat the process again with the right nostril.\nYou may experience irritation in the nose, sneezing or coughing during the first few times. Yoga Expert Vikram Malik suggests that you must ensure that your nose feels dry after completing the entire process. If there is some moisture, you can practice Kapalbhati Pranayama after performing Jal Neti which will draw out any remnants.\nAccording to Dr. Ashutosh Gautam, Jal Neti should be practiced carefully else it may have serious consequences. For instance, moisture in your nasal passage can cause infections. So, make sure your nose is dry after performing the technique. Do not ingest the water as it can give bacteria a way to enter the body and cause fungal growth. If you are practicing Jal Neti for the first time, ensure that you are fit to practice this process and even if you are, it is recommended to practice it under expert supervision.', 'Vinyasa Yoga definition/types\nVinyasa literally means to place in a certain way or put together. The Sanskrit breakdown of vinyasa is as follows:\nVi – a particular way\nNysasa – to place\nSo, what does vinyasa mean in a yoga context? Here are some examples:\nA Vinyasa yoga class, as defined by Mark Stephens, is a “Continuous sequence of breath-linked postures.”\nVinyasa yoga is an umbrella term referring to a yoga practice that uses the breath, body, and mind to ‘flow’ through postures. The counterpart of a Vinyasa-style yoga class would be Yin or Restorative yoga which promotes long holds with less focus on transition. The key component to a Vinyasa practice is the synchronization of the breath and movements. If your breath is in harmony with the movements of your body (ex: inhale arms above your head, exhale arms at your sides), you are doing Vinyasa yoga!\nKrama means progression of movements. Vinyasa Krama is the progression from simple to complex. In a yoga class, this means starting with a basic technique then adding on, working towards a more advanced technique. An example of this could be:\nwide-legged forward fold > tripod > headstand\nThis can be either a sequence within a class, or the entire Vinyasa class preparing for an advanced posture (also known as a “peak pose”).\nVinyasa can also refer to a specific sequence of postures within a yoga class. The most common being:\nexhale: low plank > inhale: up dog > exhale: down dog\nThis vinyasa can be cued in ANY yoga class, not just a Vinyasa yoga class. Sometimes a short flow can feel nice after a long hold in a Yin practice.\nElements of Practice\nVinyasa is the 5th element of yoga according to Ashtanga tradition and refers to putting elements 1-4 (breath, lock, focus, posture) all together to form a whole yoga practice. If you want to learn more about the 5 Elements of Practice, click here! [insert link to YOGA FOR BEGINNERS article]\nHelpful Tips for a Vinyasa Practice\n- Ujjayi pranayama\n(Also called ‘victory breath’) is commonly cued during a vinyasa practice. Ujjayi involves breathing in and out of the nose with a slight constriction at the back of the throat. This constriction causes the breath to sound like ocean waves or Darth Vadar’s breath. If you are uncomfortable doing Ujjayi breath, simple deep breathing in and out of the nose is an excellent alternative. Either way, try to match your inhales and exhales.\n- Mind/Body/Breath Connection\nIf you are having trouble keeping up, don’t stress! Breathing is the most important part of any yoga practice. Come back to your breath at any time in an easy posture, such as a child’s pose.\n- Ask questions!\nVinyasa classes can vary from high-intensity power yoga to slow flow. If you are unsure about the nature of a class, ask the teacher or call the studio. Yoga teachers are always willing to suggest modifications for postures or techniques beyond your comfort zone. We want yoga to be accessible and fun for everybody!']	['<urn:uuid:d2430917-bfd4-4d7f-acbc-5e60522372af>', '<urn:uuid:7b13603d-93f3-4ed7-92d9-2bc8bf954df1>']	open-ended	with-premise	long-search-query	similar-to-document	comparison	expert	2025-05-12T21:31:42.699391	14	118	1196
89	pregnant woman foods vs teenager foods how much calcium needed daily difference	Teenagers and pregnant women have different daily calcium requirements. Adolescents ages 13-18 need 1,200 mg of calcium daily, as this is one of the fastest growth periods when people reach their maximum bone density - by age 17, they will have established more than 75% of their adult bone mass. Pregnant women under 19 also need 1,200 mg daily, while adult pregnant women need 1,000 mg, as the unborn child requires calcium for bone structure development. If insufficient calcium is available in the blood vessels, the body will use reserves from the pregnant woman's bones. Both groups can obtain calcium from similar sources like milk products, green vegetables, and fortified foods, but teenagers particularly need to avoid sodas as they interfere with calcium absorption.	['Calcium (Ca) is one of the most well-known minerals and most strongly represented minerals in the human body in terms of quantity. A calcium deficiency in the human body is understood to be a concentration of calcium in the blood serum that is too low, i.e. less than 2.3 mmol/L (millimoles per litre).\nIt is recommended that adults take in 1,000 mg of calcium daily via food intake. Milk and milk products are good sources, but plant-based foods as well as certain mineral waters also provide a contribution towards calcium supply.\nCalcium is required for many bodily functions. It plays an important role in blood clotting, muscle and nerve activity, defence against inflammations and allergies as well as the function of heart, lungs and kidneys. But perhaps the most well-known task of the essential mineral is the development and preservation of bones and teeth. About 99 % – approximately 1.2 kg – of the total amount of calcium in the body is in the bones and teeth.\nBones are living tissue that is subject to constant development and decomposition processes. There is a dynamic balance in which bone minerals are absorbed from the blood vessels and released again.\nIf not enough free calcium is available in the blood vessels, the body falls back on the calcium reservoir and removes it from the bones. So the concentration of calcium in the blood is initially kept in the normal range in case of insufficient calcium intake.\nA more long-term calcium deficiency can lead to a decalcification of bones and teeth, which among other things can result in an increased risk of bone fractures and bone deformations. Osteoporosis (bone loss) can also be a possible consequence of a deficiency.\nThe basic prerequisite for healthy bone tissue is a sufficient supply of nutrients such as calcium and phosphate, because they are the main constituents of bones. They give the bone matrix its stability.\nHormonal disorders such as parathyroid hormone, oestrogen or testosterone are a frequent cause of a calcium deficiency as well as a vitamin D deficiency.\nSince our body cannot produce the mineral calcium itself, it is imperative to ingest it regularly and sufficiently via food intake.\nHowever, the German “National Consumption Study II” (Nationale Verzehrsstudie II) revealed that men with an average consumption of 807 mg of calcium as well as women with an average consumption of 738 mg of calcium are below the daily recommended intake. Only about two-thirds manage to cover their calcium requirement via food intake.\nWomen in the course of and after menopause should pay particular attention to an ample supply of calcium, because then certain hormones such as oestrogen decrease in concentration. But this controls the absorption of calcium in the bone structure.\nThe following causes can lead to a calcium deficiency:\nSince calcium is involved in many different bodily functions, symptoms of a calcium deficiency are also very varied. The intensity and duration of a deficiency also play a role.\nThe following symptoms can be indicative of a calcium deficiency:\nA person loses about 300 mg of calcium every day via sweat, urine and stool. How much calcium we should take daily depends on age, sex and hormones, among other things. Adolescents have a particularly high calcium requirement, since bone mass is still continually increasing.\nBut the requirement during pregnancy and breastfeeding is also increased, since the unborn child requires the mineral for development of bone structures. If not enough calcium is available in the blood vessels, the body falls back on the reserves in the bones of a pregnant woman.\nReference values for the daily requirement of calcium:\n|Infants||220 – 330 mg|\n|Children ages 1-3||600 mg|\n|Children ages 4-7||750 mg|\n|Children ages 8-9||900 mg|\n|Children ages 10-12||1,100 mg|\n|Adolescents ages 13-18||1,200 mg|\n|Pregnant women / breastfeeding mothers (< age 19)||1,200 mg|\nA vitamin D deficiency is one of the most common causes of a calcium deficiency. Calcium in the body is primarily absorbed in the intestines. In this connection, vitamin D (cholecalciferol) promotes the absorption from the small intestine and the integration of calcium in the bones.\nVitamin D thus plays a very crucial role in the resorption of calcium – an adequate calcium intake would not be possible without this. That is why vitamin D is also referred to as an “integration helper” for calcium in the bones and teeth.\nVitamin D is for all intents and purposes not a vitamin at all, but the precursor of a hormone that the body can produce itself. The basic prerequisite for this purpose is sunlight, because the exposure of ultraviolet light to our skin cells leads to the formation of vitamin D, and over further processes in the kidneys and liver it leads to the production of the active substance, called calcitriol.\nUnfortunately, the German “National Consumption Study” shows that 91 % of women and 82 % of men are not sufficiently supplied with vitamin D. Even Germany’s geographical location with too little solar radiation in the winter months complicates an adequate supply situation.\nIn particular, elderly people particularly often have low vitamin D levels, because self-production in the body decreases with increasing age.\nThat is why regular consumption of food rich in vitamin D is highly advisable, whereby it is found in relatively few foodstuffs. For example, cod liver oil and fat-rich fish as well as eggs and various types of mushrooms are sources of vitamin D.\nIn order to regulate the vitamin D balance in case of insufficient solar radiation, dietary supplements are also recommended.\nMilk and milk products are among the most well-known foodstuffs rich in calcium. Even 200 ml of cow’s milk provides about 240 mg of calcium and thus already covers nearly a fourth of the daily calcium requirement. Incidentally, the fat content of milk does not play a role, since the mineral content is not influenced by this.\nCheeses – particularly hard and semi-hard cheeses such a Parmesan, Emmentaler or Tilsiter – are particularly rich in calcium. For example, a 30 gram slice of Tilsiter provides about 250 mg of calcium.\nBut we can also cover our calcium requirement by means of plant-based foodstuffs. In particular, green leafy vegetables such as kale, rocket salad or fennel are recommendable, but herbs such as parsley as well as nuts and seeds also contribute towards targeted calcium supply.\nMany people are not aware that mineral water can also be a good source of calcium. Mineral waters particularly rich in calcium can contain over 500 mg of calcium/litre. Mineral water with a content of 150 mg of calcium/litre or more may be called rich in calcium.', 'I often have to remind my teen to drink milk and force yogurt or cheese for a snack. As teenagers, kids become more picky about the foods they eat and are not easily influenced by parental suggestions. For me, it’s not just the hassle of trying to come up with ‘new’ and ‘interesting’ foods every day, but I worry about making sure she meets her nutritional requirements.\nAdolescence is the transition period between childhood and adulthood, a time of life that begins at puberty. For girls, puberty typically occurs between ages 12 and 13, while for boys it occurs between ages 14 and 15. Adolescence is a phase of increased nutritional demands on a kid because puberty is one of the fastest growth periods of a person’s life. Add to that participation in sports, the demands at school, and extracurricular activities, teenage years can easily become a setup for nutritional deficiencies. Thinking carefully about nutrition is not just about the calories but also about ensuring adequate protein, calcium and iron intake.\n1. Why is Calcium so important for teens? Calcium is the main mineral that strengthens bones and this is very important during adolescence when teens are going through their growth spurt. People reach their maximum bone density during adolescence and gradually lose bone mass the rest of their lives. By age 17, our kids will have already established more than75 % of their adult bone mass.When adolescents get enough calcium during the teen years, they can start out their adult lives with the strong bones and significantly reduce their risk for fractures as an adult. Inadequate calcium intake during adolescence and young adulthood puts individuals at risk for developing osteoporosis later in life. However, according to recent statistics 9 out of 10 teenage girls and 7 out of 10 teenage boys do NOT get enough calcium in their diet.\n2. How much calcium does your teen really need? Nutrition guidelines recommend that children ages 9 through 18 get about 1300 mg or 4 servings of calcium every day, but how much calcium does that really mean? A serving size of calcium is about 300 milligrams. This is the same as an 8 ounce glass of milk or calcium fortified juice, 1 cup of yogurt or 2 slices of American cheese.\n3. What are the dietary sources of calcium? While whole milk and dairy products are an excellent source of calcium, a lot of “non-dairy” foods can give you your calcium supply, for example a cup of white beans, broccoli or tofu. Food labels can also tell you how much calcium is in one serving of food. Look at the % Daily Value next to the calcium number on the food label. Green vegetables such as spinach, collard greens, turnips, cabbage and kale are excellent sources of calcium.\n4. What about the teens who are ‘careful’ about their fat intake? Many adolescents may decide to avoid eating dairy foods they think will make them fat. It is important to let them know that fat-free and low-fat milk are excellent ways to get enough calcium without adding a lot of extra fat to their diet. An 8 ounce glass of skim milk has only 80 calories and zero fat and supplies 1/3 of a teenager’s recommended daily calcium intake. You can also offer low-fat and nonfat dairy products as healthy alternatives to whole milk products. Orange juice fortified with calcium is an excellent alternative, especially instead of sodas and sugary fruit drinks that have very little nutritional value.\n5. What if your child is lactose intolerant? In cases of children who are lactose intolerant and can’t drink milk, there are plenty of other ways to get your calcium. These include fortified soy milk, fortified juice, or Lactaid milk (milk with a lactase enzyme additive). You may also take lactase enzyme tablets before eating dairy products to help digest the lactose sugar in the milk. Some people who are lactose intolerant can tolerate having milk or other dairy products in smaller amounts.\n6. What’s another good reason to limit soda intake? Just as important as drinking milk, it is also important to look at the other beverages your teenager consumes. Soda intake is not only empty calories but also counterproductive to other nutritional demands in a teen’s life. Most teens drink more soda than milk which is also concerning because soda and other caffeinated beverages can interfere with the way the body absorbs and uses calcium. According to a Harvard study, 9th and 10th grade girls who drink soda have three times the risk of bone fractures compared with those who don’t drink carbonated beverages. Worse, the most physically active soda slurpers put themselves at five times the risk of abstainers.\n7. What about Calcium supplements? While it’s best for kids to get the calcium they need through a calcium-rich diet, sometimes this may not be possible. If you’re concerned that your teen isn’t getting enough calcium, discuss calcium supplements with your teen’s doctor. Calcium carbonate and calcium citrate supplements are good choices. Most calcium supplements have between 200 and 500 milligrams of calcium. Remember, your goal is 1,300 milligrams of per day. When taking more than one supplement per day, it is best to take them at different times of the day because your body can only absorb about 500 milligrams of calcium at a time.\n8. Is it only about the intake of calcium? Consuming enough calcium is not the only way to build strong bones. Be sure to encourage your child to be involved in regular physical activities and exercise, which is a very important factor in building healthy bones. Weight-bearing exercises such as jumping rope, jogging, or walking can also help develop strong bones by forcing your bones to work harder and build up bone mass. It is recommended to do 30 minutes of weight bearing exercise at least 3 times a week. Bones are very similar to muscles and you have to use them regularly to help them build up strength.\nYou have most likely seen the “milk mustache” on your favorite star, but are you sporting your own? Make a strong case for calcium and try to incorporate these changes for the whole family because chances are you could use some extra calcium too!']	['<urn:uuid:9512fd9e-a732-447a-9bf5-4ae2257bf5d3>', '<urn:uuid:a06497a6-0f25-4905-b7e2-e919bf4c9946>']	open-ended	with-premise	long-search-query	distant-from-document	comparison	novice	2025-05-12T21:31:42.699391	12	124	2150
90	differences between educated and uneducated parents choosing schools	More educated parents are more likely to leave traditional public schools. Active choosers in public and private choice systems are typically better educated than non-participants, and these well-informed parents exert a disproportionate effect on the production of services in the educational marketplace.	"['By Scott Abernathy\nAt school selection and the way forward for American Democracy, Scott Franklin Abernathy exhibits what\'s misplaced within the institution selection debate. Abernathy seems at mom and dad as electorate who exert strength over the academic procedure via every thing from their votes on institution budgets to their club on institution forums. difficult the belief that public faculties will enhance whilst faced with market-based reforms, Abernathy examines the prospect that public faculties becomes extra disconnected and remoted as civic existence is privatized.""Scott Abernathy takes up great questions and gives solutions grounded within the advanced truth of coverage and politics. institution selection and the way forward for American Democracy is a publication written in case you keep in mind that the area doesn\'t healthy the easy causes too frequently positioned forward.""--Clarence Stone, Professor Emeritus, collage of Maryland, and study Professor, George Washington University""Will university selection revive or eviscerate democratic methods and associations? Will it slim or exacerbate the diversity of academic inequities? This booklet takes numerous in a different way angled slices into those questions and attracts exciting answers.""--Jeffrey R. Henig, academics university, Columbia collage, and writer of Rethinking institution selection: Limits of the marketplace Metaphor""Through broad study and refreshingly neutral research, Scott Abernathy probes how using industry ideas to reform public colleges impacts democratic citizenship. Treating electorate firstly as clients, he unearths, threatens civic engagement and the overall healthiness of colleges, in particular within the nation\'s neediest groups. This considerate and balanced appraisal is must-reading for these involved in the way forward for American schooling and democracy.""--Suzanne Mettler, Alumni affiliate Professor, Syracuse collage, and writer of infantrymen to voters: The G.I. invoice and the Making of the best GenerationScott Franklin Abernathy is Assistant Professor of Political technology, college of Minnesota\nRead Online or Download School Choice and the Future of American Democracy PDF\nBest reform & policy books\nThe results of a examine undertaking, this paintings, an try and document on what has really been taking place in our colleges, solutions such questions as: what distinction have schooling reforms made to scholars\' adventure in colleges? and the way has contemporary schooling coverage impacted on teenagers at the present time?\nDuring the last 30 years there were a number of makes an attempt at deliberate academic switch. it truly is commonly accredited that the advantages haven\'t equalled the price, and all too frequently the placement has looked as if it would aggravate. during this publication, Michael Fullan distils from those reports the main robust classes approximately the way to deal with, and impact, academic switch.\nSpirituality, schooling and society: An built-in method argues the worth of spirituality in schooling so one can handle the lived studies and private wisdom of scholars, with the objective of making a extra holistic, transformative academic technique. This edited quantity has a wide range of viewpoints which all element to the significance of spirituality within the authors\' own lives, their groups and society at huge.\n- Private And Public School Partnerships: Sharing Lessons About Decentralization\n- American School Reform: What Works, What Fails, and Why\n- The Troika of Adult Learners, Lifelong Learning, and Mathematics: Learning from Research, Current Paradoxes, Tensions and Promotional Strategies\n- Black Men in the Academy: Narratives of Resiliency, Achievement, and Success\nExtra info for School Choice and the Future of American Democracy\nThere is increasing empirical evidence that active choosers in public and private choice sys tems are better educated and better off than people who do not participate in choice programs. 8 An important theme in studies of school choice and parents is that a 24 SCHOOL CHOICE AND THE FUTURE OF AMERICAN DEMOCRACY subset of parents is well informed and exerts a disproportionate effect on the production of services in that marketplace. 9 Although the authors of one study found evidence that the competitive pressures exerted by marginal customers benefited all, activism was not independent from identity and economics.\n41 Though parents who send their children to private schools tend to have higher incomes, parents who choose public school choice are actually likely to have lower incomes than a comparable group of parents who send their children to traditional public schools. This makes sense, as most pub lic choice programs are in inner cities, and many wealthier parents have paid a premium to purchase access to high-quality suburban public schools. More educated parents, however, are more likely to leave the tra ditional public schools, a finding consistent with both the literature on school choice and the findings regarding the Milwaukee parental choice program.\nGiven the results from the Milwaukee data, however, it is likely that it is some of both. The second step in the analysis is to use fitted values from the regression to predict a probability of leaving the public schools for each parent who remains in the assigned public schools. Each public school parent, in other words, is ranked according to the likelihood that they will enroll their child in either a public choice or a private school. Of course these parents have not exited; they all report sending their children to assigned public schools.\nSchool Choice and the Future of American Democracy by Scott Abernathy']"	['<urn:uuid:f2ba12e7-b98e-4367-aaf9-1c848956a093>']	factoid	direct	long-search-query	distant-from-document	single-doc	novice	2025-05-12T21:31:42.699391	8	42	862
91	How do modern vehicle warning systems differ from Arduino-based orientation devices?	Modern vehicle warning systems and Arduino-based orientation devices differ significantly in their complexity and purpose. Vehicle warning systems incorporate multiple components including sophisticated electronic sirens, air horns, LED light bars with 360-degree coverage, and complex dual-mode signaling systems (primary mode for emergency response and secondary mode for blocking right of way). They require extensive integration with vehicle systems and must meet industry standards. In contrast, Arduino-based orientation devices use basic components (compass module, piezo buzzer, Arduino board) in a simple circuit configuration, providing single-purpose directional feedback through basic audio signals. While warning systems focus on immediate attention-getting and hazard communication, orientation devices aim to gradually enhance spatial awareness through consistent directional feedback.	['Bicycle North Indicator using Arduino\nMagnetoception is the ability of some animals to detect magnetic fields as a means of orienting themselves. Although humans do not seem to posses the same biological mechanisms that allow other animals to sense magnetic fields, there are still many ways that we can improve our sense of spatial orientation.\nThis project is an attachment to your bicycle that beeps to let you know that you are moving in the direction of magnetic north. It uses a parallax digital compass module, an Arduino, and a piezo buzzer to give you directional feedback as you bike. The basic idea here is that by providing a constant source of stimuli in a certain direction, you might find yourself becoming more aware of other visual or sensory cues in your commute that indicate your orientation: geographical landmarks, air currents, inclines, or the position of the sun, moon, and stars.\n(1x) Parallax 3-Axis Compass Module Radioshack 276-123\n(1x) 85dB Piezo Buzzer Radioshack 273-060\n(1x) Arduino Uno REV 3 Radioshack 276-128\n(1x) 9V Alkaline Battery Radioshack #23-866\n(1x) Heavy-Duty 9V Snap Connectors Radioshack #270-324\n(1x) PC Board with Copper Radioshack #276-147\n(1x) SPST PC-Mountable Submini Toggle Switch Radioshack #275-645\n(2x) Male Header Pins Jameco 103393\n(1x) Female Pin Sockets Jameco 308567\n(1x) 9-Volt Battery Holder Radioshack 270-326\nStep 1: Schematic\nThe schematic above shows how simple this project is. A 9V battery and switch are connected in series to the Vin and ground pins of the Arduino- this is the power supply for the project. The Arduino supplies power to two pins of the compass module (see note in the image above) and analog pins A4 and A5 are used to receive data from the compass module. the piezo buzzer is controlled by Arduino digital pin 7.\nStep 2: Solder header pins to perf board\nSolder three rows of header pins to the copper side of the perf board so that the Arduino snaps on. The headers pins should fit inside the Arduino’s power pins, analog inputs, and digital i/o 0-7. I did not solder header pins for pins 8-13 because the spacing between digital pins 7 and 8 is not the standard 2.54mm spacing. It is a little challenging to solder the header pins to the copper side of the board (I got some extra solder on some of the copper pads), but it is necessary so that the compass module can snap on the perf board without touching the Arduino.\nStep 3: Parallax compass\nSolder a row of 5 female sockets on the protoboard as shown in the images above. The Parallax compass chip should fit nicely in this socket. Connect the pin labelled ground to Arduino’s ground, Vin to Arduino’s 5V, SCL to Analog 5, and SDA to Analog 4 as indicated in the schematic above.\nUpload the following code onto the Arduino. It will print the raw x, y, and z data from the compass chip (the directions are indicated on the chip in figure 4). If you have wired everything correctly you should see the raw data printed on your serial monitor (control/command+shift+m) in the Arduino IDE. I found this code on the parallax website.\nStep 4: Piezo buzzer\nThe piezo buzzer says that it should run off a 9-16V supply, but if you hook it up to 5V it will still run, just somewhat quieter. I didn’t necessarily want it to buzz at maximum loudness so it worked out well to hook it directly to one of the Arduino’s digital pins for this project. If you really want to run the buzzer at 9V, you can wire it up with a transistor to the battery supply and use a transistor to switch it on and off.\nSolder the red lead from the piezo buzzer to digital pin 7. Solder the black wire to Arduino ground.\nFor more detail: Bicycle North Indicator using Arduino\nJLCPCB – Prototype 10 PCBs for $2 (For Any Color)\nChina’s Largest PCB Prototype Enterprise, 600,000+ Customers & 10,000+ Online Orders Daily\nHow to Get PCB Cash Coupon from JLCPCB: https://bit.ly/2GMCH9w\nThis Post / Project can also be found using search terms:\n- bike indicator project arduino', 'As the bay doors open and we pull into traffic, I know this is going to be a challenging emergency run. It’s 5 p.m. on a hot summer night and there’s bumper-to-bumper traffic. The incident we were just dispatched to sounds very serious, so I know time is of the essence. Someone’s life depends on our unit getting to them quickly and safely in the chaos of Friday night rush hour.\nWhen we hit Prince and Main Street I can see we aren’t going anywhere quickly: Gridlock! I change the siren tone and lay on the air horn, but no one’s moving. Why? Are they distracted by texting, loud stereos, telephone calls or crying children? Maybe their air conditioning is running and their windows are sealed tightly.\n“What’s going on with these people, don’t they see or hear us?” I think to myself. “No one’s moving and I’m in a hurry to save a life.”\nThen slowly, like Moses parting the Red Sea, a path begins to open and we carefully maneuver through. Will we be there in time to make a difference?\nThese individual surface-mounted lights above the cab are mounted directly on the body. The clear lenses on each light allow them to perform multiple functions and flash different colors in different patterns.\nPhoto courtesy Wayne Zygowicz\nWe live in a mobile society that’s always in a hurry to get somewhere fast. Each year traffic increases and our roadways and interstates become clogged with drivers who are often impaired by fatigue, alcohol, drugs or distractions.\nDriving an emergency vehicle is a risky business that can have grave consequences around each turn. Each time we get behind the wheel of our ambulance and turn on our lights and sirens, there’s risk involved. Therefore, it’s imperative to crew safety that motorists can quickly hear, see and identify our ambulance, or other emergency vehicle, and react fast enough to get out of our way and avoid a collision.\nA number of variables affect emergency vehicle identification: the vehicle size, color scheme, conspicuous markings and retroreflective striping, as well as environmental factors. Properly positioned emergency lighting and the use of audible sirens and horns, however, may be the most important elements for avoiding a deadly collision.\nMount grill lights near the center of the grill and away from the front headlights. Photo A.J. Heightman\nYour ambulance chassis comes with the minimum lights required by law: headlamps, tail lights, signal lights, marker lights and brake lights. However, this original manufacturer’s equipment (OEM) provides only a limited outline of your vehicle. A wealth of industry standards has been developed to guide agencies on the selection and placement of additional emergency lighting. (See sidebar “Industry Standards,” below.)\nLighting placement is important. Emergency lighting should capture the attention of motorists, bicyclists and pedestrians, and alert them to the presence of your ambulance. This is especially important when entering intersections—where many accidents occur. Adding emergency lighting increases the conspicuity of your ambulance by defining the upper and lower clearance lines of your vehicle from all sides, making it easier to spot day or night.\nEmergency lights should be mounted high on the ambulance box and as close to the corner points on all sides. Lower-level lights should be added to the front and rear corners, forward and aft of the vehicle’s two axles. LED strip lights work great for this application, plus they use minimal power.\nLighting should be installed above the cab and in the grill/bumper area to outline the front of the ambulance. These lights make your ambulance visible to low-profile vehicles as you approach them from the rear.\nPatient loading lights at the rear of the ambulance and on the interior of the door can help increase the visibility around your vehicle and improve safety for personnel working at night. Photo courtesy Wayne Zygowicz\nNew roof-mounted LED light bars are a good option for over-the-cab lighting since they use little amperage compared to older technology. Look for a light bar that provides 360-degree lighting and can be easily reprogrammed with a laptop computer. Changing the lighting configuration and flash pattern is easy and can be accomplished with a few keystrokes. Current light bar technology is brighter, more aerodynamic and more user-friendly than what was on the market in the past.\nAnother effective and economical front lighting system design, popular on European ambulances, builds the lights into the ambulance box, which makes the light bar aerodynamic with the roof line.\nA third option uses individual surface-mounted lights installed above the cab and mounted directly to the body. Installing clear lenses on each light allows them to perform multiple functions and flash different colors in different patterns. A single surface-mounted light is also easy to repair when it burns out vs. dissecting a whole light bar.\nFront grill lights come in several configurations and sizes. They should be mounted near the center of the grill and away from the front headlights.\nSiren speakers and air horns should be mounted as low and as far forward on the chassis as possible to protect the hearing of the crew. Photo A.J. Heightman\nScene lighting, loading lights and ground lighting increase the visibility around your vehicle and improve safety for personnel working at night. Two bright scene lights, individually activated from the cab or from the patient compartment, should be mounted to each side of the body.\nPatient loading lights should brightly illuminate the working area in the rear of the ambulance and activate automatically when the back doors are opened. They should also be designed to be operated manually from the cab and/or patient compartment with the doors closed.\nGround lights provide an extra margin of safety for those entering and exiting the ambulance. Mounting ground lights under the driver and crew door that activate automatically when the doors are opened and closed is recommended.\nThe Science of Color\nThe most effective color for emergency vehicle lighting has been the subject of debate for years. What’s been adopted over time has been based partly on science and partly on tradition.\nThe literature tells us that the sensitivity of human vision peaks in the yellow-green portion of the color spectrum.1–3 The most visible color is white, followed by green, yellow and red. Although white emergency lights are easily seen, they fail to adequately identify that an emergency vehicle is approaching since all other vehicles on the road have white headlights.\nIf you drive an emergency vehicle, it’s crucial to understand the laws that regulate their use and the responsibility and liability that go with utlizing them. Photo courtesy Braun Industries\nGreen, also visually effective, isn’t recommended for emergency lighting since our culture associates green with “go” and would likely cause confusion for motorists and also for other providers when at an emergency scene.Yellow and red, on the other hand, are associated with “danger” and have traditionally been used for warning lights or caution signs.\nCurrent standards recommend using a combination of red, blue and yellow (amber) lights and limiting the use of any white lights to the front of the vehicle.4–7\nBuilders often recommend installing amber lights on all sides of the vehicle that activate when the ambulance is parked in or along the roadway. Some people theorize that red and blue lights actually distract other drivers, drawing their attention to the side of the road long enough to potentially cause a crash.\nAll current standards recommend that your lighting system operate in two separate signaling modes.4–6 Each mode activates different lighting combinations and colors.\nThe primary mode, “calling for the right of way,” signals drivers and pedestrians that your ambulance is responding to an emergency. This display activates when the lighting master switch is on, parking brake is off and the transmission is in drive. Primary mode incorporates mostly red and blue lights used for emergency response.\nThe secondary mode, “blocking the right of way,” activates amber lights and reduces red and blue lights. This mode activates automatically when the ambulance is stopped and the transmission is in park with the master lighting control on.\nCan You Hear Me Now?\nOur hearing is one of our most acute and primary senses. Just try shouting loudly or blowing a whistle in a crowded venue and you see how effective you are in getting people’s attention. Loud auditory sounds exert an immediate response and draw attention to the sound’s location.\nThe use of sirens and audible warning devices on emergency vehicles dates back hundreds of years. Barking Dalmatian dogs used to run ahead of horse-drawn fire brigades not just to rally and motivate the horses, but to warn pedestrians to get out of the way.\nToday, many state laws require that the ambulance siren is always activated when responding with emergency lights on.\nTo be effective, your siren has to compete and overpower surrounding urban environmental noises and penetrate the sound-proofing insulation found in modern automobiles. This can be difficult even with a very loud siren, and will reduce the effective distance at which your siren can be heard. This was very well demonstrated in an award-winning EMS10 Innovator of the Year public service announcement produced by the MONOC EMS System in New Jersey.8\nThe traditional motor-driven wind-up mechanical sirens like the Federal Q siren has been used on fire apparatus for decades. These sirens were very effective because of their decibel level and penetration power, but they’ve been replaced by electronic sirens with new technology and less expense. I recommend purchasing an electronic siren with ample power that produces a wide spectrum of frequencies and has multiple signaling modes.\nCurrent recommendations are that the siren speakers should be mounted as low and as far forward on the chassis as possible to reduce the potential for hearing damage to the vehicle occupants. Recessed bumper-mounted speakers are perfect for this application.\nA new siren concept utilizes sound waves that not only can be heard but physically felt by motorists. Recommended for urban environments with heavy vehicle and pedestrian traffic, these sirens emit low frequency sounds that can penetrate and shake solid materials with their sound waves. Not only do drivers hear and feel the sounds, they may experience their rear-view mirror shaking from the penetrating sound waves.\nThe manufacturers suggests that these sound waves can be heard and felt from at least 200 feet away with these highly effective sirens. Two examples of this technology are the Whelen Howler Low Frequency Emergency Siren and Federal Signal Rumbler Siren. Current standards specify a traffic horn should be used in conjunction with the electronic siren. Air-powered horns, typically used on larger trucks, are now common on most ambulances. Although little research has been done on air horns, ambulance builders suggest a dual-trumpet air horn that produces two different tones. Air horns should be mounted low on the chassis or in the bumper, not on the cab roof, to reduce noise exposure to the crew.\nHearing loss due to loud sirens and air horns is well documented in first responders.9–11 A concerted effort should be made to prevent noise levels in the cab from exceeding safe occupational standards. Simple measures like adding extra sound-dampening insulation to the cab, closing the windows while responding and mounting audible devices low will help reduce unwanted noise exposure.\nWireless noise-canceling headsets, integrated with the ambulance’s primary radio, also help eliminate siren noise while enabling effective communication with dispatchers and crew members throughout the vehicle. In addition, the use of available wireless devices allows you to exit the cab or patient compartment and stay in communication with the driver while you’re backing the ambulance.\nRisk vs. Benefit\nIf you drive an emergency vehicle, it’s important to know how your emergency signaling devices are designed, installed and activated. It’s also crucial to understand the laws that regulate their use and the responsibility and liability that go with utilizing them.\nRemember one important point: emergency lights and sirens are extremely limited warning devices. Even when designed and installed correctly, active warning devices can’t overcome preoccupied drivers. Emergency vehicle operators must drive with caution and adopt a defensive posture while driving with lights and sirens. There are countless lawsuits that have occurred following emergency vehicle crashes that resulted in injury or death.\nAnd, let’s not forget the patient. Most EMS veterans will tell you that driving emergent usually doesn’t improve patient outcomes or significantly reduce scene or hospital arrival times. Rarely does a minute or two make the difference between life and death on most of the calls we run. Some authors actually believe lights and sirens may cause more injuries than they prevent.12\nYour agency should develop standard operating procedures that address emergency driving and each member should be trained when to activate warning systems and risk associated with them. The life you save may be your own.\n1. De Lorenzo RA, Eilers MA. Lights and sirens: A review of emergency vehicle warning systems. Ann Emerg Med. 1991;20(12):1331–1335.\n2. Emergency Vehicle Safety Initiative. (February 2014.) United States Fire Administration. Retrieved June 1, 2016, from www.usfa.fema.gov/downloads/pdf/publications/fa_336.pdf.\n3. Emergency Vehicle Visibility and Conspicuity Study. (August 2009.) United States Fire Administration. Retrieved June 3, 2016, from www.usfa.fema.gov/downloads/pdf/publications/fa_323.pdf.\n4. Commission on Accreditation of Ambulance Services. (March 28, 2016.) Ground vehicle standards for ambulances. Ground Vehicle Standard. Retrieved June 1, 2016, from www.groundvehiclestandard.org/wp-content/uploads/2016/03/CAAS_GVS_v_1_0_FinalwDates.pdf\n5. NFPA 1917: Standard for Automotive Ambulances. (2016.) National Fire Protection Agency. Retrieved June 1, 2016, from www.nfpa.org/codes-and-standards/all-codes-and- standards/ list-of-codes- and- standards?mode=code&code=1917.\n6. NFPA 1901: Standard for Automotive Fire Apparatus Ambulances. (2016.) National Fire Protection Agency. Retrieved June 1, 2016, from www.nfpa.org/codes-and-standards/all-codes- and-standards/list-of-codes-and-standards?mode=code&code=1901.\n7. Federal Specifications for the Star-of-Life Ambulance. (August 1, 2007.) United States General Services Administration. Retrieved June 1, 2016, from www.ok.gov/health2/documents/ KKK-A-1822F%20%2007.01.2007.pdf.\n8. Siren public service announcement. (n.d.) MONOC. Retrieved Aug. 17, 2016, from www.monoc.org/newPub/sirenPSA2.cfm.\n9. Hong O1, Samo DG. Hazardous decibels: Hearing health of firefighters. AAOHN J. 2007;55(8):313–319.\n10. Fernandez AR, Crawford JM, Studnek JR, et al. Hearing problems among a cohort of nationally certified EMS professionals. Am J Ind Med. 2010;53(3):264–275.\n11. Clark WW, Bohl CD. Hearing levels of firefighters: Risk of occupational noise-induced hearing loss assessed by cross-sectional and longitudinal data. Ear Hear. 2005;26(3):327–340.\n12. Wolfberg D. Lights, sirens and liability. JEMS. 1996;21(2):38–40.\nEmergency warning devices should be incorporated into all ambulance specifications. In addition, buyers should review current industry standards and examine international best practices before developing their design specifications. The internet makes this very easy to do. After doing your homework, consult with your ambulance builder, who’s the expert in industry standards, lighting design and audible warning devices. Also network with other industry leaders to see what they’re building and what works and what doesn’t. This is helpful to avoid making mistakes others have already made.\nIndustry standards will help guide your lighting and siren purchasing choices. The federal Star of Life specifications, which for years were used to set minimum ambulance standards, are being replaced by new and improved guidelines.\nCurrent industry standards include:\n>> National Fire Protection Agency (NFPA) 1901 Standard for Automotive Fire Apparatus;\n>> NFPA 1917 Standard for Automotive Ambulances; and\n>> the Commission on Accreditation of Ambulance Services Ground Vehicle Standards (GVS) for Ambulances.\nBuyers should also consult their own state and local requirements governing emergency lighting. Most states have statutes that outline what type of vehicles can use emergency lighting, color schemes and rules for their use.']	['<urn:uuid:57871328-0c60-4cbd-bdb0-d305399e905b>', '<urn:uuid:cf9ac852-0429-4767-8602-512d8254d140>']	open-ended	direct	concise-and-natural	similar-to-document	comparison	expert	2025-05-12T21:31:42.699391	11	112	3254
92	What can be done to prevent another major financial crisis like the one that happened with Lehman Brothers?	According to research, it is possible to quantify the risk that each financial transaction adds to the system, and implementing a small transaction tax based on this risk could help discourage risky transactions and stabilize the entire financial system.	['08 June 2016\nIn September 2008, the collapse of Lehmann Brothers sent ripples through financial markets, destabilizing banks around the world. As bank after bank fell into crisis, governments rushed to stop the catastrophic spread of failures. In subsequent years, the cascading nature of the global financial crisis drew attention not just from financial regulators and politicians, but also from scientists who study network theory. By looking at the financial system through the lens of network science, researchers found ways to identify those players in the financial system that make the system most vulnerable to shocks.\n“Basically, every transaction between financial actors adds to systemic risk of the total system—the risk that the whole financial system collapses,” says IIASA researcher Stefan Thurner. In a recent study, Thurner and colleague Sebastian Poledna showed that it was possible to quantify the amount of risk that each transaction adds to the system, and furthermore, that a small transaction tax based on this risk could de‑incentivize risky transactions and help stabilize the entire financial system.\nOne factor that allowed this major insight is the huge volume of data that is now available. “We’ve been thinking of these methods for maybe a decade now. Only recently fantastic data sets have become available, where you often have almost complete information about a whole system,” Thurner says. “With these data we can now show that our thoughts on systemic events, evolving systems, and evolutionary systems really do work. I think we have reached a new level of understanding and ability to model the dynamics of networks, of restructuring networks, and more recently, of dynamically restructuring multilayer networks.”\nThurner and colleagues have now also applied similar methods to other areas. For example, in one recent study, the researchers showed that shortages of minerals such as copper, aluminum, and mercury could lead to cascading shocks that could cause instabilities in the global trade system. In another project, the researchers are exploring how medical data could be used to improve the efficiency and efficacy of the healthcare system. Using these data across an entire country, the researchers have found how networks of symptoms and diagnoses can be used to estimate the chance that people will develop certain diseases in the future.\nThurner explains, “We start with trying to understand a problem, and then formulate it in mathematical ways that can be solved. Once we have developed such a new method, often involving dynamical networks, we use data to test our theories. If it works, we might provide novel solutions.”\nPainting by numbers\nNetworks are ubiquitous in nature and society. Yet looking at a network from a different point of view can bring different insights. Matthias Wildemeersch, one of Thurner’s colleagues in the institute’s Advanced Systems Analysis Program (ASA) also studies networks, but from a different perspective, with different methods.\nIn one recent study, Wildemeersch built a model to explore opinion dynamics, or how ideas and opinions can spread through society—networks of people. The model examined how the way society thinks can be changed, for instance by artificially controlling or repeating messages. People perceive often‑repeated information as more important. The study determined how frequently a message needs to be repeated to lead to a change in majority opinion. It also showed that nodes in a network that stubbornly hold onto their opinion—which could represent media organizations or opinion makers—could actually provoke a polarization of opinions. While the study was theoretical in nature, such ideas could lend new insight to the study of social and political movements.\nThe big picture\nIn any given network, there are different rules, dynamics, and events that control how it works. Such controls can also be modeled using mathematical tools other than those specifically aimed at network analysis. ASA can be viewed as a network in its own right, bringing together mathematicians, economists, and physicists, to exchange and combine ideas and come up with new ways to solve problems. “Many of the problems we are interested in at IIASA require us to combine several formal methods. So you might start from the concept of a network and then apply other mathematical methods such as control, game theory, or optimization,” says Wildemeersch.\nASA Program Director Elena Rovenskaya explains that such disciplinary diversity creates the kind of environment that helps speed the ripening of new methods. “Real‑life systems are becoming more and more complex and dynamic. Today, in the 21st century, we are approaching or sometimes even hitting planetary boundaries. It turns out that the behavior of human‑made systems, as well as natural systems, actually becomes different and more complex, compared to what we are used to. Those models that worked well when systems were far away from planetary boundaries may not work anymore. We need new systems analysis approaches to help us analyze complex problems, accounting for nonlinearities and possible regime shifts. The methodological diversity of the ASA group and IIASA in general puts us at forefront of this endeavor.”\nText by Katherine Leitzell\nLast edited: 28 June 2016\nOPTIONS SUMMER 2016\nCherp A, Vinichenko V, Jewell J, Brutschin E, & Sovacool B (2018). Integrating techno-economic, socio-technical and political perspectives on national energy transitions: A meta-theoretical framework. Energy Research & Social Science 37: 175-190. DOI:10.1016/j.erss.2017.09.015.\nVanham D, Hoekstra AY, Wada Y, Bouraoui F, de Roo A, Mekonnen MM, van de Bund WJ, Batelaan O, et al. (2018). Physical water scarcity metrics for monitoring progress towards SDG target 6.4: An evaluation of indicator 6.4.2 “Level of water stress”. Science of the Total Environment 613: 218-232. DOI:10.1016/j.scitotenv.2017.09.056.\nZhang R, Fujimori S, Dai H, & Hanaoka T (2018). Contribution of the transport sector to climate change mitigation: Insights from a global passenger transport model coupled with a computable general equilibrium model. Applied Energy 211: 76-88. DOI:10.1016/j.apenergy.2017.10.103.\nInternational Institute for Applied Systems Analysis (IIASA)\nSchlossplatz 1, A-2361 Laxenburg, Austria\nPhone: (+43 2236) 807 0 Fax:(+43 2236) 71 313']	['<urn:uuid:8ac7faef-ada8-40e9-a113-ec18246cb0b6>']	factoid	direct	verbose-and-natural	distant-from-document	single-doc	novice	2025-05-12T21:31:42.699391	18	39	976
93	What were Michelangelo's final wishes regarding his burial place?	While church authorities wanted to bury Michelangelo in St. Peter's Basilica in Rome, his family secretly transported his body to Florence to fulfill his last wish. He had requested: 'take me back to Florence, when I am dead, because I cannot go back there while I am alive.' He was ultimately buried in the Basilica of Santa Croce in Florence, where his tomb remains today.	['On March 6, 1475, in a tiny Tuscan village of Caprese Michelangelo, the most famous artist of all times Michelangelo Buonarroti was born. Michelangelo was a universal renaissance artist – painter, sculptor, architect, poet, engineer, recognised as an artist while he was still alive and called “il divino” (English: the divine). True, he was also known for his foul temper – he was constantly quarrelling with other artists and clients, even the Popes of Rome. On the occasion of genius’ birthday, UnseenTuscany.com gives you a brief synopsis of his life and works that you can see during a visit to Florence.\nThe Buonarroti family, who were occupied in banking and marketing in Florence, were known since the 13th century. In the 15th century, the family was going through a difficult phase, Michelangelo’s father was appointed as the elder of a tiny village Caprese. After the birth of the artist, the family moved to the suburbs of Florence. Michelangelo’s father sent his son to Florence to study grammar with a teacher Francesco da Urbino. However, Michelangelo was more interested in art; he was painting and tried to get acquainted with the Florentine artists. In order to deter Michelangelo from arts, his father even beat him. Nevertheless, the stubborn Michelangelo had his will, got acquainted with Domenico Girlandaio and became his apprentice. Later Lorenzo il Magnifico personally took care of a young genius.\nAfter several long trips to Rome, in the 1501s Michelangelo returned to Florence, where he was already known as the author of ‘Pieta’. In Florence he got involved in the process of creating ‘David’ and at the same time was busy with the sculptures of Siena cathedral. The ‘David’ was completed in about two years and now this unique sculpture can be seen in the Galleria dell’Accademia in Florence.\nIn the 1518s Michelangelo returned to Florence and began the decoration works of the San Lorenzo church facade. He went to Carrara marble quarry to look for proper stones, but the project was ended until it was begun – Pope Leo X withdrew it due to the lack of funds. The Pope offered to take another project – to create a mortuary chapel for members of the Medici family and the Laurentian Library (Italian: Biblioteca Medicea Laurenziana) for the collection of the manuscripts that belonged to the Medici, located in the Basilica of St. Lawrence (which lacks a façade to this day).\nIn 1529 Michelangelo was appointed as one of nine republican officials, responsible for the design and implementation of defensive bastions and other fortifications. The main task for Michelangelo was the fortifications of the most important Florentine fortress on San Miniato hill.\nMichelangelo was invited to paint a huge fresco in the restored Palazzo Vecchio Palace, but this project was never realized. True, you can now see his sculpture ‘Genio della Vittoria’ in the main hall of the palace.\nMichelangelo died on February 18, 1564, just before his 89th birthday. During his long life Michelangelo worked for 7 popes, survived the great geographical discoveries, the Reformation, the seizure of Rome in 1527, the collapse of the ideals of the High Renaissance and the establishment of Mannerism in art, the announcement of Copernicus’ Heliocentric theory. He was never married and did not have any children. Church authorities hoped to bury him in St. Peter’s Basilica in Rome, but Michelangelo’s family almost secretly transported his body to Florence in order to satisfy his lasts will: ‘take me back to Florence, when I am dead, because I cannot go back there while I am alive’. Michelangelo was buried in the Basilica of Santa Croce in Florence, where his tomb remains till today.']	['<urn:uuid:aa09fff6-b4fc-4e6a-af31-c41b3411b404>']	open-ended	with-premise	concise-and-natural	similar-to-document	single-doc	expert	2025-05-12T21:31:42.699391	9	65	610
94	what techniques for stretching warm up cool down exercises when to use each one best results	For warm-ups, which should be performed 15 minutes before exercise, dynamic stretching is the best technique. Dynamic stretching involves gradually increased movements that improve muscle temperature, blood flow, and muscle contraction quality. This leads to better movement control and reduced injury risk. For cool-downs after exercise, static stretching is the recommended technique. Static stretching, which involves holding a muscle in a lengthened position, is effective for reducing muscle stiffness and improving range of motion in fatigued muscles. Research shows that using the wrong technique at the wrong time can be detrimental - for instance, static stretching before exercise can reduce power output and sprint performance, while mobility training and dynamic stretches are proven to enhance performance when used in warm-ups.	['Muscle injury is one the most presented injuries which is sustained by both professional level and recreational level athletes who perform regular sports and exercise activities. It is also the most frequently presented issue that requires treatment here at Bodyworks. It is widely regarded by both therapists and sports coaches that performing a warm up and cool down which includes a stretching protocol is an effective way to aid in reducing the risk of muscle injury. However it is often misunderstood which stretching techniques should be used in both the warm up and cool down to get the most effective results to prevent muscle injury. Hopefully the information presented in this piece can clear up some grey areas and help educate our patients on the best stretching methods to use before and after exercise.\nA warm up prior to exercise is a method used to prepare the body for the upcoming activity and has many injury reducing benefits. Warm-ups are shown to have the best results for injury reduction when performed 15 minutes prior to the main exercise activity. For a warm up, Dynamic Stretching interventions should be included. Dynamic stretching is referred to as a series of movements where the movement is gradually increased to help increase the range of movement within muscle groups and within a joint,resulting in the muscles being dynamically stretched. By increasing the movements gradually many physiological effects within muscle groups occur, which include improving muscle temperature, improving blood and oxygen flow to working muscles, and improvement of muscle contraction quality. These physiological factors influence muscles to have more efficient control of movement action which in turn can reduce risk of injury as the muscle groups are working in a better condition than they would be prior to performing a warm up.\nPerforming a cool down is another widely regarded method to reduce the risk of muscle injury. A cool down includes a series of actions and stretches to bring down the heart rate after activity and to restore and maintain length in muscle groups used within the main activity. Static Stretching is an efficient method to use in a cool down to help to stretch off muscle groups that become fatigued and shortened as a result of exercise. Static stretching is a useful method to restore and maintain flexibility as it is an effective method to reduce musculo-tendon stiffness which aids in improving range of motion within muscle groups.\nIt is important as an athlete to understand where to include static stretching in your exercise routine. Although static stretching is a universally regarded method to reduce muscle stiffness and improve range of motion it can have negative effects if used as part of a warm up. In a study which focused on netball, a comparison was done using a warm up including static stretching and a warm up including dynamic stretching, the static stretching warm up reduced power output in the athletes when performing 20 meter sprints and vertical jump. Although both static stretching and dynamic stretching both help to improve range of motion, it is the physiological effects of a dynamic warm up that makes it a better stretching strategy to include in the warm up as it will promote better overall muscular control when performing the main exercise activity, in comparison to static stretching having better results in stretching shortened muscles and reducing muscle stiffness which is the result physical activity on the working muscle groups. The use of these methods appropriately will contribute to keeping the muscles in good health and aid in reducing the risk of muscle injury.\nMcMillian, D., Moore, J., Hatler, B. and Taylor, D. (2006). Dynamic vs. Static-Stretching Warm Up: The Effect on Power and Agility Performance. The Journal of Strength and Conditioning Research, 20(3), p.492.\nWoods, K., Bishop, P. and Jones, E. (2007). Warm-Up and Stretching in the Prevention of Muscular Injury. Sports Medicine, 37(12), pp.1089-1099.\nYoung, W. and Behm, D. (2002). Should Static Stretching Be Used During a Warm-Up for Strength and Power Activities?. Strength and Conditioning Journal, 24(6), pp.33-37.', 'Improve Performance With Mobility Training\nThere are several factors in our modern lifestyles that can affect our physical well-being and leave us feeling aged beyond our years. Sitting for extended hours throughout the day at a desk, overall lack of movement, or even repetitive movements that comprise our range of motion can leave our bodies feeling stiff and tight. For participants of physical activity, this can also take a toll on performance. While past research has lead us to stretching as a solution, recent studies have found the benefits of mobility training to surpass those of a static stretch, particularly for those engaging in physical activity.\nStatic stretching primarily focuses on lengthening a muscle until either a stretch sensation (Cronin et al. 2008) or the point of discomfort is reached (Behm et al. 2004) and then holding the muscle in a lengthened position for a prescribed period of time (Ebben et al. 2004). For example, think 30 seconds of holding a standing quadricep stretch or sitting and holding a forward bend to engage the hamstrings. Although stretching is known to improve flexibility, recent studies have found that it does not necessarily improve performance when it comes to those engaging in physical activity.\nA 2014 study found that endurance runners were on average thirteen seconds slower when they performed static stretching immediately prior to a one mile run at a five percent uphill grade. The researchers measured more than just the time it took to complete the mile. If there was a difference in performance, they wanted to know why. They examined the ground contact time and also muscle activation via electromyography (EMG). The researchers found that muscle activity and ground contact both increased after stretching, indicating it took more effort to complete the run which resulted in the slower times. Researchers concluded that endurance runners may be at risk for decreased performance after static stretching and static stretching should be avoided before a short endurance bout (Lowery et al. 2014).\nRelated Article: Sitting Disease – The New Smoking\nMobility training, according to physical therapist and mobility expert Kelly Starrett, is a “a movement-based integrated full-body approach that addresses all the elements that limit movement and performance including short and tight muscles, soft tissue restriction, joint capsule restriction, motor control problems, joint range of motion dysfunction, and neural dynamic issues (Mobility vs. Flexibility, 2015).” A person with great mobility is ultimately able to perform exercises or day-to-day movements with no restrictions in the range of motion. A flexible person however, may or may not have the strength, balance, or coordination to perform the same movements.\nA study done by researchers from the University of Luton in the UK examined 97 union rugby players to determine the effects of mobile, dynamic stretches vs. static stretches prior to a 20-m sprint. The main finding from this study was a significantly faster sprint time when mobile, dynamic stretching was incorporated into a warm-up, with significantly slower sprint times observed for subjects employing either static active or passive stretching regimes. Researchers concluded that for the majority of sports performers needing to optimize sprint performance over a relatively short distance, a mobile, dynamic stretch was advisable (Fletcher et al. 2004).\nConclusively, mobility training is a beneficial tool to help improve our overall well-being and performance regardless of age or varying fitness regimens. Try the following routine as your next warm up for exercise, prior to bed, or even during the lunch hour to break the monotony of being in the same posture for an extended period of time.\nLeg Swings (Groin/Hip): Stand 2-3 feet from a wall with palms flat against wall at shoulder height. While keeping the right foot pointing straight ahead, swing the left leg in a pendulum motion from side to side. Gradually increase the range of comfortable motion. Perform this drill for ten repetitions, three times on each leg.\nScapular Wall Slides (Back/Shoulders): Begin by standing with your back against a wall with correct posture. Raise arms out to your sides so that your forearms rest vertically against the wall. Maintain this contact throughout the exercise. Slide your arms up until your arms are straight and then back down all the time focusing on pulling your shoulder blades together and down. At the bottom of the movement bring your elbows into your body and squeeze your shoulder blades together. Perform this drill for eight to ten repetitions.\nRocking Ankle Mobilization: Begin on all fours, and pike your hips so your torso is relatively straight. Place your feet relatively close to your body to start. Position one foot flat on the ground, with the second foot gently resting on top of the planted foot’s ankle. Gently rock forward and backward so your heel touches the floor and then raises off of it. Slowly walk your feet backwards if you feel comfortable with the stretch and your mobility improves. Perform this drill for eight repetitions on each side.\nRelated Article: Staying In Shape For The Long Haul\n(Source: Mobility drills adapted from Eric Cressey, Certified Strength and Conditioning Specialist (CSCS) through the National Strength and Conditioning Association)\nCronin J, Nash M, Whatman C. 2008. The acute effects of hamstring stretching and vibration on dynamic knee joint range of motion and jump performance. Phys. Ther. Sport 9: 89-96 CrossRef, Medline.\nBehm DG, Bambury A, Cahill F, Power K. 2004. Effect of acute static stretching on force, balance, reaction time, and movement time. Med. Sci. Sports. 36: 1397-1402 Medline.\nEbben WP, Carroll RM, Simenz CJ. 2004. Strength and conditioning practices of National Hockey League strength and conditioning coaches. J. Strength Cond. Res. 18: 889-897 CrossRef, Medline.\nRyan Lowery, et. al. 2014. Effects of static stretching on 1-mile uphill run performance. Journal of Strength and Conditioning Research, 28(1).\n“Mobility vs. Flexibility.” Invictus | Redefining Fitness. CrossFit Journal, 2015. Web. 15 Jan. 2017.\nFletcher I, Jones B. 2004. The effect of different warm-up stretch protocols on 20 meter sprint performance in trained rugby union players. J Strength Cond Res 18: 885–888.\nYou Might Like:']	['<urn:uuid:7b750381-3fe9-42e6-85ec-5457f86a8aab>', '<urn:uuid:db2c9d6f-ba30-4fa8-a193-603ba80e7eed>']	open-ended	with-premise	long-search-query	similar-to-document	multi-aspect	novice	2025-05-12T21:31:42.699391	16	120	1681
95	What's the relationship between shock duration and injury severity?	The relationship between time and injury severity is directly proportional - if the duration of an arcing fault doubles, the available energy doubles, while halving the duration cuts the energy in half. This is why using faster-acting protective devices is crucial for safety. Additionally, the extent of internal organ damage depends on both the duration of shock and its magnitude.	"[""All right, today we're gonna be talking about what does current do to the human body when you get shocked? I got a volunteer named Charlie who has agreed to give us life force to show us what happens inside your body. And let me introduce you to Charlie, the hot dog. Does anybody know what a hot dog's made out of? You probably don't wanna know or you'd quit eating these things, but basically it's a combination of meat and moisture and it very closely represents the human body. So what we're gonna do is we're gonna put Charlie on this special hot dog cooker that we prepared for it and we'll just slide him on there. And what I got here is there's gonna be 120 volts applied to the hot dog. Current's gonna flow through him and what we're gonna observe is he's gonna cook from the inside out. The ground rod is disconnected. I'm connected into a GFCI. And as soon as I plug this thing in I should have 120 volts across the hot dog. So I'm gonna put my safety glasses on and I'm gonna put my rubber gloves on because it's gonna be energized.\nNow those of you sitting in the front row you may wanna move back a little bit. One of the things I've learned over the years of doing this test is hot dogs are like people, no two hot dogs are created equal. And I never know what these things are gonna do. So if it blows up, it's gonna spread hot dog guts out across those of you in the front row. So you may wanna move back a little. All right, so we're gonna plug Charlie in and what do we see? Absolutely nothing. So let's take our voltmeter and let's just see how much voltage that we're putting on Charlie. So I'll put it across the 2 terminals and I measure 120 volts. Right on 120. Do you see anything happening with the hot dog? Nothing. Nothing is going on. Now this because the ground is disconnected, represents you with a drill that somebody cut the ground pin off on, and it shorts out and now you're laying on the ground held onto this drill and you can't let go and you're getting cooked like Charlie.\nNow we're starting to see a little bit of smoke coming out at the end of him and that's very common with what you'll see on an entry. You'll have an entry wound and an exit wound. All right? But externally what do we see on Charlie other than smoke coming out the end of him? Nothing. So let's look and see what happens that soon as I establish a ground I should hear my GFCI trip, right? Charlie sitting here just cooking away but externally do you see anything? No. Watch, did you hear the GFCI trip? As soon as I got a ground circuit my GFCI tripped. All right. So let's take Charlie, he's now dead, he's laying on the floor dead. What do you do on an accident when somebody dies? EMT show up, they pick him up, and they do what? They haul him off to the hospital. Do we see anything on Charlie? Nothing, other than an entry and an exit wound, which is very common. What happens to a dead person when he shows up at the hospital? They do an autopsy. So let's take Charlie and we're gonna do an autopsy on him and see if he cooperated with us to show us what happens to the human body when you get electrocuted.\nThere's what happens. This guys is your guts, your liver, your kidneys, your vital organs. Externally nothing. He's got a little bit of a temperature but no external damage. Everything has been done internally and his guts have been cooked out. Now here's the scary part, this happens every time you get shocked. The order and magnitude of it depends on the duration of the shock and any value of it. Twenty, 30, 40 years later your kidneys fail and you don't know why, and it coulda been as a result of getting a big shock back in your 20s. So the bottom line, guys, the message here is be safe, wear your rubber gloves, and don't get shocked."", 'The Importance of Arc Flash Mitigation\nDe-energizing equipment does not absolve the facility from the responsibility of performing an arc ﬂash analysis or providing the necessary PPE.\n- By Antony C. Parsons\n- Jun 01, 2017\nWhile the threat of shock and electrocution from inadvertent contact with energized parts has long been recognized, the arc ﬂash and arc blast hazards have only fairly recently been incorporated into the electrical safety standards. The Occupational Safety and Health Administration (OSHA) enforces electrical workplace safety standards outlined in the National Fire Protection Association\'s NFPA 70E: Standard for Electrical Safety in the Workplace®.\nBasic compliance with NFPA 70E-2015 Edition guidelines can be established with a ﬁve-step process:\nStep 1: Develop and audit an Electrical Safe Work Practices (ESWP) policy.\nStep 2: Conduct an electrical system study to determine the present degree of arc ﬂash hazards and label the equipment.\nStep 3: Ensure adequate supplies of personal protective equipment (PPE).\nStep 4: Conduct regular safety training and employee assessments.\nStep 5: Maintain all electrical distribution system components.\nCompanies can take additional steps to reduce the potential for an arc ﬂash. The remainder of this article will focus on mitigating arc ﬂash hazards, speciﬁcally engineering controls.\nWhat is Arc Flash Mitigation?\nAccording to Webster\'s dictionary, mitigation is deﬁned as ""to make milder, less severe or less violent."" When applied to electrical workplace safety, arc ﬂash mitigation involves taking steps to minimize the level of hazard and/or the risk associated with an arc ﬂash event.\nANSI Z10-2012, Occupational Health and Safety Management Systems, included a hierarchy of arc ﬂash mitigation controls, as shown below:\nPPE is often mistakenly viewed as the ""solution"" to arc ﬂash hazards. The reality is that even when PPE is properly selected, it does not guarantee freedom from injury. NFPA 70E only makes the claim that injuries sustained during an arc ﬂash event would be ""reduced"" and ""survivable"" due to mitigating effects of arc-rated PPE.\nAn effective safety program does not settle for ""survivable"" injuries; instead, the most effective arc ﬂash safety programs look to incorporate ""safety by design,"" primarily through application of engineering controls. Though not as effective as substitution or elimination, the goal of engineering controls is to reduce the degree of hazard. Administrative controls and warnings are less effective because they rely on workers following proper procedures and safe work practices.\nThe engineering controls covered in this article will either:\n1) Reduce arc ﬂash energy to a level where permitted tasks can be performed, or\n2) Locate the worker so that he/she is not subject to harm.\nArc ﬂash reduction systems do not eliminate the electric shock hazard of working on or inside energized equipment. The amount of arc ﬂash energy reduction will be determined by an engineering analysis. The goal of reducing arc ﬂash energy levels is to reduce the severity of the potential arc ﬂash hazards to which a worker may be exposed. PPE is still required when an arc ﬂash energy reduction system is employed, but the level of PPE may be reduced.\nWhile arc flash mitigation has always been a good idea, there have not been specific requirements for it in industry codes and standards until recently. However, NEC 240.87 was significantly revised in 2014 and now requires Arc Energy Reduction means to be applied on LV circuit breakers 1200A or larger. In the next section of this article, we will explore options for mitigating arc flash hazards that may be used to meet this requirement or for other mitigation purposes.\nReducing Arc Flash Energy Levels\nHere we\'ll discuss various ways to reduce arc flash energy levels, including circuit breakers, over-current protective device coordination studies, specialized relaying, and other arc flash mitigation systems.\nThe Role of the Circuit Breaker or Fuse in Lowering Arc Flash Energy Levels\nWhy is a circuit breaker or fuse always considered in the arc ﬂash analysis? Because arcing time is the key determining factor for arc ﬂash energy. Per the equations in IEEE Std. 1584-2002, arc ﬂash incident energy varies linearly with time. If the duration of the arcing fault doubles, the available energy doubles; halve the duration and you cut the energy in half.\nBecause incident energy is proportional to arcing time, the use of faster-acting devices is key. As a result, proper selection of overcurrent protective devices—in particular, selecting devices that will quickly clear arcing faults from the power system—is a powerful mitigation strategy.\nOver-Current Protective Device (OCPD) Coordination Study\nAn OCPD coordination study optimizes the protective device setting for reliability and arc ﬂash protection. While an OCPD study is not a requirement of an arc ﬂash analysis, it is recommended to have this study completed as a component of an arc ﬂash analysis. The OCPD coordination study will determine whether minor adjustments in circuit breaker (or other over-current protective device) settings can lower incident energy levels. However, settings must be chosen to properly protect equipment while still allowing for normal load currents and routine temporary overcurrents (e.g., motor starting current) to ﬂow without causing a trip.\nSpecialized Relaying, Such as Optical Technology\nQuickly clearing faults is a key to arc ﬂash mitigation. Circuit breaker or relay settings near the source of power may have signiﬁcant time delays to allow for coordination of downstream devices. A relatively new way to address this issue is to use relays that detect the presence of arcing faults by looking for the ﬂash of light associated with the arcing fault in addition to the characteristic current ﬂow.\nHowever, for an arcing fault to be detected, both the high current and a burst of light must exist. When both conditions are present, the optical relay can operate very quickly and send a signal to an upstream breaker or interrupter to clear the fault. Optical relays also can be used as the protective relay in a virtual main conﬁguration.\nVirtual Main Arc Flash Mitigation Systems\nSwitchgear and switchboards can be subjected to dangerous levels of arc ﬂash incident energy when fed directly from a power transformer. The addition of a virtual main system, where a digital relay and overcurrent sensing is added to the low-voltage side of the service transformer, can reduce the arc ﬂash energy on the entire switchgear, including the main incoming section. The system is designed to trip an existing upstream fault breaking device, often a medium-voltage circuit breaker or other vacuum interrupter. Additional features that help further reduce energy or maintain selective coordination may be added, such as:\n1) A maintenance selector switch, which temporarily lowers the instantaneous short circuit current setting. The maintenance setting lowers the available arc ﬂash incident energy and temporarily forfeits selective coordination.\n2) Zone-selective interlocking with downstream branch circuit breakers in the switchgear eliminates the need for the maintenance selector switch. Arc ﬂash energies can be permanently reduced with zone-selective interlocking.\nThe remaining section focuses on removing workers from potential harm as a strategy to mitigate risk.\nRemoving Workers From Harm\'s Way\nDe-energizing equipment does not absolve the facility from the responsibility of performing an arc ﬂash analysis or providing the necessary personal protective equipment (PPE).\nOSHA and NFPA have rules that prohibit energized work. In order to establish that a circuit is de-energized, the circuit must be approached to verify that it has been de-energized. Until then, the circuit must be considered energized per NFPA 70E and the worker who performs the veriﬁcation test must wear full PPE. Hiring contractors to perform electrical work does not absolve the facility from performing the calculations and providing the arc ﬂash exposure levels to the contractor’s workers.\nThe following arc ﬂash mitigation solutions remove a worker from the location of exposed energized parts or place a barrier between the workers and those energized parts.\nInfrared Viewing Windows\nHaving infrared (IR) windows permanently installed into electrical equipment enables IR scans to be performed without removing equipment covers, potentially saving the worker from exposure to hazardous energy. IR windows are made of a glass-like material that is transparent to infrared rays and allows hot spots to be registered by a thermographic camera. They also facilitate permanent access for inspection of electrical components without disturbing operations.\nOnline Temperature Monitoring\nOnline temperature monitoring, via wireless sensors, provides continual access to critical connection points where traditional thermography cannot be used. This technology evaluates the equipment’s current condition without exposing workers to energized parts because equipment covers do not have to be removed. The sensors are installed during a planned outage and can be used in equipment with high arc ﬂash ratings without a risk of danger to personnel or equipment.\nRemote Racking System\nA remote racking system (RRS) allows circuit breaker racking operations to be performed via a control panel located away from the cell, removing the operator from manual contact with the circuit breaker. If the operator controlling the RRS is located outside the arc ﬂash boundary, the need for PPE is eliminated.\nElectrical hazards are a signiﬁcant safety and ﬁnancial risk for electrical workers and their employers. OSHA mandates that work on electrical equipment must be performed in a manner that does not expose the worker to undue risk of injury. Complying with the safe work practices dictated by NFPA 70E and implementing arc ﬂash mitigation strategies through engineering controls will enhance workplace safety for employees and reduce the ﬁnancial risk for your company.\nThis article originally appeared in the June 2017 issue of Occupational Health & Safety.']"	['<urn:uuid:96fcec3e-a7ad-4cfa-b8db-d8761247d2a3>', '<urn:uuid:d4a7810d-08b4-4d09-b552-6f719bf98feb>']	factoid	with-premise	concise-and-natural	distant-from-document	three-doc	expert	2025-05-12T21:31:42.699391	9	60	2296
96	Which helps more with recovery: going to AA meetings or having social support?	Both AA meetings and social support networks are important for recovery, with research showing that 12-step participation (including AA meetings) is associated with improved outcomes. However, having broader social support and relationships that foster love, friendship, and hope is considered one of the four key dimensions that support recovery according to SAMHSA. Both elements work together - AA provides specific addiction support while social networks provide the essential foundation of relationships needed for sustained recovery.	"['Continued Growth Throughout Recovery\nBY: T. Franklin Murphy | December 17, 2019\nRecovery is a process of growth. There are four domains of growth that we should concentrate on to catapult us back into the game of life.\nA common obstacle to recovery is confusion on what to do after detox. Expelling the toxins from our blood doesn’t magically heal life. It just prepares us to engage in life. Continued human growth is a noble endeavor for everybody but especially relevant after detox.\nBut where do we begin? Franklin D. Roosevelt wisely counsels, “There are many ways of going forward, but only one way of standing still.” Years of addiction often leave a life in shambles, putting life back together is a chore. Successful recovery capitalizes on our capacity to grow. We can move forward in many different directions; we just can’t choose to stand still.\nDetox is a momentous step—getting clean is a priority. A chemical reliance is an ominous foe to recovery. For many, just getting out of the starting blocks seems impossible. Detox rightly deserves full attention. But once the demons of dopesickness are dispatched, we must shift. A gigantic hole needs filling. We must reengage in life-long development—human growth. Recovery is returning to life, growing through emotional and socially developing activities.\nMedically assisted treatment (MAT) has scientifically proven success, lifting many past dopesickness and pushing them towards sustainable recovery. Uncertainty about the next step after the initial success of detox, often ends with an unwise decision to prematurely eliminate the medical prescriptions. With so many honorable paths available for continued development, eliminating medicinal assistance can be postponed. We start the process best while still benefiting from chemical stabilization. Get life in order first, establishing new habits and then eliminate any medically prescribed assistance.\n""With so many honorable paths available for continued development, eliminating medicinal assistance can be postponed.""\nThe first months of recovery are bleak (see Boredom and Relapse). Our brains are still sensitive to the faulty reward system, heavily influenced by immediate rewards and blind to future costs and benefits. External stimulus frustrates and frightens a delicate recovery. Hang-on; it gets better—much better. Our brain rewires; it just needs time. The initial changes to our motivational programing takes months, but once engaged, the growth can continue for the remainder of our lives.\nGrowth occurs naturally and intentionally. Spontaneous growth is spurred by new environments. Internal elements and external surroundings stimulate or hinder development. We can intentionally influence this process by creating environments that reawakening our potentials.\n“One day at a time,” a staple motto of AA and NA groups is applicable. But at the dawning moments, success is more constrained, a day is much too long and full of too many temptations, “one moment at a time” may be more appropriate. This is the necessary beginning. New automobile drivers find the multi-tasking overwhelming. The inexperienced driver must first concentrate on immediate threats; a person in the first stage of recovery must do the same. Both driver and newly sober addict are vulnerable. But with experience, the expert driver lifts their gaze and prepares for future hazards, leaving more time to adjust. And with experience, the recovering person lifts their sights, prepares for obstacles and avoids costly last minute decisions.\nMaking it through detox is a courageous accomplishment. The next goal is creating a protective environment that promotes growth. We must lift our gaze. It’s time to develop skills to successfully live in a complex society. There are four key areas: life skills, emotional skills, self-discipline, and social expertness.\nThese domains overlap. Our mastery in these key areas will guide us through recovery and place us firmly back in the game of life. After years lost to addiction, we must first brush away interfering justifications. Any inkling that we have mastered any of these primary domains is certainly a protective lie that must be dismantled. We must humbly accept that our addictions have rendered us dysfunctional and we need repairing. We are not stronger, smarter, or more capable than others lost in addictions. They need help; we need help.\nLife skills are necessary for survival. In addiction we lose critical skills. The addicted life is driven by need. Time loses power. Internal cravings rule the clock. However, modern society is structured. Clocks and calendars are basic tools as necessary as the spear and fire were to our ancestors. Bills, appointments, and services have order, specific hours of operation, and defined rules of engagement. For us to successfully navigate these demands, we need new habits and constraints.\nUnfortunately, social agencies appointed to assist the needy are not driven by normal market influencers. The red tape is knotted and confusing. My experience is that many government employees distastefully and self-righteously judge the clients as inferior. They prefer to send away an applicant and move to the next in line rather than assist. Navigating these mazes of bureaucracy often requires multiple trips, analyzing conflicting advice, and enduring dehumanizing stares. Life skills are essential.\n“After stopping, anyone who has been addicted is extremely vulnerable. The addiction itself has taken a toll both physically and psychologically. They have forgotten how to deal with everyday life without the drug and they have to start learning how to cope. A radical turnaround starts with simple steps in mastering life skills, as well as filling the gap left by the absence of drug-determined activities” (Addenbrooke, 2011).\nFull time employment is necessary. The world of work is wrought with challenges to newly developing capabilities. Each employer has their own set of rules. We must skillfully perceive written and unwritten laws and work within these boundaries.\nEmotional Development (Emotional Regulation)\nOne of the most vicious consequences of addiction is that it retards emotional growth. The powerful chemicals interfere with reality and altered existence sparsely provides opportunities for development—we can’t adapt to realities that we habitual avoid. Addiction is a powerful defense to discomforting emotion—a maladaptive reaction.\n“When adults use drugs to cope with their problems, they. . . undermine the adaptive responses they might have learned, and they fail to develop the new ones that are needed to survive in our continuously changing world” (Rusche, 2007, p. 107).\nWhen new to sobriety, we will experience a rollercoaster of heightened emotion; normal emotional sways are magnified. These emotions don’t have to be frightening. I prefer to think of them as exhilarating, jarring from side to side with slow raises and significant drops. We mature with regular emotional contact, learning to redirect their energy towards healthy action. Emotion becomes the wind blowing against our sails, moving us over the choppy waters from the ebbs and flows of life. We just need to control the rudder.\nAt first, contact with feeling can overwhelm. We must learn to feel and accept; emotions are not the enemy. The first step for emotional development is dismantling of impenetrable barriers—the protective defenses. We must become open to emotion. Compassionate exposure to feeling developments our capacity to adapt to emotion without maladapted reactions. Our emotional development flattens the emotional waves. We can then move forward, feeling the richness of the joys and sorrows of existence. (See Focusing on Feeling).\nWe must learn to feel and accept; emotions are not the enemy.\nPutting our lives in order is work. We must accomplish tasks that avoid immediate harm but also lighten burdens on our futures. Future focused action must be forced at first; only to become habits later. We rewire our brains through forced attention. Overtime, deadlines and opportunities become emotionally salient, creating their own motivation. Our bodies adapt to the habitual presence of the future, spurring action to invite non-immediate rewards and to avoid the risks of future set-backs. We must lift our gaze.\nWhen lost in addiction, action waits for emergency to call. This programming doesn’t work for recovery. We can’t succeed through thoughtless reactions. The reactionary style creates the problem of addiction and leads to dreaded relapses. The anxiety of reoccurring (self-made) emergencies whittles at resolve. When we wait for the emergency to act, life obliges, providing constant emergencies that demand action. We must grab this angry bull by the horns and change it. When unprepared, the onslaught of offensive charges never stops. We can’t wait for breathing space to adjust—the space will never arrive. The unplanned expense, illness, or traffic jam wait to pounce and consume. Our lives already burdened from neglect can’t absorb too many more set-backs.\nLife set-backs can’t be eliminated; but can be minimized. Goals, planning, and preparation greatly enhance our ability to combat the unexpected. Many emergencies never materialize when we are prepared. Preparation is action when an immediate response isn’t necessary. So, get out of bed, turn off the video game, and do some preparatory work.\nA recent study on ambiguous risks published in JAMA Psychiatry expands on this topic. The world of everyday choices is full of ambiguous risks. Most problems that haunt our futures are not predictable. We know things will happen, and we know there will be obstacles; we just don’t know what they will be or when they will happen. A dark cloud surrounds our futures that greatly diminishes visibility. This study tested 70 opioid use patients in treatment to measure their known and ambiguity risk tolerance. They compared these findings with later successful completion of treatment. They discovered that relapse was significantly associated with higher levels of tolerance for ambiguous risk (Konova, et al. 2020). This is significant. The common risks of normal daily life were typically ignored by those who failed in recovery. The unknown probability of something bad happening did not motivate preparation or avoidance.\nRisk ambiguity is the chance of being caught in a crime, having a lie detected, or falling back into tempting environments. We must fortify our lives, avoiding actions that invite harsh consequences. We can’t just avoid the obvious stupid things; we must avoid the actions that randomly punish. I most likely can drive home drunk and get away with it. I can’t drive home drunk several nights a week and get away with it. I’ll either crash or be arrested. I’ll eventually suffer the consequence, maybe tonight, or may be next year. I just don’t know.\nDrug overdose is the current leading cause of unintentional death in the United States. Opioids (including fentanyl) accounts for the largest cause of these lethal doses (47,600 deaths in the United States in 2017). However, the growing literature and general knowledge of these dangers have little impact on the continued use. An intravenous drug user will typically explain that their specific habits of injection eliminates the risk. It doesn’t; and many unfortunately will die. Their tolerance for personal risk is gently wrapped in swaddling lies of self-justification.\nLeon Festinger’s theory of cognitive dissonance explains this paradox of self-deception. Festinger’s theory tells us that becoming aware of conflicts between our beliefs and our actions violates the natural human striving for mental harmony, or consonance (Banaji & Greenwald, 2016, p. 59). Basically, the mental gyrations of justification alleviate the appearance of risk. And since ambiguous risk is far from certain, bypassing of appropriate caution routinely goes unpunished. And as in the cited JAMA article is also predictive of failure. Habitual ignorance of risk is different than calculated exposure. Repeated disregard of potential dangers surely will backfire. Every now and then, we lose and get bitten—and consequences bite hard. The habitual risk taker loses another job, destroys another relationship, stumbles into another arrest, and finds themselves back in the throes of addiction.\n""The road to recovery will not always be easy, but I will take it one day at a time, focusing on the moments I\'ve dreamed about for so long.""\nSuccessful recovery requires avoiding places and circumstances where the possibility of relapse is present—very ambiguous. Recovery is achieved through a chain of non-emergency choices. Success in most ventures is achieved with proactive action before the emergency. We read the assigned chapter in biology tonight because we don’t know what next Tuesday (the night before the exam) will be like. We don’t shoot heroin because we don’t know how we will react in dopesickness. Ignorance of ambiguous risks are magnified by poor predictive abilities. This awful mixture is a destructive cocktail of dysfunctional adjustment. We ignore the risks and mis-predict our response, relying on strength we don’t have and ignoring susceptibility that is present.\nA Self-disciplined response is proactive preparation for possible complications that can beset the recovery progress; actions not motivated by known emergencies.\nCarter lived under the cement structures of a busy overpass near the San Francisco Bay. He was one of many intravenous drug users calling this small dirt lot their home. Although Carter spoke often of change, the heavy ball of heroin and unbreakable chain of addiction scarcely prevented any delays to his next fix. Carter’s moment of freedom, paradoxically, came from incarceration. After a short stay in jail, he was transferred to an inpatient recovery program. Upon graduation, Carter joyously swung his fists in the air signifying is momentous conquest. He escaped the horrors of homelessness and addiction.\nAssisted by the program, Carter found a union job with a nice paycheck and plenty of benefits. But his recovery was not complete. Carter’s freedom wasn’t an expression of strength; but a change in circumstances. He needed continued development. We’re all weaklings, given enough adversity we collapse. If we believe differently, then hard lessons will soon follow. Carter, believing he was now free, ignored the ambiguous risks of relapse. Carter ignored the repeated warnings of occasional use and old social scenes. Carter felt stronger than other people in recovery. He would never exchange his new life for the old existence under the bridge. He returned to his old social circles, bragging of his employment and housing.\nCarter’s story of recovery ended as quick as it started. His life ended, lying in the cold dirt under a familiar overpass. Carter subordinated everything in his life (shelter, sustenance, and family) to injecting heroin—twice (Bourgois & Schonberg, 2009).\nA lethal consequence of addiction is the loss of the protective shield provided by a family or a group (Jung 1976, p. 624). The stigmatized conception of addiction fills victims with shame and guilt. In the face of unconquerable cravings and misguided judgments, drug users sever relationships. The drug of preference becomes the primary relationship and only others willing to accept the addiction are allowed into the circle.\nOddly, those that allow the dysfunction to continue are praised. “They are the only one’s that understand me.” Anyone that sparks discomfort is avoided. This ugly part of addiction prevents growth, ensuring the drug remains the most powerful influence in the addict’s life.\nIn Mary Addenbrooke’s intriguing book, Survivors of Addiction: Narratives of Recovery, she explains, “The addicted lifestyle has to be protected, because of the shame. This is often hidden under a cover of bravado and it may involve secrecy and complicated deception – so that the addiction can continue unhindered” (2011).\nAddiction is isolating. The drug user willingly violates rules of social decency. These patterns of engagement become intricately woven into the brain. Social sense is exchanged for the next fix. Changing dysfunctional relationship adaptations is essential for recovery. Others matter; trust matters. Reconnecting with healthy people (employed and stable) is necessary. They hold us accountable. Social ethics of honesty, respect, and compassion must be reestablished. People matter more than substances.\nAddenbrooke in remarks following the self-report of a client that habitually failed in treatment wrote the following, “the addiction has taken over Lee’s life so that in place of growth, there is deterioration. It has led him into an enfeebled infantile position, dependent on others. It has a life of its own which is sucking his life-blood dry. . . The drugs have ‘got him’” (p. 144). Lee couldn’t escape his justifications. He continued to avoid healthy relationships and truncate emotions. He was left to contend with life ill-prepared and alone. His recovery never gained traction.\nWe need people to succeed. Recovery is an exercise in humility. The twelve-step programs emphasize the powerlessness of addiction. The years of false bravado, expressed intelligence, and followed by incomprehensible action must be reversed. We must embrace the fear, accept our ignorance and begin to act with wise preparation. This requires humility and courage.\nOthers aren’t perfect. Most are inexperienced with the temptations and obstacles that a person addicted must face. We quickly discover that helping hands give way. Friends, family and professionals occasionally disappoint in their humanness, falling short of our expectations. Our social development comes from tolerating the complex and mixed feelings that accompany imperfect relationships. We can’t demonize the people that give imperfect support. We must capitalize on these valuable resources. We need these people. Life flourishes with relationships that are simply ‘good enough’ to promote development.\nRecovery that journeys through these four essential domains repairs our brokenness. Our successes (and even failures) can lift us from helplessness back to a healthy life. Our mental health, physical wellness, and emotional stability will improve, creating the atmosphere where healthier relationships can be established. In this state of being, we can declare freedom from the demons, and glory in a promising future of love and comfort.\nPlease support FLS with a share:\nAddenbooke, M. (2011) Survivors of Addiction: Narratives of Recovery. Routledge; 1 edition.\nBanaji, M.R. & Greenwald, A. G. (2016) Blindspot: Hidden Biases of Good People. Bantam; Reprint edition\nBourgois, P. & Schonberg, J. (2009) Righteous Dopefiend (Volume 21) (California Series in Public Anthropology). University of California Press; First edition\nKonova, A., Lopez-Guzman, S., Urmanche, A., Ross, S., Louie, K., Rotrosen, J., & Glimcher, P. (2020). Computational Markers of Risky Decision-making for Identification of Temporal Windows of Vulnerability to Opioid Use in a Real-world Clinical Setting. JAMA Psychiatry, 77(4),\nRusche, S. (2007) False Messengers: How Addictive Drugs Change the Brain. Kindle Edition', 'The first step toward alcohol recovery is accepting that alcohol use interferes with your life. Then you can take the necessary next steps to recover. The recovery process can take time and patience and may affect as many parts of your life as alcohol misuse currently touches—such as your relationships, finances, or career. There are various paths to recovery and the process differs from person to person.\nIn this article:\nThe idea of “recovery” can feel insurmountable, especially when viewed through the biomedical disease model, which considers recovery to be abstinence that must be maintained for the rest of an individual’s life.1\nHowever, a 2012 study about the criteria for defining the “success” of alcohol addiction treatment notes that:\n“Relying on DSM criteria to define a sample of individuals in recovery may unintentionally exclude individuals who are engaging in non-abstinence or harm reduction techniques and making positive changes in their lives.”3\nThe study suggests that individuals whose misuse of alcohol has decreased and whose lives have improved in measurable ways are, in fact, in recovery. 3 Some researchers argue that improved psychosocial functioning and quality of life are more important recovery goals than maintaining strict abstinence with no relapses.4 In fact, many clinicians embrace a harm-reduction approach in which abstinence is included, but not necessarily the only goal.\nThe Substance Abuse and Mental Health Services Administration (SAMHSA) defines recovery as “a process of change through which individuals improve their health and wellness, live self-directed lives, and strive to reach their full potential.”5\nSAMHSA has listed four dimensions that support a life in recovery:5\n- Health—Overcoming or managing one’s substance dependency or use disorder while making informed, healthy choices that support physical and emotional well-being.\n- Home—Having a stable and safe place to live.\n- Purpose—Engaging in meaningful activities such as a job, school, hobbies, and having the independence, income, and resources to participate in society.\n- Community—Having social support and relationships that foster love, friendship, and hope.\nThe recovery process is supported by social networks and relationships. Your support network may include family and friends who become advocates for your recovery.5 Many treatment programs and community resources offer family and friends support groups for this purpose.\nTreating Alcohol Use Disorder\nMany individuals who struggle with severe alcohol use disorder (AUD) or with alcohol misuse over a long period of time enter an inpatient treatment facility as part of starting their recovery process. Inpatient or residential treatment programs offer various types of therapeutic modalities. Depending on the type of facility, additional amenities may be offered.\nIf you have a physical dependence on alcohol and experience severe withdrawal symptoms when you do not use it, you may need to enter an inpatient detoxification facility before beginning a rehabilitation or other treatment program.\nSome standard interventions offered in inpatient alcohol treatment centers include:\n- Educational groups—Support on specific issues such as anger management, trauma, and relapse prevention.\n- Medication-assisted treatment (MAT)—If medical staff is employed onsite, some treatment programs may administer and oversee medication-assisted therapy.\n- Individual counseling—Counselors meet with clients in one-on-one sessions to discuss treatment goals, challenges, and other life issues.\n- 12-step meetings—Clients may attend Alcoholics Anonymous (AA) or other 12-step meetings while in treatment.\n- Family counseling and education—These counseling sessions allow friends and family to engage in the patient’s treatment.\nSome additional amenities offered in some inpatient treatment centers include:\n- Recreational activities—Fitness workouts, swimming pools, outdoor excursions, movies, games, and sober activities can be a component of treatment.\n- Adventure therapy—Fun physical outdoor activities are organized for clients.\n- Equine Therapy—This modality includes activities with horses to improve mental health.\n- Mindfulness—Yoga, meditation, and other mindfulness practices may be offered in some facilities.\n- Nutritional education—Clients may attend educational groups on health and well-being, nutrition, and lifestyle.\n- Community service projects—Service work and group projects are a great way for clients to get involved in their community.\n- Employment training—Services for clients searching for employment or job skills training may be offered.\nNear the end of inpatient treatment, the client’s needs are assessed by staff members who work to develop an ongoing treatment plan for the client upon discharge. This plan, known as aftercare, may include arrangements for housing or employment, goals for recovery, and recommendations.\nMany treatment centers include a recommendation to attend Alcoholics Anonymous 12-step meetings as part of an aftercare plan. AA attendance is highly encouraged if the individual went to meetings during inpatient treatment and found them useful.6\nResearch shows that 12-step participation—which may include having a sponsor, attending meetings, and performing service work—is associated with improved outcomes in alcohol recovery.6 Many people attend 12-step programs most of their lives for the support, encouragement, and camaraderie found in the meetings.\nCall 800-839-1686 Toll Free. Privacy Guaranteed. No Commitment.Help is standing by 24 hours a day, 7 days a week.\nBeginning Your Journey\nWhen researching alcohol rehab centers, it is important to look for programs that cater to your specific needs, including rehabilitative programs that address multiple areas of your life. You may want to speak to the admissions department to learn more about the facility, clinicians, and program.\nIf you or a loved one is ready to get help, you may find the following questions helpful:\n- How long have you or your loved one been struggling with alcohol misuse? Individuals with a long history of alcohol misuse are more likely to experience severe withdrawal symptoms that require medical monitoring in an inpatient detox center as part of the recovery process.\n- Do you have medical or mental health concerns? Co-occurring physical or mental health conditions can impact how an addiction treatment specialist addresses your needs. Be upfront with your physician and treatment clinicians about your medical history.\n- Are you interested in a program that serves a specific population (teens, elderly, faith-based, LGBT, etc.)? Many treatment centers focus on specific demographics in order to offer highly relevant activities, group counseling, and other amenities. Attending a program with peers who not only understand your goal of long-term sobriety, but share similar life experiences may enhance your treatment experience.\n- Do you have other needs in treatment such as job skills training, family or couples therapy, or the ability to complete schoolwork in treatment? These options may be available as part of your treatment program. For example, if you have obligations you cannot set aside during treatment, you may decide to opt for an intensive outpatient (IOP) program rather than an inpatient program.\n- Does your insurance cover alcohol rehab treatment? In addition to speaking to the admissions department at your chosen treatment facility, you may need to contact your insurance carrier regarding the specifics of your coverage. In some cases, the facility or your primary care physician may need to request a preauthorization for inpatient services based on your medically indicated needs.\n- Do you want to stay close to home for treatment or do you have the ability to travel? You may choose to receive treatment closer or farther from home for a number of reasons. For example, you may have more treatment program options farther away from your hometown if you live in a rural location.\nRecovery from AUD is an individual path. But many people find recovery by taking advantage of their personal support networks, local community resources, and professional treatment. If you or a loved one is looking for treatment, call to speak to an addiction treatment specialist at 800-839-1686Who Answers? for assistance finding alcohol rehab.\n- National Institute on Drug Abuse (2014). The Science of Addiction, Treatment and Recovery.\n- Subbaraman, M. S., & Witbrodt, J. (2014). Differences between abstinent and non-abstinent individuals in recovery from alcohol use disorders. Addictive Behaviors, 39, 1730–1735.\n- Witkiewitz, K. (2012). “Success” following alcohol treatment: Moving beyond abstinence. Alcoholism: Clinical and Experimental Research, 37(s1), E9–E13.\n- Mocenni, C., Montefrancesco, G., & Tiezzi, S. (2019). A model of spontaneous remission from addiction. International Journal of Applied Behavioral Economics. 8(1), 21-48.\n- Substance Abuse and Mental Health Administration. (2015). Recovery and Recovery Support.\n- Witbrodt, J., & Kaskutas, L. (2005). Does diagnosis matter? Differential effects of 12-step participation and social networks on abstinence. The American Journal of Drug and Alcohol Abuse, (31), 685-707.']"	['<urn:uuid:84c024a3-77ad-41ac-a68b-519a353a33ff>', '<urn:uuid:c977dc37-8d08-48a1-818b-795fd8f6b871>']	factoid	with-premise	concise-and-natural	distant-from-document	comparison	novice	2025-05-12T21:31:42.699391	13	75	4346
97	What surprising discovery did a paleontologist make while working at a dig site in the Sahara Desert just before Halloween?	While at a dig site in the Sahara Desert, paleontologist Paul Sereno made an unexpected discovery when he was showing a research assistant their planned work area. He spotted a human foot protruding from the ground, which led to the discovery of a 7,000-year-old graveyard. This ancient burial ground was located on top of a dinosaur fossil bed, and they found hundreds of human remains at the site.	['Every week, embark with host Boyd Matson on an exploration of the latest discoveries and interviews with some of the most fascinating people on the planet, on National Geographic Weekend.\nPlease check listings near you to find the best way to listen to National Geographic Weekend, or pick your favorite segments and listen now below!\nEpisode: 1315 – Air Date: April 14, 2013\nMost people retire when they want to slow down and stop to smell the roses. But Len Stanmore decided to speed up when, early in his retirement, he was trekking on Mt. Kilimanjaro and learned of the quest to climb the tallest mountain on each continent, known as the Seven Summits. Just a few years later, he had achieved that task. And he ran 155 miles across the world’s most forbidding deserts. And he skied to both poles. But his adventures found meaning when he linked up with Trekking for Kids, to help improve the lives of orphans around the world.\nMost biologists are careful not to give human traits and motivations to the animals that they study. But certain animals – apes, notably – do exhibit traits that once were understood as uniquely human. Comforting a group member in grief, providing care to the elderly, and expectation of equal treatment are all moral behaviors performed by primates that help explain a social moral code in humans. Frans de Waal is a primatologist whose new book, The Bonobo And the Atheist: In Search of Humanism Among the Primates outlines the evolution of ethics and morals in apes in an attempt to explain the birth of religious belief in humans.\nBiologists studying animals in the field often have to hike miles through jungles and brush just to get a single glimpse of their species. But Luke Dollar and Andrew Jacobson tell Boyd that drone technology allows biologists to survey a much wider area. Lightweight aircraft are fairly easy to control and can follow animals as big as elephants or as small as lemurs, in a wide variety of terrain. They brought a drone to Madagascar to track and study the fossa, which can be elusive in the jungles.\nOn a recent trip to Botswana, Boyd heard tales of locals who would steal a kill from a lion. While visiting Kwando Lebala Lodge, Boyd’s guide, Balepi Mokwami tells him that they would regularly encourage a lion to share the meal that they’ve killed. Mokwami says that surprising a lion can be the biggest mistake a person will make. His trick is to slowly approach the cat in plain view, and appear as unafraid as possible, even if the lion starts to charge.\nDavid Braun, editor of National Geographic Daily News, tells Boyd about a submerged continent that was recently discovered under the island of Mauritius.\nMost significant scientific discoveries of significance are made after years of painstaking research. But sometimes, a scientist is in the right place at the right time. One night, just a few days before Halloween, on a dig site in the Sahara Desert, National Geographic Explorer and paleontologist Paul Sereno was showing a research assistant where they planned to work when he saw a human foot sticking out of the ground. Sereno tells Boyd that they happened into a 7,000 year old graveyard that lay on top of a dinosaur fossil bed where they located hundreds of human remains.\nThe human body is amazing. Just like rivers can reverse their flow, the human body can accept food through the rectum and be nourished. In her new book Gulp: Adventures on the Alimentary Canal, science writer Mary Roach points out that after an assassination attempt left President Garfield’s liver damaged, the surgeon general cooked up a recipe for beef enema to keep the president alive. Roach also explains how people can detect very minor smells and tastes in wines and olive oil, and she puts the myth of “The Old Lady Who Swallowed a Fly” to rest.\nMany species of shark have their numbers dwindling, as each year more than 100 million individual sharks are pulled from the oceans. But following the recent CITES conference in Bangkok, five species of shark have been listed, so that countries that plan on continuing to harvest must track the numbers of whitetip, hammerhead, and porbeagle sharks that pull from the waters and do so in a sustainable way. IFAW Program Director Kelvin Alie is encouraged by what he calls “decisive action” to stop the unregulated trade in wildlife parts.\nWhile demand for elephant ivory is causing the rapid decline of their numbers in Africa, a bloodless trade in elephant parts has originated in Siberia’s thawing tundra. Woolly mammoth tusks that have been frozen into the ground for thousands of years are providing a growing industry for tusk hunters who try to supply the increasing demand. The last mammoths died out 3,700 years ago, but many of their tusks can be found in pristine condition. Brook Larmer, author of “Of Mammoths and Men” in the April, 2013 issue of National Geographic magazine, tells Boyd that 90% of the tusks are shipped to China, where they’re turned into ornate carvings that can fetch up to a million dollars.\nBoyd loves spending time in Africa’s dry deserts and grasslands. After hunting lions on the hunt on foot, and learning to get close enough to an antelope to spear it, he gets a tutorial in quenching his thirst by digging up a water tuber and mashing its bulb to a juicy pulp and drinking the resulting moisture.']	['<urn:uuid:4a87e555-b419-49ae-9007-33fc78538384>']	open-ended	direct	verbose-and-natural	similar-to-document	single-doc	novice	2025-05-12T21:31:42.699391	20	68	924
98	What characteristics make Pseudomonas aeruginosa particularly dangerous in patients with cystic fibrosis, and how does its ability to form pigmented colonies relate to its defense mechanisms?	In cystic fibrosis patients, P. aeruginosa exploits the increased viscosity of mucoid secretions to form biofilms in the lungs, with infection rates reaching 60% and almost always becoming chronic. The bacteria produces fluorescent cultures due to pyoverdins, which are yellow-green siderophores that help secure iron ions. Additionally, it produces the blue pigment pyocyanin, which causes oxidative damage. These pigments, combined with its ability to form biofilms and develop antibiotic resistance through continual antimicrobial exposure, make it particularly difficult to treat, especially in healthcare settings where resistant strains can be transmitted between patients.	['Chloroflexi - Cyanobacteria - Actinobacteria - Endobacteria - Proteobacteria - Sphingobacteria - Archaebacteria\nBacteria are prokaryotes, with relatively simple cells commonly only a few micrometres in size. The DNA usually consists of a single main chromosome and often smaller plasmids, contained directly in the cell and replicated during binary fission. Some have internal vesicles but there are no more complex organelles as in eukaryotes, and most occur as single cells or simple colonies.\nMany bacteria have a peptidoglycan cell wall and a lipopolysaccharide outer membrane, though some groups are exceptions. Motile kinds variously have flagella or glide by secreting material. Much more than structural diversity, though, they have a tremendous range in metabolism, variously depending on photosynthesis, organic food, inorganic sources of energy, or switching between them.\nChloroflexi lack lipopolysaccharides. Many form filaments and some can move by gliding, principally the Chloroflexales, generally called filamentous green bacteria though a few are orange to red. These are usually chemoorganotrophs in aerobic conditions but also grow as anaerobic photoheterotrophs. Some kinds produce sulfur though not oxygen.\nMost have bacteriochlorophylls c or d as the main light gathering pigments, contained in vesicles called chlorosomes. These are actually mixtures of related phytochlorins. Chloroflexus aurantiacus for instance has mainly 31R-[8-E,12-M]-BChl cS, a stearol ester, but also the 31S isomer and some with other alcohol side-chains, especially in response to high-intensity light.\nThe accessory pigments are γ- and β-carotene inside chlorosomes and carotenoid glucosides outside them. C. aurantiacus in light produce fatty acid esters of OH-γ-carotene glucoside, and in dark echinenone and keto-myxocoxanthin glucoside. The last and its esters are also typical of orange species, which lack chlorosomes, as well as other groups like myxobacteria.\nCyanobacteria are photoautotrophic bacteria that produce oxygen, assimilating carbon from CO2 using water as a hydrogen source. They vary from single cells to branching filaments, and some are capable of gliding or have specialized cells. They occur in all sorts of aquatic habitats, and some are terrestrial or symbiotic in plants, chromists, fungi, or animals.\nIn most genera the main light-gathering pigments are bluish-green chlorophyll a and phycobiliproteins. The latter occur in aggregates on membranes and are coloured by bilin groups, notably phycocyanobilin in blue and phycoerythrobilin in red types, which differ in two bonds. These are attached to cysteine residues by opening the double bonds at the ends.\nThe main carotenoids are usually β-carotene, echinenone, often zeaxanthin, and myxoxanthophylls; the last are combinations of myxol and deoxy sugar residues as 2′-glycosides. Many kinds in mats and soils also produce pigments in external sheaths, most notably yellow-brown scytonemin, which serves as a sunscreen absorbing most UVA radiation.\nOscillatoriales are a grade of filamentous cyanobacteria with an irregular or radial arrangement of thylakoids, the inner membranes where photosynthesis takes place. The cells are all similar to one another, often much shorter than wide and sometimes forming several series in a common sheath. They are ancestral to the Nostocales.\nA variety of strains in these orders produce toxins. Aside from peptides the most widespread is anatoxin-a, a fast-acting neurotoxin that binds to acetylcholine receptors. This occurs in freshwater genera like Limnospira and Phormidium here or Dolichospermum in Nostocales, and can cause paralysis and death in exposed animals.\nMarine Okeania and Trichodesmium have more diverse compounds. A notable example is aplysiatoxin in the latter and its debromo form in both genera, also taken up by some sea hares that graze on cyanobacteria. This acts as a tumour promoter and inflammatory agent which can cause dermatitis in swimmers.\nNote: aplysiatoxins and lyngbyatoxin are commonly listed in Lyngbya, Moorea, Leibleinia, and so on, but these all seem to be misidentifications. I have not seen this discussed so have written up details here for anyone interested.\nNostocales are set apart by the production of heterocysts, pale enlarged cells specialized for fixing nitrogen, and sometimes also thick-walled akinetes that act as resting spores. They vary from simple to tapered, branched, and multiseriate filaments, some occurring singly and others forming aggregates that can reach macroscopic sizes.\nSaxitoxin and related compounds are a second kind of neurotoxin scattered here, blocking entry of sodium ions into active neurons. They are mainly found in Raphidiopsis, Aphanizomenon, and Dolichospermum as well as in marine dinoflagellates, apparently obtained by horizontal gene tranfer. These are the cause of paralytic shellfish poisoning.\nTwo more guanidine alkaloids are also found in these genera. First cylindrospermopsin in Raphidiopsis and Chrysosporum among others, which causes damage to the liver and other organs; then guanitoxin exclusive to some Dolichospermum and Sphaerospermopsis, another fast-acting neurotoxin that blocks acetylcholine breakdown.\nActinobacteria typically have a thick cell wall with either no outer membrane, or in some a unique type containing mycolic acid. Most have DNA with high G+C content and are largely aerobic. They vary from cocci and irregular rods to branching filaments called hyphae, often with differentiated substrate and aerial hyphae or sporangia somewhat like in molds.\nMany kinds accumulate carotenoid pigments, which give colonies colours from yellow to rose. The most widespread are regular carotenoids like β-carotene and canthaxanthin, extended C50 derivatives like decaprenoxanthin and its glycosides, and derivatives with aromatic rings like isorenieratene. The last is also found in green sulfur bacteria and sponges.\nStreptomycetales include one family that form extensively branched and non-fragmenting hyphae. Spores are ultimately produced in chains on aerial filaments, either at the ends or in whorls, and occasionally on substrate filaments as well. They are aerobic and primarily terrestrial, common in soil and litter, with a few pathogens of mainly plants.\nStreptomyces produce a great variety of antibiotics and have been their leading source for clinical use after penicillin. These began with streptomycin from S. griseus, the first treatment of mycobacterial infections like tuberculosis. Several other aminoglycosides have been important against mainly proteobacteria, for example kanamycins from S. kanamyceticus.\nBroader spectrum antibiotics include chloramphenicol from S. venezuelae, with use limited by high toxicity, and yellow tetracyclines like chlortetracycline from S. aureofaciens. All these interfere with bacterial ribosomes. Among others, daunorubicin from S. peucetius is used against leukemia, and macrolides like amphotericin B from S. nodosus against various fungi.\nSome insects also benefit from actinobacterial compounds, and in fungus-growers they can be key in maintaining cultivation against competing fungi. Among leaf-cutter ants some Acromyrmex have streptomycetes producing candicidin with this effect. Likewise mycangimycin was found in a strain from some Dendroctonus pine beetles, which carry fungi into trees they attack.\nEndobacteria typically have DNA with low G+C content and many produce endospores, distinctive heat-resistant stages formed internal to ordinary cells. Only a few have outer membranes; like Actinobacteria many others have thickened cell walls, recognized by positive Gram stains, but some endosymbiotic groups have instead lost them entirely.\nMost kinds are anaerobic to aerobic chemoorganotrophs. There is also photosynthesis in one family, the Heliobacteriaceae, found mainly in soil. These are brown to green anaerobes with thin cell walls, unique in having bacteriochlorophyll g as the light-gathering pigment as well as main pigment in reaction centres.\nThis is a bacteriochlorin but related to chlorophyll a, differing in just one double bond and in occurring as a farnesol ester. C20 terpenes are not produced at all, so the accessory pigments are only C30 carotenoids, mainly 4,4′-diaponeurosporene or derivatives. Most pigments in aerobic species are also carotenoids of this type.\nStaphyloxanthin is notable as a defense in Staphylococcus aureus, which cause a variety of skin and tissue infections including some notorious for antibiotic resistance. This causes the yellow colour of most strains and helps them resist oxidative attacks by the immune system. It is a fatty acid ester of the glucosyl diaponeurosporenoate shown.\nProteobacteria are the most diverse group of bacteria; they are united based on genetic trees, but aside from a thin cell wall and outer membrane without some peculiarities found in other groups, there is little they all have in common. Many have polar or peritrichous flagella, but some are non-motile or gliding, and they cover a very wide range of metabolic types.\nPhotosynthesis is found in a number of separate families, called purple bacteria. This usually only occurs under anaerobic conditions, though there are some aerobic chemoorganotrophs that use it to supplement their energy. None produce oxygen. In most bacteriochlorophyll aP is the only type present but a few have BChl b, which has an extra double bond, or other related forms.\nThe other light-gathering pigments are usually acyclic carotenoids. The most common are spirilloxanthin, which gives them a reddish colour, or else its precursors like lycopene and rhodopin, which colour them brownish. Some other aerobic proteobacteria also accumulate carotenoids, but usually cyclic types like zeaxanthin, astaxanthin, and their glycosides.\nMyxococcales, the myxobacteria, are aerobic rods that move by gliding. They feed on complex organic compounds and most prey on other microbes in soil and decomposing matter, where enzymes are concentrated by swarming. When nutrients are depleted the cells aggregate and differentiate to produce characteristic fruiting bodies, inside of which they form resistant spores.\nMost cell masses appear yellow to red from protective carotenoids, especially in response to light. These are mainly monocyclic types more like in other gliding bacteria than proteobacteria, for instance keto-myxocoxanthin glucoside or myxobactone, which occurs as fatty acid esters in Myxococcus and related genera.\nXanthomonadales include one family of strictly aerobic rods, relying on oxygen for respiration and mostly unable to reduce nitrate. Gliding is found in long and flexible Lysobacter, which like myxobacteria break down other microbes, but the others are non-motile or have usually one polar flagellum.\nMany types have coloured colonies; in particular esters of yellow xanthomonadins, a group of brominated polyenes, are characteristic of most Xanthomonas including many plant pathogens causing leaf spots, cankers, and so on. A red phenazine, myxin, is one of various antibiotics found in Lysobacter and has been used to treat fungal skin infections in animals.\nChromatiales mainly include the purple sulfur bacteria. These are photoautotrophic in anaerobic conditions, and some are also capable of aerobic chemolithotrophic growth. In both cases they oxidize sulfide, forming sulfur globules as an intermediate, usually inside the cells. They occur as single cells or aggregates, found in a variety of aquatic habitats with both light and sulfide.\nMost kinds have acyclic carotenoids like spirilloxanthin, but species in several genera produce okenone instead, which has an unusual aromatic end group. When present this is the only major carotenoid, colouring them purple-red, and so gives an advantage gathering light in habitats like deep water.\nPseudomonadales include two chemoorganotrophic families that mainly rely on aerobic respiration. Most are rod-shaped, with polar and lateral flagella variously present or absent. They play an important role in breaking down many compounds including mineral oils, and many are symbiotic or pathogenic in plants and animals.\nPseudomonas from soil are often associated with plant roots and some benefit them by suppressing other microbes. Many of these form fluorescent cultures owing to pyoverdins, where a quinoline chromophore is combined with a peptide chain. These are yellow-green siderophores released to secure iron ions for uptake by the cells.\nA number of different antibiotics occur in this group. For instance pyrrolnitrin and 2,4-diacetylphloroglucinol, both toxic to many fungi, and pyoluteorin, which inhibits water molds, are all found in P. fluorescens among others, used as a biological control for plant diseases. Mupirocin, a topical treatment for staphylococcus infections, also comes from this species.\nMany also have phenazine pigments, which cause oxidative damage to a variety of organisms. Pyocyanin is characteristic of most P. aeruginosa, opportunistic pathogens in injuries like burns, and gives infections a bluish discolouration. They also share orange oxychlororaphin and its matching acid with other species; some P. chlororaphis have a green reduced form.\nAlteromonadales include one family of chemoorganotrophs, some only capable of aerobic respiration and others also able to reduce nitrates. The cells are straight or curved rods, typically with one polar flagellum in motile kinds. Most are found in marine habitats and require sodium for growth.\nPseudoalteromonas produce many protective compounds. Some form biofilms on seaweeds or sessile animals that inhibit other fouling organisms; these have antimicrobial peptides and often purple violacein, found in various proteobacteria and toxic to grazing protozoans. P. luteoviolacea also have halogenated antibacterials like pentabromopseudilin.\nP. tunicata have both violacein and yellow tambjamine YP1, an antifungal compound related to pigments in bryozoa and sea squirts, as well as to red prodiginines like cycloprodigiosin in P. rubra. Another antifungal, isatin, occurs in P. issachenkonii on algae and an uncertain species on palaemonid shrimp eggs, where it has been found necessary for their survival.\nStill other species make tetrodotoxin, including P. tetraodonis and Shewanella algae from marine pufferfish. This is a strong neurotoxin similar to saxitoxin, and while the meat from these fish is sometimes eaten for the tingling sensation caused by minute traces, if the parts storing the toxin are not carefully removed it causes fatal respiratory failure.\nTetrodotoxin or its epimers are also reported in some other proteobacteria like certain Vibrio, and as taken up by a wide variety of marine animals: as defenses in pufferfish, gobies, crabs, snails, and certain worms, and in the predatory arrowworms and blue-ringed octopuses as venoms. They are also found in some terrestrial newts and frogs.\nSphingobacteria are again mainly united based on genetic trees. Flagella are only present in a few organotrophic kinds, and most are either non-motile or move by gliding. Some more unusual lipid types like sphingolipids or sulfonolipids, associated with such movement, are also relatively common in certain orders.\nChlorobiales, the green sulfur bacteria, are photoautotrophic bacteria that use sulfide or sulfur to assimilate carbon from CO2, depositing sulfur granules outside the cells as an intermediate. All are strictly anaerobic. They occur as either single cells, sometimes irregularly-shaped, or aggregates. Different species are coloured green or brown.\nThe primary light-gathering pigments are bacteriochlorophylls c, d, or e contained in chlorosomes, similar to filamentous green bacteria. Here these occur mainly as farnesol esters and have varying numbers of extra carbon atoms. Chlorobium tepidum for instance contains 31R-[8-E,12-E]-BChl cF along with smaller amounts that have propyl or isopropyl groups.\nThe main accessory pigments are carotenoids with one or two aromatic rings, depending on the bacteriochlorophyll. Green species have BChl c or d, which lacks the methyl on the main ring, and chlorobactene. Brown species have BChl e, a unique type with a carbonyl group as in chlorophyll b, and get their colour from isorenieratene.\nArchaebacteria have a single cell membrane with mostly only ether lipids, rather than esters as in other groups, and many live in conditions where those would not be stable like very high temperature or acidic water. Most have cell walls but no peptidoglycan, and flagella when present are also a unique type that grows at the base rather than the tip.\nThese are the closest relatives of eukaryotes, sharing a number of genetic features. They are often considered separate from the bacteria as the Archaea, though this generally presumes the rest all form a single lineage in their absence, and there is reason to think some are instead closer to them than to others.\nHalobacteriales, the halobacteria, are found in hypersaline habitats such as salt lakes, spoiled salted meats, and salterns. These are typically aerobic and chemoorganotrophic, but many can also use energy from light or conduct anaerobic respiration. They variously take the form of cocci, rods, or irregular cells, and with few exceptions are coloured bright red.\nThe main pigments are the C50 carotenoid bacterioruberin and its derivatives. Cells also have purple bacteriorhodopsin and other membrane proteins with retinal groups, involved in both detecting light and capturing its energy. This involves pumping ions in or out of the cells to maintain a potential, rather than electron transport as happens with phyto- or bacteriochlorins.', 'Pseudomonas aeruginosa is an opportunistic pathogen associated with a variety of life-threatening diseases. Infections caused by P. aeruginosa can be nearly untreatable because of its multidrug-resistance. One of the characteristics of P. aeruginosa that helps it survive in high drug concentrations is its ability to form biofilms–large communities of cells encompassed by extracellular polymeric substances that defend against many antibiotics. In fact, sub-minimum inhibitory concentrations of antibiotics stimulate biofilm production. This project aims to identify genes associated with biofilm induction in P. aeruginosa by screening a transposon mutant library for mutants that fail to show increased biofilm production when exposed to sub-minimum inhibitory concentrations of cefixime, tobramycin, and thiostrepton. So far, we have identified one gene, PA2714, that encodes a predicted molybdopterin oxidoreductase required for biofilm production. Because of the strong association between biofilm production and antibiotic tolerance in P. aeruginosa, the gene identified in this screen may be a useful therapeutic target for novel antimicrobials that can disrupt biofilm formation.\nPseudomonas aeruginosa, a gram-negative, rod- shaped bacteria, is a leading cause of hospital acquired infections; it remains difficult to treat because of its multidrug-resistance, especially in susceptible cystic fibrosis (CF) patients.1 CF is a genetic disease caused by mutations in the cystic fibrosis transmembrane conductance regulator (CFTR) gene.2 One characteristic of CF, increased viscosity of mucoid secretions, leads to environments conducive to the growth of P. aeruginosa biofilms in the lungs, resulting in 3 The prevalence of P. aeruginosa infection in CF patients is 60%, with acquired infections almost always becoming chronic.4 In CF patients, de novo emergence of antibiotic resistance has been observed after continual exposure to antimicrobials.2 In combination with patient-to-patient transmission of resistant strains in healthcare settings, this leads to the emergence of multidrug- resistant strains.2 It is important to continue studying the development of antibiotic resistance in P. aeruginosa to inform potential interventions.\nThe severity of P. aeruginosa infections is driven by its ability to form biofilms.5 A biofilm is a complex community of microorganisms embedded in an extracellular polymeric substance (EPS) that adheres to a surface. The EPS consists of a variety of macromolecules, including polysaccharides that act as a protective barrier against antimicrobials, proteins that provide a nutrient source via digestion of exogenous macromolecules, and extracellular DNA that facilitates horizontal gene transfer and provides structural integrity to the biofilm matrix.6 The benefits conferred by biofilms offer a more suitable environment, compared to planktonic growth, for bacteria.5\nSub-minimum inhibitory concentrations of antibiotics (sub-MIC), or concentrations below the lowest concentration that prevents growth, stimulate biofilm production in P. aeruginosa.7 Although the protective effect of the EPS is primarily structurally-related, the efficacy of antibiotics that can diffuse through this barrier is reduced against biofilms. One possible explanation for this phenomenon is that antimicrobial activity in biofilms can be quenched through diffusion-reaction inhibition via chelation of metal antimicrobials or enzymatic degradation of anti-microbials by proteins in the EPS.8 By lowering the concentration of antimicrobials to sub-lethal levels, cell survival within the biofilm community is promoted. In addition, there is a high degree of physiological heterogeneity within these cellular communities; in response to their local micro-environment some cells are more resistant than others due to their growth state or patterns of gene expression. The diverse set of genetic traits found in biofilm communities can be disseminated through horizontal gene transfer in order to increase overall antibiotic tolerance within the community. The acquisition of resistant genes can occur through uptake of extracellular DNA.5 It would be useful to investigate the genes associated with biofilm production in antibiotic conditions to gain insight into molecular mechanisms behind the antibiotic tolerance conferred by bio-films and to determine novel therapeutic targets for antimicrobials. We aim to identify genes that are essential to biofilm production by screening a library of transposon mutants for their ability to form biofilms in sub-MIC levels of the antibiotics cefixime, tobramycin, and thiostrepton. Genes associated with biofilm production can be elucidated by comparing biofilm production in mutants relative to wild-type P. aeruginosa. Furthermore, we will investigate the link between biofilms and antimicrobial activity by demonstrating that sub-MIC levels of antibiotics can stimulate biofilm production in wild type P. aeruginosa.\nA transposon mutant library of P. aeruginosa PAO1 KP containing the Himar1 Mariner trans- poson element was generated. A transposon is a mobile genetic element that can create mutations within the genome. The mutants in this library were created by mating PAO1 KP with Escherichia coli containing a plasmid carrying the Himar1 Mariner transposon, which inserts itself between TA dinucleotides, and a transposase that excises the transposon element from the plasmid.9 Containing a gene that encodes for gentamicin resistance, the transposon is inserted into a suicide vector unable to replicate in P. aeruginosa.9 Mutants that contained the transposon element in their genomic DNA were then selected by gentamicin screening, yielding ~13,500 mutants.\nThe library is currently being screened for mutants demonstrating a lack of biofilm stimulation in the presence of sub-MIC (5 μM) cefixime. The cut-off for increased biofilm production in wild-type PAO1 KP was established to be a >200% increase in biofilm in the presence of sub-MIC cefixime, compared to the vehicle control (dimethyl sulfoxide). Thus, hits from the screen were defined as all mutants that did not meet this cut-off (i.e. produced less than 200% of biofilm when exposed to the antibiotic, compared to DMSO). The bacteria were grown overnight in 150 μL of 10% lysogeny broth (LB) in a 96-well plate and subcultured for 2 hours in a 1:25 dilution. They were then grown overnight in a 96-well plate with a 96-peg lid with 5 μM cefixime or DMSO added. Biofilm was allowed to form on the pegs. After rinsing with 1X PBS to remove non-adherent bacteria, the biofilms on the pegs were stained with 150 μL of 0.1% (w/v) crystal violet for 15 minutes and de-stained into 200 μL of 33% (v/v) acetic acid for 5 minutes. Absorbance of the crystal violet in acetic acid was measured at 600 nm to quantify the relative amounts of biofilm.10\nFalse-positives were eliminated by retesting mutants identified from the initial screen through a dose-response experiment, with a concentration range spanning the MIC of cefixime, tobramycin, and thiostrepton to confirm that the mutants showed inhibited biofilm formation when compared with PAO1 KP in varying concentrations of three different antibiotics. Reduced biofilm formation would suggest that the gene disrupted by the Himar1 Mariner transposable element is essential for biofilm stimulation in the presence of sub-MIC antibiotic. The transposon insertion sites were then identified using two rounds of polymerase chain reaction (PCR). The first round was a touchdown PCR performed with a Himar1 Mariner-specific primer and an arbitrary primer. The second round of PCR was performed using the Himar1 Mariner-specific primer and a primer specific to the arbitrary primer used in the first round. PCR products were gel-purified and sequenced.\nOf the 4,200 mutants screened, one displayed reduced biofilm production in the presence of cefixime, tobramycin, and thiostrepton compared to the PAO1 KP wild-type control, hereafter referred to as BBTn41_G1. We also demonstrated that the highest levels of biofilm were observed at the highest concentration below the MIC of cefixime (5 μM) for both wild-type and the mutant. The results from the second round of screening (false positive elimination) for BBTn41_G1 is shown in Figure 1.\nTouchdown PCR determines that the transposable element in BBTn41_G1 is located at the start of the gene PA2714. Although this gene has yet to be characterized, the nucleotide sequence suggests that it encodes a molybdopterin oxido-reductase. Activated by binding to the cofactor molybdopterin, this family of enzymes catalyzes electron transfer reactions.11\nSo far, we have identified one possible gene that may contribute to biofilm formation. Based on its nucleotide sequence, PA2714 is likely to be gene encoding a molybdopterin oxidoreductase.11 This improves our understanding of the mechanism of molybdopterin-induced biofilm production. Loss-of-function mutations in molybdopterin synthase–an enzyme family that synthesizes the molybdopterin cofactor–increase biofilm formation.12 A possible explanation for this is that molybdopterin synthases consume guanine triphosphate (GTP), thereby reducing the amount of intracellular GTP available to form c-di-GMP, an important component of bio-films.12 Thus, decreased molybdopterin synthesis caused by mutations in molybdopterin synthases could increase intracellular GTP and c-di-GMP production, ultimately increasing biofilm production. In our experiment, we observed a decrease in biofilm formation with a molybdopterin oxidoreductase mutated by a transposon element.\nMoving forward, we will continue screening the rest of the library for genes that affect bio-film formation at sub-MIC antibiotic levels. Our findings may be useful in designing novel antimicrobials. With a better understanding of genetic factors affecting biofilm production, we can design drugs that inhibit biofilm formation and subsequently reduce antibiotic resistance.\nIn addition, many of the current high-throughput screens for new antimicrobials involve testing a huge library of drugs at arbitrary concentrations for indications of cell death.13 These screens could yield false negatives because MICs of new molecules are unknown; therefore, many potential antimicrobials are missed simply because they were screened at a concentration that was too low. Using biofilm production as an indicator of antimicrobial activity may be a more useful screening method, as we have demonstrated that relative amounts of biofilm production can be observed and measured at concentrations above and below the MIC of antibiotics.\nAntibiotic resistance represents a major global health threat. The deteriorating effectiveness of antibiotics renders both common and life-threatening diseases more difficult to treat.14 Accordingly, it is of paramount importance for the scientific community to better understand the molecular mechanisms behind antibiotic tolerance, so that novel and reliable antimicrobial treatments can be developed.\nThe World Health Organization has classified P. aeruginosa in the critical category of “priority pathogens” to target because of its multidrug-resistance and its threat in healthcare settings.15 In this project, we have demonstrated that P. aeruginosa has the ability to form biofilms in sub-MIC levels of antibiotics. In addition to identifying genes associated with biofilm production, we hope this project can serve as a proof-of-concept that biofilm production is a reliable indicator of antimicrobial activity in P. aeruginosa, a possibility that may be useful for drug screening. With the dearth of effective antibiotics, scientists may consider novel antimicrobials that target biofilm production as their mechanism of action.\nFigure 1: Effects of cefixime on PAO1 KP and BBTn41_G1 growth and biofilm production.\nA. Effects of cefixime on PAO1 KP planktonic growth and biofilm formation as a percentage of PAO1 KP in DMSO (n=1, 3 technical replicates).\nB. Effects of cefixime on BBTn41_G1 planktonic growth and biofilm formation as a percentage of BBTn41_G1 in DMSO (n=1, 3 technical replicates).\nBiofilm production increased for BBTn41_G1 and PAO1 KP with increasing concentration of drug until the lethal dose (10 μM). Both BBTn41_G1 and PAO1 KP showed maximum biofilm production at 5 μM of cefixime. However, at this concentration, BBTn41_G1 produced less than 600% more biofilm than the mutant in no drug, while PAO1 KP produced around 800% more biofilm than with no drug. Similar reduction in biofilm production was seen in tobramycin and thiostrepton.\nI would like to thank all the members of the Burrows Lab, especially Michael Ranieri, who created the transposon mutant library and aided me in labwork and manuscript preparation. I would also like to thank my supervisor, Dr. Lori Burrows, for her guidance.\nWritten by Jessica Chee\nReviewed by Dr. Lori Burrows, Professor, Department of Biochemistry and Biomedical Sciences & Department of Pathology and Molecular Medicine, McMaster University.\nReferences may be found in the journal.']	['<urn:uuid:7f3ef671-fd37-4cd8-bdce-a5d8a7f3ed5b>', '<urn:uuid:20652181-ad04-4e92-a24d-c054817b22ed>']	factoid	direct	verbose-and-natural	distant-from-document	multi-aspect	expert	2025-05-12T21:31:42.699391	26	92	4521
99	thailand coastal history environmental protection what traditional symbols show about water importance and modern ocean conservation efforts	Thailand's provincial emblems reflect historical connections to water - Nakhon Si Thammarat shows the Boromthat Chedi surrounded by zodiac signs from ancient times, while other coastal provinces feature maritime symbols. Today, protecting these coastal areas is crucial as human civilization developed along coasts worldwide, with everything eventually connecting to oceans through water cycles. Modern conservation efforts focus on reducing marine pollution through multiple strategies: implementing better waste treatment systems on ships, using environmentally-friendly technologies like Teflon-based hull coatings instead of toxic chemicals, and reducing emissions through improved engine efficiency. The goal is to maintain environmental quality while allowing sustainable development that doesn't compromise future generations' needs.	"['| Details :\n||The Emblem of Province : The Local Symbol\nProvince is called ""Changwat"" in Thai which change from ""Muang"" since King Rama VI. Thailand is devided into 75 provinces and special governed districts : the capital Bangkok, of which is also at a province level. So some Thai people count Bangkok as a province, making Thailand has 76 provinces.\nThe seals of the provinces in Thailand are derived from the official seals of the provincial governors during the period of absolute monarchy and the seals on the scouts\' standards during the fourteen cycles or the Monthon admistration in the reigns if King Rama VI and King Rama VII. In 1941, the government, under the preniership of Field Marshal of Fine Arts was responsible for designing the seal according to the wishes of each province. Today, when a new province is established, the seal of the new province will be designed and some of former seals may have to be been changed or adjusted as appropriate.\nThe second series have 10 stamps after last series sorted by Thai alphabet of their name are following:\n- Nakhon Ratchasma - the picture of the Statue of Tao Suranaree (or Khun Ying Mo) and the Chumphol Arc, representing the victory of Tao Suranaree who rescued the city from the domination of Chao Anuwong of Vientiane.\n- Nakhon Si Thammarat - the picture of the Boromthat Chedi with radiance surounded by the Twelve Zodiac Signs, which was originally the emblem of an ancient city under the domination of Nakhon Srithammarat.\n- Nakhon Sawan - the image of the ""Wiman"" which is believed to be the residence of angels and those endowed with merit. The Wiman correspondes to the meaning of the name of the city, the City of Heaven\'s Residents.\n- Nonthaburi - the image of an exquisitely designed water carrier, which is derived from the city\'s famous art of pottery, which is the main occupation of the residents.\n- Narathiwat - the picture of a sailing boat with fully unfurled sail bearing a picture of the ""Phra Sri Nararath Rajkirini"" White Elephant in full regalia. This elephant is one of the white elephants that is under the protection of His Majesty the King.\n- Nan - the image of the Auspharat Cow, which appeared in a dream of the city founder, and Phra That Chae Haeng -- a revered place of worship for te townpeople.\n- Buri Ram - the picture of the Prasat Phanomrung Sanctury, an important place of worship, and dancing angels who give blessing to the townpeople.\n- Pathum Thani - an image of a lotus, which suggests the meaning of the name of the city, and a spray of rice, which symbolizes fertility and abundance.\n- Prachuap Khiri Khan - the image of the Kuha Sawan Pavillion in the Phraya Maha Nakhon Cave, which Koh Lak and Koh Raed - two important islands of the city in the background.\n- Prachin Buri - an image of the Phra Sri Maha Bodhi tree, believed to have been taken from the Bodhi Tree under which the Lord Buddha was seated when he reached the Enlightenment in the City of Buddha Kaya\nQuantity of stamps: 900,000 pieces per design\nComposition: 10 stamps per sheet (different designs)\nPrinting Process: Lithography Multi-colour\nDesigner: Mr.Udon Niyomthum (Thailand Post Company Limited)\nFDC Price: 102 Baht (Salable at a set of 10 covers)\nQuantity of FDC: 13,000 covers', 'State of the planet2 - Marine environmental polution\n""It does not matter where on Earth you live; everyone is utterly dependent on the existence of that lovely, living saltwater soup. There\'s plenty of water in the universe without life, but nowhere is there life without water. The living ocean drives planetary chemistry, governs climate and weather, and otherwise provides the cornerstone of the life-support system for all creatures on our planet, from deep-sea starfish to desert sagebrush. That\'s why the ocean matters. If the sea is sick, we\'ll feel it. If it dies, we die. Our future and the state of the oceans are one.""\n• Sea Change A Message of the Oceans\nSylvia Earle, 19\nIn the beginning it was all water ,Human civilization itself start along all coat of the world , be it river , lake, ocean - and all of the world are linked to the ocean -not only linked together but also almost everything we use eventually get in touched with the water- under cycle water in the ecosystem - all activities we do on land eventually get washed buy the rain and pass through the ground and have encounter with the water table that merge to the inland water which then run into the ocean- the ocean get evaporated , for cloud which eventually turn to rain - all In the name of supporting human\nHuman benefit from marine and coastal ecosystem and activities\n• Coastal tourism =161 billion American dollars\n• Trade and shipping =155 billion American dollars\n• Offshore oil and gas = 132 billion American dollars\n• Fisheries = 80 billion American dollars\nTherefore it is important to be careful and maintain balance in dealing our activities The popular media attention is concentrated on loss of life and property. There is little prospect for preventing many of the disasters from occurring although much could be done to reduce their severity. Many impacts could be mitigated through better vulnerability and risk assessment, predictive modeling, information dissemination, and policy development.\nMajor source of pollution are:\n- air pollution\n- dredge disposal and contaminated sediments .\n- endangered and threatened species\n- landbased water pollution\n- oil pollution - regulatory compliance\n- ship/port generated waste - partnerships\nEffect from industry and household that run of into the river • Oil pollution • Chemical pollution • Harmful substances in package form • Sewage • Ballast water • Garbage • emission • Dumping of wastes liquid,solid)\nMain source of marine pollution:\nMarine Pollution I -Point form polution - Oil Pollution • ,Toxic Contaminants ,• Marine Debris • Mining and Dumping •\nMarine Pollution II - Nonpoint Pollution, • Sewage ,• Alien Species • Watershed Issues\nMain source from ships is in form of:\n•Operational - Through socio - economic impacts to marine ecology, habitat, and coastal infrastructures are affected though operational activities that results from: Oil spill, Emission, Ballast Water, Garbage, contamination, Antifouling Dredging activities.\n• Accidental risk - marine accident that could result to oil spills which then, end up degrading our environment about 400-300 thousands of oil entered the world ocean, collision with marine mammal, which then cause propeller injuries through : Grounding ,Stranding, Loss of oil, Hazardous cargo, Noxious liquid, collision with marine mammals.\n• Design - Risk associated with environmental issue n ship and in ship designing are : In the context of ship design the impacts areas are: Shipping Trends, Channel Design Criteria, Ship Maneuverability, Ship Controllability, and Use of Simulators in Channel Studies. Since world II many nations built port but forget about maintaining them while shipyard continues to build larger ships. Physical dimension and ratio of ships to channel has got impact in today\'s ship controllability.\nDischarge could be:\n• Intentional and unintentional discharge (oil, garbage, antifouling paint, air emission, on indigenous species from ballast water • Environmental damage and pollution due to port activities • Disturbance of marine environmental (collision and noise) • Emission from scraping of ships at the end of their life cycle Direct Discharges • Direct discharges are defined here to include releases from vessels, discharges of municipal and industrial wastewater via pipelines, and dumping of waste materials, such as dredged material, into ocean waters\nIndirect discharge • One to two-thirds of pollutants contributing to the degradation of coastal and marine waters are from indirect sources, and include sediments, nutrients, pathogens, and toxic compounds. Pollutants from agricultural and pasture lands include sediments, fertilizers, pesticides, herbicides, and animal wastes which contain bacteria and nutrients\nAccidental Releases- • Oil spill and bunkering fuel, Emmsion)Sox,Nox,CFC &VoC,Antifouling toxins,Ballast water discharges,Noise,Watse disposal at sea,Dredging @dispersal of soil Impacts • Habitat Destruction (overview) • Loss of Wetlands • Tourism and Recreation • Deforestation • Fresh Water Alterations • Fishing Issues (overview) • Over fishing • Ecosystem Changes • Bombs, Poison, Scrapes • By catch • Global Change Climate Change • Ozone Depletion • Coastal Development • Population\nOther impacts are -\nthe introduction of pathogens to coastal waters - alteration of water tables - modification of nutrient cycles or soil fertility - increased erosion - interference with navigation - a reduction in sport and commercial fishing yields - negative impacts on recreational boating and beach use .\nThe introduction of non-indigenous species often results in unexpected ecological, economic, and social impacts to the coastal and marine environment. Predation and competition by non-indigenous species has resulted in the eradication of some native populations and the drastic reduction of others, thereby altering local food webs. This process is often compounded by the exploitation of commercial fish. Overpopulation of some non-indigenous species has resulted in the degradation and loss of wetland vegetation and other submerged aquatic vegetation as a result of overgrazing.\nBecause industrialized society depends on petroleum products to maintain its accustomed standard of living, large volumes of petroleum are transported each day in the coastal and marine environment. Spills and leaks cause the formation of tar balls, oil slicks, and tar mats, and can impact the micro-layer, the benthos, the coast, and marine life.\nSustainability capacity building, efficiency optimization of development, practice and operations that meets the needs of the present generation without compromising the ability of future generation to meet their need\nGood environmental quality is essential for sustaining coastal and marine ecosystems, commercial and recreational fisheries, and economic growth in coastal communities.\nIt is also an important means of providing natural protection against rising sea levels and storm damage. The health of coastal and marine ecosystems is affected by water quality, and in turn, water quality is dependent upon ecosystem health. If one is impaired, the other is threatened.\nDespite their value and the programs designed to protect them, many coastal waters are being degraded at an alarming rate in addition to this, other advantages are: • compliance with all applicable environmental laws and regulations; • No significant adverse environmental impacts; • Wastes treated or destroyed on board to the extent practicable; • No inappropriate dependence on shore facilities for waste off-load and disposal; • Minimal energy consumption; • Minimal logistical costs for waste management; and • Minimal use of hazardousmaterials.\n• Low exhaust emission diesel engine achieves a 25% reduction in air emissions\n• Expected to result in reduced maintenance, better engine performance, and decreased fuel consumption\n• Hull of boat was coated with a Teflon-based coat that contains no toxic chemicals • Holding tank for waste prevents discharges\n• Port\'s attempt to reduce emissions from marine vessel engines.\n• Particular features of existing tug engines were modified to keep combustion temperatures as low as possible; this optimizes engine efficiency and produces fewer emissions.\n• With a Teflon coat, hulls are easy to clean, contain no toxics, and no longer need to be painted, reducing pollution from waste products generated during painting.\n• Surface sediments contaminated with metals, PAHs, PCB, and other organics\n• All clean material deposited in Mass Bay Disposal Site - innovation/uncommon\nothers - transferable - response to EPA or government initiative - significance and breadth of benefits - effectiveness and results - acknowledgement by others - Beneficial disposal of dredged material - Treating waste as resources and put them under recycling\nStrategies for shipboard control\n• Shipboard and waste emission outline -treatment and elimination - Pollution Prevention (P2) or Pollution Control-this is backbone of the thrust in achieving clean ship. Pollution Prevention Use fewer environmentally harmful substances and generate less waste on board.\nPollution Control: Increase treatment, processing, or destruction of wastes on board. The basic P2 principles follow:\nEliminating the use of environmentally harmful chemicals, such as ozone-depleting substance (ODSs), toxic antifoulant hull coatings, and other hazardous materials, may be the best approach for some potential of :\n• Sulfur reduction in bunker fuel\n• Nitrogen reduction to choice of propulsion system\n• On board Cataleptics system like charlatanic converter, water injection, emulsion\n• Operationally sped reduction and use of shore power connection has\nWhat is expected from youWe\'re all responsible for this mess, and it will take all of us to stop it from getting worse. It\'s time to completely rethink how we as a society use (or abuse) plastic. Here are some things that you can do right now:\nEvery time you see litter, pick it up and dispose of\nReduce, Reuse, Recycle - you\'ve heard it before, but\nnow you know what happens when you don\'t. Be conscious of all\nthat you buy, and be sure to avoid products with excessive\npackaging, especially in disposable products.\nDemand more and better recycling facilities in your\nTake part in local stream, river and beach cleanups -\nor organize one yourself. Though these don\'t solve the\nproblem, they are very effective at drawing attention to the\ngreater problem offshore.\nIf you live near the ocean, or a river that drains\ninto it, your storm drains are probably washing garbage right\nout to sea. Be conscious of this and any other potential\nsources of marine litter in your area. Demand that these are\n- Be very conscious of your ecological footprint. Encourage change though your decisions and do no accept the current paradigm of use and waste.\nFeature environmental technology\n- Ozone safe substances- 200-Ton Air-Conditioning Plant Conversion Kit -The CG-47and DDG-51 plants have been successfully converted to the ozone-friendly refrigerant HFC- 236fa conversion kit .\n-Solid waste - Solid-Waste Pulpers -The\npulper (especially the large pulper) is the machine into which\nyou dump tremendous quantities of paper, cardboard, or food\nwaste.The waste mixes with seawater to form slurry, which is\nthen discharged overboard. Studies show an immediate\n100,000-to-1 dilution when discharged into the wake of a ship.\nShips equipped with a pulper can dispose off their paper,\ncardboard, and food waste just about anywhere and at anytime-at\nsea including MARPOL areas.\nLiquid waste - OWS and Bilge water Polishers: Many bilge cleaners the Navy uses today contain long-lasting emulsifying agents, which produce stable oil-in-water emulsions that shipboard OWSs cannot effectively process. The popular media attention is concentrated on loss of life and property.\nThere is little prospect for preventing many of the disasters from occurring although much could be done to reduce their severity. Many impacts could be mitigated through better vulnerability and risk assessment, predictive modeling, information dissemination, and policy developmeent .\n""... [M]an\'s fingerprint is found everywhere in the oceans. Chemical contamination and litter can be observed from the poles to the tropics and from beaches to abyssal depths...But conditions in the marine environment vary widely. The open sea is still relatively clean...In contrast to the open ocean, the margins of the sea are affected by man almost everywhere, and encroachment on coastal areas continues worldwide...If unchecked, this trend will lead to global deterioration in the quality and productivity of the marine environment."" The State of the Marine Environment, 1989; Group of Experts on the Scientific Aspects of Marine Poll comments will appreciated -']"	['<urn:uuid:ed0f497a-b5b9-477d-ad19-edb498c74d5d>', '<urn:uuid:3c66f1d5-b4e8-4e49-8d6d-90b988085123>']	open-ended	with-premise	long-search-query	similar-to-document	multi-aspect	novice	2025-05-12T21:31:42.699391	17	106	2544
100	I need help shopping smarter. What should I look for and avoid in supermarkets?	Modern supermarkets are filled with carbohydrate-dense foods (CDFs) that have been carefully manufactured to be pleasurable but are packed with sugar calories, even when they don't taste sweet. These include items like instant oatmeal, multigrain cereals, muffins, trail mix, and fruit juices. When shopping, you should avoid foods with long lists of additives and instead choose items containing ingredients you'd find in your own kitchen. Be particularly careful with prepackaged produce - check use-by dates as older produce has fewer nutrients. Watch out for signs of contamination like bulging cans, open packages, or frozen vegetables frozen into a single lump. Even 'prewashed' salads may need additional washing to prevent food-borne illness.	"[""THE 9 RULES OF HEALTHY EATING\nOkay, now the details...\nRead Part 4 of this series: FOOD: Protein, Fat, Carbs and Fiber; What are They? PROTEIN\nWhat exactly are CDF's?\nA stroll through a modern American supermarket reveals a cornucopia of products which, from an evolutionary standpoint, constitute brand new forms of food. These products have been manufactured very carefully to provide high levels of eating pleasure but are packed with sugar calories. Many of these foods do not taste sweet, which can be confusing to people whose work and education do not include an in-depth knowledge of food biochemistry. These foods are quite literally being tested on us, the consumers, and while an underfunded Food and Drug Administration (FDA) is doing its best to protect us, the food industry has been extremely successful in bringing to market hundreds of common items which have not been properly vetted for the long-term health and safety of consumers. In the context of two thousand millennia of human evolution, instant oatmeal, multigrain cereals, muffins, trail mix, and fruit juices are all brand new additions to our food supply. Marketed as healthy food options, these CDFs are in fact the occult drivers of illness. A Big Gulp did not exist one hundred years ago. Nor did cheese puffs, protein bars, or fast food restaurants selling bread made from genetically modified (GMO) wheat, meats from unhealthy animals treated with hormones and antibiotics, and drinks loaded with more sugar than humans for two million years might typically have consumed in a week.\nSuch products are exceedingly easy to mass produce and they yield high profits for BIg Food manufacturers. What makes it worse is that they are literally designed by food scientists to push pleasure buttons in our brains making them particularly enjoyable and even addicting to consumers. Big Food has heavily marketed CDFs, linking their consumption psychologically with celebration, fun, happiness, and family. What would a summer barbecue be without hot dogs, hamburgers, and soft drinks? What would a birthday party be without cake? For that matter, what would breakfast be without toast? At Thanksgiving we eat stuffing, mashed potatoes, sweetened cranberry sauce, and pies. We celebrate Easter with chocolates, Christmas with candy canes and gingerbread cookies, and passover with matzah. CDFs have been marketed as integral components of our national identity and our daily lives. One can chart the expansion of CDFs into the TAD alongside the incidence of obesity and chronic diseases and see a nearly perfect parallel. For two million years we lived on the Human Diet. Over the last 50-60 years that diet has been turned upside down, with minimal fiber and two-thirds or more of our calories coming from carbs. Since the introduction of CDFs as dietary staples, the United States has become the poster nation for obesity and chronic illness.\nIf we want to be healthy, we must learn to eat healthily, which means a return to the nutritional blueprint of the Human Diet. It means learning how to make smart choices amidst an ever-widening field of cheap, unhealthy options scientifically manufactured to hook us. It means learning how to think, budget, and shop a certain way. We must learn and adopt a strategy for how to eat mostly natural, whole foods that are high in fiber and nutrients and lower in carbs. These are the foods to which we are evolutionarily well-adapted. In broad nutritional terms, it means eating in a way that keeps our blood sugar relatively stable. This paper is about how to create good dietary habits that can be sustained for the long-term.\nIf you do not care to delve too deeply into the details of this publication, you may refer to the following nine rules and use them as a guide for creating your own healthy diet. As of the writing of this essay (October, 2015), the following represent the rules that I follow based upon my best assessment of the highest quality and most compelling and current research in the medical, nutrition, and basic science literature.\nREAD ON: The 9 Rules of Healthy Eating\nStart from the beginning of this series of posts on Food and Nutrition\nHumans have been evolving for about two million years (two thousand millennia). Our brains have grown in size and function, enabling us to survive ever more by our ability to plan and less by physical prowess. Our communities have been ravaged repeatedly by infectious diseases and our immune systems have evolved to defend us against millions of strains of potentially harmful microorganisms. And throughout these two thousand millennia of evolution humans have survived on a relatively consistent diet of leaves, stalks, and bark, berries from bushes, wild grasses (all sources of fiber), whatever animals we could kill (protein and fat), nuts and seeds (fat, protein, plus some fiber and carbohydrates), flowers, roots, and the occasional fruits and vegetables available during warm seasons (fiber and carbohydrates). During this protracted period, the human digestive tract evolved to accommodate to this diet, which was rich in antioxidants, high in fiber, moderate in protein, and relatively low in fat and sugar (carbs). I call this diet the Human Diet. Very carbohydrate-dense foods like tropical fruits and sugarcane were not part of the Human Diet (with the exception of tiny populations of peoples living near the equator) and did not make their way into wider consumption until very recently in the history of our species. Even grains such as wheat and rice, which, like tropical fruits, contain very high levels of carbohydrates, were not incorporated into the Human Diet until about 10,000 years ago–a short time from an evolutionary standpoint, and, as will be discussed below, those grains bore little resemblance to the ultra carb-dense, genetically modified versions which have come to dominate the commercial food market over the last 50-60 years. Perhaps the most important point I hope to make here is that for about two million years humans evolved alongside a diet that kept blood sugar relatively stable because our food did not contain carbohydrate dense substances like bran muffins, apple juice, and candy bars.\nEvolution is an extremely slow and incremental process. It takes hundreds of generations for a species to adapt biologically to an environmental shift as radical as the recent changes in the typical American diet. The Human Diet to which we are evolutionarily adapted is the nutritional blueprint from which our bodies have learned to extract maximal nutrition and digest food with the least amount of stress. It is the diet that best enables us to develop properly, grow strong well-balanced immune systems, and live long healthy lives. The Human Diet offers us the best chance to avoid becoming obese and developing chronic diseases like hypertension, diabetes, and atherosclerosis which are now epidemic and quite literally ruining the health of our bodies and our economy.\nIn the last 50-60 years breakthroughs in the fields of biochemistry, engineering, and genetics have been applied to the manufacturing of fertilizers, insecticides, and crop breeding; this has utterly changed farming and has led to the introduction of new food substances into today’s typical American diet (TAD). Many of these new foods are fundamentally nutritionally different from those to which we are evolutionarily well-adapted. They contain proteins to which we have never been exposed, synthetic hormones, germicidal chemicals, and substances not found in anywhere in nature. Perhaps most pernicious of all, today’s food supply is dominated by carb-dense foods which cause blood sugar levels to soar. Research over the last 25-30 years has shown conclusively that consumption of carbohydrate dense foods (CDFs) provokes unprecedented escalations in blood sugar, and that these blood sugar spikes are responsible for the inflammatory changes that lead to obesity, illness, and drive the aging process. We now know that regular consumption of CDFs is the controllable behavior chiefly responsible for obesity and chronic diseases. The prevalence of CDFs in the TAD (typical American diet), more than any other single factor, is the cause of our current health and healthcare crises.\nRead Part 3 of this series: Carbohydrate Dense Foods (CDF's)\nFood consists of four categories of substance: protein, fat, carbohydrate, and fiber. All of our nutrients come from proteins, fats, and carbohydrates. Fiber supplies no nutrients, but is important to our health because it enables proper digestive and excretory functioning and enables healthy bacteria which live in our intestines to flourish.\nMuch disagreement has existed over the last few decades regarding what constitutes a healthy diet. The rules have changed dramatically in response to scientific research over the last few decades and my patients have a lot of questions. How many times a day should I eat? How much of my diet should come from carbohydrates? Which fats are healthy? How important is it to eat organic food? How many calories are safe to consume each day? What are GMO foods and are they safe to eat? Should I be avoiding gluten? How about red meat?\nI will attempt to answer these and other questions in some detail below, but first let’s take a moment to lay out in very general terms what I have come to see as the basic idea behind a healthy diet.\nRead Next Article in this series on Nutrition:Background Information Informing My Approach to Healthy Eating"", 'While processed and prepackaged foods can be fast and convenient options for meals and snacks, they aren\'t always the healthiest choices. Making your meals from whole foods or using minimally processed ingredients, such as frozen fruits or vegetables, is usually a better option due to the high levels of unhealthy ingredients commonly found in highly-processed foods.\nHealthy people should limit consumption of sodium to less than 2,300 milligrams per day, and those who are African-American, elderly or who suffer from high blood pressure, diabetes or kidney disease should limit their sodium intake to less than 1,500 milligrams. Most of the sodium in the American diet comes from processed and packaged foods, including pizza, snacks, soups, breads, meat dishes, pasta dishes, sandwiches, processed meats and cheese.\nSugary drinks, sweets and baked goods contribute high levels of added sugars to your diet. Many processed foods also contain added sugars, including high fructose corn syrup. Consuming high amounts of added sugars increases your risk for diabetes, high cholesterol, heart disease, obesity and tooth decay. Women should consume no more than 100 calories from added sugars per day, and men should consume no more than 155 calories. Each gram of sugar in your food equals 4 calories.\nPrepacked food often contains fat, including trans fat. Trans fat helps give prepackaged foods a longer shelf life. Any food that lists hydrogenated oil contains at least a small amount of trans fats, which increase your risk for high cholesterol, heart disease, stroke and heart attack and should be avoided as much as possible. Packaged foods also often contain unhealthy saturated fats, which should account for no more than 7 percent of your daily calories.\nAdditives are often used to improve the flavor, shelf life, safety or nutritional quality of prepackaged food or make it look more appealing. Although only additives generally regarded as safe by the U.S. Food and Drug Administration can be used in foods, some people may have adverse reactions to certain additives. These include olestra, aspartame, artificial colors and flavors, monosodium glutamate, saccharin, sodium nitrate, sulfites, cyclamate, caffeine, BHA, BHT and acesulfame-potassium. Foods that contain a shorter list of ingredients including only those you are likely to have in your own kitchen are healthier than those that contain a lot of additives.\nPrepackaged produce, such as bags of apples, carrots or salad mix, can be nutritious and convenient, but they are often treated to delay the signs of aging. Check the use-by dates to make sure your food is fresh. The older the produce, the fewer the nutrients it contains. Consumer Reports found that even those salads that were labeled ""prewashed"" still had some contamination, so wash your produce before consuming it to limit your risk of food-borne illness. Avoid any bulging cans, open packages and frozen vegetables that are frozen into a single lump, as these are all signs of potential contamination or lower nutritional quality.\n- Centers for Disease Control and Prevention: Where\'s the Sodium?\n- Family Doctor: Added Sugar: What You Need to Know\n- University of Kentucky: Chemical Cuisine -- Commonly Used Food Additives from A-Z\n- University of Maryland Medical Center: Trans Fats 101\n- Consumer Reports: Our Tests of Prepackaged Salads Reveal Bacterial Contamination\n- fruit and vegetables image by photo25th from Fotolia.com']"	['<urn:uuid:cca71e7a-02f3-4f1e-b8bb-9fd5095f9ce2>', '<urn:uuid:f714f6dd-99cf-4880-90e3-6b7e12c9c712>']	open-ended	with-premise	concise-and-natural	distant-from-document	multi-aspect	novice	2025-05-12T21:31:42.699391	14	111	2092
