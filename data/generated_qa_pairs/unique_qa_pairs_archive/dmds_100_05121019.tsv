qid	question	answer	context	document_ids	question_factuality	question_premise	question_phrasing	question_linguistic_variation	question_multi-doc	user_expertise-categorization	generation_timestamp	question_length	answer_length	context_length
1	What is the difference between dementia and Alzheimer's disease, and how has the timing of these conditions changed in recent years?	Dementia is a broader term describing a group of conditions characterized by gradual impairment of brain function, affecting memory, speech, cognition, behavior, and mobility. Alzheimer's disease is a specific type of dementia caused by nerve cell death resulting in brain shrinkage. Recent research indicates that people are developing these conditions later in life - there has been an increase in the mean age of dementia onset, with patients living with the condition for a shorter period. This trend has been observed over multiple time periods spanning 30 years.	"[""age-standardisation: A way to remove the influence of age when comparing populations with different age structures. This is usually necessary because the rates of many diseases vary strongly (usually increasing) with age. The age structures of the different populations are converted to the same ‘standard’ structure, and then the disease rates that would have occurred with that structure are calculated and compared.\nage structure: The relative number of people in each age group in a population.\nAlzheimer's disease: A degenerative brain disease caused by nerve cell death resulting in shrinkage of the brain. A common form of dementia.\nassociated cause(s) of death: A cause(s) listed on the Medical Certificate of Cause of Death, other than the underlying cause of death. They include the immediate cause, any intervening causes, and conditions that contributed to the death but were not related to the disease or condition causing death. See also cause(s) of death.\nasthma: A common, chronic inflammatory disease of the air passages that presents as episodes of wheezing, breathlessness and chest tightness due to widespread narrowing of the airways and obstruction of airflow.\nbronchitis: Inflammation of the main air passages (bronchi). May be acute or chronic.\nburden of disease and injury: The quantified impact of a disease or injury on a population, using the disability-adjusted life years (DALYs) measure.\ncancer (malignant neoplasm): A large range of diseases where some of the body’s cells become defective, begin to multiply out of control, invade and damage the area around them, and can then spread to other parts of the body to cause further damage.\ncardiomyopathy: A condition where there is direct and widespread damage to the heart muscle, weakening it. It can be due to various causes, such as viral infections and severe alcohol abuse. It can lead to an enlarged, thickened and dilated heart as well as heart failure.\ncardiovascular disease/condition: Any disease of the cardiovascular system, namely the heart (cardio) or blood vessels (vascular). Includes angina, heart attack, stroke and peripheral vascular disease. Also known as circulatory disease.\ncause(s) of death: All diseases, morbid conditions or injuries that either resulted in or contributed to death—and the circumstances of the accident or violence that produced any such injuries—that are entered on the Medical Certificate of Cause of Death. Causes of death are commonly reported by the underlying cause of death. See also associated cause(s) of death and multiple causes of death.\ncerebrovascular disease: Any disorder of the blood vessels supplying the brain or its covering membranes. A notable and major form of cerebrovascular disease is stroke.\nchronic: A term describing something that is persistent and long lasting.\nchronic diseases/conditions: A diverse group of diseases/conditions, such as heart disease, cancer and arthritis, which tend to be long lasting and persistent in their symptoms or development. Although these features also apply to some communicable diseases (infectious diseases), the term is usually confined to non-communicable diseases.\nchronic obstructive pulmonary disease (COPD): Serious, progressive and disabling long-term lung disease where damage to the lungs (usually because of both emphysema and chronic bronchitis) obstructs oxygen intake and causes increasing shortness of breath. By far the greatest cause of COPD is cigarette smoking.\ncoronary heart disease: A disease due to blockages in the heart’s own (coronary) arteries, expressed as angina or a heart attack. Also known as ischaemic heart disease.\ndementia: A term used to describe a group of similar conditions characterised by the gradual impairment of brain function. It is commonly associated with memory loss, but can affect speech, cognition (thought), behaviour and mobility. An individual’s personality may also change, and health and functional ability decline as the condition progresses.\ndiabetes (diabetes mellitus): A chronic condition where the body cannot properly use its main energy source—the sugar glucose. This is due to a relative or absolute deficiency in insulin, a hormone produced by the pancreas that helps glucose enter the body’s cells from the bloodstream and be processed by them. Diabetes is marked by an abnormal build-up of glucose in the blood; it can have serious short- and long-term effects. The three main types of diabetes are type 1 diabetes, type 2 diabetes and gestational diabetes.\ndisease: A physical or mental disturbance involving symptoms (such as pain or feeling unwell), dysfunction or tissue damage, especially if these symptoms and signs form a recognisable clinical pattern.\nemphysema: A chronic lung disease where over-expansion or destruction of the lung tissue blocks oxygen intake, leading to shortness of breath and other problems.\nheart failure: A condition that occurs when the heart functions less effectively in pumping blood around the body. It can result from a wide variety of diseases and conditions that can impair or overload the heart, such as heart attack, other conditions that damage the heart muscle directly (see cardiomyopathy), high blood pressure, or a damaged heart valve.\ninfluenza (flu): An acute contagious viral respiratory infection marked by fever, fatigue, muscle aches, headache, cough and sore throat.\nInternational Statistical Classification of Diseases and Related Health Problems (ICD): The World Health Organization’s internationally accepted classification of death and disease. The Tenth Revision (ICD-10) is currently in use.\nlife expectancy: An indication of how long a person can expect to live, depending on the age they have already reached. Technically, it is the number of years of life left to a person at a particular age if death rates do not change. The most commonly used measure is life expectancy at birth.\nmortality: Number or rate of deaths in a population during a given time period.\nmultiple causes of death: All the causes listed on the Medical Certificate of Cause of Death. These include the underlying cause of death and all associated cause(s) of death. See also cause(s) of death.\npneumonia: Inflammation of the lungs as a response to infection by bacteria or viruses. The air sacs become flooded with fluid, and inflammatory cells and affected areas of the lung become solid. Pneumonia is often quite rapid in onset and marked by a high fever, headache, cough, chest pain and shortness of breath.\npotentially avoidable deaths: Deaths among people younger than age 75 that are avoidable in the context of the present health care system. They include deaths from conditions that are potentially preventable through individualised care and/or treatable through existing primary or hospital care. They are a subset of premature deaths. The rate of potentially avoidable deaths in Australia is used as an indicator of the health system’s effectiveness. Potentially avoidable deaths are classified using nationally agreed definitions.\nrate: A rate is one number (the numerator) divided by another number (the denominator). The numerator is commonly the number of events in a specified time. The denominator is the population ‘at risk’ of the event. Rates (crude, age-specific and age-standardised) are generally multiplied by a number such as 100,000 to create whole numbers.\nstroke: An event that occurs when an artery supplying blood to the brain suddenly becomes blocked or bleeds. A stroke often causes paralysis of parts of the body normally controlled by that area of the brain, or speech problems and other symptoms. It is a major form of cerebrovascular disease.\nunderlying cause of death: The disease or injury that initiated the train of events leading directly to death, or the circumstances of the accident or violence that produced the fatal injury. See also cause(s) of death and associated cause(s) of death.\nWe'd love to know any feedback that you have about the AIHW website, its contents or reports.\nThe browser you are using to browse this website is outdated and some features may not display properly or be accessible to you. Please use a more recent browser for the best user experience."", 'Summary: Researchers reveal people may be developing Alzheimer’s later in life and living less time with the neurodegenerative disease.\nSource: UT San Antonio.\nThe diagnosis is one that a family never wants to hear: Your father has Alzheimer’s disease. Your mother has stroke-related dementia.\nA recently released study, included in a special supplement to the Journal of Gerontology, indicates that dementia’s impact might be compressing a bit. That is, people might be developing dementia later and living with it for a shorter period of time.\nSudha Seshadri, M.D., professor of neurology and founding director of the Glenn Biggs Institute for Alzheimer’s & Neurodegenerative Diseases at UT Health San Antonio, is the senior author on the study, which draws evidence from the Framingham Heart Study.\nIn data from four different time periods over a period of 30 years, the mean age at dementia onset increased, while the length of time living with dementia decreased. Is it because prevention and care of stroke today is superior compared to decades ago? Stroke is a major risk factor for dementia.\n“Prevention of stroke and reduced impact of stroke are great advances, but neither completely explains the trend we are seeing,” Dr. Seshadri said. “We are looking at other causes, such as lower burden of multiple infections because of vaccination, and possibly lower levels of lead or other pollutants in the atmosphere. Early education and nutrition might also play a role.”\nStroke risk has decreased because of greater control of blood pressure. Dr. Seshadri again cited Framingham data: “In the past, if you had a stroke you were at 90 percent greater risk to develop dementia. Today, you have a 40 percent greater risk,” she said.\nAbout this neuroscience research article\nSource: Will Sansom – UT San Antonio Publisher: Organized by NeuroscienceNews.com. Image Source: NeuroscienceNews.com image is in the public domain. Original Research: Abstract for “Are Trends in Dementia Incidence Associated With Compression in Morbidity? Evidence From The Framingham Heart Study” by Carole Dufouil, PhD Alexa Beiser, PhD Geneviève Chêne, MD, PhD Sudha Seshadri, MD in Journal of Gerontology. Published April 16 2018. doi:10.1093/geronb/gby001\nCite This NeuroscienceNews.com Article\n[cbtabs][cbtab title=”MLA”]UT San Antonio “Dementia Trend Shows Later Onset With Fewer Years of Disease.” NeuroscienceNews. NeuroscienceNews, 23 April 2018. <https://neurosciencenews.com/later-dementia-onset-8863/>.[/cbtab][cbtab title=”APA”]UT San Antonio (2018, April 23). Dementia Trend Shows Later Onset With Fewer Years of Disease. NeuroscienceNews. Retrieved April 23, 2018 from https://neurosciencenews.com/later-dementia-onset-8863/[/cbtab][cbtab title=”Chicago”]UT San Antonio “Dementia Trend Shows Later Onset With Fewer Years of Disease.” https://neurosciencenews.com/later-dementia-onset-8863/ (accessed April 23, 2018).[/cbtab][/cbtabs]\nAre Trends in Dementia Incidence Associated With Compression in Morbidity? Evidence From The Framingham Heart Study\nObjectives Several epidemiological studies suggest declining trends in dementia over the last three decades with both decreasing age-specific prevalence and incidence. There is limited data on whether this delayed clinical onset is accompanied by a shorter postdiagnosis survival.\nMethods A total of 5,205 participants from the Framingham Original and Offspring cohorts were studied. Four epochs were considered from 1977–1984 to 2004–2008. Gender and education adjusted 5-year mortality risks were estimated using delayed entry Cox models with the earliest epoch as reference category. Stratified analyses by sex, education, and age were undertaken. A nested case control study of 317 dementia cases and 317 controls matched on age, gender and epoch was initiated.\nResults In the whole sample, 5-year mortality risk has decreased with time, it was 33% lower in the last epoch compared to the earliest. In the 317 persons who developed dementia, age at onset increased (1.5 years/epoch), and years alive with dementia decreased (1 year/epoch) over time. We observed however, a decreased adjusted relative mortality risk (by 18%) in persons with dementia in 1986–1991 compared to 1977–1983 and no significant change from then to the latest epoch. The nested case control study suggested in matched controls that 5-year mortality relative risk had increased by 60% in the last epoch compared to Epoch 1.\nDiscussion In the FHS, in the last 30 years, disease duration in persons with dementia has decreased. However, age-adjusted mortality risk has slightly decreased after 1977–1983. Consequences of such trends on dementia prevalence should be investigated.']"	['<urn:uuid:d239ba78-8438-4162-b95d-b366b3f2ecbd>', '<urn:uuid:1b0d3d83-a900-4930-ad5a-df40f1b0fc4a>']	factoid	direct	verbose-and-natural	distant-from-document	multi-aspect	novice	2025-05-12T10:19:05.071190	21	88	1955
2	Why do some training programs need realistic physical controls?	Realistic physical controls are important in training because they help develop muscle memory and provide multi-sensory environmental cues that build better intuition. They're particularly crucial for scenarios involving tactile feedback, low-frequency vibrations, and secondary effects like condensation on cold pipes. For critical missions, such as astronaut training, recreating the physical environment as accurately as possible is essential for proper preparation.	['Defining fidelity in Simulators\nA major concern in planning a simulator for training is whether to go all-out and implement custom hardware, making the simulator as close a copy of the machinery being simulated as possible, or to create a lightweight representation that has functional equivalents for major systems, saving on cost what you lose in fidelity. Three parameters define the overall fidelity and accordingly, cost of development, in simulators.\n1. Functional fidelity is a measure of how well we emulate the user input -> System Response -> Output to User chain. Creating functional equivalence can be as simple as coding a few equations to approximate physics, or as complicated as replicating the entire code and hardware responses of a complex control system. This is also the area where most of the unavoidable cost and development time arise.\n2. Audio-visual fidelity is a measure of how realistically the environment is presented to the user in terms of visuals and audio cues. Budgeting for Visuals and Audio is a well-trodden road, and there are multiple methods to reduce costs and to cater to your particular budget.\nExample of Low vs High Visual Fidelity in a Flight Simulator\n3. Physical fidelity is a measure of how closely the interface to the simulator resembles that of the actual simulated machine. E.g. when simulating a switch, a touch interface has a higher physical fidelity than one based on mouse movements, while an actual physical switch has the highest fidelity. Perfectionism in this aspect of simulator development can greatly inflate your budget and you can save a lot of cost and reduce risk considerably if you implement the interface smartly.\nSimulating a spacewalk on the ISS, low vs high physical fidelity\nCost breakdown based on simulation quality levels\nSo how does a simulator project breakdown look like based on the three parameters above? Based on my own experience and reports from friends who work on similar projects in a wide variety of fields, this is what I have found:\nSimulation project cost estimation based on how much fidelity is required\nNote the above are just rough estimates of the cost breakdown, but there are a few trends that are easy to spot:\n1. Increasing a project’s fidelity, particularly physical and visual fidelity by a step can increase costs by a lot.\n2. Physical fidelity costs rise very quickly the closer you try to emulate the original object\n3. Audio/visual fidelity costs go up fast too, but not as badly as physical fidelity.\nThis means that in order to keep costs low, you need to limit physical, and audio/visual fidelity to what is absolutely necessary. Additionally, if you can achieve our training goal with a lower overall fidelity, you should not aim higher initially. I would suggest developing a low-fidelity simulator, and getting real-world data on training efficacy, before moving up to the next step. While some of the visual or physical assets will not be reusable for a later upgrade, most of the functional simulation will.\nNow that we have established increased physical fidelity as the major culprit in inflated simulator costs, let’s see what can be done about it.\nHow much physical fidelity is enough?\nThe answer to this question really depends on the situation. Mimicking the hardware as much as possible allows trainees to gain muscle memory and test the limits of their physical abilities in a realistic environment. For critical missions, like those performed by astronauts in orbit, recreating as much of that environment as physically possible makes sense. On the other hand, a race for maximal hardware parity can lead to budget over-estimations and hence project abandonment. The efficacy of simulators as a learning tool has been proven by many studies, thus it is worth looking at possibilities to capture the essence of the learning experience they provide, at the lowest possible cost to the organization. To understand what features can be scrapped without losing a whole lot of training value while seriously cutting down cost, let’s have a look at the benefits of high hardware fidelity:\n- The tactile sense of realistic hardware controls helps train muscle memory. (commercial haptic feedback devices are still way behind the curve compared to other user interface technologies)\n- Touch – there is no way to convey low-frequency vibration. (high frequency vibration can be conveyed by sound), etc. (e.g. in an HVAC or car repair simulator)\n- Secondary and tertiary effects that would be too hard to simulate in software. E.g. a very cold pipe in the real world will form condensates on it – this would have probably not been simulated in a normal simulator, thus the cue would not be available to the trainee.\n- Intuition is developed far better due to the multi-sensory environmental cues in a hardware based simulator and in the real world, compared to software sims.\n- Software sims depend on a level of abstraction and not all actions involved will be simulated, e.g. in a 2D simulation of a rotary gauge, the parallax effect will be “abstracted out” hence the user never learns how avoid this effect. (3D sims can of course teach this)\n- It is doubtful that cheap generic hardware for simulating full-body vibration, angle of body, gravity, acceleration, etc. will become commercially available. This means that ultra-realistic flight simulators e.g. where such effects are important (e.g. an airplane shaking violently might require input from the pilot) will still require expensive hardware.\nAside from cost reduction, moving towards software-only solutions with a lower physical fidelity offers other benefits:\n- Software-only controls allow for easy modifications without incurring any manufacturing of custom parts\n- software-only solutions can be installed on multiple machines and used by many users concurrently\n- software-only simulators ride the wave of advancement driven by gaming and software technology – much faster than advances in manufacturing, miniaturization and machining\n- software-only solutions allow for simulating machinery that is still in early design and planning stages and provide invaluable information to interface designers\n- software based solutions generally have a much lower cost to operate and a negligible maintenance cost\nIf you are looking to create a simulator for a vehicle, equipment or a skill/process, start by implementing a low fidelity (particularly physical and visual) solution. In most cases this can be done on a very low budget in the tens of thousands to hundreds of thousands of dollars. Once you have evaluated the training efficacy for your target audience, it is easy to ramp up visual and in particular, functional efficiency. Avoid increasing physical fidelity until you have reasonable proof that it is absolutely critical.']	['<urn:uuid:10729856-bd63-43fc-8f32-66bce7e1e5c4>']	open-ended	direct	concise-and-natural	distant-from-document	single-doc	novice	2025-05-12T10:19:05.071190	9	60	1102
3	I see a lot of hair products with wheat protein, but I also know wheat is important in our diet - how does wheat protein work differently in hair products versus when we eat it?	Wheat protein functions very differently in hair products versus food. In hair products, wheat protein is extensively chemically processed (hydrolyzed) to make it effective for reconstructing hair, as it's a good non-animal source of Keratin protein. The processing changes the protein's structure so significantly that even its gluten component is transformed into a different form that isn't an allergen. In contrast, when consumed as food, wheat protein is broken down by the body into amino acids through digestion and is used to assist in repairing cells, creating new cells, and maintaining tissues. While wheat protein in food can trigger gluten allergies, the same protein in hair products, due to its chemical modification, typically doesn't cause allergic reactions.	['Lee Hunter, Ph. D.\n“Natural”/”Organic” Hair Care Ingredients- Facts Frequently asked questions about a very confusing topic\nSince ancient Egyptian times, natural ingredients have been used as wonderful moisturizing and conditioning agents for hair. That fact is still true today as we all use natural things like coconut oil and panthenol (vitamin B) to achieve excellent moisturizing in products for skin and hair. However, Marketing claims with the term “natural” have created a great deal of confusion due to the fact that many of these claims are inaccurate and purposely misleading.\nI want to clear up some of the information “haze” with straight, honest FACT by laying out the science-based answers to some of the questions that I am asked most frequently about “natural” and “organic” ingredients and their use in hair care products.\n“Natural” has become a very common statement and claim made by hair products manufacturers. There is a belief that “natural” equals “good”. Is this generally true? The answer is yes and no. There is no question that many natural materials make excellent conditioning ingredients for shampoos and conditioners. However, at the same time, many of the most irritating and even dangerous materials on earth are also natural; disease- causing bacteria and viruses; plant toxins such as poison oak; allergens like pollen are excellent examples. Obviously, we cannot assume that just because something is “natural” that is good for us as an ingredient in the products we use.\nWhat does “Organic” mean? “Organic” refers to ingredients that are derived purely from vegetable sources. In other words from plants. An example would be aloe vera. Ingredients that are derived from non-vegetable sources are called “Inorganic”. An example would be silicone.\nIs it possible or practical to formulate “all-natural” or “all-vegetable” hair care products?\nNo. To my knowledge, there are no 100% natural hair products on the market today that meet the minimum safety and performance expectations of stylists and consumers. Virtually all of the hair products that I am aware of that claim to be “all natural” are not 100% natural or organic. There are many ingredients that are used in hair products that make them non-organic. Silicone is the most common inorganic element used that is inorganic; not organic and not vegetable sourced..\nWhat about “Vegan” hair products?\nAgain, the answer is No. Vegan is a term that refers to food and it specifically defines a 100% “natural vegetable” food diet that also excludes secondary products from animals such as eggs and dairy. Since no hair products are 100% “natural vegetable,” I would say they cannot be “vegan”. The use of the term “vegan” with respect to hair products is mis- leading and just another attempt to say “natural”. Applying food nutritional terms like “vegan” to non-food categories like shampoo creates confusion and is simply incorrect and mis-leading.\nWhat does “Naturally-Sourced” mean? If ingredients in products are called “naturally sourced” does it mean that they are “natural”?\nIn most instances, naturally-sourced ingredients are very far from being “natural” by any definition because most of them are synthesized by an extensive chemical manufacturing process involving strong acids and other chemicals including petrochemicals (derived from petroleum). Many very important and useful ingredients in hair products are manufactured starting with natural ingredients and other ingredients, but the final product is no longer natural. Since one of the starting materials is “natural”, then some product manufacturers deceptively refer to the finished raw material as “naturally-sourced.” This is untruthful and purposely misleading. I am not saying that the finished “unnatural” ingredients are bad; in fact they are normally very good. It’s just not accurate to imply that they are “natural” by calling them “naturally-sourced.”\nCoconut oil is one of the most common natural starting materials in the manufacture of raw materials used in hair conditioners and shampoos. Many excellent detangling conditioning ingredients (look for quaternerary amines on the ingredient label) are made through extensive chemical processing that involves many steps, starting with coconut oil.\nAnimal Protein Ingredients. Animal Protein ingredients are not used by any important hair care product lines that I know of today; and they have not been used for over twenty years! Therefore any reference to animal proteins as possibly being used by competitive hair products would be totally mis-leading. No hair products have them!\nKeratin Protein. Most hair stylists have been taught that human hair contains a very high percentage of Keratin Protein. Most protein like human skin is soft, but human hair is hard due to the high concentration of Keratin Protein. It is the Keratin Protein in hair that contains the disulfide bonds that are so important to permanent waving and straightening. When hair has been damaged and is missing protein, it is Keratin Protein that is most effective for reconstructing the hair. The best non-animal source of Keratin Protein is wheat which explains why most hair products contain wheat protein.\nProtein Ingredients. Virtually all protein ingredients contained in shampoos and conditioners today are made by very extensive chemical processing starting with natural materials like wheat grain and soy beans. (Look for the term “hydrolyzed” in the name, which indicates that the protein ingredient has been processed to make it work in hair.) If the wheat grain and soy beans were not so extensively processed chemically, they would not have any beneficial effect at all for conditioning and restructuring hair because the proteins that they contain consist of very large (too large) molecules that have no effect on hair.\nSoy Protein. Soy is a good natural source of protein, but the technology for processing soy to obtain good effective Keratin Protein has not been developed as well as for wheat protein at this time. This is why almost all manufacturers, including most “natural lines”, use wheat protein. (The largest selling “natural line” in the Salon industry uses wheat protein and Keratin amino acids according to their ingredient labeling.)\nWhat is the significance of Gluten-free hair products?\nGluten from wheat is a significant food allergen; not a significant external skin allergen. I have personally researched the issue of gluten as an external allergen extensively since the most effective reconstructors in the industry all use protein sourced from wheat. What I have found regarding the allergic effect of gluten from shampoos and reconstructors is that it is a very, very minor issue. First of all external exposure from a hair product is logically much less important than internal ingestion as a food. This is obvious. Next, the concentration of wheat protein in hair products is far lower by a factor of 50 to 100 times or more than in bread and other foods. Thirdly, and most importantly, the very rigorous chemical hydrolyzation process use to make usable protein in hair products substantially changes the chemical nature of the gluten into a completely different form that is not an allergen at all! Further on the issue of allergic responses to hair products with or without gluten, the main culprit is usually some ingredient in the fragrance of the product involved. Therefore, the “gluten-free” claim is something that is only significant in regard to food, not hair products. I believe that it is as mis-leading and deceptive to use “gluten- free” as a hair care claim as it is to state that products are “vegan”. Think of the relevance of a car manufacturer stating that their new sedan was safer because it was caffeine-free!\nShampoo Surfactants. The chemical processes for manufacturing many lather and cleansing agents (surfactants) used in shampoos start with the natural material coconut oil. Examples of such ingredients that can be found on the labels of the products are: sodium lauryl ether sulfate, Sodium Lauryl Sulfoacetate, Disodium Laureth Sulfosuccinate, Cocamidopropyl Hydroxysultaine, Cocamidopropylamine Oxide, Sodium Lauroyl Sarcosinate, Sodium Lauryl Sulfoacetate, Sodium Lauroyl Sarcosinate, Sodium Cocoyl Isethionate, Cocamidopropyl Hydroxysultaine, Cocamidopropylamine Oxide\nIn all of these examples, the actual ingredients used in shampoos may be called “naturally-sourced,” but they are far from “natural” since they have been chemically processed and converted into “unnatural” materials. In fact, in most instances the chemicals required for successful processing are very harsh materials like sulfuric acid and sodium hydroxide; and the “pieces” of molecules that are added on to the coconut oil molecule to make them useful and effective ingredients, are not necessarily natural materials themselves. In fact, they are frequently petroleum-based chemicals. This chemical processing does not make them unsafe or bad in any way; it just makes the reference “naturally-sourced”, or “all-vegetable” very misleading and deceptive if the manufacturer is obviously trying to lead the stylist or customer to believe that it is actually “natural” or “vegan”.\nWhat is the significance of “sulfate-free” products? Are they safer or healthier in any way? Are products that are not sulfate-free bad for hair color?\nProducts that contain sulfate surfactants are not a health safety issue. This has been verified by the CTFA and the FDA. No responsible scientist questions this. If the safety of sulfate surfactants is questioned by a competitive manufacturer of shampoo, they are not being honest! Period!\nColor-safe? Just like any other surfactant ingredients that lather and clean, sulfate surfactants must be formulated properly to be color-safe. Just because a shampoo contains a sulfate surfactant, it does not mean that it is not color-safe. The key to using sulfate surfactants most effectively is to formulate with the pH of the shampoo near neutral pH of 7.0; which is the same pH range as the body’s natural fluids such as blood, sweat and tears. I call the range around pH 7.0 the “Neutral Zone.” Shampoos with sulfate surfactants that are formulated in the neutral zone are more color-safe than the leading sulfate-free shampoos according to extensive repeated shampoo studies that we have performed. In fact, both shampoos were good for color fastness; but we have found that the sulfate-free shampoos that we tested did not clean as well as “neutral-zone” sulfate shampoos; and they were susceptible to build-up of conditioning and styling ingredients with multiple uses by consumers at home. Typically, consumers who use sulfate-free shampoos regularly find it necessary to trade off to a more thorough cleaning shampoo at least weekly to eliminate the build-up; then return to using the sulfate-free shampoo.\nAre styling polymers good or bad? The answer is that polymers are very good. In fact, the very substance of our skin and hair is made of natural collagen and protein polymers. Keratin protein that is the very backbone of hair is a polymer. Virtually all of the “hold ingredients” of styling products are polymers. Starch and natural gums are polymers, but when they are adapted for ingredients in styling products, they are extensively processed chemically in order to make them into useful styling ingredients. Most styling gels and pastes and mousses and sprays in the salon industry all use styling polymers that have been chemically processed, including the products that refer to their styling ingredients as starches or gums or other “naturally sourced” materials. This includes virtually all of the styling products in the “natural” lines that I am aware of in the industry today.\nIs “plastic” in hair products good or bad? “Plastic” is good! “Plastic” is a term used to describe a broad range of polymers. The main criteria that defines a “plastic polymer” is that it is “formable” into any desirable shape. This feature of some special “plastic polymers” is very desirable for hair styling products! Some examples of “plastic” styling polymers are as follows: PVP/VA; Octylacrylamide/Acrylates/Butylaminoethyl Methacrolate Copolymer; Butyl Ester of PVM/MA, PVP; and there are hundreds more. Incidentally, not only are these polymers “plastic”, all of them are not “natural” or “vegetable” or “vegan” by any possible definition. There are some natural “plastic” polymers that are used in hair styling products as well like Dehydroxantham gum. Since it is a “formable polymer”, it is “plastic” too!\nWhat is the significance of “paraben-free”? This is another attempt at deceptive marketing. Parabens have been used as very effective and safe preservatives in hair and skin products for over fifty years. In the past ten years, there have been some questions raised about the safety of the parabens. These questions have been researched extensively with in-depth scientific safety studies by top scientists for the CTFA (Cosmetic Toiletries Fragrances Association) and the FDA (Food and Drug Association) and they have found the parabens to be safe for use in cosmetics, including hair products. The CIR (Cosmetic Ingredient Review Committee) which is a committee of top scientists under the authority of the CTFA and FDA, has certified the parabens as safe and effective for use in cosmetics. Unfortunately, some companies in our industry, without a conscience, will take single articles of negative information that have been totally disproven; and ignore the evidence of disproof; and use the negative information to say that the parabens are unsafe. This is very unscrupulous and irresponsible! Especially when it comes to trying to use the word “Cancer” to sell shampoo! All of us in society need to focus honestly on the search for the causes of cancer; and the cures for cancer; without confusing the issue with inaccurate and mis-leading information. Distorting the issue of Cancer in order to sell shampoo is totally without any Integrity!\nConclusion. In conclusion, “natural” ingredients are mostly very good for hair products. Natural materials also make wonderful starting materials for the chemical manufacture of ingredients for hair products. However, it is important to know the true facts about “natural” and other misleading claims that are made regarding issues like “naturally- sourced”, “sulfate-free”, “gluten-free”, “paraben-free”, “polymers” and “vegan” in hair products in order to be a well-informed stylist and consumer with integrity.\n“Science Makes a Better Hairstylist”\nDr. Lee Hunter\nDr. Lee Hunter is President and CEO of ProDesign. He has had thirty-seven years of successful and distinguished experience in the personal care products industry leading R&D groups for several major corporations. Dr. Hunter is generally recognized as one of the world’s experts in the science of hair and personal care product development. Dr. Hunter holds a BS degree in Mathematics and Chemistry; and a Ph. D. in Physical Chemistry from Iowa State University.', 'The purpose and function of proteins in your diet is to assist the human body in the process of repairing cells, to create new cells, maintain tissues, and synthesize new proteins which are important for the body to perform basic bodily functions – essential for growth and development and human function.\nMuscle tissue contains myosin, actin, myoglobin, and a number of other proteins. Hair nails and the skin’s outer layers are made of the protein Keratin. Bones are also made up of protein. The outer layer of the bone structure is hardened with minerals such as calcium, and the inner structure is made up of bone marrow and this all contains protein.\nRed blood cells contain haemoglobin. Haemoglobin is a protein compound which transfers oxygen throughout the body. The clear fluid in blood is called Plasma and this also contains protein and fat. These particles are known as lipoproteins. Lipoproteins carry cholesterol around and out of the body.\nHuman cell creation, hair, nails, blood, muscle cells, chromosomes, neurotransmitters and many other parts within the human body require or are made up of protein. Proteins are present in the outer and inner membranes of every living cell. Therefore, protein makes up an essential part of the human body and its function.\nProtein Food Sources:\nProtein is an important nutrient that can be found in animal products and other sources such as; vegetables, nuts and seeds. Approximately half the protein we consume in our diets goes into creating enzymes. Enzymes are proteins that have a specific role such as; digestion of food and molecule assembly and division to make new cells and chemical substances within the body.\nEnzymes carry out these functions by the utilisation of certain vitamins and minerals. The human body breaks proteins into parts called amino acids. This process is carried out by digestion. The body requires a number of amino acids in a large amount – this is essential to maintain good health. Amino acids are found in animal sources such as milk, meats, fish, and eggs.\nYou do not need to consume animal products to get protein in your diet. Protein can also be found in vegetables, plant sources , nuts and grains such as; soy, legumes, lentils, beans and some grains (such as wheat germ and quinoa).\nStudies show that consuming protein can also help with weight loss. It boosts your metabolic rate and can reduce your appetite. Protein is much more satiating than both fat and carbs.\nHow much protein do you need in your diet?\nThe amount of protein each individual requires depends on your overall calorie needs. The daily recommended intake of protein for healthy adults is 10% to 35% of your total calorie needs.\nChildren and teens may need different amounts of protein depending on their age. The requirement of Proteins is significantly increased in people who are physically active, as well as in elderly individuals and those recovering from injuries.\nTo conclude; the human body breaks proteins into parts called amino acids. Some of these amino acids can be produced by the human body, while we must get others – “essential” amino acids – from our dietary sources. Protein is vital for the human body, its function and for health.\nThank you for reading!']	['<urn:uuid:e46fb56a-aa4d-4a85-ab4c-5bd0f9e3154a>', '<urn:uuid:8e1ee596-e248-41be-8424-2eb25eedde9f>']	open-ended	direct	verbose-and-natural	distant-from-document	comparison	novice	2025-05-12T10:19:05.071190	35	117	2900
4	coral bleaching issues vs stinger jellyfish threats	Coral bleaching is a major ecological issue caused by temperature stress that disrupts symbiosis between coral hosts and zooxanthellae, leading to reduced photosynthesis and potential coral death. In contrast, stinger threats like box jellyfish are primarily a seasonal safety concern during summer months (October to March) in coastal waters, though they're not typically found on outer reef areas.	"['Send the link below via email or IMCopy\nPresent to your audienceStart remote presentation\n- Invited audience members will follow you as you navigate and present\n- People invited to a presentation do not need a Prezi account\n- This link expires 10 minutes after you close the presentation\n- A maximum of 30 users can follow your presentation\n- Learn more about this feature in our knowledge base article\nDo you really want to delete this prezi?\nNeither you, nor the coeditors you shared it with will be able to recover it again.\nMake your likes visible on Facebook?\nConnect your Facebook account to Prezi and let your likes appear on your timeline.\nYou can change this under Settings & Account at any time.\nTranscript of Coral Bleaching\ncreate skeletons out of calcium carbonate (limestone)\nrequire algae zooxanthellae for survival (zo-zan-THEL-ee) Soft Coral: soft, bendy, resemble plants http://en.wikipedia.org/wiki/File:Pink_soft_coral_Nick_Hobgood.jpg http://underwater.com.au/image/id/1067 Coral Reefs support ~25% marine species Home to 4,000 species of fish, 700 different corals and thousands of plants and other animals! Coral Reefs can be found in 100 different countries around the globe including Eastern, Western and Central Pacific waters, the Caribbean and the Indian Ocean!!! http://en.wikipedia.org/wiki/File:World_Map_FIVB.png Great barrier Reef\n- Queensland Australia - typically found in shallow waters within tropics\n- ecologically, economically + culturally sig.\n- ""hot-spots"" of biodiversity + important for fisheries and tourism http://www.bayactionplan.com/2011/02/comparing-chesapeake-bay-and-great-barrier-reef/ ...the result of the disruption of symbioses between the coral hosts + photosynthetic endosymbionts (zooxanthellae) whitening of the coral reef results... http://www.phoenixislands.org/bleaching.html Recorded since 1970-1980s\na growing concern for ecosystems\nsparking study + observation Causes of Coral Bleaching?! Many triggers for causing bleaching Triggers... In a laboratory bleaching can be caused by several factors... Extreme temperature change http://www.projectaware.org/blog/blueseasonbali/mar-04-12/zooxanthallae-adapt-temperatures Solar light Micro-organisms Pollution Majority of the world’s major coral reefs have been affected by bleaching due to thermal changes Research conducted at the Houtman Abrolhos Islands\nVery temperate and tropical temperature\nThere is an array of diversity - 184 species With increase in seawater there was a widespread of bleaching events in the Abrolhos\nSummertime bleaching linked to temperature change http://en.wikipedia.org/wiki/List_of_islands_in_the_Houtman_Abrolhos Experiment conducted to see how light change affects symbiont photosynthesis\nChlorophyll tissue extracted from zooxanthella and subjected to different light frequencies http://www.lizasreef.com/HOPE%20FOR%20THE%20OCEANS/coral_reef_ecology.htm Overall result was that the affects of light vary among dinoflagellates High exposure to UV results in more photosynthesis which can in turn result in bleaching http://jeanninezito.blogspot.ca/2011/04/mr-sun.html Bacteria can also result in bleaching 1993 rod shaped bacteria found attached to the bleached zone of a coral Samples collected from the Mediterranean and cream colored bacteria on all of the bleached ones\nAbsent from the unbleached Many different types of exp. conducted - resulted in bacteria causing bleaching\nBacteria placed on a healthy coral in a sterile environment\nBacteria placed on two healthy corals and antibiotics given to one Sunscreen (Pollution)\nPollution is a recent cause discovered for bleaching\nCompounds of sunscreen destroy the dinoflagellates on the coral causing them to bleach http://fitnessandfrozengrapes.com/2012/06/21/5-smart-tips-to-stay-cool-during-your-workout/ Conservation in a changing climate require pragmatic conservation actions informed by sitespecific understanding of susceptibility to climate change and capacity of societies to cope with and adapt to change.\nBuilding of large reserves protects biodiversity which enhances the maintenance of genetic diversity. Loss of genetic diversity can decrease population viability (through genetic drift and inbreeding) and decrease the ability to adapt to a changing environment. 1. Marine protected area/ Coral reserves Great Barrier Reef Marine Park (GBRMP), Queensland, Australia (image from reference) Reserve consideration: a) Reserve location\nRich in biodiversity, resilience (resilience population can save others) b) Reserve size\nLarger are better (can sustain multiple species, conserve genetic diversity) c) Reserve spacing\nClose enough to ensure connectivity, prevent risk of wide-spread damages 2. Satellite and in situ temperature observations Increased the ability to detect anomalous and persistent warm water and are being widely used to predict climate change, coral bleaching and mortality. Identification of ‘bleaching hotspot’ Hotspot evolution in the western Indian Ocean in 1998 and 2005. (image from reference) Site specification enable the study of reef recovery after past bleaching events\nAct as an early warning system 3. Conservation genetics seek for a better understanding of the resilience of corals reefs; that is, how much they can absorbed, and recovered from, before ecosystem functions are lost. http://www.psypost.org/2012/08/remaking-history-a-new-take-on-how-evolution-has-shaped-modern-europeans-13350 Emphasize:\n(i) Evolutionary History\n(ii) Genetic connectivity\n(iii) Molecular markers for coral stress;\n(iv) The role and genetic identification of coral-inhabiting algal endosymbionts. THE END THANKS FOR WATCHING 4. Global Partnership Climate change is global, a global action is needed\nIncreasing concern on global coral reef status promote formation of various international partnerships such as ICRI The International Coral Reef Initiative (ICRI) is an international partnership, established to reverse the global degradation of coral reefs\nConduct seminar, convention, workshops, assessment reports http://www.gbrmpa.gov.au/about-us/corporate-information/our-organisation/international-coral-reef-initiative Assessment report: Status of Coral Reefs of the Pacific and Outlook: 2011 Why Is Coral Bleaching an issue? Temperature stress = lower photosynthetic capacity of corals\nDisrupts symbiosis between coral hosts and zooxanthellae\nPhotosynthesis 2 (and electron transport chain) impaired Associated organism communities http://www.underwaterphotography.com/photo-contest/default.aspx?countryid=324&CurPage=5&Chapter=0 Lizard Island, Australia: coral dwelling crab, Trapezia cymodoce (Red dotted coral crab)\nDecrease in crab density\nReduced fecundity (40% smaller clutch size)\nIncreased emigration and aggressive interactions (competition) Note: Figure from a reference Corallivorous filefish local population completely disappeared\nCorallivorous butterflyfish switched to more abundant coral prey Additional disturbances: disease, predation Caribbean (2005): disease outbreak = coral cover decline by 61% in following 2 years Glance at the future: if temperature continues to increase in frequency, reproduction will decline = reducing stability of populations Abrolhos Islands (2011): decrease in dispersal + coral followed by bleaching = decrease supply of larvae = mean mortality rate of 48.57% Doi:10.1007/s00338-011-0748-0 Abrolhos prior to 2011 bleaching Abrolhos after to 2011 bleaching http://www.advancedaquarist.com/blog/higher-latitude-corals-experience-bleaching-at-the-houtman-abrolhos-islands The Big Picture: Coral bleaching events cause significant host mortality and shift abundance and community composition of symbiotic organisms.\nWhile some corals may recover, negative impacts on populations of symbiotic organisms remain. Morphological changes in coral\nBranching corals replaced by bleaching- resistant corals which contain less structural complexity and support less diversity Acknowledgments We would like to thank the following:\nA.E. Douglas. 2003. Coral Reef Bleaching-How and Why?. Marine Pollution Bulletin, 46: 385-92\nA. Kushmaro, Y. Loya, M. Fine, and E. Rosenberg. 1996. Bacterial Infection and Coral Bleaching. Nature, 380: 396\nAndrew Chin, Thierry Lison de Loma, Katie Reytar, Serge Planes, Karin Gerhardt, Eric Clua, Lauretta Burke, and Clive Wilkinson. 2011. Status of Coral Reefs of the Pacific and Outlook: 2011. Global Coral Reef Monitoring Network, 1-260.\nCoral Reef Alliance. 2012. Coral Reef Overview. <http://www.coral.org/resources/about_coral_reefs/coral_overview>\nD. Abdo , L. Bellchambers, and S. Evans. 2012. Turning up the Heat: Increasing Temperature and Coral Bleaching at the High Latitude Coral Reefs of the Houtman Abrolhos Islands. PLOSone, 7: 1-11\nG. R. Almany, S. R. Connolly, D. D. Heath, J. D. Hogan, G. P. Jones, L. J. McCook, M. Mills, R. L. Pressey, and D. H. Williamson. 2009. Connectivity, biodiversity conservation and the design of marine reserve networks for coral reefs. Coral Reefs, 28: 339-351\nI. J. Dight, L. M. Scherl. 1997. The International Coral Reef Initiative (ICRI): Global priorities for the conservation and management of coral reefs and the need for partnerships. Coral Reefs, 16: S139-S147\nJ. S. Stella, P. L. Munday, G. P. Jones. 2011. Effects of coral bleaching on the obligate coral-dwelling crab\nTrapezia cymodoce. Coral Reefs, 30: 719-727\nMadeline J.H. Van Oppen and Ruth D. Gates. 2006. Conservation genetics and the resilience of reef-building corals. Molecular Ecology, 15: 3863-3883\nM. Hoogenboom, D. Campbell, E. Beraud, K. DeZeeuw, and C. Ferrier. 2012. Effects of Light, Food Availability and Temperature Stress on the Function of Photosystem II and Photosystem I of Coral Symbionts. PLOSone, 7: 1-14\nN. Parks. 2012. Sunscreen Cause Coral Bleaching. The Ecological Society of America, 12:39\nP.W. Glynn. 1993. Coral reef bleaching: ecological perspectives. Coral Reefs, 12: 1-17\nT.R. McClanahan, J.E. Cinner, J. Maina, N.A.J. Graham, T.M. Daw, S.M. Stead, A. Wamukota,\nK. Brown, M. Ateweberhan, V. Venus, & N.V.C. Polunin. 2008. Conservation action in a changing climate. Conversation Letters 1: 53-59\nT. R. McClanahan, M. Ateweberhan, C. Ruiz Sebastian, N. A. J. Graham, S. K. Wilson, J. H. Bruggemann, M. M. M. Guillaume. 2007. Predictability of coral bleaching from synoptic satellite and in situ temperature observations. Coral Reefs, 26: 695-701\nW. K. Fitt and M. E. Warner. 1995. Bleaching Patterns of Four Species of Caribbean Reef Corals. Biological Bulletin, 189: 298-307', ""Reef - The Whitsundays -\nThe Great Barrier Reef is said to be the\nlargest structure on earth ever created by living creatures.\nThe Great Barrier Reef World Heritage Area is 347 800 square\nkilometres in area ( an area bigger that the United Kingdom,\nHolland and Switzerland combined). It is the largest World\nHeritage Area and marine protected area in the world.\nThe reef contains over 2,900 reefs which includes 760 fringing\nreefs, and 300 coral cays.\nCoral - Great Barrier Reef\n|You can experience one the of the “7” wonders\nof the world with a Day Trip to one of the pontoons with underwater\nviewing platforms with optional Diving and Snorkelling or you\ncan experience the reef and islands on a Sailing and Diving\ncharter boat, “highly recommended”.\nThe Great Barrier Reef is the world’s largest and most complex expanse\nof living corals reefs supporting varied forms of marine life. As the world’s\nlargest coral reef ecosystem, the Great Barrier Reef is home to over 1500 species\nof fish, about 400 species of corals, 4,000 species of molluscs, 215 species\nof birds, 6 species of sea turtles and a host of sponges, anemones, worms, crustaceans,\nshells, sea stars, urchins…. Scuba diving and snorkelling are the most\npopular ways to experience the unique and beautiful underwater world of the Great\nMost commonly asked questions\nREEF FACTS AND FIGURES\nThe Great Barrier Reef Marine Park was\nestablished in 1975, and it is the world largest marine protected\narea in the world. It is approximately 348,700 square kilometres\nin area and approximately 2,300 kilometres long, running from just\nnorth of Bundaberg to the tip of the Cape York Peninsula. The reef\ncontains over 2,900 reefs which includes 760 fringing reefs, and\n300 coral cays. There are also 618 continental islands, which were\nonce part of the mainland. As the world's largest coral reef ecosystem\nthe Great Barrier Reef is home to approximately:- 1,500 species\nof fish 400 species of corals 4,000 species of molluscs 500 species\nof seaweed 215 species of birds 16 species of sea snake 6 species\nof sea turtle and some of the largest populations of dugong in\n||WHAT FISH IS THAT ?\nWith over 1500 species of fish on the reef the answer to this question is not\nan easy one. The use of identification books and underwater cards can be useful\nin identifying commonly encountered species. Body and mouth shape are often\nthe best key features in identifying the type of fish. The reef fish section\nof the marine biology manual will outline the features of the mostcommonly\nencoungered families of fish. Aim to learn the name of just one fish every\ntime you visit the reef, and you will quickly know the most commonly encountered\n|ARE WE GOING TO SEE ANY SHARKS ?\nIf you see a shark while visiting the reef, consider yourself very lucky as sharks\nare not frequently encountered by visitors. Of those which are seen the most\ncommonly encountered are the white tip reef and black tip reef sharks. Easily\nidentified by the white markings on the tips of the dorsal fins, they are often\nfound resting upon the sea floor. Like most sharks white tip's are extremely\ntimid and won't stay long around divers. Most sharks found on the reef are\nfish eaters and therefore pose no threat to visitors. Do not harass or block\noff a shark's exit as they may attack out of fear.\n|WHAT ABOUT STINGERS ?\nThe box jellyfish is found in the coastal waters of North Queensland during summer\nmonths (October to March). Visitors wishing to swim during this period should\nonly do so in protective swimming enclosures or wear protective clothing. The\nbox jellyfish is a coastal species and is not found out on the reef, but they\ncan sometimes be found around islands close to the mainland. Other stingers\nthat are sometimes encountered on the reef include the irukandji and blue bottle.\nBoth can cause a nasty sting, Vinegar can used on both box jellyfish and irukandji\nstings but not on blue bottle stings. For blue bottles use cold water and ice.\n|WHY ISN'T CORAL VERY COLOURFUL ?\nMost visitors to the reef comment that the coral isn't very colourful as they\nare used to seeing brightly coloured images in books and on television. Natural\nwhite light is made up of all the colours of the rainbow; underwater, these\ncolours are filtered at different depths with red and yellow disappearing first.\nThis gives the reef a predominantly blue/green appearance. Photographs and\nvideo are taken using lights to show the true colours of the reef. So the colours\nare there, it's just that you need white light to see them. This is why night\ndiving on the reef is so spectacular.\n|WHAT ARE CORALS ?\nCoral are made up of a thin layer of living animals called polyps, which secrete\na chalky, limestone skeleton as they grow. Coral colonies grow as the polyps\ndivide and multiply in a process known as budding. In addition to catching\nplanktonic prey with their tentacles corals also derive nourishment from simple\nsingle celled algae called zooxanthellae (pronounced zoo-zan-thelly). living\nwithin their tissues. Like all plants, zooxanthellae photosynthesize, producing\nnutrients from the suns energy which are used by the polyp for its own nutrition.\nCorals with zooxanthellae are able to lay down limestone skeletons up to three\ntimes faster than those corals without.\n|WHAT TYPE OF CORAL IS THAT ?\nTrying to identify particular species of coral is very difficult. What makes\nit so difficult is that one type of coral may appear as a branching form in\ncalm water and look like a plate coral in another area. In many cases it is\nthe environmental conditions, such as wave action, light levels and the amount\nof sediment in the water, that influence coral colony shape. The easiest way\nto identify corals is by their appearance * boulder * branching * plate * table\n* vase * bushy * solitary\nWHAT ABOUT CORAL SPAWNING ?\nEvery year over one third of the reef's 350 species of coral reproduce sexually\nduring a mass spawning event. The majority of inner reefs spawn around November\nwith the outer reefs spawning later in December. Spawning always takes place\nat night, and follows any time up to six days after the full moon. Eggs and sperm\nare released into the water where they eventually combine to form a free swimming\nplanktonic larval stage.\nWHY IS THE REEF SO FAR OFFSHORE ?\nMost of the Great Barrier Reef is located off the mainland of Queensland. Corals\nneed clear waters which are low in nutrients. They cannot tolerate freshwater\nor nutrients carried in the water run-off from the mainland. That is why the\nmost diverse and abundant corals grow offshore where the environmental conditions\nare more suitable.\nWHAT IS THE WATER CLARITY GOING TO BE\nThe clarity of water on the reef is determined by a combination of the amount\nof sediment and the amount of phytoplankton in the water. Sediment becomes suspended\ndue to increased water motion caused by tide changes, high winds and storms.\nPhytoplankton are the microscopic plants that drift around in the water. They\nare more numerous in areas where the nutrient levels of the water are higher\nparticularly around coastal reefs which receive nutrient rich runoff from the\n|ARE WE GOING TO SEE\nANY WHALES ?\nWhales are normally encountered during the winter months when they migrate up\nto the reef from Antarctic waters to mate and give birth. One of the most spectacular\nvisitors during this period is the Humpback. They are seen in the shallow coastal\nwaters of the Great Barrier Reef ranging from Hervey Bay to\nPort Douglas. Whale watching is conducted by a number of tourist operators through\nthese areas. The Minke is another species of whale seen during winter, particularly\naround the Ribbon Reef area. The smallest whales, the dolphins can be seen all\nyear round in most parts of the reef. * What about Crown-of-Thorns Starfish?\nThe cause of Crown of Thorn Starfish outbreaks is still the focus of a lot of\nresearch and debate. Increased nutrients from the mainland and effects due to\nEl Nino are all being investigated as is the possibility that it is a naturally\noccurring event. Crown of Thorns starfish may actually serve to maintain coral\ndiversity on the reef by feeding on the fast growing species, that if left unchecked,\ncould dominate the reef.\n|WHERE CAN WE GO FISHING ON THE REEF\nFishing is not allowed in green national park zones or pink preservation and\norange scientific zones. In other zones fishing is allowed subject to Queensland\nfisheries restrictions. Legal sizes, closed seasons and catch quotas also apply\nto a variety of fish and shellfish. The following animals are totally protected:\nwhales, porpoises, dolphins, dugong and turtles, clam, trumpet and helmet shells,\nfemale crabs all grouper and cod over 1.2 metres. What about the weather? In\ngeneral the average passenger is not so much concerned with the weather as they\nare with how it will influence their day at the reef. Therefore an answer should\nbe given in reference to their concerns eg.sea sickness, water clarity, and the\ncolour of the reef. Whats that slick? When good growth conditions exist, blooms\nof a simple floating algae called Trichodesmium are often confused with oil and\ncoral spawn slicks. Blooms can be easily identified by their rusty brown colour\nas they occur in wind rows along the surface of the water. Slicks of coral spawn\ngenerally do not last more than two days after coral spawning. Any oil spill\nshould be immeditely reported to the local maritime authority.\nWhitsunday Islands - Airlie Beach - Queensland""]"	['<urn:uuid:bc6d95de-1b2f-417e-a6a9-f53d435b39b2>', '<urn:uuid:eba993c1-fc2b-4020-8952-763be1f6f97c>']	factoid	direct	short-search-query	similar-to-document	comparison	novice	2025-05-12T10:19:05.071190	7	58	3023
5	what are key features smart factories need monitoring systems security flexibility benefits	Smart factory monitoring systems need several key characteristics: They must be lean to reduce resource consumption (CPU, memory, disk space), flexible to adapt to structural and functional changes, scalable to handle millions of components, ubiquitous to support distributed systems, and secure to protect monitored data. These features are essential because smart factories rely heavily on IT components and services for production processes.	"['[IOT] XMPP IoT SIG - Call for Participation\nconzon at ismb.it\nFri Dec 16 13:00:46 UTC 2016\nhere my considerations about the questions you have\n>> What are the IoT use cases you want to solve with XMPP?\nThis is the smart-factory use case that I propose:\nIndustry 4.0 (this term refers to a vision of a ""fourth\nindustrial revolution"") is a concept related to smart\nfactories, which allow individual customer requirements to\nbe met. For example, in these smart factories, dynamic\nbusiness and engineering processes enable last-minute\nchanges to production and deliver, to be more flexible in\nthe production and to react efficiently to failures.\nFurthermore, Industry 4.0 will address some other\nimportant challenges, like resource and energy efficiency.\nThe Industry 4.0 revolution will be driven by the\nincreased use of information and communications technology\nas part of production processes, through the integration\nof hardware and software resources. Smart factories will\nleverage concepts like virtualization and mobile\ncommunication, enabling the introduction of new forms of\nToday, physical and virtual worlds can converge, thanks to\nthe fact that powerful and autonomous microcomputers are\nbeing wirelessly networked with each other and with the\nInternet. Particurarly, the so called Cyber-Physical\nSystems (CPS)s are the main example of this integration.\nThey have three main features: computation, communication,\nand control. These systems are the basis of Industry 4.0,\ncommunicating wirelessly, but carrying out real, physical\nprocesses. Furthermore, innovations in Internet protocols\n(e.g. IPv6) provide a number of addresses, sufficent to\nenable universal addressing of smart objects, through the\nInternet. This allows to create the Internet of Things and\nServices (IoTS), for interconnection of resources,\nservices and people. In the next years, the IoTS will\nchange manufacturing fundamentally in terms of\norganization, technology and processes, introducing\napproaches like Service Oriented Architecture (SOA),\nvirtualization, cloud computing and machine-to-machine\n(M2M) communication. For this reason, major processes in\nproduction will directly depend on the availability and\nperformance of IT components and services.\nTechnology used in smart factory scenario needs to provide\nseveral characteristics: the monitoring must be lean, to\nreduce the footprint of monitoring in terms of resource\nconsumption, CPU, memory and disk space. Indeed, to\nmonitor an infrastructure where embedded systems and CPS\nare used on the shop floor, it is inadequate to use a\ntraditional, datacenter type of server. The monitoring\nneeds to be flexible, because needs to support the\ndynamics and volatility by providing the ability to adapt\nquickly to structural and functional changes. It must be\nscalable, because today the IT in production is both\nrather specialized and small, but, smart monitoring needs\nto scale up to millions of components to be monitored.\nMonitoring must be ubiquitous, because it needs to be\navailable to support the more distributed and pervasive\nnature of systems in the context of smart factories.\nFinally, it must be secure, to protect the data monitored\nby the systems.\nXMPP is a a technology that provides all the features\nlisted before: It allows lean monitoring because it\nenables the use of an unique protocool for different\ngoals: messaging, data exchange, control and management.\nThe solution is flexible, because its naming and\naddressability mechanisms are based on the use of JID,\nwhich are not tighted with the position. XMPP is scalable,\nbecause it can scale to thoousands of users, also using\nonly not expensive hardware. The protocol can be used to\nenable the communication also on embedded devices, which\ncan be used for ubiquitous applications. Finally, XMPP\nembeds natevily security mechanisms, like SSL and TLS for\nauthentication and crittography, which guarantees the\nprotection of the data exchanged and of the privacy of the\nusers. For this reason, XMPP can be used as the standard\nprotocol to enable Machine-to-Machine and Machine-to-Human\ncommunication, inside smart-factories.\nIn an industry 4.0 scenario, a set of sensors can be\nspread in a factory to monitor several environmental\nparameters like temperature, humidity, light and noise. To\nsimplify the deployment of this set of sensors the\nXEP-0347 can be used to safely install, configure, find\nand connect massive amounts of Things together. All these\nparameters measured may affect both the production process\n(for products, like perishable goods) and the comfort of\nthe work environment of the workers. In this scenario, the\nsensors spread can be connected (wireless) to\ntiny-computer (i.e Raspberry-Pi) to send the data\nmonitored using the XEP-0323. The data collected are used\nto monitor the status of the work environment. Thresholds\ncan be set to rise alarms in case some parameters exceed\nthe optimal values. Furthermore, using the XEP-0325, the\ncentral control system can control actuators to try to\nnormilize the status, turning on/off devices or changing\nthe temperature of the cooling/heating system. Since this\nsystem can affect the production process, it is important\nthat all the operations are executed in a secure way, in\norder to avoid to allow attackers to access to the data\nmonitored or worstly to keep control of the actuators.\nBeside the security provided by the mechanisms embedded in\nthe XMPP protocol, the XEP-0324 can be used to control the\nacces to the different devices in the factory, in order to\nlimit it, based on the role of people.\n>> What are the strengths of XMPP wrt. IoT?\nThe main reasons that can lead to choose XMPP as\ncommunication protocol for an IoT solution, in my opinion,\n- The fact that is a standard technology (standardized in\n2004 by IETF), free and open, with a large community of\ndevelopers, which continuously extend the protocol.\n- The support of the publish/subscribe paradigm, useful to\nbuild an event-driven architecture.\n- The possibility to use as clients, common chat clients\nused in every day life by the users (like Pidgin).\n- The possibility to unique name and address the entities\nusing XMPP accounts, not connected with the physical\n- The security mechanisms natively included in protocol\n(TLS for encryption and SASL for authentication), both for\nclient-server and server-to-server communication.\nRecently, recommendations have been made to ensure that\nall the connections (both client-to-server and\nserver-to-server) use encryption.\n- The possibility to have additional features typical of\nthe instant messaging protocols, like the presence\nmechanism, which can be used not only to know in real time\nthe availability of the connected entities, but also to\nhave some additional info on their status (for example, a\ndevice can be online, but not working).\n-The scalability guaranteed by the protocol:\n- Its “push” model of information transfer solves\nserious scaling problems associated with traditional\nHTTP-based polling approaches.\n- An XMPP network can be composed by an unlimited\nnumber of servers in order to balance the load of traffic\nand every XMPP server can scale to handle thousands of\nalso if installed in standard computers, without\nrequiring expensive hardware.\n- Finally, open source XMPP servers can be installed\nin the cloud, with also better scalability performances.\n- The level of scalability provided by the protocol\nis proved by the infrastructures that are currently using\nit, as messaging protocol, like:\n- Google Cloud Messaging (used with every\n- Google Cloud Printing\n- Intel Common Connectivity Framework\n- UPnP Forum\nCurrently, there is no another solution providing all\nthese features together.\n>> What are the weaknesses of XMPP wrt. IoT?\nI think what is generally considered weaknesses of xmpp\ncompared with other protocols, in the IoT scenario, are:\n- The fact that XMPP is not a protocol designed for IoT\nsolutions, but a messaging protocol adapted to thi\n- The fact that the protocol is based on the use of xml\n(with the overhead connected with this).\n- The amount of traffic generated by the presence\nmechanism, which makes it not adapt for system, where many\nresource-constrained devices have to communicate.\n>> How could the weaknesses be mitigated?\n>> How fit the existing IoT XEPs into this picture?\nI think that the current IoT XEPs are generally going in\nthe right direction to address the issues listed before.\nIn general, the introduction of a list of XEPs allow to\ndefine more easly XMPP as an ""IoT protocol"", defining\nstandard ways to do IoT typical functionalities, like\ndiscovery and provisioning. Furthermore, the introduction\nof the EXI XEP (XEP-0322) and of the friendship concept\nallow to mitigate the other two issues.\nFinally, another way to mitigate the XMPP weakeness is to\ndraft ""Interworking"" specifications (i.e. specifying how\nXMPP-IoT should be mapped to interact with other\nprotocols), to promote the use of XMPP in combination with\nother procotols, with different strenghts/weaknesses).\nOn Mon, 12 Dec 2016 21:22:58 +0100\nFlorian Schmaus <flo at geekplace.eu> wrote:\n> Hello everyone,\n> Rikard and I would like to start the second phase of the\n>IoT SIG and\n> invite you to participate in the IoT SIG, in order to\n> technical aspects of the IoT XMPP extensions protocols.\n> interested in XMPP in the Internet of Things is\n>welcomed. The rough\n> road-map is to collect IoT use cases and how they could\n>be solved using\n> XMPP in the first 2-3 meetings. After those virtual\n> is invited to meet face-to-face at the XMPP Standards\n> Summit, which will take place on the 2. and 3. February\n> Brussels, Belgium . The Summit is right before FOSDEM\n>2017  and a\n> great opportunity to discuss how the existing IoT XEPs\n> Rikard and I looking forward to see you there. :)\n> The regular IoT SIG meetings will take place every 14\n>days using Jitsi\n> Meet. I\'d like to see if we can schedule a first meeting\n> Christmas. Please use the following Doodle pool if you\n> http://doodle.com/poll/y5d6h62y68gp3yf4. Note that we\n>will close the\n> poll by the end of the week. If you can\'t make it before\n> don\'t worry: The next meeting will be in the second week\n> (2017-01-09 - 2017-01-13). Please use the following\n>doodle in for this\n> time-frame: http://doodle.com/poll/b58st8u5yw3ipges.\n> In order to prepare to the meeting, please try to answer\n> - What are the IoT use cases you want to solve with\n> - What are the strengths of XMPP wrt. IoT?\n> - What are the weaknesses of XMPP wrt. IoT?\n> - How could the weaknesses be mitigated?\n> - How fit the existing IoT XEPs into this picture?\n> 1: https://wiki.xmpp.org/web/Summit_21\n> 2: https://fosdem.org/2017/\nMore information about the IOT']"	['<urn:uuid:d395db50-3680-4861-9193-eb94b26cacb1>']	open-ended	direct	long-search-query	distant-from-document	single-doc	novice	2025-05-12T10:19:05.071190	12	62	1714
6	How can AI help with employee training and skills development?	AI machine learning algorithms can learn and recommend training opportunities, understanding employees and pushing relevant skills development resources. This leads to higher participation in learning programs, improved return on training investment, and a workforce with growing skills.	['(Editor’s Note: The following is an excerpt from “AI and the World of Work: Embracing the Promises and Realities,” a free white paper available on the Allegis Group website that tackles the rise of AI and its influence on workforce strategy and innovation.)\nMuch of the job of talent acquisition is about establishing a relationship between a company and an employee. Once the offer is made and the candidate accepts, a new journey begins — one that focuses on fulfilling an initial promise of value and growing it over time. It’s about employee engagement, performance management, skills development, and a host of related time- and resource-intensive functions. Moving forward, artificial intelligence (AI) will provide a significant advantage in helping companies to better understand and engage with their workers. AI’s impact will be felt across these key areas of employee experience.\n- Post-Offer Acceptance and New Hire Onboarding: Once a candidate accepts a job offer, an AI could engage and follow up with the candidate to accelerate acceptance and reduce the gap between hiring decision and start date. Following the hire, a candidate undergoes onboarding and orientation. While orientation introduces new hires to company culture, processes, and policies, an AI can fill in the gaps by answering common questions and providing information and resources. Considering that 90 percent of employees forget what was covered in a meeting, training, and conference calls, the information and learning reinforcement of AI can prove valuable.\n- Re-Engagement: Maintaining contact with past candidates can help an organization improve its future talent supply; unfortunately, candidates’ records often go untouched after the job requisition closes. AI can help address this issue by allowing an organization to re-engage a targeted group of candidates to determine their interest in a role while also using the engagement opportunity to keep abreast of new skills or experience the candidate may acquire. By automating this process, AI can remain in contact with past candidates and maintain a potentially valuable portion of the future talent pool.\n- Skills Development: Training can be a key to employee value and retention. Unfortunately, many organizations take a passive approach to administering their programs. A curriculum and resources may be available, but it is often up to the employee to decide whether to use those learning resources. Machine learning computer algorithms could “learn and recommend” when it comes to employee training, helping to understand the employee and push skills development opportunities in a way that is relevant and compelling. This improved communication can lead to higher participation in learning programs, improved return on training investment, and, most importantly, a workforce with growing skills.\n- Career Development: Employees have questions and need support. They may need customized training, learning, and career path information that a boss or leader can’t provide. Along with advanced training and skills development guidance, AI offers the ability to facilitate a more holistic career development, mentorship, and company coaching program.\n- Employee Relations: Some employee questions are simple (e.g., benefits, vacation, and pay) while others are complex and require an in-depth conversation with an HR manager or coordinator. AI technology can be used in chat form, email, or a virtual meeting room, answering many questions, understanding the issue, and, if needed, booking a meeting between HR generalists and employees. Automating this activity can significantly reduce a time-consuming communications burden and allow HR to focus on critical employee interactions.\n- HR Compliance and Case Management: Many case management software solutions provide employees with resources and information based on the questions they send to an email inbox. After that submission, if the resources and information are not clear or helpful, HR and legal resources may become involved in answering those questions. AI opens the door to more advanced functions where incidents are documented and employee investigations are submitted with an automated assistant tool. The tool would ask a series of questions and gather information when a complaint is registered, helping to reduce the need for expensive human interaction early in the case management process.\n- Attrition Mitigation: AI can quickly identify factors affecting high staff turnover. For example, a company’s current pay scale may match the market, but if it veers toward the lower end, the business risks losing talent to competitors. Historical data might also reveal who out of a group of candidates is most likely to remain in the position for the longest time.\nThese areas of workforce management represent a taste of the challenges and opportunities that AI is poised to address as new solutions enter the market. As with previous waves of innovation, an initial flurry of standalone, niche applications may be followed by increased adoption within larger solutions. If the history of enterprise systems, applicant tracking systems, recruitment marketing, and related technologies are any indication, the pace of change may vary, but the strategic value will continue to grow as AI applications begin to span the multiple functions of HR, from recruiting to compensation and performance management.\nTalent Implications, Ethics, and the Forces of Change\nThe examples of innovations so far show that most solutions are aimed at solving specific challenges in talent management, but on the whole, AI is not limited to a single function. As AI evolves to work across the silos of talent, the solutions will likely grow more impressive.\nImagine a recruiter speaking into a mobile device and saying, “Cortana, I need a data engineer with SQL skills.” What if the program asked more questions about the type of person needed for the job, then looked into its database, found the right person, scheduled an interview, facilitated the selection process, oversaw onboarding, and managed that person’s payroll, benefits, and skills development program? That’s a powerful capability, and it’s one that the larger players in the space are reaching to achieve. The vision is likely part of the reason Microsoft acquired LinkedIn.\nAlong with its powerful promise, AI also poses ethical questions as pointed out by an active player in the AI space, Shon Burton, CEO and founder of HiringSolved. “AI is good for pattern matching and prediction, but is it ethical to predict race, gender, honesty, intelligence, performance, reliability, or culture fit? ... Imagine how it might be to have to qualify, negotiate pay or communicate healthcare concerns to an AI-based system who all the while is analyzing your interactions with it and using that information to predict your suitability for the role or the lowest salary you are likely to accept.”\nWhile the concerns are real, they also highlight another fact about AI and talent. That is, HR depends on humans to do the most important parts of its function, interacting with candidates and employees, finding talent, determining strategy, and evolving with the business. Innovations in technology will not only help HR do its job better, but they will change what that job is. More than ever, a static, process-driven approach to talent will be replaced by a focus on strategic vision and agility.\nTalent Acquisition: Smart Technology, Efficient Processes, Enabled People\nAI’s Impact on Jobs: Reading the Indecipherable Tea Leaves\nSurvey Results: What HR Pros Think (Infographic)']	['<urn:uuid:3fc60e9b-6cbb-4047-9a3c-5d8172ca8d19>']	factoid	direct	concise-and-natural	similar-to-document	single-doc	expert	2025-05-12T10:19:05.071190	10	37	1181
7	How does frequency affect signal transmission in wireless networks?	Higher frequencies like 2100MHz or 2600MHz are more difficult to transmit through air and cable compared to lower frequencies like 900MHz. For wireless networks, modern 802.11ac routers can provide internet speeds up to 12 times faster than 802.11n routers. When installing repeater systems, higher frequencies require shorter cable lengths due to greater signal attenuation.	['For a home or office wireless network to function properly, it requires a variety of hardware components. Unless you are an IT expert, you may not be familiar with all of these components. The purpose of this write-up is to introduce you to the various hardware you need to create a wireless network. Here we go.\nLet’s Take a Look at the Hardware you Need for Wireless Networks\nWireless Network Adapters\nThe other names for the wireless network adapters include the wireless network cards or wireless NICs. They are required for older computers and laptops which do not incorporate wireless capabilities. If you have the newer smartphones, tablets, and laptops, you can be sure they already have wireless capability. For the older computers, buy add-on wireless adapters to be connected either via USB or PCMCIA. This will enable these computers to automatically connect to your wireless network.\nWithout a wireless router, there would probably be no network. Just line cable routers, these devices are the first to receive internet signals and then transmit to devices such as computers, laptops, smartphones, tablets, cameras, and so on.\nCurrently, the recommended wireless router standard is the 802.11ac, which allows for smooth online video streaming and responsive gaming. Even though a little slower, older routers are still up to the task. The good thing with an AC router is its incomparably fast speed. They can provide fast internet by more than 12 times the 802.11n routers.\nIf you want a router that’s capable of handling multiple devices, buy the AC router. Picture the many devices in your home. Most likely, you have desktop computers, laptops, tablets, smartphones, streaming boxes, smart TVs, smart speakers, smart cameras, and so on. You need a router with the capability to supply internet to all of your devices without breaking down.\nThe work of a wireless router is to tap an internet connection from the modem by a wire connection. It then creates a wireless network to which all of the other devices connect. It is, therefore, an integral part of your wireless network.\nWireless Access Points\nThe purpose of access points is to connect wireless networks to existing wired networks. If your home or office already has a wired router, you will need to have an access point. As such, you won’t need to replace your router with a wireless one.\nThe work of access points is to connect devices to a wireless network without the need for cables. With a wireless network, the number of devices can increase almost infinitely. What the access point does is to amplify the network. The wired router is the one providing the bandwidth while the access point only extends it to support a number of devices. It also extends the range of the network so that the devices can connect to it from further off.\nIn addition to amplifying and extending the network, an access point supplies key information on all the devices connected to the network. It also secures the network and ensures hackers don’t get easy access. When buying an access point, consider the standard on which it operates. Depending on the standard, access points can deliver varied bandwidth on varying frequencies. The number of channels supported also greatly differ.\nEven with the best routers and access points, your Wi-Fi signal may not reach as far as you would hope. That’s why you need a dedicated wireless antenna. Its purpose is to increase the range of the wireless radio signal. For the modern routers, the antennas are in-built. However, on older equipment, they are removable, since they are optional. Whatever the case, you can mount add-on antennas on the existing equipment to widen the range of the Wi-Fi signal. If you are building a home wireless network for your home, you may not require a wireless antenna. For a large office complex, the antenna is necessary to enable internet access from all locations.\nThe work of a wireless repeater is to extend the strength and reach of the network by connecting to an access point or router. A wireless repeater may also be referred to as a range expander or signal booster. It is basically a two-way station for relaying radio signals to connect equipment to the network. It makes it easier for far-off devices to also access the internet. If you have a large home with rooms that aren’t receiving the Wi-Fi signal, you definitely need a wireless expander.\nClearly, there are different kinds of hardware required to build a wireless network. For these hardware components to work properly, the right software must be installed. Also, you need an internet connection from a broadband provider. When you have all the necessary ingredients, there is just no reason you cannot have a home or office wireless network.', 'How do we find out precisely the maximum length of cable you can install between the outdoor antenna and the repeater?\nIn all the Stelladoradus repeater kits we provide a standard 15m length of cable to be installed between the outdoor antenna and the repeater. This is satisfactory for most customers.\nHowever, sometimes it would be useful to install a longer length of cable instead. Perhaps the roof is far away from the area where you would like mobile coverage, and you need to install a long cable. In many professional installations, this is the case.\nNow, we have a method with which you can use to calculate this maximum length more accurately. In many cases, you will find this length to be much greater then 15m.\nThe key tool you will need is an i-Repeater. The i-Repeater has an LCD screen that will tell you the “total loss” figure in dB’s for each frequency. Once you know this, it is easy to calculate maximum cable length.\nGo to the roof with a yagi antenna, an i-repeater, and a power source for the i-repeater.\nPoint the Yagi at the nearest suitable tower.\nPower on the i-repeater and attach the Yagi to the outdoor antenna port.\nOn the LCD, press “menu” and Click on “dB page”. Take a note of the “Total loss” values for each frequency.\nAnalyzing the data:\nThese “total loss ” values are the amount of attenuation the repeater is applying to the signal due to the signal being too strong.\nYou should focus on the highest frequency that you intend to provide in this installation\nIf you are installing an LG repeater (800MHz, 900MHz ), then you only need to focus on 900MHz.\nIf you are installing a GDW system (900,1800,2100mhz), then you will need to focus on 2100MHz.\nRULE: it is more difficult to get the higher frequencies to travel through air and cable.\n2100MHz will be more attenuated in cable than 900MHz, so designing a system that incorporates 2100MHz (or 2600MHz) is going to be more challenging, and you will find your maximum cable length will need to be shorter.\nLooking at the example in the image above, we see a total loss dB of 16dB at 2100MHz.\nThis is a large value for 2100MHz and would be typically be the case in urban areas.\nAs we intend to install a GDW system (W=2100MHz), we need to focus on 2100MHz.\nRULE: There is a loss of 0.2dB per meter at 2100MHz in SD400 cable(standard cable we supply).\nSo we divide our Total loss dB figure by 0.2 to get our maximum cable length: 16 / 0.2 = 80\nSo, we can use an 80m length of cable between the outdoor antenna and the repeater.\n80m is far better then the specified 15m. This is very useful if you need to get the signal deep into a building. (You will probably not need a pre-amp)\nWhat if the db loss value is zero?\nTheoretically, in this case, we should have a zero length piece of cable between the Yagi and the repeater. However, this is not practical, so we have to accept some loss.\nThis is why we specify a 15m cable in our repeater kits, so as to ensure we do not have too much loss in the case of a weak mobile signal outside on the roof of the building.\nWe have a new online repeater design tool. To use this online tool, email [email protected] and we set you up with an account.']	['<urn:uuid:b836f70f-dad9-4c22-8887-301b3b95fddb>', '<urn:uuid:bdfbc908-f327-4547-a725-3049fffab98c>']	factoid	with-premise	concise-and-natural	similar-to-document	three-doc	expert	2025-05-12T10:19:05.071190	9	54	1388
8	Can you compare storage properties of normal saline and lithium batteries?	Normal saline is a simple isotonic solution that requires no special storage conditions, while lithium batteries have complex storage requirements due to their chemical composition - particularly their electrolytes which contain potentially toxic organic solvents like DMC and lithium salts that can hydrolyze into poisonous HF when exposed to water.	"[""normal saline solution\nAlso found in: Dictionary, Thesaurus, Legal, Acronyms, Encyclopedia.\nagreeing with the regular and established type. When said of a solution, it denotes one containing one chemical equivalent of solute per liter of solution; e.g., a 0.5 normal (0.5 N) solution has a concentration of 0.5 Eq/l. The use of standard units (Eq/l) is now preferred.\nnormal saline solution (normal salt solution) physiologic salt solution.\nnormal saline solution\nEtymology: L, norma, rule, sal + solutus, dissolved\na 0.9% w/v (grams of solute per milliliter of solution) sterile solution of sodium chloride in water that is isotonic with blood and injectable intravenously.\nsolution(so-loo'shon) [L. solvere, to loosen, dissolve]\n1. A liquid containing a dissolved substance.\n2. The process by which a solid is homogeneously mixed with a liquid, solid, or gas so that the dissolved substances cannot be distinguished from the resultant fluid.\n3. A mixture formed by dissolution of substances.\nThe liquid in which the substances are dissolved is called the solvent and the substance dissolved, the solute.\nA solution containing water as the solvent.\n1. Isotonic solution.\n2. A solution whose concentrations are matched physiologically to the part of the body in which it will be infused or used for irrigation.See: isotonic solution\nBenedict solutionSee: Benedict solution\nA solution of a weak acid and its salt, e.g., carbonic acid, sodium bicarbonate, important in maintaining a constant pH, esp. of the blood.\nBurow solutionSee: Burow solution\nA solution, usually combined with dextrose and other agents, to prevent blood clotting. It allows whole blood to be stored until it is needed for transfusion.\ncobra venom solution\nA sterile physiological salt solution containing minute quantities of cobra venom.\nA solution in which the solute is suspended, not dissolved, such as gelatin or albumin.\nDakin solutionSee: Dakin solution\nFehling solutionSee: Fehling solution\nHartmann solutionSee: Hartmann solution\nheparin lock flush solution\nA solution of unfractionated heparins formerly used to keep intravenous infusion devices from clotting. Heparin flushes are now seldom used because they are more expensive than saline flushes and pose a risk of heparin-related thrombocytopenia, a potentially life-threatening allergy.\nhistidine-tryptophan-ketoglutarate solutionAbbreviation: HTK solution\nA preservative to protect a harvested organ before its transplantation into a donor. It is typically infused into the donor organ before the organ is removed from the body and then used to bathe the organ while it is kept in storage at 4° C before implantation into the recipient.\nA solution with a specific gravity and an osmotic pressure greater than one, or greater than the solution to which it is being compared. It is important in injecting medicines or anesthetic agents into the spinal fluid in the spinal canal.See: hyperbaric chamber\nA solution having a greater osmotic pressure than that of cells or body fluids; a solution that draws water out of cells, thus inducing plasmolysis.\nA solution having an osmotic pressure less than that of cells or body fluids; a solution that will cause water to enter cells, thus inducing swelling and possibly lysis.\nA solution of iodine or potassium iodine used as a source of iodine.\nAny fluid used to rinse an organ or body cavity. SEE: irrigationSee: irrigation.\nA solution with a specific gravity equal to one or equal to the solution with which it is being compared.See: hyperbaric solution\nA solution having the same hydrogen ion concentration or pH as another.\nA solution with the same osmotic pressure as the solution with which it is being compared.\nA solution that has a concentration of electrolytes, nonelectrolytes, or both that will exert osmotic pressure equivalent to that of the solution with which it is being compared. Either 0.16 molar sodium chloride solution (approx. 0.95% salt in water) or 0.3 molar nonelectrolyte solution is approx. isotonic with human red blood cells. Synonym: balanced solution\nJessner solutionSee: Jessner solution\nlactated Ringer solutionSee: Ringer, Sydney\nAn aqueous solution of nonvolatile substances formerly used to prepare medicines.\nLocke-Ringer solutionSee: Locke solution\nLugol solutionSee: Lugol solution\nmolar solution1 M\nA solution containing a gram molecular weight or mole of the reagent dissolved in 1 L of solution.\nAn obsolete term for a solution in which 1 L contains 1 g equivalent of the solute. This term is discouraged in the SI system.\nnormal saline solution\nAn isotonic saline solution.Synonym: physiological saline solution See: isotonic solution\nA sterile preparation suitable for instillation in the eye.\noral rehydration solutionAbbreviation: ORS\nA solution used to prevent or correct dehydration due to diarrheal illnesses. The World Health Organization recommends that the solution contain 3.5 g sodium chloride; 2.9 g potassium chloride; 2.9 g trisodium citrate; and 1.5 g glucose dissolved in each liter of drinking water.\nphysiological saline solutionNormal saline solution.\npolyethylene glycol electrolyte for gastrointestinal lavage solution\nA solution for cleansing the bowel before colonoscopy and barium enema examinations. It is an isosmotic solution for oral administration, containing 236 g of polyethylene glycol 3350; 23.74 g of sodium sulfate; 6.74 g of sodium bicarbonate; 5.86 g of sodium chloride; and 2.97 g of potassium chloride added to water to make up a 4-L solution. For adults 4 L of the solution are given at the rate of 8 oz (240 mL) every 10 min until exhausted. The bowel will be cleansed within 3 to 4 hr.\npotassium arsenite solution\nAn arsenical solution containing 0.95 to 1.5 g of arsenic trioxide for each 100 ml of solution.\nAny solution given intravenously to treat an electrolyte or metabolic disturbance.\nA liquid given to a patient to increase concentrations of specific electrolytes or minerals. It is usually given intravenously, orally, enterally, or interosseously.\nRinger solutionSee: Ringer, Sydney\nA solution of a salt, usually sodium chloride, and distilled water. A 0.9% solution of sodium chloride is considered isotonic to the body. A normal saline solution consists of 0.85% salt solution, which is necessary to maintain osmotic pressure and the stimulation and regulation of muscular activity.\nA solution containing all the solute it can dissolve. See: saturation point\nA colloquial term for a skin cleanser for the removal of debris, dirt, microorganisms, oils, and scales from the skin of a patient before incision or instrumentation.\nseminormal solutionAbbreviation: 05N or N/2\nA solution containing one-half of a gram equivalent weight of reagent in 1 L of solution.\nsodium iodide I 125 solution\nA standardized solution of radioactive iodide, 125I.\nIn comparison or analysis, a solution containing a definite amount of a substance.\nstrong ammonia solution\nA solution containing approx. 28% ammonia in water.\nA solution in which the saturation point is reached but when it is heated it is possible to dissolve more of the solute. See: saturation point\nA dissolved reagent used for a specific laboratory purpose.\ntricitrates oral solution\nA solution of sodium citrate, potassium citrate, and citric acid in a suitable aqueous medium. The sodium and potassium ion contents of the solution are approx. 1 mEq/ml.\nTyrode solutionSee: Tyrode solution\nVleminckx solutionSee: Vleminckx solution\nA standard solution containing a definite amount of a substance in 1 L of solution; used in volumetric analysis."", ""Pros and cons among different lithium batteries\nNot only smartphones and laptops, but even bicycles and cars, all kinds of tools we use in our daily life are powered by electricity. Improving battery performance has important implications for improving the ease of use of these devices. Among them, lithium batteries have attracted the attention of the industry in recent years.\nHere we talking about the classification and application of different lithium batteries\n1. What is a lithium battery\nStrictly speaking, lithium batteries are divided into two types: lithium metal batteries and lithium ion batteries. This is defined according to the form in which lithium exists.\nLithium metal batteries use metal lithium as the electrode, while lithium ion batteries exist in the electrode in the form of ions. Lithium metal batteries generate electricity through the corrosion or oxidation of metal lithium, and they are useless when they are used up and cannot be charged, so they are also called primary batteries.\nLithium-ion batteries are a type of rechargeable batteries that use graphite or other carbon materials as the negative electrode and lithium-containing compounds as the positive electrode, so they are also called lithium secondary batteries. It is a type of battery that uses lithium metal or lithium alloy as the positive/negative electrode material and uses a non-aqueous electrolyte solution.\n2. The classification and characteristics of lithium-ion batteries\nAccording to the different metal materials used in the positive electrode, lithium-ion batteries are divided into several types. The metal material used in the positive electrode of the original lithium-ion battery is cobalt. However, the output of cobalt is almost as small as lithium, and it is also a rare metal with high manufacturing costs. Therefore, cheap and environmentally friendly materials such as metals such as manganese, nickel, and iron have been used.\nLithium-ion batteries are classified according to the materials they use. Let's take a look at the characteristics of each type.\nCobalt lithium-ion battery\nUse Lithium cobaltate for the positive electrode. Lithium cobaltate is relatively easy to synthesize and easy to use, so the earliest mass production of lithium-ion batteries is lithium-cobaltate batteries. But because cobalt is a rare metal and expensive, it is hardly used in auto parts.\nManganese lithium-ion battery\nUse lithium manganate for the positive electrode. The advantage is that the voltage can be similar to that of cobalt lithium-ion batteries, and the manufacturing cost is cheap. The disadvantage is that manganese may melt into the electrolyte during charging and discharging, shortening the life of the battery.\nIron phosphate lithium-ion battery\nUse lithium iron phosphate for the positive electrode. The advantage of the iron phosphate lithium-ion battery is that it is difficult to damage the internal heating structure, has high safety, and uses iron as the raw material, so the manufacturing cost is lower than that of the manganese lithium-ion battery. But the voltage is lower than other Lithium-ion batteries.\nTernary lithium-ion battery\nThe ternary lithium-ion battery is a battery made of three materials, cobalt, nickel, and manganese, in order to reduce the amount of cobalt used. At present, most of the ternary lithium-ion batteries have a high proportion of nickel. Although the voltage is slightly lower than that of cobalt-based and manganese-based, it can reduce manufacturing costs. However, despite this, the synthesis and preparation of each material is difficult, and the stability is low, and there are still problems to be solved as practical materials.\n3. Lithium battery pollution\nAccording to the composition of the lithium battery, lithium iron phosphate in the positive electrode material does not contain heavy metals and basically has no pollution, while lithium manganese, ternary lithium, and lithium cobalt have heavy metal elements (manganese, cobalt, nickel) and have heavy metal pollution. Compared with lead-acid and nickel-chromium batteries, lithium iron phosphate battery’s pollution is very small, but for the environment, chemicals are still harmful.\nThe electrolyte is composed of organic solvents and lithium salts. Organic solvents generally include PC, EC, DEC, DMC, DME, etc., among which DMC is slightly toxic, and others are non-toxic. The lithium salt is lithium hexafluorophosphate (the most used, and there are other lithium salts), it will be hydrolyzed into HF when it meets water, which is poisonous. The shell, diaphragm adhesive will cause white pollution, and the rest of the battery materials can be recycled without pollution.\nWe don't need to be careful about environmental pollution when we use batteries normally, but we need to pay attention to recycling when used batteries are scrapped to prevent the leakage of battery materials from causing pollution and burden to the environment.\n4. Working performance comparison(cell)\n5. Lithium battery application\nLithium cobalt : Because of its excellent energy density, it has a smaller weight and volume under the same energy and is used as a small energy battery, becoming a popular choice for mobile phones, notebook computers and digital cameras.\nLithium manganate: Although the overall performance is average, industrial applications require a battery system with good load capacity, long life, and safety. It’s commonly used in power tools, medical equipment, electric transmission systems, etc.\nLithium iron phosphate: It has good safety and long life, moderate energy density and strong discharge capacity, and is used as power lithium battery and energy storage battery.\nTernary lithium: It has high energy density and low temperature resistance, the biggest application is in new energy vehicles, such as Tesla's ternary lithium power battery.\nAbove we talked about the five aspects of lithium batteries, each of which has its own unique advantages and has a very wide range of applications in individual fields.\nOf course, now and in the future, different lithium batteries will appear and be applied in more fields, providing continuous development for the new energy industry.""]"	['<urn:uuid:a05a1697-c123-40b3-877c-290af020aa49>', '<urn:uuid:62b575df-76d9-4e6e-905f-3a3cb5b98153>']	factoid	with-premise	concise-and-natural	distant-from-document	multi-aspect	expert	2025-05-12T10:19:05.071190	11	50	2119
9	Who fought to get public access to Montana's rivers and streams?	A passionate group of men from the Butte area, led by Tom Bugni and Jerry Manley, along with Tony Schoonen, formed the Montana Coalition for Stream Access in 1980. Using their own money, they filed lawsuits to fight for public access rights. They won important cases against Dennis Curran and Lowell Hildreth, which established the public's right to float, fish, and recreate between the high-water marks of Montana's waters.	['There’s some misinformation floating around this election cycle, some candidate co-opting of the stream access issue, as well as some general lack of understanding of how Montana achieved some of our recreational access. We didn’t have it handed to us on a silver platter, it was fought for and at times paid for by some key individuals who were passionate about access. We thought it a good time to shine a light on some truths, otherwise, we could be looking at an “Access” Ground Hogs Day in our future. Let’s break this down to bite size pieces.\nSince 1933, Montana Code Annotated (MCA, law) recognized the public’s right to use the water and banks up to the “high water flow line” of “navigable” rivers, streams, and sloughs for fishing.\nStream Access and the Public Trust Doctrine even entered into Montana’s 1972 Constitutional Convention discussions, adopting two proposed constitutional provisions involving a clean and healthful environment, now a part of the ratified Constitution by the people of Montana. One became an inalienable right – Article 2, Sec. 3, “All persons are born free and have certain inalienable rights. They include the right to a clean and healthful environment and the rights of pursuing life’s basic necessities, enjoying and defending their lives and liberties, acquiring, possessing and protecting property, and seeking their safety, health and happiness in all lawful ways. In enjoying these rights, all persons recognize corresponding responsibilities.” The 2nd, a duty, Article 9, Sec. 1(1), “The state and EACH PERSON shall maintain and improve a clean and healthful environment in Montana for present and future generations.”\nThat’s right, you not only have a right, but as a responsible citizen, you have a duty to “maintain and improve a clean and healthful environment in Montana for present and future generations.” So think about that when you vote in this and future elections – what elected officials will best represent you in achieving this objective?\nAn access visionary legislator, and an attorney, Cedor B. Aronow from Shelby, MT, served from 1949-1953. Aronow was voted the temporary president of the 1972 Constitutional Convention, and authored the amendment “for the use of it’s people.” In the discussion of the amendment, Aronow was questioned as to a private landowner fencing off public access so that a person couldn’t fish, boat or go up and down the river? Aronow replied, “No, that is not the sense of my remarks. You can go up and down that stream all you want to. But the only thing is, you can’t drive across the rancher’s lands willy-nilly in order to get to it. You can go along the county roads or wherever there’s access. And you certainly may boat. You may hike up and down that stream.”\nIn the late 70’s, Dennis Curran and his oil company harassed and interfered with the public’s recreational use of the Dearborn River. So on April 14, 1980, a passionate group of men from the Butte area led by Tom Bugni and Jerry Manley, joined by Tony Schoonen (beginning with their own money) and the Skyline Sportsmen Assoc., formed the Montana Coalition for Stream Access, Inc. They decided to fight back. MCSAI filed a lawsuit in Montana’s First Judicial District Court – MCSAI vs. Curran, asserting the public had a right to float, fish and recreate between the high-water marks. Fish, Wildlife & Parks and the State of Montana joined as plaintiffs. The District Court decision was that members of the public have the right to float and recreate in non-navigable streams, that the public may wade and use the banks up to the ordinary high-water, and that these rights are founded in statutory language. That passionate group of hunting/angling conservationists won.\nOn March 31, 1980, a number of the same hunters and anglers also began addressing our public access to state lands. Tony Schoonen, Jack Atcheson Sr. and Jack Jones formed and filed the Montana Coalition for Access on State Public Lands, Inc., out of their own pockets. The name would later be changed to Montana Coalition For Appropriate Management of State Lands, Inc. They led the charge for our state lands access, but this is a whole other beautifully detailed story.\nAbout a year later they took on Lowell Hildreth, who had installed a dangerous fence across the stream and was preparing to install a cable across the river further upstream, and was also harassing and interfering with the public. The District Court held, “The Beaverhead River, where it runs through the property of the Defendant, is navigable under the pleasure-boat test of navigability, and as such, members of the public have the right to float the river and use its banks up to the ordinary high water mark free from interference from the Defendant.” The Court also held that the Public could portage around obstacles. Again, the Coalition won for the Public.\nMONTANA SUPREME COURT\nBoth cases were appealed and ended up in the Montana Supreme Court, which issued their decisions in the summer of 1984. Concerning the Curran case, the Court affirmed the lower District Court’s ruling, “In sum, we hold that, under the public trust doctrine and the 1972 Montana Constitution, any surface waters that are capable of recreational use may be so used by the public without regard to streambed ownership or navigability for recreational purposes.” Additionally, “states hold title to navigable waterways in trust for public benefit…all the waters of the state are owned by the state and are held in trust for the people.”\nThis is very important. See the States, as sovereigns, were given title to the beds of navigable waters – to be held in trust for future states under the Equal Footing Doctrine, conferred by the United States Constitution. This is kind of like the bogus argument of the Sagebrush Rebellion stating they want the western State’s land back from the Fed. It was never theirs to begin with. Back to the waters though, don’t be confused, Montana has Class 1 waters and Class 2. Class 1 waters are navigable, Class 2 are non-navigable. But, regardless of title, Montana Stream Access Recreation Is Not Dependent On Title! Don’t be sucked into that landownership argument.\nA little over a month later the Montana Supreme Court, in the Hildreth case, reinforced Curran and stated, “the capability of the use of the waters for recreational purposes determines whether the waters can be so used. The Montana Constitution clearly provides that the state owns the waters for the benefit of its people. The Constitution does not limit the waters’ use. Consequently, this Court cannot limit their use by inventing some restrictive test.”\nThe Court further addressed the public’s right to bed and bank, “Under the 1972 Constitution, the only possible limitation of use can be the characteristics of the waters themselves. Therefore no owner of property adjacent to state-owned waters has the right to control the use of those waters as they flow through his property. The public has the right to use the waters and the bed and banks up to the ordinary high water mark.” Hildreth’s claim for inverse condemnation was dismissed, “public use of the waters and bed and banks of the Beverhead up to the ordinary high water mark was determined, not title… Hildreth has never owned and does not now own the waters of the Beaverhead River… he was not deprived of a property right by the district court.”\nIn 1983 the Legislature sought to address stream access issues. Four bills were introduced, with only one passing, HJR 36, a bill to study the rights of the public to access and use public lands and waterways. One of the failed bills, HB 799, would have violated the US Enabling Act, the Montana Constitution and our Public Trust Doctrine by transferring title of the river/stream beds to private riparian landowners. Sound like Ground Hogs Day? With the Supreme Court’s rulings in 1984, this threw the study in a quandary with the legal and policy landscape changed radically. The Legislature was advised, that since the Court based the decision on the Public Trust Doctrine, “the Legislature cannot substantially modify the result of those decisions. Had the court based its decision on narrower grounds (e.g., statutory grounds), the Legislature would have been able to modify the results of the decisions by changing statutes.”\nSenator Jack Galt, a vocal opponent to Stream Access, asked if prohibiting use of river beds would be legal and was advised, “… the right to use the bed is so fundamentally related to the public’s interest in the water that even the legislature cannot take it away.”\nWHO REALLY VOTED FOR STREAM ACCESS?\nNow comes another fallacy of Stream Access, that it is “us against them, the haves versus the have nots”. Opponents of Stream Access continually try to make it a landowner or agriculture versus recreationist issue; it is not, it is a Public Trust Doctrine issue that benefits all.\nIn 1985, 11 access bills were introduced. The Legislature passed HB 265, sponsored by Bob Ream, which was created and promoted by a coalition of groups representing landowners, agriculture interests, recreationists and Fish, Wildlife & Parks. The bill stated Class 1 and Class 2 waters could be used by the public, “… without regard to the ownership of the land underlying the waters”, for all the defined recreational uses the waters are capable of, including the use of the beds and banks up to the ordinary high-water marks.\nSome of the groups supporting HB 265 were The Montana Stockgrowers Assoc., Montana Wool Growers Assoc., Montana State Assoc. of State Grazing Districts, Montana Cowbells, Montana Farmers Union, Montana Cattlemen’s Assoc., Montana Cattle Feeders Assoc., Montana Farm Bureau Federation, and the Montana Water Development Association. The discussions with sportsmen were said to be, “… a product of cooperation between significant Montana interest groups.” Senator Galt again tried to interfere by proposing an amendment to remove from the definition of “surface water” the part that allowed recreationist to use the bed and bank of a stream, including navigable rivers. This amendment was removed and HB 265 became our Stream Access Law on April 19, 1985.\nOne day before HB 265 became law, to address the ongoing issue of the public losing legal access to our federal public lands, retired Supervisor of the Gallatin National Forest, Gene Hawkes, led individual foresters to form and file for the Public Lands Access Association, Inc. PLAAI later changed their name to include Montana’s stream access issues to Public Land/Water Access Association. The purpose of the organization was to inform the public about the deteriorating access situation and to take the lead in regaining the right to public access to all the public lands within the state. Many of the Montana Coalition For Stream Access members became PLAAI members, carrying on the access work under this new flag.\nGALT LAWSUIT AGAINST STREAM ACCESS\nNearly 2 months after HB 265 became law, Sen. Jack Galt, his wife and 8 other landowners, including Hildreth, filed a lawsuit against the State Fish, Wildlife & Parks and the State of Montana, in the 1st Judicial Court, on June 14, 1985. Galt v. State challenged the constitutionality of the law. The complaint stated our Stream Access Law was a taking and asked it be declared, “illegal, unconstitutional and void.” The district court denied HB 265 was unconstitutional and upheld the Curran and Hildreth decisions. Galt appealed to the MT Supreme Court, whose decision was a mix. The Court upheld the Law, but decided some limitations on the bed and bank while recreating – no big game hunting, no camping, no temporary duck blinds, or boat moorage, and landowners did not have to pay for construction of portage access.\nTHE ACCESS ASSAULT CONTINUES\nThat was not the end of the Stream Access debate. Attempts through the legislature have continued to chip away at or overturn our public access in one form or another. These newer attempts focused on access from bridges. PLWA was critical in defending the public’s access.\nMeanwhile, the expansion of police powers and rule making authority over the Public’s access and recreational use of Montana’s waters have been increasingly handed to our Fish, Wildlife & Parks Commission over the years. The Commission can consider petitions from persons asking the Commission to limit public recreational use for a variety of reasons, such as protecting the ecology of the stream, prevent damage to property or limit use of Class 2 streams to the actual capacity of the stream.\nThe Smith, Beaverhead and Big Hole are examples. A current example of this Commission authority was the recent petition in April 2016 by a Big Timber landowner and outfitter on the Boulder River where he outfits. He protested the use by the public, citing it was a conflict, asking the Commission, “limit the number of boats floating the Boulder through one of the many options available to the Commission.” The FWP analysis said that most of the issues on the river involved private landowners “who have had relatively exclusive use of the river.” FWP found “No degradation of the fishery in the Boulder River has been documented and we have no objective indication otherwise that the fishery itself is in jeopardy.” The Commissioners voted to deny this particular petition.\nIn 1988, the Montana Coalition for Appropriate Management of State Lands, Inc. filed a lawsuit against the State for hunting and fishing access to State lands (the other detailed story).\nIncreasingly wealthy out of state landowners picked up the public land and water access torch, and with deep pockets, have continued this legal battle for decades, despite losing the court battles. One such landowner has been James Cox Kennedy, involving the Seyler Lane bridge access on the Ruby River. On June 2, 2000, Attorney General Mazurek issued an opinion, which functioned as law until it was overturned in court or replaced by legislation (2009 HB 190 Bridge Access Law sponsored by Rep. Kendall Van Dyk), recognizing access at bridges and the public’s constitutional right to use all steams and rivers capable of recreational use. Mazurek reasoned that using a public right-of-way road to access a stream or river, which is another public right-of-way, is consistent with and reasonable to the public’s right to travel on county roads.\nOUT OF STATE INTERESTS\nIn May 2001, an out of state legal organization based in Colorado, the Mountain States Legal Foundation, with an interesting history and agenda (MSLF’s director ran for Montana’s governor in 2003. Can you imagine what Montana’s public access would be like if he won?), picked up the anti-stream access torch in Montana, filing a lawsuit in federal court, on behalf of a handful of landowners against FWP Director, Graham; MT FWP and the FWP Commission. Madison v. Graham sought to have “Montana’s Stream Access Law declared unconstitutional on the grounds that it (1) violated their Fourteenth Amendment substantive due process rights, and (2) was void for vagueness.” The district court listed a variety of reasons why the case must be dismissed, including the complaint for failure to state a claim upon which relief can be granted and statute of limitations. The court reviewed the major federal and state constitutional challenges to the Stream Access Law and found the laws to be constitutional, minus the provisions the MT Supreme Court already addressed in Galt. The plaintiffs’ complaint was dismissed with prejudice. On appeal, the Ninth Circuit Court affirmed the district court’s decision and disposed of the case.\nMontana’s Stream Access Law has withstood challenges to it’s constitutionality in both state and federal courts that Stream Access is not a taking of private property.\nPUBLIC POLICY REPORT\nIn May of 2006, the University of Montana, Public Policy Research Institute published a report – Stream Access in Montana, covering the 20 years since the 1985 Stream Access Law. “Twenty years after passage of Montana’s Stream Access Law, people are still talking—and, in some cases, arguing—about its provisions. This policy report aims to illuminate the unresolved issues and misunderstandings regarding the law, and to lay out options for moving\nThe findings of their interviews were, “Most of the people we talked with—recreationists and landowners alike—said that the Stream Access Law works well and has been very successful, as evidenced by the hundreds of thousands of anglers, boaters, and other recreationists using Montana streams each year with few if any conflicts with landowners. Several people also said that enactment of the law did not dramatically change people’s behavior—there has been no stampede of anglers and boaters and no avalanche of disputes.”\nStream Access Law expressly protects private property rights while providing for recreational\nuse of an established public resource. Especially during this election, let’s not forget the Public Trust champions that fought for our rights, our awesome stream access history and the duty that we, as individuals, have to “maintain and improve a clean and healthful environment in Montana for present and future generations.”\nDON”T JUST BE AN ACCESS USER, BE AN ACCESS SUPPORTER!']	['<urn:uuid:25f4c4f6-a2a8-4e54-82bd-8efb1bf533ee>']	open-ended	with-premise	concise-and-natural	similar-to-document	single-doc	novice	2025-05-12T10:19:05.071190	11	69	2835
10	construction industry types safety monitoring methods	The construction industry encompasses various types like commercial construction (including skyscrapers, bridges, and highways) and private construction projects (self-funded work done by private companies). Regarding safety monitoring, traditional methods involved physical inspections, but now drones are revolutionizing safety monitoring by performing quick aerial inspections, keeping workers away from dangerous situations. Drones can create 2D and 3D maps, collect elevation data, and allow real-time viewing of construction progress from remote locations, making safety monitoring more efficient and comprehensive.	['Building and construction is a broad term indicating the scientific research and art of to form valuable systems, things, or organisms, and comes from Latin ad infinitum and Old French construction. To construct essentially implies to make or create: to construct is the verb, as well as words is building and construction: exactly how a point is built. There are lots of forms of building, such as geometric, structural, building, as well as creative building and construction, as well as others, such as engineering and thermodynamics construction. There are also various kinds of building and construction, such as power-to-mass production, which refers to the production of one-way or self-supporting energy, such as steam wind turbines; as well as armed forces building, such as fortifications. It can be made use of to explain the construction of anything, including cities, skyscrapers, bridges, high-rise buildings, aircraft, spacecrafts, personal or public structures, irreversible frameworks (such as health centers), as well as even the motion of the planet on its axis.\nOne of the most typical kind of building is commercial construction, which includes every little thing from sky scrapes, high-rises, bridges, passages, parking lot, buildings created organizations, freeways, streets, freeways, bus stations, toll plazas, park frameworks, as well as maintaining wall surfaces made use of for civil/park/historical structures. A few of these are non-combustible. Air-conditioning towers as well as water tanks, as an example, can not shed gas and are not vulnerable to fire. Various other sorts of building, such as those that include steel in enhanced concrete, fireproof materials, strengthened steel, enhanced concrete, as well as warm liquified steel, are not fire resistant, yet are typically much more pricey to construct and extra unsafe to workers.\nIndustrial building describes the building or fixing of buildings or frameworks by any approach, whether it be manual work or equipment manufacturing. This might include road or rail building and construction, buildings for mining, buildings to produce food, structures to manufacture garments, office buildings, factories, storehouses, train stations, power generating plants, and also factory. Manufacturing can also be included under the heading of industrial building and construction, as can delivery and associated markets. Freight transportation consisting of air cargo and also ocean freight is taken into consideration part of industrial building and construction, as is telecom, like telecommunication, cable telephones, radio, as well as television transmission. Professional solutions, like bookkeeping, regulation, financial, computer technology, engineering, training, monitoring, architects, and various other kinds of service organizations are additionally included under the heading of commercial building and construction.\nConstruction labor shortages are a major problem in numerous locations of the country. In The golden state, as an example, among every seven people is used in the building market. As a result of building and construction labor lacks and high price of living, services are required to raise prices, which consequently, make the tasks more difficult to handle and also finish. These rate boosts inevitably entered impact when tasks are announced. When an organization attempts to begin a brand-new project and also can not locate enough workers, they will raise the prices, which additionally reduces the motivation to utilize brand-new employees.\nPersonal Building Projects: Tasks of this type are normally self-funded. The building work is done by a company that is either an exclusive company or a restricted liability business. A specialist is responsible for finishing the job, which includes both technical as well as civil work. In most circumstances, these specialists will certainly hire subcontractors, who will certainly then complete the jobs that have actually been passed on to them.\nWhen it comes to exclusive tasks, there is generally no technique of securing repayment safety and security. In many cases, a firm might pick to make use of an escrow or post-sale note. Post-sale notes require that the customer pay the balance of the construction job within a defined time period, typically 6 months, after which the purchaser is responsible for paying the balance of the funding. An escrow or note form is typically used for huge building and construction jobs, such as bridges and also malls, which are a lot larger and require multiple specific repayments to be paid at different times throughout the year.\nConstruction Bonds: Construction bonds are commonly protected by the real property associated with the building task. Most of the times, the bonds are taped by the county during the construction stage. Normally, this is done after the bond vendor has actually obtained repayment from the specialist. These projects may likewise be carried out through building and construction liens, however technicians liens are the most usual type used for these kinds of building and construction jobs.\nIt is important to keep all of the information concerning any kind of building and construction task confidential. In many cases, this is necessary due to the sensitive nature of the building job that is occurring. If any type of info is disclosed, it needs to be appropriately encrypted prior to it is shared with various other events. All firms that participate in any kind of building job should be certified and certified according to regional as well as federal guidelines in order to ensure that they are complying with all of the needed treatments and that the workers are being held to the highest criterion of care.\nBuilding is an umbrella term used to define the scientific research and also art of constructing points, systems, or organisations, and derives from Latin advertisement- constructum and also Old French construction. To construct is likewise the verb: to construct, as well as the item is building and construction: the actual nature of the physical structure. It is made use of to refer to any type of technical effort that deals with the construction of new points, or the renovation of existing things (including design, design, mining, metallurgy). It can additionally be made use of in a non-technical context to explain a system of practices, organisation, or procedure, such as language, art, songs, etc.\nThe extent and design of building and construction undergo cultural as well as political impacts. For example, in many nations construction is an important contributor to the building of brand-new buildings and also is particularly important in developing economies where brand-new houses are usually constructed in establishing locations. In the United States, the majority of brand-new houses are constructed in the inner cities. Similarly, in industrialized nations most structures are built within highly structured as well as managed atmospheres defined by premade advancements. To put it simply, construction generally has a tendency to be culturally as well as socially established by the society into which it is put.\nThere are 4 wide sorts of building and construction, each epitomized by their distinct attributes. The very first are generalised structure tasks, which are usually either residential or commercial in character. These are essentially mass buildings (such as a health center, institution, or workplace complicated), however they tend to concentrate on bigger tasks over brief ranges or over longer durations than regular specialized industrial construction. Generalized business construction includes apartment building, townhouses, condominiums, skyscraper towers, and also pre-development land books. Residential structures, such as houses and houses, can be created separately or as part of bigger developments. concrete company cincinnati ohio\nThe second significant type of building is domestic real estate construction. This type of construction entails the conversion of an existing structure into a domestic home. This can take the kind of a freshly created structure, yet can likewise involve redesigning an existing framework or the conversion of an existing structure into residential units. The domestic housing sector is especially energetic in the USA, Canada, as well as Mexico.', 'October 26, 2016\nDrones could soon be as commonly seen on construction sites as trucks and excavators, thanks to the many ways they can be used to help workers better monitor and capture the construction of a project from inception to completion.\nConstruction sites around the world depend on inspections and surveys to obtain a comprehensive look of a builds progress, and to ensure that strict safety standards are being met in the process. With cutting-edge drone technology, operators can now easily and quickly perform inspections in a cost-effective and efficient way, while also keeping employees away from dangerous environments or situations.\nWith data and imagery collected via a drone in the air, users can create 2D and 3D maps, orthomosaics, collect elevation data, and gather volumetric measurements to better plan for, build, and develop projects.\nThe high-quality information collected by a drone also give users the opportunity to view a site in real-time as it progresses, to better manage resources and keep projects on schedule. Additionally, drone technology gives those in the industry the valuable ability to view a project from the comfort of their office without having to physically step foot on a site. This way, many different parties can work together to easily track development and collaborate to improve decision-making.\nBefore drone technology, gathering aerial data and imagery was a difficult and expensive task in comparison to traditional on-foot or manned aerial vehicle methods. Now, a drone can quickly and easily perform an inspection of a job site in less time and at a lower cost than ever before. Additionally, with less time wasted on inspections, operators can maximize efficiencies and review projects without disrupting work, while also keeping extra personnel away from situations that could pose potential safety risks.\nWith pre-programmed flight paths that can be repeated on a daily basis, operators can view real-time imagery of projects as they are built to better monitor sites, optimize efficiencies, and quickly identify any problems before they develop. Ultimately saving on expenses, avoiding wasted resources, and maintaining safety standards.\nDrones are increasingly being used as a survey and inspection tool, to help operators record conditions of a job site from beginning to end. With a drone the amount of time spent collecting data can be cut in half, and the information collected can be generated into orthomosaics and geo-referenced digital images accurate to 1.5 cm per pixel.\nA quick drone survey can benefit every stage of the construction process. Before construction begins, a drone can fly over a site to gather data and 3D maps of an area to better plan out a build. Once construction starts, daily or weekly autonomous drone flights can monitor the different stages of a build and help operators better manage their resources. Finally at the end of a build, a drones imagery of the final product can be compared to original plans, or shared with clients for review or marketing purposes.\nA drone can give users a clear picture to compare a projects current conditions versus the original plans. With a drone users can also collect data to assess reference points, elevation models, break lines and contour lines, or even to calculate volumes of stockpiles.\nWith the information collected by a drone, clients, managers, and investors can easily observe a timeline of a sites progress to keep track of work being done. The data and imagery collected can be run through software that can then create models to compare original plans with construction work, to show users how a site is progressing, and what parts of a project may be falling behind. All of the information can then be reviewed from any office around the world, without anyone having to physically step foot on a job site.\n3D maps, elevation data, volumetric measurements or orthomosaics can continuously be updated on interactive maps to give users better control over work progress and keep investors and clients in the know. By closely monitoring and analyzing up-to-date imagery and data, construction projects can save large amounts of time and money, while also improving communication and collaboration between different parties to enhance planning as a site grows, and improve decision making.\nDrone technology is quickly advancing, and everyday more uses for these flying robots are being discovered. Particularly in the construction industry, drones can give users the ability to monitor and track construction progress in a way never before possible.\nWhen it comes to the future of drone technology automation will be a key advancement. A drone will be able to automatically deploy, fly a pre-set route, and land all on its own. This way, 2D and 3D images of a job site can be collected and reviewed everyday, allowing users to easily analyze and cross-reference progress.\nThe applications for drone usage in the construction industry are endless, and more and more, the industry is discovering the many ways the technology can be used to reduce expenses, save time, and prevent safety hazards. With drone technology continuing to rapidly advance, usage in the construction industry is projected to skyrocket as they help to reduce expenses, improve safety, and maximize efficiency.\nOctober 20, 2023']	['<urn:uuid:5af3b39c-76a7-4b9b-b896-b653965703f2>', '<urn:uuid:2b354a30-c723-40c8-b249-7fec01c49f2e>']	open-ended	direct	short-search-query	similar-to-document	multi-aspect	novice	2025-05-12T10:19:05.071190	6	77	2128
11	Which company focused more on male dancers, RAD or Ballets Russes?	While the RAD focuses on general ballet training for all students with specific considerations like pointe work readiness for female dancers starting around age eleven, the Ballets Russes distinctively challenged ballet traditions by placing special importance on the male body in their aesthetic approach. The Ballets Russes notably shifted away from ballet's traditional focus on the idealized female body, making the male dancer particularly central to their artistic vision.	['What is the RAD?\nThe Royal Academy of Dance (RAD) is an international dance education and training organization, and examination board that specializes in Ballet. The RAD was established in London, England in 1920 as the Association of Operatic Dancing of Great Britain, and received its Royal Charter in 1936. HM Queen Elizabeth II is currently the patron of the RAD and Darcey Bussell CBE has served as President since 2012.\nThe RAD was created with the objective to improve the standard of ballet training in the UK and, in pursuit of that goal, a new teaching method and dance technique was devised for the Academy by a group of eminent European dancers. The RAD is one of the largest dance organizations in the world with over 13,000 members in 79 countries, including over 7000 who hold Registered Teacher Status. There are currently over 1,000 students in full-time or part-time RAD teacher training programs and each year the examinations syllabi are taught to over 250,000 students. For more information, go to www.rad.org.uk.\nHow are students placed in classes?\nStudents are placed in a level appropriate for both their age and level of physical and/or technical development and based on the syllabus of the Royal Academy of Dance. Our instructors are trained professionals who provide age appropriate dance training for all our young artists. Placement within a level is subject to change solely at the discretion of the Artistic Director.\nWhat do we mean by developmentally appropriate?\nIt is important to understand that ballet is a rigorous activity both physically and mentally. When young students begin ballet training, it is extremely important to remember that their bodies are still developing. At an early age the bones, ligaments, tendons, and muscles are not necessarily strong enough for the support of the body required by formal dance training. Strength, flexibility, coordination, confidence and the love for dance may be gained during these early years and will prepare the student for the demands of the art. For the same reason, students should not be put on pointe too soon. Eleven is generally an appropriate age for beginning pointe. Readiness for pointe is assessed on an individual basis taking into consideration age, years of training, strength and the understanding and physical demonstration of necessary skills and the required technique.\nWill I be able to see my child perform?\nStudents of the Alabama Ballet School present “Report of Progress” performances at the end of each year instead of a year-end recital. These reports are presentations performed at our dance studios, for each level, and are open to parents and family friends. They show a cross-section of class to illustrate the work that has been done throughout the year. Advanced level students will have an end-of-year performance on in the “black box” theatre at the ABCD.\nReports of Progress are presentations of each class for parents, family, and friends showing a cross section of class to illustrate the work that has been done throughout the school year. No separate costumes are required.\nThere will also be an optional performance opportunity at the end of the year. This year’s production will be Peter and the Wolf.\nStudents are encouraged to audition for community cast roles in George Balanchine’s The Nutcracker® and other productions of The Alabama Ballet. Auditions and rehearsals are held at the Alabama Ballet Center for Dance.\n- Observation of classes is only permitted during Parent’s Observation Week, or by special arrangement with the Artistic Director and faculty. It is highly inappropriate and disruptive to the class for parents and friends to wander the hallway looking in on classes. This is to ensure that the students stay focused on their class work and teacher, rather than visitors.\n- The ballet and school are not responsible for personal property. Please put your name in all your belongings.\n- All students are expected to wear the dance apparel specified. (see dress requirement) Dance clothes and shoes are to be kept clean and sewn.\n- All students are required to wear street clothes (a complete cover-up) and street shoes when entering and exiting the building. This preserves the life of our highly specialized dance floor as well as students’ dance shoes.\n- All parties picking up children or carpool participants must enter the building to pick students up at the end of classes. Please inform the school office if there is anyone with whom your student is not to leave the building. Do not park in the “drop-off” circle to come into the building. This causes traffic problems. If you need to come in, please park in a designated parking space.\n- The Alabama Ballet is a smoke-free environment. No gum chewing is allowed in the building. No food and drink in the studio except water.\n- Once class has begun, students are not permitted to enter without permission of the teacher.\n- Students are to use only the front school entrance to enter and leave the building except in case of emergency.\n- Students are not to open any outside doors in the building for any person other than a faculty or staff member. When in doubt, don’t let them in.\n- Students are to check the school message board daily for changes in the schedule, special events, messages, etc. You can also check the web site – www.alabamaballet.org – for additional information.\n- Except in the case of an emergency, office staff will not interrupt class for phone calls, messages, etc. Messages will be taken and given after class.\n- Absences must be reported to the office prior to class time.\n- Students may make up missed classes in a level equal or below their own. This must be coordinated with and approved by the school administrator and primary instructors.\n- A release of claims and treatment authorization form must be completed and on file before a student will be allowed to take class. This should be completed at the time of registration.\n- For all students – no skirts except those in the adult classes.\n- No jewelry (watches, necklaces, etc.) except small earrings are to be worn.\n- No loose hair around the face.\n- No plastic, rubber, cloth pants, or loose shirts are to be worn in class. With permission from the teacher, close fitting warmers may be worn in case of injury.\n- If a student is excessively tardy, incorrectly dressed or is disrupting the class, he/she may be asked to observe class rather than participate.\n- All dancers must wait for their parents inside the lobby.\n- No food or drink in the studios. You must clean up after yourself.\n- Parents, we respectfully request that you wait for your child in the lobby area and please monitor younger siblings. The lobby and hallways must remain quiet in order for the administrative employees to conduct their business.\n- We ask that all students do their best to keep the hallways, lobby and studios clean.\n- Every student, parent, and teacher in the school is expected to conform to the rules and regulations and to behave throughout the facility in a disciplined, responsible, and courteous manner. The school reserves the right to suspend or dismiss any student or family whose attitude, attendance or conduct is found to be unsatisfactory.', 'The Legacy of the Ballets Russes by FIRE AND AIR Assistant Director Sophie Andreassi\nSerge Diaghilev’s Ballets Russes existed for a mere twenty years, and yet it is difficult to overstate the company’s influence on dance, art, music, and design in Western Europe and beyond. From its inception in 1909 to Diaghilev’s death and its dissolution in 1929, the company drew throngs of spectators, eager to witness the product of Diaghilev’s latest, carefully-orchestrated collaborations among promising artists of different disciplines. The itinerant company’s experiments galvanized the art world in their time, and forever altered the trajectory of art and culture in the 21st century.\nDiaghilev capitalized on a vibrant Paris arts scene and engaged his talented circle of Russian émigrés. Artists who secured Diaghilev’s approval were poised to take on a near-cult following. Visual artists and designers such as Pablo Picasso, Salvador Dalì, Coco Chanel, Henri Matisse, Joan Miró, Natalia Goncharova, André Derain, Léon Bakst and Georges Braque enjoyed the exposure afforded them by creating works for the company’s stages. The company’s composers included Claude Debussy, Francis Poulenc, Sergei Prokofiev, Erik Satie, Igor Stravinsky, and Richard Strauss. If not already prominent in their field, these and other composers achieved legendary status, especially after the creation of such enduring works as Stravinsky’s Rite of Spring and Debussy’s L’après-midi d’un faune.\nDiaghilev’s choreographic protégés, including Michel Fokine, Vaslav Nijinsky, Bronislava Nijinska, Léonide Massine, Serge Lifar, and a young George Balanchine, tested the viability of currents within modernism at the level of the body while exploring the limits of the classical ballet vocabulary. Significantly, the Ballets Russes challenged the centrality of the idealized female body to ballet. It was the male body was of special importance to Diaghilev’s aesthetic.\nFollowing Diaghilev’s death, offshoots of the company formed to continue to tour the company’s existing repertoire internationally. Over the course of the 1930s and 1940s, national companies on both sides of the Atlantic grew in prominence, often under the purview of Ballets Russes disciples. On the American side, Fokine, Massine, and Nijinska were some of the first choreographers to be engaged by the newly-formed Ballet Theater, now known as American Ballet Theater, in New York in 1939. Balanchine, too, moved to the States and, with Lincoln Kirstein, created the School of American Ballet in 1934 and the New York City Ballet in 1948.\nIn the 1970s, Robert Joffrey, director of the Joffrey Ballet, began a project of reconstructing select pieces from the Ballets Russes canon, inviting Massine to oversee some of his and Fokine’s ballets. Later, Joffrey set about the ambitious project of reviving works by Nijinsky, including Rite of Spring, relying on extensive research by dance historians to attempt to capture the spirit of the piece in the absence of any films.\nUnder Diaghilev’s ingenious, if heavy-handed, supervision, the Ballets Russes reimagined ballet as the fusion of movement, art, and music. The product of these experiments was a rich legacy of innovation and an addicting history that continued to inspire artists, scholars, and the popular imagination through to the present day.\nPhoto: Alexandra Danilova and Serge Lifar in the Ballets Russes’ Apollo (originally Apollon musagète) in 1928.']	['<urn:uuid:ea85ce77-83d0-462c-8592-66e987515153>', '<urn:uuid:13fabf4d-0b4c-48f1-85e4-2eb99468e986>']	open-ended	with-premise	concise-and-natural	distant-from-document	comparison	novice	2025-05-12T10:19:05.071190	11	69	1740
12	As a first-time mom-to-be who's into fitness - what's considered safe exercise intensity during pregnancy, and will I need to pump and dump breast milk if I drink alcohol while nursing?	During pregnancy, exercise should be moderate-intensity where you can still maintain a conversation (known as the 'Talk Test') - if you're too out of breath to talk, you need to slow down. As for alcohol and breastfeeding, moderate consumption is considered safe and there's no need to 'pump and dump' as alcohol passes freely in and out of breastmilk at the same rate as your blood alcohol level. While only very small amounts enter breastmilk, some babies may become fussier and sleep less after their mother consumes alcohol.	['Diet and breastfeeding\nMany mothers worry about how their diet will affect their baby whilst breastfeeding. There are memes and articles all over the place with great long lists of ‘foods to avoid whilst breastfeeding’! The truth is, as long as your baby does not have any allergies, you can eat whatever you like (with a couple limitations). Whilst it’s always best to eat a good diet to maintain your own health, what you eat does not have a huge effect on the quality of your milk or your supply.\nModerate alcohol consumption whilst breastfeeding is considered safe. Alcohol passes freely in and out of breastmilk and rises and falls at about the same rate as your blood alcohol level, so there is no need to ‘pump and dump’. Some mothers find that their babies are fussier and sleep less if they have consumed alcohol, so follow your baby’s lead. Although only very small amounts of alcohol enter your breastmilk, this has not been extensively studied. More info on this can be found here.\nThe NHS recommend that you limit caffeine intake to about 200mg/day. Caffeine does pass into breastmilk. Some babies are quite sensitive to it and others aren’t. It tends to have a greater effect on younger babies. Some women find they can drink coffee all day with no effect on their baby, others find that just one cup of tea has their baby over-stimulated. Follow your baby’s lead.\nOily fish is a fantastic source of Omega 3 and vitamin D, but it also contains low levels of pollutants and so should be limited to two portions a week.\nThere is no reason to avoid acidic food such as citrus fruits, tomatoes, vinegar etc. Eating a large amount of acidic food does not change the pH of your blood plasma. Your breastmilk is made from your blood—not the contents of your stomach.\nIBCLC on maternal diet\nSpicy and gassy food\nThere is no reason to avoid spicy foods whilst breastfeeding. Women from some cultures eat lots and lots of spicy foods, and their babies suffer no more digestive issues than babies who’s mother’s eat no spicy foods.\nSome vegetables, such as cabbage, beans and Sprouts, might make you feel gassy and bloated. This gassiness and bloating is caused by your body breaking down the carbohydrates in those foods. Those carbohydrates do not enter your milk undigested, so this will not effect your milk. As above, your milk is made from your blood, not the contents of your stomach.\nIt is possible for your baby to have an intollerence to the proteins in your diet, but this is very rare. If you think your baby has an intollerence to a food in your diet causing fussiness or digestive problems, try cutting it out for a couple weeks, then reintroducing it to see if symptoms return.\nUnless your baby has an allergy or intolerance to cows milk (or you’re vegan!) then there is no need to avoid dairy. CMPA is rare in breastfed babies, affecting around 0.5%. Symptoms generally include:\n• Projectile vomiting\n• Stools which are loose, frothy, mucousy and/or bloody stools\n• Failure to thrive/weight loss/ slow weight gain\n• Hives or other skin rashes\n• Wheezing or breathing difficulties\nYou can find more info on CMPA here.\nCMPA is massively over diagnosed. A diagnosis of CMPA more often than not leads to women moving their baby on to a hydrolysed formula. In the past decade, despite there being no evidence that cases of CMPA have actually increased, the number of diagnoses have. Prescriptions for hydrolysed formulas have increased by 500%, and the amount of money which formula companies have made from sales of prescription formulas has increased by 700% (the cost of which falls to the NHS to cover). Formula companies funded the writing of guidelines for diagnosing CMPA. They are a main sponsor of the charity Allergy UK who run a helpline for parents, and they also run allergy training days for Dr’s, health visitors, and other healthcare professionals. Formula companies profit directly when babies are diagnosed with CMPA and given prescription formula, and it is believed that their influence is the driving force behind the huge increase in babies diagnosed with CMPA. A report on this was published in the British Medical Journal here.\nHumans don’t really need cows milk in their diets, so if you want to cut out dairy that’s absolutely fine! Anecdotally, some women think that it helps for issues such as reflux and colic, though there isn’t any evidence to support this in the absence of an alergy. However, it is hidden in A LOT of unexpected places and takes up a bigger percentage of most people’s diets than you would expect! So if you do decide to cut it out, make sure that you’re finding other sources of calories and calcium to make up for it so that you do not lose too much weight or become deficient.\nLactose intolerance is extremely rare in breastfed babies. Lactose is the main sugar in your milk, whether you consume dairy or not.\nBreastfeeding Network factsheet on CMPA', 'All pregnant women need to know about safe exercise in pregnancy − safe exercise types, prescription, intensity & what exercise is not safe. I see too many pregnant women who either avoid exercise as they don’t know what is safe or the other extreme who carry on doing what they did pre-pregnancy. I want to make sure this isn’t you by educating & empowering you to do exercise that is safe for you & your unborn baby.\nBefore starting any exercise in pregnancy you need to obtain clearance from the medical professional in charge of your care. This is to ensure there is no medical reason why you should avoid exercise.\nThe below information & advice is based on an uncomplicated pregnancy, current guidelines, available evidence & my Masters training on safe exercise in pregnancy.\nSafe types of exercise in pregnancy:\n1. Swimming- avoid breaststroke if you have pelvic pain\n2. Aqua aerobics\n4. Stationary bike\n5. Elliptical machines- avoid if you have pelvic pain\n6. Pregnancy Pilates- modify if generic class\n7. Pregnancy Yoga- modify if generic class\n8. Low impact aerobics or gym class- modify if generic class\n9. Strength training- machine weights, dumbbells, resistance bands or body weight\n- Use lighter weights & increase repetitions.\n- Aim for 3 strength sessions per week (this includes Pilates & Yoga).\n- Do 1 set of 12-15 reps of up to 8-10 exercises.\n- Work all large muscle groups.\n- Make sure you don’t feel excessive strain anywhere in your body.\n- Focus on technique, posture, core activation & maintaining breath.\n- Valsalva maneuver is a big no-no (holding breath whilst bearing down).\n10. Combination of above:\n- I recommend doing a combination of low impact cardiovascular exercise, strength training, core exercise and stretching in your pregnancy.\nSafe exercise prescription in pregnancy:\n- The most recent guidelines from the American College of Obstetricians and Gynecologists recommend 30 mins per day (or most days) of moderate intensity.\n- Sports Medicine Australia recommends closer to 60 mins of exercise most days, as it reduces your risk of gestational diabetes.\n- If you choose to exercise >30 mins, you must be in thermoneutral or controlled environment (i.e. air conditioned).\n- Don’t exceed 60 mins to avoid risk of hypoglycemia (low blood sugar).\n- If you haven’t exercised prior to pregnancy, then it is best to gradually increase over time.\nSafe intensity of exercise in pregnancy:\n- Moderate-intensity so your heart race increases and you sweat.\n- Measure with the “Talk Test” − you can still have a conversation during exercise.\n- If you are too out of breath to talk then you have pushed yourself too much & need to slow down.\nWhen is it safe to start exercise in pregnancy?\n- As soon as you can! It was found that starting regular exercise in the first trimester was beneficial in increasing placental growth.\nMake it enjoyable!\n- Choose safe exercise in pregnancy that you enjoy! Try different types to see what feels good for your body.\n- If exercise doesn’t come naturally, make it more appealing by listening to a podcast or meeting up with a girlfriend.\n- If you need extra inspiration be sure to check out my last blog- Why Is Exercise In Pregnancy So Important?\nExercise that is NOT safe in pregnancy:\n1. Contact sport- your risk of injury is higher due to having increased laxity in your ligaments.\n2. Any exercise with risk of abdominal or breast trauma e.g. contact sport, ball sports (e.g. hockey, soccer, tennis), boxing, fencing.\n3. Any exercise with a risk of falling e.g. road cycling, horse riding, water/snow skiing.\n4. High impact exercise/sport- anything involving jumping, running, skipping, sudden stop/starts or change of direction.\n5. Heavy weights of any kind including cross fit.\n6. Any exercise involving motionless prolonged standing.\n7. Any exercise lying FLAT on your back for LONG periods of time >16 weeks gestation. This is to avoid compression of the main vein in your trunk. If you feel comfortable lying on your back past 16 weeks, you can still do some exercises/stretches in this position, but don’t stay here for prolonged time & be sure to move if you feel unwell, dizzy or short of breath. You can also place a pillow under your right buttock to alleviate pressure off the vein.\nNote: Sports Medicine Australia advises to avoid prolonged exercise on your back from 28 weeks gestation. Be guided by how your body feels.\n8. Any exercise in hot/humid conditions- e.g. Hot or bikram yoga, spas, saunas, or in hot/humid climates.\n9. No scuba diving.\n10. No exercise at high altitudes >2500m.\nStop exercise if you feel any of the following:\n- Shortness of breath prior to exercise\n- Chest pain\n- Muscle weakness\n- Calf pain or swelling\n- Vaginal bleeding\n- Preterm labour\n- Amniotic fluid leakage\n- Decreased foetal movement\n- Bladder leakage\n- Vaginal heaviness\n- Back or pelvic pain\nOther safe exercise tips:\n- Start with a 10-15min warm up at a lower intensity.\n- Finish with a 5-10min cool down.\n- Remain hydrated throughout.\n- Maintain adequate nutrition.\n- Avoid overstretching- only stretch to a point where you can feel the stretch & avoid pushing past your comfort zone.\n- Listen to your body- if an exercise doesn’t feel right in your body then stop.\nAfter reading this post, I hope you have the knowledge & confidence to exercise in your pregnancy. I’d love to hear in the comments below what safe exercise in pregnancy do you enjoy?\nP.S Do you want to avoid pelvic and back pain in your pregnancy? I think that’s a big fat “YES”!! Click here to find out how.\nAmerican College of Obstetricians and Gynecologists (2015) Physical activity and exercise during pregnancy and the postpartum period. Committee Opinion No. 650. Obstet Gynecol 125: 135-142.\nDowns et al (2012) Physical activity and pregnancy: past present evidence and future recommendations. Res Q Exerc Sport 83: 485-502.\nEvenson et al (2014) Guidelines for physical activity during pregnancy: comparisons from around the world. Am J Lifestyle Med 8: 102-121.\nKehler A, Heinrich K (2015) A selective review of prenatal exercise guidelines since the 1950s until present: written for women, health care professionals, and female athletes. Women Birth 28: 93–98\nPersinger R et al (2004) Consistency of the talk test for exercise prescription. Med Sci Sports Exerc 36: 1632-6.\nSports Medicine of Australia (2016) Exercise in pregnancy and the postpartum period.\nStefani L, Mascherini G, Galanti G (2017) Indications to promote physical activity in pregnancy. Journal of Functional Morphology and Kinesiology 2: 1-7.\nWolfe L, Mottola M (2002) Physical Activity Readiness Medical Examination for Pregnancy: PARmed-X for Pregnancy; Canadian Society of Exercise Physiology & Health Canada: Ottawa, ON, Canada.\nWolfe L, Weissgerber T (2003) Clinical physiology of exercise in pregnancy: a literature review. Journal of Obstetrics and Gynecology Canada 25: 473-483.']	['<urn:uuid:2a0a3f77-e4ce-469c-b29c-b25a8c1e7c4f>', '<urn:uuid:d828d75b-a0ad-48f9-b508-6a3202803811>']	factoid	with-premise	verbose-and-natural	distant-from-document	multi-aspect	novice	2025-05-12T10:19:05.071190	31	88	2003
13	How do TRAPPIST-1 planets compare to Kepler-186f?	Both TRAPPIST-1 and Kepler-186f are notable exoplanet discoveries, but they have key differences. The TRAPPIST-1 system, located 40 light years away, has seven rocky planets that could potentially have liquid water, with three in the habitable zone. In comparison, Kepler-186f is a single confirmed Earth-sized planet (just 10% larger than Earth) located 500 light years away in the habitable zone of its star. While TRAPPIST-1 planets orbit in resonant configurations suggesting they may be cores of gas giants, Kepler-186f is likely to have a rocky composition similar to Earth based on its size and planetary models.	"['‘Habitable’ Exoplanets Might Not Be Very Earth-Like After All\nMore caution is needed in describing newly-found exoplanets as habitable given the limitations of observations, researchers warn.\nOne of the most exciting moments in exoplanet science came in late February, when NASA\'s Spitzer Space Telescope announced the discovery of seven rocky planets orbiting in or near the habitable zone of their parent star, TRAPPIST-1, which lies 40 light years away from Earth.\n""The discovery sets a new record for greatest number of habitable-zone planets found around a single star outside our solar system,"" NASA said in a statement. ""All of these seven planets could have liquid water - key to life as we know it - under the right atmospheric conditions, but the chances are highest with the three in the habitable zone.""\nComparing the habitability of our own planet to the conditions on newly-discovered exoplanets, however, could be misleading, according to the authors of a commentary in the journal Nature Astronomy. They argue that even though scientists have found hundreds of Earth-sized planets, there is no available technology to show us if their surfaces are remotely Earth-like.\nLead author Elizabeth Tasker said the language used by NASA and others can be ""unnecessary and dangerous,"" in the sense that the public could get overly hopeful that life exists on these other planets.\nTasker, who is an associate professor in the department of solar system science at the Japan Aerospace Exploration Agency, was not involved in the TRAPPIST-1 discovery.\nIn studying the data, however, she noted that the team found that many of the planets orbit in resonant configurations, meaning that one planet\'s orbit is a direct ratio of another. In other words: The inner planet in the TRAPPIST-1 system, for example, orbits eight times in the period it takes the outer planet to orbit two.\nTasker said this probably suggests these worlds formed further out from the star and over the years, their mutual gravitational attraction pulled them in closer. ""They may not be terrestrial planets, but maybe the cores of gas giants because they formed in a similar region to our own solar system,"" she said.\nTasker\'s interest in the habitability of new-found planets sprung from the announcement of Gliese 832c in 2014. In the scientific paper announcing the discovery of the planet, the authors cautioned that this ""super-Earth"" is more likely a ""super-Venus"" with a massive atmosphere. On Venus, the surface roasts at oven temperatures and can crush unprotected spacecraft in moments. Yet, Tasker said that many of the media stories fawned over Gliese 832c as a potentially habitable world.\nTasker said the oversimplification of comparing exoplanets to our own planet arises from something called the Earth similarity index. The index uses four parameters: the radius of the planet, the radius of Earth, the stellar flux (or radiation) of the exoplanet\'s star, and the solar flux of our own sun. This metric is used by entities such as the Planetary Habitability Laboratory at the University of Puerto Rico at Arecibo, which ranks planets on a habitability index.\n""In practice, these [parameters] aren\'t independent,"" Tasker said. For example, the flux coming from the star gives equilibrium temperature, which is different from planetary surface temperature - and doesn\'t take into account how much radiation the planet receives, she said.\nTasker acknowledged it is difficult for people to stop comparing planets to one another, but she did advocate for - at the least - a better description of what constitutes a habitable zone. She said that the media should specify that the zone is where liquid water could exist on a rocky planet orbiting the star, rather than just saying where liquid water could exist. She also suggested that if possible, reports specify that the habitable zone is dependent on many factors, such as whether the star has variable radiation, the tilt of the planet\'s axis, and the planet\'s atmosphere.\n""Our knowledge is far from sufficient to comparatively rank the ability of planets to support life,"" the Nature commentary concludes. ""Unless we want to risk destroying the chance to find out if the Earth is unique, we need to stop pretending that we already know.""\nWATCH VIDEO: Proxima B: Another Earth Just Next Door', '“In space there are countless constellations, suns and planets; we see only the suns because they give light; the planets remain invisible, for they are small and dark. There are also numberless earths circling around their suns, no worse and no less than this globe of ours.”\n- Giordano Bruno, 1584\nIn recent years, many planets have been found within the habitable zone - the region around a star where the temperatures are just right for a planet to sustain liquid water on its surface. By analysing data from NASA’s Kepler space telescope, a team of astronomers have announced the discovery of the most Earth-like planet yet detected. Kepler is a planet-hunting telescope that measures subtle changes in the brightness of stars to see if an orbiting planet is crossing in front of a star. This newfound planet, dubbed Kepler-186f, is the first confirmed Earth-sized planet in the habitable zone of another star. “This is the first definitive Earth-sized planet found in the habitable zone around another star,” said Elisa Quintana, research scientist at the SETI Institute at NASA’s Ames Research Center, and lead author of the paper published on April 18 in the journal Science.\nFigure 1: Artist’s depiction of Kepler-186f, the first validated Earth-sized planet to orbit in the habitable zone of another star. Image credit: NASA Ames/SETI Institute/JPL-Caltech.\nBefore the announcement of Kepler-186f, the smallest habitable zone planets known are all somewhat larger than the Earth, placing them in the super-Earth-sized rather than Earth-sized regime. Although these planets are still potentially habitable, their environments are likely to be quite different compared to the Earth. Examples of these habitable zone super-Earths include Kepler-62e, Kepler-62f and Kepler-22b. Furthermore, Kepler has also detected a number of planets the size of Earth or smaller around other stars. However, these planets all orbit too close to their host stars and are therefore too hot to be habitable. Examples of these close-in planets include Kepler-20e, Kepler-20f, Kepler-78b and Kepler-37c. Unlike these planets which are either ‘at the right distance but too large’ or ‘at the right size but too close’, Kepler-186f is the first confirmed planet that has both the right size and the right distance.\nKepler-186f is part of a planetary system with 4 other known planets. The 4 companion planets, Kepler-186b, Kepler-186c, Kepler-186d and Kepler-186e, circle around their host star every 3.89, 7.27, 13.3 and 22.4 days. All 4 planets orbit much closer-in than Kepler-186f and are therefore too hot to be habitable. These 4 planets range in size from 1.0 to 1.5 Earth-radius and were confirmed using the first 2 years of data collected by Kepler. The detection of Kepler-186f, the fifth planet, required an additional year of data.\nKepler-186f measures only 10 percent larger than Earth (1.11 ± 0.14 Earth-radius), making it a truly Earth-sized planet. Its host star is a red dwarf star about half the size of Earth’s Sun and located about 500 light years from Earth. Kepler-186f orbits within the habitable zone of its host star - the “Goldilocks” zone that is neither too hot nor too cold for liquid water to exist if Kepler-186f has an Earth-like atmosphere. Being a red dwarf star, the host star of Kepler-186f is cooler and dimmer than the Sun. It means that the habitable zone around the host star of Kepler-186f is located much closer-in compared to the habitable zone around the Sun.\nFigure 2: The diagram compares the planets of our inner solar system to the planetary system hosting Kepler-186f. The parent star of Kepler-186f is a red dwarf star with half the size and mass of the Sun. Image credit: NASA Ames/SETI Institute/JPL-Caltech.\nFigure 3: The 5 planets of the Kepler-186 planetary system compared to the inner planets of our solar system. Image credit: NASA.\nKepler-186f circles its host star in the habitable zone once every 130 days. The orbit of Kepler-186f around its host star is actually slightly smaller than the orbit of Mercury around the Sun in our own solar system. Kepler-186f’s orbit places it near the cooler, outer edge of the habitable zone. Despite receiving only 32 percent of the intensity of stellar radiation that Earth receives from the Sun, Kepler-186f is in fact comfortably within the habitable zone. This is because light from a red dwarf star is “redder” than light from stars like the Sun and it changes how an Earth-like planet would interact with the star’s light.\nCompared to the Sun, whose dominant form of radiation is in the visible wavelength, red dwarf stars are cooler and a larger proportion of their energy output is in the form of infrared radiation. For Earth-like planets around red dwarf stars, infrared radiation is absorbed by ice instead of being reflected. Additionally, water vapour and carbon dioxide also absorb and trap infrared radiation. These characteristics make Kepler-186f more efficient at absorbing energy from its host star to avoid freezing over. As a result, Kepler-186f is still considered habitable even though it receives less light from its host star than Mars receives from the Sun.\nFigure 4: Position of Kepler-186f within the habitable zone, show in comparison with a number of other known planets. Image credit: NASA/Chester Harman.\nAlthough the size of Kepler-186f is known, its mass is not known since data from Kepler is unable to yield such measurements. In spite of this, Kepler-186f is small enough for its composition and mass to be well constrained using existing planetary models. These models predict that planets smaller than 1.5 times the size of Earth are unlikely to be dominated by hydrogen-helium gas envelopes like Jupiter or Neptune in our own solar system. Mass estimates for Kepler-186f range from 0.32 Earth-mass for a pure water/ice composition to 3.77 Earth-mass for a pure iron composition. The composition of Kepler-186f is highly unlikely to be anywhere close to these two extremes. Instead, Kepler-186f probably lies somewhere in the middle, most likely with a rocky composition similar to Earth. For an Earth-like composition, Kepler-186f would have a mass of 1.44 Earth-mass.\nRed dwarf stars, like the host star of Kepler-186f, are by far the most common type of star in the galaxy. Such stars make up over 80 percent of the closest stars to the Sun. They are less massive, smaller, cooler and dimmer than the Sun, and they range in size from around 10 to 50 percent the size of the Sun. Besides being the most abundant type of stars, red dwarfs stars are good targets in the search for transiting habitable Earth-sized planets. This is because a red dwarf star is smaller than a Sun-like star, so an Earth-sized planet around a red dwarf star would have a larger planet-to-star size ratio. As a consequence, an Earth-sized planet transiting a red dwarf star would create a deeper transit than if the same planet were to transit a larger Sun-like star. Deeper transits are easier to detect than shallower ones.\nFigure 5: Diagram showing the deeper transit depth of an Earth-sized planet transiting a red dwarf star. Image credit: NASA.\nFurthermore, transiting planets in the habitable zone of red dwarf stars would undergo more frequent transits than those in the habitable zone of Sun-like stars. This is because the habitable zone of red dwarf stars is much closer-in, resulting in shorter orbital periods than those around Sun-like stars. Given the abundance of red dwarf stars, planets such as Kepler-186f are almost certainly the most abundant type of habitable planet. Unfortunately, Kepler-186f itself is just too far away for even future NASA missions, like the Transiting Exoplanet Survey Satellite (TESS) and the James Webb Space Telescope (JWST) to determine its composition and atmosphere. Nevertheless, TESS and JWST will be able to detect and characterize Earth-sized planets around the nearest stars. Most of these planets would be around red dwarf stars, much like Kepler-186f.\nThe discovery of Kepler-186f supports the emerging view that Earth may not be such a unique place, and that the galaxy is home to billions and billions of habitable worlds. “The discovery of Kepler-186f is a significant step toward finding worlds like our planet Earth,” said Paul Hertz, NASA’s Astrophysics Division director at the agency’s headquarters in Washington. “Future NASA missions, like the Transiting Exoplanet Survey Satellite and the James Webb Space Telescope, will discover the nearest rocky exoplanets and determine their composition and atmospheric conditions, continuing humankind’s quest to find truly Earth-like worlds.”\nQuintana et al., “An Earth-Sized Planet in the Habitable Zone of a Cool Star”, Science 18 April 2014: Vol. 344 no. 6181 pp. 277-280.']"	['<urn:uuid:0b1429ed-4df7-45c0-b030-17400460912f>', '<urn:uuid:c54e7c66-b214-4763-9bd7-1e1e45cbea2d>']	open-ended	direct	concise-and-natural	similar-to-document	comparison	novice	2025-05-12T10:19:05.071190	7	96	2124
14	As a music teacher, I'm curious about how melody and harmony work together in music, and how they affect our emotions when we listen to them. Can you explain both aspects?	Melody and harmony have a mutual dependency in music where they complement each other to create a complete musical experience. This can be demonstrated through pieces like 'America the Beautiful,' which becomes unrecognizable with only chordal accompaniment and needs both elements to be effective. When it comes to emotional impact, these musical elements work through specific brain pathways - they activate the mesolimbic pathway (the reward center), the amygdala (which modulates emotional networks), and the hippocampus (involved in emotional attachment). The relationship between melody and harmony can create both tension and release in listeners, contributing to emotional responses that vary based on whether the music meets our expectations of how the elements will interact.	"['CHILDREN’S EDUCATIONAL CONCERT\nby Dr. Richard Dowling, pianist\n“THE FOUR ‘NUTRITIONAL’ ELEMENTS OF MUSIC”\nMelody - Harmony - Rhythm - Tempo\n1. Program begins with attention-getter: “Maple Leaf Rag” by Scott Joplin.\n2. Give short personal bio and state program title. Relate the four musical elements to the importance of vitamins, minerals, carbohydrates, and protein in food. (Children learn about food groups and their components in school.)\n3. Perform Mozart’s “Variations on ‘Ah, dirai-je vous maman’ “—better known as “Twinkle, twinkle little star”—as an example of a piece with a well-rounded musical diet.\nTheme: Melody Var. 5: Articulations (staccato & legato)\nVar. 1: Rhythm Vars. 7 & 8: Major & Minor Keys (happy & sad)\nVar. 2: Harmony Var. 9: Dynamics (piano & forte: soft & loud)\nVar. 3: Tempo Var. 10: Finale (all elements together)\n4. MELODY: The Sentences of Music\nSome are easy to sing, some not. Some are long, some short.\nPerform two short pieces with contrasting melodic character:\na. Easy to sing and remember: “Für Elise” by Beethoven (written for a little girl).\nb. Difficult: “Minute Waltz” by Chopin (inspired by dog chasing its tail).\n5. HARMONY: The relationship between melody and the chords in music\nPerform several works to demonstrate mutual need for one another.\na. Play “America the Beautiful” with only chordal accompaniment. Ask if children can recognize piece of music. Repeat with melody. Have children sing along.\nb. Perform segment of Bach’s “Prelude in C”—a piece with no melody.\nc. Perform Chopin’s “Nocturne in E-flat, Op. 9 #2” first melody alone, then harmony alone, then both together.\n6. RHYTHM: Duple vs. Triple beat patterns\na. Perform a Sousa March as example of duple pattern. Children clap along with music to get feel of 2/4 meter. Heavy emphasis on first beat.\nb. Perform a Strauss Waltz as example of triple pattern. Children clap along with music to get feel of 3/4 meter. Heavy emphasis on first beat.\nc. QUIZ TIME: Perform segments of Prokofieff’s Gavotte, Joplin’s The Entertainer, and a Chopin Waltz. Have children guess correct meter.\nd. Complex rhythm (syncopation): Perform Gershwin Prelude #1 or 3. Ask children to guess meter (confusion)...jazz has rhythms with accents in unexpected places.\n7. TEMPO: The Speed of Music\na. Tempo and rhythm work together just like melody and harmony.\nb. Music uses a “secret code” to tell the performer how fast or slow to play. (Look for these words in the printed program when attending concerts.)\nc. Display large posters with printed Italian terms. Pronounce and define: “Largo, Adagio, Andante, Moderato, Allegretto, Allegro, Presto”\nd. Perform segments of pieces to illustrate different tempi.\ne. Have children guess which tempo marking applies to each piece.\nf. Rimsky-Korsakov: “Flight of the Bumblebee” (presto)\ng. Beethoven: “ ‘Moonlight’ Sonata - first movement” (adagio)\nh. Schubert: “Impromptu in G-flat Major, Op. 90 #3” (andante)\ni. Chopin: “Prelude in C Minor, Op. 28” (largo)\nj. Schubert: “Impromptu in A-flat Major, Op. 90 #4” (allegretto)\nk. Chopin: “Fantaisie-Impromptu, Op. 66” (allegro & moderato)\nExtra “Nutritional Aspect” to be presented, if time and attention span permit:\na. Music creates emotional response. Perform excerpts from different styles of music. Ask children how each piece makes them feel: happy, sad, excited, sleepy, etc.\nb. Perform excerpts from: a Scarlatti Sonata, Debussy “Clair de lune,” Chopin “Prelude in A,” Gluck “Dance of the Blessed Spirits,” Chopin “Grande Valse Brillante,” or Gershwin Preludes\nPerform Chopin’s famous “Polonaise in A-flat Major, Op. 53”—a piece containing all the elements discussed and two distinct styles.\n(c) 1991 Richard Dowling, D.M.A.', 'Why Do We Get Emotional When We Listen To Music?\nAny human of a music-listening persuasion will know the tie between music and emotions: the rush of happiness at a good gig, the delight of singing angrily along to a ferocious song after being dumped. As with many widespread universal experiences, this one has attracted a huge amount of scientific attention, because there\'s a question at the heart of it: why? Why does the noise produced by instruments and voices create emotions in us, how does it do it, and are the emotions that we feel from music of a specific kind or type?\nHumans have been listening to music for an incredibly long time; it\'s been proposed that it was actually a kind of protolanguage before we developed words to communicate. Researchers in 2013 found that people listen to music for 126 different reasons, grouped into roughly three: mood analysis and regulation, self-awareness, and social relatedness. Emotion has a high significance when it comes to our music choices and habits, but experts continue to disagree on some of the finer points of the relationship between a great tune and the tears rolling down our cheeks.\nLet\'s examine what\'s really going on when it seems as if a song touches your heart.\nDo We Experience ""Real"" Emotions In Response To Music?\nDo we actually feel emotion in response to music? It\'s an intriguing question that would seem to have a simple answer; but some theorists don\'t think so. Instead, there\'s an argument that controversially suggests we aren\'t experiencing traditional emotions in response to music at all. What we\'re feeling, the theory suggests, is a kind of tension and relaxation in turns, based on whether or not our expectations of what a piece of music will do next are met. We feel happy, according to this idea, when the next note or movement fulfills what we think might happen, while we get frustrated or feel on edge when it doesn\'t.\nHowever, there are a lot of ways to rebut this, or at least to argue that it\'s part but not all of how our emotional responses to music seem to work. There\'s a lot of physical evidence that we seem to experience emotion while we listen to music, from heart rate increases in response to tense or fast music to reports of emotional response among listeners. And it\'s not purely straightforward, either; we may ""feel"" the emotion of a piece of music as sad, but actually experience pleasure while we listen to it, as research in 2013 discovered. Emotion does seem to be involved beyond just tension and expectation, but it\'s a complicated picture.\nHow music embedded itself in human emotional response is an open question. It\'s been suggested, for instance, that we respond with particular emotional vehemence to songs that might recall the ""calls"" of our pre-language ancestors. ""Upwardly rising, staccato sounds tend to put us on edge, while long descending tones seems to have a calming effect,"" the BBC noted of this theory in 2015. Emotion from music might therefore have an evolutionarily useful aspect. And the composer Joel Douek, writing for Frontiers In Systems Neuroscience, notes that many of the cues used by music-makers to elicit particular emotions or feelings are ""primal responses"" that appear to cross cultures, suggesting some kind of deep historical memory. Another, suggested by the neurobiologist Mark Changizi, is that music echoes human ""expressive movement:"" fast tempo seems to be running from something or doing celebratory dancing, for instance. In these contexts, music is something that echoes parts of our shared human history and survival throughout our species\' evolution.\nHow Music Works In The Brain\'s Emotional Centers\nOne of the most interesting areas of emotional-music science is the part that delves into the brain, and asks the question: does music set off particular parts of the brain\'s emotional systems, and do it differently depending on the emotion of the song? The answer appears to be ""yes.""\nGroundbreaking research published in Nature in 2014 found that there are distinct correlates between music and different areas of the brain, many of which are intimately tied to emotional processing. And, interestingly, tension and expectation play a role. The study found that music that creates pleasurable emotions lights up the mesolimbic pathway, the reward bit of the brain that gives us happy feelings. But that wasn\'t all; music also creates responses from the amygdala (which modulates emotional networks) and hippocampus (which centers on emotions around attachment). And part of the pleasure was definitely centered on tension and its release, but not all of it.\nWe also respond strong to dissonance and whether or not we find it pleasant, according to a study that looked at cerebral blood flow. When played dissonant music, subjects\' brains surged blood to parts of the paralimbic system associated with various kinds of emotions. But the brain\'s response to music isn\'t just embedded in the here and now; it\'s also acutely attuned to the past.\nIt appears that music has a unique power to evoke emotional memory. Memories formed around music can have strong emotional centers, and those involving emotions can be drawn out by using music that was either explicitly part of the memory, or is tangentially related to it. And this relates to something else odd about music and emotion: it grows with familiarity. Our emotional response to a piece of music, according to a 2011 study, is much more intense if we\'re familiar with it and carry the memory of our previous emotional reactions. Music may well soothe the savage beast, but it also appears to be intricately tied to the ways in which we preserve emotional memory.\nHow The Psychology Of Music & Emotions Works\nHow do we ""read"" a song as one kind of emotion or another? It\'s a question that music theorists have spend a lot of time debating. There are structural aspects to music, they believe, that read in different ways to our emotional understanding, whether from learned evolutionary responses or something else.\nThe parts of music that ""talk"" emotion run the gamut, and are referred to as musical codes. In The Aesthetic Mind, theorists William Forde Thompson and Lena Quinto famously outlined a vast bunch of codes, all talking to us on an emotional level. Structurally, there\'s dissonance, loudness, how far or close they are from the tonal center, and how much they keep to their structure. Tempo is another: when we hear slow-tempo music we tend to think it\'s serene, calm or pensive, while fast-tempo music is joyful or restless. Pitch also contributes, and that\'s before we get into how these different aspects change and shift, reflecting shifts in how we ""read"" them. (A fast song that slows, for instance, means a change in mood.) There\'s an awful lot to music beyond just the lyrics.\nIt\'s one thing to read an emotion in a song, though. It\'s quite another to actually feel it. And there are various theories about how that might work. One, most prominently argued by the thinker Stephen Davies, is the idea of ""emotional contagion,"" where we ""mirror"" what we think we get from music rather in the same way that we mirror emotions in other humans. It\'s a theory founded on the fact that our brains contain mirror neurons, neurons that react in exactly the same way to our performing an action and seeing somebody else do it. Mirror neurons have explained a lot of our mimicking behavior and the way in which we make others comfortable by unconsciously mirroring them, and the emotional contagion theory takes it a step further: we feel the emotions from sad music because they\'re contagious.\nThe idea\'s also bolstered by the fact that there\'s an intriguing gap between sensing an emotion in a piece of music and actually feeling it for yourself, and that the gap seems to narrow the more empathetic you are. A study in 2012 found that more empathetic people reacted more strongly to musical pieces, even if everybody read them the same way. Davies pointed out the mirroring of the sadness in a piece of music is in fact pretty unique:\n""The music is the perceptual object and cause of the listener\'s echoing sadness; it is her attentional focus, and her reaction tracks the unfolding of the music\'s expressiveness. However, the listener does not believe that there is anything unfortunate or regrettable about the music (or anything else) and she is not sad about or for the music. In other words, her response lacks the usual emotion-relevant beliefs and does not take the music as its intentional object. Despite this, the mirroring response is emotion-like rather than mood-like or irrational.""\nIn other words, we don\'t feel sad for sad music necessarily; we feel sad because it is communicating sadness to us, using various codes.\nSo next time you feel yourself coming over all angsty because a song by Adele comes over the radio, be assured that your brain is doing a lot of complex work to make sure that you\'re swept away by emotion on the bus ride home.']"	['<urn:uuid:9eec82da-754d-47d7-9ac5-264a1d987736>', '<urn:uuid:cd5a17c0-a70a-49fb-8201-0a549288b067>']	open-ended	with-premise	verbose-and-natural	distant-from-document	multi-aspect	novice	2025-05-12T10:19:05.071190	31	114	2108
15	I'm new to gardening and wondering if beans and tomatoes take the same amount of time to grow and be ready for harvest. Which one is faster?	Beans are much faster to harvest than tomatoes. While beans are not explicitly given a total time to mature, tomatoes take 70-80 days to mature. Additionally, tomatoes need to be started indoors 6-8 weeks before the last frost date, while beans can be sown directly in the garden, making the total growing process for tomatoes significantly longer.	"['How to Start Tomatoes, Peppers, and Eggplant by Seed\nThe sight of the wind blowing through the midwinter barren oak branches hardly encourages me to begin to start my summer garden. The way I feel doesn’t matter, however, because around the corner comes the summer heat and for my garden to be part of it, I have to start now. Sure, I could buy the vegetables for the garden when I do buy them, I choose Bonnie Plants – they are GMO free), but planting by seed gives me access to many more varieties than those available at the nursery and allows me to grow my garden less expensively. I also plant a lot of heirloom vegetables from seeds saved from the previous year.\nSince the nightshade vegetables, such as tomatoes, peppers, and eggplant, take the longest to produce fruit, they should be started indoors before any of the other vegetables.\nPlan to seed tomatoes around 6-8 weeks before your average last frost. If you live in the South, they should be the only vegetables that you HAVE to start early.\nSure, you could get a head start with your other vegetables, such as squash and melons, a couple of weeks before the last frost, but they produce relatively quickly and often have trouble transplanting. I find it best just to plant them directly into the garden.\nSTEP 1 – Prepare Soil\nTo begin, prepare small containers (cell trays that contain many small compartments on one flat work great) with a good seed-starting soil medium. Don’t substitute this soil medium for potting mix, which contains large pieces of bark, or garden soil, which compacts easily. Believe me; results are poor.\nSTEP 2 – Plant Seeds\nPlace 2 or 3 seeds in each container, at 1/4” deep. Pat down soil medium, and water the soil.\nSTEP 3 – Keep Plants in Warm Location\nPlace the newly sown vegetables in a warm location, preferably 75° or warmer, but temperatures of 65° or greater will work fine are much easier to accommodate (like in the house). Like most plants, the vegetables need to consistently be in moist (but not saturated) soil throughout their growing season. Once the seedlings emerge, relocate to a sunny locations where they can receive 5-6 hours of light and enjoy the comfortable temperature of around 70°. Water the seedlings with a weak, half strength solution of liquid fertilizer when needed.\nSTEP 4 – Transplant and Harden Off\nOnce the first true leaves appear on the seedling (when they are around 2 inches tall), transplant vegetables in 3-4 inch pots filled with similar soil medium. Also at this time, snip the extra seedlings you planted in each cell so that each pot contains only one plant.\nA couple of weeks before planting, begin to harden off the seedlings by bringing then outside. Start by placing them in a partly sunny area for about an hour one day when the weather begins to warm. Follow this by a couple hours of sun the next day. By the end of a week or 10 days of hardening, the tomatoes are ready for the garden.\nA Few Extra Tips for Tomatoes\nTomatoes, however, benefit from a couple more details. When you transplant the tomato seedlings to bigger pots, plant the tomatoes a little deeper than originally (up to the true leaves). Also, following transplanting, occasionally run your hand through the tops of the tomatoes or have a fan gently blowing on them (to simulate wind) for sturdier plants.\nBy simply following these guidelines, you will have great healthy young plants that will give you amazing flavor fantastic dishes for your family. Enjoy the great outdoors and the harvest of your hands!', 'Have Seeds will Garden (Planting Guide for a wide variety of vegetables)\nSo you want to start a garden but you don\'t know exactly how deep to plant the seeds or how far apart. Check out this list for tips and guidance. ~Happy Gardening!\nAsparagus: Soak seed overnight. Sow ½ inch deep. 2 inch apart. 60 degree soil. Let grow 1 yr. then transplant to permanent area. Harvest begins 3rd year.\nBeans/Peas: Sow 1 inch deep. 24 inches apart. For peas and pole beans provide a trellis. Peas and pole beans can be planted a bit closer at 6-10 inches apart. Full sun/well drained soil. Keep pea pods picked early and often to keep plants producing, same for bush beans- keep bean pods picked often so that plants keep producing blossoms.\nCabbage: Sow 1/8th inch deep as soon as soil can be worked. Full sun. Thin to 24 inch apart. Average 100 days to mature.\nCarrot: Sow seed ¼ inch deep. Thin to 6 inches apart. Full sun. About 65 days to harvest.\nCucumber: Plant 4 seeds per mound. ½ inch deep. 1 foot apart. Provide trellis. About 60 days to harvest.\nEggplant: Start indoors. Transplant to garden when 6-8 inch tall. Full sun/well drained soil. About 80 days to mature.\nEndive: Sow seed directly in garden as soon as soil is workable. Thin to 12 inch apart. 85 days to mature.\nLettuces/ Chards/Greens: Sow directly in garden. No deeper than ¼ inch. Thin to 12 inch apart. Full sun. Average 40-55 days to mature.\nTomatoes: Sow seed 1/4 inch deep. Start indoors 6-8 weeks before last frost date. Transplant to garden when 6-8 inches tall. Space plants 2-3 ft. apart. Full sun. Provide trellising or tomato cage. Full sun. Matures at about 70-80 days.\nPeppers: Sow seed no deeper than ¼ inch. Peppers need soil temp. of 70 degrees to germinate well. You can provide a heat mat underneath your seed tray to assist in germination. Germination takes 7-24 days. Transfer plants to garden when 6-8 inches tall. Full sun. 70-80 days to mature.\nRadish: Sow direct in garden ¼ inch deep. Full sun. Thin to 2 inches apart. Matures in 28 days.\nOkra: Plant seed ½ inch deep directly in garden. Thin to 18 inch apart when 2 inches tall. Harvest pods when 3 inches long. Keep pods picked to ensure good production. About 55-70 days to mature.\nPumpkins: plant 2 seeds per hill 1 inch deep. 4-6 ft. apart. Full sun. Keep free of weeds. Rich fertile soil. Average 95-110 days to mature.\nMelons: Watermelon / plant 2 seeds per hill 3 ft. apart. 1 inch deep. After danger of last frost. Full sun. Keep weed free. 75-80 days to mature.\nCantaloupe/ plant 2 seeds per hill 1 inch deep. 4 ft. apart. Full sun. 10-20 days to germinate. 80-90 days to mature.\nSquash: Summer squashes/ Sow 2 seeds per hill. 1 inch deep. 3 ft. apart. Full sun. 60-65 days to mature.\nWinter squashes/ Sow 2 seeds per hill. 1 inch deep. 4-6 ft. apart. After danger of last frost. 80-90 days to mature. Full sun.\nTurnip: Sow seed directly in garden as soon as soil is workable. ¼ inch deep. Full sun. 50 days to mature.\nHerbs: Sow seed direct in garden or pot at a depth of 1/8th inch. Full sun.\nI\'m a photographer...and a country girl at heart.Born and raised in Florida, I have lived in many small/rural towns in Southern Florida. I love capturing the innocence and joy of childhood through photography.Along with photography I absolutely love gardening.\nContent /Writing copyright Tina Leavy/The Victory Gardener blog and The Urban Farmette blog and may not be reproduced without my written consent. Thank you for understanding. (images that have been used from Dk Images free clip art website have been duly noted..all other content/photos is/are copyright to the author of this blog)\nVisit our online Community Blog\n""Small Town Living""\nMy Victory Garden Etsy Shop\nI\'ve been stalked/My Victory Garden Shop Item featured here.']"	['<urn:uuid:b30aa546-199c-4efe-bd78-ccc2304aa902>', '<urn:uuid:0f75a53a-d09b-47fa-a79b-2697792db029>']	factoid	with-premise	verbose-and-natural	distant-from-document	comparison	novice	2025-05-12T10:19:05.071190	27	57	1288
16	How do you calculate the error in a linear equation?	The error in a linear equation is calculated using the sum of the squares of the residuals, which are the vertical line distances between the fit line and each data point. This method is called Ordinary Least Squares (OLS). It minimizes the sum of the squared differences between observed and predicted values.	"['Regression With Scikit Learn (Part 2)\nThis is Day 30 of the #100DaysOfPython challenge.\nThis post will continue on from part one and break down the basics of linear regression and also explain how we can take the work that we did and expand upon that to apply a train-test split to our dataset.\nSource code can be found on my GitHub repo\n- Familiarity Conda package, dependency and virtual environment manager. A handy additional reference for Conda is the blog post ""The Definitive Guide to Conda Environments"" on ""Towards Data Science"".\n- Familiarity with JupyterLab. See here for my post on JupyterLab.\n- These projects will also run Python notebooks on VSCode with the Jupyter Notebooks extension. If you do not use VSCode, it is expected that you know how to run notebooks (or alter the method for what works best for you).\n- Read ""Regression With Scikit Learn (Part One)""\nLet\'s create the\nregression-with-scikit-learn-part-two by cloning the work we did yesterday. The packages required will be available in our\nIf you are unsure on how to activate the\nconda virtual environment, please look to the prerequisites or resources section for links on\n# Make the `regression-with-scikit-learn-part-two` directory $ git clone https://github.com/okeeffed/regression-with-scikit-learn.git regression-with-scikit-learn-part-two $ cd regression-with-scikit-learn-part-two\nAt this stage, the file\ndocs/linear_regression.ipynb already exists and we can work off this material.\nBefore we start, let\'s go over the basics of linear regression.\nLinear regression basics\nThe line equation to calculates the linear line is described as the following:\nThe statement can be broken down into the following:\n|Parameters of the model|\nTo calculate the values of\nb, we need to define an error function (also known as the cost function or loss function) for any line and choose the line that minimizes the error function.\nThe aim is to minimize the vertical line distance between the fit line and the data point.\nThe distance itself is known as the residual. Because a positive and negative residuals (from data points above and below the line) will cancel each other out, we use the sum of the squares of the residuals.\nThis will be our loss function and is called Ordinate Least Squares (OLS).\nWikipedia describes OLS as the following:\nOLS chooses the parameters of a linear function of a set of explanatory variables by the principle of least squares: minimizing the sum of the squares of the differences between the observed dependent variable (values of the variable being observed) in the given dataset and those predicted by the linear function of the independent variable.\nGeometrically, this is seen as the sum of the squared distances, parallel to the axis of the dependent variable, between each data point in the set and the corresponding point on the regression surface — the smaller the differences, the better the model fits the data. The resulting estimator can be expressed by a simple formula, especially in the case of a simple linear regression, in which there is a single regressor on the right side of the regression equation.\nTo put that into some human speak, the\naxis of the dependent variable is our\ny axis, and so we sum the square between each data point in the set and the corresponding point on the X-axis to the regression line. The smaller the distance, the better the fit.\nWhen we call the\nfit method from our\nLinearRegression object, we are actually calculating the parameters of the line by performing OLS under the hood.\nHigher dimensions of linear regression\nSo far, the examples we have done are working on a dimension that is easily understood with\ny being calculated by one feature on the X-axis (from our example yesterday, this was the ""Number Of Rooms (feature) vs Value Of House (target variable)"""").\nHowever, in the real world, we often have more than one feature.\nTo calculate multiple features (or dimensions), our linear regression equation becomes the following:\nLinear equation for higher dimensions\nIn application, the Scikit-learn API can help us with this as we pass two arrays to the\n- Array with he features.\n- Array with the target variable.\nLet\'s do just that and see how it works.\nApplying the train/test split to our dataset\nIn our file\ndocs/linear_regression.ipynb, we can add the following:\nfrom sklearn.model_selection import train_test_split from sklearn.linear_model import LinearRegression X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) reg_all = LinearRegression() reg_all.fit(X_train, y_train) y_pred = reg_all.predict(X_test) print(reg_all.score(X_test, y_test)) # outputs 0.711226005748496\nThe default score method for linear regression is\nR squared. For more details, see the documentation.\nNote: You will never use Linear Regression out of the box like this. You will almost always want to use regularization. We will dive into this in the next part.\nToday\'s post spoke to the math that describes our linear line generated by the linear regression fit.\nWe then spoke about how this calculation is worked out with more dimensions added into the mix.\nFinally, we demonstrated this with a\nAs noted in the last section, this is not how you would use Linear Regression in practice. You will (almost) always want to use regularization.\nThis will be our topic in tomorrow\'s post.\nResources and further reading\n1,200+ PEOPLE ALREADY JOINED ❤️️\nGet fresh posts + news direct to your inbox.\nNo spam. We only send you relevant content.\nRegression With Scikit Learn (Part 2)']"	['<urn:uuid:02c80897-409a-4dd6-8e46-6efb3e70149a>']	factoid	direct	concise-and-natural	distant-from-document	single-doc	novice	2025-05-12T10:19:05.071190	10	52	890
17	I'm curious about lupus - what role do interferons play in causing it, and are there any promising new treatments being developed to fight this disease?	Type II interferon (IFN-γ) plays an essential role in the development and severity of systemic lupus erythematosus (SLE), as demonstrated by both spontaneous and induced models of lupus in animal studies. Regarding treatment, researchers are developing new targeted therapies focusing on the TACI molecule, which is key in driving BAFF-mediated autoimmunity. By targeting TACI on B cells, they aim to provide a safer treatment that avoids the serious side effects of current B cell depletion therapies while still preserving general B cell immune function.	"[""Systemic autoimmune diseases\nAbstract: The term interferon describes a family of proteins consisting of three major types (I, II, and III) which differ in their primary protein sequences, cognate receptors, genetic loci, and cell types responsible for their production. The interferons, including types I and II, overlap significantly in the genes they control resulting in a shared spectrum of diverse biological effects which includes regulation of both the innate and adaptive immune responses. As such, the interferons are major effectors in the pathogenesis of autoimmunity, especially systemic autoimmunity. The type I IFNs, because they are produced during the early stages of the innate immune response, are thought to play the foremost role in autoimmune responses. However, numerous studies have found that the single type II IFN, IFN-γ, plays an essential role in the development and severity of systemic autoimmunity, particularly systemic lupus erythematosus. This is supported by animal studies where IFN-γ is uniformly required in both spontaneous and induced models of lupus. Although expression of IFN-γ in cells of the innate immune system is almost immediate after activation, expression in adaptive immunity requires a complex orchestration of cellular interactions, signaling events, and epigenetic modifications. The multifaceted nature of IFN-γ in adaptive immunity identifies numerous possible therapeutic targets that, because of the essential contribution of IFN-γ to systemic autoimmunity, have the potential for producing benefits.\nThe term interferon (IFN) was first coined in 1957 to describe a factor with the ability to interfere with the growth of live influenza virus (Isaacs and Lindenmann, 1957). Since then it was found that the IFNs are a family of proteins consisting of three major types; Type I (IFN-α, -β, -ε, and -ω), Type II (IFN-γ), and Type III (IFN-λ1, IFN-λ2, and IFN-λ3, also called IL-29, IL-28A, and IL-28B, respectively) (Meyer, 2009). The three types differ in their primary protein sequences, cognate receptors, chromosomal location, and the types of cells responsible for production or response (Table 1).\nYou might also like\nYou have an overactive autoimmune systemby start_from_within\nFor sunblock, try something with Titanium Dioxide or Zinc Oxide. Stay away from Coppertone products, for the most part they are crap (except the ones with Titanium Dioxide or Zinc oxide, but even then there are other brands that are much better than coppertone).\nFor the overall autoimmune oversensitivity, try boosting your Omega 3 intake (through eating sardines or other fish like salmon rather than flaxseed. Although flaxseed oil that you get in the fridge section of vitamin stores are OK).\nAlso try evening primrose oil.\nTry to decrease your wheat consumption, replacing with alternative whole grain products like millet, quinoa, brown rice, etc\nContraception and Pregnancy in Patients with Rheumatic Disease\nVaccine Induced Immune Overload and the Epidemic of Chronic Autoimmune .. — The People's Voice\nFamily studies indicate type 1 diabetes is linked to the development of several different autoimmune diseases including organ specific autoimmune diseases and rheumatoid diseases.\nConnective Tissue Diseases: Holistic Therapy Options--Sjoegrens Syndrome; Systemic Sclerosis - Scleroderma; Systemic Lupus Erythematosus; Discoid Lupus Erythematosus; Secondary and Primary Raynauds phenomenon; Raynauds Disease; Polymyositis Dermatomyositis\nBook (Imprint Books)"", ""Developing new therapies for Systemic Lupus Erythematosus\n|Professor Fabienne Mackayemail@example.com||+61 3 834 42726||View page|\nProfessor Mackay's laboratory has an interest in autoimmune diseases and mechanisms leading to loss of immune tolerance, in particular that of B-lymphocytes. Systemic Lupus Erythematosus (SLE) is a debilitating autoimmune disease driven by aberrant B cells producing antibodies against self. This leads to inflammation and ultimately tissue destruction. Today we have no cure for SLE and the current treatments are symptom-focussed. Attempts to treat lupus by targeting B cells come at the high price of immunosuppression and opportunistic infections. Therefore, a more targeted approach is needed.\nOur lab has shown that the molecules BAFF and its receptor TACI play a vital role in autoimmune diseases. Excess BAFF leads to autoimmunity in mice and is also associated with human autoimmunity, in particular Systemic Lupus Erythematosus (SLE) and Sjögren's syndrome. We have demonstrated that TACI-mediated production of autoantibodies is key to drive BAFF-mediated autoimmunity and that loss of TACI on B cells protects from the disease without compromising B cell adaptive immunity in general. We are currently developing strategies targeting TACI to provide a therapy with a higher safety profile that avoids the serious adverse effects of global B cell depletion.\nConcurrently, we also investigate the effects of dietary changes on the development and treatment of autoimmune conditions like SLE as we have recently found that a high fibre diet has the potential to reverse disease and reduce pre-established auto-antibodies. Using various approaches we investigate how changes in metabolites and gut microbiota lead to the re-programming of immune responses in the SLE model.\nProfessor Fabienne MAckay, Lab Head\nDr William Figgett, Postdoctoral Researcher\nProfessor Chris Goodnow, Garvan Institute, Sydney\nProfessor Carola Vinuesa, John Curtins School, ANU, Canberra\nDr Stuart Tangye, Garvan Institute, Sydney\nDr Robert Brink, Garvan Institute, Sydney\nDr Matthew Cook, John Curtins School, ANU, Canberra\nDr Ansem Enders, John Curtins School, ANU, Canberra\nDr Jon Sprent, Garvan Institute, Sydney\nNHMRC Grant: Molecular and cellular basis of inflammatory and immunodeficiency diseases - 2012-2016\nNHMRC Development Grant: Development of a safer new treatment for systemic lupus erythematosus that preserves B cell immunity - 2016-2019\nThis research project is available to PhD students to join as part of their thesis.\nPlease contact the Research Group Leader to discuss your options.\n- Mackay F, Schneider P. Cracking the BAFF code. Nat Rev Immunol 2009; 9: 491-502\n- Fairfax KA, Tsantikos E, Figgett WA, Vincent FB, Quah PS, LePage M, Hibbs ML, Mackay F. BAFF-driven autoimmunity requires CD19 expression. J Autoimmun 2015; Aug 62: 1-10.\n- Figgett WA, Deliyanti D, Fairfax KA, Quah PS, Wilkinson-Berka JL, Mackay F. Deleting the BAFF receptor TACI protects against systemic lupus erythematosus without extensive reduction of B cell numbers. J Autoimmun 2015; Jul 61: 9-16.\n- Figgett WA, Vincent FB, Saulep-Easton D, Mackay F. Roles of ligands from the TNF superfamily in B cell development, function, and regulation. Semin Immunol 2014; Jun 26(3): 191-202.\nFaculty Research Themes\nSchool Research Themes\nFor further information about this research, please contact the research group leader.""]"	['<urn:uuid:dcd4f183-4ecd-4626-96c6-d7d788ad302d>', '<urn:uuid:1f56a309-58ea-4af2-9522-1cb5985d4f4a>']	factoid	with-premise	verbose-and-natural	distant-from-document	multi-aspect	novice	2025-05-12T10:19:05.071190	26	84	1020
18	farming data collection benefits drawbacks	Data collection in farming offers significant benefits but also presents challenges. On the benefits side, U.S. farmers collect extensive data through technology-enabled farm equipment, allowing them to track and improve their operations while demonstrating their sustainability practices to buyers. This data helps Consumer Packaged Goods companies improve their sustainability stories and build trust with consumers. However, there are also privacy and security concerns - Gradable's system has to ensure environmental transparency while maintaining security over farmer data, allowing farmers to anonymously share their information with grain buyers.	['Consumer packaged goods (CPG) companies across the globe are continually looking deeper into their supply chains for responsible and sustainable sourcing opportunities. They are looking for suppliers that share their values, and they need information to show consumers that they are a brand worth purchasing and trusting.\nThe U.S. soybean industry collects an enormous amount of data about how we run our business, from the field to the market. This information, in the proper format, can help CPG and food companies improve their sustainability stories. Given the right partnerships, the U.S. Soy sustainability platform can help food companies protect their reputation and grow their businesses.\nThis report provides a broad scan of the current U.S. Soy industry as it impacts international soybean purchasers and guides the work of the United Soybean Board and U.S. Soybean Export Council. Each target area section provides a summary of the market condition and considers challenges and opportunities identified for meal, oil and sustainability. In this section, our focus is on sustainability.\nU.S. SOY SUSTAINABILITY\nSustainability started as an environmental stewardship movement based on the concept of using natural resources responsibly to preserve them for future generations. Initially, concerns centered around water use, emissions and waste. For our part, the responsibility of the soybean industry was in decreasing our inputs, water, fertilizers, fuel, pesticides, etc.\nHowever, this early definition of sustainability has evolved. Sustainability now includes deforestation, other biodiversity issues, land use rights and working conditions.\nWhile our competitors are struggling with some of these issues on a global scale, for U.S. Soy, our challenges are more localized. For example, one U.S. growing region may have requirements for a watershed area or a protected habitat for a local species. But generally speaking, sustainability is where U.S. Soy has a competitive advantage.\nU.S. farmers have long embraced environmental best practices on their farms to lower costs and for the value it gave to their products, their land and future generations of their own families. Many U.S. soybean farms have been handed down from one generation to the next, each generation having a responsibility to use resources wisely to protect them and ensure the land’s viability for decades to come.\nThis commitment to responsible farm practices is why our customers and end users choose U.S. Soy. They know that the U.S. Soy industry invests heavily in research and data to help us do more with less, growing soybeans more efficiently.\nOur data is critical to our buyers because the world is putting pressure on them to also create more with less throughout their supply chain. They need products and ingredients that help them support their story, and that’s where U.S. Soy has an advantage.\nCPGs need our help to tell the story. U.S. Soy can and will be a partner to help CPGs and other end users share their positive sustainability story.\nConsumers want to know where their food comes from, and they want assurance that the person who produced their food shares their values. They want to know their food producers care about the environment and in reducing their impact to make the world a better place for future generations. This is the story U.S. Soy can tell every day, and every day we’re working to make it an even better story.\nInvestments in programs like Take Action have the potential to make an incredible and positive impact. Take Action, a farmer-focused education platform designed to help farmers manage herbicide, fungicide and insect resistance, puts the power of knowledge and recommended best management practices into their hands when making purchases and planning decisions for the next planting season. Take Action is a program developed not because farmers were told to do it, but because it is good for the farmers, good for the land and good for the product.This is part of the sustainability story we have to tell.\nU.S. farmers have a lot of data — technology has allowed for many of the pieces of on-farm equipment to generate and track data like never before. Add to that the research and data we gather as an industry through checkoff-funded projects, and information is abundant. The challenge is how we use that data to not only improve the work we do but also how we share the data in a meaningful way. The data show how the improvements we are making as an industry, and from farm to farm, make U.S. Soy a sustainable, responsible choice.\nLooking ahead, U.S. Soy knows the value our data can have for CPGs as part of their sustainability stories. We’ll need to find better ways to connect data sources and have them align so it’s easier to share information with our buyers in a way they can easily communicate with their customers. We need to aggressively stay “tuned in” to the fast paceof technology and the potential impact on farms.\nIn the U.S., we are fortunate to have an exceptional infrastructure system as a competitive advantage, which includes extensive railways, waterways and byways. Efficient infrastructure reduces the impact transportation has on U.S. sustainability data. But our transportation system has been working very hard for many, many years. U.S. Soy remains committed to continuous improvement of our infrastructure system, which is critical to meeting the needs of U.S. Soy customers worldwide.\nU.S. Soy’s Story\nTrust has become quite a valuable part of thebrand and product.\nCPGs are looking for partners to defend their reputations. When we protect our reputation, we protect those who buy from us. When we reduce our environmental footprint, we reduce that of those who buy from us. We should be more aggressive in understanding the reputation and need of our end users and customers.\nOne trending practice leading international customers and CPGs to source soy from high-risk areas is the adoption of standards and certifications. There is a laundry list of them: RTRS, 2BS, ISCC and RSPO, etc. The certification programs provide rules and criteria farmers must follow. When followed, the certification story follows the chain of custody to the repository, and the end user receives assurance that what they receive is a sustainable product.\nAround the globe, U.S. sustainability is verified by the U.S. Soy Sustainability Assurance Protocol (SSAP), an aggregate, quantifiable third-party verification process for sustainable soybean production.1The SSAP provides assurance for international buyers who are seeking a verification of sustainability by ensuring a compliance with a national system of laws and regulations. With more than 95 percent of U.S. soybean farmers participating in SSAP, to date, more than 16 million tons of U.S. Soy has been certified.1The participation in the program is used to determine the percentage of the U.S. Soy crop that meets the SSAP criteria.\nU.S. Soy continues to build on this program by aggressively seeking partnerships and communicating the benefits of SSAP from grower to international soy buyers.\nU.S. soybean farmers committed to sustainability long before it became a buzzword for consumers. They didn’t tell their story because their farm management practices were natural and ingrained from generations before them. But with a global focus on environmental responsibility and lessening environmental impact and footprint, U.S. Soy is in a position to help international buyers, CPGs and others tell an incredible story of a commitment to doing more with less and being a good steward of resources.', 'By Min Xu and Kurt Alles\nGradable is helping farms move to and be recognized for sustainable cropping systems. Sustainable agriculture means stewarding the resources farms rely on, including enhancing nitrogen use efficiency, reducing greenhouse gas emissions, improving water quality, investing in soil health, and maximizing farmer profitability. Over decades, producers have led the charge on developing new regenerative practices, namely cover cropping, reducing or eliminating tillage, rotating crops, incorporating livestock, and promoting biodiversity.\nAdvancements in and widespread adoption of agriculture technology on farms across the country have eliminated much of the burden of field- and, in some cases, bushel-level data collection. Machine data collected by combines and other farm implements now flows seamlessly from the field to the cloud, allowing farmers to monitor their farm and business health in more detail than ever before.\nConsumer demand for sustainably grown agriculture products presents a new opportunity for growers to be rewarded for their stewardship and conservation. Gradable enables farmers to anonymously share their data with grain buyers, translating operational complexity into identity preserved sustainability metrics that buyers value. This creates environmental transparency in the agriculture supply chain while maintaining security over farmer data.\nIdentity preservation in the agriculture supply chain means environmental characteristics can flow from field to consumer. As with organic or non-GMO, identity preservation requires a degree of verification. Gradable is streamlining verification and reducing the burden on producers to access new markets. Using computer vision and machine learning algorithms, Gradable processes satellite imagery and millions of acres of crop production data to analyze results and verify field activity.\nBelow, read more about Gradable’s methodologies for verifying cover cropping and tillage, both essential practices in regenerative systems.\nGradable uses European wide-swath, 10-meter resolution Sentinel-2 twin satellites imagery, weather data, and practice data, gathered from FBN®’s 21,000 farmer members comprising over 59 million acres, to develop models for verifying tillage practices and cover crop usage at the field level.\nFBN data on field-level cover crop use and tillage practices from 2017 to 2019 crop years were used as the ground truth in the model. Off-season (late October to early May) satellite data are sourced from Sentinel-2, and cloudy pixels are removed before image analysis. Precipitation data is used to account for soil moisture effects, and temperature data is used to track the cumulative heat units for cover crop development. Cash crop harvest and planting dates are estimated at the county level if unavailable.\nCover Crop Mapping\nThe model utilizes a time series of multiple spectral bands and vegetation indices from Sentinel-2 satellite imagery and weather data during the off season to create a classifier with 96% accuracy to examine spatial cover crop occurrences (Table 1). The intensity and range of winter greenness and timing of the greenness peak are the key features to train the cover crop classifiers. The training model uses data from Illinois, Indiana, Iowa, South Dakota, Minnesota, Ohio, Nebraska, Kansas, and Wisconsin from 2017 to 2019. Figure 1 shows examples of Sentinel-2 satellite imagery and Enhanced Vegetative Index (EVI) for fields with and without cover crops.\nThe challenges and misclassifications associated with cover crop mapping are related to cover crop establishment on cover cropped fields and significant weed presence in non-cover cropped fields. The type of cover crop being used also affects mapping accuracy, especially for cover crops that are not winter hardy, where availability of cloud- and snow-free satellite images can be a limitation.\nTable 1. Matrix of cover crops mapping at the field scale using 10-fold cross validation.\nFigure 1. Sentinel-2 satellite imagery and Enhanced Vegetative Index (EVI) show the differences between fields with and without cover crops.\nTillage Practice Mapping\nTo verify tillage practice, Gradable categorizes three different tillage practices (conventional tillage, reduced tillage, and no-till) using remote sensing. Normalized difference tillage index (NDTI) is used to distinguish field crop residue from soil based on cellulose and lignin absorption features found between 2100 nm and 2300 nm in the shortwave infrared region of the electromagnetic spectrum (Hively, et al., 2018). The time series of NDTI along with multiple spectral bands is analyzed for consistency and patterns to detect tillage practices on the farm. Figure 2 shows an example of NDTI change under no-till and conventional tillage practices.\nUsing FBN network data on field tillage practices, Gradable’s model classifies conventional tillage, reduced tillage, and no-till with an average accuracy of 77% and classifies no-till at 84% accuracy across 2017–2019. Data coverage includes Illinois, Iowa, Nebraska, Kansas, South Dakota, North Dakota, Minnesota, Ohio, and Wisconsin.\nUnlike cover crop classification, the accuracy of tillage practice classification is significantly impacted by soil and residue water content, especially under wet conditions. Moreover, the presence of green vegetation can also physically obscure crop residue. Crop residue type also affects tillage estimation, as the amount of residue left varies with crop type.\nTable 2. Matrix of tillage practice mapping at the field scale using 10-fold cross validation.\nFigure 2. Satellite imagery and NDTI from Sentinel-2 demonstrating the differences between different tillage practices (Top: no-till after corn; Bottom: conventional tillage after corn).\nAbout Farmer’s Business Network, Inc. and Gradable: FBN® is an independent agriculture technology platform and farmer-to-farmer network with a mission to power the prosperity of family farmers around the world, while working towards a sustainable future. FBN’s network consists of over 21,000 farmer members comprising over 59 million acres. FBN launched Gradable to provide technology and services to growers and buyers to facilitate the scoring, sourcing, and pricing of Low-Carbon Grain, building the infrastructure to make environmental transparency in the agriculture supply chain a reality.']	['<urn:uuid:3b4081f1-121a-4c71-b478-12dd8652915e>', '<urn:uuid:973f9d56-f4a5-46c4-9c2f-953f6ca02af7>']	open-ended	direct	short-search-query	distant-from-document	multi-aspect	novice	2025-05-12T10:19:05.071190	5	87	2141
19	I've noticed different challenges with boat positioning in both situations - how do wind and current considerations differ when you're trying to maintain position during anchoring versus during the final approach to a dock?	When anchoring under sail, wind management focuses on keeping the bow pointed into the wind during anchor deployment, with specific concerns about the boat potentially falling broadside to the wind after losing forward way. The boat's keel type affects this behavior - fin-keeled boats may fall off more readily than full-keeled ones. For docking, wind and current must be factored into the approach vector to the slip, with all passengers needing to remain seated to maintain stability. In both cases, observation of conditions is crucial, but anchoring allows for multiple passes to assess conditions, while docking typically requires a more direct, committed approach.	"['In four years of cruising aboard MonArk, we’ve lost our engine twice. On the second occasion, in San Francisco Bay, the alternator seized and we were faced with the prospect of anchoring under sail. At the time, dozens of questions came to mind. How quickly will the boat decelerate when turned into the wind? Should we enter the anchorage under mainsail, headsail, or both? How long will it take MonArk to tack through the wind under headsail alone? Of course, we’d have had the answers to these and other questions had we previously taken the time to practice anchoring under sail.\nAfter that incident, we were sufficiently motivated to invest a little time in practicing these skills. In fact, we now make anchoring under sail a part of our regular sailing routine, though always with the engine on so we can quickly bail out if the situation gets hairy. Along the way we’ve learned a few things, and have developed routines that work well for us when setting or getting off the hook under sail. Other sailors on other boats might need a different approach, so we recommend all sailors experiment and practice to establish routines that work the best in a variety of situations. Before getting into the mechanics of how we anchor under sail, I want to call out three hard-won lessons that make all the difference for us: slow down, observe the conditions, and discuss the game plan. Whether sailing on or off the anchor, we take our time to make sure we have everything prepared. Are all the sails ready to be doused? Have we taken a couple of passes around the anchorage to find the right spot? Next, we take note of our surroundings and observe the conditions. Where are the wind and waves coming from? How much water do we have under the keel? Is there a current? Where are the potential hazards? Lastly, we huddle up as a crew and discuss the game plan. Are we both clear on the plan, as well as on the backup plans and the role they will play before we commit? It’s tempting to rush (especially when it feels like the whole anchorage is watching), but a bit of planning and communicating makes things go a lot smoother.\nAnchoring under sail\nTo select a good spot to drop the anchor, we sail slowly around the anchorage a couple of times to get a sense of the conditions, note the locations of other boats and their anchors, and plan our approach.\nMaking the approach – Having experimented with different sail plans, we find anchoring under main alone works best for us as it’s usually just the two of us on board. We learned early on that our fin-keeled, sloop-rigged boat has a tendency to fall off broadside to the wind after it loses forward way and drifts back as we set the anchor. If she falls far enough off, the boom will lie against the shrouds, allowing the main to fill and send the boat sailing off its anchor. That’s not a lot of fun in a crowded anchorage in a 30-knot wind— as we can attest from experience! Aware of this tendency, we get the main down quickly. A full-keeled boat might not have the same issue, and on a ketch or a yawl the mizzen can be left up to act as a wind rudder to hold the bow into the wind. One technique does not work for every boat and crew. As we approach the drop location, I furl the headsail from the cockpit to slow the boat (3). I then bring the boat up into the wind and luff the main (4). If the boat has too much speed and appears likely to overshoot the intended mark, my husband, Robin, (at the bow) holds the boom to windward, backing the sail to put on the brakes.\nSetting the anchor – As soon as the boat is no longer moving forward, Robin drops the anchor (5) and, as we drift backward, slowly pays out chain until the scope is 3:1. He keeps a very slight tension on the rode as he deploys it, using the resistance of the anchor and chain to keep the bow into the wind. Ina light breeze with no current, I’ll go on deck and back the mainsail to help push the boat sternward. Once Robin feels a strong tug indicating the anchor has dug in, he cleats off the rode and comes aft to help me bring down the mainsail (6). In situations where the wind or current is strong, he will pay out additional rode (up to 5:1) before coming aft to help with the main.\nStriking the mainsail – Together, we quickly drop the luffing mainsail and tie it to the boom. Once that’s done, Robin pays out the remaining scope and we wait for the boat to settle with her bow pointing into the wind. We then put our sail away properly (7).\nSailing off the anchor\nIn preparation for sailing off the anchor, we first raise the main and prepare the sheets and furling line for the headsail so it’s ready to be unfurled (1). We also look around the anchorage to determine what our favored tack will be as soon as we are off-anchor.\nSailing up to the anchor – Robin brings in the rode while I tack back and forth, beating as close to the wind (and anchor) as I can (2). As MonArk sails away from the chain on one tack, the tension gradually increases until it pulls her nose around into the wind. When this happens, I tack her (3). This takes tension off the rode, making it easier for Robin to pull it in.\nBreaking out the anchor – Reading the markers we’ve attached to our chain, Robin gives me a signal once he’s brought in most of the chain and is at the point of breaking the anchor out of the bottom. At this point, I place the boat on the favored tack and Robin hoists the remaining rode (4).Unfortunately, the action of breaking out the anchor sometimes brings the boat head to wind, in irons. When this happens, I release approximately 6 feet of the headsail furling line. Robin pulls the sail out by hand and holds the clew to windward to bring us back onto the favored tack while I get the leeward headsail sheet ready. Once we’re on the desired tack, I sheet in the headsail and Robin hauls up the anchor (5).\nAs we learned from our episode in San Francisco Bay, the safest and most stress-free time to practice sailing onto and off the anchor is while the engine is still working! So get out there and practice before your engine fails. You’ll earn kudos from spectators as you breeze out of the anchorage under sail. More important, you’ll have the confidence to do it safely when your engine leaves you no choice.\nThis article was originally published in Sep/Oct 2017 issue of Good Old Boat magazine. For more articles on seamanship and boating skills, pickup a subscription to Good Old Boat.', ""What Is Recommended When Docking a Boat?\nThere are many aspects of boating; many rules and regulations that need to be maintained while you are boating, much like when you are driving your car on the road.\nThese rules and systems are there set in place to make your lives easier and to ensure that you don't fall in any problem in the water.\nYou will have to follow a system or process when you want to dock your boat. The task might seem intimidating if you happen to be new to boating or even if it is your first time docking a boat. If you keep your head in the task, then it shouldn't be tough.\nSo, let’s get to know the essentials of docking a boat.\nWhat Should You Do to Dock a Boat?\nThe steps you have to undertake are relatively easy, and you shouldn't have much of a hard time following along; you just have to make sure you don't miss out on any of the steps. Also, keep in mind that it might take you some amount of time to get the hang of things, so be patient.\nStep 1: In the very beginning, you have to arrange the dock lines on your stern and bow, then you have to attach or connect your fenders.\nStep 2: Lining your approach is your next step; you also have to keep your eyes opens and be on the lookout around the docking area.\nStep 3: While docking, the condition of the water is very important, this goes for most aspects of boats. So, you have to check the wind and the current before you do anything.\nStep 4: Now is the time you have to move ahead towards the dock. Move-in slowly, there is absolutely no need to rush into it, as you might hit the dock or other boats. Use intermittent acceleration to get the job done.\nStep 5: You should be very careful while you approach a dock, as there is a very high chance you might hot it. Sometimes the speed can't be controlled, and you do not want any accident during this time.\nStep 6: When you have approached the dock, you have to navigate your way into the boat slip where you can keep or park your boat.\nStep 7: And finally, as your very last step, you have to tie up your boat to the dock with the use of the docking lines. Make sure to do it tightly so it doesn’t come loose otherwise your boat might float away.\nWell, now you have known the basic how-to when it comes to docking your boat. There are obviously many other things you should consider or keep in mind, but for a beginner, these should be enough.\nHow to pick the right pontoon boat?\nCauses of Most Boating Accidents?\nWhat Should You Do When Your Boat Goes Aground?\nHow to Dock in a Slip?\nIf you happen to be someone who is very much into boating, then docking your boat in a slip is going to be a very common occurrence in your life; whether you like doing it or not. You will have to do this in both private and public areas.\nNow, as a boater, you should always have two things with you prepared ahead of time so that you don't face any difficulties when you are docking. The two things are, your fenders and your docking line on both sides of the boat. This will make the whole process easier for you.\nLike I have said before, surveying your surroundings and being on the constant lookout is essential if you want to avoid all incoming threats on your way. You will be able to save yourself and those in the boat with you if you are aware of what is happening around you.\nWhen you are about to enter into a slip you are given very little space to maneuver your boat around, so you have to be cautious and make sure you don’t hit the dock or another boat in your quest. Keep the speed at a minimum, as there is no way or scope for you to make a mistake in this situation.\nWhile easing or reversing into the slip, make sure all the passengers are in their seats and not moving around, as it might rock the boat and therefore, a bit of the control will be lost in the process. So, for the safety of the passengers as well as the stability of the boat, everyone should be seated.\nWith the last bit of power, move forward to stop your boat from going back or going in reverse. And as that should be done in a hurry, don't forget to tie the boat up.\nHow Can You Tie a Boat to the Dock?\nThe best and the most widely used method or way of tying up boats to the dock by most boaters is Cleat Hitch. This particular type of knot is so popular as they tend to be easier to both tie and untie, but it will also be able to hold the boat to the dock. This can surely secure your boat, and you will not have to worry.\nNow, how will you tie the Cleat Hitch? At first, you have to take your line and wrap it around the cleat's base once. Once you have done this one time, then you have to do the same thing in the opposite direction. Make sure to bring the line over the top of the base.\nThe next step for you would be to go over the arm again with your line and wrap it around to create a figure-eight, both around and over the cleat. Then you have to make an under loops which you must slip through the arm. By doing so, you will make sure that the hold is secure.\nAnd at last, just pull the end as tight as possible to complete the task.\nThere are a few other ways with which you can tie your boat to the dock. You can employ any of them to get the job done.\nA Few Additional Tips:\nThere are a few tips and tricks here for you which you can follow if you want to make the whole experience easier for you.\n1. Don’t Get Distracted\nDo not let people distract you. Many times, you will see that there is someone on the dock who is trying to offer you advice and telling you how to navigate the boat. This might actually end up distracting you more than helping you out. So, don't let them get you into a mess.\nWhile operating the boat, you should be aware and know what must be done. If you are new to this, then just take some time to make your own way.\n2. Approach the Dock Slowly\nYou need to slow down! This is something which has been mentioned before, but the importance of it still stands. You will not be able to approach the dock right if you are speeding. A slower speed will give you more control, and therefore it is safer.\nNot only do you have to keep the speed at a minimum, but you also must make sure that there are no sudden bursts of power. This will firstly rock the boat, and you will also lose control over what you are doing. You might even hit something in front of you.\n3. Leave the Boat If Things Go Wrong\nJump off if things are not going as you planned. There will be times when things will not be going the way you planned, and therefore you will foresee something bad happening to you or the people on the boat with you. In this case, your first instinct should be to abort.\nYour lives are obviously more important.\nDocking a boat is not a very difficult task if you can follow along with the instructions provided. And once you get the hang of it, all of this will become like second nature.\nLast Updated on March 9, 2021""]"	['<urn:uuid:e3edfa5d-e80c-4d7a-a292-65a465cad628>', '<urn:uuid:f5648ace-9f0d-4aec-b168-d1f8df098ee7>']	open-ended	with-premise	verbose-and-natural	distant-from-document	comparison	expert	2025-05-12T10:19:05.071190	34	103	2554
20	When applying sand infill to a home practice putting surface, what's the final step to achieve the correct turf exposure for optimal ball roll?	You need to sweep across the grain to remove any excess infill and expose 1/32-inch of the turf.	"[""It's long been observed that the quickest way to subtract strokes from your game is to improve your putting. While nothing will substitute for the real thing, building your own putting green is a wonderful way to easily practice from the comfort of your own home. By using turf, you can recreate the feel of a green without the intense groundskeeping required of actual golf courses. For some, this may seem like a daunting and difficult task. It is quite manageable if you have the right tools and landscaping know-how. It is quite labor intensive, however, so remember that before starting the project. When done correctly, building your own artificial turf putting green can bring much fun and enjoyment for the entire family.\nDifficulty: Moderately Challenging\nMap out your area. Find an area in the backyard (or wherever you are doing this) that is big enough for this project. Ideally you want it to be the same shape and size of a traditional putting green, but a more compact straight green will also work. Also, find an area that sits away from any structure and doesn't collect water when it rains.\nClear out the areas. Remove all objects, including grass and other debris. You are now ready to shape the green and loosen the soil. Shape the green for how you will put down the turf that will house the putting area.\nWet the dirt only slightly. Use a plate compactor to compress the damp soil until it is smooth and firm. Take the landscape fabric and roll it over the compressed soil.\nEdge the green. You can do this with an edging or block border, whichever is more comfortable to handle.\nPlace your crushed stones or other base materials. Distribute the materials evenly over the landscape fabric with the metal rake. Make sure that the green is properly sloped 1 inch every 12 feet. Repeat step 3 on the base material this time, instead of the soil.\nInstall cups in the base material. Do this by digging holes that are 2 inches wider and 6 inches deeper than the cup. You will need to install two or three cups. Once you are done digging the holes, set a cup in the middle of each hole and pour quick-drying concrete around them. Be sure to leave at least 1 inch of the cup left exposed. Wait until the concrete is dry and cover the cups with more base material.\nSmooth out any ridges in the base material with a shovel. Coat the smoothed base material with sand. First drop the sand with a drop spreader then sweep it across the base.\nLay the turf roll. Start at the edge of green and continue to roll over the entire green, making sure to smooth out all the creases. Join any pieces with outdoor adhesive or tape if your green is wider than 12-foot turf roll. Tuck in the edges.\nGet out the drop spreader again and spread infill sand over the turf. Make sure that the sand is properly distributed by sweeping it. You may have to repeat this step several times until the green is fully filled with infill.\nRemove any infill from the green. Do this by sweeping across the grain. Make sure to get any excess and expose 1/32-inch of the turf.\nFind the spots where you installed the cups. Take the utility knife and cut around the inside of the cup to remove the turf in these areas. Be sure to trim the edges.\nFinish up by rolling the green with a water-filled roller. This is very important to ensure the green's speed, so roll several times.""]"	['<urn:uuid:a50ad3c2-eefa-425d-9645-8ce2ea5d9b40>']	factoid	direct	verbose-and-natural	distant-from-document	single-doc	expert	2025-05-12T10:19:05.071190	24	18	610
21	how high mountain vineyards armenia grow	Armenian vineyards grow at remarkable elevations, ranging from 2,000 feet to 5,700 feet above sea level, making them among the highest vineyards in the northern hemisphere, with only a small patch in Colorado being comparable. This high elevation, combined with Armenia's extreme continental climate, creates unique growing conditions that distinguish Armenian grapes and contribute to their distinctive characteristics.	['Armenia holds a remarkable depth and diversity of cultural and natural landscapes, all within an area approximately the size of the state of Maryland. This diversity and richness is evident in music, cuisine, art, and—increasingly—Armenia’s burgeoning wine industry.\nWhat makes a wine Armenian? I posed this question to Irina Ghaplanyan and Vahe Keushguerian, two wine professionals passionate about the country’s current wine renaissance. Ghaplanyan represents Vineyards of Armenia, a group of almost a dozen winemakers from across the country, including both boutique and large-scale wineries. Keushguerian is a winemaker and the owner of Semina Consulting, a winery consultancy based in Yerevan. He has been involved in the wine industry for over twenty-five years, first in the San Francisco Bay Area as a wine importer, then Tuscany, Puglia, and now Armenia.\nGhaplanyan and Keushguerian emphasized the unique grape varietals indigenous to Armenia and the country’s exceptional terroir, the characteristics that soil, weather, climate, and other environmental factors impart to the grapevines. Armenian wine is also connected across time with a more than six thousand-year-old history of viniculture.\nIn 2011, researchers from UCLA and the Armenian Institute of Archaeology and Ethnography discovered the oldest winemaking facility in the world, dating from approximately 4100 BCE. This Areni-1 cave complex sits at the mouth of a gorge leading to the medieval Noravank monastery, at the outskirts of Areni village.\nThe discoveries at Areni-1 place Armenia at the fore of an emerging “Historic World” of wine, including Georgia, Iran, Israel, Lebanon, Turkey, Jordan, and Syria. Working with Historic World colleagues, Armenian wine geneticists, archaeologists, and producers are rediscovering ancient varietals that are the ancestors of today’s wine varietals. In a region fraught with conflict, this is a kind of “wine diplomacy,” bridging contemporary geopolitical borders.\nGhaplanyan and Keushguerian recently traveled to northwestern Iran, near the Hajji Firuz site where the oldest winemaking related evidence has been discovered, to search for grape varietals in abandoned vineyards.\n“Genetic analysis found that four of these varietals had unique genotypes, which is fascinating, and another clue to understand the period of grapevine domestication, and also the way it began migrating to Europe,” Ghaplanyan remarks. These discoveries may “explain how they migrated and the links between the current European varietals and the varietals we have here.”\nArmenian grape varietals, like Areni, Kakhet, Voskehat, Khatun Kharji, Movuz, Sireni and Chilar, are still relatively unknown outside of the region. Indigenous to the area and distinct from both the European Old World and New World varietals, these are the grapes that, as Keushguerian puts, “would perplex a wine professional.” He continues, “Their flavor profile is different than what you’re used to—not too different, but distinct. The sensation is different.”\nGhaplanyan and Keushguerian are propagating many of these vitis vinifera varietals in nurseries. They grow rediscovered varietals and produce wine in a process called “microvinification,” creating wines from very small batches of grapes to learn more about the different flavor profiles possible from these ancient grape types. One of their discoveries is the sheer number of flavor profiles possible from only one varietal.\n“If you compare different grapes, there’s usually a set amount of flavor profiles that a grape can produce,” she explains. “With the Voskehat grape, the aroma profiles and complexity are much higher than average. It is also a very terroir-driven grape. In 2013, a colder year, it acquired a very stone fruit flavor, very peachy, apricot. 2014 was a lot warmer, so we had tropical notes from banana to pineapple to passion fruit.”\nSituated at the intersection of the European and Arabian tectonic plates, Armenia has frequent seismic activity. The resulting volcanic soil is ideal for grape cultivation, enriching terroir. Georgian wines also benefit from this soil type, but Armenia’s higher elevation and extreme continental climate distinguishes its grapes. Ghaplanyan says very hot summers and very cold winters give the grapes boldness and “a certain maturity.”\nArmenia’s extreme climate is surprising when you consider its latitude—roughly the same as Sicily and Mt. Etna. The Armenian highlands produce a range of microclimates, and diverse grapes grow from the lowest elevations of 2,000 feet above sea level to the highest at 5,700, where Keushguerian grows the grapes for his Keush sparkling wine.\n“What we’re doing is something like ‘extreme winemaking,’” he says. “We are pushing the limits of the frontier. Armenian vineyards are some of the highest in the northern hemisphere, apart from one small patch of vineyard in Colorado.”\nThis elevation also breeds a high concentration of polyphenols. These chemicals, which may be linked to prevention of degenerative diseases, lend Armenian wine “a higher structure, a certain je ne sais quoi … a certain elegance similar to northern Italian wines,” as Keushguerian describes.\nThe extreme climate also breeds grapes with high acidity, so there is no need to add tartaric acid during the winemaking process, which is common with wines of the New World. High acidity without additives made early winemaking possible, when early inhabitants of the South Caucasus may have used wines for ritual purposes.\nWinemaking in Armenia dates from at least 4100 BCE to the present day, but Armenian viniculture has been disrupted throughout history by imperial conquests, political revolutions, and shifts in society. During Soviet times, and especially when Stalin came to power in the 1930s, the government asked Armenian winemakers to shift production toward cognac and European-style fortified wines (i.e. sherry, madeira) instead of table wines. This system rewarded quantity rather than quality.\n“We can give one credit to the Soviets,” Ghaplanyan reminds. “They created collective nurseries, where they would preserve the historic varietals. They didn’t industrially use them because they weren’t as weather resistant, but they didn’t ignore them. They had a collection which we lost during the collapse of the Soviet Union.”\nInstability, privatization, and conflict characterized this collapse in the late 1980s and early 1990s, and the wine industry lay dormant for years. The vineyards, which need steady and constant care, fell into neglect until the late 1990s.\nSince 1998, they and their fellow winemakers have been rebuilding and expanding the Armenian wine industry, and today there is a renaissance. The 2010 vintage was a watershed year, and Yerevan now boasts almost a dozen wine bars, while Armenian wines are appearing on global top ten lists.\nGhaplanyan and Keushguerian see this evolving industry as a boon to local communities. As wine quality improves, so does the value of the final product and the grapes themselves. Within a more profitable economy, many farmers who have been steadily leaving for work in Russia might be able to afford to stay on their land. In marginal plots of land, on hillsides dotting the Armenian highlands, winemakers, communities, and families are cultivating the future of Armenia—by revisiting and reinterpreting its historic grapevines.\nVahe Keushguerian and Irina Ghaplanyan will present a selection of Armenian wines with the Smithsonian Associates on May 18, including a tasting of Voskehat.\nRebecca Wall is a historian and fellow with the Smithsonian’s Office of International Relations, where she works on international programs including My Armenia.']	['<urn:uuid:42274d51-2da9-45c2-bb77-a4e61767da75>']	open-ended	with-premise	short-search-query	distant-from-document	single-doc	novice	2025-05-12T10:19:05.071190	6	58	1170
22	What safety issue exists with venting fuel tank into return line?	Venting the fuel tank into the pressurized return line is dangerous because any loss in height of the line would cause hot diesel to be pumped directly into the engine compartment.	"['Keeping the Fuel System Clean\nHere is a partial view of the diesel tank in our Prout Escale and the four lines that penetrate it. From left to right: Tank tender, diesel return from engine, diesel to engine, diesel furnace.\nThe tank-tender (a level sensor) for the diesel tank didn\'t work properly until we discovered all the built-in kinks in the line and removed them. The lines running to the water tanks had fewer corners to snake around, so they tended to kink less.\nThe return line carries unused diesel back from the engine. The diesel intake now runs to a RACOR filter mounted in this compartment instead of the engine compartment where Prout had installed the OEM Lucas unit.\nNot only does the RACOR unit incorporate a priming pump, it also offers much easier access to the filter elements - the Lucas unit was mounted so out of the way that access was only for the very limber and long-armed. We replace the primary and secondary filter elements every year, even though the engine only runs less than a hundred hours on each one.\nThe single line that carries fuel to the diesel furnace is a 3/16"" OD copper line that we installed when we discovered that Prout Catamarans had used plastic when they installed the old Espar furnace. Note how the first three terminals were welded into the tank, while the third consists of a screw-on terminal not meant for marine installations (i.e. it\'s not made of stainless and rusting). The bulkhead at the bottom of the page separates the engine and diesel compartments.\nWe have been thinking hard about our Diesel tank options since the 28 gallon diesel tank installed by Prout Catamarans is totally inadequate for anything but coastal sailing. Given our thirsty engine, it runs dry rather quickly. We plant to retrofit larger tanks in the rear crash compartments where they would help counteract the constant list of our Prout Escale while at anchor without the dinghy.\nBesides retrofitting ""reserve"" tanks, we also had to replace the diesel\ndelivery and return infrastructure installed by Prout Catamarans. I have\nnever been a great fan of fixed copper pipe in boat installations. Our\npipes were soldered together with lead and were degraded quickly. Fuel\nhose is more flexible, can be installed in one seamless piece, and will\nnot degrade as quickly as copper (particularly if you use the gasoline-grade\nhose in a diesel installation).\nSo while we fixed the diesel intake line when we installed the Racor filter, we waited with replacing the diesel return line until we pulled the engine for maintenance. This was a big mistake since we would have discovered another classic Prout Catamarans construction ""feature"".\nThe return line carries the portion of unused but pressurized (and\nhot!) diesel back to the tank that the engine did not use. Prout Catamarans\nopted to incorporate the vent for the diesel tank INTO\nTHIS LINE. Never mind that the line is pressurized and that any loss\nin height of the line would cause LOTS of HOT diesel to be pumped directly\ninto the engine compartment! Diesel may have a fairly high ignition\ntemperature but this still does not excuse the hair-raising installation\nwe have witnessed on our Prout Escale. Furthermore, the US Coast Guard\ndoes not care if a construction failure made your boat pollute the ocean\n- the fine is yours to pay.\nWe discovered the vent the hard way when it dropped 1"" below installation height during engine maintenance. Luckily, since the engine hatch was open, we noticed the spilled diesel quite quickly - but the soundproofing in the engine compartment had already absorbed a good amount of diesel which had to be painstakingly squeezed out of it.We replaced the copper return line with a rubber one and are currently evaluating better fuel tank venting options. Is it me or is it strange to vent a fuel tank into the engine compartment instead of venting it externally?\nOur current diesel tank now has a rubber return line, and the proper tube and filter for the diesel furnace. Last but not least, we installed a pull-handle into the hatch for the diesel compartment. It makes accessing this area much easier.\nSuch a handle is water-proof when installed properly, so there is no\ngood reason not to install one in hatch surfaces that otherwise risk\nfingernails to be opened.\nBest Estimate for Time Required:\n|Remove old filter mounting, re-plumb hosing||1 hour|\n|Install RACOR unit, bleed Diesel system||1 hour|\n|Install Copper Line for Furnace System||2 hours|\n|Discover brain-dead vent installation||2 minutes|\n|Squeeze Diesel out of Soundproofing||1 hour|\n|Retrofit proper vent hose||2 hours|\n|Repair built-in kink in tank tender unit||1 hour|']"	['<urn:uuid:0af4c457-6235-416b-8630-3e6e2808dc5e>']	factoid	with-premise	concise-and-natural	similar-to-document	single-doc	expert	2025-05-12T10:19:05.071190	11	31	783
23	What are cities' ancient origins and current housing challenges?	Cities originated around 10,000 years ago with agriculture and permanent settlements, with places like Jericho dating to 9000 BC. Today, both urban and rural areas face housing challenges - cities have only 42 affordable units per 100 low-income renters, while rural areas have higher homeownership (81% vs 60% in urban) but struggle with poor quality housing and lower median incomes.	['“A city is more than a place in space, it is a drama in time.”\n- Patrick Geddes, first urban planner, Cities in Evolution (1915)\nIn the year 1800, only 3% of the world population lived in cities. Today there are 4,000 large cities worldwide. By 2030, it is predicted that 60% of us will live in cities—and some very large ones. The 33 megacities, with 10 million or more, (UN, 2018), are growing, many in Asia. The great majority of human beings, 75-85% of the populations of North America and Western Europe, now live in major cities and their metroplex extensions, as does half the world population. Rome passed the one-million mark by the end of the first century BC, then declined when the Western Empire’s capital moved to Ravenna in 402 AD.\nBy 1850 London became the first city after antiquity to attain a population of a million; fifty years later, in 1900, Greater London had 5 million people.\n60,000 years ago, modern humans had populated Europe. The Holocene period saw agriculture and domesticated animals—and the appearance of Jericho, an ongoing small walled settlement on the West Bank that dates to 9000 BC. Around 5000 BC, people began to live together in formal groups with work and social roles in permanent settlements—like Athens in Greece, and Byblos in Lebanon. In Serbia, Belgrade remains one of Europe’s oldest cities, with settlements back to the Neolithic in 7000 BC. Plovdiv, Bulgaria is nearly as old, back to 6000 BC. Towns and cities began to form the beginnings of urbanity, predating the Bronze Age back to the Neolithic.\nA social psychology of urbanity developed as the mindset that we now recognize as civilized thought and behavior. The proximity principle – the propinquity effect that contributes to attraction between people--also requires moderation of behavior, voice and speech, manners, working and leisure behavior—like sports--and the management of complex networks of people, activities, and beliefs. Religious affiliation is an example of a continuously self-reinforcing colleagueship that builds community and subcultures. But group continuity is also a lightning rod for conflicts between groups. Clashes between and among Christian, Moslem, and Jew, and between their many sects, date from their founding and are ongoing global forces.\nThe practice of group living allowed specialized knowledge that led to the metamorphosis of small settlements into larger and larger groups, cities, and regional civilizations with lasting influence on human development. As the historian Jacob Bronowski noted in The Ascent of Man, agriculture and the settled way of life engendered “a form of human harmony which was to bear fruit into the far future: the origin of the city.” In the Mesopotamian world of Turkey, Iraq, and Syria, the first networked urbanization occurred among city-states, leaving the largest set of ancient artifacts, the invention of writing, the wheel, the 60-based counting system, and large-scale agriculture. In the Indus Valley developed water management and drainage, harvesting routines, and town planning. Egypt produced the pyramids, astronomy for horticulture, and engineering. On the Yucatan Peninsula, the Mayan world evolved astronomy and calendars, large-scale agriculture, and engraved stone architecture. All these developments called for management science. Cities are the curators of human knowledge and the care and feeding of that knowledge—the art and science of knowledge preservation and transmission.\nThe city is the original and ultimate mixed-media creation including augmented reality, arts and architecture, customs and etiquette, words and images, ancient craft and state-of-the- art technology, life and art. Every city is a multi-layered reality that presents with novelty and dynamism on one hand, as well as established structural, infrastructural, and natural or peri-natural features on the other. Trees, parks, hillsides, mountain, field, beach and ocean backdrops –these landscapes are there as staging, folded into the city’s design as ornamentation. The city’s setting changes the way we look at nature as aesthetics, a kind of design operating ancillary to streets, monuments, buildings, open spaces, and the city profile, the skyline. These are in constant flux.\nCities are complex permanent exhibits that, like museums and theme parks, invite us back by steadily updating their content and presentation. Commercially, we see this everywhere, from supermarket shelves to the showcase windows of Tiffany’s and the bakery case at Starbucks. The human pageantry—the population who live in and use the site--is the theatrical vitality that makes everything come alive as what Disney Imagineers call “streetmosphere,” the cityscape of human drama. Just by sheer numbers alone, the myriad factors that make up a city can be combined and recombined to recapitulate the history of the world and prefigure the future as a design for living. Along with their human inhabitants, cities showcase diversity across many categories—as fashion, music, food, literature, the arts, industry, language, religion, and learning. This diversity extended beyond the arts in the form of intermarriage between groups, mixing the gene pool and raising opportunities for new adaptations that hastened evolutionary advantage.\n10,000 years ago, agriculture as a revolutionary way of life for communities made cities possible, as did language, writing, money, science, and universal religion – forming the first shared cultural platform that would make living together possible. The world’s oldest surviving city is, by a margin of millennia, Damascus in Iraq, the first sizeable city of 2 million still standing (Jericho, which may be the first town, dating from 9000 BC, is a small settlement of 20,000 today). Just as agriculture at a distance (the “hinterlands”) made the city possible, high and mobile tech have made of the megacity the smart and super-extended city-state, like Singapore or Dubai. From their origins as protective fortresses that were adaptable as well as livable, cities have blossomed into the megastructures we know today that carry forward the cultural mandate to evolve our thinking and expression.\nAs cities began to proliferate, they promoted large-scale cooperation, density, and diversity, and role-identity versus just personality differences between citizens. From 4000 BC the urban lifestyle has given us 6,000 years of living with the stressors of hierarchy, activity schedules matched to commerce, worship, sociability, industry and knowledge work, and seasonal events, sports culture, and the natural strain of strangers stressing each other out by constant needs to be mind-readers, wary and watchful about the behavior and motives of people with different agendas. Unlike small Stone Age village life, cities mean daily contact with strangers, people who serve very contained work roles, and a 24-hour media presence —with whom we have no ongoing personal relationship but a steady psychological one.\nThe urban way of life kicked off networking, social striving, and multiple associations, and constant change as an expected part of human existence, as well as placemaking, managerial and middle classes, and consumerism as the “urban species” lifestyle (Monica L. Smith, Cities: The First 6000 Years, 2019). Smith calls cities “the first internet.”', 'This is the fourth post in a series about reducing the divide between urban and rural communities.\nTo bridge the gap between urban and rural areas, we first need to understand what sets these communities apart and where they have common ground. Below are a few key stats about urban and rural populations, economies and housing, and the different types of critical challenges these communities face.\nDespite the vast amount of discussion about the urban-rural divide, there is actually little agreement about what these terms mean. The most common definitions are from the Census Bureau — “mostly urban,” “mostly rural” or “completely rural” — and the Office of Management and Budget (OMB) — “metropolitan” or “non-metropolitan.”\nThe problem with these definitions is that more than half of the country’s rural population, as defined by the Census, lives in less densely populated parts of OMB metropolitan areas.\nNo matter how you slice it though, the clear majority of Americans live in urban areas. This share continues to grow as people move from rural to urban regions. Those who live in rural communities tend to be older (with an average age of 51 years, versus 46 in urban communities) and less educated (20 percent with a bachelor’s degree versus 29 percent in urban communities).\nRural and urban unemployment rates have improved in recent years, but labor force participation — the share of the population working or seeking employment — remains below pre-recession levels. Specifically, the gap between urban (63 percent) and rural (59 percent) labor force participation is significant and largely attributed to an aging rural workforce. Additionally, most rural communities still have not recovered the jobs they lost during the recession.\nHowever, not all rural communities are the same, and some are outpacing the growth of urban areas on key economic indicators. For example, many rural areas have higher rates of entrepreneurship, and the National League of Cities’ (NLC) own research found that businesses that export their goods and services are thriving in rural communities. Rural areas in many states are also making outsized contributions to their states’ GDP.\nDespite their differences, affordable housing is a prevalent concern amongst both urban and rural communities. Across the U.S., only 46 units are available for every 100 extremely low-income renter households. The problem is more severe in cities, which typically have only 42 units per 100 low income renters. Rural area housing challenges are compounded by the fact that residents typically have lower median incomes and available affordable housing is often poor quality.\nRural residents have higher homeownership rates (81 percent) than urban residents (60 percent). Younger adults are both more likely to rent and more likely to live in urban areas. Rural areas have an older demographic, whom are both more likely to own their homes and age in place. Lack of mobility within rural housing markets contributes to an overall housing shortage in these communities, limiting business expansion and attraction opportunities.\nOver the years, several other differences have become prominent between urban areas and their rural counterparts:\nIn a world dependent on online communications, broadband access remains a challenge in rural areas. In all states, broadband access is higher in urban areas than in rural ones. While 63 percent of rural Americans say they have a broadband internet connection at home, increased from 35 percent in 2007, there are still many challenges to improving accessibility.\nThe cost of providing services is the most significant hurdle. Even where broadband is available, it is often prohibitively expensive, leading to gaps not only in access, but also in adoption. Lack of internet has widespread consequences, particularly for rural economic and educational opportunities.\nHealth and Opioids\nThere were more than 33,000 opioid-induced deaths in 2015, a fourfold increase from 2000. Although overall opioid mortality rates are higher in urban counties, mortality rates in rural areas have increased more quickly across all regions over the last two decades. Between 1999 and 2016, opioid death rates in rural areas have quadrupled among the 18 to 25 age group, and tripled for females. To treat and prevent opioid addiction, various healthcare services are required. Unfortunately, resources are more limited in rural areas.\nThe lack of improved access to healthcare services also has significant impacts on life expectancy. On average, from 2005-2009, the life expectancy in rural areas was 76.7, compared with 79.1 for urban dwellers, a gap that has widened significantly over the past 50 years. People living in rural areas are also more likely to die from the five leading causes of death, including heart disease, cancer and stroke, than their urban counterparts. In 2014, approximately 71,000 deaths among rural residents were potentially preventable.\nDespite the wide gulf between urban and rural communities, there are a number of challenges they share, from affordable housing to opioid addiction. These common challenges provide opportunities for shared solutions.\nAbout the Author: Rose Kim is a research associate in NLC’s Center for City Solutions.']	['<urn:uuid:e29d6a4c-9f31-40f5-96d4-7eeb0667f507>', '<urn:uuid:8185aa9c-c26b-44ec-99aa-ee0ffaa16ac2>']	factoid	with-premise	concise-and-natural	similar-to-document	multi-aspect	novice	2025-05-12T10:19:05.071190	9	60	1953
24	ergonomic office chair seat depth width requirements proper positioning	A good ergonomic chair should accommodate both small and large people through proper seat depth and width. When seated, a person should be able to rest their back on the backrest while maintaining a two to five-inch gap between the back of their knees and the lip of the chair seat. This typically requires an adjustable tilt feature on the chair.	['One of the undeniable facts about working for any company and honestly even working for your own small business is that you’ll most likely spend a substantial amount of time sitting in an office chair of some sort. This position alone is, unfortunately, one that will produce added stress to the body and more specifically the spinal column. In order to combat this additional stress, it’s imperative to take the necessary measures to ensure that your posture is correct and also that lower back is properly supported. The most effective way of accomplishing this task is to research and invest in an office chair that is ergonomically sound. [no_toc]\nRead more: Ergonomic office chair review\nWhat is an Ergonomic Chair?\nBefore we can go into detail as to how to choose the best ergonomic office chair, it’s only right that we first describe what is actually meant when the term ergonomic chair is thrown out there. According to Herman Miller (the leader in ergonomic office furniture), a chair becomes ergonomic when it specifically adapts to its user’s size, workstation, and daily tasks. In other words, an office chair that is specifically designed around persons overall productivity can be correctly labeled as ergonomic.\nSo How Do I Choose the Right Ergonomic Chair Then?\nNow that you understand what an ergonomic office chair is the question remains as to which one is best. The fact is that there are many ordinary computer chairs that claim to be ergonomic but in actuality are no more than a fancy leather chair with no real back support. While there isn’t a singular “best office chair” that exists (due to the fact that every individual has a different body structure and thus requires certain features) one can make an informed decision by simply knowing which features that good ergonomic chairs possess.\n#1 A first feature that all ergonomic office chairs should possess is the ability to adjust the height of the seat. In most cases, this is done with the use of a pneumatic level and is quite simple to accomplish. The benefit of this feature is that it allows the person seated in the chair to properly place their feet flat on the floor. This position is one that promotes proper blood flow and improves overall concentration during long work sessions.\n#2 Secondly, a good ergonomic chair should be able to accommodate both small and large people simultaneously. In order to do this proper seat depth and width are required. Broken down into easy to understand terms, this means that an individual that is seated in the chair should be able to comfortably rest his or her back on the backrest while still keeping a two to the five-inch gap of space between the back of the knees and the lip of the chair seat. In most cases, this requires an adjustable tilt on the chair being used.\n#3 Thirdly, if you really want to correctly call your task chair ergonomic, then it must properly support your spine. A human spine is naturally curved inward and so a good office chair must accommodate this unchangeable feature about our bodies. Failure to do so certainly leads to a flattening of the spine’s natural curve which in turn creates unnecessary stress and strain on the lower spine. On an office chair, this translates to having the ability to properly adjust the back of the chair so it matches up with the spine of the person seated.\nThe next feature of a great ergonomic chair is an adjustable backrest. As previously stated the backrest should properly support the natural curve of the spine and as no two backs are the same (meaning width or height) backrest adjustment is a must on any good office chair.\n#5 The fifth thing that defines a good ergonomic office chair is armrests that are adjustable. When seated in the proper position, your elbows should form a 90-degree angle and your forearm should only be lightly resting on the armrests. Putting unnecessary weight on your forearms can lead to wrist and hand problems down the line, therefore one should adjust their armrests to prevent this.\n#6 Finally, the material used to make your ergonomic chair is an important factor that should not be overlooked. On the market today you see a ton of leather office chair and would possibly assume that leather is a superior product but the reality would surprise you. A mesh ergonomic chair is actually preferred because of the fact that material is breathable yet still comfortable enough to sit on for prolonged periods of time.\nThese six basic features outline not only the benefits but also the makings of a quality ergonomic chair. Be sure to grade your own computer/office/task chair on them to reveal as to whether or not it should be truly labeled “ergonomic”.\nIf you want something outside your usual everyday office chair, then fortunately for you there are a variety of styles of ergonomic chairs to choose from. The great thing about each of these ergonomic chairs is that although they aren’t your traditional office chair, they still were all designed to provide support, comfort and promote healthy posture.\nTwo Nontraditional Ergonomic Chairs\n#1 Ergonomic ball chair – The first nontraditional office chair we will discuss is the ergonomic ball chair. As the name implies, it is simply a ball that is big enough to hold the weight of a user in ordinary office usage. Where this differs is that the user of this chair is actually using his or her core abdominal muscles as well as back muscles to stabilize themselves. While sitting in this chair, a slight bouncing is encouraged as it actually promotes proper and healthy circulation in the legs, while simultaneously reducing stress buildup in the other parts of the body.\nThe major of the advantage of a ball ergonomic chair is that slouching while sitting in this chair is impossible because in order to sit in this chair proper posture must be in place or you will fall off. Of course, these chairs include various accessories to fit under different desk sizes and due to the fact that all men were not created the same height. This chair definitely takes some getting used to but after regular usage, the user will see a dramatic improvement in their posture and overall productivity.\n#2 The saddle chair – The second and last ergonomic chair we will be discussed is the saddle chair. The saddle ergonomic chair can either be used as a traditional office computer chair or a desk chair. As the name implies the chair actually is shaped like a horse’s saddle and the position that is used on this chair is a cross between sitting and standing. Due to this unique position, the legs are allowed to hang naturally which again forces the lumbar region into a proper and healthy posture.\nMost often doctors recommend saddling ergonomic chairs for patients that have been complaining of chronic back problems as they are an effective method to strengthen those same back muscles. Furthermore, physicians generally will recommend these office chairs to customers or patients that have circulation issues as the natural seated position while in this chair promotes excellent blood circulation.']	['<urn:uuid:182bd893-34b7-4641-9a67-d3749665e508>']	open-ended	with-premise	long-search-query	similar-to-document	single-doc	expert	2025-05-12T10:19:05.071190	9	61	1209
25	australian open 2023 how did sebastian korda beat medvedev in day 5	Sebastian Korda defeated Daniil Medvedev in straight sets with a score of 7-6(7), 6-3, 7-6(4) during the night session on Rod Laver. Korda played a near-flawless match, utilizing his improved serve and willingness to move forward, which kept him in control throughout the match. Although the scoreline appears straightforward, Korda did allow Medvedev back in a couple of times when he was well ahead.	['Day 5 on the 2023 Australian Open is full and contained extra surprises as final 12 months’s finalist, Daniil Medvedev, fell in straight units to Sebastian Korda.\nElsewhere Jiri Lehecka took out the eleventh seed Cameron Norrie in 5 units, and Denis Shapovalov suffered the identical destiny by the hands of Hubert Hurkacz.\nOn the similar time, Stefanos Tstisipas made mild work of Tallon Griekspoor, and he, alongside Yoshihito Nishioka, are the one two gamers on this half of the draw but to drop a set en path to the final sixteen.\nYou’ll be able to see the whole outcomes from day 5 of the lads’s singles under.\nDay 5 Australian Open 2023 Outcomes\n|Stefanos Tsitsipas||Tallon Griekspoor||6-2,7-6(5), 6-3|\n|Felix Auger-Aliassime||Francisco Cerundolo||6-1, 3-6, 6-1, 6-4|\n|Sebastian Korda||Daniil Medvedev||7-6(7), 6-3, 7-6(4)|\n|Hubert Hurkacz||Denis Shapovalov||7-6(3), 6-4, 1-6, 4-6, 6-3|\n|Jiri Lehecka||Cameron Norrie||6-7(8), 6-3, 3-6, 6-1, 6-4|\n|Jannik Sinner||Marton Fucsovics||4-6, 4-6, 6-1, 6-2, 6-0|\n|Karen Khachanov||Frances Tiafoe||6-3, 6-4, 3-6, 7-6(9)|\n|Yoshihito Nishioka||Mackenzie McDonald||7-6(6), 6-3, 6-2|\nTstitsipas Takes Down Griekspoor\nStefanos Tsitsipas continued his glorious begin to the 12 months with a 6-2, 7-6(5), 6-3 victory over Tallon Griekspoor.\nThe Greek needed to save a set level within the second set, however I like how he’s taking part in; it isn’t lights-out stuff, however he’s defending his serve effectively and getting the fundamentals proper.\nThis was Federer’s blueprint for achievement to rack up wins within the mid phases of Grand Slams when he wasn’t in JesusFed-type mode, and I feel Tsitsipas has a sensible probability on the title.\nLehecka Negates Cam Norrie\nCam Norrie was the best rank Brit within the draw, however he faltered in 5 units to Jiri Lehecka 6-7 6-3 3-6 6-1 6-3.\nNorrie regarded massive and in cost when he took a two units one lead, however from there, Lehecka gained 10 of the following 12 video games earlier than comfortably serving it out.\nThe Czech faces Felix Auger Aliassime within the spherical of 16, who had a extra comfy victory than his first two rounds in Melbourne.\nHurkacz Sinks Shapovalov\nHubert Hurkacz improved to 4-1 in his head-to-head report in opposition to Denis Shapovalov with a 7-6(3), 6-4, 1-6, 4-6, 6-3 after three hours and 40 minutes.\nThe Canadian, as anticipated, was the flashier of the 2 with some eye-catching ball placing, however it was Hurkacz who remained the extra constant within the fifth set, touchdown extra first serves (72% to 52%) and profitable 90% behind it to progress.\nKorda Mauls Medvedev\nSeb Korda continued his spectacular begin to the 12 months by gorgeous Daniil Medvedev 7-6(7), 6-3, 7-6(4) within the night time session on Rod Laver.\nThe seeds and my picks for the final 4 have fallen by the wayside within the earlier two rounds, and now the Russian, who was defending huge factors after making the ultimate final 12 months is on his means residence too.\nKorda performed a near-flawless match, and his new beefed-up serve, plus the actual fact he was prepared to maneuver ahead, meant he was at all times in cost.\nAnd though the scoreline appears to be like simple, it may have been even simpler as Korda did enable Medvedev again in a few occasions when he was effectively in entrance. A formidable win nonetheless.\nDifferent Matches of Notice\nKhachanov Tops Tiafoe\nKaren Kachanov took out Frances Tiafoe, 6-3, 6-4, 3-6, 7-6(9), in three hours and 33 minutes to make the fourth spherical in Melbourne for the primary time in 5 makes an attempt.\nAuger-Aliassime Aces Cerundolo\nAuger Aliassime has been made to work onerous in Melbourne, however as we speak he got here by a cushty 6-1, 3-6, 6-1, 6-4 win in opposition to twenty eighth seed Francisco Cerundolo.\nThe Canadian needed to come from behind in rounds one and two, however he was at all times forward of the eight ball, firing 38 winners and breaking his opponent six occasions.\nSinner Fights Previous Fucsovicw\nJannik Sinner sailed by his opening two rounds, however within the final 32, he needed to recuperate from two units right down to see off Marton Fucsovics 4-6, 4-6, 6-1, 6-2, 6-0.\nThe Italian began off wildly however as Fucsovics drained later within the match, his groundstrokes began to do extra harm and he now not needed to redline his recreation to discover a means by.\nNishioka Manouvers Previous McDonald\nYoshihito Nishioka took down Rafael Nadal’s conqueror Mackenzie McDonald 7-6(6), 6-3, 6-2. The American’s recreation doesn’t match up effectively with Nishioka’s, who’s fast across the court docket and is hard to\nTomorrow’s Fascinating Matches\n- Andrey Rublev vs Dan Evans\n- Grigor Dimitrov vs Novak Djokovic\n- Andy Murray vs Roberto Bautista Agut\nWhat did you consider day 5 on the 2023 Australian Open? Had been you stunned to see Medveved crash out in straights?']	['<urn:uuid:3f37e489-b36d-4123-b025-58e3f2219676>']	open-ended	with-premise	long-search-query	similar-to-document	single-doc	novice	2025-05-12T10:19:05.071190	12	64	797
26	What role do block sizes play in data compression efficiency?	Block sizes significantly impact compression efficiency across formats. In LZ4, blocks are typically fixed-size units (like 8MB in the legacy format) and directly control memory buffer requirements for decoding. The blocks are independent, allowing for multi-threaded compression and decompression. The format also needs to handle the last block, which may be smaller than the standard block size. In FLAC, block size is a crucial encoding parameter - if too small, frame overhead reduces compression, while if too large, the compressor's modeling stage cannot generate an efficient model. For 44.1kHz audio using linear prediction, optimal block sizes typically range between 2-6 ksamples, with 4096 being the default. When using fast fixed predictors, smaller block sizes are usually preferable due to smaller frame header sizes.	"['After spending some time on expected properties at streaming level, let\'s now get to the core of the objective, regarding the compressed data parameters.\nAs stated previously, a compressed stream consists of several members, the most important ones being compressed data sets. Each member starts with a header, in order to identify its content. And each header starts with a magic number, a kind of \'ID tag\'.\nWe\'ll focus here on ""LZ4 compressed data set"". The stream design above allows adding any future compression algorithm at a later stage.\nAnd let\'s take as an example the old legacy framing format, defined into lz4demo.\n1) There is a magic number, which is 0x184C2102,in little endian format.\n2) There are no explicit parameters. In fact, all parameters are implicit.\nThey are :\n- The compressed data set is cut into blocks of 8MB\n- Each block starts with a field giving its size (therefore, the compressed size)\n- Blocks are independent\n- The original data size is not stored. It will be known on decoding completion\n- There is no checksum\nWell, even with such limitations, the format nonetheless works perfectly fine. It\'s just a little too restricted to become a ""generic format"", and therefore, the objective of the specification is to provide more room for parameters selections.\nWe have already established in previous blog posts that allowing checksum for Error detection is an important selectable feature.\nAnother important one is the ability to select block size, since they directly control the amount of memory buffers necessary at decoding side.\nLet\'s now study and establish potential needs for a few other properties :\n- Source data size\nThe original size of source data is not an absolute necessity : it\'s always possible to decode without it, as long as buffer sizes are properly described.\nBut it is nonetheless useful. For example, thanks to this information, the number of blocks within the current member can be calculated beforehand. Moreover the amount of data to decode from the last block is known.\nOr, if there is a single block, the exact amount of memory can be allocated, instead of the block maximum size.\nIt is also useful to display the processing position (yep, we decoded 80MB, but does that represent 10% or 90% of the stream to decode ?)\nHowever, there are also circumstances in which this data is not known. For example, if the input was piped to the compressing process, then the size will be known only on hitting its end. This might be too late to ""retrofit"" the output.\nAnother situation is when several compressed data sets are appended into a single stream : then the ""source data size"" field only applies to the current data set, but the total size is not known.\nTherefore, since it is useful but not compulsory, this information shall be present, but as an option only.\n- Uncompressed blocks\nA simple but important feature, since it avoids the bandwidth overhead and CPU consumption of the compression format when it\'s useless.\nThis could be done very cheaply, by accepting that, if the size of the compressed block is the same as the defined one, then it\'s necessarily uncompressed.\nThis suggestion looks simple enough for most blocks, except for the last one, which size is unknown (but capped).\nTherefore, it would be necessary to know the size of the last block to compare it to the compressed one, and therefore determine if the block is compressed or not.\nAnother idea would be : let\'s give up this complexity, the last block is always compressed, even if compression is either useless or detrimental.\nActually, it\'s not a good idea to ""not handle the last block"", since there is a disastrous corner case : supposed that the compressed size of the last block is exactly the size of an uncompressed full block : then the decoding will assume it\'s uncompressed, leading to data corruption.\nThis corner case can be avoided by enforcing a simple rule : a compressed block is necessary smaller than original size. Therefore, as the last block has a size <= block size, its compressed size is necessarily < block size. Hence, if the size of this last block is the maximum size, then we are in the specific but valid corner case where the last block size is exactly the maximum size of a block, and is not compressible.\nOK, enough of corner cases, let\'s now be in the normal situation where the last block size is a fraction of the maximum block size. How could we know it is uncompressed ?\nThis problem could be mitigated by inserting an information to know that we are dealing with the last block. For example, knowing the original size of the source data is enough for this need.\nBut it\'s not always available. As said previously, this is just an option, since in some streaming mode, this information is unknown. Therefore we need another indicator.\nIt could be something as simple as a bit, which simply tells that there is another block to follow, and as a consequence, the current block is full sized. As a bonus, this mechanism also protects against ""silent block truncation"" (when the compressed stream is cut exactly at the border between 2 blocks).\nOn reaching the last block, we need another piece of information, either the uncompressed size of the block, or if the block is compressed. The latter seems more compact.\n- Zero-filled blocks\nThis idea was actually proposed by Charles Bloom : it\'s not rare, for a section of input data, to be filled with zeros.\nThe idea would be to ""mark"" such blocks with a specific prefix, such as ""0"".\nFor such situation to have reasonable chances to happen, the block size must be small enough. For example, this will probably almost never happen with lz4demo block size (8MB), while this is going to be much more frequent with very small blocks, such as 4KB ones.\n- Error correction\nWhile error detection has been much talked about, nothing has been said up to now about error correction.\nThat\'s both because this feature is much more complex to provide and of questionable usefulness.\nError correction is mostly useful in situations when there is no way to ""resend"" erroneous data. This applies to real-time codec (such as voice or video) and stored archive.\nThe difficulty in both cases is that erroneous data tends to be ""bursty"". For example, when a storage sector fails, we don\'t lose just a few bytes, but an entire sector size, typically 4KB. Same for transmission, where the most common error is a missing packet.\nDealing with large burst of errors requires some specific techniques, which unfortunately cost much processing power and memory. As a consequence, the CPU and memory budget for error correction is way beyond LZ4 one, which makes the association a questionable choice.\nTherefore, it seems this feature is not expected to be ""generic enough"" to reserve a place into the generic framing format specification. Obviously, forking is always possible, and even recommended, to support specific features.\n- Allow multi-threaded compression and decompression\nMulti-threaded compression is easily achievable thanks to the division of input data into ""blocks"".\nMulti-threaded decoding is also possible if those blocks are ""independent"".\nBoth mode shall be possible, and selectable\n- Variable block sizes\nThis one is tricky : up to now, we have been talking about ""fixed size"" blocks only, with only the last block of a compressed data set having an unknown (but capped) size.\nThe idea here would be to authorize blocks of arbitrary size, instead of fixed ones.\nThe benefits are two-fold :\n- Separate data on ""natural boundaries"", in order to improve compression ratio and speed\n- Allow data insertion of any size\nThe first point is simple to argue with : such benefit only occurs with very-high ratio (and slow) compression algorithms, such as CM, which ""learn"" the data behavior through statistics. There is no tangible benefit in trying to do the same for LZ4.\nThe second benefit is more interesting, since it authorizes some flexibility in archive management.\nActually, this is mitigated by the possibility to concatenate compressed data sets (or ""members"") together in a single stream or file.\nInserting data could therefore be realized by cutting the initial member into 2 parts, inserting the new member, and concatenating the 3 members together.\nAs a consequence, it seems the format already supports such scenario, without needing variable block sizes.\n- Partial extraction and Quick Jump Table\nAnother potential idea is that, within a member, one could need to only extract a specific portion.\nIt\'s always possible to decode the full data set and get to the required location, but sometimes this might be overkill. For example, one may need a few MB of data which happen to be a few GB away from the starting point.\nHowever, the idea to decode just the necessary part introduces a few restrictions :\n- First, the input media should be seekable. It makes little sense to partially decode a piped streams, since the decoding process is likely faster than the pipe itself.\n- Second, the compressed data shall be cut into independent blocks. Otherwise, it would be necessary to decode, and therefore read, all previous blocks\n- Third, to avoid to decode ""too much data"", the blocks shall be small enough, with corresponding impact on compression ratio (the smaller the block, the lower the compression ratio).\n- Fourth, since the i/o process is likely slower than LZ4 decoding, there is a benefit only if it is possible to quick-jump to the right location immediately.\nThis can be achieved thanks to a table at the beginning of the compressed file. Such a table can only be filled after compression, and therefore is incompatible with non-seekable output.\n- Fifth, such ""table"" mechanism at member level would be useless in members appending scenarios.\nThese are quite many restrictions, for the benefit of a hardly-requested feature.\nSo probably this capability shall be left to a dedicated framing format.\nMoreover, should the input stream be seekable, it\'s still possible to ""hop"" over blocks without reading/decoding them. This is still slower than a direct jump, but still a sensible potential speed improvement.\n- Error detection algorithm\nAs a quick follow up of selecting-checksum-algorithm, one could note that i had not specified a preferred checksum algorithm, only a preferred checksum size (32-bits).\nAlthough at this stage i\'m somewhat inclined to use xxhash-strong, due to its speed and very good distribution property, there is still a chance that the algorithm might be found unsuitable at a later stage. Therefore, some provision should be left to allow another algorithm to take over later on if need be.\nPushing the idea a bit further, one could think ""let the user select its own checksum algorithm"". While the idea may sound nice, it goes against the principle of interoperability, which is exactly what this framing format tries to achieve. Therefore, only clearly defined checksum algorithms shall be allowed.\nI believe this post went through most foreseeable requirements for the LZ4 framing format.\nSo now seems a reasonable time to start a header specification.\nSo now seems a reasonable time to start a header specification.', 'The basic structure of a FLAC stream is:\n- The four byte string ""fLaC""\n- The STREAMINFO metadata block\n- Zero or more other metadata blocks\n- One or more audio frames\nThe first four bytes are to identify the FLAC stream. The metadata that follows contains all the information about the stream except for the audio data itself. After the metadata comes the encoded audio data.\nFLAC defines several types of metadata blocks (see the format\npage for the complete list). Metadata blocks can be any length and new ones can be defined. A decoder is allowed to skip any metadata types it does not understand. Only one is mandatory: the STREAMINFO\nblock. This block has information like the sample rate, number of channels, etc., and data that can help the decoder manage its buffers, like the minimum and maximum data rate and minimum and maximum block size. Also included in the STREAMINFO\nblock is the MD5 signature of the unencoded\naudio data. This is useful for checking an entire stream for transmission errors.\nOther blocks allow for padding, seek tables, tags, cuesheets, and application-specific data. There are flac options\nfor adding PADDING\nblocks or specifying seek points. FLAC does not require seek points for seeking but they can speed up seeks, or be used for cueing in editing applications.\nAlso, if you have a need of a custom metadata block, you can define your own and request an ID here\n. Then you can reserve a PADDING\nblock of the correct size when encoding, and overwrite the padding block with your APPLICATION\nblock after encoding. The resulting stream will be FLAC compatible; decoders that are aware of your metadata can use it and the rest will safely ignore it.\nAfter the metadata comes the encoded audio data. Audio data and metadata are not interleaved. Like most audio codecs, FLAC splits the unencoded audio data into blocks, and encodes each block separately. The encoded block is packed into a frame and appended to the stream. The reference encoder uses a single block size for the whole stream but the FLAC format does not require it.\nThe block size is an important parameter to encoding. If it is too small, the frame overhead will lower the compression. If it is too large, the modeling stage of the compressor will not be able to generate an efficient model. Understanding FLAC\'s modeling will help you to improve compression for some kinds of input by varying the block size. In the most general case, using linear prediction on 44.1kHz audio, the optimal block size will be between 2-6 ksamples. flac\ndefaults to a block size of 4096 in this case. Using the fast fixed predictors, a smaller block size is usually preferable because of the smaller frame header.\nIn the case of stereo input, once the data is blocked it is optionally passed through an inter-channel decorrelation stage. The left and right channels are converted to center and side channels through the following transformation: mid = (left + right) / 2, side = left - right. This is a lossless process, unlike joint stereo. For normal CD audio this can result in significant extra compression. flac\nhas two options for this: -m\nalways compresses both the left-right and mid-side versions of the block and takes the smallest frame, and -M\n, which adaptively switches between left-right and mid-side.\nIn the next stage, the encoder tries to approximate the signal with a function in such a way that when the approximation is subracted, the result (called the residual\n, or error\n) requires fewer bits-per-sample to encode. The function\'s parameters also have to be transmitted so they should not be so complex as to eat up the savings. FLAC has two methods of forming approximations: 1) fitting a simple polynomial to the signal; and 2) general linear predictive coding (LPC). I will not go into the details here, only some generalities that involve the encoding options.\nFirst, fixed polynomial prediction (specified with -l 0\n) is much faster, but less accurate than LPC. The higher the maximum LPC order, the slower, but more accurate, the model will be. However, there are diminishing returns with increasing orders. Also, at some point (usually around order 9) the part of the encoder that guesses what is the best order to use will start to get it wrong and the compression will actually decrease slightly; at that point you will have to you will have to use the exhaustive search option -e\nto overcome this, which is significantly slower.\nSecond, the parameters for the fixed predictors can be transmitted in 3 bits whereas the parameters for the LPC model depend on the bits-per-sample and LPC order. This means the frame header length varies depending on the method and order you choose and can affect the optimal block size.\nOnce the model is generated, the encoder subracts the approximation from the original signal to get the residual (error) signal. The error signal is then losslessly coded. To do this, FLAC takes advantage of the fact that the error signal generally has a Laplacian (two-sided geometric) distribution, and that there are a set of special Huffman codes called Rice codes that can be used to efficiently encode these kind of signals quickly and without needing a dictionary.\nRice coding involves finding a single parameter that matches a signal\'s distribution, then using that parameter to generate the codes. As the distribution changes, the optimal parameter changes, so FLAC supports a method that allows the parameter to change as needed. The residual can be broken into several contexts\n, each with it\'s own Rice parameter. flac\nallows you to specify how the partitioning is done with the -r\noption. The residual can be broken into 2^n\npartitions, by using the option -r n,n\n. The parameter n\nis called the partition order\n. Furthermore, the encoder can be made to search through m\npartition orders, taking the best one, by specifying -r m,n\n. Generally, the choice of n does not affect encoding speed but m,n does. The larger the difference between m and n, the more time it will take the encoder to search for the best order. The block size will also affect the optimal order.\nAn audio frame is preceded by a frame header and trailed by a frame footer. The header starts with a sync code, and contains the minimum information necessary for a decoder to play the stream, like sample rate, bits per sample, etc. It also contains the block or sample number and an 8-bit CRC of the frame header. The sync code, frame header CRC, and block/sample number allow resynchronization and seeking even in the absence of seek points. The frame footer contains a 16-bit CRC of the entire encoded frame for error detection. If the reference decoder detects a CRC error it will generate a silent block.\nAs a convenience, the reference decoder knows how to skip ID3v1 and ID3v2 tags\n. Note however that the FLAC specification does not require compliant implementations to support ID3 in any form and their use is strongly discouraged.\nhas a verify option -V\nthat verifies the output while encoding. With this option, a decoder is run in parallel to the encoder and its output is compared against the original input. If a difference is found flac\nwill stop with an error.']"	['<urn:uuid:3169f9a8-7117-4921-8ce4-6704e0748386>', '<urn:uuid:5df82eae-2aef-47b1-9ef2-6223ea63260e>']	open-ended	with-premise	concise-and-natural	distant-from-document	three-doc	expert	2025-05-12T10:19:05.071190	10	123	3111
27	water bpa pollution sources effects wildlife	BPA pollution in water comes from both direct industrial wastewater discharge and atmospheric releases from industrial sites. Studies have shown that BPA concentrations near atmospheric release sites can be up to ten times higher than normal. These contaminations have severe effects on wildlife - for example, in fish, BPA exposure causes males to lose their distinctive coloring and leads females to become less selective in mate choice, resulting in more mixed-species breeding that threatens native species populations.	"['Water contamination by endocrine and hormone disrupting chemicals is threatening global water conditions. Numerous studies have proven that harmful concentrations of BPA have been leached into waterways from pollution, but little is known about how atmospheric releases of BPA might affect water quality. Now, researchers at the University of Missouri and the U.S. Geological Survey have assessed water quality near industrial sites and wastewater treatment plants and have discovered that atmospheric releases create a concern for environmental, animal and human health.\nHere’s the press release:\nCOLUMBIA, Mo. – Water contamination by hormone-disrupting pollutants is threatening water quality around the world. Existing research has determined that harmful concentrations of Bisphenol-A (BPA), a chemical used in consumer products such as plastic food storage and beverage containers, have been deposited directly into rivers and streams by municipal or industrial wastewater. Now, researchers from the University of Missouri and the U.S. Geological Survey (USGS) have assessed Missouri water quality near industrial sites permitted to release BPA into the air. As a result, scientists now believe that atmospheric releases may create a concern for contamination of local surface water leading to human and wildlife exposure.\n“There is growing concern that hormone disruptors such as BPA not only threaten wildlife, but also humans,” said Chris Kassotis, a doctoral candidate in the Division of Biological Sciences in the College of Arts and Science at MU. “Recent studies have documented widespread atmospheric releases of BPA from industrial sources across the United States. The results from our study provide evidence that these atmospheric discharges can dramatically elevate BPA in nearby environments.”\nWater sampling sites were selected based on their proximity to the Superfund National Priorities List (NPL) or locations with reported atmospheric discharges of BPA as identified by the Environmental Protection Agency. Current or historical municipal wastewater treatment sites, which have been shown in the past to contribute hormonally active chemicals to surface water from urban or industrial sources, also were tested. Finally, relatively clean sites were chosen to serve as the control group.\nThe water then was analyzed for concentrations of BPA, Ethinyl estradiol (EE2), an estrogen commonly used in oral contraceptive pills, and several wastewater compounds. Scientists also measured the total estrogen and receptor activities of the water. This approach is used to measure all chemicals present in the water that are able to bind to and activate (or inhibit) the estrogen or androgen receptors in wildlife and humans. Levels of chemicals were highest in samples with known wastewater treatment plant discharges.\n“In addition, we were startled to find that BPA concentrations were up to ten times higher in the water near known atmospheric release sites,” said Don Tillitt, adjunct professor of biological sciences at MU, and biochemistry and physiology branch chief with the USGS Columbia Environmental Research Center. “This finding suggests that atmospheric BPA releases may contaminate local surface water, leading to greater exposure of humans or wildlife.”\nConcentrations of BPA measured in surface water near these sites were well above levels shown to cause adverse health effects in aquatic species, Kassotis said.\nThe study, “Characterization of Missouri surface waters near point sources of pollution reveals potential novel atmospheric route of exposure for bisphenol A and wastewater hormonal activity pattern,” was published in the journal, Science of the Total Environment, with funding from the University of Missouri, the U.S. Geological Survey Contaminants Biology Program (Environmental Health Mission Area), and STAR Fellowship Assistance Agreement no. FP-91747101 awarded by the U.S. Environmental Protection Agency. The views expressed are those of the authors and of the U.S. Geological Survey; however, they are not the views of the U.S. Environmental Protection Agency. Any use of trade, firm, or product names is for descriptive purposes only and does not imply endorsement by the U.S. Government.\nSource: (formerly the MU News Bureau)\nDivision of Marketing & Communications University of Missouri, http://munews.missouri.edu', 'Despite only being around for the past century or so, plastics have become ubiquitous in modern life and for good reason: the final product is incredibly versatile. From grocery bags to IV bags to the teflon on non-stick pans, plastics really do make almost everything possible.\nBut, such a useful product comes at a cost. One of the chemicals used in making certain plastics, BPA, has been linked to a suite of ecological and human health problems. Now, scientists have discovered that the effects of BPA are so strong, certain species of fish lose their ability to tell their own species apart from another.\nBPA is the building block of polycarbonate plastics, and is used in other kinds of plastics alter their flexibility. The trouble is, BPA doesn\'t stay neatly locked in - it\'s known to leech out, contaminating food and liquids that come in contact with BPA containing plastics. Studies have shown that BPA is now in our lakes and rivers, affecting all kinds of creatures that rely on those water sources.\nThe real trouble with BPA is that it looks a lot like one of the most potent animal hormones: estrogen. It tricks animal cells. Because estrogen controls a number of very important bodily functions, the potential affects of BPA on animals - including us - are severe and range widely.\nIn animals like mice and rats, doses as low as 0.025 g/kg/day can causes permanent changes to the genital tract and predispose breast cells to cancerous activity. Between 1 and 30 g/kg/day can lead to long-term reproductive changes like earlier puberty and longer periods, decline in testicular testosterone, and prostate cell changes indicative of cancer, as well as behavioral effects like decreased maternal instincts and even reversed sex roles.\nJessica Ward and her colleagues were particularly concerned with how BPA is affecting fish in contaminated waters. In Georgia waters, an introduced species of fish - the red shiner (Cyprinella lutrensis) - is encroaching upon the habitat of a native species, the blacktail shiner (Cyprinella venusta). To determine the short term effects of BPA exposure on these two species, the research team placed male and female fish in BPA and control treatments for two weeks, then looked for physical and behavioral changes.\nMales that were exposed to BPA changed color, losing some of their distinctive coloring that females use in mate choice (image from the paper on the right). This loss of color affected the females\' behavior: they were less choosy when it came to their mates. Exposure to BPA led to more mixed-species pairings.\n""This can have severe ecological and evolutionary consequences,"" said Ward, ""including the potential for the decline of our native species."" Already, hybridization with red shiners is altering the community composition of native shiners in southern waterways and facilitating the invasion. With BPA and other hormone-mimicking pollutants speeding up the process of invasion, our native species are in for the fight of their life.\nWhile we knew BPA was a problem, this is one of the first studies to reveal how broad its effects really are. ""Until now studies have primarily focused on the impact to individual fish, but our study demonstrates the impact of BPA on a population level,"" said Ward. Additional studies like this one on other species, from insects to mammals, will help us better understand how BPA and other hormone-mimicking chemicals are affecting our ecosystems. Given the dire situation many of our ecosystems currently face, such knowledge is vital in the effort to protect what biodiversity we have left for further generations.\nCitation: Ward, J.L. & Blum, M.J. (2012). Exposure to an environmental estrogen breaks down sexual isolation between native and invasive species, Evolutionary Applications, n/a. DOI: 10.1111/j.1752-4571.2012.00283.x']"	['<urn:uuid:be3963dc-1773-4729-bf79-38ff08ccab00>', '<urn:uuid:159d3a95-89e4-48ce-9788-f7947de6c828>']	factoid	direct	short-search-query	distant-from-document	multi-aspect	novice	2025-05-12T10:19:05.071190	6	77	1257
28	How do coal mines handle methane emissions today, and what environmental impact do these emissions have on greenhouse gases?	Coal mines today address methane through monitoring systems that continuously track gas levels in active mining areas, using ultrasonic instruments to measure airflow and detect accumulations. From an environmental perspective, while methane from coal mining has decreased from 18% to 5% of UK emissions since 1990, it remains a significant greenhouse gas concern as it is 20 times more powerful than carbon dioxide as a greenhouse gas.	"['Mining Topic: Monitoring Ventilation Parameters and Accumulations of Combustible Gas\nWhat is the health and safety problem?\nThe proper control and distribution of ventilation air in working areas of underground mines is crucial to the health and safety of mine workers. Many underground coal mines cover vast areas where workers are not located. Continuous knowledge of the ventilation system status, along with information on the presence of methane or products of combustion from fires, is critical for detecting and correcting problems in the mine atmosphere in their earliest stage.\nTo this end, monitors can be used in targeted areas of the mine to collect environmental data on levels of combustible gas or products of combustion that may provide an early indication that heating is in progress. In addition to levels of combustible gas, monitoring airflow velocity and direction continuously can improve the safety of the underground workforce. However, continuous measurements of air velocity may not always provide accurate assessments of ventilation conditions, as these values can change rapidly depending on measurement location and airflow conditions.\nWhat is the extent of the problem?\nUnder MSHA regulations for atmospheric monitoring systems, monitoring in active mining areas usually involves methane and products of combustion around electrical and belt installations. Monitoring of ventilation airflow velocity is less common, with little information gathered on airflow direction. Large portions of many underground operations may not be continuously monitored for airflow velocity and methane accumulations.\nHow is OMSHR addressing the problem?\nThe Office of Mine Safety and Health Research (OMSHR) is investigating the use of ultrasonic velocity instrumentation in the underground mine environment. Ultrasonic anemometers are known for their accuracy, capability to measure low airflows, continuous measurement characteristics, and ability to provide a directional sign to the air velocity. This can be especially beneficial in situations with low air velocity in that airflow reversals may occur.\nCurrent OMSHR research efforts are focused on the installation of integrated airflow and methane monitors in active areas of underground coal mining operations. This work is identifying specific sampling protocols for these devices and is assessing the behavior of these instruments in relation to the presence of airway obstructions such as personnel and equipment. OMSHR is also identifying potential applications and limitations of integrated air velocity and methane monitoring systems and developing guidelines to interpret air velocity and methane monitor outputs.\nWhat are the significant findings?\nOMSHR laboratory testing showed that air velocity readings from continuous recording instruments correlated well with those obtained using standard measurement techniques. Obstructions upstream of a continuous recording anemometer led to noticeable impacts on air velocity readings, suggesting that movement of these obstructions in the ventilation airstream could be correlated to changes in instrument readings. In conditions with steady airflow, recorded velocities showed little variation over time, whereas turbulent conditions produced greater variability. This work also identified the minimum number of instrument readings needed to obtain an accurate assessment of ventilation air velocity in both steady and turbulent airflow conditions.\nWhat are the next steps?\nFuture work in this area will deploy sensors in underground coal mine environments to collect long-term data on airflow velocities and directions and on methane levels. This information will be analyzed for any trends that suggest the development of ventilation conditions conducive to accumulations of methane gas.\nNoteworthy Publications & Products\n- Composition Change Model for Sealed Atmosphere in Coal Mines (2010-06)\nThis paper presents a mathematical model based on the conservation of mass principle describing the flow of air (nitrogen and oxygen), methane, and carbon dioxide into and out of a sealed atmosphere and time-dependent changes in gas concentration.\n- Methods to Determine The Status of Mine Atmospheres - An Overview (2006-03)\nThis paper serves as an overview to remind and/or instruct readers about gas-sampling methodologies and gas analyses to assist in determining the status of underground atmospheres.', 'Coal bed methane While much of the media attention on unconventional gas in the UK has been focused on shale gas, coal bed methane is as great a problem, and a bigger threat in many areas. Historically, miners took canaries down coal mines to warn of the dangers of methane. Today, new warnings\nThe most dangerous gas in coal mines was called firedamp. It was mainly composed of methane, like the natural gas that we use for cooking and heating today. If a miner came into contact with firedamp underground, the flame of his candle would sometimes cause the gas to explode.\nApr 22, 2016· For its firstyear accounts Eden said ""the principal activity of the company in the year under review was to develop and commercially exploit its UK Coal Bed Methane, Coal Mine Methane and Natural Gas Interests"". The accounts show zero turnover and a loss of £1,333 in administrative expenses.\nThe National Coal Mining Museum is a great day out giving visitors the opportunity to go down England\'s last deep coal mine talking to charismatic former miners. Then explore the galleries, meet the ponies, walk through the woods and let off steam in the adventure playground.\n1839 The national output of coal was estimated to be 31,000,000 tons. The Mining Record Office was established in London. 1840\'s Hemp ropes and flat winding chain was increasingly replaced by flat wire rope. Most collieries, particularly the deep collieries of the Northeast, still\nIt began in 2003, partly in response to a suggestion by Alison Henesey, then librarian at the Yorkshire Coal Mining Museum, that ""it would be useful to have a map which showed where the collieries were"". Ten years later the Northern Mine Research Society made this valuable tool freely available to anyone interested in coal mining history.\nMinerals and Mining: Geological reporting for UKbased deep drilling programme, interpretation and analysis of geological data for coal and shale gas projects in Colombia and the UK. Experienced (6 years) wellsite geologist on large onshore deep drilling projects (>1 km depth) including potash and coalbed methane exploration.\nmain problem in Serbian coal mines is the methane released during and after mining operations, usually called coal mine methane (CMM). Machine learning methods can be applied for the prediction of methane concentration in an underground coal All remotely monitored data are transferred to control room. Ergonomic\ngreenhouse gas (GHG) and coal mining operations are a significant source. Coal production and handling accounts for 5% of current UK methane emissions, down from 18% in 1990 (DECC 2013). 2. Coal seams are in some locations recognised as productive sources of gas for energy, variously termed coal bed methane (CBM) or coal seam gas (CSG). 1\nMethane in the Atmosphere: Coal mine methane emissions from underground mining are often caught and used as town fuel, chemical feedstock, vehicle fuel and industrial fuel – but very rarely is everything captured. Methane is less prevalent in the atmosphere as compared to carbon dioxide, but it is 20 times more powerful as a greenhouse gas.\nThe United Kingdom leads the world in abandoned coal mine gas extraction and use in power generation. So far, a total of 5 abandoned coal mine gas power generation projects are in place in the UK and all are in normal operation. Abandoned coal mine gas extraction for power generation in the UK has a very promising future.\nAug 26, 2011· Coal mine methane: Coal mine methane (CMM) is the methane gas released during coal mining operations, and it is also a greenhouse gas. CMM emissions are projected to grow by 20% between 2000 and ...\nSep 17, 2010· Methane prediction for underground mining. ... from residual coal within strata disturbed by mining activity. ... of coal mine methane utilisation from coal mines goaf in Australia will depend on ...\nThe UK\'s largest manmade source of methane is through rotting rubbish in landfills. Methane is also released throughout the mining and distribution of fossil fuels (oil, coal and gas). What is it used for? Methane is the main constituent of natural gas that is used for commercial and domestic heating and to create electricity in power stations.\nThe top 20 companies on the list have contributed to 35% of all energyrelated carbon dioxide and methane ... to take urgent measures to rein in their activities. ... from coal, oil and gas ...\nHere, we describe the use of several complementary molecular ecology techniques to investigate for the first time the diversity of active methanotrophs in alkaline soil (pH 9) from a venthole located at the exit of a Chinese coal mine that is exposed to relatively high concentrations of methane.']"	['<urn:uuid:9c6b6e23-fb39-4165-880a-4befdcb479e1>', '<urn:uuid:9e5af636-97a0-4fe3-b36d-0f10fd89266d>']	factoid	direct	verbose-and-natural	similar-to-document	multi-aspect	novice	2025-05-12T10:19:05.071190	19	67	1415
29	best engine type saltwater boat use	For permanent saltwater use, inboard engines with closed cooling systems are the best option. I/O (sterndrive) engines with raw water cooling should be avoided as they require expensive maintenance, including $5,000 manifold and riser replacements every 6 years. Outboard engines can work for occasional saltwater use since they can be lifted out of water when not in use, but still need regular freshwater flushing. Inboard engines with full closed cooling systems that cool both the block and exhaust components offer the best protection against salt corrosion.	"['Compare the performance, reliability, maintenance, convenience and efficiency when choosing between sterndrive IO and inboard engines for power boats.\nMany express cruiser style power boats offer both sterndrive inboard outboard (IO) and inboard (direct drive or V-drive) engine propulsion system options. Choosing between these engine drive types should be given careful thought for a new or used boat.\nBoth sterndrive and inboard boat engine types have advantages and disadvantages when it comes to evaluating the overall engine performance, maintenance, reliability, convenience and efficiency in power boat.\nEvaluating Sterndrive Inboard Outboards\nA stern drive IO, also known as an inboard outboard, has an outboard lower unit connected through the transom of a boat to an inboard gas/petrol or diesel engine.\nWidely available on boats under 30 feet (or 9 meters), sterndrive inboard outboards have a lot of advantages when compared to inboard boat motors.\n- Performance: An inboard outboard motor is known for more speed and boat trim ability while cruising. The outdrive positioning in the far back of the boat (as opposed to further forward like inboards) helps improve the performance.\n- Maintenance: One of the biggest disadvantages of an IO drive is the higher maintenance requirements. While not as big of an issue in freshwater, a sterndrive IO exposed to salt or brackish water is prone to corrosion. To minimize the damage, zinc anodes should be replaced often and antifouling outdrive boat paint should be used annually.\n- Reliability: Due to the potential corrosion issues, a sterndrive IO may have a shorter lifespan than an inboard boat motor. A sterndrive replacement is a likely repair with an older boat.\n- Convenience: The tilt mechanisms on an IO outdrive enables a boat to go into shallower water than an inboard engine boat. Maneuverability is also easier for a boat with inboard outboard drives.\n- Efficiency: Known to be an efficient propulsion design, inboard outboard motors are faster and have better fuel economy than inboard engines.\nThe two leading manufacturers of sterndrives are Volvo and MerCruiser. Both offer options for counter-rotating propellers that improve propulsion efficiency, speed, acceleration and handling even more. MerCruiser’s Bravo 3 outdrives and Volvo’s DPR DuoProp sterndrives are popular choices. Boat resale values are generally well-maintained for boats with upgraded outdrive models.\nComparing Inboard Boat Engines to Inboard Outboards\nInboards use direct drive or V-drive transmissions to power engines and are prevalent in larger power boats over 30 feet (or 9 meters). With only the drive shaft and propellers submerged, the entire inboard boat motor sits deep inside the hull of a boat.\n- Performance: The weight of inboards centered deep in the hull of a boat improves stability because they lower the boat’s center of gravity. Inboard boat motors give a larger boat a smooth and stable ride.\n- Maintenance: Inboard engines require much less maintenance than outdrives, especially in salty or brackish water. Boat zinc anodes are still required on drive shafts to reduce corrosion on shafts and propellers. Engine access on smaller boats with inboards can also be a tight fit.\n- Reliability: Because inboard motors are more resistant to corrosion they tend to last longer than sterndrive inboard outboards.\n- Convenience: Propellers are tucked away under the boat further than a sterndrive, but without a tilt-up mechanism an inboard engine boat has a larger draft. This is one of the biggest disadvantages of an inboard boat motor as compared to an IO motor.\n- Efficiency: Due to the angle of the propeller shafts, inboard engines have lower fuel efficiency and less overall performance with the same size motor as inboard outboards.\nSelecting a Drive Type for Boat Engine Propulsion\nA boat’s size and the type of water a boat will be used in (freshwater vs. saltwater) are two of the biggest determining factors when selecting drive type. When comparing sterndrive IO motors vs. V-drive inboard motors, evaluate the pros and cons for both performance and maintenance requirements. Each engine propulsion system option for a power boat has trade-offs.\nThe final choice on engine drive type should be based on an individual boater’s preferences and how the boat will be used.', ""When buying your first boat, there's a lot of information to process. The most important question I had is whether it matters if I buy a freshwater or saltwater boat. So I did some research to find out where they differ.\nWhat's the difference between freshwater and saltwater boats? Saltwater boats have systems to better deal with salt corrosion, like a closed cooling system and Mercathode system. They don't differ in hull type or material. Some people refer to offshore boats as saltwater boats. These have a different hull shape and a keel (fin) that's suitable for offshore sailing.\nIt's noteworthy that people mean different things when they say 'saltwater'. Also, using your boat in either salt or freshwater means you have to maintain them in a different way. Let's find out why that is.\nSaltwater vs. Freshwater\nFreshwater is inland water. Saltwater is seawater and it contains a lot more salt.\nBoats that are in saltwater mainly have to deal with salt corrosion, but other than that, there's no real difference. Generally, a saltwater boat doesn't have a different hull shape and doesn't use other materials.\nBut because of the salt, saltwater boats corrode more quickly than those in freshwater. Salt eats away at iron and aluminum. If saltwater runs through the engine you get to deal with galvanic corrosion: which simply means the engine dies. Saltwater in an engine will reduce performance and ultimately block it.\nEspecially engine parts such as the block, exhaust, manifolds, and driver and rise system are affected by saltwater. Luckily, there are plenty of systems to deal with the extra corrosion (more on this later).\nThey also suffer of hull fouling or bottom fouling. Hull fouling is the growth of marine organisms on the hull. It can cost a lot of money and effort to clean up, so that's not good. This is a major problem for boat owners in warm water ports especially. Barnacles that grow on the hull can slice open the bellows, which isn't great also.\nMostly, the terms 'freshwater' or 'saltwater' refers to the boat's history. Boats that have seen salt are believed to fall apart instantaneously (which of course isn't true at all). But still, they are less likely to sell quickly. Calling a boat freshwater to get a better price is marketing, plain and simple. A well maintained saltwater boat is just as good as a freshwater boat, and sometimes better if the freshwater one isn't maintained at all.\nTo be completely clear: some people call offshore sailing boats 'saltwater boats'. They mean something completely different. Offshore sailing boats have a different hull shape: they have a keel that makes them more stable in larger waves.\nUsing a boat that is build for inland water out on sea can be dangerous. It's just not stable enough. So make sure you don't confuse these terms (many people do). If you're looking for a 'sea boat', simply get the right hull type (one with a keel), and you'll be fine. It doesn't matter if it's been in salt or freshwater.\nCan a Freshwater Boat Be Used in Saltwater?\nYou can use a freshwater boat in saltwater just fine. If you haven't upgrade your boat for saltwater use you have to keep some things in mind to avoid unnecessary corrosion.\n- Make sure you flush the engine with freshwater after you're done\n- Also hose down the boat with freshwater thoroughly. Any salt that remains on the hull or parts will start to eat them away\nLet's say you decide to get the boat out of the salt ASAP (directly after your trip). You hoist it onto your trailer. That's probably a good idea. There's a catch though. Salt is so aggressive on iron, it will get into places you can't really clean. It starts to slowly eat away your trailer as well. So I encourage you to hose down your trailer after each trip as good as you can.\nIf you're talking about using an inland sailboat offshore (I'm talking bluewater sailing here): I wouldn't recommend it. They can't cope with large waves and the risk of capsizing is too great. But: some sailboats that are designed for inland use have a keel you can lower if needed (my Cornish Crabber had one). You can use these in coastal waters as well. This is great in my opinion.\nHow to Deal With Saltwater Corrosion\nFor starters let me first say that saltwater corrosion isn't the end of the world. As I said, there are plenty of ways to deal with salt corrosion. As a matter of fact, you can convert you're own boat for saltwater use. Using a boat in saltwater requires better maintenance in general. So if done right, it will probably increase the lifespan of your boat. But it also costs a pretty penny.\nThe first thing you should do is probably to upgrade the systems. I encourage to take a better look at the following upgrades and systems:\n- installing a closed cooling system, or, if necessary, replacing the engine (more on choosing an engine later)\n- installing a Mercathode system - running electric current through your engine (I'll explain later on)\n- installing freshwater flush system for the engine (if you're not getting the closed cooling system)\n- replacing fasteners with stainless or anodized (coated) alternatives\nTo deal with the bottom fouling of saltwater you want to apply a layer of anti-fouling paint to your hull. Anti-fouling paint is an underwater hull paint made to protect from corrosion. If you leave your boat throughout the season, even in freshwater, I'd recommend to use it.\nWhat's a Mercathode system? A Mercathode system is a cathodic protection system that runs a tiny electrical current through your engine block and outdrive. The electric current is used to stop galvanic corrosion metal parts that are submersed.\nChoosing the Right Engine\nIf you plan on using your boat in saltwater permanently you should pay close attention to your engine. Remember this one thing:\nAvoid I/O engines with raw water cooling at all cost!\nInboard with closed cooling (or a cheap outboard) is the way to go.\nThere are basically three types of engines:\n- Outboard engines\n- Inboard engines\n- I/O or sterndrive engines - has the propellor on the outside, the engine block lies below deck\nThese engines can use two types of cooling systems:\n- 'raw water' cooling system\n- closed cooling system (best option)\nRaw water simply means the engine uses the water you're sailing in to cool the engine. If this is saltwater, salt comes into the engine. Salt will cause galvanic corrosion, which basically means the engine will ultimately die. But first, it will perform less and less, until it blocks.\nI/O’s with raw water cooling are the worst combination for salt water usage. If you use a raw water cooling system, you should expect to have to replace your risers and manifolds every 6 years (on average). It's expensive. It will cost you roughly $5,000, depending on availability of parts and cost of labor of course.\nAn inboard with closed cooling is generally the best option for permanent saltwater use. Closed cooling means the engine uses a closed system for cooling. So it uses its own cooling fluid. It doesn't use the saltwater for cooling, which means it doesn't have a chance to corrode the engine.\nOutboard engines don't have closed cooling systems, inboard engines do.\nHowever, an outboard engine can be alright for occasional saltwater use, since you can lift them out of the water if you're not using them. So even if you keep your boat in a saltwater marina, your engine wouldn't be in salt all the time. But you do need to flush it, otherwise you still end up with costly maintenance every couple of years.\nPlease note: closed cooling systems come in two types. One only runs coolant through the block, but not the exhaust manifolds and risers. The second also uses closed cooling for the exhaust. The second option is the best option. Otherwise, you'd still need to replace the manifolds and risers every six years.\nCan you run a saltwater motor in freshwater? You can run a saltwater motor in freshwater without problems. Every engine type with closed cooling system can run in either salt and freshwater. If you use an engine with raw water cooling in saltwater, be sure to flush it with freshwater after use.\nHow to tell if a boat has been in salt water? Look under the exhaust flaps for rust. Rust is the best indicator for saltwater use. Salt also forms a white film in corners and on nuts and bolts. If the boat comes on a trailer, you could also check the trailer for rust. An old boat on a new trailer might indicate saltwater use as well.\nCan all boats go in the ocean? It's not recommended to use inland boats on open seas. Boats for offshore use have a different hull shape and a keel (fin) to provide stability and deal with larger waves and stronger current. An inland boat isn't stable enough for ocean use and might capsize. Also, engines with raw water cooling corrode in salt water.\nDid you find the answer to your specific question?\n👍 2 👎 0""]"	['<urn:uuid:bf1870f3-ea49-48ae-adba-ce79c5f22ace>', '<urn:uuid:edac5d23-d4a4-4f84-9a7a-98db1fe384bb>']	factoid	direct	short-search-query	similar-to-document	three-doc	novice	2025-05-12T10:19:05.071190	6	86	2240
30	what process creates those beautiful colored photos from outer space telescopes	The Hubble Space Telescope doesn't actually take colored pictures. Instead, it captures images in black and white using electronic detectors, similar to a digital camera. The colored images we see are created by combining various black and white exposures taken through colored filters.	['Humans have always been fascinated with the infinite large sky and the possibilities that exist beyond our world. But scientists have gone ahead and given us answers to questions pertaining to universe. The Hubble Space Telescope has been a catalyst in our quest to exploring the universe. It has confirmed the expanding universe theory and given us beautiful pictures of the same universe. Let’s take a look at the 10 things you need to know about the Hubble Space Telescope.\n#1. Took several decades to make\nIt took several decades to launch the massive telescope. The idea started in 1946 when astrophysicist Lyman Spitzer Jr. wrote a paper on space-based observatory. It further took him several years to convince the government why this was necessary. It was only in 1977 that the funding for this enormous telescope was approved.\n#2. Named after an astronomer\nThe Hubble Space Telescope is named after a very famous astronomer Edwin Hubble who has contributed greatly to the field of space and expanded our knowledge of universe. One of the things he proposed was that Andromeda (considered a nebula at that time) was a separate galaxy which was thousands of light years away fro our Milky Way.\n#3. It has provided the deepest images of the universe\nThe giant telescope has given us a plethora of images that peer over the darkest parts of universe. The telescope is responsible for giving us pictures of never- before-seen galaxies and distant star systems.\n#4. A delayed launch\nIn 1986, the space shuttle Challenger exploded killing 7 astronauts. This proved to a hurdle in launching Hubble Telescope as it depended on the shuttle for transport and maintenance. This proved to set the whole launch behind by 7 years and about $1 billion over budget.\nYou may already know that it is a huge telescope. But how big exactly is something that can shock you. It is 15.9 meters long and weighs 11 tonnes.\nHubble Telescope is fast and power efficient. It only uses about 2800 Watts(slightly more than a kitchen kettle that uses 2200 watts). it is powered by solar panels that are 2.6 X 7.1 meters. The telescope is also pretty fast and speeds around 28,000 km/h. It is 12 times as fast as the cruising speed of Concorde supersonic airliner.\n#7. Taking Pictures\nIt might be a giant camera but it doesn’t take pictures on a film. It is more like a digital camera where pictures are captured on electronic detectors. In fact, these images are not in fact colored, they are captured in black and white. The pictures we see are a result of combination of various black and white exposures made via coloured filters.\n#8. Anyone can make a request to use the HST\nThe option to use the HST is open to all but the competition is extremely tough. Every year, john Hopkins University’s Space Telescope Science Institute makes an open call to astronomers everywhere to use the telescope. But only a selected few actually get to use it.\n#9. The age of the universe\nVarious calculations have been done to measure the age of the universe. But it was the HST that gave us the most accurate figure till date and measured the age of the universe to be about 13.8 billion years old.\n#10. The replacement\nThe HST is in its final phase now but we already have a replacement for that. The James Webb Telescope will be launched somewhere in 2018.']	['<urn:uuid:ea73ecba-72e2-4e8f-9547-a87a5eb34a92>']	open-ended	direct	long-search-query	distant-from-document	single-doc	novice	2025-05-12T10:19:05.071190	11	43	581
31	hilton worldwide energy consumption reduction since 2009 carbon waste water percentage	Since 2009, Hilton has reduced energy use by 14.5%, carbon output by 20.9%, waste output by 27.6% and water use by 14.1%.	['GLOBAL REPORT—Leadership starts at the top, and when a top executive is committed, the impact is far reaching and lasting. A champion of community-based causes and volunteer-driven organizations, it is no surprise Hilton Worldwide CEO Christopher J. Nassetta is also dedicated to mobilizing his employees to execute charitable work around the world.\n“We have senior leader Chris Nassetta that makes it a priority and it’s really a grass roots movement where our team members become so passionate. The support is coming from all ends to help us do amazing things. When you see it come to life, it’s amazing to witness the impact and the difference we can make as a company,” said Max Verstraete, VP of corporate responsibility, Hilton Worldwide.\nHilton’s corporate responsibility strategy, “Travel with Purpose,” is an action-oriented philosophy that seeks to identify, engage and address issues of need in three areas: economic, social and environmental.\n“We approach and respond to needs around the world by taking advantage of our global footprint, and then scale to find solutions. When you look at our scale, we are the leading hospitality company as we have the leverage of working with best in class organizations to drive thought leadership and improvement of certain issues,” shared Verstraete. “We’re present in more than 100 companies and we work at the local level. With over 4,600 hotels, we can have a significant impact.”\nCreating opportunities for youth is paramount as an estimated 74 million young people are unemployed, according to the International Labor Organization. To address this challenge and armed with a goal to reach a million young people by 2019, Hilton and its team members are facing the issue via initiatives with nonprofit International Youth Foundation and through activation programs within the hotels. An expected outcome is that these trained youth will be primed for future employment with Hilton.\n“There are barriers to progress within communities and we are bringing our skills there. We call it smart volunteering, providing a level of skills such as culinary, financial and sales skills. It’s about figuring out what are the local needs of the community and bringing those skills to overcome these barriers,” he said. “We have a growing pipeline and we need young people and talents to fill these positions.”\nHilton is taking action to protect environmental resources as well, requiring hotels to make sure there are efficiencies enacted in the areas of water and energy consumption, waste management and then measuring those efforts to gauge impact. As a result, the brand has reduced energy use by 14.5%, carbon output by 20.9%, waste output by 27.6% and water use by 14.1 since 2009.\n“There are improvement projects in place at all times. It’s as important as having the right shampoo and conditioner. Adhering to brand standard verifies compliance,” he said. “As a company, we’re leaders in energy management as we have earned a system-wide ISO 50001 energy management certification across our entire portfolio and we’re the first hospitality company to achieve a superior energy performance certification from the U.S. Department of Energy at three hotels.”\nAn active commitment to energy conservation only scratches the surface of their holistic approach to reducing environmental impact. Additional efforts include seeking greater reductions in carbon and water over a period of six years; working with the World Wildlife Fund on the sustainable sourcing of seafood; and partnering with multiple organizations around soap recycling, among other projects.\nHilton maintains a strong presence in the global communities it serves through volunteerism projects. The brand encourages team members at its hotels to look at the needs from a micro level and find ways to meet those needs. According to Verstraete, Hilton’s community involvement is mutually beneficial as it provides value for business and the community by supporting the local economy, creating local talent and fostering relationships.\n“The approach to this needs to be from a shared value and everything we do needs to bring value to the community, our team members, our guests and business partners,” he said.']	['<urn:uuid:acbf362b-7b00-45e6-8a69-7bc0b85d3e80>']	factoid	direct	long-search-query	similar-to-document	single-doc	expert	2025-05-12T10:19:05.071190	11	22	667
32	why is voodoo associated with horror movies	The association between Vodou and horror comes from racism and historical fears. During slavery, Vodou was practiced by slaves of African descent whose culture was feared and ridiculed. After the successful Haitian Revolution led by Vodouist slaves, other colonies feared similar uprisings. Over time, American culture began depicting Vodou in movies and books as sensationalized horror. Most disturbing images we associate with 'Voodoo' were created by Hollywood, forming a mythology that has been taken as truth. The old fears of slave rebellions combined with Hollywood's fictional portrayals have made 'Voodoo' part of modern folklore as something evil and dangerous.	['What do you know about Vodou?\nWhere did you get that impression?\nVodou probably isn’t what you think it is. It might be easier to start with what Vodou isn’t.\n- Vodou isn’t accurately portrayed in most movies, TV shows and books. Even some documentaries and non-fiction books are very inaccurate.\n- Vodou isn’t a cult, black magic or devil worship. People who practice Vodou are not witchdoctors, sorcerers or occultists.\n- Vodou isn’t a practice intended to hurt or control others. Most Vodouists have never seen a “Voodoo doll” (unless, like you, they saw it in a movie).\n- Vodou isn’t morbid or violent.\n- Vodou, like other religions, isn’t the same everywhere. Not everyone who practices Vodou does it in exactly the same way or agrees on exactly the same things. (This document only represents my understanding of Vodou. I can’t speak for everyone!)\n- Vodou isn’t usually treated with the same respect shown to other religions.\nSo…what is Vodou?\n- Vodou is a religion that originates in Africa. In America and the Caribbean, it is a combination of mostly African, Catholic and Native American traditions.\n- Vodou has no scripture or world authority; it is community-centered and supports individual experience, empowerment and responsibility.\n- Vodou is different in different parts of the world, and varies from community to community. This document is mostly about Vodou in New Orleans and Haiti.\n- Vodou embraces and encompasses the entirety of human experience.\n- Like all religions, Vodou is practiced by people who are imperfect and may use religion for their own purposes, good or bad.\nWhat do Vodouists believe?\nTo understand what they believe, you have to first understand how a Vodouist sees the world. Those who practice Vodou believe that there is a visible and an invisible world, and that these worlds are intertwined. Death is a transition to the invisible world, so our predecessors are still with us in spirit.\nIn addition to loved ones we knew in life, there are the Lwa: universal spirits that are archetypes of human personalities (the warrior, the healer, etc.) and others that embody more specific concerns or localities. Vodouists develop relationships with these Lwa to seek their counsel and help with concerns in the visible world. In some ways this is not dissimilar to the secular practice of studying and honoring remarkable historic figures. For example, someone who wishes to effect social change might find inspiration from Martin Luther King Jr. or Mahatma Gandhi and feel a kinship with them. They may read their books, keep a poster of them on the wall, place significance on their day of birth or death and try to live by their example. In a similar fashion, a Vodouist develops a relationship with particular Lwa, seeks to understand and embody the principles they represent, connects spiritually in order to affect personal transformation and manifest this energy in the visible world to help the living.\nLike the Catholic saints or angels, the Lwa are familiar and accessible whereas the “great good God,” although loving, is distant, and somewhat above individual human concerns. Vodou has ordained clergy, both Hougan (priests) and Manbo (priestesses) that make a commitment to a spiritual path and can offer guidance when needed, but it is believed that each person is responsible for their own actions and capable of self-actualization. Vodouists especially places value on the strength of community for support and enrichment.\nJust as there are differences within Christianity, there is variation within Vodoun beliefs and practices. In places and times where conditions are very desperate, Vodou is often focused on survival. Many Vodouists feel that part of religion is service to their community, so there may be emphasis on healing and social activism.\nIf Vodou is just another religion, why does everyone think it’s scary?\nRacism clouds our view of Vodou. It is rooted in slavery and intricately connected to this hemisphere’s political and social evolution. Vodou was first practiced in America & the Caribbean by slaves of African descent, whose culture was both feared and ridiculed. Slaves were not considered fully human. Their religion was dismissed as superstition, their priests were denigrated as witchdoctors, their Gods and Spirits were denounced as evil.\nThe only successful slave revolution in modern history occurred in Haiti in the late 1700s when slaves of African descent overthrew French rulers and took control of the country. Most slaves were Vodouists, and some military leaders were priests who inspired and organized their communities to fight for freedom. The Haitian Revolution provoked fear in other European-American colonies that were reliant on vast numbers of slaves as plantation labor. The imagery and vocabulary of Vodou (and other Afro-Caribbean religions) became threatening and ingrained in those cultures as something horrifying, associated with bloodshed and violence. It was brutally repressed in most places. It became taboo.\nOver time, American culture became fascinated by this mysterious tradition and began to depict it in movies and books as sensationalized horror. “Voodoo” practices were dreamed up by Hollywood; most of the disturbing images fixed in our minds are something we saw in a movie. Hollywood has created a mythology that we have taken as truth. Between that and the old fears of slave rebellions, “Voodoo” has become part of modern folklore as something evil that can hurt us.\nVodou is still widely practiced in Haiti, and it is still relevant in politics there. Politics and religion make a controversial mix; in that regard, Vodou is as problematic as any other belief system. In the USA, many Vodouists are afraid of how they will be treated so they hide their religion. While this is understandable, it also reinforces suspicion that they practice in secret to conceal something bad or violent. Fear begets fear.\nWe aren’t always aware of the origins of our beliefs; now and then we need to reassess what we know and how we know it. There were times in our nation’s history that other groups (e.g. Jews, Catholics) were similarly reviled. It’s only through education and getting to know those with different beliefs that we can overcome our fear and realize that they are ordinary people who enrich our communities.\nBut what about…\nFood, and its life-giving energy, is one of the many offerings given to the Lwa and usually shared afterwards by the community. In a meat-eating society, animals are food; in small-scale cultures, people slaughter animals at home rather than buying meat at the grocery store. Animal sacrifice is not about the death of the animal, but the offering of life-giving energy in the preparation of sacred food. Many Vodouists in the USA don’t consider animal sacrifice culturally appropriate and do not practice it, preferring to offer store-bought food. Others consider this an integral part of their tradition and continue to take animal life with reverence, cook the meat, and eat it. As long as hunting, animal testing and the slaughter of animals for meat are accepted in our country, less familiar cultural practices that take animal life must also be respected.\nPart of Vodou is building a relationship with the Lwa. During ceremony, Vodouists invite the Lwa to possess them and speak through them. There are degrees of possession, just as there are degrees of any emotional or religious experience. Vodouists have an affinity for the Lwa, and welcome the experience. Many other religions invite some type of possession and believe that at times, Spirit, however that is understood by their tradition, speaks through people.\nAll the skeletons?\nIn Vodou, death is considered natural, a part of every person’s experience and a transition, not an ending. Gede (pronounced GEH-day), the Lwa of the dead, is often represented as a skeletal figure wearing a top hat and carrying a cane. He is a dapper fellow, funny and welcoming. During the days of slavery, death was the only release from horrific conditions, so the figure of death became a friendly one.\nYou’re kidding, right?\n“Le Grand Zombi” is the name of a Vodou spirit associated with snakes, but not to be confused with “zombies” as we understand them. Zombies are the boogeyman of Vodou culture. Zombies in movies and books today are apparently self-propelled and lurch out of their graves for a variety of reasons, but older folklore depicted them as silent, mindless laborers with no free will, completely controlled by a master. That sounds like the life of a slave! To people who lived in slavery, nothing could be worse than imagining an afterlife of similar servitude. Zombies reflect cultural fears so perfectly that we’ve made them our own. Movie zombies today are often created by a terrible virus, shadowy corporate dealings or a botched government program. These are things that make us anxious, even without zombies chasing us around as a result.\nWhy don’t you spell it “Voodoo?”\nThe original language of Vodou is not English, so spellings vary. Lwa may be spelled “Loa,” “Gede” may be spelled “Guedeh” and so on. One is not more correct than the other, but different communities may have a preference.\nMany years ago, “Hindu” was spelled “Hindoo.” The “oo” ending can be associated with outdated translations and a cultural climate that was derogatory towards other belief systems. “Voodoo” has also come to be synonymous with black magic, curses, Voodoo dolls and so on. Many Vodouists consider “Vodou,” closer to the correct pronunciation and may also use the term as an attempt to distance themselves from negative stereotypes. The word originates in Africa and can be spelled several ways in English, e.g. “Vodu” or “Vodun.” It simply means “spirit.”\nWhere can I get more information about Vodou?/Author’s Disclaimer\nThis document is written assuming the reader has some familiarity both with American culture and Christian traditions, so uses those as a basis of comparison. Vodou is misunderstood, centuries old and practiced by millions of people around the world. I can’t do it justice in three pages! I am personally familiar with Vodou in New Orleans; I can’t stress enough that beliefs and practices vary. Every person has their own view and experience. I couldn’t represent the diversity of this tradition in thirty pages.\nI encourage further reading, but also recommend caution; a lot of material in print and on the web is very sensationalized and inaccurate. If you are curious about a religion, it’s a good idea to talk to a variety of people who practice it, attend ceremonies and see what it’s about.\nHere are some books and links to get to you started:\nMaya Deren Divine Horsemen: The Living Gods of Haiti\nKaren McCarthy Brown Mama Lola: A Vodou Priestess in Brooklyn\nSallie Ann Glassman Vodou Visions: An Encounter with Divine Mystery\nPatrick Bellegarde-Smith: various books and essay collections\nRobert Farris Thomson: various books\nA more in depth Vodou FAQ:\nInterview w Manbo Sallie Ann Glassman:\nNPR interview with Dr. Patrick Bellegarde-Smith:\nWikipedia (the quality of articles varies):\nWhy did you write this document?\nA personal note from Saumya:\nI practice New Orleans Vodou as a Manbo Asogwe (priestess) and study it at Harvard as an ALB (undergraduate) candidate in Religious Studies. I’m tired of jokes about Voodoo dolls, and of people in my community being mocked, insulted and discriminated against because of their religion. I’m also involved in interfaith and intergroup facilitation and hope to help people learn about misunderstood traditions (such as Islam, Wicca, Pentecostal Christianity, Vodou and atheism) so they can stop fearing them.\nI’m American and my family background is Hindu. Growing up in Minneapolis in the 1970s, I heard many ignorant and hurtful things said about Hinduism and the Yoga/Meditation traditions that my family taught and practiced. My teachers and textbooks portrayed our religion in misleading and disturbing terms. Hindus in movies always seemed to be wearing weird costumes and involved in bloody rites. People thought we were a cult of devil worshippers.\nHappily, things change. Today, many people have friends, neighbors and colleagues who are Hindu. Practicing this religion is usually viewed as one aspect of holistic personhood, not something that defines someone as dangerous to others. Yoga studios are commonplace; no one who walks into one worries that they might be identified as being in a cult (at least, I hope not). My family, and many other families, played a part in this by practicing our traditions openly, answering questions, and responding to insults, fear and misconceptions with humor, patience, honesty… and occasionally, exasperation and outrage. The patience was more useful!\nThere are other religions practiced by Americans today that are misunderstood, and there are many communities who live in fear. I aim to change that. I am active in educating the public about Vodou but choose to focus on actual events, educational publications and news agencies. Popular entertainment is meant to entertain, not inform, which is why I write to the New York Times and the Oxford English Dictionary (they have added Vodou to their list of words to update!) but not Warner Brothers or Columbia Tri-Star Pictures.\nEveryone has the right to their own religion and culture. Everyone has something valuable to contribute to the conversation that is America. Our country was founded on these principles; we uphold them not just with laws and policies but in how we speak and listen, in how we live our lives every day.\nYours in Service,\nReverend Saumya Arya Haas\nHindu Pujarin (priestess)\nManbo Asogwe (Vodou priestess)\nALB Candidate in Religious Studies, Harvard University, School of Extension Studies\nDirector of Spiritual and Interfaith Program, New Orleans Healing Center\nPlease feel free to contact me with any further questions email@example.com']	['<urn:uuid:e09e18af-6c0d-4445-a480-2ce5cf4ba96a>']	open-ended	direct	short-search-query	distant-from-document	single-doc	novice	2025-05-12T10:19:05.071190	7	99	2265
33	which underground music venue came first prophet bar dallas phoenix theater petaluma	The Prophet Bar in Dallas came first, opening in 1985, while the Phoenix Theater in Petaluma began hosting rock and roll shows in the early 1980s when Tom Gaffey became manager in 1982.	"['“If you really want to see what Deep Ellum is about, The Prophet Bar is the best place to do it.” That’s the advice you’ll get from Russell David Hobbs about visiting one of the most popular neighborhoods in Dallas. He may be a little biased, though: He founded the place 30 years ago, and as the bar prepares to celebrate its third decade, he\'s still at the helm. But a whole lot else has changed in that span of time.\nWhen Hobbs moved to an empty warehouse in Deep Ellum back in 1984, he was really just looking for an escape from the starched, mainstream vibes of Dallas. Hobbs had a dream to clear the way for artists to be able to showcase their crafts without jeopardizing the quality or the richness of their talents. And with the help of friends Jeff Liles and Mark Lee, he did just that when he opened the Theatre Gallery. A year later, Hobbs opened up The Prophet Bar across the street and, years later, opened The Door in the old Gypsy Tea Room location.\nLike many of us, Hobbs had absolutely no idea what he wanted to do in college. So, logically, he went underground. “Honestly, I was going to college at [University of] North Texas in Denton and I was studying business,"" he recalls. ""I just didn’t really see myself in a shiny shirt, wearing a tie, going to work at the top of a tall, shiny building. So, I rented a warehouse in Deep Ellum in ’84, just to get away from it all.""\nHobbs cites his many artist friends as his motivation to create an artistic space where their work could be displayed and promoted. But it wasn’t long before he began booking shows, because at the time, as he puts it, “There was no original, live music scene in Dallas. All there was, was cover bands playing bars.”\nAs Liles remembers it, Hobbs’ intentions came from a desire to cultivate and encourage art in one of Dallas’ forgotten neighborhoods. He recalls Hobbs went to the library, did some research on Deep Ellum and discovered this rich past, filled with live-music and art. “It was completely sub-cultural and he just wanted to reconnect with that,” Liles says. “That was the spirit of the Theatre Gallery. It was this completely illegal, completely underground thing.”\nAnd underground it was. According to Liles, the Theatre Gallery didn’t have licenses for anything. Rather, it was a place where music and art enthusiasts could escape, pay a $5 or $10 cover for endless beer and listen to some good music.“It existed under the radar for about a year, a year-and-a-half before we ever had a telephone. We didn’t have any licenses; I mean the whole place was nailed together with plywood that Russell had leftover from a construction job,"" Liles says. “It was in the same spirit as those underground bars and live music venues in the Depression.”\nBut roughly a year after getting started, the Dallas Observer caught wind of their shindigs and began to publicize the happenings. Shortly thereafter, the venue owner began receiving unannounced visits from the Dallas Police. It was then that Hobbs decided to get legit, rent a space across the street [where Uncle Uber’s is today] and open up The Prophet Bar; both Hobbs and Liles even lived above the venue to get it up and running.\nDuring one of these early nights, Liles recalls a visit from Jim Heath, later known as The Reverend Horton Heat. Heath came by, feeling bummed about being kicked out of his former band. But with encouragement from both Liles and Hobbs, Heath began playing every Monday night at The Prophet Bar. To Hobbs, this was a prime example of what his purpose was as a venue owner in Deep Ellum. “The thing is, a venue really is the mediator between the artist and the people. That’s the job,” he says.\nIf you like this story, consider signing up for our email newsletters.\nSHOW ME HOW\nYou have successfully signed up for your selected newsletter(s) - please keep an eye on your mailbox, we\'re movin\' in!\nFrom there, things only went up. The Prophet Bar drew in big names like the Old 97’s and Jane’s Addiction. And in 1998, Hobbs expanded his venue ownership to include The Door in the old Gypsy Tea Room space. But by 2007, the Prophet Bar was moved to its current location at 2548 Elm St. — mainly because the size of its shows became too much for the original site to handle.\nHobbs has kept true to his eclectic nature though, incorporating artists from every genre and creed at his venues. ""Deep Ellum started off as a really laid-back, no neon, real showcasing art, organic experience in the ’80’s,” he adds. “And really, The Prophet Bar and The Door, we still operate within those lines.”\nNow, 30 years after going above ground, Hobbs and The Prophet Bar are ready to commemorate those years past. This weekend the bar will host a two-day event celebrating the Deep Ellum community, artistry and the perseverance of a venue-owner who took a chance on a run down, abandoned part of the city. And hey, in case you want a chance to relive the \'80s, Soul Asylum, the Meat Puppets and Ten Hands (among others) are playing.\nHobbs sees no reason to change course any time soon, either.”We’re just going to keep trying to showcasing real art, as much as we can, for a good price. We’re just going to keep doing what we’re doing — pumping out 30 to 50 bands every week.”', 'Petaluma’s Phoenix Theater has a century of colorful and varied history. From opera house to movie theater to rock and roll venue and teen center, the Phoenix has repeatedly emerged from the ashes, recreating itself as a valuable community resource.\nAncient Times—Opera, Fire, and Silent Film\nThe Phoenix began as a rare old bird—a small town opera house. Opened in 1905 as the Hill Opera House, its stage was graced by the likes of Harry Houdini, Enrico Caruso, and Lily Langtree.\nIn the roaring 20’s it was gutted by fire, to rise again as a movie theater two years before The Jazz Singer brought sound to popular film. It became the California Theater in 1935 and for decades continued to bring movies to Petaluma audiences.\nThe Middle Ages – More Fire, Tom Gaffey, and Rock & Roll\nFire struck again in 1957, destroying the ceiling, and once again our Phoenix returned to life, this time as the Showcase Theater. The owner hired a teenager named Tom Gaffey, who was fond of hanging around the building with friends. Tom left Petaluma to travel, but when he returned in 1982, the new owner hired him to manage the theater. It then became The Phoenix Theater, for its ability to rise from the ashes.\nDuring the early 1980’s the Phoenix began to augment film screenings with late night shows from nationally known bands, including the Ramones, the Red Hot Chili Peppers, and X. Local bands like then-unknown Metallica and Primus polished their acts during these late night gigs.\nOver the years, the Phoenix grew in popularity with the local youth. It wasn’t just the music; Tom opened the doors in the afternoon to kids who might otherwise be out on the streets, or at an empty home. The Phoenix became a haven for kids who found a safe place to spend unstructured time with their peers, around a responsible adult who understood, respected, and supported them—and who grew up in the same environment.\nThe Phoenix occasionally pulled duty for other events, too. It was a popular venue for midnite screenings of the cult classic Rocky Horror Picture Show. In 2000, it was the home to the World’s Wristwrestling Championship.\nThe Modern Era—Telecom to the Rescue & The Petaluma Phoenix Center Nonprofit\nWhat fire could not accomplish was almost achieved in the late 1990s. The landlord announced plans to sell the Phoenix for demolition and reconstruction as an office building. The community rallied to save it, but there was little they could do, and for a while it seemed the venerable Phoenix was doomed.\nThe sale was in escrow when four employees of Cerent Corporation (all of them musicians, two of them former Phoenix patrons), stepped forward and took over the escrow. (The four men recently had been enriched from a dot com buyout by Cisco Systems.) With other prominent local citizens, they formed the non-profit Petaluma Phoenix Center, Inc, to own The Phoenix Theater and maintain and grow the services the Phoenix provides to the community.\nPhoenix Performers Over the Years\nAgainst Me! · Bad Religion · Blink 182 · Blue Oyster Cult · Bone Thugs-n-Harmony – Buddy Guy · Cherry Poppin’ Daddies · Count Basie · Cowboy Junkies · Deftones · Devo · Digital Underground · DRI · Enrico Caruso · Faith No More · Fishbone · Gatemouth Brown · George Thorogood · Goldfinger · Green Day · Hanson · Harry Houdini · Hillary Duff · Huey Lewis · Ice T · Incubus · It’s a Beautiful Day · Jimmy Cliff · Jimmy Ray Vaughn · Ladysmith Black Mombazo · Lawrence Ferlinghetti · Less Than Jake · Lily Langtree · Little Tin Frog · Los Lobos · Mac Dre · Mac Mall · Metallica · Misfits · Mr. Bungle · MxPx · Neville Brothers · No Doubt · NOFX · Offspring · Penny Wise · Primus · Prong · Ramones · Rancid · Ray Charles · Reel Big Fish · Red Hot Chili Peppers · Run-DMC · Santana · Social Distortion · Sublime · Suicidal Tendencies · The Specials · The Velvet Teen · The Wailers, without Bob Marley · Tiger Army · Tsunami Bomb · Van Morrison · Violent Femmes · X\nAND MANY MORE']"	['<urn:uuid:4f5bd3df-d2e8-4f1b-9a4d-882e3e7bebe9>', '<urn:uuid:5a29212c-8298-440e-a93d-594196371f33>']	factoid	direct	long-search-query	distant-from-document	comparison	novice	2025-05-12T10:19:05.071190	12	33	1647
34	which more long term planning agenda 2063 or fiji climate act	Agenda 2063 has a longer planning horizon of 50 years, while Fiji's Climate Change Act focuses on achieving net-zero emissions by 2050, which is approximately 31 years from now. Agenda 2063 is implemented through rolling plans of 25, 10, and 5 years, while Fiji's Climate Act provides a framework for shorter-term actions and immediate measures like plastic bag bans starting from 2020.	['1. Agenda 2063: What it is\nAgenda 2063 is both a Vision and an Action Plan. It is a call for action to all segments of African society to work together to build a prosperous and united Africa based on shared values and a common destiny.\nIn their 50th Anniversary Solemn Declaration, the Heads of State and Government of the African Union (AU), while acknowledging past successes and challenges, rededicated themselves to the continent’s accelerated development and technological progress. They laid down vision and eight ideals to serve as pillars for the continent in the foreseeable future, which Agenda 2063 will translate into concrete objectives, milestones, goals, targets and actions/measures. Agenda 2063 strives to enable Africa remain focused and committed to the ideals envisaged in the context of a rapidly changing world\n2. Why a 50 Year Agenda?\nFifty years is, undoubtedly, an extremely long development planning horizon, however strategic we may call it. Is this a serious choice? How can one realistically plan given breath-taking changes globally and within the continent? What are the benefits of such long-term planning? These are some of the questions raised by many people from all corners.\nThe choice of a 50 year time must be understood within the context of the 50th Anniversary of the establishment of the OAU; and the need for the continent to take stock of achievements, successes/ failures and map out a long-term vision as well as set goals and targets.\nIn operational terms, the Agenda 2063 would be a rolling plan of 25 years, 10 years, 5 years and short term action plans.\n3. Agenda 2063 - Why now?\nThis new effort to envision Africa’s long-term development trajectory is timely for several reasons:\nThe above factors constitute a unique opportunity for Africa to capitalize upon. Agenda 2063 while seizing these opportunities, underlines the fact that success depends on unity of purpose; transparency; placing citizens’ first; sound governance; willingness and capability to assess performance and correct mistakes timely.\n4. What is the added value of Agenda 2063 and how does it take account of past and ongoing policies and programmes at national, regional and continental levels?\nAgenda 2063 should be seen as a logical continuation of recent developments with renewed and invigorated efforts to catalyze development and strengthen African integration and unity. It aims to build upon the achievements and draw lessons from earlier strategic planning efforts at the regional and sub-regional levels, including the Lagos Plan of Action, the Abuja Treaty and the NEPAD, to address new and emerging issues in the continent over the short, medium and long-term. Indeed, Agenda 2063’s guiding principle is continuity of actions, drawing appropriate lessons, building upon what has worked in the past; and in brief making every effort to do things better.\nThus Agenda 2063 should be seen as a unique opportunity to recreate the African narrative by putting it into perspective to energize the African population and harness citizens creativity through a set of clear goals that citizens participate in setting for unity, peace and development in the 21st century. The thrust of Agenda 2063 is a program of social, economic and political rejuvenation that links the past, present and the future in order to create a new generation of Pan Africanists that will harness the lessons learnt and use them as building blocks to consolidate the hope and promises of the founding parents for a true renaissance of Africa.\nAgenda 2063 is expected to be a source of inspiration for development of national and regional sustainable development plans.\n5. How would strong political commitment for Agenda 2063 be sustained over the long-term?\nThrough the Solemn Declaration of the Assembly of Heads of State and Government of the African Union at Addis Ababa, on 26th May 2013, Africa’s top political leadership committed itself to long term national and continental development and technological transformation through a strategic planning and implementation process. This is an encouraging starting point that offers a solid entry point to translate political statements and commitments into concrete actions, provide the means to track progress and take timely decisions on corrective measures.\nAgenda 2063 envisions several measures at the national level that include: among others, designation of national focal points for Agenda 2063, integrating Agenda 2063 milestones/targets into national planning frameworks, as well as regular progress reports on Agenda 2063 to cabinet retreats and to National Assemblies/ Parliaments.\n6. What are some of the critical success factors to be factored in the conception, design and implementation of Agenda 2063?\nApart from the strong political commitment stated above, other critical success factors include the following:', '“Climate Change Act will make sure we deliver climate action and ambition in Fiji” – Minister of Economy Aiyaz Sayed-Khaiyum’s Ministerial Statement\nI wish to inform Parliament and the nation of an important new measure that the Fijian Government intends to implement over the coming months to better prepare us for the challenges we face from climate change. It is a Climate Change Act that will enshrine, in law, our domestic response to the climate threat and the threat to our oceans and place that law at the heart of our national policies and priorities.\nStretching across the whole of government and much of the private sector and necessitating changes to a range of existing legislation, the Act will be comprehensive, holistic and easily understood. It will:\n- provide a framework to guide Fiji’s implementation of the Paris Agreement;\n- co-ordinate our national response to climate change and ocean health; and\n- give us the legal tools to carry out our most important job of safeguarding the Fijian people and their communities from the climate threat now and into the future.\nMr Speaker Sir, work has been underway on this legislation for the past six months and we will soon embark on an appropriate period of public consultation on the draft bill, with further input from international experts to ensure it meets the standard of world’s best practice. We are, in fact, one of the few small number of countries in the world to produce such a bill and right from the start of this process, we have been determined to make it the best.\nThe climate bill will be introduced in Parliament in the September session before the United Nations Secretary General’s Climate Summit and, with the intention of being enacted into law before COP25 in Chile in December of this year. We hope that other nations will of course emulate it.\nSome of its provisions have already been announced in the Budget, including measures to reduce plastics pollution – a key threat to our waterways and oceans.\n- a ban on all single-use plastic bags including those with and without handles from 1 January next year;\n- an increase in the plastic bag levy from 20 cents to 50 cents on low density polyethylene (LDPE) plastic bags from 1 January next years;\n- a ban on styrofoam from 1 January 2021 following a series of national consultations;\n- and zero duty on non-plastic food packaging such as straws, containers and cutlery.\nPut simply Mr Speaker sir, Fijians need to wean themselves off plastic if we are to improve the health of our oceans on which much of our food and livelihoods depend. So we encourage the use, for instance, of reusable shopping baskets and bags made and woven in particular by Fijian women and artisans. Of course before the advent of single use plastic bags which is only a few decades ago, we all used these reusable or nature based bags.\nMr Speaker Sir, before we provide information on some of the other provisions of the Act, we wish to explain its importance, the context in which it is being introduced and the absolute imperative of everyone getting behind it.\nThe Fijian Government seeks cross-party support and indeed national support for this new legislation in the Parliament, along with the support of every Fijian. Because on any fair appraisal, it is responsible, it is balanced and it is necessary. In the interest of demonstrating Fiji’s resolve and persuading other nations to enact similar laws, for climate change. We must speak with one voice and support the Climate Act.\nCombined with the increase we intend to make in our Nationally Determined Contribution – our NDC – to reduce greenhouse gas emissions, we will be sending the world yet another powerful message: That our successful presidency of COP23 and the unprecedented mana it gave us in the community of nations was not the end of Fiji’s global climate leadership. Far from it. It was merely the opening round of our fight for survival here in the Pacific in which we intend to keep punching above our weight in the coming years for ourselves and our neighbours.\nMr Speaker Sir, after we surrendered the COP presidency last December, we continued to play a leadership role at COP24 in Poland, where our concept of a Talanoa Dialogue to boost climate ambition was enthusiastically embraced by the nations of the world. In our own region, our leadership of the Climate Action Pacific Partnership in May has set a new standard of inclusiveness and partnership between governments, civil society and the private sector.\nNext comes the Pacific Islands Forum in Tuvalu next week in which Fiji re-joins the Leaders’ Summit to make the case for more ambition on the part of the island states and our larger neighbours. Then comes the UN Secretary General’s Climate Action Summit in New York next month, where Mr Guterres has issued a challenge to the whole world: come with plans to increase your 2030 emission targets and achieve carbon neutral economies by 2050.\nMr Speaker Sir, all of us in this Parliament and every Fijian can be proud that we intend to meet this challenge with the revised NDC that we will be taking to New York through the Honourable Prime Minister – a faster transition to renewable energy, more efficient utilisation of our forests and mangroves – nature’s carbon capture – and a range of innovative mechanisms to achieve the net zero emission target 31 years from now.\nThen comes COP25 in Santiago, Chile, in December, where the Honourable Prime Minister will again spearhead our campaign on behalf of the Pacific and other vulnerable nations as we know that we are also chairing PSIDS. The Honourable PM has become an acknowledged statesman in the climate arena. He is on the High Level UN Panel put together personally by the Secretary General on Ocean with other heads of state and government across the globe , including those from Australia, Canada, Indonesia, Chile, Mexico ,Portugal and, a number of others. I am also on the Global Commission on Adaptation that is chaired by Ban Ki- moon, the former UN Secretary General, and co-chaired by Bill Gates and the CEO of the World Bank.\nMr Speaker, Sir, we must support the Honourable Prime Minister and the rest of the Fijian climate team as we rise to the challenge before us over the coming months. Because if anything, Mr Speaker, that challenge is greater now than ever before.\nJuly, Mr Speaker, has been the hottest month the world has ever recorded. And all over the planet are alarming signs that the climate threat is escalating at a far greater pace than scientists had predicted. The Arctic ice is melting faster than we thought and the European summer has seen record temperatures across the continent, including the hottest day ever recorded in the French capital and cradle of the Paris Agreement – 42 degrees.\nThere are more droughts. More floods. More devastating wildfires across the world. Ever more serious impacts on agriculture and the world’s ability to feed itself. And here in the vast Pacific sits our beloved Fiji. Small and increasingly vulnerable as we scan the horizon anxiously year by year for the kind of extreme weather event that three years ago took the lives of 44 of our loved ones and inflicted damage equal to one third of the value of our GDP. And which, God forbid, could be much worse in the event of a direct hit on the whole of Fiji and especially our capital.\nMr Speaker Sir, that is the grave situation in which we find ourselves through no fault of our own and why this Government puts such a strong emphasis on the climate issue. This is a fight for our lives and our livelihoods. There is no room for cynicism, no room for complacency. We cannot afford climate change fatigue to set in Fiji. Because if anything, the outlook is worsening.\nMr Speaker Sir in Bonn in May, we suffered a significant setback when the nations of the world – under pressure from certain fossil fuel producers – set aside the IPCC scientific report endorsing the call for global warming to be capped at 1.5 degrees Celsius above that of the pre-industrial age. Mr Speaker Sir you will recall that the 1.5 degree target has been central to the efforts of Fiji and other Pacific island nations to achieve a greater response to the climate threat. It was first enshrined in the landmark Suva Declaration of the PIDF four years ago before going truly global, embraced by countries around the world. Well, now that 1.5 is off the table altogether at the UNFCCC level – the global climate negotiations, it is a blow to our hopes for more ambition that Fiji regards as grossly irresponsible and selfish.\nMr Speaker Sir, as the impacts of climate change accelerate and attempts are made to weaken global ambition, we must listen more than ever to the scientists, not the climate deniers or those motivated by self interest or political interest. So we will be renewing and reinvigorating our campaign for 1.5 to be reinstated as the benchmark ambition. We will be marshalling those who share our concerns and asking the Chilean Presidency to do everything possible to put 1.5 back on the agenda at COP25.\nMr Speaker Sir, at home, our Climate Change Act will be a legal umbrella to make sure we deliver climate action and ambition in Fiji – to lead by example rather than merely look to others to solve the problem. Let me highlight some of the key features Mr Speaker Sir:\n- The Act will implement Fiji’s commitments and obligations under the Paris Agreement and our enhanced NDC.\n- It will establish a comprehensive framework for Fiji to achieve its long-term emissions reduction target of net-zero by 2050 by establishing a system for the measurement, reporting and verification of greenhouse gas emissions.\n- It will set up a national greenhouse gas inventory and outline pathways for mitigating our emissions.\n- It will legally support key policy documents already issued, including the National Climate Change Policy, the Low Emissions Development Strategy, the National Adaptation Plan and the National Planned Relocation Guidelines.\n- It will provide a framework for adaptation and resilience development, including conducting an audit of existing infrastructure. Decision makers will be required to ensure that new infrastructure has undergone a climate risk and resilience assessment.\n- The Act will require government, public and private actors and other entities to consider climate change risks in making decisions.\n- It will require environmental impact assessments when we build new infrastructure so that where possible, losses from events like Cyclone Winston can be reduced. And where we rebuild, we will build back better.\n- The Act will reward Fijians who develop projects that assist the climate effort and reduce emissions with Fijian carbon credits that can potentially be sold in Fiji or traded internationally. This includes those who provide cleaner transport or protect forests and mangroves and who will benefit from mechanisms in the Act that support this.\n- It will establish principles and procedures for the relocation of communities that are at risk from the adverse effects of climate change and support the Relocation Trust Fund that has already been approved by Parliament. We will be doing an international launch through the Honourable Prime Minister of this Trust Fund at the UNSG Climate Summit next month.\n- And the new Act Mr. Speaker Sir, will recognise the need for a healthy ocean to deliver a healthy climate and establish that the two cannot be separated in an ocean-based nation such as Fiji and indeed for the Globe.\nMr Speaker, Sir. We want to help businesses and our communities manage and adapt to the impacts of climate change. As the first and one of the few governments in the world to combine our Ministry of Economy and the department of climate change, we understand the intrinsic relationship between a strong economy and a stable climate agenda.\nMr Speaker Sir we want to assure Honourable Members that the Government will engage with all stakeholders in the spirit of cooperation and goodwill. Because under pinning the new Act. Mr Speaker is the principle the Honourable Prime Minister has always enunciated both domestically and locally that all of us are in the same drua, the same canoe, when it comes to climate change. We are, in fact, hoping that as many Fijians as possible take advantage of the opportunities the Act will create to undertake projects that reduce emissions. And in doing so, earn carbon credits provided for under the legislation.\nMr Speaker Sir, we hope that our development partners will recognise the importance of this Act and assist us with its implementation, noting that countries like Australia, NewZealand and the United Kingdom all have deep expertise in this area that we have drawn on in designing our own law.\nWe also need to constantly find innovative and affordable financial products that will assist us to provide sustainable funds to meet our climate and development-related objectives-indeed we say they are the same. We need to be able to leverage off not just our biltateral and multilateral partners but also involve the private sector.\nWe look to our development partners to assist us with implementing specific provisions that qualify for access to the increasing pool of funding available for climate-vulnerable nations and for improving the state of our oceans.\nOur Green Bond launched at the London Stock Exchange by the Honourable Prime Minister last year made history as the first to be issued by an emerging economy. We have plans to bring additional climate finance to Fiji and you will be hearing more about our efforts to attract this investment in the coming months. We must in this respect also ensure that the costs of bonds are attractive and sustainable. We look forward to working with our development partners to assist in crowding in private funds in the climate bond market at attractive and sustainable rates.\n- In respect to that Mr Speaker, Sir we are starting work on the design of a new Blue Bond to attract climate finance to support the restoration, rehabilitation and preservation of mangroves and sea grasses in Fiji and other Pacific countries. As a pilot project under Article 6 of the Paris Agreement, this Blue Bond would generate mitigation and adaptation outcomes for investors while at the same time building sustainable fisheries and assisting local communities, for example local fishermen and women etc in meeting compliance and safety requirements and costs.\n- We are exploring, with the Green Climate Fund, opportunities to secure early results-based payments for protecting our forests under the GCF’s REDD+ scheme.\n- And Mr Speaker, Sir we are leading, with the Marshall Islands, the Pacific Blue Shipping Partnership that is working on a blended and innovative finance structure to support the decarbonisation of domestic marine transportation fleets and facilities in Fiji and across the region. This means replacing inter-island ships with more efficient hybrid ships, thereby reducing fuel costs and emissions.\nMr Speaker Sir, as we have already signalled, a key focus of our Climate Change Act is recognising the need for a healthy ocean to deliver a healthy climate. As the Members may know, we made reference in the Budget to the Government’s plan to strengthen the Exclusive Economic Zone around Fiji.\n- We are pleased to announce the development of a National Oceans Policy under which Fiji plans to moves to a 100 per cent sustainably managed Exclusive Economic Zone, with 30 per cent of this being earmarked as a marine protected area by no later than 2030. Fiji will be asking other Pacific nations to join us in this ambitious venture at next week’s Pacific Islands Forum and at the UNSG Summit that will be held next month.\n- Mr Speaker, Sir Fiji is also asking other Pacific nations at the Pacific Islands Forum leaders meeting next week to join us in imposing a 10 year moratorium on seabed mining from 2020 to 2030.This would allow for a decade of proper scientific research of our economic zones and territorial waters.\n- In addition, Fiji and other Pacific countries are seeking funding to develop a world-class Marine Research Centre and Marine Education Programmes at the Fiji National University and University of the South Pacific.\nMr Speaker, Sir in conclusion we ask all Honourable Members to support these initiatives which we have outlined, all of which are part of a considered and holistic response by the Government to the increasing challenges we face from climate change and the pressures of development on land and at sea. We have an absolute and whole-of-government commitment to the 2030 global sustainable development agenda. It is the cornerstone of our national policies as we have the Sustainable Development Goals incorporated into our five and twenty year National Development Plans, with specific targets and policies aligned with the long-term transformation of our nation and its economy Mr Speaker, Sir.\nMr Speaker, Sir we also express Government’s appreciation to those of our development partners who are assisting with this national effort and ask them to do what they can to provide continuous support and assistance for the various initiatives that we have outlined today.\nThank you, Mr Speaker.']	['<urn:uuid:785efafe-5cc6-474e-b744-01f837bc9275>', '<urn:uuid:871a6f17-2312-4925-962c-809b8a4a616c>']	factoid	direct	short-search-query	distant-from-document	comparison	novice	2025-05-12T10:19:05.071190	11	62	3670
35	I'm interested in farming technology and heard about new ways to detect plant diseases early. What are some innovative methods being developed to spot plant diseases before they become visible to farmers?	Several innovative technologies are being developed to detect plant diseases early. Plants naturally emit volatile organic compounds (VOCs) as signals when they're under attack, which can now be detected using miniaturized gas chromatographs mounted on robotic arms. This allows detecting diseases before visible symptoms appear. Additionally, biosensors like the LAMP assay system are being developed to analyze airborne disease spores, and companies like FERA offer DNA testing services to measure disease presence in leaf samples. Some systems even use 'electronic noses' to sniff crops for early disease detection, and there are technologies that analyze electrical signals emitted by plants to provide early warnings of crop stresses and pathogens before visual symptoms appear.	"['Each year, U.S. farmers lose up to 12 percent of their crops to disease and another 12 percent to pests. Sickly plants account for billions of dollars in lost profit in the agriculture industry.\nThe trouble is, plant diseases are hard to spot before it’s too late. By time there are visible symptoms—wilted leaves, for instance, or discoloration—the disease has advanced enough as to be untreatable.\nBut plants do let their surroundings know when they’re sick or under attack, just not in a way we’ve ever been able to understand in real time—until now. A team of engineers led by Gary McMurray, division head for food processing technology at the Georgia Institute of Technology (Georgia Tech), has developed a method to monitor and decode those messages to allow farmers to identify and treat diseases before they take root. His vision: robotic arms that capture and identify plants\' natural disease signals on the fly.\nAll plants produce natural signals, in the form of volatile organic compounds (VOCs), to alert the surroundings of what’s attacking them. McMurray’s system captures those compounds using a miniaturize version of a gas chromatograph (called a micro GC). The devices, which were originally developed around the turn of the 20th century, are used to separate chemicals within complex samples—in McMurray\'s case, the gasses a plant emits. As gas is heated and passed through a column, and an electronic sensor detects the compounds the sample contains.\nThere’s no pinching or plucking of samples and running them back to the lab for analysis—a process that can take days or weeks to complete. All the information the micro GC needs is in the air, which means treating diseases, or saving plants from them, could get a whole lot faster.\nThe applications for gas chromatography stretch well beyond crops. The Department of Homeland Security, for instance, uses them to detect certain types of gases and hazardous chemicals. And researchers have are also using them to screen for certain types of digestive disease in humans.\nTraditionally, chromatographs have been large devices anywhere from 3 to 10 meters long; advances in nanomaterials and manufacturing have allowed McMurray to create one that’s the size of an 8-volt battery.\n“We could have never built this; even five years ago it wouldn’t have been possible,” he says.\nMcMurray\'s micro GC could be mounted to a robotic arm on existing farm equipment, such as a tractor or plow. The arm would hold the device above the plant leaves and gather air samples. A small computer, no more powerful than a basic laptop or iPhone, can then process data from the samples to identify any pathogens; a quick flush of helium prepares the sensor to evaluate its next sample.\nBecause they\'re so small, ""[we] can build multiple micro GCs onto one robot,"" McMurray says. “I can have 10, 20, even 100 of them on a single tractor.”\nThat means one tractor could gather samples from stalks, roots and buds simultaneously.\nThe micro GC system could also be used to screen all of our food—from fruits to vegetables and grains—for disease.\n“Fresh produce that’s being shipped around [the globe] could be carrying various pests or diseases. If you have some sort of field-deployable or mobile sensor, you could detect the VOCs that come off of the plants,” McMurray postulates. “It could solve some very big problems.”\nThe team is currently finishing lab testing for the micro GC to make sure its results are consistent with those from larger chromatographs. In August or September, they will run their first field test, in which a researcher will walk a micro GC through peach fields to test for Peachtree Root Rot.\nWhile that initial test will focus on a specific disease, gas chromatography can screen for dozens of pathogens at once, which also distinguishes it from other approaches, McMurray says.\nEven with the technology, plant pathologists will still have to map the VOC emissions associated with certain plans and certain diseases.\nPlants whose VOC emissions are already well documented will have a leg-up when micro GC screening gets underway; others will require more time and research to diagnose and treat. “The pathogen we chose, no one knows anything about,” McMurray says, “but, for example, there is a lot known about certain fungi.”\nBut the Georgia Tech micro GC is an important turning point in usability and scalability, McMurray says.\n“What we have we think is unique,” McMurray says, “these new manufacturing processes are opening up a whole new era of sensors.”', 'Five years ago, when people started talking about Digital Farming and Big Data, most were not sure what this was or how it related to field base crop production. Since then, writes Keith Norman, the speed of technical innovation, its benefits and uptake by industry has accelerated beyond the hobbyists, which goes to show the financial benefits are being increasingly recognised and exploited by farmers and growers.\nHowever, there is still a long way to overcome some of the barriers to uptake, such as standardisation of connectivity, conformity of data for cross platform usage, rural connectivity and speed of data transmission, cost and resistance to change.\nSoil sampling for P&K and pH has been routinely carried out for many years using the traditional approach of walking a “W” shape in each field. The introduction of grid sampling, using one sample point per hectare, combined with Kriging allowed contour maps to be generated for each nutrient, which can be used for variable rate applications using precision equipment. Electromagnetic conductance maps have also been developed illustrating soil type variation within a field, from which more targeted sampling can take place.\nNutrient mapping technology has moved further with scanning equipment using Gamma Radiometrics. The level of four naturally occurring isotopes (Caesium, Uranium, Potassium, Thorium) are detected in the subsoil/topsoil, providing an overview (more than 800 reference points per hectare) of all nutrient levels, plus pH, soil texture, organic matter and cation exchange capacity (CEC): in total 21 different metrics. This technology is not affected by soil moisture, compaction, crop or cultivation, which offers a much wider sampling window. Terramap from Hutchinsons has recently been independently endorsed in an independent evaluation by NIAB whereby it was compared to grid sampling and EC Scanning. A similar endorsement for gamma radiometrics compared to conventional approaches has been published by Wageningen University.\nIn situ soil nutritional sensors have also started making an appearance on the market, providing real-time measurements of soil moisture, salinity, NPK, aeration, respiration, air temperature, light, and humidity. These are useful to monitor real time nutrient uptake by the surrounding crop to ensure there are no phases of crop development where nutrients fall below thresholds. Some probes, such as FungiAlert’s SporSenZ go further, allowing farmers to understand the soil’s microbial community. Such soil health indicators could also be used to understand the “inner workings” of the soil microbiome and how management practices impact on the delicate balance of its components. One such sensor is being developed in an Innovate UK project led by PES Technologies that hopes to give an instant, in-field measurement of the activity of the soil microbiome through sensing VOCs (Volatile Organic Compounds). The Small Robot Company, which is also part of the consortium is looking to automate the sampling process.\nRemote Tillage Detection (RTD) could also become important should carbon sequestration and a trading market develop in the UK. Such a market exists in the US with sequestrated carbon selling for about $15-20 per tonne. There are many management practices a farmer can use to sequester carbon, all of which need to be verified, for example, rotation and cropping, previous crop residue removal, and more importantly cultivation type used. Hummingbird Technologies is developing the capability of remotely detecting tillage method (plough, min-till or direct drilling). Crop type recognition is also being developed so that field specific rotations can be identified using archive satellite data. In North America, Dagan Inc, and Radicle are also developing such capabilities.\nSubsoil compaction is known to affect soil function and root development, but conventional sampling methods do not facilitate the acquisition of high-resolution spatial compaction data on a field wide basis. A team at Wageningen University is using ground penetrating radar to map subsoil compaction, with the aim of creating a compaction map for each field, which can then be interpreted for a differential approach to subsoil management, only subsoiling areas that need it, and at the correct depth. The benefits of a spatial approach to subsoiling are significant in terms of labour cost, fuel usage and time.\nVariable seed rate technology is available from several suppliers, and has seen a good uptake by industry. There are clear benefits to having a uniform plant population throughout a field across different soil types, mostly from targeted nitrogen application, tiller management and growth regulation.\nOnce crops are established, plant counting and sizing are now possible for some horticultural crops such as lettuce, cauliflower broccoli and pumpkins with companies such as Hummingbird Technologies, Solvi and Earth Rover. Not only can individual plants in a field be counted, but sizing information can also be presented, which is really important when it comes to matching size to supermarket contractual obligations.\nThere are now many types of optical sensor for looking at crop health, biomass and chlorophyll in cereals. Crop reflectance works in the visible and near infrared region (NIR) of the spectrum, and at least two wavelengths are combined to calculate vegetation indices. The near infrared light, not visible by the human eye, is reflected by the leaf mesophyll cells, resulting in a much higher reflectance than visible light. Using both wavelengths it is possible to evaluate the colour and biomass of a crop using a measurement called NDVI (normalised difference vegetation index). NDVI is very useful for agronomists and farmers to inspect areas of crops where problems are highlighted and where remedial action is necessary, rather than walking lots of “good” hectares. Similarly, remote sensing / imaging technologies are now widely available from many manufacturers and may be tractor-mounted, hand-held or use satellite technology through a computer or phone app.\nVariable rate nitrogen mapping, offered by many, exploits this natural soil/crop variation and matches nitrogen amount to biomass and growth of the crop. Examples of this include; Yara’s AtFarm and VRN products from Hummingbird, SOYL, DroneAg, Rhiza, Precision Decisions and Omnia\nGrowth stage prediction is a feature that some providers offer, such as Omnia from Hutchinsons, which uses climatic information combined with physiological crop modelling to predict when key growth stages are reached. For root crop growers, ground penetrating radar is being developed to provide a non-invasive way of looking at crop development in terms of size and shape. An Innovate UK funded project led by B-Hive Innovations is developing capability to identify tuber size and shape and quantity.\nMost yield prediction systems are based on the original WOFOST model, originally developed by Wageningen University but now maintained by the Joint Research Centre of the European Commission. The more recent introduction of AI makes modelling very large datasets of weather and crop metrics a much easier task, meaning it is now possible to get yield estimates down to individual field level.\nAccuracy increases as harvest date approaches. Most yield prediction is reasonably accurate at 30-days pre-harvest, but the challenge is to extend that to 60- and 90-days pre-harvest so there is still the potential to change input choice within and between fields to maxmise gross margin.\nCrop inventory and field benchmarking not only benefits farmers but it is also key information for many commodity trading organisations, government and supply industries, with an accurate hectarage, on a crop-by-crop basis, in regions or at national level.\nThere are many new innovations being developed for the detection of diseases, all of which work in a slightly different way but aim to give an early warning a specific disease of interest is in the early stages of infection. This “intelligence” at field level enables growers to use the optimum selection fungicides and timings, rather than using a prophylactic spraying approach. For example, Burkard and Rothamsted are developing a LAMP assay bio sensor, where air is sucked into the sensor and the spores are disrupted to release DNA for identification and quantification by a series of ‘in-trap’ laboratory tests. Results are then sent wirelessly to a server, using an internal 4G router. The Earlham institute (EI) has developed a similar system called AirSeq using nanopore technology, a new generation of DNA sequencers which read longer pieces of DNA than previously possible called the MinION platform This was discussed at a recent EI event attended by CHAP.\nThe University of Manchester and Sony have developed a totally different type of biosensor with Gates Foundation backing that uses a simulated leaf surface, impregnated with specific biochemicals known to stimulate the germination of the target disease. A micro camera in the sensor detects the hyphal growth and, through AI, recognises the target pathogen and sends an alert.\nFERA offer a qPCR service for farmers to send in leaf samples and the DNA of any latent target disease is extracted and measured in picogrammes. This gives a grower an indication of the amount of disease present, but as yet there is no real calibration between the resulting pico grammes of a disease and what level of fungicide is needed to control it. However, it is a very useful tool during dry springs when deciding fungicide mixtures and rates.\nPre-symptomatic disease detection is only one part of the story: the resulting spatial application of fungicides, in different combinations and dose rates is also an important consideration. Various digital technologies are being developed to deliver the science into practice in field. Often variable rate fungicide application assumes that thicker, higher biomass areas of crops have a microclimate more conducive to disease development. NDVI maps of a field can help identify such areas to create a targeted spatial application map.\nHowever, there are problems varying the rate of one tank mix component, such as a fungicide, while delivering a constant rate for other components such as insecticides or trace elements. A similar scenario exists with variable rate growth regulator and other tank mix partners.\n“On the move” rate variability can be overcome using direct injection spraying systems, where undiluted pesticide is placed into canisters on the sprayer, and plain water is in the sprayer tank. The pesticide is then metered and introduced into the water on the pressure side upstream from the boom sections, with the rate being varied by the speed of the direct injection pump. One example of this is Raven’s Sidekick Pro available as factory option on Case and John Deere sprayers, or as a retrofit to any sprayer. In addition to these technologies, small scale precision application is also being explored by the industry, such as CHAP’s Innovate UK project Slugbot, which precisely applies biopesticides where they are needed.\nNorth Carolina State University is developing a Volatile Organic Compound detector for late blight in potatoes. A small electronic nose “sniffs” the crop and gives an instant readout if the very early stages of blight are detected. The device is still handheld at the moment, but there are plans to make it drone mounted so crops can be flown and random points of inspection can be made to check a crop’s disease status.\nVolatile Organic compound detection is being used in Florida’s Orchards to detect a disease called “citrus greening” which affects orange, lemon and grapefruit trees. Sniffer dogs can detect diseased trees up to two years before symptoms appear. Similar work is happening in Canada with dogs sniffing Clubroot. The aim is to develop and automate a similar detection methodology on a drone.\nDisease modelling has an integral part to play in spore detection technologies. Knowing that there are spores in the air is one thing, but knowing how they will develop is another. The University of Reading and Rothamsted have developed a Septoria prediction model based on accumulated rain and accumulated minimum temperature pre GS31. The known varietal resistance is also used in the model output.\nCHAP and Fera have partnered to create the disease prediction tool CropMonitor Pro, which provides information sourced from monitoring sites located across the country and reports up to date measurements of crop pest and disease activity in arable crops throughout the UK.\nIntelligent insect traps are being developed whereby insects are lured into a trap, and are quantified and identified because of their size and shape. One example is DTN’s SmartTrap, which can identify up to 16 different crop pests through an onboard camera that counts and reports on them in near real time. It can even distinguish between target and non-target pests.\nAnother bio-sensor based technology that records and analyses electrical signals emitted by plants is being developed by Vivent. The system called PhytlSigns, provides early warnings of a wide range of crop stresses, including nutrient deficiencies, environmental stresses, pathogens and insect infestations well before visual symptoms, enabling farmers to action early interventions.\nYield mapping is a well-established technology in cereal combine harvesters, but the technology is now being developed for forage harvesters and root crop harvesters too. This digital technology gives farmers and growers realtime evidence to look back at their different approaches to crop production and management. It also allows field zones to be identified that are consistently high or low yielding, therefore allowing a differential approach of crop inputs to be deployed.\nMany manufacturers including New Holland, CASE, Claas, AGCO and John Deere all provide a yield-mapping function and most incorporate moisture analysis for cereal harvesting. Some also provide additional elements, Claas has a very useful Telematics option called Fleet View, which informs the field team about the position of the machines and their grain tank fill levels. Everyone will know which machine needs to be unloaded next. This avoids idle time and unnecessary vehicle travel, save fuel and make full use of the harvesting machines’ capacity.\nSensors installed in John Deere’s ActiveYield system combines, not only weigh the amount of grain coming into the tank, but NIR sensors can also measure protein content. The weight sensors avoid the need for weighbridge calibration. This is useful in gauging the success of late nitrogen applications for protein in wheat. Other such as Bayer’s FieldView enables remote real-time harvesting information, yield mapping while the combing operation is happening, and also gives access to in-season satellite imagery allowing evaluation of crop health.\nYield mapping can also be used to examine other factors such as PCN affected areas in potato. Allowing differential strategies for nematicide usage / varietal choice to be implemented in specific situations. Soil Essentials’ EssentialsRootYield weighs potatoes coming over the harvester’s web, with the data then integrated with Trimble FmX® and TMX-2050 guidance to display yield while on the move and as overall yield maps. Similarly, knowing the yield and quality of forage as it is being cut is a very useful management tool for livestock farmers to know what is coming in for storage and to assess feed potential. An example of this is the recent John Deere, HarvestLab, which can measure yield and dry matter of forage on the move, but also analyse dry matter, crude protein, starch, crude fibre, NDF, ADF, sugar and crude ash.\nIs digital farming only for developed countries? The widespread use and availability of mobile phones, 3G, 4G and internet in the third world means that some of the small-scale technologies could be used with minimal costs. Seventy per cent of the poorest population and 20 per cent of the low and middle-income countries have access to a mobile phone, and one in three people have internet access.\nThe priorities will be different in developing countries, for example pest control practices and the prediction of unforeseen extreme weather events will give a much greater payback than disease control. There will be an associated need for digital literacy which will be essential to make these systems work.\nAs technological uptake increases, so will the need for agricultural advisory services in smallholder farming communities. This could be done as distance learning from anywhere in the world.\nSeveral technologies such as drone-mounted sensors, laser weeding and insect trapping are all really suited to small scale agriculture if properly coordinated at a local scale.\nThere is a huge potential for local and regional coordination of crowd-sourced information and feedback, together with reporting on the success of the various technical solutions that have been used and their efficacy.\nFrom a mechanisation point of view, digital tools are being developed that aim to connect tractor owners and farmers for example Hello Tractor in Nigeria; EM3, Trringo and farMart in India; Trotro Tractor in Ghana; and Rent to Own in Zambia.\nThere is no doubt that we are in the midst of a Digital Farming revolution. As this article illustrates, there are a lot of technologies being developed, covering a wide range of crops, that will ultimately increase crop output and profitability.\nThe skill will be how all these individual components are bought together in a workable “systems-based approach”.\nFarmers very much favour the one-stop-shop or grower-portal approach, where one log in provides visibility of all the technologies active within a farming business. This concept is very much behind the development of the technologies themselves but needs to gain momentum to keep up with the pace of technical development.\nThe rapid technical development within the industry is also beneficial in attracting new entrants who may have previously viewed farming as rather traditional and low-tech.\nCropMonitor Pro is a state of the art sophisticated decision support platform which has been developed by Fera with Crop Health and Protection funded by Innovate UK\nCropMonitor Pro is a state of the art sophisticated decision support platform which has been developed by Fera with Crop Health and Protection funded by Innovate UK.\nCHAP’s Fine Phenotyping Lab is based at Rothamsted Research in Hertfordshire.\nCHAP’s precision machinery is housed at Stockbridge Technology Centre (STC), near Selby.\nCHAP has four mobile laboratories.\nCHAP plays a key role in developing new control strategies, which are going to be essential for the farmers and growers who are having to deal with the loss of actives in the market.- Dr Tom Ashfield , Rothamsted Research\nFor more information on our capabilities or to discuss a collaboration and/or grant for a commercially funded project, complete the form below.']"	['<urn:uuid:eeaf5b00-c21a-4cd7-b1eb-8564fd62fe90>', '<urn:uuid:163fa02d-e3f2-4968-8079-433beabbfadf>']	open-ended	with-premise	verbose-and-natural	similar-to-document	three-doc	novice	2025-05-12T10:19:05.071190	32	112	3730
36	I was wondering if both the Japanese Children's Day and Chinese Double Fifth Festival happen at the same time of year?	While both celebrations occur in the fifth month, they fall on different days. Kodomo no Hi (Japanese Children's Day) is celebrated on May 5th in the modern calendar, while the Chinese Double Fifth Festival (Dragon Boat Festival) occurs on the fifth day of the fifth lunar month, which varies each year in the Gregorian calendar (for example, in 2022 it fell on June 3rd).	"['Kodomo no Hi\n🎏 Koinobori and Boys\' Day in Japan\nKodomo no Hi, literally ""children’s day"", is a Japanese holiday taking place on May 5 at the end of the Golden Week, the national vacation week. This celebration, held in the family circle, originated from China and after many evolutions, is now dedicated to children, especially boys. The koinobori flag carp is one of its iconic symbols as well as the miniature samurai’s armor and helmet decorating the houses.\nMay 5 or Kodomo no hi, is one of the five sekku (節句) celebrations, that indicate the passing of seasons, with:\n- January 1 (New Year),\n- March 3 (Hina Matsuri, the dolls’ festival or girls’day),\n- July 7 (Tanabata, the star festival); and,\n- September 9 (Kiku no sekku, chrysanthemums’day).\nThis celebration originates from China and is widespread in Asia. In Japan, it was called tango no sekku (端午の節句) until 1948, when it became a holiday and was renamed Kodomo no hi. Tango meant ""first day of the fifth month"" in the former Chinese lunar calendar. The fifth month of the year was considered unlucky, but it was also the rice seedlings planting period, an important task that was traditionally performed by women.\nRituals involving odoriferous plants such as sweet flags, a kind of marsh herb whose Japanese name is shobu (菖蒲), were performed to protect the community, and especially women, from misfortune. Thus, on the first day of the fifth month, women were gathered and cloistered in a house whose roof was swept with sweet flags. Tango no sekku was originally a celebration to protect women.\nThe importance of the sweet flag in this celebration gave it its other name in Japanese: shobu no sekku (菖蒲の節句).\nA celebration whose meaning changed over time\nA ritual for global protection\nTango no sekku was introduced in Japan during Nara period (710 - 794) and first performed at the imperial court. Dignitaries decorated their head with sweet flags and gathered to receive kusudama (薬玉): spherical paper containers filled with medicinal herbs donated by the emperor. At the time in the aristocracy, offering medicinal herbs packets was a common practice.\nAt the same period, the celebration day also shifted from the first to the fifth day of the fifth month, probably due to the homophony of ""go"" in tango with number 5 that reads ""go"".\nThe shift to boys’ protection\nAs soon as Heian period (794 - 1185), warrior families with boys displayed a nobori (幟) pole and a vertical banner (hata sashimono 旗指物) a few days before tango no sekku at the entrance of their house.\nThanks to a similar reading, the other name of the celebration, shobu no sekku, refering to sweet flags, was associated to shobu (尚武) meaning ""martial spirit."" From Kamakura period (1185 - 1333), as the warriors’ class came into control of the society, the martial meaning spread and the celebration started to be solely associated with boys.\nTango no sekku thus became an important celebration among warriors’ families. The custom of ornamenting a room with a samurai helmet or armor also began at the same time. It was a mirroring of the warriors’ tradition to offer one set of such protections to shrines in a request for divine protection during battle.\nDuring the celebration, prayers were made for the clan’s prosperity and the heir’s health. The helmet and armor symbolized the wish to educate strong and powerful sons, but first and foremost the wish to protect the child.\nAmong other rituals, stone fights were also held until Edo period (1603 - 1868) when they became prohibited due to the number of injured and deaths they caused. They were replaced by shobu fights, as the sweet flags’ leaves remind of the shape of a sword.\nAt the same period, the clear distinction between girls’ day (March 3) and boy’s day (May 5) was established. At Edo castle 🏯, the habit developed to display a helmet each time a boy was born to the shogun. The population adopted the custom by making a helmet and an armor from paper for this occasion, items that gradually miniaturized to their present days\' size. In the 18th century, the merchants and artisans classes started to celebrate tango no sekku with the symbol of the carp.\nIntroduction of koi nobori during Edo period\nThe koi nobori (鯉のぼり) is a pole on which one to several flag carps are attached. Its use developed in the merchants and artisans classes of Edo period and mainly in Edo (former Tokyo) and in Kanto area. The symbolic of the carp is quite powerful: this fish can as easily live in clear waters than in in ponds or marshes\' muddy ones. Moreover, its ability to upstream swimming made its reputation of a perseverant being with a lot of vitality.\nThis fish is also a reference to a Chinese legend in which a carp became a dragon and flew to the sky as it swam upstream the waterfall Ryumon (竜門 ""gate of the dragon""). The story gave birth to the expression toryumon (登竜門), the ""gate to the success.""\nThe carp symbolizes the wish for the child’s success, regardless of his environment and of the circumstances.\nHow to celebrate Kodomo no hi?\nKodomo no hi, or tango no sekku, is celebrated on May 5, but the feast often begins on the day before. It is a family gathering favored by the succession of holidays during the Golden Week. As in all traditional celebrations, Kodomo no hi has its specialty dishes to enjoy with family and friends:\n- Dishes made with carp and chestnuts,\n- Chimaki (粽), a salted glutinous rice cake, often wrapped in a bamboo leaf,\n- Kashiwa mochi (柏もち), a glutinous rice pastry filled with red sweet been paste and wrapped in a sweet oak (Kashiwa) leaf.\nThe habit of eating kashiwa mochi is specific to Japan. The pastry’s name derives from the sweet oak, a tree that does not lose its older leaves before new buds have appeared. It symbolizes the continuity between generations.\nContinuation of ancient rituals\nTango no sekku is also inseparable from the sweet flag, or shobu (菖蒲) herbs renowned for repelling evil spirits. This plant is used in many ways for the occasion:\n- Brewed in sake (shobuzake 菖蒲酒), a drink to prevent diseases,\n- Placed on the houses’ roof or eaves to protect its inhabitants,\n- Shaped as pillow (shobumakura 菖蒲枕 ) to protect sleep,\n- Immersed in the bath’s water (shobuyu 菖蒲湯), to prevent diseases.\nAnd naturally, the house is decorated with the celebration’s regalia: koinobori 🎏 outside, and samurai armor and helmet inside.\nThe first Kodomo no hi\nA boy’s first tango no sekku is the most important and is celebrated while he is still a baby. The ritual hatsu sekku (初節句) (first sekku), has been practiced since Edo period to ensure a strong and healthy growth to the child.\nTraditionally, a warrior doll and / or a samurai helmet is offered, mostly by grandparents on the mother’s side. Nowadays however, both families can share the expense of the present.\nThe other members of the family, or close friends, will rather give a Kintaro or Momotaro doll, or a koi nobori.\nIn some areas, the celebration also consists in kite flying (tako age 凧あげ).\nWhat are the different decorations and their meanings?\nAlso called satsuki nobori (皐幟) or koi no fukinagashi (鯉の吹き流し), the koi nobori is a pole placed in the garden near the house, decorated with carp motifs. The shapes can be made from paper or fabric, depending on the period, and flutter in the wind.\nThe inside of the house is sometimes decorated with kazarikoi (飾り鯉), or ornamental carps.\nA typical koi nobori includes, from top to bottom:\n- A kaitenkyu (回転球) or kagodama (かご玉), a kind of spherical object placed at the tip of the pole,\n- A yaguruma (矢車), a kind of windmill,\n- A fukinagashi (吹流し), a wind sock-like element colored in 5 hues (goshiki fukinagashi 五色吹流し). It can also be decorated with lucky motifs: flying dragon, clouds or carps (gara mono fukunafashi 柄物吹流し),\n- The large black carp magoi (真鯉), that existed from the early days of the tradition,\n- The red and yellow carp higoi (緋鯉), added during Meiji Era, and,\n- The small blue carp kogoi (子鯉 or seigoi 青鯉), added in the early 20th century to have the whole set symbolize a family.\nThe black carp was sometimes decorated with a red-body boy holding the carp firmly, in a reference to Kintaro’s legend.\nBefore World War II, magoi and higoi carps represented respectively the father and the son. From the 1950-1960’s and the evolution of family structure, the higoi carp started to represent the mother, and the blue carp become the one symbolizing the child. More recently, small kogoi carps of vivid colors have been added to represent all the siblings, girls and boys altogether.\nHowever, the falling birth rates in Japan, combined with the widespread habit to live in residential buildings without garden, tends to undermine the koi nobori custom.\nOn a side note, the tradition is particularly strong in Kyushu and Shikoku, even if it started more recently there.\nThe gogatsu ningyo\nA gogatsu ningyo (五月人形, literally ""fifth month doll"") was originally the doll or the miniature helmet offered to the boy for tango no sekku. Now this term relates to all the objects or decorations displayed for this celebration, including sometimes the koinobori.\nDolls owners take as much care of them as they do for Hina Matsuri 🎎 dolls, and they sometimes display them on a three-step stand, named sandai (三台), that only the wealthiest families can afford.\nOn the highest step, the items placed are:\n- In the center: the samurai helmet (kabuto 兜) and / or the armor (yoroi 鎧). They still have their symbolic role of protection from contemporary dangers, such as traffic accidents. They are supposed to receive the misfortune and luck instead of the child.\n- The kabuto sashi (兜差 , ""helmet wearing character"") can replace or be associated with the former. It is a boy’s doll wearing the helmet and / or donning a general’s attire. These items are offered by the family or relatives for hatsu sekku.\n- On the right, a bow and arrows to destroy demons (hamaya 破魔矢), from the legend of Shoki, a Chinese warring deity. In Kamakura period, arrows were offered to a warrior family where a boy was born, as an auspicious ornamentation.\n- On the left a sword (tachi 太刀), not for battle but to ward off demons thanks to its brightness, as they fear light. Initially, it was used during rituals before becoming an ornamentation for this celebration.\nUsually a gilded screen (byobu 屏風) is placed behind the displayed elements to highlight them, but also for its initial purpose of separating and protecting. It can be adorned with several kinds of motifs:\n- Fujin (風神) or Raijin (雷神), respectively the gods of wind and thunder, protectors from evil spirits,\n- The dragon to attract luck,\n- The tiger for a healthy growth,\n- The carp for vitality.\nThe second step displays three indispensable items for a battle (sanpin 三品):\n- Gunsen (軍扇), the war fan to give direction to the army,\n- Jingasa (陣笠), the foot soldier’s hat; and,\n- Jindaiko (陣太鼓), the war drum to give the signal to the troups.\nLastly, the third step is dedicated to the three treasures (sanbo 三宝):\n- Heishi (瓶子), in the center, are sake containers in which shobu stems were planted,\n- Kashiwa mochi on the right; and,\n- Chimaki on the left.\nFamilies usually settle for the first step, or the kabuto sashi doll, that can be complemented by a banner bearing the name of the boy (namae hata 名前旗) placed on the side.\nIdeally, every boy in the family should receive his own gogatsu ningyo, but due to the lack of storing space and their cost, the habit of displaying one banner per boy has developed. In the same way as the other objects, the banner symbolizes the prayer for each boy to grow healthy.\nDolls to foster moral values\nOther dolls can be offered for Kodomo no hi. They represent real or legendary characters who are appreciated for their warring abilities and their intelligence. Among the most popular are:\n- Ushiwakamaru (牛若丸), Minamoto no Yoshitsune (1159 - 1189)’s youth name, he was renowned for being a genius tactician and a full-fledged warrior. Despite a difficult start in life, he succeeded to raise to the shogun position and founded Kamakura’s bakufu (government). He is pictured as a young boy, with a flute in the hand in reference to his meeting with Benkei.\n- Benkei (弁慶, 1155 - 1189) was a very violent warrior monk. He became Yoshitsune’s vassal after he attacked him to steal his weapon. His strength was only equal to his loyalty towards his lord, and the legend has it that his body was still standing after his death. He is pictured carrying a bunch of weapons on his back.\n- Momotaro (桃太郎), the legendary boy born from a peach, although a little bit lazy, was renowned for his extraordinary strength. He chased away demons from Onigashima with the help of animal he brought alongside. He symbolizes the ability to repel evil and to unite. He is pictured wearing lavish clothes and bearing a banner stating 日本一 (nippon ichi) ""Japan’s best"".\n- Kintaro (金太郎) was the infant name of Sakata no Kintoki, a half-legendary character who supposedly lived in the early Heian period, and who also served the Minamoto’s. He was raised in the mountains by an ogress, and was blessed with a superhuman strength and an unflinching courage. A famous story tells how, as he was still a child, he captured a carp that was bigger than him. He is pictured as a child with a red body and fighting a carp or dressed in a red apron and wielding an axe.\n- Shoki (鍾馗) supposedly lived in China between the 7th and 9th centuries. He studied hard to become a civil servant, an important position at the time, but he was rejected due to his ugliness. He committed suicide in protest, and the emperor, moved by his story, had his funeral held with compassion. Later, as he was severely ill, the emperor dreamt of a big demon chasing a smaller one. It was Shoki, who came to help the emperor and show his gratitude for his consideration. Upon waking up, the monarch, healed, ordered that Shoki was pictured as he appeared to him. His frightening image is used to ward off diseases, but also to ensure academic and professional success for the child and pray for him to be a grateful person.\nLastly, paper-mache tiger (hariko no tora 張子の虎) represents a roaring tiger, whose head is mounted on a longer than nature neck that can oscillate from top to bottom. Since Edo period, it is offered at the fist tango no sekku, mainly in Kansai area. Placed alongside the samurai helmet, it symbolizes the parent’s affection and their will to protect their child in the same way a tiger defends her cubs.\nWhere to see koinobori in Japan?\nContrary to Hina Matsuri, Kodomo no hi is not characterized by public celebrations. The Children’s day on May 5 is celebrated at home, at small scale. It is however visible throughout Japan in decorated sites where families can gather:\n- On river banks: Sumida River in Tokyo, Kamogawa in Kyoto, or Akuta River in Takatsuki in Osaka prefecture,\n- In parks: Tokyo Midtown or Tokyo Dome City, among others; and,\n- Near some touristic attractions such as Tokyo SkyTree or Tokyo Tower.\nWhile walking around, it is possible to spot flags shaped as carps floating from the houses. Some neighborhood Japanese pastry shops prepare the traditional rice cakes.\nShops for tourists usually sell objects in relation to this time of the year, such as small koinobori hangings to bring back as a souvenir. Very few stores display the traditional gogatsu ningyo in their windows. The best places to see them are hotels 🏨 lobbies, such as in Kyoto for example, or shops specialized in Japanese dolls, throughout the country in April, or in one of the dedicated museums on the rest of the year.', 'How to Celebrate Chinese Double Day Festivals\nBeing one of the most culturally diverse countries, China celebrates a lot of traditions and culture-based festivals that are nothing but pure joy to be a part of. There are countless festivals in China, but today, we bring you the fascinating Chinese double-day festivals. If you are deeply interested in Chinese culture and traditions, this is a good chance to learn about these special Chinese festivals.\nThe Double Fifth Festival\nThe dragon boat festival, or Tuen Ng Festival, falls on the fifth day of the fifth lunar month (according to the Gregorian calendar, in 2022, it falls on June 3rd). On this day, people honor the death of Qū Yuán, a Chinese poet and politician who became a national hero for his patriotism and contributions to traditional poetry.\nQū Yuán 屈原 lived during the early feudal dynasties in China, in the Warring States period to be specific (approximately 475 to 221 BC). It was typical for Chinese intellectuals to serve in government during the period, and Qū Yuán was no exception. He was a dedicated government official in the State of Chu for some time. However, later he was exiled after being slandered by another official.\nThough Qū Yuán was exiled, he spent most of his time producing many works of poetry in order to show his love for the country. Upon hearing the news that the State of Chu was not doing well politically and was defeated at the hands of competing states, Qū Yuán was in a deep depression. Eventually, after finishing his final poem, he committed suicide by drowning himself in Hunan Province’s Miluo River as a symbol of despair over the current political situation.\nDuring his exile, Qū Yuán had risen in popularity with the local community. After hearing this tragic news, villagers took boats and dumplings to the middle of the river in an attempt to save Qū Yuán, but their efforts were in vain. Villagers began banging drums, splashing water with paddles, and tossing rice dumplings into the water, serving both as a gift to Qū Yuán’s soul as well as a means to keep the fish and evil spirits away from his body. The search for Qū Yuán’s body produced the dragon boat races, while the rice dumplings became the zòngzi we know today.\nCultural traditions and activities\nParticipating in dragon boat races\nThe most well-known activity of the Double Fifth Festival is the tradition of racing dragon boats. Originally, boats were made of teak and painted “dragon” heads and tails. Today, they are made of modern materials such as fiberglass.\nDragon boat racing has evolved into a sport with a variety of rules and regulations throughout the years. This sport has lately acquired appeal with people all over the world, especially in the United States. Many races have been held in big cities such as New York and Chicago.\nEating zòng zi\nZòng zi 粽子 is the most popular treat to eat during the Dragon Boat Festival. It’s a rice dumpling composed of sticky rice wrapped in bamboo leaves and knotted with plant stems to make a pyramid shape. The fillings are also various, including egg yolk, lotus seeds, ham, red bean paste, rose, melon, date, chestnut, and pork.\nYou might also be interested in: Chinese Dragon: Why It’s A Cultural Symbol?\nThe Double Seventh Festival\nQī xī festival, also known as the Double Seventh Festival, or Chinese Valentine’s Day, is one of the traditional Chinese festivals. This holiday celebrates the romantic story of a weaver girl and a cowherd. Of course, as it name suggests, this festival falls on the seventh day of the seventh lunar month. (this year, it falls on August 4th in the Gregorian calendar)\nThe Chinese Valentine’s Day centers around two star-crossed lovers, Zhī nǚ 织女, a weaving girl and the daughter of a goddess, and Niú láng 牛郎, a lowly cowherd. Niú láng met Zhī nǚ, they fell in love, married, lived happily, and finally gave birth to two children, a boy, and a girl. When Zhī nǚ’s mother, a goddess, learns that her daughter has married an ordinary man, she brings her to heaven and casts a gigantic river between heaven and earth to separate the pair — this river is now known as the Milky Way.\nSeparated from their loved one, Niú láng and his children could only weep bitterly. Fortunately, their love caused all the magpies to pity them, and they flew up into heaven to create a magpie bridge across the river, allowing Niú láng and Zhī nǚ to meet there. Zhī nǚ’s mother afterward was moved by their love and enabled them to meet on the magpie bridge every year on the seventh day of the seventh lunar month.\nCultural traditions and activities\nThe most common tradition for ladies is to demonstrate dexterity abilities. It’s a chance for young women to show off their skills by speedily threading a needle under the moonlight. They’re also competing in carving exotic flowers, animals, and unusual birds, usually on melon skin.\nWorshiping the weaver fairy, Zhī nǚ\nWomen prepare offerings of tea, wine, flowers, and various fruits to pray to Zhī nǚ for wisdom and to grant their wishes. Young single women often pray for a good spouse, while newly married women wish to bear a baby.\nEating qiǎo guǒ\nThis is a popular treat that is often prepared during Qī xī festival. Qiǎo guǒ is a fried thin pastry of different shapes, made with oil, flour, sugar, and honey. People believe that by eating qiǎo guǒ, it will help reunite the couple on the magpie bridge.\nYou might also be interested in: How to Celebrate Chinese New Year: 5 Main Traditions and Activities\nThe Double Ninth Festival\nThe Double Ninth Festival, also referred to as the Chóng yáng festival, is held on the ninth day of the ninth lunar month (this year’s festival is celebrated on October 4th in the Greogorian calendar). In Chinese culture, nine is regarded as the number of Yang. The ninth day of the ninth month is the day that has two Yang numbers, and ‘Chóng’ in Chinese means double, which explains the name Chóng yáng. Since 1989, this festival has been designated as the Senior’s Festival to show respect for the elderly.\nLegend has it that during the Han Dynasty, a devil inhabited the Nu River and caused disease for neighboring people. Because of the devil’s power, the parent of a young guy named Hengjing passed away. For that reason, Hengjing went to find an immortal who could teach him swordsmanship to drive the demon from the people.\nThe immortal told Hengjing that the devil would appear on the ninth day of the ninth lunar month, and he would need dogwood and chrysanthemum wine to scare off the devil. Hengjing took bags of dogwood and some chrysanthemum wine and returned to his hometown. On the morning of the ninth day of the ninth lunar month, Hengjing led all the villagers to the nearest mountain, with each holding a piece of dogwood leaf and a cup of Chrysanthemum. When the devil came out from the Nu River, it came to a halt because of the scent from the dogwood and the chrysanthemum wine. While the devil was out in the open, Hengjing stabbed it with his sword a few times and took it down.\nSince then, the custom of climbing mountains, drinking chrysanthemum wine, and holding onto dogwood on the ninth day of the ninth month has become a widespread tradition.\nCultural traditions and activities\nPeople believe that climbing or hiking on the Double Ninth Festival can ward off diseases as in its legend. Not to mention, it’s a golden time to enjoy natural beauty and relax with family and friends.\nEat Chóng yáng cakes\nIn Chinese, cake 糕 is pronounced “gāo” which is similar to the word for “high” 高 gāo. Therefore, people consider climbing to be the same as eating cake on this occasion. Chóng yáng cake is a nine-layer cake shaped like a tower with a great wish that one will make improvements to a higher level after eating the cake.\nDrinking Chrysanthemum wine\nThe festival would not be complete without the consumption of Chrysanthemum wine. Chrysanthemum is a flower that acts as an antitoxin and may ward against evil. Many people believe that by drinking chrysanthemum wine, they may heal and avoid all types of diseases.\nYou might also be interested in: Lucky and Unlucky Numbers in Chinese Culture\nExperience the festivities for yourself!\nChinese festivals are always full of joy and activities to take part in. If you happen to be in China during any of the festivals listed above, make sure to take time to enjoy the celebrations.\nIf you’re interested in Chinese culture, history, or language, find more posts like this on Pandanese. Pandanese is a Chinese learning platform that helps users learn Chinese easily by employing mnemonics and Spaced Repetition System (SRS). We encourage you to check out our Pandanese blog and read as much as you like about language and culture topics before starting your learning!']"	['<urn:uuid:60f51ade-aac7-442a-891f-6922af176cf7>', '<urn:uuid:c739e8af-366b-43da-bc6c-9b053e4ab877>']	factoid	direct	verbose-and-natural	distant-from-document	comparison	novice	2025-05-12T10:19:05.071190	21	64	4245
37	what percent california freshwater supply groundwater	Groundwater constitutes roughly half of California's freshwater supply	['With increasing attention on methods of recharging depleted aquifers, groundwater moves to the frontlines of drought mitigation in California\nFor the better part of a decade California has experienced a drought that has significantly depleted water reserves throughout the state. Surface water has long been the focus of California’s water supply planning, but a new awareness of groundwater is changing that. With the dawning recognition that groundwater is a finite and diminishing resource, new projects for recharging groundwater reserves are changing the nature of California’s approach to water management.\nGroundwater constitutes roughly half of California’s freshwater supply, with the other half supplied by surface water sources such as reservoirs. Until 2014, there was no statewide system for groundwater monitoring or regulation. Property owners could use as much groundwater as they wanted without keeping tabs on how much they were using.\nUnder pressure from the drought, in the fall of 2014 California passed the first statewide legislation to address the problem. The Sustainable Groundwater Management Act (SGMA) provides standards and assistance to local water agencies, and mandates that local systems establish Groundwater Sustainability Agencies to oversee improved monitoring and sustainable management of groundwater stores.\nTo explain the relationship between surface water and groundwater, Graham Fogg, a professor of hydrogeology at University of California, Davis, used the metaphor of bank accounts.\n“Imagine you had all of your money in two bank accounts that are linked,” Fogg said. “For one of them you know the balance and the inputs and outputs, but for the other one you don’t. It would be really hard to manage your money sustainably. When one account gets low you have automatic withdrawal from the other. That’s how we manage our water.”\nIn addition to his post at UC Davis, Fogg is a director at UC Water, an organization formed after the passage of SGMA to unite the expertise of UC system researchers and help the state move forward with future planning.\n“We have to be much more aggressive about not only groundwater management but recharging the groundwater in ways that we haven’t in the past,” Fogg said.\nRecharge is the naturally occurring process by which water seeps into the ground and below river and stream beds after rain, filling up aquifers and the space between rocks and soil. Irrigation also is responsible for groundwater recharge. During drought conditions, not enough water is returning to the ground to replace the amount that users are pumping out of the ground. In California, depleted groundwater supplies are causing multiple problems such as subsidence and saltwater intrusion.\nOn this topic, Andrew Fisher is ahead of the game. A professor of hydrogeology at University of California Santa Cruz, Fisher began studying groundwater 15 years ago when he sought to focus his research efforts on a topic that would have an impact in the public realm. Fisher had “an epiphany” that groundwater recharge was that topic.\nRecharge is “an area that’s a hydrologic frontier, an area where there’s a lot that’s not known and where as a scientist I could make a contribution that would make a difference,” Fisher said.\nTo confront these issues, Fisher founded the Recharge Initiative to involve students in collaboration with government agencies, farmers, policymakers, and other stakeholders to study and implement recharge projects statewide. Through the Initiative, Fisher directs his students in research into local and regional conditions that support or deplete groundwater stores, including mapping where recharge occurs and computer modeling to test their results and make predictions to optimize groundwater recharge projects.\nFisher advocates managed recharge, an umbrella term for using various techniques to actively introduce excess surface water, including stormwater, treated wastewater, and agricultural runoff, into aquifers. Certain areas of California have been using some of these methods for decades. For instance, the city of Roseville manages injection wells that force water into the ground, and the Coachella Valley diverts excess flows from the Colorado River into spreading basins to collect water and allow it to seep into the ground. But until recently, these methods have not been widely used.\nPart of the problem is a lack of data about what methods work best in different regions. Because groundwater resources, surface water availability, geology, climate, and financing are so different from place to place, what works in one place won’t necessarily work in another.\nFisher is also a director at UC Water, which is currently supporting several new recharge projects, including floodplain recharge and agricultural field flooding. Before the 19th century, the rivers in the Central Valley would regularly flood, filling up the floodplain with water that would eventually percolate down to underlying aquifers. This natural recharge disappeared 150 years ago when levees were built to control the flooding. Recently, The Nature Conservancy (TNC) has removed the levees on a study site along the Cosumnes River. Because the area is not urbanized and the floods are not a danger to homes or businesses, TNC is working with UC Water and other partners to let the floodplain flood, and measure the recharge benefit.\nAnother promising recharge project is the winter irrigation of fallow farm fields. The project involves re-routing excess water from wet winter conditions onto farms.\n“Irrigation has probably increased the groundwater recharge perhaps tenfold,” Fogg said.\nHelen Dahlke, a researcher at UC Davis, is currently conducting a number of demonstration and field test studies on the benefits and drawbacks of winter irrigation, including its impacts on crops, groundwater levels, and groundwater contamination.\n“We’ve pumped roughly 100 million acre-feet out of the groundwater aquifers in the Central Valley,” Dahlke said. “So that is a void that can be filled again.”\nFor instance, in an almond orchard in Modesto, Dahlke and partners are applying six vertical inches of water from city storm drains to the orchard. Over the course of the winter, they will flood the field with two feet of water that would otherwise end up in the Tuolumne River. They will monitor the tree response to the additional water to make sure it doesn’t negatively impact the trees or the almond yield the following summer.\nThese projects seem promising, but with climate change advancing, where will the water come from? Many climate models forecast that the state will receive the same amount of precipitation that it has historically, but that it will come in different ways, such as bigger floods alternating with more frequent and severe droughts. This shift in climate patterns will require new strategies by water managers to divert higher flood flows, when they happen, into groundwater basins.\nFisher and his colleagues conducted a study in 2012 that looked at extreme precipitation for the last 120 years, and found that the average amount of rain each year hasn’t changed, but more of the rain is falling during a smaller number of more intense storms.\n“Everything else being equal, shorter more intense rain storms tend to soak into the ground less,” Fisher said. “You tend to get more runoff, and that leads to flooding, and even when it doesn’t lead to flooding, it adds to less recharge.”\nThe effect is amplified under changes in land use such as increasing urbanization and agricultural development, which makes preparation for climate change a challenge. In addition, Fisher noted that climate models have a difficult time making precipitation predictions.\nFogg adds that over a geologic time scale, California is prone to long periods of drought. Whether the changes are due to climate change or not, he said, “We anticipate that the drought could continue for a long time …We have to try and prepare for long periods of scarcity or at least intermittent periods of scarcity which means not predominantly relying on storage of water in our surface reservoirs to weather those droughts.”\nThe variety and diversity of local conditions, coupled with the changes wrought by a shifting climate, underscores the need for so many different approaches to recharge. In studying recharge, hydrogeologists have learned that it does not occur in most places.\n“You might look across an area of a thousand acres, and if you were to throw water equally on that acreage, you might get 95 percent of the recharge from only 5 or 10 of those acres,” Fisher said.\nCurrently, there is no statewide mapping system to collect and monitor conditions to determine the most effective locations for managed recharge projects.\nBut with the new legislation taking effect, the impetus and financing are finally there for water systems across the state to start considering the viability of groundwater projects, Fisher said.\n“Right now there are dozens of groundwater basins and regions around the state that are scrambling to get organized,” he said. “There’s still just a tremendous amount of work to be done to help figure out what the best way is to manage resources.”']	['<urn:uuid:13c837d7-c85b-49e9-8586-413edd14e08b>']	factoid	direct	short-search-query	similar-to-document	single-doc	expert	2025-05-12T10:19:05.071190	6	8	1460
38	What's needed for signal data quality control and risk monitoring compliance?	Signal data quality requires organized collection processes with granular data in queryable fields using standard code-lists and normalized datasets. For risk monitoring compliance, healthcare facilities need a common platform to manage patient records and treatment options, while connecting initiatives to potential dangers for detailed risk mitigation plans.	['There is a growing emphasis on signal detection and signal management in Pharmacovigilance. Companies must be able to manage signals detected: evaluate them to understand the clinical risk in light of multiple impacting factors, in a demonstrably controlled manner. Benefit/risk balance for a drug, vaccine, or treatment is impacted by the disease the product is intended to treat, distribution of the disease and product, clinical trial data, environmental influences, and other epidemiological factors. Signal management requires considered workflow and rigorous process control.\nPharmacovigilance software is changing\nThis is not news. The evolution of Pharmacovigilance is impacting the way we approach drug safety management of potential safety signals.\nPharmacovigilance initially was a reactive function. Only after the infamous thalidomide tragedy, and others, did regulators and industry become attuned to their shared responsibility to be good stewards of approved products.\nAnd look at how far we have come!\nThanks to the work of dedicated experts worldwide, committed to ensuring data quality, compatibility and utility, we now have:\n- Pages of standard code lists\n- Common standard file formats\n- Globally aligned systems\n- …and more and more data collected.\nNot only are the systems and processes in place, we have moved the needle from a purely required practice to a truly value added activity – where patients, practitioners, medical product manufacturers and regulators alike view Pharmacovigilance for what it is: a means to take data from a variety of environments – be it controlled clinical trials or “glorified gossip” from the field – and evaluate it for trends or signals, that can be validated or discarded.\nFor various reasons, such as regulatory compliance in some regions, good manufacturing practices, and responsible product monitoring in others, collection of adverse event (AE) data with approved, marketed product is an important part of product stewardship.\nAn effective Pharmacovigilance software (PV) system should not only efficiently collect this information, but also include a proactive risk management strategy with methods for identifying, analyzing, evaluating, communicating and mitigating risk; as well as an escalation plan for potentially serious issues.\nThe problem is, Industry is under a crunch:\n- Increasing approved product AE caseloads (8-10% animal health, 10-12% human health) year on year, driven by new product launches, increased PV awareness, and evolving PV regulations worldwide\n- Tighter budgets and fewer resources\nSo far, the focus has been on automating accurate case capture, entry and processing, but what about making the second part more efficient – proactive signal detection, risk identification and risk management.\nWhat is a signal and how do we find them?\nIn the simplest terms, a signal in Pharmacovigilance software is an instance when the data is trying to alert us that something is different compared to prior or expected data.\nEffective signal detection is in fact an interplay between the safety reviewers or medical experts and the data, via continuous data monitoring and detection of disproportionate reporting patterns among product-event pairs. Accurate signal detection depends on quality data, knowledge of the database, and tools designed to visualize, summarize, and evaluate the data.\n#1 Quality data in, Quality surveillance out:\nFirst, it is critical to have an organized, standard process as well as a controlled system for collecting and managing adverse event data, independent of the source.\nIdeally it is best to collect data at the most granular level, stored in data fields that can be queried via standard code-lists. Collecting individual data points within a case may seem tedious but proves highly valuable during data analysis and evaluation. It is much easier to aggregate data points than to tease it apart later.\n- Collecting data in discrete fields rather than in free texts allows for more structured querying, classification, and summarization.\n- Utilizing standard code-list ensures that the data is consistent across systems, categories, and time. It also prevents masking of potential signals due to disorganized data.\n- Normalizing datasets to comparable units – e.g. converting patient age data collected at source in units of weeks, months, and years to a ‘lowest common denominator’ unit.\nData collected in free text or in other non-standardized format requires individual case review, which can be time consuming and resource intensive. Alternatively, case data where patient, product, clinical sign, causality, seriousness, and other case elements are captured via standard code lists (e.g. ICH standard, MedDRA coding dictionary) can be queried, tabulated, graphically displayed, and evaluated not only by safety reviewers, but also integrated into algorithms to facilitate artificial intelligence (AI).\n#2 Know your Data\nSecond, it is imperative that those who are evaluating the data have a good understanding of the database –i.e.\n- Patient groups\n- The environment in which the product is used\n- Market trends\n- Product distribution\n- Data source\n- …and other relevant factors that may impact the frequency and nature of adverse events reported.\nAdditionally, it is helpful to categorize data to account for case quality and relatedness. Without this knowledge, there is a risk of mistaking otherwise expected data changes as disconcerting trends or signals. What better way to get to know your data than to “see” it in real time, learn its normal patterns. Then, develop early detection indicators to pick up on disconcerting trends and take action, before they have significant impact to patients.\nDid you know: At certain times it is expected to see elevated frequency of adverse event reports, depending on the product’s life cycle stage , seasonality of distribution, or because of publicized events. Differences also occurs in trends based on product categories – e.g. biological products versus pharmaceutical products, due to their intrinsic attributes and manufacturing processes.\n#3 Have the right tools to detect true signals:\nThe main goal of signal detection is to identify either new risks with a product in a patient population or identify a worrisome trend which may negatively impact patient health.\nNo one wants to create more, non-value-added work for themselves or their team. That’s why we must have the right tool to do the job. Otherwise, important signals may be missed or erroneous signals found. Ideally, a customizable computerized system, designed to fit the specific needs of your product and market which accurately identifies valid signals, aligned with a risk management strategy, are the best tools to have.\nIdentification of a signal does not mean that there is a problem, but is an indicator that needs to be evaluated to determine its risk level. By working up potential PV risks in the context of a well-defined risk management program, medical product manufacturers and drug sponsors can systematically evaluate the level of impact, define a communication plan, risk mitigation actions, and risk review timelines.', 'Risk Management Strategies\nRisk management strategies are created to help health care organizations manage risks and ensure patient security. Firstly, companies should make sure that employees, both medical and non-medical staff, adopt a reporting tendency. If incidents that could place a patient at risk are reported to the relevant authority, the occurrences will reduce. For example, negligence, health workers will stop disregarding clients to avoid risking their jobs. Secondly, maintain proper communication across all the involved departments (Rister & Bourdeau, 2021). Communication channels ought to be clear enough to enable smooth transitioning of patient records in the system. This will guarantee easy access to medical records and discovery of patient needs.\nThirdly, harmonize processes in all the departments in the healthcare organization. Coordinating functions provides a smooth implementation of strategies in the business and enhances the principles, ethics, and rules. Besides, certify that risk management has been prioritized in the firm. This enables the establishment of measures that could be used in the eventuality of a risk emerging. Lastly, monitoring and assessing the advancement of work enables managers to beware of possible dangers and to strategize so as to avert them.\nPrevention of negligence in a healthcare organization is essential as it prevents more harm to the patient. Slackness mainly occurs when the health care practitioner breaks the responsibility of care to the sick. This can be mitigated by following specific treatment protocols and worksheets that ensure every procedure has been observed as it ought to. A worksheet is an essential tool as it ensures no operation is skipped and that all the treatment procedures are followed to the later. The techniques involved should be revisited before administering treatment to the patient to ensure that it is the right kind of management that the patient requires.\nAddressing malpractice litigation in healthcare is a noble duty for one to avoid getting in trouble. For instance, a doctor ought to do their very best as they conduct treatment to a patient. In doing so, one can prevent misconduct litigations by being careful. Moreover, following medical procedures improves health care, which can guide clinical lawsuits as long as they are certified. Documenting every treatment administered is a moral practice as it proves that particular medication was directed and enables one to comprehend what happened to the patient (Svensk & McIntyre, 2019). Vicarious liability is a situation where an employee of the hospital conducts a misconduct that harms a client. In this case, the employer is held responsible, mainly if the involved member of staff worked within the bounds of their employment act.\nManagement of urgent triage in high-risk situations of health care\nAddressing narcotics in emergency triage could occur in a series of operations involving pretreatment care, outpatient care, residential and inpatient treatment. A triage system could affect the various critical areas of addressing including creating databases with management resources offered in the healthcare organization. Other issues comprise outlining key characteristics of available medical programs and whom they concern, recording transfer processes with detailed time structures, and communication patterns. Systems could also be used to decide which patients require prioritized care for the existing resources (Ali, 2019). Policies in managing recorded pregnancies in high-risk situations involve prioritizing the ones in danger to save both the mother and child in the first instance. Managing the blood-borne disease sector could involve a few steps beginning with recognizing and identifying blood-borne infections, procuring protective equipment for the healthcare workers, and disinfecting and sterilizing lab equipment without intoxicating the environment.\nChallenges in managing and controlling high risk in health care\nHealthcare providers experience several challenges when dealing with high-risk case scenarios as outlined. Firstly, poorly designed structures in managing high-risk cases lead to inadequate treatment quality and, consequently, great dissatisfaction among the patients involved. Systems need to be developed to ensure that proper communication among all the health care practitioners involved communicating efficiently to enhance the quality of care administered to a patient (Chakravorti, 2018). Secondly, shortages in the number of health care providers prove to be a great challenge. This is because one doctor cannot fully provide the medical treatment required to a patient without the aid of other professionals such as a physician, a practicing nurse, and even a nurse aid. Another challenge faced in the provision of health care in high-risk care is the failure of the health sector to tap the use of information technology. This can be used in numerous ways to improve the provision of health care to patients. This derails the efforts of providing quality care by consistent application of traditional methods of communication in the health care system.\nRisk management in a health facility is critical as it averts further harm to the patient. The following strategies can be used to monitor, evaluate, and maintain compliance in a high-risk health care situation. Implementation of a common platform such as a healthcare management system where all other resources such as patient record database and treatment options are all controlled from one platform. Connecting initiatives to dangers enable the organization to beware of the potential risks and create a detailed plan to mitigate the risks in case of an eventuality.\nAli, S. (2019). How do I decide which patients with gynecomastia need more detailed evaluation. Endocrine Abstracts. Web.\nChakravorti, B. (2018). Patient-centric healthcare industry: Pharmaceutical companies can do more to put patients first. Primary Health Care Open Access, 08. Web.\nRister, A., & Bourdeau, D. (2021). Collaborative writing and presenting: A course template for teaching students how to collaborate on communication projects across all classroom modalities. Communication Teacher, 1-7. Web.\nSvensk, J., & McIntyre, S. (2019). Using QR Code technology to reduce self-administered medication errors. Journal of Pharmacy Practice, 089719001988524. Web.']	['<urn:uuid:2d3d6064-6cb9-44c6-aca6-b55cb7ec7521>', '<urn:uuid:6cc0dce1-0c33-4175-9711-8d47163d32a3>']	factoid	with-premise	concise-and-natural	similar-to-document	multi-aspect	expert	2025-05-12T10:19:05.071190	11	47	2053
39	why salem massachusetts became halloween tourist destination	Salem became a Halloween destination largely due to the 1970 sitcom 'Bewitched', which filmed episodes there including one about witch trials. After the show, Salem began embracing its history, starting an annual Haunted Happenings festival in 1982 that became a month-long Halloween celebration. The witch trials became a way to boost the local economy during hard times.	"['Found 52 Resources containing: Sitcoms\nOn September 19, 1970, “The Mary Tyler Moore Show” premiered: a mainstream sitcom about women in the workplace that millions of Americans could relate too. Today, its star, a feminist icon in her own right, Mary Tyler Moore, died. She was 80 years old.\nThough “The Mary Tyler Moore Show” ran for seven season and became one of the most decorated shows of all time, it almost didn’t make it past its first season. The reason was because of its time slot, explains Jennifer Keishin Armstrong in her definitive book on the series, Mary and Lou and Rhoda and Ted: And all the Brilliant Minds Who Made the Mary Tyler Moore Show a Classic.\nThe show, Armstrong writes, was initially slated to run on Tuesday nights on CBS. The competitive lineup would have spelled doom for the fledgling sitcom. But then, CBS’ head of programming Fred Silverman got his hand on the pilot. What happened next changed the show\'s fate. Silverman was so impressed that after he finished screening the episode, he immediately called up his boss. “You know where we’ve got it on the schedule? It’s going to get killed there, and this is the kind of show we’ve got to support,” he said, as Armstrong reports.\n“The Mary Tyler Moore Show” got moved to Saturdays at 9:30, and the rest was history.\nIt\'s not hard to see why the pilot episode had Silverman hooked. Just take the scene where Moore\'s character, Mary Richards, gets hired as an associate producer for a Minneapolis television station—it\'s one of the most famous job interviews in television history.\nDuring it, news producer Lou Grant (a loveable Ed Asner), gives Richards a hard look. “You know what? You’ve got spunk,” he says, grudgingly.\nMoore, wearing a long brown wig to differentiate herself from the character she played on “The Dick Van Dyke Show,” nods, graciously. “Well, yes.”\nGrant’s face then does a 180. “I hate spunk,” he says, his eyes bugging out.\nThe scene is played for laughs, but it also served as an important mission statement for what “The Mary Tyler Moore Show” would be. In its 24-minute pilot, the show set itself up to tell the story of a 30-something single woman in the workplace with unapologetic “spunk.”\nThe last episode of “The Mary Tyler Moore Show” aired seven years later on March 19, 1977. Fittingly called “The Last Show” it serves as a poignant way to say goodbye to Moore today. After her character turns off the lights in the newsroom for the last time at the end of the episode, the entire cast comes on for the show’s first and only curtain call.\n“Ladies and gentlemen,” an announcer tells the Hollywood studio audience to thunderous, poignant applause. “For the last time, Mary Tyler Moore.”\nThe pink lower bodice and skirt of the dress are self-lined, with an additional paler pink petticoat attached under the lining of the skirt. The upper bodice is lined with a brown net, giving a darker hue to this section of the gown under the chiffon overlay. The sleeves are not lined.\nOrange elastic is stitched to the interior of the center front below the bustline to create a gathered empire waist. The elastic is covered by orange chiffon where it is attached. The length of orange elastic reaches beyond the stitched front portion, being long enough to reach around to the wearer\'s back waist for additional bust support. No back closure method is apparent on the elastic except for dark brown threads, indicating the elastic was sewn closed after the dress was put on by the wearer. The dress is not lined. An off-white fabric label with black embroidered text is sewn to the interior proper left side of the neck that reads: ""THE WARDROBE WING / HOLLYWOOD CALIFORNIA"". Three (3) off-white fabric labels are sewn into the side seam just above the hem detail including a retailer\'s label for ""I MAGNIN"", care instructions, and sizing.\nThe dress (a) is a knee-length A-line style with full length sleeves and a forward point collar. It has a concealed zipper closure at the center front, with the light brown zipper reaching from the waist to the collar. Above the top of the zipper closure, the front of the dress folds open at the center neck. There are two (2) rectangular patch pockets, one at each front hip. The set-in sleeves are straight cut with elastic at the wrist and no cuff. There is piping, in the same gingham check, along the neckline and in vertical strips on the proper left and proper right sides on both the front and back of the dress. A single gore of gingham check is sewn to the outside portion of the dress beside the piping at the proper left and proper right sides on the front and back, beginning approximately sixteen (16) inches above the hemline. Hem weights are sewn inside the dress hem, except at the front and back center panels. The dress is not lined. A rectangular white paper label is attached to the seam allowance at the interior proper left front side of the bodice that reads in handwritten black ink: ""252"".\nThe string belt (b) is a length of the gingham check cotton sewn on the bias. One end is turned and finished, while the other is unfinished but previously may have come unstitched.\nThe cast of the television sitcom ""All in the Family"" came to the National Museum of History and Technology, now the National Museum of American History, to donate Archie and Edith Bunker\'s chairs to the ""A Nation of Nations"" exhibit in September of 1978. (L-R): Jean Stapleton, Secretary (1964-1984) S. Dillon Ripley, Norman Lear, Sally Struthers and Rob Reiner as they peer into the case where the chairs are displayed.\nWith Starbucks coffee bars sprouting up on every corner, and a yen for espresso sweeping the land, author Minna Morse went in search of America\'s hot new hangout: the neighborhood coffeehouse.\nAs the craze for coffee has gathered steam retail sales of specialty coffee beans and beverages will have doubled by the end of the decade- so has the yearning for a welcoming place to drink it. From Caffè Trieste in San Francisco, to the Politics and Prose bookstore and coffeehouse in Washington, D.C., patrons are singing the praises of places where they can find cappuccino and camaraderie.\nThe coffeehouse clientele harks back to a noble tradition. The world\'s first coffeehouse is said to have opened in Damascus in 1530. Coffee began appearing in Europe in the early 17th century, with each country evolving a distinctive coffeehouse culture, from England to Austria.\nToday, in a world where everyday life seems increasingly anonymous and fragmented, students and freelance workers, retirees and mothers with babies, singles and couples, are finding a new community center: their local coffeehouse. Even in sitcom land, we\'re heading there--as the characters in the megahit Friends can attest, converging weekly at Central Perk.\nDespite its reputation for progressive politics, Hollywood has been surprisingly skittish when it comes to publicly acknowledging sexual difference. Thus it was a landmark event when in April 1997, comedian Ellen DeGeneres told Time magazine, ""Yep I\'m Gay."" DeGeneres went on to say that she was ambivalent about coming out, feeling that there should be a zone of privacy allowable to public figures and celebrities. But she also felt that not openly acknowledging her lesbianism meant that she lacked control over her own life and career. Coming out, she said afterward, ""has been the most freeing experience because people can\'t hurt me anymore."" Social issues aside, DeGeneres is one of America\'s most successful show business figures: star of her own sitcom and talk shows and host of the Oscars and other awards shows, she continues to perform as she began-as a stand-up comedian.\nThis 1994 caricature by John Kascht was the first color drawing to appear in the Arts and Leisure section of the New York Times.\nGeorge Carlin was an American stand-up comedian known for his blunt and unapologetic approach to taboo subjects, including politics, language, psychology and religion. Carlin’s “Seven Words You Can Never Say on Television” routine led to his arrest in 1972 for violating obscenity laws. The routine later became central to the 1978 U.S. Supreme Court case F.C.C. v. Pacifica Foundation. In a 5–4 decision, the Court affirmed the government’s power to censor material on public airwaves.\nCarlin was a frequent performer and guest host on The Tonight Show during Johnny Carson’s tenure as host. He also appeared in numerous films, including the cult classics Bill & Ted’s Excellent Adventure (1989) and Bill and Ted’s Bogus Journey (1991). He starred in the sitcom The George Carlin Show from 1993 to 1995 and released fourteen HBO comedy specials. In 2008, Carlin was posthumously awarded the Mark Twain Prize for American Humor.\nHow did Salem, Massachusetts become a Halloween destination? For centuries, the New England town avoided any association with its infamous Puritan ancestors, who executed 19 people under suspicion of practicing witchcraft. The surprising answer, author Stacy Schiff writes for The New York Times, has a lot to do with the sitcom ""Bewitched.""\nThese days, Salem is rife with kitschy witches and Halloween attractions. But before the late 20th century, town citizens rarely acknowledged the Puritan trials. When playwright Arthur Miller visited Salem to research ""The Crucible"" in 1952, locals refused to help him. ""You couldn\'t get anyone to say anything about it,"" he complained, according to ""Salem: Place, Myth, and Memory.""\nUntil ""Bewitched,"" that is. In 1970, the popular sitcom filmed episodes on location in Salem, including one where Samantha Stephens, the titular sorceress, travels back in time and is put on trial. Schiff writes:\nAccused of witchcraft in old Salem, she winds up manacled, on trial for her life. She admits to the charge. But she announces to the courtroom that she will also prove that no 17th-century suspect was a witch [...] ""How can you imprison someone who can vanish before your very eyes?"" she demands. Firmly she sets our Puritan forebears straight: ""The people that you persecuted were guiltless. They were mortals, just like yourselves. You are the guilty,"" she informs the old Salemites, before she vanishes, at long last clearing the air.\nAfter ""Bewitched,"" Salem began to embrace a tourist-friendly version of its grim past. The town began hosting an annual Haunted Happenings festival in 1982, which quickly exploded into a month-long Halloween celebration. In 2005, a statue of Elizabeth Montgomery, the actress who played Samantha, was erected in Salem\'s town square. Today, even the city\'s police cars are decorated with witch insignia.\nStill, ""Bewitched"" isn\'t the whole story. For centuries, Salem\'s port made it one of the wealthiest towns in New England. The town had fallen on hard times when the sitcom arrived, Schiff explains. The witch trials became a way to boost the local economy. ""There is, perhaps, something a little ghoulish in all this,"" Matt Crowley, who grew up in Salem, writes for the A.V. Club. ""After all, we can follow a straight line from a bunch of murders in the 1690s to a Ferris wheel today.""\nMaybe that\'s why residents opposed Montgomery\'s statue when it was first proposed. While supported believed it would be good for tourism—akin to Philadelphia\'s statue of Rocky Balboa— others thought it was an affront to the innocent people who were murdered. \'\'It\'s insensitive to what happened in 1692,"" Salem resident Jean Harrison told the Boston Globe\'s Kathy McCabe in 2005. \'\'She was a fictional witch, but the people who died were not witches.""\nNevertheless, as Schiff writes in the November issue of Smithsonian, the legacy of the Salem witch trials linger in modern American culture:\nWhen computers go down, it seems far more likely that they were hacked by a group of conspirators than that they simultaneously malfunctioned. A jet vanishes: It is more plausible that it was secreted away by a Middle Eastern country than that it might be sitting, in fragments, on the ocean floor. We like to lose ourselves in a cause, to ground our private hurts in public outrages. We do not like for others to refute our beliefs any more than we like for them to deny our hallucinations.']"	['<urn:uuid:40be17b2-6953-4b5b-83ab-d40c6914f2b6>']	factoid	direct	short-search-query	distant-from-document	single-doc	expert	2025-05-12T10:19:05.071190	7	57	2050
40	covid lymphopenia mucormycosis relationship mechanism explain	COVID-19 infection affects CD4+ and CD8+ T-cells, leading to lymphopenia (reduction in lymphocytes and T-cells). This is significant because Mucorales-specific T-cells (CD4+ and CD8+) produce cytokines like IL-4, IL-10, IL-17 and interferon-gamma that damage fungal hyphae. Therefore, lymphopenia could increase the risk of developing invasive mucormycosis, while recovery of lymphocyte count could improve adaptive immunity by inducing production of Mucorales-specific T-cells that help control the infection.	['A middle-aged woman with diabetes presented with left-sided facial pain, complete ptosis and fever of short duration. On presentation, she had hyperglycaemia without ketosis. There was total ophthalmoplegia of the left eye with a visual acuity of 6/36. She incidentally tested positive for COVID-19. CT paranasal sinus and MRI brain revealed left-sided pansinusitis with acute infarct in the left parieto-occipital region without angioinvasion. An emergency functional endoscopic sinus procedure was done, which confirmed mucormycosis on histopathological examination. After 1 week of conventional amphotericin B and antibiotics, repeat CT brain showed improvement in mucosal thickening and sinusitis. This case is a rare presentation of mucormycosis associated with rapid progression to orbital apex syndrome with brain infarction in a patient with non-ketotic diabetes and COVID-19. Early diagnosis and treatment are essential to prevent further end-organ damage. It is also interesting that there was no angioinvasion and transient periarterial inflammation was attributed to brain infarction.\n- tropical medicine (infectious disease)\nThis article is made freely available for use in accordance with BMJ’s website terms and conditions for the duration of the covid-19 pandemic or until otherwise determined by BMJ. You may use, download and print the article for any lawful, non-commercial purpose (including text and data mining) provided that all copyright notices and trade marks are retained.https://bmj.com/coronavirus/usage\nStatistics from Altmetric.com\nMucormycosis is an angioinvasive disease caused by fungi of the order Mucorales like Rhizopus, Mucor, Rhizomucor, Cunninghamella and Absidia. The prevalence of mucormycosis in India is approximately 0.14 cases per 1000 population, about 80 times the prevalence in developed countries.1 COVID-19 infection has been associated with fungal infections. Mucormycosis is more often seen in immunocompromised individuals, and complications of orbital and cerebral involvement are likely in diabetic ketoacidosis and with concomitant use of steroids. The most common risk factor associated with mucormycosis is diabetes mellitus in India.2 In the background of the COVID-19 pandemic, only a limited number of cases of mucormycosis have been reported, but there are no known documented cases of sudden-onset visual loss with incidental COVID-19 infection in a newly detected young non-ketotic diabetic.3\nA middle-aged woman, with newly detected diabetes mellitus (haemoglobin A1c: 12.39%), presented with left eye complete ptosis, facial pain for 5 days and preceding fever for 3 days with no symptoms of rhinitis or sinusitis. Clinical examination revealed tenderness of all sinuses on the left side. There was a complete internal and external ophthalmoplegia of the left eye, with absent left eye direct light reflex and left eye visual acuity of 6/36 (figure 1). The right eye movements and vision were normal. The rest of the neurological examination was within normal limits. The patient was afebrile on presentation. Pulse was 78 bpm, blood pressure was 124/80 mm Hg and oxygen saturation was 98% on room air. Systemic examination was unremarkable, and there was no clinical evidence of ketoacidosis.\nAdmission glucose was 378 mg/dL without ketosis. Serum glycated haemoglobin was 12.39%. A CT paranasal sinus showed total opacification of the left ethmoid, maxillary and frontal sinuses (figure 2). An MRI brain showed acute infarct in the left parieto-occipital lobe with a subperiosteal abscess in the superomedial extraconal aspect of the left orbit (figure 3) with thickening and perineural enhancement of the left optic nerve. There was no thrombus in the left internal carotid artery (ICA); however, there was some periarterial inflammation of the left ICA (figure 4). The total leucocyte count was 10.6 × 109/L with a lymphocyte count of 1696 cells/uL. Serial monitoring showed a falling trend of lymphocyte count from 1696 (day 1), to 1246 (day 5), to 924 (day 10), to 666 (day 14) cells/uL. However, CD4 or CD8 testing could not be done. Serum liver biochemistry was within normal limits. Blood urea was 12.3 mg/dL, and serum creatinine was 0.44 mg/dL. The serum electrolytes were within normal limits. She had a B-positive blood type. Inflammatory markers were C reactive protein, 68.35 mg/L (reference range: ≤6 mg/L); procalcitonin, 0.069 ng/mL (reference range: <0.5 ng/mL); D-dimer, 0.80 μg FEU/mL (reference range: <0.5 μg FEU/mL); and serum ferritin, 180.2 mg/dL (reference range for men, 30–400 ng/mL; women, 13–150 ng/mL). She tested positive incidentally for COVID-19 by real-time PCR (nasal and oropharyngeal swab) on day 2 of hospitalisation using TRUPCR SARS-CoV-2 RT qPCR KIT (version 3.2). Chest X-ray (figure 5) was unremarkable, and the patient never required oxygen therapy. She was classified as having a mild COVID-19 infection. Functional endoscopic sinus surgery (FESS) was performed on an emergency basis due to orbital apex syndrome as she fell into group ‘A’ of endonasal surgery indications4 and showed unhealthy, polypoidal mass and slough in the maxillary, anterior and posterior ethmoid sinuses with pus in the frontal sinus and polypoidal mucosa in the sphenoid sinus. Histopathological analysis of the biopsy sample from ethmoid sinus showed fungal colonies of broad aseptate hyphae at an obtuse angle with periodic acid–Schiff stain, which was consistent with mucormycosis. Fungal culture of the sample obtained after sinus debridement in FESS confirmed mucormycosis (Rhizopus species). However, serological fungal markers and fungal culture sensitivity were not tested. The patient was hospitalised for 17 days. Her sugars were well controlled after the initiation of insulin therapy. However, there was no resolution of ophthalmoplegia or ptosis until she was discharged.\nShe was initiated on conventional amphotericin B (given for 11 days) and aspirin for acute cerebral infarct. Post FESS, CT paranasal sinus imaging was done after 1 week of treatment with antifungal therapy and showed a reduction in the diffuse opacification of the left ethmoid, frontal and maxillary sinuses (figure 6).\nAn active search of literature reviewed few reported rhino-orbitary cases associated with COVID-19.3 5 6 Diabetes mellitus is an independent risk factor for rhino-orbital–cerebral mucormycosis in a meta-analysis of 600 series with 851 cases. The most common species isolated was Rhizopus species, with an overall mortality of 46%.7\nA case of COVID-19 with rhino-orbital mucormycosis coinfection associated with ketoacidosis was reported in a patient with recent-onset diabetes mellitus.8 Pathogenic mechanisms involved in fungal aggressiveness include decreased phagocytic activity, accessible amounts of iron due to the displacement of protons by transferrin in diabetic ketoacidosis and fungal heme oxygenase, which promotes iron absorption for its metabolism.\nIn a case described of severe COVID-19 associated with fungal coinfection, cell counts revealed that there was a progressive increase in white blood cell count and neutrophils while lymphocytes progressively decreased.9 It is hypothesised that SARS-CoV-2 infection may affect CD4+ and CD8+ T-cells, which are highly involved in the pathological process of COVID-19 infection. It has been shown that in severe COVID-19 cases, there is a reduction in the absolute number of lymphocytes and T-cells, which is associated with the worst outcomes. Mucorales-specific T-cells (CD4+ and CD8+) produce cytokines such as interleukin (IL) 4, IL-10, IL-17 and interferon-gamma (IFN-γ) that damage the fungal hyphae. Such specific T-cells were seen only in patients affected by invasive mucormycosis, and they concluded that they could be a useful surrogate diagnostic marker of an invasive fungal disease. It might be speculated that lymphopenia could increase the risk of developing invasive mucormycosis, while the recovery of lymphocyte count could improve the adaptive immune system and induce the production of Mucorales-specific T-cells, which might have a role in controlling the invasive infection.\nThere are a significant number of reports showing alterations in cell-mediated immunity, such as chemotaxis, phagocytosis and cytokine secretion in both type 1 and type 2 diabetics. Individuals with diabetes have been described to have alterations in innate immune system components. Natural killer cell activity is reduced in individuals with diabetes, and more pro-inflammatory M1 macrophages are present. Furthermore, T-cell activity is skewed. Disease severity in patients is due to not only the viral infection but also the host response. Elevated glucose levels may also suppress the antiviral response. In the context of COVID-19, severe disease progression is described by a delay in IFN-γ response with a prolonged hyperinflammatory state and lower CD4 and CD8 cell numbers. Regardless of the involvement of the endothelial cells, the initial delay in IFN-γ response together with the hyperinflammatory response in individuals with diabetes may exacerbate the ‘cytokine storm’ and increase COVID-19 severity. Increased vascular lesions, endothelial inflammation and vasoconstriction associated with endothelial dysfunction put individuals with diabetes at a greater risk for endotheliitis in several organs. Change of vascular tone towards more vasoconstriction can lead to subsequent organ ischaemia, tissue oedema and a procoagulant state. Finally, dysregulated immune cell populations and activity observed in individuals with diabetes play a critical role in aggravating the severity.10\nA case series in the Indian subcontinent reported six cases of rhino-orbital–cerebral mucormycosis following COVID-19 infections.11 The mean duration between the diagnosis of COVID-19 and the development of symptoms of mucormycosis was 15.6±9.6 days. Control of hyperglycaemia, early treatment with liposomal amphotericin B and surgery are essential for the successful management of mucormycosis. Thus, the use of glucocorticoids in mild COVID-19 cases (without hypoxaemia) or the utilisation of higher doses of glucocorticoids should be avoided. Further, in the absence of a clear benefit, drugs targeting immune pathways such as tocilizumab should be discouraged. For successful management of mucormycosis, a high index of clinical suspicion, low threshold for diagnosis in patients with risk factors, neuroimaging and specific diagnostic tests with a coordinated effort from a multidisciplinary team including ophthalmology, otorhinolaryngology, infectious diseases, neurosurgery, critical care, microbiology and pathology department are crucial. A delay of even 6 days in initiating treatment doubles the 30-day mortality from 35% to 66%.11\nSimple tests like vision, pupil, ocular motility and sinus tenderness can be part of routine physical evaluation of a patient with COVID-19 hospitalised with moderate to severe infection or diabetics with COVID-19 or those receiving systemic corticosteroids. Visual prognosis, however, continues to remain poor.\nThus, it is important to have a high index of suspicion for fungal coinfection in patients with COVID-19 presenting with comorbidities. Further, they should undergo immediate imaging studies with an emphasis on the requirement of surgical intervention. There is a need to stress on the judicious use of steroids to avoid flaring up of the fungal infection.\nThis case is an unusual presentation of rapidly developing fungal infection in a patient with non-ketotic diabetes in the background of COVID-19. Severe disease progression in the absence of use of immunosuppressants makes it a rare case.\nAn alteration in the T-cell population in COVID-19 infection is linked to the pathogenesis of fungal infection.\nEarly diagnosis and treatment of mucormycosis that involve antifungal therapy and surgical debridement are necessary to reduce mortality and prevent end-organ damage.\nJudicial use of immunosuppressive therapy in COVID-19 infection should be considered particularly in regard to treatment of fungal coinfections.\nContributors All four authors were involved in patient care directly. Writing the initial manuscript was done by SMR. Selecting appropriate image templates was done by VVK. The necessary corrections and final outcome of the article were done under the guidance of SPS and LS.\nFunding The authors have not declared a specific grant for this research from any funding agency in the public, commercial or not-for-profit sectors.\nCompeting interests None declared.\nProvenance and peer review Not commissioned; externally peer reviewed.\nIf you wish to reuse any or all of this article please use the link below which will take you to the Copyright Clearance Center’s RightsLink service. You will be able to get a quick price and instant permission to reuse the content in many different ways.']	['<urn:uuid:0419cc06-8800-41cd-8467-90adc4377911>']	open-ended	with-premise	short-search-query	distant-from-document	single-doc	expert	2025-05-12T10:19:05.071190	6	66	1894
41	worried about bladder infection how many women get urinary tract infections in life	More than half of all women will experience a urinary tract infection (UTI) in their lifetimes, and 20 percent to 40 percent of those patients will be affected by recurrent UTIs.	"[""Bacteria that cause urinary tract infections invade bladder cells\nThe finding confirms a controversial revision of scientists' model of how bacteria cause UTIs. Previously, most researchers assumed that the bacteria responsible for infections get into the bladder but do not invade the individual cells that line the interior of the bladder.\n“Our animal model of UTIs has allowed us to make a number of predictions about human UTIs, but at the end of the day, we felt it was critical to show this in humans, and now we've done just that,” says senior author Scott J. Hultgren, Ph.D., the Helen L. Stoever Professor of Molecular Microbiology at the School of Medicine.\nThe results appear in the December issue of Public Library of Science Medicine.\nFully understanding what bacteria do in the bladder is critical to developing better diagnoses and treatments for UTIs, Hultgren says. The bacterium Escherchia coli is thought to be responsible for 80 percent to 90 percent of UTIs, which occur mainly in women and are one of the most common bacterial infections in the United States. Scientists estimate that more than half of all women will experience a UTI in their lifetimes, and recurrent UTIs will affect 20 percent to 40 percent of those patients.\n“Recurrence is one of the biggest problems of UTIs,” says Hultgren. “Even though we have treatments that eliminate the acute symptoms, the fact that the disease keeps recurring in so many women tells me that we need to develop better treatments.”\nPrior to the work of Hultgren and his colleagues, most microbiologists and urologists believed for a variety of reasons that E. coli wasn't getting into bladder cells.\n“For example, there is a barrier in the bladder that prevents toxins and other things in your urine from leaking back into the body,” notes David Rosen, an M.D./Ph.D. student at the School of Medicine and lead author of the paper. “And it was thought that bacteria could not penetrate that barrier.”\nA biopsy could reveal the presence of bacteria in bladder cells, but taking a tissue sample in an infected bladder incurs an unacceptable risk of allowing bacteria to spread into the bloodstream, a dangerous condition called sepsis.\nScientists also thought that if the bacteria were getting into bladder cells, they would replicate and spread rapidly, sometimes leading to sepsis. But after Hultgren first discovered that bacteria are able to invade bladder cells in 1998, he later found evidence in his animal model that bacteria could establish residence inside those cells. He showed that this process involved several behavioral changes that allow the bacteria to form cooperative communities known as biofilms. By working together, bacteria in biofilms build themselves into structures that are more firmly anchored in infected cells and are more resistant to immune system assaults and antibiotic treatments.\nTo prove that the model correlates with human infections, Rosen led an analysis of human urine samples sent from a clinic at the University of Washington in Seattle. The 100 patients who gave samples were either suffering from an active, symptomatic infection or had previously suffered infections. Researchers analyzing the specimens were not told which group of patients individual specimens had come from.\nUsing light and electron microscopy and immunofluoresence, scientists found signs of bladder cell infection in a significant portion of the samples from patients with active UTIs. These included cells enlarged by bacterial infection and shed from the lining of the bladder.\nIn addition, Hultgren's experiments had previously suggested that some bacteria progress to a filament-like shape when exiting out of the biofilm. Rosen was able to identify bacteria with this filamentous morphology in 41 percent of samples from patients with symptomatic UTIs.\nNeither indicator was detected in urine from women who did not have active infections. This was anticipated: Hultgren's animal model work suggests that when women are between episodes of symptomatic infection, intracellular E. coli may be in dormant phases where there would be little cause for bacteria or the cells they infect to be shed into the urine.\nFurther research is needed to determine if the infection indicators Rosen detected in urine samples from symptomatic women are signs of increased risk of recurrent infection. But looking for those signs using immunofluorescent staining and a variety of microscopy methods is unlikely to be practical on a widespread clinical basis. So to follow up, Hultgren plans a search for biochemical indicators linked to higher risk of recurrent UTIs and of infection spreading to a patient's kidneys. His lab also continues to be involved in many different efforts to develop new vaccines and treatments.\n“What we're learning about how bacteria behave in the bladder may also have application to other chronic, treatment-resistant infections such as sinus infections and ear infections,” he says. “We're increasingly starting to realize that biofilm formation is generally an important strategy bacteria use to evade host responses and antibiotic therapies. Attacking biofilms is going to be a really important approach as we enter a new era of fighting infectious diseases.”\nAll news from this category: Health and Medicine\nThis subject area encompasses research and studies in the field of human medicine.\nAmong the wide-ranging list of topics covered here are anesthesiology, anatomy, surgery, human genetics, hygiene and environmental medicine, internal medicine, neurology, pharmacology, physiology, urology and dental medicine.\nRare quadruple-helix DNA found in living human cells with glowing probes\nNew probes allow scientists to see four-stranded DNA interacting with molecules inside living human cells, unravelling its role in cellular processes. DNA usually forms the classic double helix shape of…\nA rift in the retina may help repair the optic nerve\nIn experiments in mouse tissues and human cells, Johns Hopkins Medicine researchers say they have found that removing a membrane that lines the back of the eye may improve the…""]"	['<urn:uuid:91bb56a2-356e-452c-ae4d-fe8337964252>']	factoid	with-premise	long-search-query	distant-from-document	single-doc	novice	2025-05-12T10:19:05.071190	13	31	963
42	How does nature symbolism connect to spirituality in Celtic traditions?	In Celtic traditions, nature symbolism was deeply connected to spirituality through multiple aspects. Medieval poems and religious texts show how natural elements like bees, linnets, heather, and water were seen as manifestations of divine presence and order. The hermits appreciated these natural elements not just aesthetically but as resources provided by God. However, while Celtic Christians had a deep appreciation for nature, this wasn't unique to Celtic Christianity - rather, it was part of their broader religious practice that balanced both God's immanence in nature and His transcendence as the high King of Heaven.	"['One of my favorite poets is W.B. Yeats (you’re shocked, I know)—and one of my favorite of his poems is “The Lake Isle of Innisfree.” I’m not alone in liking this poem—in fact, for quite a few years, Irish schoolchildren were required to memorize it, as it is aesthetically beautiful and contains nationalist overtones.\nI’d like to share with you both the poem and my interpretation of some aspects of the poem.\n“The Lake Isle of Innisfree”\nby W.B. Yeats\nI will arise and go now, and go to Innisfree,\nAnd a small cabin build there, of clay and wattles made:\nNine bean-rows will I have there, a hive for the honey-bee;\nAnd live alone in the bee-loud glade.\nAnd I shall have some peace there, for peace comes dropping slow,\nDropping from the veils of the morning to where the cricket sings;\nThere midnight’s all a glimmer, and noon a purple glow,\nAnd evening full of the linnet’s wings.\nI will arise and go now, for always night and day\nI hear lake water lapping with low sounds by the shore;\nWhile I stand on the roadway, or on the pavements grey,\nI hear it in the deep heart’s core.\nIt’s short and sweet, but we can say so much about it!\nHere’s what I have to say about this poem, in the context of the medieval Irish tradition of Green Martyrdom. This article was published in 2009 in Volume 26, Issue 1 of the Yeats-Eliot Review.\n“A Millennium’s Journey Into ‘The Deep Heart’s Core’:\nWilliam Butler Yeats’ ‘The Lake Isle of Innisfree’ and Green Martyrdom”\nby Maura Grace Harrington\nMost readers agree that the persona in William Butler Yeats’ “The Lake Isle of Innisfree” is longing for and celebrating his mental return to a bucolic setting, despite his physical presence in a modern city. First published in 1893 in Yeats’ second poetry collection, The Rose, the poem reflects the isolation and exile that the poet felt in his surroundings in London. He would feel more at home on a comparatively deserted island, Innisfree, likely based on the island of the same name in County Sligo, near where he grew up. Through his mind’s journeying to this place, where he can be alone and attain “some peace” (ln. 5), the persona of the poem hopes to fulfill a vocation that he “hear[s] in the deep heart’s core” (ln. 12). This poem carries universal meaning as a statement of an individual isolated by the hustle and bustle of modern urban life who wishes to return to the natural world and therefore to return to himself, in a Wordsworthian sense. However, the poem also has a particularly Irish flavor in its references to the medieval Irish practice of green martyrdom.\nBecause at the time of Ireland’s wholesale conversion to Christianity there was a dearth of opportunity for physical blood (or red) martyrdom, the faithful often sought other ways to make penitential sacrifices to God. Strongly influenced by the asceticism of St. Anthony of Egypt and of the desert fathers, who chose to deny themselves physically in order to grow spiritually, a crop of Irish hermits sprang up, secluding themselves in the wilderness in order to enhance their religious devotion. Poetry was written by the hermits themselves and by others about the hermits’ affinity with nature, as experienced through this green martyrdom. Though written in Irish, these poems have enjoyed frequent translation into English. The themes of peacefulness, order, slowness of pace, and an almost mystical natural beauty are evident in the medieval poems, and also find their way into Yeats’ “The Lake Isle of Innisfree.” Not only are the tropes and themes of the medieval poems updated for a modern audience through Yeats’ poem but the medieval and modern counterparts also share specific word choice and imagery, which indicates similarity in purpose and kinship in attitude and taste.\nDrawing out the similarities among three medieval poems, “The Hermitage” attributed to St. Manchan of Offaly and “The Hermit” and “The Hermit’s Song” by unknown writers, and comparing these to Yeats’ “The Lake Isle of Innisfree” will demonstrate that Yeats’ poem, though singularly beautiful, is by no means the first of its tradition in Irish poetry. Indeed, it is perhaps because Yeats so adeptly translates the controlling ideas in the earlier poems into those suited to a modern audience that ever since the work’s composition, this poem has been so highly esteemed as representative of its nation of origin. The three medieval poems focus on the benefits to the Christian faith of experiencing nature, although the speaker in “The Hermit’s Song” does not explicitly invoke God. While Yeats’ poem includes no direct reference to God, the poem’s other components, when taken together, indicate, as do the three medieval poems, a desire to retreat to nature in order to experience peace, order, a slowing down, and nature’s beauty and plenitude, all of which phenomena are considered, in the medieval poems and in the Irish Christian tradition generally, as stepping-stones to the experience of God.\nPerhaps the logical place to embark on an investigation into the similarities among the poems is at the beginning of these works. In all cases, early in the poems the speakers describe the physical habitations that they will enjoy in their retreats into the wilderness. All of the personae describe humble dwellings, some of which are constructed by nature itself, others of which are constructed from unadulterated natural materials. Yeats’ speaker, for example, will construct “a small cabin…of clay and wattles” (ln. 2). The simplicity of the edifices that will protect the retreatants from the elements of nature implies a trust that they will be safe, regardless of, or perhaps because of, the lack of fortifications constructed against nature. Significantly, Yeats’ persona describes his cabin immediately prior to the line in which he explains that he wants to have “a hive for the honey bee” (ln. 3) and two lines before he praises the virtues of “the bee-loud glade” (ln. 4). By juxtaposing his simple dwelling and his appreciation for bees, Yeats’ persona brings to mind the beehive huts that medieval Irish hermits often crafted in the woodlands so that they could encounter God in solitude. In a similar vein, in “The Hermit’s Song,” the persona praises “Bees and beetles” as “music-makers” (ln. 41) and St. Manchan esteems “Salmon and trout and bees” (ln. 40) as a source of wholesome food. In addition to evoking the beehive huts through his placement of the lines on bees, then, Yeats also situates his work on the praise of nature in the tradition of the mystics who appreciated bees, which creatures could be considered pesky but were not believed to be so by the green martyrs, who were happy to use all of the resources with which God had provided them.\nOther natural imagery is common among the poems. Additional animals, for example, are mentioned in the poems, almost in catalog fashion. The linnet is mentioned as a staple of the milieu in both “Innisfree” and “The Hermit’s Song.” Also, there is consistency in the plant names in the poems. In “The Hermit’s Song,” the persona notes that the landscape is marked by “upland heather” (ln. 51), and he relishes the “Heathberries and whortleberries [that he eats] / For a sweet” (ll. 27-28). The speaker in “The Hermit” celebrates the fact that flora compose parts of his abode: “The doorsteps are of heather, / the lintel of honeysuckle” (ll. 5-6). Additionally, the persona is aware of the heather that provides him with berries to eat and that marks the landscape of the area surrounding him (ll. 36 and 56). More obscurely for those who do not speak Irish, “Innisfree” includes a mention of heather in its very title. P. W. Joyce, author of Irish Names of Places, originally published in 1869 and commonly read by the Irish at the time of Yeats’ composition of the poem, provides the etymology of locations in Ireland. Joyce notes that “Innisfree” in Lough Gill in Co. Sligo is named “heathy island,” from “Inish” for “island” and “fraeigh,” the genitive form of the word for “heather” (520). It is reasonable to expect that Yeats, well-read in Irish cultural history, would have been aware of the derivation of the name of his title location. A place named after such foliage fits in well with the loci described in the medieval predecessors of the poem.\nIn the medieval poems and also in Yeats’ “Innisfree,” order and harmony contribute to a sense of peace that the personae feel or anticipate feeling in their bucolic abodes. St. Manchan and Yeats refer to order in nature in an explicit manner in their numbering and clearly laid-out plans for their habitations. St. Manchan, who will not be entirely alone but seems to be planning to form a monastic community in the wilderness, has a plan for how many of his companions will be in each row in their simple chapel. In a less rigorous sense, the anonymous authors of “The Hermit” and “The Hermit’s Song” imply the order of nature by virtue of the fact that the disparate elements of nature provide for the needs of the hermits. “Sparkling wells and water-torrents, / best for drinking” (ll. 21-22) abound in “The Hermit,” and the green martyr is a guest at “meals [to which] the woods invite [him]” (ln. 21), featuring such delicacies as “water, herbs and cresses, / Salmon, trout” (ll. 23-24). Having one’s needs met leads to a feeling of peace, as expressed more or less explicitly by each of the four personae. For the speaker in “The Hermit’s Song,” the slowness of pace and softness of sound of his surroundings contribute to the peace he feels. For the persona in Yeats’ poem, “peace comes dropping slow” (ln. 5), tying together peace and a slowing of pace. For two of the three medievals, of course, the peace is also induced by Christian meditation on God, while in Yeats’ poem religious devotion is not explicitly described.\nHowever, the four poems might in fact share a similar religious impulse. St. Manchan hears the call of God to “pray in every place” (ln. 44), and is actively engaged in seeking out a natural setting in which to pursue spiritual fulfillment (ll. 1-4). The speaker in “The Hermit” celebrates the solitude of his abode by virtue of the fact that “none knows it but the Lord, my God” (ln. 2), and in the final three stanzas of the poem, he acknowledges Christ as the source of all of the bounty from which he benefits in his “bothy” (ln. 72). The speakers in both “The Hermit’s Song” and “Innisfree” experience a calling in their bucolic bungalows. The persona of “The Hermit’s Song” proclaims that “in wreathed boughs the wind is whispering” (ln. 57) and that “river water falling / Is calling too” (ll. 59-60). While the least explicitly Christian in religious bent of the four poems, Yeats’ work is perhaps the one most strongly influenced by the notion of vocation. In declaring at the beginning of the poem “I will arise and go now, and go to Innisfree” (ln. 1), the speaker conjures up the image of rising above his current circumstances. Additionally, the use of the word “go” twice in this first line gives the word emphasis, and highlights the movement to which the speaker feels compelled. This incipient movement suggests that he is on a mission and aligns him with the green martyrs who left their homes and families to venture into the wilderness. The repetition, in the first line of the final stanza, of the clause “I will arise and go now” (ln. 9) serves to give further emphasis to this impetus of motion for a higher cause. The persona in this poem is, of course, in the middle of a modern city at the time of his recitation of the virtues of Innisfree, but despite his physical location, he has a deep connection with the island, so deep that he can assert: “I hear lake water lapping with low sounds by the shore; / … / I hear it in the deep heart’s core” (ll. 10-12). Such a statement indicates that the persona experiences a vocational call to the island where he can experience the order and bounty of nature, which will give him peace. Although he does not claim a specifically Christian mission, this modern speaker is acting on the precedent of his early Christian compatriots who sought to “arise and go” in order to hear Him who speaks “in the deep heart’s core.\nAccess to rare texts provided by M.J. Mac Manus Collection, Seton Hall University Archives and Special Collections Center.\n“The Hermit.” Medieval Irish Lyrics. Ed. James Carney. Berkeley: U of California P, 1967. 66-73.\n“The Hermitage.” Kings, Lords, and Commons: An Anthology from the Irish. Ed. Frank O’Connor. New York: Knopf, 1959. 5-6.\n“The Hermit’s Song.” Trans. Frank O’Connor. The Penguin Book of Irish Verse. Ed. Brendan Kennelly. New York: Penguin Books, 1981. 48-50.\nJoyce, P. W. Irish Names of Places. Vol. 1. Dublin: Phoenix Publishing Co, Ltd., 1869.\nYeats, William Butler. “The Lake Isle of Innisfree.” The Collected Poems of W. B. Yeats. Ed. Richard J. Finneran. New York: Scribner Paperback Poetry, 1996. 39.\nNow, here’s the fun part: What do YOU think of this poem? Do any lines or images strike you? What about this poem is meaningful to you?', 'The Link: Modern Myths of the Medieval Past\nIf you go into any of the larger bookshops and look in the sections designated Religion or Mythology or even New Age, probably you will encounter books on ""Celtic"" themes, including ""Celtic Christianity.""\nThe subject can also be encountered beyond the bookshops. It is actively fostered by some religious communities, and Celtic liturgies are being produced for those who wish to incorporate such prayers into their worship. Study guides are available for those wanting to explore Celtic Christianity in group discussion.\nIt is easy, indeed comforting, to assume that modern Celtic Christianity offers the public a rediscovery of a faith well-known in the early medieval period but subsequently lost.\nBut does it?\nReality of the past\nMuch in the modern explanations of Celtic Christianity does go back to the medieval Irish, much that is unique in the history of Christianity.\na faith well-known in the early medieval period but subsequently lost.\nCeltic Christianity has left us many splendid legacies: stone crosses, beautiful metal artifacts, remote churches. Literature from this period has survived, too: sermons in Gaelic/Irish, and hymns and poems from the early Middle Ages. A few early Irish hymns have been translated into English, like ""Be Thou My Vision,"" and ""Saint Patrick\'s Breastplate."" Some modern writings on Celtic Christianity are concerned with the exposition of these early hymns.\nHow Celts pursued their faith was unique in many respects because they adapted it to the social structures of their time and place, a predominantly rural society. For example, Irish monasteries, which came to be mini-cities, adapted the traditional ring-fort of early Irish society.\nConsequently, Celtic Christian faith differed in some ways from the Roman Christianity practiced in Europe. For example, the Celts kept Easter at a different time and had a different shape of tonsure (monastic head shaving), though gradually these differences were ironed out. Otherwise, the Celts adhered to the same fundamental doctrines of faith.\nThis, however, is not the Celtic Christianity often presented today. To be sure, modern interpretations of Celtic Christianity range from pagan to evangelical Protestant (the latter deeply influenced by the charismatic and house-church movements). Yet most writers tend to look back to the Celtic period as a kind of golden age.\nSeveral emphases resonate throughout the writings of modern ""Celtic Christians,"" depending on the theology of the writer. Six of the most prominent are:\nGod\'s nearness, or immanence, rather than his transcendence. According to devotees of Celtic Christianity, God is all around, involved in people\'s daily life (""immanent""), rather than distant, alien, and overly holy (""transcendent"").\nGod\'s love rather than his judgment. Some enthusiasts claim Celtic Christians had little or no sense of sin.\nSimplicity of structures. The modern church is portrayed as little more than a religious bureaucracy, and early non-Celtic missionaries, like Augustine of Canterbury, are seen as power-hungry clerics wanting to control others. The ancient Celts, on the other hand, were allegedly gentle and mystical.\nTolerance of paganism. Some writers argue that Celtic Christianity simply absorbed Celtic paganism. Celtic Christians, it is claimed, were kind to secular culture. The modern church, by contrast, has written off too much of the secular world.\nFeminism. Celtic Christians, it is said, had abbesses and female druids and were not devoted solely to male ministry.\nEnvironmental concern. It is claimed that Celts lived happily with nature, whereas modern Christians have dominated and spoiled it.\nThen and now\nGiven the range of genuine texts, one can find something that seems to support one claim or another (e.g. the Celts\' love of nature or the immanence of God), but it is doubtful whether one can extrapolate a systematic Celtic theology that justifies these claims. There was no one Celtic church, only a variety of monastic centers that celebrated faith in similar but diverse ways.\nFurthermore, many of the supposed features of Celtic Christianity are responses to falsely perceived contrasts. Indeed the Celts had a deep appreciation of God\'s immanence, but also of his transcendence: ""Be Thou My Vision"" repeatedly exalts the ""high King of Heaven.""\nThey gloried in God\'s love, but they also feared his judgment. The composer of the Altus Prosator (""High Creator,"" a hymn usually ascribed to Columba) wrote, ""It seems doubtful to no one that there is a hell down there … where there is screaming of men, weeping and gnashing of teeth.""\nChristians in Celtic lands may have created simpler ecclesiastical structures, but only because their social situation or spiritual aspirations demanded it. The larger Irish monasteries were often complex in terms of layout and the number of buildings within the enclosure. Hermits lived in simple cells, but not all monks were hermits.\nCeltic Christians certainly weren\'t always tolerant toward paganism; stories of the saints confronting and defeating druids (even causing their deaths), like Patrick at Tara, abound in Irish literature. Some early Irish poets, like Oengus Cele De, rejoice in the triumph of Christianity: ""Paganism has been destroyed though it was splendid and far-flung; the kingdom of God the Father has filled heaven and earth and sea.""\nAs far as feminism, the stories of Brigit, even if they are true, are the exception that prove the rule: social structures in the medieval world were patriarchal.\nAnd as for the environment: they may have appreciated creation more than their European brethren, but as Gilbert Márkus puts it, ""There is nothing distinctively Celtic about the sense of God\'s presence in the natural world.""\nThe current movement, then, is mostly a reflection of the needs and feelings of modernity, and that is one reason it is so popular. To put it simply, Celtic Christianity is partly seen as a remedy for modern maladies.\nMany today are wearied of existing trends in politics and national life, having suffered burnout in the fiercely competitive world of the 1980s and 1990s. Many are also weary of conventional churches, which they feel have failed to address contemporary issues. During the 1980s, a number of concerns, from ecology to ecclesiology began to surface in the British Isles (and elsewhere).\nThe environment was a primary concern. As cities grew, the ozone layer began to deplete. Questions about how to use land and how to care for the natural world became significant. Celtic lands began to be viewed afresh as a region where life could be lived as God and nature intended.\nThere was also increasing fascination with simple lifestyle, frustration with global mass culture, increased support for women\'s rights, more concern about mistreatment of animals, and a host of other issues.\nThe churches have faced criticism because of their failure to tackle these issues, and they\'ve seen massive changes in the religious map. Beginning in the 1960s, people in the West began to show interest in Eastern religions. The growth in global communication and increasing ease of transport allowed the East to evangelize the West in new ways.\nThose disillusioned with contemporary Christianity could now choose from a menu of alternative religions. The various items on the menu could be mixed on a single dish; religion was now becoming a matter of personal taste rather than ecclesiastical imposition. The New Age movement, a pastiche of cults and isms, ancient and modern, Eastern and Western, became pervasive.\nAs some people went East to find new religions, others dug back into history to find a remedy for their disillusionment. This was how Celtic Christianity was ""rediscovered."" In many circles, Celtic Christianity is a kind of hodgepodge of past and present, with modern issues much to the forefront. It presents aspects of the past, but it is also deeply influenced by contemporary trends.\nIn terms of classic evangelical theology, even much of ancient Celtic theology (which mirrored medieval Catholic theology) can be questioned: Celtic Christians celebrated the Mass, applied penance, and believed in the powers of saints and relics. They lived long before the Reformation and belonged to the Catholic faith. We must view them in that context. There is much that all Christians can appreciate in the hymns, prayers, and stories of Irish saints, although their message can be fully understood only by diligent scrutiny of the texts in their original forms.\nModern Celtic Christianity, however, is very much a mixture of philosophies and ideas, bringing under one label a range of products, old and new. Much of it is a construct made to meet the needs of the postmodern spiritual consumer. It is indeed necessary for the church to address modern concerns, but the solutions lie not in a retreat to a mythical Celtic past but, as always, in a sensitive Christian engagement with the present and the future.\nDonald E. Meek is professor and chair of Celtic studies at the University of Aberdeen.\nCopyright © 1998 by the author or Christianity Today/Christian History magazine.\nClick here for reprint information on Christian History.']"	['<urn:uuid:6c455f59-f860-4448-8366-d55967baf9dd>', '<urn:uuid:3dc9a825-1adc-418d-a13f-a55bcb317fef>']	open-ended	direct	concise-and-natural	similar-to-document	three-doc	expert	2025-05-12T10:19:05.071190	10	94	3711
43	Which flooring option offers more color variety - stained concrete using acid-based stains or bamboo flooring with different finish options?	Acid-based concrete stains offer limited color options with only about eight hues available, mostly earth tones like tans, browns, terra cottas and soft blue-greens. Bamboo flooring offers more variety through different finishing processes including natural bamboo, carbonized brown bamboo, tiger striped (alternating carbonized and natural), and direct printing of different grain patterns, plus the option of staining.	"['- Stained Concrete Home\n- Stained Concrete Pictures\n- Sample Stain Colors\n- Design Ideas for Stained Concrete\n- Stained Concrete FAQs\n- Pricing Stained Concrete\n- Comparison Chart: Stained Concrete Versus Other Flooring Materials\n- Stained Concrete How-To\'s\n- Concrete Stain Application\n- Sealing Stained Concrete\n- Related Information:\n- ""Floor Me"" Video Series\n- Lea acerca de Cemento Manchado en Español\n- Other Resources:\n- Information About: Concrete Floors\n- Find Products: Concrete Stains\n- Design Ideas: Concrete Staining Info\nConcrete Floor Stains\nStained Concrete FAQsWondering if stained concrete is right for you? To help you decide, here are answers to common questions about stained concrete benefits, appearance, performance and maintenance.\nWhy do people choose stained concrete?\nStained concrete appeals to many people who want to achieve unique decorative effects for a reasonable cost. For as little as $2 per square foot, you can use stains to create an infinite array of colors and special effects on both interior and exterior surfaces. Concrete stain does more than simply add color. Rather than produce a solid, opaque effect like paint or colored coatings, stains permeate the concrete to infuse it with rich, deep, translucent tones. Some stain manufacturers use adjectives such as ""antiqued,"" ""variegated,"" or ""mottled"" to describe the distinctive look. Even when treated with the same staining product in the same shade, no two concrete floors, walls, or countertops will look alike due to factors such as the composition and age of the concrete and surface porosity.\nCan I stain concrete myself?\nWhen applying stain, using the proper tools and application techniques is vital to achieving good results. Once the stain is down, the color is permanent—there\'s no going back. If you have any doubts, hire the services of a professional, especially if you want to incorporate multiple colors and elaborate decorative effects. (See Stain Application: DIY or Hire a Pro?)\nAnother factor to consider is safety. When working with acid-based chemical stains, it’s important to take the proper precautionary measures because they often contain corrosive components that can cause eye and skin irritation and produce strong odors.\nCan all concrete be stained?\nBoth acid and water-based stains can be applied to new or old and plain or integrally colored concrete. They can also be used both indoors and out, on everything from concrete floors and kitchen countertops to pool decks and driveways.\nThe most important consideration is the condition of the surface. If the concrete is covered by grime, glues, coatings, curing membranes, or sealers that inhibit the stain from soaking in, the stain won\'t be able to penetrate and achieve full color development.\nWhat are my color options with stained concrete?\nYour color options will vary depending on whether you are using an acid or water-based stain. With acid stains, your color choices will be limited. Most manufacturers offer only eight hues, mostly subtle earth tones, such as tans, browns, terra cottas, and soft blue-greens. Although the basic color palette is sparse, you can mix two or more stain colors before application to achieve a different shade or apply one color over another. You can also produce deeper color effects with a stain by applying two coats.\nIf you want to go beyond the subtle drama and subdued color palette of acid staining, water-based acrylic stains will give you a wider spectrum of hues to choose from. Most manufacturers offer dozens of standard colors, including black and white and even metallic tints. And in many cases, the different colors can be mixed, like water-based paints, to broaden your options. (See Creating Excitement with Color and Water-Based Stain Gives Concrete Contractors a Full Palette.)\nHow do I choose the right stain color?\nColor choice is often dictated by personal preference or by a desire to match or complement an existing color scheme, such as staining a concrete floor to mirror the same tones in a wood-paneled wall. Because stain color is permanent, many homeowners opt for neutral tones, such as light tans, browns, grays and greens. Regardless of what stain colors you choose, be aware of the following caveats:\n- With acid-based stains, wide color variations are normal. Surfaces will have a mottled, variegated appearance, and these variations will be emphasized when the final coat of sealer is applied.\n- With some acid stain colors, what you see in liquid form may not be what you get once the stain has reacted with the concrete surface. The stain may not reveal its true color until it has been allowed to remain on the concrete for several hours or longer. Always apply the stain to a small test area before covering the entire surface.\n- Color effects will generally be more intense on new concrete than on older or weathered concrete.\nMost stain manufacturers will provide color charts or even actual samples of stained concrete to help you visualize the options. Contractors may also be able to provide samples of the various stain colors they work with.\nWhat special effects are possible with stained concrete?\nDepending on the color and application techniques used, stained concrete can be made to mimic everything from polished marble to tanned leather to natural stone or even stained wood.\nSome of your options include:\n- Applying multiple colors of stain, either by layering or blending stain colors (see Methods for Blending Two Colors of Acid Stain.)\n- Using stains in combination with dyes (see Concrete Dyes Expand the Color Palette of Concrete Stains).\n- Using thicker gelled stains with stencils to create artistic patterns and other decorative effects. (Learn how it\'s done.)\nTo see several examples of what creative contractors are doing with concrete stain, used alone or in combination with other decorative techniques, see Six Concrete Staining Techniques Revealed.\nWhat are the differences between acid stains and water-based stains?\nAcid-based concrete stains are made up of inorganic metallic salts dissolved in an acid and water solution. They penetrate into the surface and react chemically with the concrete to form a permanent bond. The color they impart is translucent rather than opaque, resulting in deep, rich tones and attractive marbling effects.\nNon-reactive water-based stains (typically a blend of acrylic polymers and pigments) fill the pores of the concrete surface to produce a colored film or coating, ranging from translucent to opaque depending on the product. The key difference is that no chemical reaction takes place, so the color is more consistent. Most of these products are also low in VOCs (volatile organic compounds) and safer to apply because they are free of solvents and acids. To learn more, read New Products for Staining Concrete.\nHow much does stained concrete cost?\nThe cost of staining will vary considerably depending on the complexity of the stain application, surface prep requirements, and the size of the project. A basic one-coat application of stain on concrete requiring minimal surface prep will run about $2 to $4 per square foot, while more elaborate staining projects involving multiple colors and special design details can cost $15 per square foot or more due to the time and skill level involved. See this overview of stained concrete costs and price ranges.\nWill the color fade?\nBecause stains penetrate into the concrete surface, their color is durable and long-lasting. When applied to properly prepared concrete, the color will not fade, chip, or peel away.\nHow do I maintain stained concrete?\nAlthough concrete stain is permanent and won\'t flake off like paint, it penetrates only the top layer of the concrete surface and will eventually wear away as the surface is worn by traffic or weather exposure. To prolong stain life, you should protect exterior stained concrete surfaces with a clear sealer and interior floors with a good floor wax. To keep your stained concrete looking its best, you will also need to clean it periodically by dry dust mopping and occasional wet mopping with a neutral-pH cleaner. Get more tips on how to seal and maintain stained concrete.\nReturn to Stained Concrete', 'In this article we look at the factors affecting the cost of bamboo flooring and installation prices. The average cost of bamboo wood flooring installed is $7.85 per square foot. There are many bamboo floor cost factors including DIY installation, so the potential cost range is less than $4.00 to more than $15 per square foot.\nCost factors are detailed below and summarized in this table:\nBamboo Floor Cost Factors\n|$3.50-$6.50/sq. ft.||$6.50-$9.00/sq. ft.||$9.00-$15.00/sq. ft.|\n|Subfloor Installed||No||Yes or No||Yes|\n|Pre-finished||Yes or No||Yes or No||Yes|\n|Hand scraped||No||Yes or No||Yes or No|\n|Installation||DIY or Pro||DIY or Pro||Pro|\nNote: Most online bamboo hardwood flooring cost estimates are too low.\nThis seems to be the result of overlooking premium material costs and not considering potential job extras and complexities.\nIn this guide, we seek to cover all possible costs and give you specific cost factors. They should help you estimate your bamboo and installation costs accurately given the factors related to your project. For further guides to buying, installing and cleaning bamboo floors go to our main Bamboo and Cork Flooring page Cost factors are divided into Floor Preparation Factors, Material Factors and Installation Factors.\nFloor Preparation Cost Factors\nAny type of bamboo floor can be installed on a wood subfloor. Only engineered bamboo flooring can be installed on concrete.\nUnless you make the happy discovery that your existing subfloor is in good condition, once having removed old flooring, you’ll probably need subfloor repair or replacement.\nTo install plywood 4’x8’ sheets and fasteners costs about $0.75 (75 cents) per square foot. Labor for pro installation costs less than $1.00 per square foot. Total installed cost of a plywood subfloor is about $1.35 per square foot.\nA concrete subfloor must be covered in a moisture barrier before bamboo floors are installed. It might also need to be leveled. Pros charge $1.50 to $2.50 per square foot to level concrete using self-leveling mix. Installing the vapor barrier costs about $1.25 per square foot for materials and installation.\nMaterial Cost Factors and Finish Options\nCost is affected by the bamboo flooring type and finish; here are the main aspects to consider.\nHorizontal Bamboo Plank Prices\nLow: $1.35 per square foot | Average: $3.75 per square foot | High: $8.00\nHorizontal bamboo planks are constructed of stalks sliced thinly and glued side by side. The interior off the stalks face upward to create the floor surface. This exposes the bamboo grain and the nodes called knuckles.\nVertical Bamboo Plank Prices\nLow: $2.15 per square foot | Average: $3.50 per square foot | High: $7.00 per square foot\nVertical bamboo planks use stalks sliced thin and glued so that the edges of the stalks face up to make the floor surface. This produces a more consistent, smoother appearance.\nStrand-woven Bamboo Plank Prices\nLow: $3.00 per square foot | Average: $4.15 per square foot | High: $8.00 per square foot\nBamboo is shredded and the strands are combined with resin and pressed together in molds. The resulting block is cut into flooring planks. There’s no grain pattern as a result, but strand bamboo flooring is the hardest of all bamboo types.\nQuality and bamboo age\nMature bamboo offers better quality, is harder and is backed by a longer warranty than immature, softer bamboo. Solid and engineered bamboo hardwood flooring is available, but there is little cost difference. Laminate bamboo is a cheap type of engineered flooring with a very thin bamboo veneer. It cannot be refinished and should be avoided. To find out more, have a look at our article reviewing the the best types and brands of bamboo flooring.\nFinish factors that can increase cost include:\n- Unfinished bamboo floors cost $1-$2.50 less than finished flooring, depending on other cost factors. Unfinished flooring requires sanding, staining if desired, and then sealing with several coats of protective polyurethane or similar. It’s a good way for DIY cost savings. However, buying finished flooring costs less than buying unfinished bamboo and hiring a pro to finish it.\n- Staining the bamboo adds less than $1 per square foot to the cost\n- Carbonizing the bamboo is achieved by boiling it until the sugars in it caramelize, turning the material a rich brown. This increases cost by about $1 per square foot.\n- Tiger striped flooring is produced by alternating strips of carbonized and natural bamboo. This flooring is usually in the Average to High cost range.\n- Hand scraped and hand sculpted flooring for an aged or antique look are the most expensive finishing processes.\n- Direct printing the bamboo is the process of printing a different grain pattern, such as oak, onto the flooring. It gives it the look of hardwood while maintaining the benefits of bamboo. This increases cost by less than $1 per square foot.\nInstallation Cost Factors\n|Subfloor installation (if needed)||$0-$2.50/sq. ft.||Type, whether leveling is needed|\n|Base installation cost||$3.00-$4.00/sq. ft.||Experience, reputation, local costs|\n|Stairs||$40-$75 per step||Treads only or risers, width, obstacles|\n|Remove/dispose old flooring||$0.40-0.75/sq. ft.||Type, how fastened, local disposal cost & distance|\n|Transitions||$2.50-$5.00/linear ft.||Floor types being joined, length, shape|\n|Room clearing||$20-$50 per room||Number and weight of pieces to move|\n|Appliance disconnect/ connect||$15-$30 per appliance||Electric ($) or gas ($$) and weight|\nFlooring installers produce estimates starting with a base price of approximately $3.00 to $4.00 per square foot. The price for each installer is based on their reputation, local competition and the cost of living.\nIf the job is simple, then the installation estimate will fall in that range.\nCost rises to as high as $8.00 per square foot for very difficult work. Stairs are priced separately, as you’ll see below.\nHere are installation factors affecting total cost:\n- Removing old flooring: Removing and disposing of old material costs $0.40 to $0.75 (40 to 75 cents) per square foot.\n- Installation type: Floating floors that are fastened only at the perimeter install quickly and at the lowest cost. Nailed floors cost a bit more. Those that are glued down take the longest to install, so cost is highest.\n- Flooring quantity: Small jobs that take less than a full day cost more per square foot, because the installer must cover travel time and the time required to set up and take out equipment.\n- Room sizes: Small rooms require more time-consuming trimming per square feet of flooring, so cost per square foot goes up.\n- Obstacles: The amount of trimming and piecing go up when there are obstacles to work around such as cabinets, fireplace tiles or a hearth, bath fixtures and similar.\n- Clearing rooms: Installers might charge extra if they are asked to empty rooms of furniture and appliances prior to installation. $20-$50 per room is typical depending on the number and weight of items to be moved.\n- Cost of living: It’s not unusual for prices to be 30 to 50 percent higher in large metropolitan areas, especially in the Northeast and along the Coasts than in small towns and rural areas.\nWant to know whether Bamboo is the right flooring for you? Check out our Bamboo Vs Cork Flooring Guide.\nInstalling Bamboo Flooring on Stairs\nInstallation on stairs goes slowly due to the amount of trimming required. Whether just the treads or the risers too are covered affects cost. Cost is highest when trimming and piecing around stair spindles is required.\nExpect estimates of $40-$75 per stair – or $650 to $900 for a full set of stairs.\nSaving Money on Bamboo Flooring Installation Costs\nThe best way to get a fair price for quality installation is to get written estimates from several bamboo flooring contractors. Let them know you’re getting competing estimates, and they’ll give you their best bid.\nBefore hiring a floor company, make sure its installation crew has experience and a reputation for good workmanship. That’s worth paying for – and there will be less chance you’ll have to pay someone else to “do it right” at a later time.']"	['<urn:uuid:007d5f76-2569-48eb-9046-fc418583157a>', '<urn:uuid:36f00a18-9117-45fa-8c68-564c0663f387>']	factoid	direct	verbose-and-natural	similar-to-document	comparison	expert	2025-05-12T10:19:05.071190	20	57	2647
44	How do the performance limitations of parallel processing compare between single-machine multithreading and distributed computing when dealing with tasks like calculating medians or handling small files?	In single-machine multithreading, performance is limited by the number of CPU cores and threads available, with tasks like calculating medians requiring all data to be on one machine for sorting. In distributed computing, while adding more machines can speed up certain tasks, it may actually create more overhead when working with many small files. The performance is affected by factors like latency (time to transmit results) and bandwidth (amount of data transmitted per time) in distributed systems, whereas in single-machine processing, the main bottleneck is the speed of data movement between storage levels (hard drive, RAM, and CPU cache).	"['Summary: Data can be processed in parallel by using multiple threads on a single CPU or by passing code to the data in systems like the Hadoop Distributed File System.\nImagine you’re driving your car on the way to work. You keep an eye on the road ahead, the side mirrors, the rearview mirror, and even what’s on the radio.\nYou’re changing the channel on the radio and you like the song but not sure if you want to stay on this station. Suddenly, a car pulls up from behind you and moves into your blind spot. While the music still plays in the background, you see him start veering into your lane. You deftly avoid a mild collision and, after a second to compose yourself, you go back to scanning for a good song on the radio.\nA Single CPU with Multiple Threads\nThe CPU is the brain of your computer. It does all of the processing from running your operating system to doing calculations for your deep neural network. All of this processing is done by executing commands at a mind-numbingly fast pace.\nHowever, a single CPU needs to get its data to process from somewhere. A CPU looks for data in a few places. In order from last to first…\n- Hard Drive: Slow to access, last to search\n- RAM: Faster to access\n- CPU Cache: Fastest to access, first to seearch\nIf the CPU can’t find the data it needs in the cache, it moves up the storage hierarchy. As a result, your processing stops as the data is being pulled. A multithreaded CPU, which most modern chips are, can multitask while waiting.\nWhen a task is not running due to waiting for data, a new task can be started and ran until it completes or needs data from outside the cache. Usually you have twice as many threads as the number of cores on your CPU.\nBack to the driving example, you really can only do one thing at a time while driving. You can look ahead, to the side, or behind you or you can start scanning the radio for a good song. We do these things very quickly, but for milliseconds, your attention has to be focused on only one of these things.\nThe same idea applies to CPU tasks. One task stops, the other one picks up where it left off. As another car pulls out from behind you, your attention is diverted to that car. Once that care passes you (safely) your attention returns to the road ahead of you.\nYour attention likely flits back and forth between the car passing you and the road ahead of you. You can think of this as your brain waiting for more data and thus turning its attention to the next task that’s ready.\nBack to the technical details…\nMy personal PC has 6 cores but 12 threads. This allows me to do an incredible amount of processing in a short amount of time. However, if the algorithm or process isn’t designed to be done across multiple cores, I’m stuck with running on a single thread…\nParallel Processing Across Multiple Machines\nWhen you involve multiple machines you can cut the processing time down. Instead of relying on one machine and maybe the multi threading capability, you can distribute the work across many machines.\nThe Hadoop Distributed File System (HDFS) is an example where you are sending code to multiple machines. This differs from what a normal PC file system does.\n- The CPU is essentially the driver\n- A regular PC will move data from disk to cache\n- Then the CPU continues its program with the new data\nDistributed computing instead has lots of data stored on multiple machines and the processing instructions are sent to those machines. This decreases the amount of time spent moving data around.\nThe only trick is that each computer in HDFS only has a part of the data and thus needs to eventually send their data (hopefully aggregated in some way) across the network to be further refined (or reduced) into what the user wants.\nIntuitively, letting more machines, which have more CPUs, do the work makes sense. If you have so much data and have a lot of processing needs, you can add more machines to potentially speed up the processing.\nSome examples of useful applications of distributed computing:\n- Matrix Multiplication: Classic example. Take chunks of a matrix A and multiply it by the corresponding parts of matrix B. This creates smaller chunks of data and you can combine them on one machine.\n- Training Multiple Models: If you’re building an ensemble, you can build one model on one machine and another on the next. Then, once they’re all trained, you can bring them back to one machine for prediction.\n- Iterating Through Lots of Settings: If you need to brute force your way through something, like trying hundreds of different parameters in an algorithm, you can send the task to multiple machines and it will be obviously faster than building the models one at a time.\nSome examples where adding more machines won’t be much help:\n- Calculate the Median (or any specific percentile): In order to calculate the median, you need to sort the data and find the middle value. This requires you to have all the data on one machine and know the order.\n- Working with many small files: If you’re working with thousands of small files and doing some light processing of the data, having multiple machines might create more work than necessary.\nOne of the biggest challenges in parallel processing is that some algorithms need to be changed. Think about a decision tree. In a parallel world, you don’t have access to all the data on one machine. So how do you determine the best split?\n- Each machine gets a representative sample of the data.\n- Each machine evaluates the possible splits on that training set.\n- Bring all the possible best decisions together on one machine.\n- Evaluate which of these finalist decisions is the best.\n- Repeat the process with the updated decision tree.\nIf you want a really detailed understanding of the decision tree implementation in Spark, take a look at this presentation (slides and video links at the bottom).\nSometimes distributed algorithms have to make smart shortcuts because of the distributed nature of the problem.\nDespite some limitations, parallel processing can be done on a wide variety of problems. It just takes more thought on how the algorithm will be implemented. As the user of these algorithms, you should be aware of any shortcuts or nuances that make it different than a single-machine implementation.\nFor those who want to learn more about adapting machine learning algorithms to work in parallel, try looking at the textbook Mining of Massive Datasets.', 'Parallel Processing Computations\nIndividually, each processor works the same as any other microprocessor. The processors act on instructions written in assembly language. Based on these instructions, the processors perform mathematical operations on data pulled from computer memory. The processors can also move data to a different memory location.\nIn a sequential system, it\'s not a problem if data values change as a result of a processor operation. The processor can incorporate the new value into future processes and carry on. In a parallel system, changes in values can be problematic. If multiple processors are working from the same data but the data\'s values change over time, the conflicting values can cause the system to falter or crash. To prevent this, many parallel processing systems use some form of messaging between processors.\nProcessors rely on software to send and receive messages. The software allows a processor to communicate information to other processors. By exchanging messages, processors can adjust data values and stay in sync with one another. This is important because once all processors finish their tasks, the CPU must reassemble all the individual solutions into an overall solution for the original computational problem. Think of it like a puzzle -- if all the processors remain in sync, the pieces of the puzzle fit together seamlessly. If the processors aren\'t in sync, pieces of the puzzle might not fit together at all.\nThere are two major factors that can impact system performance: latency and bandwidth. Latency refers to the amount of time it takes for a processor to transmit results back to the system. It\'s not good if it takes the processor less time to run an algorithm than it does to transmit the resulting information back to the overall system. In such cases, a sequential computer system would be more appropriate. Bandwidth refers to how much data the processor can transmit in a specific amount of time. A good parallel processing system will have both low latency and high bandwidth.\nSometimes, parallel processing isn\'t faster than sequential computing. If it takes too long for the computer\'s CPU to reassemble all the individual parallel processor solutions, a sequential computer might be the better choice. As computer scientists refine parallel processing techniques and programmers write effective software, this might become less of an issue.\nTo learn more about parallel processing, follow the links below.\nRelated HowStuffWorks Articles\n- How Bits and Bytes Work\n- How Boolean Logic Works\n- How Computer Memory Works\n- How E-commerce Works\n- How Electronic Gates Work\n- How Encryption Works\n- How Hackers Work\n- How Home Networking Works\n- How Microprocessors Work\n- How Operating Systems Work\n- How PCs Work\n- How Quantum Computers Will Work\n- How Semiconductors Work\n- How Web Servers Work\nMore Great Links\n- Brown, Martin. ""Comparing traditional grids with high-performance computing."" IBM. Jun 13, 2006. http://mcslp.com/gridpdfs/gr-tradhp.pdf\n- Dietz, Hank. ""Linux Parallel Processing HOWTO."" Aggregate.org. Jan. 5, 1998. Retrieved March 29, 2008. http://aggregate.org/LDP/19980105/pphowto.html\n- ""Parallel processing."" SearchDataCenter. March 27, 2007. Retrieved March 29, 2008. http://searchdatacenter.techtarget.com/sDefinition/0,,sid80_gci212747,00.html\n- ""Programming Models."" Tommesani. Retrieved March 29, 2008. http://www.tommesani.com/ProgrammingModels.html.']"	['<urn:uuid:7e2f1818-2150-47bd-a218-0c1bac032b6d>', '<urn:uuid:29780df8-a30e-4244-86b3-db778ac5ca93>']	open-ended	direct	verbose-and-natural	similar-to-document	comparison	expert	2025-05-12T10:19:05.071190	26	99	1661
45	What are automated network checks good for, and how do they help with documentation?	AI/ML-driven forecasting produces intelligent and reliable reports automatically, detecting anomalies that happen below set thresholds and predicting when infrastructure upgrades will be needed based on current trends. Regarding documentation, automated network checks enable near real-time documentation of network changes, which is crucial for troubleshooting, network management tool device lists, inventory tracking, validation, and audits. This documentation helps maintain precise network information and track network inventory in relation to user requirements.	"['Optimizing Utilization Forecasting with Artificial Intelligence and Machine Learning\nContextual blindness is one major risk that IT teams dealing with huge volumes of data are either unaware of or overlook, assuming it to be “the nature of their work.” Most IT teams use multiple tools for network analytics, which can cause the data obtained from these tools to have many missing links due to different formats or names for the same metrics. Without contextual data, it’s difficult to model a set of IT infrastructure resources, examine their dependencies, understand the overall business impact, analyze based on priority, troubleshoot, and resolve issues that crop up.\nWhat IT team wouldn’t want a crystal ball that could predict the future of their organization’s IT infrastructure, letting them fix application and infrastructure performance problems before they arise?\nWell, the current shortage of crystal balls makes the union of artificial intelligence (AI), machine learning (ML), and utilization forecasting the next best thing to anticipate and avoid issues that threaten the overall health and performance of your IT infrastructure components.\nUtilization forecasting is a technique that applies ML algorithms to produce daily usage forecasts for all utilization across CPUs, physical and virtual servers, disks, storage, bandwidth, and other network elements, enabling networking teams to manage resources proactively. This technique helps IT engineers and network administrators prevent downtime caused by overutilization.\nAI/ML-Driven Forecasting for IT infrastructure\nAn AI/ML-driven forecasting solution produces intelligent and reliable reports, taking advantage of the current availability of ample historic records and high-performance computing algorithms.\nWithout AI and ML, utilization forecasting relies on reactive monitoring. You set predefined thresholds for given metrics such as uptime, resource utilization, and network bandwidth, as well as hardware metrics like fan speed and device temperature. When a threshold is exceeded, an alert is issued.\nHowever, this reactive approach will not detect the anomalies that happen below the set threshold, causing indirect issues that could be detrimental to your network’s health to go undetected. Moreover, traditional utilization forecasting can’t tell you when you will need to upgrade your infrastructure based on current trends.\nHow AI/ML-Driven Utilization Forecasts Are Better\nA closer look at predictive technologies reveals the fundamental difference between proactive and reactive forecasting.\nWithout AI and ML, utilization forecasting uses linear regression models to extrapolate and make predictions based on existing data. This method involves no consideration of newly allocated memory or anomalies in utilization patterns. Also, pattern recognition is a foreign concept. Although useful, linear regression models do not give IT admins complete visibility.\nUtilization forecasting driven by AI and ML, on the other hand, utilizes seasonal-trend decomposition using the Loess (STL) method. STL allows you to study the propagation and degradation of memory, as well as analyze pattern matching, whereby periodic configuration changes in a metric will automatically be adjusted. In simple terms, STL dramatically improves the accuracy of AI/ML-driven forecasting thanks to those dynamic, automated adjustments. Moreover, if any new memory is allocated, or if the memory size is increased or decreased for a device, the prediction will change accordingly; this is not possible with linear regression.\nTo forecast utilization proactively, you need accurate algorithms that can analyze usage patterns and detect anomalies in daily usage trends—without generating any false positives. Let’s examine a simple use case.\nWith AI/ML-driven utilization forecasting, you can find a minor increase in your office bandwidth usage during the World Series, the FIFA World Cup, or other sporting events. Even if you have a huge amount of unused internet bandwidth, this anomalous usage can be detected.\nLikewise, proactive utilization forecasting lets you know when to upgrade your infrastructure by factoring your business’ new recruitment and attrition rates. With AI aiding intent-based networking, the IT admin is the gamemaster as well as the umpire, well-equipped with an extra pair of artificially intelligent eyes, ready to spot violations. Fueled by AI-based forecasting techniques, an organization’s mean time to repair (MTTR) is also bound to reduce.\nData-Driven Decision-Making with AI/ML-Driven Utilization Forecasts\nIncorporating AI-based forecasting consequently means fewer bottlenecks and increased productivity for IT admins. While previously there was a need for constant, manual network monitoring, AI and ML can now be used to generate a forecast report so an IT admin has a clearer picture of the usage levels of various devices in the network. AI can provide predictions that play a pivotal role in an IT admin’s evaluation process, thus eliminating possible human error or biases. In other words, ML coupled with human intervention is possibly the best way to go about making all important, resource-dependent IT decisions.\nIT admins can get a helping hand from an AIOps-enabled network performance monitoring (NPM) tool that offers data-driven recommendations in terms of consumable data formats of only useful information. This results in less experience-based problem-solving and more data-influenced problem avoidance. An AIOps-enabled NPM solution can alert the IT admin in advance about possible performance degradation. AI/ML-based forecasting enables organizations to raise automated device procurement requests well in advance, plan their internet billing and IT infrastructure budget, know if they need to reduce their internet usage, and modify the allocation and reallocation of virtual memory as required. This essentially means you can roll out the solution to a networking issue before it even knocks at your data center’s door.\nBeyond forecasting, ML can also be used to improve anomaly detection. Here, adaptive thresholds for different metrics are established using ML and analysis of historic data, revealing anomalies and triggering appropriate alerts. Furthermore, application and infrastructure monitoring functions will also be improved when enhanced with AI and ML technologies.', 'Configuration management is a collection of processes and tools that\npromote network consistency, track network change, and provide up to date\nnetwork documentation and visibility. By building and maintaining configuration\nmanagement best-practices, you can expect several benefits such as improved\nnetwork availability and lower costs. These include:\nLower support costs due to a decrease in reactive support\nLower network costs due to device, circuit, and user tracking tools\nand processes that identify unused network components.\nImproved network availability due to a decrease in reactive support\ncosts and improved time to resolve problems.\nWe have seen the following issues resulting from a lack of\nInability to determine user impact from network changes\nIncreased reactive support issues and lower availability\nIncreased time to resolve problems\nHigher network costs due to unused network components\nThis best-practice document provides a process flowchart for\nimplementing a successful configuration management plan. We\'ll look at the\nfollowing steps in detail: create standards,\nmaintain documentation, and\nvalidate and audit standards.\nThe diagram below shows how you can use the critical success factors\nfollowed by performance indicators to implement a successful configuration\nCreating standards for network consistency helps reduce network\ncomplexity, the amount of unplanned downtime, and exposure to network impacting\nevents. We recommend the following standards for optimal network\nSoftware version control is the practice of deploying consistent\nsoftware versions on similar network devices. This improves the chance for\nvalidation and testing on the chosen software versions and greatly limits the\namount of software defects and interoperability issues found in the network.\nLimited software versions also reduce the risk of unexpected behavior with user\ninterfaces, command or management output, upgrade behavior and feature\nbehavior. This makes the environment less complex and easier to support.\nOverall, software version control improves network availability and helps lower\nreactive support costs.\nNote: Similar network devices are defined as standard network devices with\na common chassis providing a common service.\nImplement the following steps for software version control:\nDetermine device classifications based on chassis, stability, and new\nTarget individual software versions for similar\nTest, validate, and pilot chosen software versions.\nDocument successful versions as standard for similar-device\nConsistently deploy or upgrade all similar devices to standard\nIP address management is the process of allocating, recycling and\ndocumenting IP addresses and subnets in a network. IP addressing standards\ndefine subnet size, subnet assignment, network device assignments and dynamic\naddress assignments within a subnet range. Recommended IP address management\nstandards reduce the opportunity for overlapping or duplicate subnets,\nnon-summarization in the network, duplicate IP address device assignments,\nwasted IP address space, and unnecessary complexity.\nThe first step to successful IP address management is understanding the\nIP address blocks used in the network. In many cases, network organizations\nhave to rely on\naddress space, which isn\'t Internet addressable, but can be used to\naccess the network in conjunction with\nAddress Translation (NAT). Once you have defined the address blocks,\nallocate them to areas of the network in a way that promotes summarization. In\nmany cases, you\'ll have to further subdivide these blocks based on the number\nand size of subnets within the defined range. You should define standard subnet\nsizes for standard applications, such as building subnet sizes, WAN link subnet\nsizes, loopback subnet size, or WAN site subnet size. You can then allocate\nsubnets for new applications out of a subnet block within a larger summary\nFor example, let\'s take a large enterprise network with an east coast\ncampus, a west coast campus, a domestic WAN, a European WAN, and other major\ninternational sites. The organization allocates contiguous IP classless\ninterdomain routing (CIDR) blocks to each of these areas to promote IP\nsummarization. The organization then defines the subnet sizes within those\nblocks and allocates sub-sections of each block to a particular IP subnet size.\nEach major block or the entire IP address space can be documented in a\nspreadsheet showing allocated, used, and available subnets for each available\nsubnet size within the block.\nThe next step is to create standards for IP address assignments within\neach subnet range. Routers and Hot Standby Router Protocol (HSRP) virtual\naddresses within a subnet might be assigned the first available addresses\nwithin the range. Switches and gateways may be assigned the next available\naddresses, followed by other fixed address assignments, and finally dynamic\naddresses for DHCP. For example, all user subnets may be /24 subnets with 253\navailable address assignments. The routers may be assigned the .1 and .2\naddresses, and the HSRP address assigned the .3 address, switches .5 through\n.9, and the DHCP range from .10 through .253. Whatever standards you develop,\nthey should be documented and referenced on all network engineering plan\ndocuments to help ensure consistent deployment.\nConsistent, structured use of naming conventions and DNS for devices\nhelps you manage the network in the following ways:\nCreates a consistent access point to routers for all network\nmanagement information related to a device.\nReduces the opportunity for duplicate IP addresses.\nCreates simple identification of a device showing location, device\ntype, and purpose.\nImproves inventory management by providing a simpler method to\nidentify network devices.\nMost network devices have one to two interfaces for managing the\ndevice. These may be an in-band or out-of-band Ethernet interface and a console\ninterface. You should build naming conventions for these interfaces related to\nthe device type, location, and interface type. On routers, we strongly\nrecommend using the loopback interface as the primary management interface\nbecause it can be accessed from different interfaces. You should also configure\nloopback interfaces as the source IP address for traps, SNMP and syslog\nmessages. Individual interfaces can then have a naming convention that\nidentifies the device, location, purpose, and interface.\nWe also recommend identifying DHCP ranges and adding them to the DNS,\nincluding the location of the users. This may be a portion of the IP address or\na physical location. An example might be ""dhcp-bldg-c21-10"" to\n""dhcp-bldg-c21-253"", which identifies IP addresses in building C, second floor,\nwiring closet 1. You can also use the precise subnet for identification. Once a\nnaming convention has been created for devices and DHCP, you\'ll need tools to\ntrack and manage entries, such as\nStandard configuration applies to protocol and media configurations, as\nwell as global configuration commands. Descriptors are interface commands used\nto describe an interface.\nWe recommend creating standard configurations for each device\nclassification, such as router, LAN switch, WAN switch, or ATM switch. Each\nstandard configuration should contain the global, media, and protocol\nconfiguration commands necessary to maintain network consistency. Media\nconfiguration includes ATM, Frame Relay, or Fast Ethernet configuration.\nProtocol configuration includes standard IP routing protocol configuration\nparameters, common Quality of Service (QoS) configurations, common access\nlists, and other required protocol configurations. Global configuration\ncommands apply to all like devices and include parameters such as service\ncommands, IP commands, TACACS commands, vty configuration, banners, SNMP\nconfiguration, and Network Time Protocol (NTP) configuration.\nDescriptors are developed by creating a standard format that applies to\neach interface. The descriptor includes the purpose and location of the\ninterface, other devices or locations connected to the interface, and circuit\nidentifiers. Descriptors help your support organization better understand the\nscope of problems related to an interface and allows faster resolution of\nWe recommend keeping standard configuration parameters in a standard\nconfiguration file and downloading the file to each new device prior to\nprotocol and interface configuration. In addition, you should document the\nstandard configuration file, including an explanation of each global\nconfiguration parameter and why it is important.\nManager Essentials (RME) can be used to manage standard configuration\nfiles, protocol configuration, and descriptors.\nUpgrade procedures help ensure that software and hardware upgrades\noccur smoothly with minimal downtime. Upgrade procedures include vendor\nverification, vendor installation references such as release notes, upgrade\nmethodologies or steps, configuration guidelines, and testing requirements.\nUpgrade procedures may vary widely depending on network types, device\ntypes, or new software requirements. Individual router or switch upgrade\nrequirements may be developed and tested within an architecture group and\nreferenced in any change documentation. Other upgrades, involving entire\nnetworks, can not be tested as easily. These upgrades may require more in-depth\nplanning, vendor involvement, and additional steps to ensure success.\nYou should create or update upgrade procedures in conjunction with any\nnew software deployment or identified standard release. The procedures should\ndefine all steps for the upgrade, reference vendor documentation related to\nupdating the device, and provide testing procedures for validating the device\nafter the upgrade. Once upgrade procedures are defined and validated, the\nupgrade procedure should be referenced in all change documentation appropriate\nto the particular upgrade.\nYou can use solution templates to define standard modular network\nsolutions. A network module may be a wiring closet, a WAN field office, or an\naccess concentrator. In each case you need to define, test and document the\nsolution to help ensure that similar deployments can be carried out in exactly\nthe same way. This ensures that future changes occur at a much lower risk level\nto the organization since behavior of the solution is well defined.\nCreate solution templates for all higher-risk deployments and solutions\nthat will be deployed more than once. The solution template contains all\nstandard hardware, software, configuration, cabling, and installation\nrequirements for the network solution. Specific details of the solution\ntemplate are shown as follows:\nHardware and hardware modules including memory, flash, power, and\nLogical topology including port assignments, connectivity, speed, and\nSoftware versions including module or firmware\nAll non-standard, non device-specific configuration including routing\nprotocols, media configurations, VLAN configuration, access lists, security,\nswitching paths, spanning tree parameters, and others.\nOut-of-band management requirements.\nInstallation requirements including environmentals, power, and rack\nNote that the solution template does not contain many requirements.\nSpecific requirements such as IP addressing for the specific solution, naming,\nDNS assignments, DHCP assignments, PVC assignments, interface descriptors, and\nothers should be covered by overall configuration management practices. More\ngeneral requirements, such as standard configurations, change management plans,\ndocumentation update procedures, or network management update procedures,\nshould be covered by general configuration management practices.\nWe recommend documenting the network and changes that have occurred in\nthe network in near real-time. You can use this precise network information for\ntroubleshooting, network management tool device lists, inventory, validation,\nand audits. We recommend using the following network documentation critical\nCurrent device, link, and end-user inventory information enables you to\ntrack network inventory and resources, problem impact, and network change\nimpact. The ability to track network inventory and resources in relation to\nuser requirements helps ensure that managed network devices are actively used,\nprovides information needed for audits, and helps to manage device resources.\nEnd-user relationship data provides information to define change risk and\nimpact, as well as the ability to more quickly troubleshoot and resolve\nproblems. Device, link, and end-user inventory databases are typically\ndeveloped by many leading service provider organizations. The leading developer\nof network inventory software is\nThe database may contain tables for like devices, links, and customer\nuser/server data so that when a device is down or network changes occur, you\ncan easily understand the end-user impact.\nA configuration version control system maintains the current running\nconfigurations of all devices and a set number of previous running versions.\nThis information can be used for troubleshooting and configuration or change\naudits. When troubleshooting, you can compare the current running configuration\nto previous working versions to help understand if configuration is linked to\nthe problem in any way. We recommend maintaining three to five previous working\nversions of the configuration.\nTo identify who made configuration changes and when, you can use TACACS\nlogging and NTP. When these services are enabled on Cisco network devices, the\nuserid and timestamp is added to the configuration file at the time the\nconfiguration change is made. This stamp is then copied with the configuration\nfile to the configuration version control system. TACACS can then act as a\ndeterrent for unmanaged change and provide a mechanism to properly audit\nchanges that occur. TACACS is enabled using the Cisco Secure product. When the\nuser logs into the device, he/she must authenticate with the TACACS server by\nsupplying a userid and password. NTP is easily enabled on a network device by\npointing the device to an NTP master clock.\nTopology documentation aids in the understanding and support of the\nnetwork. You can use it to validate design guidelines and to better understand\nthe network for future design, change, or troubleshooting. Topology\ndocumentation should include both logical and physical documentation, including\nconnectivity, addressing, media types, devices, rack layouts, card assignments,\ncable routing, cable identification, termination points, power information, and\ncircuit identification information.\nMaintaining topology documentation is the key to successful\nconfiguration management. To create an environment where topology documentation\nmaintenance can occur, the importance of the documentation must be stressed and\nthe information must be available for updates. We strongly recommend updating\ntopology documentation whenever network change occurs.\nNetwork topology documentation is typically maintained using a graphics\napplication like Microsoft\n. Other products like\nsuperior capabilities for managing topology information.\nConfiguration management performance indicators provide a mechanism to\nvalidate and audit network configuration standards and critical success\nfactors. By implementing a process improvement program for configuration\nmanagement, you can use the performance indicators to identify consistency\nissues and improve overall configuration management.\nWe recommend creating a cross-functional team to measure configuration\nmanagement success and improve configuration management processes. The first\nobjective of the team is to implement configuration management performance\nindicators in order to identify configuration management issues. We\'ll discuss\nthe following configuration management performance indicators in detail:\nAfter evaluating the results from these audits, initiate a project to\nfix inconsistencies and then determine the initial cause of the problem.\nPotential causes include a lack of standards documentation or a lack of a\nconsistent process. You can improve standards documentation, implement\ntraining, or improve processes to prevent further configuration\nWe recommend monthly audits, or possibly quarterly if only validation\nis needed. Review past audits to confirm that past problems are resolved. Look\nfor overall improvements and goals to demonstrate progress and value. Create\nmetrics to show the quantity of high-risk, medium-risk, and low-risk network\nThe configuration integrity check should evaluate the overall\nconfiguration of the network, its complexity and consistency, and potential\nissues. For Cisco networks, we recommend using the\nconfiguration validation tool. This tool inputs all device configurations and\ncreates a configuration report that identifies current problems such as\nduplicate IP addresses, protocol mismatches, and inconsistency. The tool\nreports any connectivity or protocol issues, but does not input standard\nconfigurations for evaluation on each device. You can manually review\nconfiguration standards or create a script that reports standard configuration\nDevice, protocol, and media audits are a performance indicator for\nconsistency in software versions, hardware devices and modules, protocol and\nmedia, and naming conventions. The audits should first identify any\nnon-standard issues, which should lead to configuration updates to fix or\nimprove the issues. Evaluate overall processes to determine how they could\nprevent suboptimal or non-standard deployments from occurring.\nRME is a configuration management tool that can audit and report on\nhardware versions, modules and software versions. Cisco is also developing more\ncomprehensive media and protocol audits that will report inconsistency with IP,\nDLSW, Frame Relay and ATM. If a protocol or media audit is not developed, you\ncan use manual audits, such as reviewing devices, versions and configurations\nfor all like devices in a network, or by spot checking devices, versions and\nThis performance indicator reviews network and standards documentation\nto ensure that the information is accurate and up to date. The audit should\ninclude reviewing current documentation, recommending changes or additions, and\napproving new standards.\nYou should review the following documentation on a quarterly basis:\nstandard configuration definitions, solution templates including recommended\nhardware configurations, current standard software versions, upgrade procedures\nfor all devices and software versions, topology documentation, current\ntemplates, and IP address management.']"	['<urn:uuid:6ac4ca63-48b4-41af-8d98-f84add71d25e>', '<urn:uuid:1a51c970-0696-4b42-b79b-998a69416090>']	factoid	direct	concise-and-natural	distant-from-document	multi-aspect	novice	2025-05-12T10:19:05.071190	14	70	3566
46	efficiency drawbacks solar panels mounted sides electric recreational vehicle	Mounting solar panels on RV sides is inefficient: flat orientation on top loses 10-15% power output, side panels at 90 degrees angle lose 25-30% power output, and panels facing North lose at least 30% power output. It's more efficient to mount an optimum amount only on the top of an RV where there is reasonable sunshine.	['In total, for a standard Class C RV I would install 500 watts of solar panels with a lithium phosphate battery bank of 360Ah. This is the kind of power you need if serious about boondocking in your RV and you can upgrade from there.\nI was prompted to write this post after coming across an article describing the new Dethleff Electric RV.\nA solar electric RV seems to be the answer to a boon-docker’s prayer, and it’s advertised as an autonomous option, but it’s far from that.\nClass C RV with solar panels\nI’m a firm believer in reverse engineering existing good solutions to any technical problem and naturally took this approach when looking at the very common question of how to work out how many solar panels are needed for the average RV.\nVideo – How many solar panels do you need on your RV?\nIn a previous post I looked at charging EVs with solar in some detail.\nIt is viable with around 2000 watts of solar panels, say 7 x 250 watts, to charge an average EV such as a Nissan leaf, but this assumes an average American commute of 30 miles a day, not the full range or capacity of the battery.\nThe Dethleff Electric RV boasts a 150 miles range without fitting out with its camper trim, so it will be less than this.\nAlso, the 3000 kW of solar panels is not going to provide nearly as much power as you might think. Look at the panel orientation:\n- Top – in flat orientation solar panels lose 10 to 15% power output\n- One side could be considered on average to be facing the sun at 90 degrees angle. This is not an efficient angle and power output will be down between 25% to 30%\n- The opposite side can be considered to be facing North (the very worst orientation) and so output from this array will be down at least 30%\nIn short, covering an RV with solar panels is a bad idea and little more than a sales gimmick.\nIt is much more efficient to mount an optimum amount on the top of an RV where there is always a reasonable amount of sunshine.\nHow Many Solar Panels Do I Need To Power My RV?\nMore and more people are considering bugging-out and living off-grid in these difficult times when the future is pretty uncertain.\nSome of the bigger RVs are ideal for couple or small families and autonomy becomes key.\nWe need to be less dependent on outside sources of almost everything, including electricity.\nSolar PV systems and RVs go together hand in glove and it’s natural to wonder about the generation capacity of standard solar panels.\nThe standard approach is to gather all the energy requirements that you have or intend to have and work out the solar panel sizes you need.\nI’m going to begin at the other end, by reverse-engineering existing systems available in the marketplace.\nAre solar panels for an RV worth it?\nSolar kits for RV are available on platforms like Amazon range from 20 watts to 300, so it isn’t surprising an RV owner can get confused.\nExactly how much solar do you need to power an RV? The key word is ‘exactly’. You really don’t need exact figures.\nThey are difficult to get anyway and all solar installers I know work on estimates for sizing and pricing.\nHow many amp hours do I need for my RV?\nThe average RV uses about 600kWh per month, so why not use this figure instead of doing all that complicated calculating? I do it anyway because this is a DIY solar blog but it isn’t really necessary.\n(It isn’t a good idea to be using AC with solar in your RV while boondocking – it will really deplete those batteries!)\nLet’s use this figure of 600kWh/month and the average Ah battery capacity is 180 (2 x 90Ah) at 12 volt which converts to 2160 watt/hours (W/h).\nIf you work on this basis you can always adjust up if needed.\nHow many amp hours is Boondocking?\n2 days is a good amount of time to be free of all constraints so you will need enough solar panel capacity to keep those batteries topped up.\nWe need to assume some level of irradiance, which of course depends on your State.\nSolar designers use the STC solar panel rating (its output in watts at an irradiance of 1000 watts/m2) multiplied by 4 hours of sunshine.\n- A 100 watt solar panel will generate about 4oo watt/hrs per day = 12 kW/month\nTo completely cover all your needs you would need 50 x 100 watt panels! Clearly not an option, but of course the real situation isn’t like this.\nThe RV engine charges up the batteries while running (always an option) and naturally you cut down on energy usage.\nHow many hours would you like to be autonomous i.e. be boondocking?\nIf not running AC this figure comes down to 10 kW/day. Let’s look at the system again:\n- Energy usage 10kWh/day\n- 180 Ah Battery supplies 2.160 kW/h (if lithium phosphate used at 100% Depth of Discharge(DOD)\n- 100 watt panels supplies 0.4 kW/h\nHow many solar panels does my RV need?\nFor minimum requirements I would:\n- Increase battery Ah capacity to 360Ah (4320kWh)\n- Install 500 watts solar for 2kWh/day\nand adjust energy usage accordingly until you approximate 2 days boondocking.\nThe very best method for assessing your energy needs is to go RVing for a day or two and measure your energy consumption while you’re out. It easily done with a simply-connected DC energy meter.\nI don’t advise using any kind of battery capacity monitor based on voltage. These are only useful if the batteries have been standing for several hours and are relatively stable.\nMPPT Controller For RV Solar Panels\nWhatever size solar array you decide on for you RV, you absolutely must control the battery charging with an MPPT controller.\nThe are two main type of controller, PWM (Pulse Width Modulation) and MPPT (Maximum Power Point Tracking.)\nWithout going into the technical details, the MPPT type is up to 30% more efficient, meaning you’ll get 30% more power out of your solar array.\nBest RV battery for boondocking\nFor many years AGM or other types of deep-cycle batteries have been the main-stay of serious RV life but this is changing rapidly.\nLithium phosphate technology is changing the way we think about batteries. They are lighter and last much longer. The only drawback at the present time is the price, which can be more than twice as much.\nNumber charge/discharge cycles\n2000 (100% DOD)\n500 (80% DOD)\n60% less than Pb\nInherently deep cycle\nDiscover your solar saving potential\nNew RVs Class C size come with a 120Ah lithium phosphate battery, which is a good basis to begin with.\nA good example of the kind of package you’re looking for is the Scandinavian Affinity Camper Van with 360 watts solar panels and a 120Ah battery – you can always add some!\nAre flexible solar panels any good for RF roof mounting?\nI’ve often seen it recommended but I wouldn’t do it myself. After all, they are made for fixing on uneven surfaces while RV tops generally have large enough flat areas to mount rigid solar panels.\nRigid is always strongest, although 4 to 5 times heavier, which isn’t really an issue with a camper van.\nBuy individual components, rather than a kit. Kits just come more expensive IMO. Panels are cheap nowadays and a good MPPT controller isn’t going to break the bank.\nThe biggest expense is going to be the LiFePO4 batteries, and they are for life. Go RVing!\nRenogy Solar RV Resources\nCan a wind turbine run an RV?\nFor sure, wind turbines can be a great option for RV life and boondocking. There a few things to keep in mind:\n- Constant wind is best – think plains or coastal areas\n- When wind speed doubles power output multiplies by 8\n- Wind power is strong! Ensure fastenings are to manufacturer’s specifications\n- Between 250 and 400 watts is a good size to begin with\n- Don’t run a wind turbine while driving\n- Wind power plus solar is the perfect combination – when one is low, the other is high\n- About half the cost of solar panels\n- Can be noisy – may need to be disabled in high winds\n- Protect against lightning\nWeb Story – Can A Wind Turbine Power An RV?\nAre solar panels for an RV worth it?\nSolar panels have come down in cost considerably in the last few year and they are considered a great addition to an RV. 2oo to 300 watts of solar panels with 180Ah of lithium phosphate will go a long way towards extending your autonomy.\nHow much does it cost to install solar panels on an RV?\nA kit consisting of 200 watts of solar panels and a basic PWM solar charge controller, including all the wiring shouldn’t cost more than $300, assuming DIY mounting and cabling.\nWhat type of solar panel is best for RV?\nThe standard rigid monocrystalline type is suitable for all RV installations. Flexible models are not easily fixed and don’t last as long.']	['<urn:uuid:81de55e6-f2ce-4627-9a0e-21d8fa8d6546>']	factoid	direct	long-search-query	distant-from-document	single-doc	expert	2025-05-12T10:19:05.071190	9	56	1559
47	How do doctors handle heart screening for endurance athletes who want to start training, and what rules do they need to follow to protect patient information during the process?	Doctors approach heart screening for athletes on a case-by-case basis, considering factors like age, medical history, and risk factors. They may recommend various tests including ECGs, stress tests, or echocardiograms depending on the individual's circumstances. All this medical information must be handled according to HIPAA requirements, which mandate proper safeguards for access control, audit controls, and person authentication to protect patient privacy.	['It’s the time of year when many endurance athletes are considering their medical and health preparations for the upcoming season, whatever their sport. One important consideration is pre-participation screening for heart problems. I get inquiries like:\nShould I see the doctor before training?\nI’m concerned about the heart risks. Should I get my heart checked out?\nDo I need an EKG? An echocardioam?\nThese are all good questions. But the answers really do depend upon an athlete’s circumstance. There’s no one-size-fits-all prescription here.\nI thought I’d gather a group of experts to give you their opinions about 4 fictitious athlete scenarios. You’ll get to see how these experts approach the athletes’ situation and how they’d handle the issue of pre-participation screening for each. None of these fictitious athletes will match your own situation, but you can learn from these scenarios so that you’re better able to have a conversation with your doctor about how to make plans that are suited for your circumstances.\nJohn M. Mandrola, MD, is a cardiologist in Louisville, Kentucky who specializes in electrophysiology, the field of cardiology that focuses on the prevention, diagnosis, and treatment of arrhythmias. He is an avid cyclist and occasional triathlete. Follow Dr. Mandrola on Twitter at @drjohnm and read his blog about medicine and cyling at www.drjohnm.org.\nJonathan A. Drezner, MD, is a member of the Family Medicine faculty at the University of Washington, with a clinical focus in sports medicine. He also serves as Team Physician for the University’s Department of Intercollegiate Athletics. Dr. Drezner has an international reputation in the areas of automatic external defibrillators (AEDs) in sports settings; emergency preparedness for sports-related sudden cardiac death; and pre-participation screening in young competitive athletes. Dr. Drezner is the current President of the American Medical Society for Sports Medicine. He is also active with charitable organizations that promote awareness for heart conditions that affect young athletes, including Parent Heart Watch, the Sudden Cardiac Arrest Association, and Heart Screen America.\nJames G. Beckerman, MD, is a cardiologist in Portland, Oregon who focuses on preventive and sports cardiology. He serves as Team Cardiologist for the Portland Timbers soccer team and as Medical Director for the Play Smart Youth Heart Screenings Program. Follow Dr. Beckerman on Twitter at @jamesbeckerman.\nPhilip F. Skiba, DO, is the Program Director for Sports Medicine at Lutheran General Hospital, in Chicagoland and Acting Program Director for Sport and Exercise Medicine at The University of Exeter, UK. He is the CEO of PhysFarm Training Systems and has coached a number of world-class triathletes.\nI asked each of the experts to comment on the 4 athlete scenarios, with the following instructions:\nWhat would you recommend for the athlete if he/she contacted you about pre-participation health screening–if YOU were going to be the athlete’s doctor? Please discuss your approach in light of the history provided by the patient in a short telephone conversation. Assume that the athlete has typical employer-based health insurance, with $1000 annual deductible, and no special wellness benefits. Would you schedule an office visit for history and physical examination? If so, what would be your focus? Would you plan any diagnostic testing–chest x-ray, blood tests, EKG, echocardiogram, stress test, etc.? If the answer is “maybe,” discuss how you (and the athlete) would decide.\nThe experts’ responses are listed with each scenario.\n24-year-old woman wants to join her girlfriends in training for her first 5k run. She is slightly overweight, but sees this as an opportunity to lose weight. She has no chronic medical problems and has no family history of heart disease. She sees a primary care physician annually and reports that her last physical examination was 8 months ago. She takes an oral contraceptive.\nDr. Mandrola: I would do nothing.\nDr. Drezner: I would encourage her to start her training program, and offer a more comprehensive heart screen that would include an ECG. The risk of sudden cardiac death in young athletes is lower in females than males, but I would still offer a screen. Most young athletes with a pathologic heart condition have no symptoms. An ECG helps to identify silent diseases that place athletes at risk of sudden cardiac death.\nDr. Beckerman: No diagnostic testing. I would counsel her that it is possible that she will not experience significant weight loss from the 5k training, but would emphasize the health benefits–short and long term.\nDr. Skiba: If I were the primary care physiciansh e sees annually, I would clear her for participation, with the caveat that she should immediately contact me for any strange symptoms, and I would carefully delineate signes or symptoms that should trigger concern. If I were not her primary care physician, I would not clear her without the pre-participation history and physical exam I provide to all athletes.\n35-year-old man is a former high school track athlete, is currently a recreational runner, and now wants to begin structured training for his first-ever marathon. He is a former smoker, having quit at age 28 after smoking 1 pack per day for 8 years. His last complete physical examination was during high school. He injured his ankle a year ago, prompting an ER visit where he learned it was sprained. At that visit, his blood pressure was high. About 8 months ago, along with his wife, he visited a health fair where he had his cholesterol tested. The cholesterol and LDL were elevated. His mother has high blood pressure. He currently takes a fish oil supplement.\nDr. Mandrola: I would recommend a complete physical exam, including an ECG and stress ECG. Even if they were normal, I would advise him to begin training very slowly and consider delaying his marathon a year or two. This will allow him to embark on such a lofty goal when he is fitter and likely with a better blood pressure. This last caveat comes from personal experience. It took me 3 years to successfully run a marathon, as each attempt was felled by an over-training related injury.\nDr. Drezner: This patient has multiple risk factors for atherosclerotic coronary heart disease–the usual culprit in adults over 30 years old that causes heart attacks or sudden cardiac arrest. I would strongly encourage this patient to come in for an evaluation, counseling and treatment of his high blood pressure and high cholesterol, and discussion of an exercise stress test. An exercise stress test is the best way to look for coronary artery disease and recommended for adults with cardiovascular risk factors who are starting a new or more intensive training program.\nDr. Beckerman: If he has normal blood pressure today, I would not recommend any diagnostic testing. I would recommend some dietary changes and would recheck his lipids in 3 months. Warrants a full cardiac exam to focus on blood pressure, murmurs, abdominal bruit if BP elevated.\nDr. Skiba: I would provide the patient with a preparticipation history and physical exam. As exercise is effective treatment for both hypertension and hypercholesterolemia, my advice would depend upon the level of elevationand, whether he required urgent treatment or whether a trial of exercise would be appropriate.\n44-year old woman is currently a competitive masters swimmer who wants to take up triathlon in the upcoming season. She has no chronic medical problems. There is a remote history of goiter for which she had subtotal thyroidectomy. She has two healthy children. Her mother and sister have high blood pressure. She takes a daily multivitamin with iron. She reports that she “blacked out” 3 years ago at the conclusion of a swim workout, but had no specific medical evaluation or treatment. She occasionally has bothersome palpitations. Her last complete physical examination was 3 years ago.\nDr. Mandrola: I would recommend a history and physical with an ECG. If the ECG and exam were completely normal, I would stop. If the QT was borderline or the palpitations struck me as ominous, I would likely add a Holter monitor and treadmill. The Holter to look for AF [atrial fibrillation] or other arrhythmia and the treadmill to look for exercise induced arrhythmia or abnormal QT response.\nDr. Drezner: This patient needs an evaluation to check her blood pressure, ECG and stress test, and perhaps a 24 hour Holter monitor. Passing out is not normal. Although it is usually benign when it occurs after exercise (not during), everybody who passes out should have at least one ECG. Her age suggests a stress test is also indicated for exercise related passing out. She also has palpitations which may indicate the presence of an arrhythmia. Extended ECG monitoring (ie, 24 hours) can sometimes capture an abnormal heart rhythm if present.\nDr. Beckerman: I would check an ECG. If palpitations are nonexertional, I would consider a Holter monitor or event monitor based on frequency. If palpitations exertional, consider exercise treadmill test. Check TSH. Warrants a full cardiac exam with a focus on murmurs.\nDr. Skiba: This is an athlete who deserves a thorough workup. Syncope during or at the end of a swim workout could be the result of shallow water blackout (depending on what she was trying to do during that last bit of swimming), or could be the sign of something more sinister (i.e., an arrhythmia). The chances that an isolated syncopal episode 3 years ago were a sign of some cardiac issue that has not presented itself again is admittedly somewhat remote, however in the setting of continued palpitations I am more suspicious. She should at least get at EKG, with possible addition of an echocardiogram and/or Holter monitoring depending on where the workup goes.\n56-year-old man, currently a triathlete, wants to begin structured training in preparation for his first-ever Ironman triathlon. He is particularly concerned about his heart risk because a friend, and fellow triathlete, recently died after a heart attack. He is a former smoker, having quit at age 38 after smoking 1 pack per day for 15 years. He currently takes an ACE inhibitor for high blood pressure and a daily aspirin. His father died at age 52 of a heart attack. Although he visits annually with his primary care provider, his last complete physical examination was 4 years ago. He feels that he is about 20 pounds overweight.\nDr. Mandrola: I would do a history and physical, blood sugar, lipid panel, ECG and stress test. If these were normal, I would stop. I can’t believe I’m saying this, but given his medium risk, a coronary CT scan might help here. In the presence of coronary calcium, I would recommend statin therapy and strongly explain the risk of doing long-distance marathon/triathlons.\nDr. Drezner: This patient has multiple risk factors for coronary heart disease, including his past history of smoking, high blood pressure, and his father’s history. A family history of early heart attacks should be taken seriously, as genetics often dictate one’s risk and the timing of potential events. Combined with his other risk factors, his family history places him at high risk for heart disease. Before he enters a vigorous exercise program, I would want to screen for coronary artery disease. A CT scan for coronary calcium scoring can detect calcium deposits in atherosclerotic plaques and provide some assessment of risk. A stress test, or in this case a stress echocardiogram, will show if there is an obstructive lesion that limits blood flow to the heart during exercise.\nDr. Beckerman: Full physical exam. Check ECG. If normal, exercise treadmill test. If abnormal, stress echocardiogram. Check Chem 7 [serum electrolytes] and CBC [complete blood count]. Recommend screening colonoscopy.\nDr. Skiba: This is another patient who requires more careful monitoring. In addition to a pre-participation history and physical exam, I would be concerned with how well the patient’s blood pressure was controlled, as well as how his lipid panel looked. By strict interpretation of guidelines, he may not require testing because e is seemingly not symptomatic. However, in roughly half of people the first sign of cardiac pathology is death. I would therefore very carefully question him with respect to symptoms he might be ignoring or “brushing off.” Given that he is hypertensive, he has a long smoking history, and a strong family history, my inclination would be to stress test him, especially if he is not (or has not) been training vigorously.', 'RESOURCES - ARTICLE LIBRARY\nStaying HIPAA Compliant with Online Data Storage\nKeeping patient records secure and private is the concern of every hospital and health care provider, but they are often overwhelmed with years and years of patient information and the lack of adequate storage space. Destroying these health records in order to make room for more storage is often not an option. Patients want access to all of their health care records, and physicians need them in order to better diagnose patients. Online data storage is a way to satisfy all of these issues. Using online storage for these records allows easier access for patients, and offers easier sharing of patient information from hospital to physician, as well as from physician to physician. Storing health records online isn’t, however, without security concerns. Patients, hospitals, and physicians want assurance that these confidential records will remain safe, private, and secure, and will only be accessed by those authorized to do so.\nWhat is HIPAA?\nHIPAA or the Health Insurance Portability and Accountability Act of 1996 was created in order to protect health information and give patients certain rights regarding their private health information. It also allows for disclosure of health information necessary for patient care. This act specifies safeguards necessary for administrative, and physical and technical handling of patient health information.\nAccording to the U.S. Department of Health and Human Services (HHS.gov) HIPAA has many requirements and restrictions. It requires safeguards for:\n- Access Control\n- Audit Controls\n- Person or Entity Authentication\nAccess control is defined in the HIPAA Privacy Rule as “the ability or the means necessary to read, write, modify, or communicate data/information or otherwise use any system resource.” It should allow authorized users to only access the minimum amount of information necessary to complete job functions. The Access Control specification also requires the implementation of an exclusive user identification or user ID, and immediate access in case of an emergency.\nWhat Type of Security is Necessary?\nWhen dealing with patient records in an office, maintaining privacy and security usually involves storing patient files in locked cabinets where the files can be physically secured and visibly monitored at all times. When you are storing patient information online, certain precautions must be met in order to maintain the same security and privacy guaranteed each patient.\nWhile HIPAA permits patient records to be transmitted over the Internet, businesses will want a service that offers file encryption, authentication and password protection in order to secure the information. Although HIPAA does not require online data storage services to have encryption, it does require that patient information be adequately protected and accessible only to authorized persons. Encryption is the best way to protect that information and ensure authorized access to those records. It is also important to offer backup services in case of a virus attack, flood, or fire. Finally, the service must offer a method of tracking any security breach, as well as the ability to lock out former employees after they have left or been terminated.\nWhen storing patient information, it is important to stay HIPAA compliant, as the fines for not doing so are expensive. While online storage for health care businesses guarantee less worry, work, and expense for health care providers, the service is only as good as the security offered. Remaining HIPAA compliant is vital in order to continue a good business relationship with the health care industry.\nContent by Managed Services Provider University']	['<urn:uuid:9391e043-9fde-4812-a875-e4d77b79f5fc>', '<urn:uuid:53c1e085-512d-454f-869c-1a294b08602e>']	factoid	direct	verbose-and-natural	distant-from-document	multi-aspect	novice	2025-05-12T10:19:05.071190	29	62	2621
48	As a drinking water quality specialist, I'm curious about how the lead safety standards compare between US and Canadian regulations - what is the difference between their maximum allowable lead levels?	The EPA in the US has set a Maximum Contaminant Level (MCL) of 15 parts per billion (ppb) for lead in drinking water, while Canada has a more stringent standard with a maximum acceptable concentration of 5ug/L (equivalent to 5 ppb) for lead in drinking water.	['The City of Flint, Michigan, and the levels of lead in their public water supply has been talked about in the news a lot lately. The EPA states that “lead can affect almost every organ and system in your body. Children six years old and younger are most susceptible to the effects of lead.” The governor of Michigan has declared a State of Emergency in Genesee County, home of Flint, “due to the introduction of lead into the public water system and subsequently elevated blood lead levels exceeding governmental standards.” However, there has been a lot of misinformation and/or questions that have been raised as conversations have been started nationwide about the safety of our public drinking water, especially concerning lead. Below, we’re going to answer some of the general questions that have been brought up in discussion.\nWhat happened to the water in Flint, Michigan?\nThe City of Flint, Michigan historically purchased treated water from the Detroit Water and Sewerage Department (DWSD). This water was sourced from Lake Huron before being processed, treated, and piped into the City of Flint. In April 2014, Flint switched their water source to the Flint River. This water had a lower pH and an increased salinity, which caused the protective oxide and phosphate coatings on older pipes and solder to degrade, exposing lead surfaces. Complete treatment of the water from the Flint River, including the addition of anti-corrosion agents was not conducted. Flint switched back to the DWSD water in October 2015, following complaints from many residents about taste, color, and odor. However, the damage to the pipes was already done, and the damaged pipes continued to leach lead into the water, in excess of the EPA’s designated Maximum Contaminant Level (MCL) for lead in potable water. Flint is currently adding increased levels of orthophosphate to the water in an attempt to reduce the amount of exposed lead in the pipes.\nHow much lead is allowed to be in potable water?\nThe EPA has set a Maximum Contaminant Level (MCL) of 15 parts per billion (ppb) for lead in drinking water (potable water).\nWhat other contaminants are tested for in my drinking water?\nThe EPA requires that many potential contaminants are tested for, and sets Maximum Contaminant Levels for both Primary and Secondary Drinking Water Standards. Some of the most commonly detected contaminants are Total Coliform Bacteria, Fecal Coliform Bacteria, Fluoride, Arsenic, Nitrate, Sodium, Copper, Barium, Haloacetic Acids, Total Trihalomethanes, Chlorine, and Radon.\nWhere does lead in drinking water come from?\nWhile there can be areas with sources of water that are naturally higher in lead than others, most lead in drinking water comes from the corrosion of old pipes, piping fixtures, or from the solder connecting pipes.\nHow can I find out if there’s lead in my drinking water?\nIf your drinking water comes from a public water supply, contact your local water authority. They are required to test the water for many contaminants, one of which is lead, on a regular basis. Most public water suppliers post an annual Water Quality Report, which is likely available at their website, which will detail whether or not they exceeded any Maximum Contaminant Levels in the past year.\nIf your drinking water comes from a private well, you’ll have to get it analyzed by a National Environmental Laboratory Accreditation Program (NELAP) accredited laboratory.\nFor more information, check out www.cdc.gov/nceh/lead/tips/water.htm or www.epa.gov/lead/protect-your-family or give us a call at 1-800-627-1806. We’ll be happy to answer any questions that you might have concerning safe drinking water.', 'Lead in Drinking Water\nMain Page Content\nWhat is lead?\nLead is a toxic metal that is found naturally in the air, soil, and water. It is also used in mining, manufacturing, and burning of fossil fuels. The amount of lead Canadians are exposed to has decreased since the 1970s, mainly due to the removal of lead in gasoline, paint, and solder in food cans (Government of Canada, 2013). Although lead water pipes were banned in the National Plumbing Code of Canada in 1975, lead solder (joins metal surfaces together) was still allowed until the late 1980’s (Health Canada, 2016).\nWho is at risk?\nAlthough people of all ages are at risk of lead poisoning, it is most serious for infants, young children and pregnant women. Children absorb lead more easily than adults and are more susceptible to its harmful effects.\nWhat are the health effects from lead poisoning?\nThere is no level of lead exposure that is safe. Even small amounts of lead can be harmful.\nSigns and symptoms of lead poisoning include:\n- General Weakness\n- Poor Attention Span\n- Learning Difficulty\n- Abdominal Pain\n- Weight Loss\n- Vomiting (Health Canada, 2009)\nWith exposure to low levels of lead, symptoms are often not obvious, but can still be a danger to your health. Talk to your health care provider if you are worried about lead exposure. A simple blood test can find out your lead levels.\nEffects of lead exposure are different for different age groups\n|Age Group||Effects of Lead Exposure|\n|Infants and Children||Permanent and irreversible effects on brain development (behavioural changes, reduced intelligence, decreased attention span, decreased learning abilities) (WHO, 2016)|\n|Pregnant Women||Miscarriage, premature birth, low birth weight, baby deformities (WHO, 2016)|\n|Adults||Kidney problems, neurological effects, increased blood pressure (Health Canada, 2016)|\nDid you know?\nThe maximum acceptable concentration for lead in drinking water is 5ug/L (Government of Canada, 2019). If your water supply is higher than 5ug/L and you have kids or are pregnant, you should use a different source, such as bottled water.\nIs there lead in my drinking water?\nThe amount of lead in natural water supplies in Canada is very low. However, lead can enter the water supply from lead pipes, lead-containing brass fittings, or faucets. Homes built before 1975 often have lead pipes and those built until 1990 may have lead containing fittings.\nLead test kits that are available from stores for testing drinking water, are generally not correct or reliable. You can contact your municipal water distribution office if you would like your water tested. Contact information will be on your water bill, or visit:\nHow do I know if I have lead in my pipes?\nCheck with your municipality to find out if there are lead pipes in your area. However, even if the municipal pipes do not have lead, the service lines connecting to your home, and plumbing within the home may have lead and are the responsibility of the homeowner. Contact a plumber to help you find out if your pipes have lead.\nHow can I lower my risk?\n*Boiling water will not remove the lead.*\n- The longer the water sits in your pipes, the higher the lead levels in your tap water. Hot or warm water usually has higher levels of lead. Use only cold water for drinking, cooking, and making breast milk substitutes (formula).\n- If water has been sitting in the pipes for a few hours, be sure to run cold water for at least 5 minutes before using. Or, flush the toilet, shower, or do laundry to clear the standing water from your pipes, then run cold water for 1 to 2 minutes before using.\nBottled water is not always lead-free. Check the label to make sure the lead ‘Pb’ value is zero.\nIn-home water treatment systems\nCarbon-based filters can be very helpful in removing lead from water. These should be NSF International Certified to remove lead and must be installed and maintained as per the manufacturer’s instructions (Health Canada, 2016). Home owners can contact the manufacturer or a professional for details.\nRemoving the lead lines\nRemoving the lead-based water lines that feed your home with water is a permanent solution to lowering your exposure. Contact your municipality and a professional to find out more or read Information for Property Owners about Lead Water Service Line Replacement from the Ministry of Ontario.\nOther Helpful Tips:\n- Do not use ceramic cookware from foreign countries to heat water, serve or store food unless you’re sure that they are lead-free.\n- Do not store drinks in lead crystal containers. Babies, children, and pregnant women should not drink from crystal glasses.\n- Some hobby activities like furniture refinishing, model building and working with metals or stained glass can be sources of lead. Wear a mask, wash hands after and keep kids and pregnant women away.\n- Do not use outdoor paint indoors.\nWhat is the Windsor-Essex County Health Unit doing?\nThe Windsor-Essex County Health Unit is working with local municipalities and utility companies to address the possibility of lead in our water supply.\nSystematic testing of water in homes is carried out by the municipalities and utility companies. When results are above the provincial standard of 10ug/L, flushing of your pipes will be recommended by the utility company. If results indicate lead levels over 30ug/L, the Health Unit will be notified and we will contact the homeowner for further follow-up. Educational resources and Public Health Inspectors are also available to answer questions Monday to Friday 8:30 am to 4:30 pm.\nFor more information, please call us at 519-258-2146 ext. 4475.\nGovernment of Canada. (2013). Lead fact sheet. Retrieved from http://chemicalsubstanceschimiques.gc.ca/fact-fait/lead-plomb-eng.php\nHealth Canada (2019). Guidelines for Canadian Drinking Water Quality. Guideline Technical Document Lead.\nHealth Canada. (2016). Water talk: Reducing your exposure to lead from drinking water.\nHealth Canada. (2009). Lead information package.\nWorld Health Organization. (2016). Lead poisoning and health fact sheet.']	['<urn:uuid:846e3de2-e804-46f6-9c4d-4d5c34cf91bd>', '<urn:uuid:3c22c276-f1ad-4d7b-8535-c28253aefa0d>']	factoid	with-premise	verbose-and-natural	distant-from-document	comparison	expert	2025-05-12T10:19:05.071190	31	46	1576
49	As an embassy designer, how do the British and Australian embassies in Jakarta approach natural lighting differently?	The British Embassy emphasizes natural light through large windows surrounding the residence's sitting and dining areas, connected to a terrace. The Australian Embassy incorporates natural light through a central courtyard covered with a transparent roof, allowing daylight to enter surrounding blocks through irregularly arranged windows.	"['Photo by Bagus Tri Laksono\nSituated inside the embassy compound in the vicinity of Patra Kuningan, South Jakarta, on 1.5 hectares, the complex was completed in 2013 and officially opened by Prince Andrew during his last visit to Jakarta. With a size twice as large as its predecessor on Jl. MH. Thamrin, the new embassy is 3800 sqm and includes the ambassador’s residence, recreational facilities, and beautiful landscape design.\nThe renowned US-based architectural and engineering firm HOK was appointed to design the sustainable embassy compound in all its aspects. Inspired by a spiral, the design folds around a central courtyard and twists upwards and outwards to form the embassy’s public entrance and give a tranquil ambiance. HOK, which is known for designing everything from skyscrapers to airports, used locally sourced black granite and metal cladding for the cantilevered canopy. From afar, the embassy appears as if it’s been carved from a single piece of black stone.\nThe way to the residence is graced by a series of beautiful Javanese wood carvings that adorn the hallway to the entrance. Situated above the embassy office, the residence is done using a modern urban approach that gives respect to space and natural light. Taking the form of a one-floor penthouse-style apartment, the residence has a large formal sitting area juxtaposed against a dining room and is surrounded by big windows and is connected to a terrace.\nWhen we talked to Rachel Malik, she said us that the embassy and residence had been built and designed prior to their arrival. “A British design team called FCO Services came to Jakarta and worked on furnishing this residence. They chose the sofas, dining table, colour scheme, and furniture throughout the apartment. We saw this modern-style apartment upon our arrival and decided that we wanted to inject a more eclectic British twist combining old antique furniture with contemporary designs and a touch of Indonesia. This also allowed us to incorporate some of the furniture from the previous residence in Menteng.”\nThe design character reflects the personality of the ambassador and his wife. It has a vibe of London sophistication and the warmth of the countryside while blending in perfect harmony through the way they did the interior. This shown in their private quarters, where mixed furniture, from a Philippe Stark Louis Ghost chair and Edwardian armchair circa 1904, sit with a Paddington Bear, as well as with Emma Bridgewater and quirky Orla Kiely ceramics.\nThe private quarters are divided into a living room and five bedrooms, including a master bedroom that has access to the terrace. Lead by decor items that echo the life journey of the couple, I found a combination of oriental bric-a-brac sitting with their British counterparts. The highlight here includes Rachel’s private basket collection, with items from around the world.\n“I bought my first basket as a teenager in Botswana in Southern Africa in 1984, and that same basket has moved with me to every home I have ever lived in. Of course, it is with us in Jakarta. I love baskets. They can be large, small, tightly woven, loosely woven. There’s something about the texture, colours and weave that I just love. We hiked up to Wae Rebo in Flores last April. The women there made incredibly beautiful baskets to collect their coffee beans from the trees. One woman kindly agreed to sell me her ‘working basket’ as she was in the process of making a new one. This is perhaps now my most favorite basket, as it reflects the reality of a woman’s life in rural Flores.”\nThis is without a doubt, not just an ambassadorial residence, but a home that showcases great eclectic British style.', 'Denton Corker Marshall completes ""expressive yet dignified"" Australian Embassy in Jakarta\nDenton Corker Marshall has built the new Australian Embassy in Jakarta, adding an assortment of metals mined in Australia and acoustic panels decorated with national landmarks (+ slideshow).\nThe Australian firm aimed to create an ""expressive yet dignified"" representation of the nation\'s character for the complex, which is Australia\'s largest diplomatic post.\nTo achieve this, the firm chose to use diverse shapes and materials to clearly express the different functions of each structure.\n""The architectural design of the new Australian Embassy compound offers a multiplicity of expressions, drawing together into a unified and cohesive whole to represent the cultural diversity of Australia,"" said Denton Corker Marshall.\nThe embassy is located close to the centre of Indonesian capital. It comprises a chancery building, a residence for the Head of Mission, 32 staff accommodation units, and medical and recreational facilities.\nThe largest and most important building is the chancery, which contains the offices of the Department of Foreign Affairs and Trade, as well as 13 other government departments and agencies.\nThis building\'s form consists of 12 interconnected rectilinear volumes. Together, they create a solid mass intended to evoke well-known Australian landforms such as Uluru – also known as Ayers Rock – and Kata Tjuta.\nEach volume is clad in a metal mined in Australia to reflect the country\'s wealth of natural resources.\nThe claddings include zinc, aluminium, copper, brass and steel, and the panels feature debossed patterns that add a layer of textural interest to the otherwise homogenous surfaces.\n""The form of the chancery is uncomplicated, direct but at the same time powerful and memorable,"" said the architects.\n""It is unequivocal and confident. It doesn\'t look superficially \'Australia\' but relies on a more subtle reading of the Australian character.""\nInternally, the forms and materials used aim to complement the exterior. A central courtyard covered with a transparent roof allows daylight to enter the surrounding blocks through irregularly arranged windows.\nPanels of Australian wood add a sense of warmth to the material palette, and also feature perforated patterns that aid acoustics.\nThese depict abstract scenes of iconic Australian landmarks, including the Bungle Bungle landforms and the Twelve Apostles rock stacks.\nPositioned next to the chancery, the Head of Mission residence is a two-storey building formed of interlocking blocks, which create staggered facades defined by varying patterns of light and shade.\nThe residence\'s entry forecourt features a large reflecting pool at its centre.\nA minimal pergola that will gradually be covered by a canopy of creepers follows the paths that extend along the edges of the pool.\nThe forecourt and pool are intended to provide a relaxed yet formal welcome to the property, which also has a swimming pool and outdoor terrace on the first floor.\nBeyond the Head of Mission residence, the staff accommodation is arranged in two rows with multicoloured facades that step in and out to add articulation to the frontages and give each house its own identity.\nA garden area between the rows of accommodation provides a secure outdoor space for residents that is enclosed at either end by the walls of the Head of Mission residence and a recreation centre.\nDenton Corker Marshall has an office in Jakarta, as well as Melbourne, London and Manchester.\nThe firm is also responsible for designing an austere black pavilion to house Australia\'s exhibitions at the Venice Biennales, and a visitor centre for the prehistoric site of Stonehenge in England featuring a roof supported by more than 200 steel columns.\nPhotography is by John Gollings.\nArchitect: Denton Corker Marshall\nBuilding surveyor: PLP\nQuantity surveyor: WT Partnership']"	['<urn:uuid:a98fe542-0b00-4f7c-a2df-67756a1dff97>', '<urn:uuid:41cd4697-e913-48be-b1af-4669f643f0e1>']	factoid	with-premise	concise-and-natural	distant-from-document	multi-aspect	expert	2025-05-12T10:19:05.071190	17	45	1219
50	What type of display does the Energica motorcycle dashboard have?	"The Energica motorcycle has a 4.3\"" dashboard with a display featuring 16.7 million active matrix colours that provides excellent visibility."	['In Modena, homeland of supercars and motor valley of Italy, the first high-performance full electric motorcycles ‘Ego’ was born in Energica Motor Company S.p.A. Ego was rigorously developed using F1 technologies and the Windform® family of high performance composite materials. Energica was created and engineered through the CRP Group, whose decades-long activity in the field of High Precision CNC machining and professional 3D printing, along with Windform® composite materials for Additive Manufacturing, allowed the creation of innovative and avant-garde solutions that have made Energica a unique model throughout the world.\nThe use of Laser Sintering technology and Windform® composite materials enabled Energica to be on the market quickly, accelerating the prototyping and product development phase.\nThe Energica project stems from the entrepreneurial vision of the Cevolini family, owners of the CRP Group, after the CRP Racing experience in parallel with eCRP back in 2009.\nThe CRP Group was fundamental for the industrial development of Energica since the very first prototypes, which contain parts made with Selective Laser Sintering technology and Windform® carbon or glass fibre filled composite materials. The materials were engineered by the RD department of CRP Technology, the CRP Group’s company that has been dealing with professional 3D printing for over 20 years.\nEnergica has also benefited from the experience gained by the CRP Group in over 45 years of activity as a supplier of innovative and cutting-edge technological solutions alongside the major F1, Moto GP, Rally Raid and ALMS teams. The Group has always provided these teams with a high level of support during the entire development phases of the projects, from the early stages of design and development to the construction process, with consequent recognition of an innovative approach in the use of new materials and technologies.\nThe object of this application case is the creation of the Energica motorcycles dashboard.\nRight from the very start, the Energica team endorsed the idea of creating an innovative dashboard capable of containing, protecting, and isolating cutting-edge and very complex electronics: the studies and tests conducted by Energica, along with the specialized and trustworthy supplier Cobo, led to the creation a 4.3″ dashboard made up of a display with 16.7 million active matrix colours that provides excellent visibility.\nThe active matrix dashboard creates a real Human Machine Interface for the motorcycles.\nThis technology allows great flexibility, supporting the flow of information and interactivity with the vehicle system. It can provide an extensive menu of configurations and advanced user diagnostics, which are easy to read.\nThe Energica team worked with the injection mould supplier to develop the dashboard. Both took advantage of the support and expertise of CRP Technology, the CRP Group company leader in the field of professional 3D printing with Windform® composite materials.\nCRP Technology handled the construction of the functional prototypes.\nCreation of the 3D printing prototypes\nCRP Technology produced three functional prototypes, using Selective Laser Sintering technology and Windform® LX 3.0, a polyamide-based composite material reinforced with glass-fibres.\nOn all three prototypes the development and validation activities of the components (hardware and software) and the verification of the design solutions were carried out applying specific test plans aimed at verifying both the full correspondence to the functional needs and compliance with the quality and reliability objectives required.\nThe decision to opt for the creation of prototypes, that would allow a thorough study to reduce the margins of error on the injection mould, is the result of a process undertaken by CRP Technology together with the Energica Motor Company team engineers.\nThe shell was made in two separate parts. “Our suppliers of traditional technology (mould makers) – they stated at Energica Motor Company – would have taken roughly five months to prepare the prototype moulds. Thanks to CRP Technology and the Windform® composite materials, we\nreceived the prototype in two days.”\nThe next phase involved assembling electronic components and securing them with mastic/glue. No particular difficulties or problems were found during this phase.\nThe Windform® prototype was then returned to Energica, which tested it by mounting it directly on the bike (in photo 3 you’ll note the holder for the bike) and tested it on the road: the prototype, in conditions of extreme stress, was suitable for protecting sophisticated electronic components thanks to the construction material used. In fact, Windform® LX 3.0, is non-conductive material, ideal for producing resistant end use parts and end-applications, which can be subjected to vibrations, impacts, and multiple weather conditions.\nThe second prototype, thanks to the material’s features, was painted in order to conduct colour tests.\nThe third prototype is a hybrid version which enabled the Energica staff and mould makers to carry out precise assembly tests: this model features the lower part made by 3D printing using Windform® LX 3.0, and the upper part in plastic injection.\nThe tests gave positive results and the Energica engineers were satisfied: the two parts were assembled without encountering any particular problems. The plastic injection tolerances calculated are the same as those of the Windform® composite material used.\nCreation of the finished product using traditional technology (injection moulding)\nOnce the project by the Energica team was approved, the mould makers developed the mould for industrial production.\nThe professional 3D printing technology and Windform® composite materials enabled the Energica team to shorten product development times, and to continue testing the component/prototype directly on the motorbike.\n“We used the functional prototypes of the dashboard for roughly six months, subjecting them to every kind of stress during the test rides.\nThanks to the prototypes in 3D printing and Windform®, we have been able to work with the mould makers in a new way. These functional components, which are much more than just aesthetic prototypes, allowed us to examine the application, and annotate some improvements for the final mould. We therefore saved time and money: when we gave the authorization to proceed with injection moulding, we knew that the pieces would come out perfect and ready to be used.”']	['<urn:uuid:7a42b86e-9ca5-4b12-a80a-bb1bc8122980>']	factoid	direct	concise-and-natural	similar-to-document	single-doc	novice	2025-05-12T10:19:05.071190	10	20	990
51	law school alma mater columbia university	The LL.M degree was obtained from Columbia University, New York with specialization in human rights.	"['Ms. Ayesha Dias has a BA (Hons) in Political Science, LL.B ,and LL.M from the University of Bombay and a LL.M degree from Columbia University, New York with specialization in human rights.\nSince 2002, Ms. Dias is a Consultant to the Bureau for Development Policy (Democratic Governance Group) UNDP, New York on access to justice and human rights issues. She has initiated and directs the program on Human Rights and the International Natural Resource Industry at the Centre for Energy, Petroleum and Mineral Law and Policy, (CEPMLP) University of Dundee, Scotland. As a Senior Adviser in Law and Policy from 1995-1999, in the United Nations, Division for Sustainable Development, Department of Economic and Social Affairs, United Nations she undertook policy studies on legal aspects of human rights, sustainable development, governance and environmental justice.\nMs. Dias has conducted training workshops for capacity building in the environmental management of natural resources in developing countries and has been a resource person in training of trainers workshops, at the Asia-Pacific Center for Environmental Law, University of Singapore and at the International Environmental Law training program conducted by IUCN and UNITAR, New York.\nShe is a contributor on human rights, environment and corporate accountability to the UNDP Human Development Report 2000. Prior to her joining the United Nations, (1992-1994), Ms. Dias initiated the environment unit for teaching and research at CEPMLP. She has published widely in the field of natural resources and human rights. She was also the editor of the Annual Review of United Nations Affairs from 1995-2004.\n- Dias, Ayesha. Beyond Corporate Social Responsibility by Jedrez George Frynas (Cambridge University Press, 2009). Book review, forthcoming in Journal of World Energy Law and Business, 2009.\n- Dias, Ayesha. Corporate Accountability in International Environmental Law by Elisa Morgera (Oxford University Press, 2009). Book review, forthcoming in Oil, Gas and Energy Law, 2009.\n- Dias, Ayesha, \'The International Context for Access to Justice\' in Dias A. and Welch G. (Eds.) UNDP Justice for the Poor: Perspectives on Accelerating Access, Oxford University Press, June 2009. Editorial Notes and epilogue as Co-Editor, UNDP Justice for the Poor: Perspectives on Accelerating Access, Oxford University Press, June 2009.\n- Dias, Ayesha, \'Promoting Access to Justice through Judicial Reform: Shared Challenges in Securing Access to Justice- the Indian and Sri Lankan Experiences\' in Armytage L. and Metzner L. (Eds.) Searching for Success in Judicial Reform - Voices from the Asia-Pacific Experience, Oxford University Press, January 2009.\n- Dias, Ayesha, ""Human Rights Accountability of the Minerals Development Industry, Glorious Quest or Impossible Dream?"", Greener Management International Issue 52, October 2007 (Greenleaf Publishing, UK);\n- Co-editor, Oil Gas and Energy Law Intelligence, Oil, Gas and Energy Law (4), 2004, (Special Feature Corporate Social Responsibility);\n- Dias, Ayesha, ""Good Governance, Business and Human Rights in Energy Exporting Countries: A Supreme Challenge for CSR and Corporate Accountability"", Oil, Gas and Energy Law, (2) Issue 4, October 2004;\n- Dias, Ayesha, ""Oil and Human Rights"", Oil, Gas and Energy Law (2) 2003, OGEL, www.ogel.org;\n- Dias, Ayesha, ""International Standard-Setting on the Rights of the Indigenous Peoples - Implications for Mineral Development in Africa,"" presented at the 1998 IBA Conference, Cape Town, South Africa. 6 South African Journal of Environmental Law and Policy (1999);\n- Dias, Ayesha, ""The Oil and Gas Industry in the Tangled Web of Environmental Regulation: Spider or Fly?"" in Environmental Regulation of Oil and Gas (Zhiguo Gao Ed., 1998);\n- Dias, Ayesha, ""The Socio-Economic Implications of Privatization of State-Owned Mineral Assets in Developing Countries,"" United Nations, ST/TCD/19 (1997);\n- Dias, Ayesha, ""At the 50th Anniversary of the United Nations- A Retrospective Look at the Development of Mineral Resources,"" Natural Resources Forum (November 1995);\n- Dias, Ayesha, ""Environmental Law for Sustainable Development for Natural Resources: Mechanisms for Implementation and Enforcement,"" Natural Resources Forum (November 1995);\n- Dias, Ayesha, ""Environmental Law, Policy and Practice in India,"" Environmental Liability, Issue 3, Vol.2 (July 1994);\n- Dias, Ayesha, ""In Light of UNCED: Permanent Sovereignty Over Natural Resources,"" Environmental Policy and Law, 24/4 (1994);\n- Dias, Ayesha, ""Judicial Activism in the Development and Enforcement of Environmental Law: Some Comparative Insights from the Indian Experience,"" Journal of Environmental Law (Autumn 1994);\n- Dias, Ayesha, ""Towards Environmental Standards,"" in Mining and the Environment, Mining Journal Books (London 1992);\n- Dias, Ayesha, ""Bonded Labour: Challenge for Human Rights Activism in India"" Peaslee Award Winning Essay at the Columbia Center for Human Rights (1987);\n- Dias, Ayesha, Transboundary Pollution Accidents: Towards an Effective International Regime, in Industrial Hazards in Transnational World: Risk, Equity and Empowerment. Council on International and Public Affairs (New York 1987); and\n- Dias, Ayesha (Ed.), Annual Review of United Nations Affairs (Oceana Publications, 1995, 1996, 1997, 1999, 2000, 2001, 2002, 2003 and 2004).\nOGEL Special Feature Corporate Social Responsibility']"	['<urn:uuid:f8817af1-365d-455e-9613-2afd4bd82f69>']	factoid	with-premise	short-search-query	distant-from-document	single-doc	expert	2025-05-12T10:19:05.071190	6	15	791
52	Looking into research funding in Switzerland, I'd like to know the duration and financial support provided for this microbial organisms study at the University of Zürich. What were the specifics?	The project received a funding contribution (Förderbeitrag) of CHF 352,000 and ran for approximately 3.5 years, from January 2017 to May 2020. It was one of five winners of the 2016 'Microbials – Direct Use of Micro-Organisms' call.	"[""Für den Inhalt der Angaben zeichnet die Projektleitung verantwortlich.\nThis project is one of the five winners of the call 2016 «Microbials – Direct Use of Micro-Organisms».\nProject partners: University of Zürich; San Diego State University\nFörderbeitrag: CHF 352'000\nDauer: 01.2017 - 05.2020\nMicrobials, seit 2016\nAsst. Prof. Dr. Martin Pilhofer\nInstitute of Molecular Biology and Biophysics\n8093 Zürich (Schweiz)\n- email@example.com. ch\nThe administration of medication is more efficient if the respective drug can be delivered in a targeted way to the exact cells or tissues of the body on which the medication is intended to act on. The delivery of effectors to the cytoplasm of target cells is therefore an important problem in medicine (drug delivery) and biotechnology. Here, we will explore the use of bacterial contractile injection systems (BCIS) as a novel delivery vehicle. These BCIS are recently-discovered bacteriophage-like particles that inject their payload into target cells. Our recent efforts identified potential effectors and showed that some BCIS can accommodate large effector payloads. In an interdisciplinary approach, we will 1) investigate the mechanism of effector targeting/binding in the inner tube and 2) re-engineer the system to accommodate novel effectors of choice. The results will help to develop a bioinformatic tool, which can be used to design novel effectors for the translocation by BCIS in the future. In sum, the project could lead to the availability of re-engineered systems that are effective for antibiotic treatment or drug delivery.\nWas ist das Besondere an diesem Projekt?\nBased on their contractile, membrane-perforating properties, BCIS have been proposed for an application as novel antibacterial agents. The lack of knowledge on how considerable amounts of effectors can be loaded into a BCIS, however, has prevented to engineer them for the use for effector delivery to eukaryotic cells. This project will pave the way to load BCIS with many copies of an effector of choice.\nImpact on drug delivery strategies and biotechnological applications:\nThe groundwork of this project to understand effector binding to the BCIS will have major impact on the function of BCIS in general. The possibility to predict how a given effector has to be engineered in order for it to be delivered to the target presents a novel strategy for effector delivery. This is significant for drug delivery to organisms, as well as for biotechnological tools that can be used for research. The design of cell line-specific BCIS will be the next step in the future, enabled by the work proposed in this project. This vision has great potential, since it might allow targeted treatment of only diseased cells (e.g. cancer cells).\nWe identified the protein 615 as an effector molecule that is delivered by BCIS to cells of a tubeworm larva. We elucidated that the protein 615 is localized inside the inner tube of the BCIS. The protein 605 is likely a targeting factor that guides 615 to the inner tube. This way of loading a BCIS with an effector is novel and presents the possibility to accommodate a high payload per BCIS. Our results also discovered that the 615 N-terminus seems to play a role in guiding the effector to the lumen of the inner tube of the BCIS. We are now in the process to investigate how these findings can be exploited to load effectors of choice and translocate them into target cells. Further exciting results indicate that the so-called “tail pins” might play a role in the binding of the BCIS to target cells. Present and future work will explore whether the re-engineering of the tail pins will allow us to specifically target a certain cell type, which can be important for the efficient delivery of drugs. We envision that our results will pave the way for BCIS being broadly used as a novel vehicle for specific and efficient delivery of drugs.\nSophie A Howard, Alain Filloux, Imperial College London, United Kingdom Bacterial Protein Secretion: Looking inside an injection system\n*Ericson CF, *Eisenstein F, Medeiros JM, Malter KE, Calvalcanti G, Zeller RW, Newman DK, Pilhofer M, Shikuma NJ A contractile injection system stimulates tubeworm metamorphosis by translocating a proteinaceous effector\n*Shikuma NJ, *Antoshechkin I, Medeiros JM, Pilhofer M, Newman DK Stepwise Metamorphosis of the Tubeworm Hydroides elegans is Mediated by a Bacterial Inducer and MAPK Signaling\n*Shikuma NJ,*Pilhofer M, Weiss GL, Hadfield MG, Jensen GJ, Newman DK Marine tubeworm metamorphosis induced by arrays of bacterial phage tail-like structures\nAm Projekt beteiligte Personen\nLetzte Aktualisierung dieser Projektdarstellung 10.02.2021""]"	['<urn:uuid:3ea4db52-90dc-4073-be20-a8c5f7f3786a>']	factoid	with-premise	verbose-and-natural	distant-from-document	single-doc	expert	2025-05-12T10:19:05.071190	30	38	741
53	How do seasonal brewing practices affect traditional German beer production?	Seasonal factors significantly impacted traditional German brewing. Before refrigeration, brewers made Märzen (March) beer and stored it in cool casks over summer. For warmer months, they increased alcohol content (minimum 6%) to prevent spoilage. Similarly, Bock beers were historically brewed during winter months and required extra months of cold storage (lagering) to achieve their characteristic smooth, strong profile.	['I’m doing a monthly series here at Food Matters that I’m calling “Friday Happy Hour,” in which I’ll delve into the science of alcohol production.\nI don’t know about you guys, but after the events of the past two weeks, I could use a drink. Though the official celebration in Germany has been over for several weeks, the style of beer served every year* since 1810 at Oktoberfest can probably still be found at your local beer seller. All kinds of breweries make octoberfest-style beers, but the right to call your product “Oktoberfestbier” is a strictly guarded trademark of the Club of Munich Brewers. Only six breweries are allowed to serve their beer at the celebration, Augustiner, Hacker-Pschorr, Hofbräu, Löwenbräu, Paulaner, and Spaten.\nThe style that all the imitators make is “Märzenbier,” a lager brewed with a bottom-fermenting yeast (I’ll talk about this in another post). Märzen means March in German, so why is a March beer being used for Octoberfest? The beer is stored (lager = to store in German) in casks to age over the course of the summer. Most lagers are brewed in the fall and stored over winter, and the cold weather would keep the beers from spoiling.\nIn the heat of the summer, before refrigeration, there was nowhere to keep the beer cool, so brewers turned to an alternative method of preservation: more alcohol! Oktoberfestbier must be at least 6% alcohol in oder to get the title, but more practically, this is to prevent the growth of unwanted microbes. But how could a 19th century brewer hope to measure the alcohol content of his beer?\nThe Science: Specific Gravity\nBeer starts its life as a substance called “wort,” a sweet liquid obtained when malted barley is bathed in heated water to extract the sugars and other flavorful compounds. This liquid has a very high density (high weight compared to volume), but once the yeast get started breaking down the sugars and converting them to CO2, the density begins to decrease. Even when the brewer switches the yeast to anaerobic conditions, and the molecules in sugar can’t be converted into a gas and released, the alcohol produced has a lower density than water, further reducing the overall weight/volume.\nThe principal of measuring density has been around a long time, ever since Archimedes stepped into a bathtub (or so the story goes). When a solid object is put in liquid, it displaces a volume of that liquid equal to its own volume. So if I have a container filled to the brim with water and drop in a bowling ball measuring 1 cubic foot, the amount of water that would splash onto the floor would also be 1 cubic foot.\nBut an object of the same size made of styrofoam would float. Why? Because the volume of water that would be displaced from the container weighs much more than the styrofoam ball. Instead of sinking, the styrofoam ball will displace a volume of water equal to its own mass, and come to float with only that volume submerged.\nThis is the principal behind a relatively simple tool used to measure the density of a fermenting batch of beer, a hyrdometer, an object (usually cylindrical) with a known weight, but a lower overall density than water. When placed in a very dense liquid (high weight per volume), the hydrometer won’t sink very far, since it doesn’t take much of the liquid to equal the weight of the instrument. As the yeast break down sugars converting them to CO2 or alcohol, the density decreases, meaning the hydrometer must push up more liquid to match its weight, causing it to sink. Markings on the side of the hydrometer indicate exactly how low it has gone.\nAncient brewers didn’t have a good way to measure absolute density, so instead they relied on relative measurements. The difference depth of the hydrometer at the beginning of brewing (the “original gravity” or OG) and the density at the end (“final gravity or FG) was the specific gravity of the beer. It’s not really possible to accurately convert specific gravity of a beer to it’s alcohol content – the exact ratio will depend on the original sugar content of the wort, the amount of time spent in aerobic (Co2-producing) vs anaerobic (alcohol producing) conditions, and a host of other factors, but this measurement allows a reasonable approximation.\nThe Review: Harpoon Octoberfest\nWhen I can, I like to stick to locally brewed beers, and it doesn’t get much more local for me than Harpoon Brewery. Founded in the early 1980′s, it was the first brewery to obtain a license to produce beer in Boston in over 25 years (they like to show off their permit on the tour – it’s numbered 0001). Their “Octoberfest” is obviously just Octoberfest-style (they don’t brew in Munich), but it’s pretty good.\nIt pours nicely with a rich head of orangish/white foam. It’s very clear, dark brown with some deep amber tinge. There’s a faint, very pleasant hops aroma, slightly sweet. Tastes pretty standard for an Octoberfest style, but it’s crisp. A little malty, a touch of hops, and a very clean mouthfeel. Perfect for a brisk autumn day.\nUp next month: Pumpkin beers! (I’m a fan so I’ll be trying a panel)\n* Actually, the celebration has been canceled 24 times, mostly due to the great wars (and a couple of cholera epidemics).', 'What follows is information about bocks, collected from a variety of sources. If you know of any additional resources about this type of beer, please let me know.\nA Comparison of Style Ranges\n|BJCP1 (6C) Dunkles Bock||14-22||6.3-7.2%||1.064-1.072||1.013-1.019||20-27|\n|GABF2 (45A) Trad. Ger.||20-30||6.3-7.6%||1.066-1.074||1.018-1.024||20-30|\n|Periodic Table7 (40)||15-30||6.4-7.6%||1.066-1.074||1.018-1.024||20-30|\n|WBC6 (43A) Trad. Ger.||20-30||6.3-7.6%||1.066-1.1074||1.018-1.024||20-30|\nBJCP Description: 6C. Dunkles Bock1\nOverall Impression: A dark, strong, malty German lager beer that emphasizes the malty-rich and somewhat toasty qualities of continental malts without being sweet in the finish.\nAroma: Medium to medium-high bready-malty-rich aroma, often with moderate amounts of rich Maillard products and/or toasty overtones. Virtually no hop aroma. Some alcohol may be noticeable. Clean lager character, although the malts can provide a slight (low to none) dark fruit character, particularly in aged examples.\nAppearance: Light copper to brown color, often with attractive garnet highlights. Lagering should provide good clarity despite the dark color. Large, creamy, persistent, off white head.\nFlavor: Complex, rich maltiness is dominated by the toasty rich Maillard products. Some caramel notes may be present. Hop bitterness is generally only high enough to support the malt flavors, allowing a bit of sweetness to linger into the finish. Well-attenuated, not cloying. Clean fermentation profile, although the malt can provide a slight dark fruit character. No hop flavor. No roasted or burnt character.\nMouthfeel: Medium to medium-full bodied. Moderate to moderately low carbonation. Some alcohol warmth may be found, but should never be hot. Smooth, without harshness or astringency.\nComments: Decoction mashing and long boiling plays an important part of flavor development, as it enhances the caramel and Maillard flavor aspects of the malt. Any fruitiness is due to Munich and other specialty malts, not yeast-derived esters developed during fermentation.\nCharacteristic Ingredients: Munich and Vienna malts, rarely a tiny bit of dark roasted malts for color adjustment, never any non-malt adjuncts. Continental European hop varieties are used. Clean German lager yeast.\nStyle Comparison: Darker, with a richer malty flavor and less apparent bitterness than a Helles Bock. Less alcohol and malty richness than a Doppelbock. Stronger malt flavors and higher alcohol than a Märzen. Richer, less attenuated, and less hoppy than a Czech Amber Lager.\nGABF/World Beer Cup Description\nA. Subcategory: Traditional German-Style Bock\nBocks are dark brown to very dark. Traditional bocks are made with all malt, and have high malt character with aromas of toasted or nut-like malt, but not caramel. Fruity-ester aromas should be minimal if present. Diacetyl aroma should not be perceived. Hop aroma is very low. Traditional bocks have high malt sweetness. Malt flavor character should be a balance of sweetness and toasted or nut-like malt, but not caramel. Hop flavor is low. Hop bitterness is perceived as medium, increasing proportionately with starting gravity. Fruity-ester flavors should be minimal if present. Diacetyl flavor should be absent. Body is medium to full.\n|The origins of Bock beer are quite uncharted. Back in medieval days German monasteries would brew a strong beer for sustenance during their Lenten fasts. Some believe the name Bock came from the shortening of Einbeck thus “beck” to “bock.” Others believe it is more of a pagan or old world influence that the beer was only to be brewed during the sign of the Capricorn goat, hence the goat being associated with Bock beers. Basically, this beer was a symbol of better times to come and moving away from winter.\nAs for the beer itself in modern day, it is a bottom fermenting lager that generally takes extra months of lagering (cold storage) to smooth out such a strong brew. Bock beer in general is stronger than your typical lager, more of a robust malt character with a dark amber to brown hue. Hop bitterness can be assertive enough to balance though must not get in the way of the malt flavor, most are only lightly hopped.\n|The dark Bock has a deep copper to dark brown color. Medium to full-bodied, malt sweetness and nutty or light toasted flavors dominate. Hop flavor and aroma can be light to non-existent.|\nCuisine (German) Cheese (earthy; Camembert, Fontina) General (Chocolate) Meat (Game)3\nChicken Depends on the dish, but these suggestions are good if it’s by itself, Hummus, Monkfish4\nGrilled Rib-Eye, Aged Swiss, Chocolate5\nSeasonality & Temperature\nLinks About Bock\n- All About Beer’s Profile\n- All About Beer’s Stylistically Speaking by K. Florian Klemp\n- Beer Advocate\n- BJCP 2008 Online (19C)\n- Brew Your Own\n- Brewery DB\n- German Beer Institure\n- Michael Jackson’s Beer Styles\n- Rate Beer\n- Bock (Classic Beer Style) by Darryl Richman\n- Page 449 of The Beer Bible, by Jeff Alworth\n- Page 105 of The Essentials of Beer Style, by Fred Eckhardt\n- No. 40 on The Periodic Table of Beer Styles\n- Page 24 of What the Hell am I Drinking?, by Don Russell\nCommercial Examples of Bock\nAass Bock, Einbecker Ur-Bock Dunkel, Great Lakes Rockefeller Bock, Kneitinger Bock, New Glarus Uff-da Bock, Penn Brewery St. Nikolaus Bock1\nGreat Lakes Rockefeller Bock, Tommyknocker Butt Head Bock, Troegs Troegenator Double Bock5\nTop 10 Examples\nKey to Sources\n1 = BJCP 2015\n2 = GABF 2015\n3 = Beer Advocate\n4 = Garrett Oliver’s Brewmaster’s Table\n5 = Brewers Association / CraftBeer.com\n6 = World Beer Cup Guidelines 2016\n7 = The Periodic Table of Beer Styles 2001\n8 = GotBeer.com\n* = Not recommended for extended aging, unless ABV exceeds average range']	['<urn:uuid:72d10603-9995-48ad-b7cd-2ad914d6110d>', '<urn:uuid:c328bf0d-e65f-467a-8268-24a05de7d27e>']	factoid	with-premise	concise-and-natural	distant-from-document	three-doc	expert	2025-05-12T10:19:05.071190	10	58	1800
54	natural flood protection city implementation challenges	The transition from traditional gray infrastructure to green infrastructure faces several challenges. Many municipalities feel locked in by existing conventional infrastructure and instinctively opt to replace aging systems rather than implement green solutions. Additionally, despite growing interest in nature-based approaches, many decision makers still prefer traditional hard infrastructure, assuming risk mitigation is quicker to materialize. Better evidence and pilot interventions are needed to demonstrate the effectiveness of nature-based solutions.	"[""Research & Initiatives\nTransitioning Urban Infrastructure\nThe need for transitioning of Urban Infrastructure Systems (UIS) is illustrated by the facts that the earth system is undergoing significant rapid changes which have developed from increased human activities, population growth and urbanization (Vairavamoorthy et al., 2008). Whereas 48% of the world's population presently live in cities and towns, this proportion is expected to increase to about 60% in the year 2030 aggravating the need for the transition of existing systems. Further more in developing countries the urban population is predicted to grow from 1.9 billion in 2000 to 3.9 billion in 2030, averaging 2.3% per year. In addition in developed countries, the urban population is expected to increase, from 0.9 billion in 2000 to 1 billion in 2030 with an overall growth rate of 1%. On the other hand, existing infrastructure systems have been gradually deteriorating due to environmental action and ageing; in many cases significantly exceeding their design life leading to failure to meet the minimum level of service.\nWith increasing global change pressures cities of the future will experience difficulties in efficiently managing scarcer and less reliable UIS. In order to meet these challenges there is a need for a fundamental change in the way we manage urban infrastructure. The current models of urban infrastructure systems, and their corresponding infrastructure, originates from the 19th century, when populations were relatively small, and there was a view that resources were in abundance and the environment benign. Today, cities all over the world are facing a range of dynamic regional and global pressures, such as climate change, population growth, urbanization, deterioration of urban infrastructure systems and more. Due to these pressures cities of the future will experience difficulties in efficiently managing these infrastructure systems.\nTo ensure a more sustainable future there is a need for more drastic measures. Technology breakthroughs and innovative designs need to be coupled with comprehensive system changes to the urban processes, institutions, and regulations that ultimately shape our cities. Cities will be faced with difficult future strategic decisions (e.g. the choice between centralized and the decentralized systems; the choice regarding the level of involvement of individual citizens, NGOs and companies; the choice between an institutional framework where separate institutions are responsible for a certain element of the urban infrastructure system or moving towards a more integrated institutional set-up). Hence it is likely that future desired UIS will look and operate differently to existing ones and will be managed and financed differently.\nA major challenge for most cities will be to establish a process for transitioning from their present situation to the desired future endpoint (transitioning describes the path taken to get from the present situation to the desired future state). Transitioning can be viewed as the long term phased gradual change of the state of a system leading to a radically new system (optimized future system).\nTitle: 'From Gray to Green: Tools for Transitioning to Vegetation-Based Stormwater Infrastructure'.\nFunder: US Forest Service, National Urban and Community Forestry Challenge Program.\nGoal: This project is will provide natural resource managers, planners, and engineers with a set of decision support tools to aid the strategic planning process for transitioning to green infrastructure systems that emphasize trees and urban forests.\nMany cities recognize the potential of green infrastructure in managing and mitigating urban stormwater runoff (e.g. nutrient load), yet most lack systematic strategies for transitioning away from their existing conventional (gray) drainage systems. Policy makers and their constituents are becoming increasingly aware of the ecological, economic, and social benefits offered by urban forests. Urban foresters are now finding themselves charged with the task of managing city trees for specific ecosystem services. A recent focus is the use of trees as part of green infrastructure stormwater best management practices (green BMPs) to manage water quality and to achieve environmental management goals such as nutrient reductions required in total maximum daily loads (TMDLs).\nTotal nitrogen and phosphate loads in urban runoff are important non-point source pollutants that often lead to eutrophication and associated water quality problems in receiving water bodies. Recent research findings indicate that green infrastructure BMPs that include urban trees and urban forests provide a viable and cost-effective strategy for managing nitrogen and phosphorus pollution in urban runoff. Some of the BMPs involving mature vegetation have been proven to be more effective than conventional 'gray' stormwater BMPs to manage nutrient pollutants in urban runoff. In addition to this stormwater function, green infrastructure strategies that emphasize trees and urban forests provide a range of social and ecological benefits not offered by traditional 'gray' infrastructure systems. Hence there is a call to promote green infrastructure options that include trees and urban forests as stormwater BMPs.\nUp to now the application of green BMPs and the supporting research are mainly focused on new urban development sites, neglecting the potential for green infrastructure in existing urban areas. It is generally easier to implement innovative green BMP solutions in new developments where there are no limiting inherited infrastructures. The downside of this strategy, if restricted in this manner, is that green infrastructure will have limited application in many of the nation's largest communities. Widespread adoption of green drainage infrastructure can only be achieved if the inherited conventional urban stormwater systems are gradually replaced by green infrastructure. This transition process is complicated as many municipalities feel that they are locked in by the existing conventional infrastructure and hence instinctually opt to replace and repair aging conventional systems rather than develop and implement green BMP solutions.\nThe transition from gray to green infrastructure for urban stormwater management and the promotion of green BMPs that emphasize tree and urban forests can only be achieved with the collaboration of urban foresters, engineers, planners, and environmental managers. Currently this collaboration is constrained by a lack of information exchange and differences in the management and design cultures of the involved disciplines. For example, traditional civil engineers often rely on the well-known costs and longevity of traditional 'gray' infrastructure and tend to avoid the more uncertain and variable performance of green BMPs, even if they hold promise as efficient and cost-effective solutions. A successful transition from gray to green infrastructure requires bridging the gap between the disciplines. Decision support tools are needed to share information and to guide a collaborative planning process in a clear and accountable manner.\nThe main objectives of this research include:\n- Development of a catalogue of stormwater BMPs options with an emphasis on urban trees and forests – providing their cost, performance metrics and site selection criteria.\n- Identification of the potential to implement green infrastructure BMP options in developed urban areas with existing conventional drainage systems.\n- Identification of the optimal combination of existing conventional gray drainage systems and new green BMP options that improves the performance at the lowest cost.\n- Development of a decision support tool to identify the preferred transition pathway from gray to green infrastructure.\n- Application of the tools in two demonstration projects with different climatic conditions to illustrate the national applicability of the toolset.\n- Development of a transitioning framework that facilitates the transitioning process from gray to green drainage systems across the nation.\nThis project will develop and demonstrate an innovative transitioning framework from gray to green infrastructure systems for urban watersheds through the execution of the tasks outlined below and illustrated in figure 2.\nThis project proposes the development of a suite of decision-making tools that will help reduce disciplinary barriers and support the transition from gray to green infrastructure. With an emphasis on user-friendliness, the toolset will make existing models and green infrastructure literature accessible to a wide interdisciplinary audience. Proposed products include:\n- GIS-tool to identify the potential to implement green BMPs, including urban trees and forests, in catchments with existing conventional stormwater systems.\n- Optimization tool to identify an optimal combination of existing gray and new green infrastructure considering water quality control and other environmental / social benefits.\n- Decision support tool to prioritize implementation of green BMPs, and determine the most effective transition pathway from gray to green infrastructure.\n- Case studies to test and demonstrate the effectiveness of the transition framework and toolset through its modeled application in urban catchments in the City of Tampa and Hillsborough County (Florida), and City of Milwaukee (Wisconsin).\n- Support documentation (publications and video sequences) for proposed toolset."", 'How can nature help build flood resilience?\nThere are many terms for disaster risk reduction and climate change adaptation approaches that use and/or work with nature to build flood resilience, for example nature-based solutions (NBS), eco-based solutions, green and blue infrastructure, or eco-system services.\nIn this article we look at approaches and share resources that use different terminology but all have the common aim of building resilience by enhancing natural capitals. You might also find How can land use planning reduce flood risk? useful.\nThe blog From Grey to Green infrastructure: a paradigm shift needed to deliver on climate action explains the concepts of grey, green, and hybrid infrastructure and the benefits of moving away from only applying hard infrastructure approaches like concrete dams or dykes to mitigate flood risk.\nMultiple benefits of using nature-based solutions to build flood resilience\nUnlike hard infrastructure solutions which usually have a single purpose: protection from floods, nature based approaches provide a range of benefits beyond flood protection.\nThe blog Solutions providing multiple resilience dividends require integrated approach, and the working paper it’s based on, looks at the multiple resilience dividends of a range of flood resilience interventions, many using nature based solutions.\nMangroves planted to reduce the impact of tidal waves and coastal erosion also become home to spawning fish, contributing to ocean bio-diversity and sustaining livelihoods of local people. Parks designed to hold rainwater excess to prevent floods also become recreational spaces for urban residents, providing opportunities for exercise, leisure, and relaxation which has positive impacts on physical and mental health.\nHow do we make the case for investing in nature-based solutions for flood resilience?\nDespite growing interest in, and advocates for, nature based approaches to disaster risk reduction many decision makers, planners, and community members still prefer traditional, hard infrastructure based on assumptions of risk mitigation from grey infrastructure is quicker to materialise.\nTo make nature-based flood resilience solutions more attractive for investment we need better evidence that they work.\nWe need pilot interventions, which’s success is rigorously assessed and evidenced, that can be used as case studies. And we need frameworks for assessing the cost and benefits of nature-based solutions for disaster risk reduction, both in comparison with traditional infrastructure, and the cost of responding to the disaster likely to happen if no risk reduction is undertaken.\nZurich Flood Resilience Alliance colleagues have written a range of blogs discussing such frameworks:\n- Solutions providing multiple resilience dividends require integrated approach\n- Is this new framework what is needed to better assess nature-based solutions for climate change adaptation?\n- Eco-solutions: Can nature solve our problems?\n- Eco-solutions: How to measure their effectiveness?\n- Eco-solutions: How to build a case for their scaling up?\n- From Grey to Green infrastructure: a paradigm shift needed to deliver on climate action\nThe WWF in partnership with OFDA has developed this guide to help support communities using natural and nature-based methods of flood risk management\nSuccessful cases of integrated urban flood risk management (IUFRM) have demonstrated significant results not only in reducing the negative impacts of\nThe objective of this document is to present five principles and implementation guidance for planning, such as evaluation, design, and implementation']"	['<urn:uuid:21c6d134-57f8-4065-a734-27a4448650df>', '<urn:uuid:ca1f6dc2-ddf3-457f-9830-72303fe02c6d>']	factoid	with-premise	short-search-query	distant-from-document	multi-aspect	novice	2025-05-12T10:19:05.071190	6	69	1920
55	I live near the coast and I'm worried about flooding - what's the outlook for coastal flooding in the coming decades, and what solutions are cities implementing to deal with this?	The outlook for coastal flooding is severe, with projections showing high-tide flooding occurring between 50-250 days per year along the East Coast by 2050. By 2100, it could happen 'every other day' (182 days/year) or even daily under current greenhouse gas emission rates. The acceleration is already visible - for example, the Mid-Atlantic saw high-tide flooding double from 3 to 6 days annually between 2000-2015. As for solutions, cities are implementing comprehensive strategies. For instance, they are building networks of green stormwater infrastructure, raising critical road corridors, constructing berms and floodwalls to protect neighborhoods, and developing microgrids for power resilience. They're also adapting buildings by floodproofing and raising residences, while augmenting coastal open spaces to buffer against storm surge.	['A new NOAA report has published startling new projections that affirm Davidson’s warning.\nBy 2100, the report says, “high tide flooding will occur ‘every other day’ (182 days/year) or more often” even under an “intermediate low scenario” in coastal areas along the East Coast and Gulf of Mexico. This scenario works under the assumption that greenhouse gas emissions — which warm the climate and speed up sea-level rise — are curbed.\nFor a more aggressive “intermediate” scenario, in which greenhouse gas emissions carry on at today’s pace, high-tide flooding is forecast to occur 365 days per year.\nSea-level rise and coastal flooding are already accelerating\nIn February, scientists published a separate study concluding that they had already detected an acceleration in sea-level rise and that it is likely to speed up more in the coming decades as ice sheets disintegrate and ocean waters expand.\nThe implications of the steepening sea-level rise on high-tide flooding are already apparent on the ground.\nWilliam Sweet, the lead author of the NOAA report, has witnessed an uptick in flooding firsthand. “I moved to Annapolis last year,” he said in an interview. “Since I moved there, I’ve seen six or seven days when waters hit storefronts. What we’re finding is along the East Coast and the Mid-Atlantic, in particular, already the flooding is accelerating.”\nIn 15 years, the incidence of high-tide flooding in the Mid-Atlantic doubled, from an average of three days per year in 2000 to six in 2015, according to the NOAA report.\n“That’s a very important outcome,” Sweet said. “What it means is that the change is not a gradual linear change, but measured in leaps and bounds. By the time you realize there is a flooding issue, impacts will become chronic rather quickly.”\nA similar acceleration in coastal flooding has been seen in other locations along the East Coast, including Florida, the Carolinas, and in the Northeast.\nPractically every time tides are intensified by the lunar cycle in South Florida, media reports show inundation from “sunny day flooding,” which seldom occurred mere decades ago.\nThis past winter, Boston observed its highest and third-highest tides in recorded history as nor’easters battered southern New England in January and March.\n“The record-breaking event of January 2018 would not have broken the record had it not been for relative sea-level rise that carried the tide and storm surge above the level reached in 1978,” Andrew Kemp, an assistant professor of earth and ocean sciences at Tufts University, told the Boston Globe.\nThe future projections are “crazy”\nThe prospect of high-tide flooding occurring every day or even every other day late this century is difficult to fathom.\nMichael Lowry, a visiting scientist at the University Corporation for Atmospheric Research, expressed shock on Twitter after seeing these projections. “It’s hard to overstate the significance of this,” he said. “That isn’t even the intermediate high, high, or extreme scenarios that bring us 365 [days per year] high tide flooding in my lifetime. It’s crazy.”\nEven by 2050, the report projects high-tide flooding will occur between 50 and 250 days per year along the East Coast, depending on the emissions scenario.\nImpacts are “immense”\n“By mid-century, the frequency of this type of ‘minor’ flooding would become so disruptive that business as usual would be practically impossible without significant adaptation measures,” Caldas said. “Without planning for flooding with measures such as protecting, elevating, accommodating the water, or even moving stuff out of the way, the impacts on the cities, their economy, and their residents would be immense.”\nShe added: “Just imagine seeing streets (and property) flooded every other day. That gives a completely new meaning to the term ‘nuisance flooding.’ Or actually, it completely obliterates the concept, as flooding would become much more than a nuisance, but a rather serious problem requiring significant resources and innovative policies.”\nNOAA’s Sweet said informing decision-makers and planners about the escalating flood risk was impetus for the report. Floodwaters “getting up to storefronts in Annapolis becomes the new tide” by late this century, he said. “When you project out that far, the assumption is that society will adapt. That’s the real reason we’re doing this: to inform decision-making so the best science is available to plan accordingly.”\nIn Washington, planners are paying attention\nIn the Washington region, significant infrastructure is at risk from rising seas, including assets around the Mall, the Southwest Waterfront, Old Town Alexandria and Annapolis.\nTo respond to this risk, an interagency team known as the District of Columbia Silver Jackets formed in 2014. It includes members from regional and federal agencies, such as the D.C. government, U.S. Army Corps of Engineers, National Park Service and National Weather Service, as well as academia.\n“While our team has made great strides in identifying and helping to mitigate flood risk in the region, there is still so much work to be done, including taking a more comprehensive, integrated and regional approach to flood risk management,” the Silver Jackets team said in a statement. “The statistics in this report are staggering, and really hit home on the fact that time is of the essence as we plan for the future and take steps now to ensure our nation’s resources and treasures are not impacted by flooding.”\nCaldas stressed global action is needed in addition to local measures to lower the risk of high-tide flooding. “Adhering to the commitments of the Paris [climate] Agreement is exceptionally important, because emissions for the next couple of decades, up to mid-century, will define how much sea level rise we will see in the end of the century — and how much tidal flooding we will see: every other day or every day,” she said.', 'The resilience strategy covers a study area that extends from downtown Bridgeport to Black Rock Harbor. The strategy outlines an integrated approach to managing long-term risk, enabling equitable adaptation and growth, and enriching and enhancing the daily lives of Bridgeport residents.\nThree Area Plans and the Water’s Edge\nThe resilience strategy is organized into three separate area plans – the areas are 1) Black Rock Harbor/Cedar Creek, 2) South End – West, and 3) South End – East. Each has distinct land uses, street and building patterns, and landmarks, as well as issues and opportunities. All are located at the water’s edge, and are threatened by rising sea levels, storm surge, and heavy rainfall. Each plan proposes a mix of structural and nonstructural measures that can be implemented incrementally to make Bridgeport stronger and more resilient over time. Learn more about these measures in the section below.\nThe Black Rock Harbor and Cedar Creek Area Plan focuses on reducing flood risk to critical facilities, transportation infrastructure, and housing, while expanding access to the Cedar Creek waterfront. The plan proposes green infrastructure, retention basins, and raised roads that improve quality of life for local residents, enhance connections and egress, improve water quality in Cedar Creek, and support the continued development of Bridgeport’s Eco-Technology Park.\nThe South End – West Area Plan proposes combined sewer separation and new stormwater infrastructure that reduce runoff and flooding for low-lying areas, new park spaces and public plazas that provide stormwater storage and anchor neighborhood revitalization efforts, and design principles that sustain historic assets like Seaside Village. The plan also outlines the proposed pilot project, which will be a critical first step towards realization of the resilience strategy as a whole.\nThe South End – East Area Plan focuses on the relationship between harbor, utilities, existing neighborhood, and the University of Bridgeport. The plan proposes a raised road that redefines the area’s southern edge, levees and floodwalls to protect regional power generation and transmission facilities as well as the historic buildings and neighborhoods, and streetscape and public space improvements from downtown to the South End that support public and private investments along Broad and Main.\nThe resilience strategy also describes the ecological and resilience opportunities that exist along the water’s edge across the entire study area. This includes stormwater and combined sewer outfalls, and the relationship between urban systems and Long Island Sound. This includes the docks, beaches, parkland, and roadways that are so vital to access and recreation. This includes the industrial waterfronts and hardened edges that characterize long swaths of the coastline. And this includes the wetlands and estuary conditions that are vital to the biodiversity and long-term adaptation of the city.\nIn its entirety, the resilience strategy puts forth a vision for Bridgeport as a city that is resilient, lively, and beautiful. Coastal areas are vital to the local and regional economy. Residents are connected to the waterfront and to coastal habitats. Roadways, public spaces, water and power networks, and buildings are designed and built for “living with water,” and a future in which heavy rainfall and periodic inundation recur more frequently.\nAugment existing open space assets like Seaside Park to ensure that inhabited areas are buffered against the direct force of hurricanes and storm surge. Improve ecosystem health, access, and connectivity along Bridgeport’s coastline.\nRaise critical corridors to provide egress and access to low-lying and vulnerable areas. Use corridors and associated signage program to minimize disruption to daily life and business, facilitate efficient evacuation and provision of emergency services, and promote early recovery.\nWhere economically justifiable, construct berms, levees, floodwalls, or floodgates to protect neighborhoods, critical facilities, development sites, and historic landmarks from storm surge. Integrate these features with raised roads and new stormwater infrastructure.\nGreen and Blue Streets\nBuild a network of green stormwater retention and conveyance features. Integrated with existing grey infrastructure, this network will allow Bridgeport’s streets and public spaces to manage the flow of stormwater through the city in order to reduce flood risk and improve water quality.\nUse localized power generation, microgrids, and thermal loops at the district scale to capture waste heat, improve efficiency, reduce energy costs, provide redundancy, and improve continuity of services even during extreme events.\nBuilding Adaptation and Development\nDemonstrate new approaches for living in low-lying areas that are sustainable and safe. Floodproof and raise residences, businesses, and institutions. Preserve historic sites and buildings. Support sustainable development practices that capitalize on Bridgeport’s water assets and regional connections.']	['<urn:uuid:0593434c-4624-42df-a9dd-bb315e7967c3>', '<urn:uuid:c5bbd0c4-10cc-453c-b011-06042030dae8>']	open-ended	with-premise	verbose-and-natural	distant-from-document	multi-aspect	novice	2025-05-12T10:19:05.071190	31	119	1690
56	old vhs tapes archive problems	VHS tapes pose significant archival problems because they deteriorate much faster than analogue films. Additionally, VHS recorded material is largely absent from audiovisual archives, creating a potential gap in cultural heritage preservation.	['Changing platforms of ritualized memory practices. The cultural dynamics of home movies\nThis project addresses a number of questions dealing with the complex interrelationship between technology, specific user generations and spaces or places of cultural memory production in home movie making and screening. More concretely we are interested in the question how changing technologies of cultural production (film, video or digital camera) have shaped new practices and rituals of memory staging (screening of the films in domestic of public venues) and thereby initiated processes of (re)negotiating user generations and group identities.\nHome movies, often referred to as family films or home videos, are situated at the heart of family memory practices, mostly displayed in the private realm of the domestic space. From the invention of the film camera at the end of the nineteenth century until well into the sixties, amateur film making on 9,5mm, 16mm or 8mm film was predominantly a domestic hobby practiced by middle class men. With the diffusion of the video camera in the 1970s and 1980s, home movies did not only spread more widely, but the context and possibilities of display also changed enormously. Subsequently, the mass dissemination of digital recording technologies in the 1990s and 2000s brought about a true revolution. Home movies have definitely abandoned their niche as domestic family ritual and conquered the public spaces of diverse internet platforms. As a new cultural practice it became part of twentieth century family life. Video portals like You Tube, blogging sites as Twitter or online social networks such as Hyves or Facebook force us to rethink the technological, social and cultural dimensions of traditional and ritualized forms of memory practices. To grasp the technical, social and cultural dimension and historical meaning of this radical change in the making and screening of home movies, this project proposes to systematically investigate the crucial relationship between memory technologies and mediated memory practices in a longue durée perspective.\nThe changing practices of home movie making and screening have not been studied systematically, from a long-term perspective, yet. As long as home movie practices are associated with nostalgic domestic imagery produced on film, there is the danger of a growing gap in our cultural heritage. The nearly total absence of VHS and, to a lesser extent, digitally recorded material in audiovisual archives and its critical material status (VHS tapes deteriorate much faster than analogue films; the digital standards change continuously) make the historical study of VHS and digital home movie practices a matter of absolute urgency, both from an archival and museological perspective. An increased attention for the pure materiality of the recorded memories both in terms of storage and accessibility is therefore of crucial importance for the project in all its dimensions.\nThis research project is supported by the Netherlands Institute for Scientific Research (NWO) for the period 2012-2015. It exists a collaboration between Maastricht University, University of Groningen en the University of Luxembourg.\n© Home Movies Project\nProf. dr. Andreas Fickers\nUniversity of Luxembourg\nDr. Susan Aasman\nDr. Jo Wachelder\nMA Tom Slootweg\nMA Tim van der Heijden']	['<urn:uuid:2a1081f9-b6b4-43f6-8261-b38ccbb82429>']	factoid	direct	short-search-query	distant-from-document	single-doc	novice	2025-05-12T10:19:05.071190	5	32	511
57	who stonewall jackson role chancellorsville battle death	Stonewall Jackson played a pivotal role at the Battle of Chancellorsville, leading a daring flank attack with 30,000 troops that surprised and routed the Union's right flank on May 2, 1863. However, while conducting a nighttime reconnaissance after this successful attack, Jackson was accidentally shot by his own men - North Carolina troops who mistook his party for Union cavalry. He sustained three wounds and died on May 10, 1863. His death created an irreplaceable vacancy in the Confederate army, though ironically his final military action contributed to what became known as 'Lee's Greatest Victory.'	"['When many Civil War enthusiasts think of the battle of Chancellorsville, the first thought that pops to their mind is the wounding of Stonewall Jackson. For many, the battle begins and ends with the daring flank attack and wounding of Lee’s third-in-command. In his most recent work, Chancellorsville: Crossroads of Fire, author and historian Chris Mackowski clearly shows that there is more to the story of the battle than Stonewall Jackson, and he provides readers with a concise summary of the battle.\nMacksowski takes readers and visitors to the battlefield and off the beaten path to tell all sides of the story. The book is set up as a combined tour book and history of the battle. According to Macksowski, “Writing about a battle that’s been so written about can be tough.” In reality, though, much that has been written about the battle has focused around Jackson, Lee and the Union 11th Corps. Therefore, there is much the story that has been overlooked. That is not to say that quality histories of the battle do not exist (see suggested readings page), but the author of this new work has a unique viewpoint regarding who comes to the battlefield, why they go there and what they actually want to see.\nMackowski started visiting battlefields years ago with his daughter Stephanie, who was fascinated by the story of Stonewall Jackson. After a visit to the Stonewall Jackson Shrine, Chris decided to volunteer at the park, and after a number of years of volunteering, he was hired as a ranger/historian at Fredericksburg and Spotsylvania Military Park. Therefore, Chris has had thousands of encounters with visitors and young historians. These encounters gave him an idea to pitch a series of books to park management and the Friends of Fredericksburg Area Battlefields, who enthusiastically agreed to support and fund a writing program. The hope was to help visitors understand aspects of the Civil War better and produce affordable quality works that support battlefield visits, while utilizing quality research material. This volume is actually the third in a series of books that began publishing in 2009.\nChancellorsville: Crossroads of Fire and its tour start at a point where most end the story at the Chancellorsville Battlefield Visitors Center: the site of Jackson’s famed wounding. To Chris this was a logical jumping off point. This is the place that most people come to see (not to mention there are restrooms on site). But where Jackson’s military career ended, the reader’s journey begins.\nAfter setting up the story, readers embark on a 13-stop tour of Chancellorsville. They visit the most recognizable places, such as the Chancellorsville Crossroads, Lee-Jackson Bivouac and Flank Attack Area. Many out-of-the-way spots, though, are also visited, including Catherine Iron Furnace, the Unfinished Railroad Cut, and the recently acquired First Day Battlefield. At each stop, a brief history is laid before the reader, reinforced by interpretation of the site. This unique approach takes readers across the Virginia countryside with dozens of illustrations. One of the best qualities of this work is that one can read it in Washington state and still see the battlefield, or conversely a person may take it in hand and walk the fields and read the descriptions on site.\nThe history in the book is very well researched. Mackowski utilized the vast knowledge of the park staff, many of whom are noted historians themselves. Add to that the voluminous amount of first-hand resources, and the author has put together a short, though comprehensive history of “Lee’s Greatest Victory.”\n“It was a fun challenge to write a book about the battle in a way that, I hope, is fresh,” Mackowski said. “Hopefully, the book helps people not only understand the battle better but also the battlefield itself.”\nAccording to Mackowski, “There hasn’t been much written about the battlefield itself, so that was a real treat.” He brings up a valid point; one must truly know the battlefield to know the battle. He has literally lived on the battlefield, so he is one of those people who truly knows the battlefield at Chancellorsville, inside and out.\nAs a tour book and short history I would recommend this title to those who truly want to explore Chancellorsville and not focus exclusively on Jackson’s wounding.\nThere was only one real drawback to the book, which is that there are only four maps, which were augmented by a few other historic maps that are mixed in with the illustrations. The historical maps include a copy of a map carried by Jackson at Chancellorsville. Still, the author worked it so that the reader on the battlefield will have many maps, not in the book, but on the interpretive signage across the field.\nOne aspect I enjoyed was seeing names and pictures of officers that few associate with the battle. Union Generals Hiram Berry and Amiel Whipple play a role in the story. These were two 3rd Corps division commanders who fell on May 3rd. May 3rd is also explored in great detail. This was the second-bloodiest day of the war, only to be outdone by Antietam. This day is often overlooked because of the wounding of Jackson. Also to my delight, the role of JEB Stuart is explored. He went from Lee’s cavalry chief to the head of Second Corps. Stuart’s new command spearheaded the May 3rd assault. The coverage of the Day Three fight is an outstanding addition to the work.\nThe two appendices are nice additions to the work, too. One covers both the rivers and fords in the Wilderness of Virginia; the other offers a brief biography of Matthew Fontaine Muray. Maury, the “Father of Modern Oceanography,” was born on what would become the Chancellorsville Battlefield. The brief, well-written bio, explores the career of a man who few know is from Spotsylvania County, Virginia.\nIn all, this work is a worthwhile investment to those who want to learn more about the battle and the battlefield. It is a great work to take along the tour of the battlefield because it not only complements the interpretive markers on the field, it enhances them, as well as your experience, on the Chancellorsville Battlefield.\nOther Titles by Chris Mackowski:', 'The locomotive ground to a halt at a little depot amidst a drenching downpour. An eager figure scanned the cars for two passengers who meant more to him than anyone else on earth.\nThe legendary ""Stonewall"" Jackson, renowned as the quintessential grim warrior, revealed his gentler nature on April 20, 1863, at Guinea Station, 12 miles south of Fredericksburg as he greeted his beloved wife and saw his infant daughter for the first time. The blissful family repaired to a nearby house and passed the next nine days enjoying the only domestic contentment they would ever share. In less than three weeks, at a small frame building near Guinea, Jackson would be dead.\nThe campaign that resulted in Jackson\'s demise, paradoxically remembered as ""Lee\'s greatest victory,"" emerged from the backwash of the Battle of Fredericksburg. That Federal debacle and subsequent political intrigue at army headquarters prompted a change of command in the Army of the Potomac. Major General Joseph Hooker, a 48-year-old Massachusetts native endowed with high courage and low morals, replaced Burnside in January. Within weeks, Hooker\'s able administrative skills restored the health and morale of his troops, whom he proudly proclaimed ""the finest army on the planet.""\nThe new commander crafted a brilliant plan for the spring that he expected would at least compel General Robert E. Lee to abandon his Fredericksburg entrenchments, and, possibly, prove fatal to the Army of Northern Virginia. First, Hooker would detach his cavalry, 10,000 strong, on a flying raid toward Richmond to sever Lee\'s communications with the Confederate capital. Then, he would send most of his infantry 40 miles upstream to cross the Rappahannock and Rapidan Rivers beyond the Confederate defenses, and sweep east against Lee\'s left flank. The rest of ""Fighting Joe\'s"" army would cross the river at Fredericksburg and menace the Confederate front as the second blade of a great pincers. ""My plans are perfect,"" boasted Hooker ""and when I start to carry them out may God have mercy on General Lee, for I will have none.""\nThe condition of the Confederate army lent credence to Hooker\'s confidence. In February, Lee detached his stalwart lieutenant, James Longstreet, with two strong divisions to gather food and supplies in southeastern Virginia. The gray commander cherished the offensive, but could not hope to move north without Longstreet. In the meantime, Lee\'s 60,000 veterans at Fredericksburg would guard their long river line against 130,000 well-equipped Yankees.\nHooker began the campaign on April 27 and within three days some 40,000 Federals had splashed through the upriver fords, their presence detected by Confederate cavalry. On April 29, a sizable Union force led by Major General John Sedgwick\'s Sixth Corps erected pontoon bridges below Fredericksburg and also moved to Lee\'s side of the river.\nWith both wings of the enemy across the Rappahannock, Lee faced a serious dilemma. Conventional military wisdom dictated that the understrength Army of Northern Virginia retreat south and escape Hooker\'s trap. Lee opted instead to meet the Federal challenge head-on. Correctly deducing that Hooker\'s primary threat lay to the west, ""Marse Robert"" assigned 10,000 troops under Major General Jubal A. Early to man the old Fredericksburg entrenchments. The balance of the army would turn west toward the tangled Wilderness to confront Hooker\'s flanking column.\nBy mid afternoon of April 30, that column, now containing 50,000 men and 108 artillery pieces, rendezvoused at the most important road junction in the Wilderness. A large brick tavern named Chancellorsville dominated this intersection of the Orange Turnpike with the Orange Plank, Ely\'s Ford, and River roads. ""This is splendid,"" exulted one of Hooker\'s corps commanders, ""Hurrah for Old Joe.""\nThe Federals had encountered virtually no opposition to this point. Moreover, they could now press eastward, break clear of the Wilderness, and uncover Banks Ford downstream, thus significantly shortening the distance between their two wings. Hooker, however, decided to halt at Chancellorsville and await the arrival of additional Union troops. This fateful decision disheartened the Federal officers on the scene who recognized the urgency of maintaining the momentum they had thus far sustained.\n""Stonewall"" Jackson, gladly seizing the initiative that Hooker needlessly surrendered, left the Fredericksburg lines at 3:00 a.m., on May I and arrived at Zoan Church five hours later. There he found two divisions of Confederate infantry, Major General Richard H. Anderson\'s and Major General Lafayette McLaws\', fortifying a prominent ridge covering the Turnpike and Plank Road. Although his corps had not yet appeared, Jackson ordered Anderson and McLaws to drop their shovels, pick up their rifles, and advance to the attack.\nJackson\'s audacity dictated the shape of the Battle of Chancellorsville. When Hooker at last authorized an eastward movement late in the morning of May 1, his troops on the Turnpike and Plank Road ran flush against ""Stonewall\'s"", outgunned but aggressive brigades. Union front-line commanders had not expected this kind of resistance. They sent anxious messages to Hooker, who quickly ordered his generals to fall back to the Wilderness and assume a defensive posture. The Federal columns on the River Road marched almost to Bank\'s Ford without seeing a Rebel. They returned to Chancellorsville fuming, fully realizing the opportunity that had slipped through their fingers.\nLate in the day, as the blue infantry threw up entrenchments encircling Hooker\'s Chancellorsville headquarters, Major General Darius N. Couch approached his superior. As the army\'s senior corps commander, Couch had advocated an offensive strategy and shared his comrades\' disappointment with ""Fighting Joe\'s"" judgment. ""It is all right, Couch,"" Hooker reassured him, ""I have got Lee just where I want him; he must fight me on my own ground.""\nCouch could barely believe his ears. ""To hear from his own lip that the advantages gained by the successful marches of his lieutenants were to culminate in fighting a defensive battle in that nest of thickets was too much, and I retired from his presence with the belief that my commanding general was a whipped man.""\nHooker\'s confidence had faded to caution, but whether he was ""whipped"" depended upon Lee and Jackson. Those two officers reined up along the Plank Road at its intersection with a byway call the Furnace Road on the evening of May 1. Transforming discarded Federal cracker boxes into camp stools, the generals examined their options.\nConfederate scouts verified the Federals\' strong positions extending from the Rappahannock River, around Chancellorsville, to the high, open ground at Hazel Grove. This was the bad news. The Southern army could not afford a costly frontal attack against prepared fortifications.\nThen, about midnight, Lee\'s cavalry chief, ""Jeb"" Stuart, galloped up to the little campfire. The flamboyant Virginian carried thrilling intelligence. The Union right flank was ""in the air"" -- that is, resting on no natural or artificial obstacle! From that moment on, the generals thought of nothing but how to gain access to Hooker\'s vulnerable flank. Jackson consulted with staff officers familiar with the area, dispatched his topographical engineer to explore the roads to the west, and tried to snatch a few hours rest at the chilly bivouac.\nBefore dawn, Lee and Jackson studied a hastily drawn map and decided to undertake one of the biggest gambles in American military history. Jackson\'s corps, about 30,000 troops, would follow a series of country roads and woods paths to reach the Union right. Lee, with the remaining 14,000 infantry, would occupy a position more than three miles long and divert Hooker\'s attention during Jackson\'s dangerous trek. Once in position, ""Stonewall"" would smash the Federals with his full strength while Lee cooperated as best he could. The Army of Northern Virginia would thus be fractured into three pieces, counting Early\'s contingent at Fredericksburg, any one of which might be subject to rout or annihilation if the Yankees resumed the offensive. To learn more about the role of McLaws\' men on May 2 see a folder for McLaws\' Trail.\nJackson led his column past the bivouac early on the morning of May 2. He conferred briefly with Lee, then trotted down the Furnace Road with the fire of battle kindled in his eyes. After about one mile, as the Confederates traversed a small clearing, Union scouts perched in treetops at Hazel Grove spotted the marchers. The Federals lobbed artillery shells at Jackson\'s men and notified Hooker of the enemy movement.\n""Fighting Joe"" correctly identified Jackson\'s maneuver as an effort to reach his right flank. He advised the area commander, Major General Oliver 0. Howard, to be on the lookout for an attack from the west. As the morning progressed, however, the Union chief grew to believe that Lee was actually withdrawing - the course of events Hooker most preferred. Worries about his right disappeared. Instead, he ordered his Third Corps to harass the tail end of Lee\'s ""retreating"" army.\nColorful Major General Daniel E. Sickles commanded the Third Corps. He probed cautiously from Hazel Grove toward a local iron manufactory called Catharine Furnace. In mid-afternoon the Federals overwhelmed Jackson\'s rearguard beyond the furnace along the cut of an unfinished railroad, capturing nearly an entire Georgia regiment. The action at Catharine Furnace, however, eventually attracted some 20,000 Bluecoats onto the scene thus effectively isolating Howard\'s Eleventh Corps on the right with no nearby support.\nMeanwhile the bulk of Jackson\'s column snaked its way along uncharted trails barely wide enough to accommodate four men abreast. ""Stonewall"" contributed to Hooker\'s faith in a Confederate retreat by twice turning away from the Union line - first at Catharine Furnace, then again at the Brock Road. After making the desired impression, Jackson ducked under the Wilderness canopy and continued his march toward Howard\'s insensible soldiers.\nActing upon a personal reconnaissance recommended by cavalry general Fitzhugh Lee, Jackson kept his column northbound on the Brock Road to the Orange Turnpike where the Confederates would at last be beyond the Union right. The exhausting march, which altogether traversed more, than 12 miles, ended about 3 p.m. when ""Old Jack\'s"" warriors began deploying into battle lines astride the Turnpike. Jackson, however, did not authorize an attack for some two hours, providing 11 of his 15 brigades time to take position in the silent forest. The awe-inspiring Confederate front measured nearly two miles across.\nAlthough individual Northern officers and men warned of Jackson\'s approach, Eleventh Corps headquarters dismissed the reports as frightened exaggerations from alarmists or cowards. Hooker\'s shortage of cavalry hampered the Federals\'s ability to penetrate the Wilderness and uncover the Confederate presence with certainty. Only two small regiments and half a New York battery faced west in the direction of Jackson\'s corps.\nSuddenly, a bugle rang out in the afternoon shadows. Bugles everywhere echoed the notes up and down the line. As waves of sweat-soaked soldiers rolled forward, the high defiance of the Rebel Yell pierced the gloomy woods. Jackson\'s Corps erupted from the trees and sent the astonished Unionists reeling. ""Along the road it was pandemonium,"" recalled a Massachusetts soldier, ""and on the side of the road it was chaos.""\nMost of Howard\'s men fought bravely, drawing three additional battle lines across Jackson\'s path. But the overmatched Federals occupied an untenable position. The screaming gray legions overwhelmed each Union stand and eventually drove the Eleventh Corps completely from the field.\nSunset and the inevitable intermingling of ""Stonewall\'s"" brigades compelled Jackson to call a reluctant halt to the advance about 7:15. He summoned Major General A.P. Hill\'s division to the front and, typically, determined to renew his attack despite the darkness. Jackson hoped to maneuver between Hooker and his escape routes across the rivers and then, with Lee\'s help, grind the Army of the Potomac into oblivion.\nWhile Hill brought his brigades forward, Jackson rode ahead of his men to reconnoiter. When he attempted to return, a North Carolina regiment mistook his small party for Union cavalry. Two volleys burst forth in the blackness and Jackson tottered in his saddle, suffering from three wounds. Shortly thereafter a Federal shell struck Hill, incapacitating him, and direction of the corps devolved upon Stuart. The cavalryman wisely canceled ""Stonewall\'s"" plans for a night attack. See text for Wounding of Stonewall Jackson Trail.\nDespite his misfortune on May 2, Hooker still held the advantage at Chancellorsville. He received reinforcements during the night and the Third Corps moved back from Catharine Furnace to reoccupy Hazel Grove. Sickles\' troops thus divided the Confederates into separate wings controlled by Stuart and Lee. Hooker, if he chose, could defeat each fraction of his out manned enemy in detail.\nThe Confederate commanders understood the need to connect their divisions, and Stuart prepared an all-out assault against Hazel Grove at dawn. Hooker made it easy for him. As the Southerners approached the far crest of Hazel Grove they witnessed Sickles\' men retiring in an orderly fashion. ""Fighting Joe"" had directed that his troops surrender the key ground and fall back to Fairview, an elevated clearing closer to Chancellorsville.\nStuart immediately exploited the opportunity by placing 31 cannon on Hazel Grove. Combined with artillery located west along the Turnpike, the gunners at Hazel Grove pounded Fairview with a spectacular bombardment. The Federals responded with 34 pieces of their own and soon the Wilderness trembled with a discordant symphony of iron. See folder for Hazel Grove to Fairview walking trail.\nThe bloodiest fighting of the battle occurred between 6:30 and 9:30 a.m. on May 3. Stuart launched brigade after brigade against entrenched Union lines on both sides of the Turnpike. Troops lost their way in the tangled underbrush and the woods caught fire, confronting the wounded with a horrible fate.\nThe see-saw fighting began to favor the Southerners as, one by one, Union artillery pieces dropped out of the contest. Hooker failed to resupply his cannoneers with ammunition or shift sufficient infantry reserves to critical areas. A Confederate projectile abetted this mental paralysis when it struck a pillar at Chancellorsville, throwing the Union commander violently to the ground. The impact stunned Hooker, physically removing him from a battle in which he had not materially been engaged for nearly 48 hours. Before relinquishing partial authority to Couch, Hooker instructed the army to assume a prepared position in the rear, protecting the bridgehead across the Rappahannock.\nStuart pressed forward first to Fairview and then against the remaining Union units at Chancellorsville. Lee\'s wing advanced simultaneously from the south and east. The Bluecoats receded at last and thousands of powder-smeared Confederates poured into the clearing, illuminated by flames from the burning Chancellorsville mansion.\nLee emerged from the smoke and elicited a long, unbroken cheer from the gray multitudes who recognized him as the architect of their improbable victory. A Confederate staff officer, watching the unbridled expression of so much admiration, reverence, and love, thought that, ""it must have been from such a scene that men in ancient times rose to the dignity of gods.""\nThe Southern commander wasted little time on reflection. He prepared to pursue Hooker and seal the success achieved since dawn. A courier bearing news from Fredericksburg shattered Lee\'s plans. Sedgwick had driven Early\'s contingent from Marye\'s Heights and now threatened the Confederate rear. This changed everything. Lee assigned Stuart to watch Hooker\'s host and sent McLaws eastward to deal with the Sixth Corps menace. See a folder for a driving tour of 2nd Fredericksburg & Salem Church.\nSedgwick, slowed by Wilcox\'s single Alabama brigade retreating stubbornly from Fredericksburg, came to grips with the Confederates four miles west of town at Salem Church. The Federals swept into the churchyard but a powerful counterattack drove them back and ended the day\'s combat. The next day Lee shoved Sedgwick across the Rappahannock at Banks Ford and once again focused on the main Union army in the Wilderness.\nHooker, however, had seen enough. Despite the objections of most of his corps commanders, he ordered a withdrawal across the river. The Federals conducted their retreat under cover of darkness and arrived back in Stafford County on May 6. Ironically, this decision may have been Hooker\'s most serious blunder of the campaign. Lee\'s impending assault on May 6 might have failed and completely reversed the outcome of the battle.\nConfederate leadership during the Chancellorsville Campaign may represent the finest generalship of the Civil War, but the luster of ""Lee\'s greatest victory"" tarnishes upon examination of the battle\'s tangible results. In truth, the Army of the Potomac had not been so thoroughly defeated - some 40,000 Federals had done no fighting whatsoever. Although Hooker suffered more than 17,000 casualties, those losses accounted for only 13% of his total strength. Lee\'s 13,000 casualties amounted to 22% of his army, men difficult to replace. Of course, Jackson\'s death on May 10 created a vacancy that could never be filled. Finally, Lee\'s triumph at Chancellorsville imbued him with the belief that his army was invincible. He convinced the Richmond government to endorse his proposed offensive into Pennsylvania. Within six weeks, the Army of Northern Virginia confidently embarked on a journey northward to keep an appointment with destiny at a place called Gettysburg.\nThe text for this section was written by A. Wilson Green, former staff historian for Fredericksburg and Spotsylvania National Military Park. It is derived from a National Park Service training booklet.']"	['<urn:uuid:7c74d8bc-dbda-46c0-8c41-4ae02edce8d9>', '<urn:uuid:b43c7b5a-8fc4-463c-9233-a748d3a28798>']	open-ended	direct	short-search-query	similar-to-document	multi-aspect	novice	2025-05-12T10:19:05.071190	7	95	3882
58	I'm working on internal grinding of small holes and wondering about the relationship between hole size and wheel diameter - what's the recommended wheel size ratio for optimal grinding?	For Superabrasive internal grinding, the wheel is generally recommended to be 40-60% the size of the hole diameter being ground. This ratio varies depending on the hole size - larger diameter holes will require a lower percentage, while smaller holes require a greater percentage. Additionally, if the wheel diameter is smaller or equivalent to the shank diameter, an undercut may be required behind the wheel for needed clearance.	"['Help to Choose the Correct Grinding Wheel\nInternal grinding is arguably the most challenging of all grinding applications. In no other type of grinding can the entire range of parameters change from one job to the next. Therefore, it is necessary to consider the specifics of each job individually in order to arrive at the wheel best suited for the job.\nIn order to maximize performance capabilities and profit margins, choosing the correct wheel for a job is extremely important. Unfortunately, this isn\'t always the easiest decision to make, as many factors play into such a selection. One must consider material to be ground, size of area to be ground, amount of stock to be removed, finish desired, machine capabilities and a number of other factors.\nThe following is a general outline of the steps which would lead to the selection of the correct wheel for you (Click each to view):\nWith the correct SFPM figure at hand, the correct bond may be chosen. In addition to the guidelines laid down by the Bond Selection Guide, a general rule to follow in bond selection by cutting surface feet per minute is as follows:\n|VM (Standard) Vitreous Metal||1,000-3,000|\nConcentration refers to the ratio of the volume of Superabrasive to the volume of bond in a grinding tool. Although there is some variation throughout the industry, generally accepted standards are as follows:\n|CONCENTRATION||ABRASIVE BY VOLUME|\nLow concentration provides more aggressive cutting, coarser finish and potentially higher wheel wear while high concentration provides less aggressive cutting, finer finish and lower wheel wear.\nDiagrind, Inc. standard is 100 concentration, variations are available upon request.\nNOTE: For grinding applications requiring a high grinding wheel diameter to hole diameter ratio (see ""Hole Size"" above) less than 100 concentration is recommended.\nRequired information includes type of machine (jig grinder, I.D. grinder, machine center, etc.) and spindle r.p.m. capabilities. Machines should have a rigid wheel head spindle, rigid bearings in the headstock spindle, and feed increments of .0001"" or smaller. Industry-wide, these machines have a wide variety of r.p.m. ranges, and Diagrind, Inc. has addressed this with the development of a variety of bonding systems which would maximize performance on any machine. Using spindle speed and the wheel diameter and the Diagrind, Inc. wheel speed calculator (free upon request), one can determine the surface feet per minute (SFPM) for a particular job, which will lead to the next step.\nGRIT SIZE SELECTION\nFinally, the grit of the wheel can be determined. This is accomplished by considering the median between stock removal and finish desired. A coarse grit will be more aggressive, but will consequently leave a rough finish, while the opposite is true for a finer grit. Diagrind, Inc. has experience working with grits as coarse as 30/40 and as fine as 20,000, so arriving at a figure to suit the need of individual customers is a commonplace operation.\nHOLE SIZE (AREA TO BE GROUND)\nIn the case of Superabrasive internal grinding, the wheel is generally 40-60% the size of the hole diameter being ground. Larger diameter holes will require a lower percentage, while smaller holes require a greater percentage (see illustrations). The depth of the hole determines the length of the abrasive as well as the undercut (if required) and the overall length of the mandrel (abrasive length + shank length). The other basic variable to be considered in wheel size is shank diameter, and that is determined by the size of the collet, or simply by choosing the stiffest shank that will fit reasonably with the wheel diameter. In most cases, if the wheel diameter is smaller or equivalent in size to the shank diameter, an undercut may be required behind the wheel for needed clearance (see Illustration 1).\nProper coolant will enhance the performance of the wheel as well as the finish in cases of heavy stock removal or fine finish requirements. Diamond wheels require a synthetic coolant, while CBN wheels use oil or water-soluble oil. See the Bond Selection Guide for usage information.\nMATERIAL TO BE GROUND\nThe deciding factor in which superabrasive to use lies in whether the material to be ground is ferrous (containing iron) or non-ferrous (carbide, glass, ceramic, etc.). Generally, ferrous materials are ground with CBN and non-ferrous materials are ground with diamond. More in-depth information on which Superabrasive to be used can be found in the Bond Selection Guide\nThis guideline can be effective in helping to determine the proper wheel for a specific grinding job. However, as stated before, every internal grinding job can be unique, and require a unique solution. With over 25-years of experience in the field, Diagrind, Inc. is well equipped to handle even the toughest internal grinding jobs, so feel free to contact us with any questions, we\'ll be glad to help.']"	['<urn:uuid:4dea2d77-2065-4e3f-8aa4-12d841667ef0>']	open-ended	with-premise	verbose-and-natural	similar-to-document	single-doc	expert	2025-05-12T10:19:05.071190	29	68	798
59	What are the main differences between a litigation divorce timeline and mediation, and how does the information gathering process differ between these two approaches?	In litigation, the divorce follows a strict timeline with formal phases: filing petition, temporary orders, discovery (5-7 months), settlement, trial preparation, and potentially trial. Information gathering is done formally through legal 'discovery' procedures and documents prepared by attorneys. In mediation, the process is much less formal and phases may occur in different orders - often the petition isn't even filed until after agreements are reached. Information is exchanged voluntarily and informally in meetings with the mediator. Both approaches require collecting income information, monthly expenses, assets, and debts, but mediation offers a more collaborative environment where parties can actively participate in decisions affecting them, potentially saving time, money, and stress associated with trial.	['The Divorce Timeline below is a general summary based on King County Family Law Case Schedules for a litigation case. If you have a case pending in any county you must check your own Case Schedule and court rules.\nGeneral Phases of the Litigation Process\n1) Filing of Petition for Dissolution of Marriage\nRestraining Orders may be sought along with the filing of the Petition for Dissolution. This event causes the Timeline to begin.\n2) Temporary Arrangements/ Temporary Orders phase.\nEither by agreement or through a court hearing your finances must be organized throughout the divorce process so that everyone can live and pay their bills, including attorney’s fees and the residences of the children must be organized. If a court hearing is required the person who files the Motion for Temporary Orders must give the other person 14 days’ notice. There must be a response filed by the responding party and a reply filed by the person who initiated the hearing. This phase of organizing how finances and the children will be handled during the divorce process can take 1 to 2 months from the date the Petition for Dissolution is filed.\n3) Discovery phase.\nDuring this phase of the process you are developing your case and finding out information from your spouse or partner such as their earnings history, list of assets and liabilities and values of each, monthly expenses. Financial experts are hired and do their work during this phase. Many issues to do with parenting after the divorce are explored during this time if parenting is not agreed upon. Mental health evaluations, court parenting experts and others are involved during this time if necessary. This phase takes 5-7 months.\n4) Settlement phase.\nAfter you have sufficient information to come to an agreement you analyze all of the information, prepare mediation materials and attend mediation to try to resolve the financial and parenting issues. In a family law litigation case this is usually a one day, 8 hour mediation. This is not the same as using mediation throughout your divorce to resolve the issues. See our page The Four Kinds of Divorce for a description of using mediation throughout the process, not just a one day mediation. This phase takes 1-2 months in litigation cases.\n5) Trial Preparation phase.\nIf you have not settled your case at mediation you must organize the information you are going to present to the judge, organize all documents you collected during the discovery phase and prepare your testimony and the testimony of other witnesses who will come to the trial. This phase takes 1-2 months.\n6) Trial phase.\nOn the date set forth on your Case Schedule you will appear at court prepared to address the judge and explain to the judge what you want the judge to order and why. Trials generally last 1 or 2 days to 2 weeks, if there are complex financial and parenting issues.\nForms and Court Deadlines After Filing the Petition for Dissolution\nA divorce is started by filing a Petition for Dissolution of Marriage. There are other forms that must be filed at the same time. When you file a Petition for Dissolution of Marriage you will get a Case Schedule that gives you all of the forms that have to be filed and deadlines you must meet.\nThe other party must be notified that you have filed the Petition for Dissolution. This is done by having a third person hand the papers to him or her, called “serving” the other person. Or, you can give the papers to your spouse or partner and they can sign an Acceptance of Service which is filed with the court letting the court know that the person has received the paperwork.\nThe other party must file a Response within 20 days of being served or signing an Acceptance of Service. In the Response the person tells the court if they agree with the statements in the Petition for Dissolution and they ask for whatever relief they want the court to provide them.\nAll parents who have children under the age of 18 must attend a Parenting Seminar within 90 days after the Petition for Dissolution is filed.\nA form called Confirmation of Issues must be filed around 90 days after the Petition for Dissolution is filed. This informs the court that the case is on track procedurally and there are no problems with necessary court forms. This form also informs the court if there is pending mediation and if there are special issues involved with the children. If a Confirmation of Issues is not filed then the parties must appear before the judge at a Status Conference to discuss the matters above.\nBeginning about 8 months after the Petition for Dissolution is filed you must begin to disclose the names of all witnesses you intend to call at trial and what they will testify about.\nIf you need to change your trial date there is a deadline for doing that which is about 2 months before your trial date.\nYou can gather information from the other person (Discovery phase above) until 6 weeks before the trial date. After that, the Discovery Cutoff Date, you cannot formally ask the other person or third parties to provide information to you.\nYou must engage in mediation one month before your trial date.\nIf you have not settled your case informally or in mediation then you have to appear before the trial judge, testify about your case and bring witnesses to testify about important financial and parenting issues. This is the trial that finalizes all issues in your divorce and ends your marriage.\nMeditation, Cooperative and Collaborative Law Case Timelines\nIn cases using mediation, cooperative law and collaborative law the parties go through most of the phases described above but in a much less formal manner and sometimes in reverse order to what is shown above. For example, often the Petition for Dissolution of Marriage is not filed until after the parties have reached agreement on most issues.\nTo resolve any case, information must be collected. Information forms the basis for agreements on finances and parenting issues. To resolve financial issues you must have income information for both spouses, what each person’s monthly living expenses will be, what assets exist to divide and what the debts are. In the family law litigation process this is done in a formal way through what is called “discovery”. In litigation cases this is done through a series of legal procedures and documents prepared by attorneys. In cases using mediation, cooperative law or collaborative law this information is exchanged voluntarily and informally in meetings with the mediator or attorneys.\nTo resolve issues concerning parenting of children sometimes evaluations by trained mental health professionals is helpful. In litigation cases, again, this is a formal process and the recommendations of a Parenting Evaluator or Guardian ad Litem are often very persuasive to the court if the court is making a decision about parenting issues. In cases using mediation, cooperative law or collaborative law input of a mental health professional is informal and not usually written. The professional often comes to meetings to offer suggestions for resolving parenting issues.\nAfter all of the information is gathered, people using mediation, cooperative law and collaborative law work to reach agreements that satisfy both people’s needs. No agreement is perfect but the agreement needs to be acceptable to both people. It is not always possible to reach agreements through mediation, cooperative law or collaborative law processes. When agreement is not possible then parties may try other informal processes or they may decide the only way they can resolve the issues is through litigation.\nConsult with a professional and organized Seattle divorce attorney at Integrative Family Law\nDivorce proceedings can seem overwhelming, but having an experienced, professional, and organized divorce lawyer on your side will help. Call us at 206.859.6800.\nNothing on this page is intended as legal advice for an individual.\nCall us now to learn more about how we can assist in your specific situation.', 'What is Mediation?\nMediation offers an informal forum for resolving problems between parties. The mediator, a neutral third party listens to both sides of your case and acts as a facilitator to help parties focus on the issues, encourage discussion and assist them in reaching a fair and mutually acceptable and voluntary agreement. By reaching a settlement during mediation, you avoid the uncertainty of a trial in which the judge dictates the results. The mediator guides the communication process so that everyone may be heard and personal feelings can be separated from the issues. Conflicts are discussed and various solutions are explored in an effort to reach a satisfactory agreement. The mediator may offer suggestions and help parties develop options to resolve the issues, but the final decisions are made by the parties.\nWho Attends the Mediation Session?\nOnly the parties, whose names appear on the Order to Family Mediation and their attorneys, if any, are included in the session. It is not necessary for attorneys to attend but is advisable if property issues will be discussed. Any third parties admitted to the session will be at the discretion of the mediator and with the approval of all parties. IN ACCORDANCE WITH FAMILY LAW RULE OF PROCEDURE 12.407, CHILDREN UNDER THE AGE OF 18 SHALL NOT BE BROUGHT TO MEDIATION.\nWhat if an Agreement is reached?\nIf an agreement is reached, the mediator will prepare a document reflecting the terms of the agreement and ask the parties to sign it. However, if the parties are represented by legal counsel who are not present at the session, the parties will have an opportunity to review the agreement with their attorneys before the document is submitted to the Court. If either party should change their mind after consulting their attorney, the attorney can object to the agreement, which would then void the document. If no objection is received within ten days of the session, the agreement will be submitted to the Court for approval.\nWhat if an Agreement is not reached?\nIf the parties do not reach agreement on any issues, the mediator will notify the attorneys, if any, and the Court, so that a trial date can be set.\nIs the Mediation Confidential?\nThe information disclosed in mediation is confidential, except that information which is required or permitted to be disclosed by law and that information which is available through other methods. Information obtained in mediation cannot be used if the case goes to trial. The mediator does not share information nor make recommendations to the judge. The parties are asked to sign a statement agreeing not to subpoena the mediator to testify on either party’s behalf.\nWhat are the Costs of Mediation?\nMediation costs are based on a sliding scale according to the parties’ incomes. Please refer to the Order to Family Mediation for these charges and for the fees for cancellations, rescheduling, or failure to appear.\nTo maximize your time for the session, you should arrive promptly, at the time specified in your Order.\nWhat are the Qualifications of Mediators?\nMediators are certified after meeting specific education and training requirements set by the Florida Supreme Court.\nDo I Need an Attorney?\nMediators do not give legal advice nor is mediation a substitute for obtaining such advice. Attorneys can help their clients understand the law and to have realistic expectations for the outcome of their case. However, no one can predict with certainty what a judge will decide in a case or be able to tell you whether a trial is worth the time or the financial and emotional expense.It is advisable for parties to consult with their attorneys\nprior to mediation and to have their attorneys review any agreement if the attorney does not attend the mediation session.\nWhat are the Advantages of Mediation?\n- Parties actively participate in the decisions that will affect them and their family members.\n- Parties determine the outcome of the process, thereby avoiding the uncertainty of a trial.\n- Immediate resolution can occur, which can save time, money, and stress that is usually associated with a trial.\n- The setting is informal and encourages better communication between the parties.\n- Parties who agree to resolution tend to be more satisfied with the results and are less likely to return to Court for further litigation of their case.\n- Parents can begin working together in a cooperative manner for their children’s sake.']	['<urn:uuid:4149fa85-05bc-4b70-9066-9229053e3ac0>', '<urn:uuid:1d695ef1-b459-4548-b442-e90254e63eea>']	open-ended	direct	verbose-and-natural	similar-to-document	multi-aspect	novice	2025-05-12T10:19:05.071190	24	112	2083
60	What role do written works play in both educating people about Epicurean concepts of material images and raising awareness about mangrove ecosystem services?	Written works served to explain complex Epicurean ideas about material images (spectra) that present themselves to the mind, as seen in Catius's attempt to create specialized vocabulary to describe these concepts. Similarly, modern written analyses help quantify and communicate the value of mangrove ecosystems - for instance, research has shown that mangroves provide fish-related services worth $37,500 per hectare per year in the Gulf of California, which is 600 times higher than the value placed on them by the Mexican government for tourism development.	"['Catius (fl. 50s–40s BC?) was an Epicurean philosopher, identified ethnically as an Insubrian Celt from Gallia Transpadana. Epicurean works by Amafinius, Rabirius, and Catius were the earliest philosophical treatises written in Latin. Catius composed a treatise in four books on the physical world and on the highest good (De rerum natura et de summo bono). Cicero credits him, along with the lesser prose stylist Amafinius, with writing accessible texts that popularized Epicurean philosophy among the plebs, or common people.\nIn a letter dated January 45 BC, Cicero speaks of Catius as having died recently. The letter is addressed to Cassius Longinus, one of the future assassins of Julius Caesar and a recent convert to Epicureanism. Cicero prods Cassius about his new philosophy, and jokes about spectra Catiana (""Catian apparitions""), that is, the εἴδωλα or material images which were supposed by the Epicureans to present themselves to the mind and to call up the idea of absent objects:\n|“||For somehow it makes you seem almost present when I write anything to you, and that not \'by way of phantoms of images,\' as your new friends express it, who hold that \'mental pictures\' are caused by what Catius called \'spectres\' — for I must remind you that Catius Insuber the Epicurean, lately dead, calls \'spectres\' what the famous Gargettius, and before him Democritus, used to call \'images.\' Well, even if my eyes were capable of being struck by these \'spectres,\' because they spontaneously run in upon them at your will, I do not see how the mind can be struck. You will be obliged to explain it to me, when you return safe and sound, whether the \'spectre\' of you is at my command, so as to occur to me as soon as I have taken the fancy to think about you; and not only about you, who are in my heart\'s core, but supposing I begin thinking about the island of Britain — will its image fly at once into my mind? But of this later on. I am just sounding you now to see how you take it.||”|\nAlthough Cicero\'s purpose is ridicule, the passage is an important source for understanding the Epicurean theory of vision. Catius\'s spectrum is equivalent to simulacrum in Lucretius, but the term spectrum does not appear again in Latin until the 17th century and must represent Catius\'s attempt to create a specialized vocabulary.\nQuintilian characterizes Catius briefly:\n|“||Among the Epicureans Catius is agreeable to read, though lacking in weight.""||”|\nEarly commentators on Horace assert that the philosopher should be identified with the Catius addressed in the fourth satire of the poet\'s second book. This Catius is introduced as delivering a grave and sententious lecture on various topics connected with the pleasures of the table. It appears from the words of Cicero, however, that the satire in question could not have been written until several years after the death of Catius. Horace may have intended to designate some gourmand of the court under a recognizably Epicurean nickname; given the poet\'s own Epicurean leanings, the passage should probably be read as a parody of the kind of false Epicureanism that disguised mere hedonism.\n- Elizabeth Rawson, Intellectual Life in the Late Roman Republic (Johns Hopkins University Press, 1985), p. 284.\n- Gian Biagio Conte, Latin Literature: A History, translated by Joseph B. Solodow (The Johns Hopkins University Press, 1999), p. 157 online.\n- Cicero, ad Fam. xv.16 = 215 in D.R. Shackleton Bailey, Epistulae ad familiares (Cambridge University Press, 1977), p. 60.\n- See article on Cassius for more on his Epicureanism; also Arnaldo Momigliano, review of Science and Politics in the Ancient World by Benjamin Farrington (London 1939), in Journal of Roman Studies 31 (1941) 149–157; Miriam Griffin, “Philosophy, Politics, and Politicians at Rome,” in Philosophia togata: Essays on Philosophy and Roman Society (Oxford: Clarendon Press, 1989); David Sedley, “The Ethics of Brutus and Cassius,” ‘’Journal of Roman Studies’’ 87 (1997) 41–53.\n- Cicero refers to Epicurus, who belonged to the Attic deme of Gargettus: D.R. Shackleton Bailey, Epistulae ad familiares (Cambridge University Press, 1977), p. 379, note 10 online.\n- Translation by Evelyn Shuckburgh, Cicero: The Whole Extant Correspondence in Chronological Order (London 1900), full text online.\n- Miriam T. Griffin, ""Philosophical Badinage in Cicero\'s Letters to His Friends,"" in Cicero the Philosopher, edited by J.G.F. Powell (Oxford University Press, 1999), p. 343 online.\n- Miriam T. Griffin, ""Philosophical Badinage,"" in Cicero the Philosopher, p. 295.\n- David Sedley, Lucretius and the Transformation of Greek Wisdom (Cambridge University Press, 2003), p. 39 online; Robert D. Brown, Lucretius on Love and Sex (Brill, 1987), p. 175 online.\n- Quintilian, Institutio Oratoria, x.1.§ 124, Latin: in Epicureis levis quidem sed non iniucundus auctor est Catius.\n- Emily Gowers, The Loaded Table: Representations of Food in Roman Literature (Oxford University Press, 1996), p. 141 online.\nThis article incorporates text from a publication now in the public domain: Smith, William, ed. (1870). ""article name needed"". Dictionary of Greek and Roman Biography and Mythology.', 'Vital Mangroves On The Edge Of Extinction Thanks to All-You-Can-Eat Shrimp (Book Review)\nPhoto by xavi talleda via Flickr CC\nMangroves are a special ecosystem. Found all over the world, they thrive in the in-between zones of land and sea, and act as nurseries for a vast variety of animals, grocery stores for humans, a buffer between homes and massive storms rolling in off the sea, a regulator of sediment and nutrients sliding from shore to sea, and more. Yet while they can feed millions of people and are an important resource, we often consider them no-mans-land, useless or worse than useless, something to be ripped up and replaced with resorts or shrimp ponds. Thankfully, there are people who see the innumerable benefits of mangroves and are coming up with ingenious ways to restore them before it is too late. Kennedy Warne is the author of Let Them Eat Shrimp: The Tragic Disappearance of the Rainforests of the Sea. He traveled the world visiting mangroves in many countries, witnessing their use, destruction, and restoration.\nPhoto by NatalieMaynor via Flickr CC\nLittle Shrimp, Jumbo Problem\nWarne launches into the problem of disappearing mangroves by highlighting one of the key culprits -- shrimp ponds. Warne illustrates how shrimp farming can be an economic boon to a country, but at the expense of local ecosystems and the people who rely on them.\nAmericans consume an average of four pounds of shrimp per person per year. It is a commodity that is taken from countries whose ecosystems are destroyed and food security weakened, and sent to wealthy countries that have little practical need for yet more protein.\nElaine Corets, Latin American coordinator of the mangrove Action Project points out that not only do the shrimp ponds ruin the land, but the sea is pillaged to keep them going as well. ""Between two and three pounds of feed are needed to produce one pound of shrimp. Depending on the proportion of fish in the feed (typically around 30 percent), shrimp farming operations may be net consumers, rather than producers, of fish protein,"" Warne writes.\nBetween the shrimp ponds and the resorts built atop filled-in mangroves (and serving all-you-can-eat shrimp buffets), the mangroves are disappearing fast.\nIn places that move into shrimp farming, the impact on ecosystems is obvious, and devastating. NASA\'s Earth Observatory shows the changes in shocking detail -- the bottom image shows how much of Guayaquil, Ecuador was converted to shrimp ponds but salt flats still remained. The top photo shows how even those were converted to shrimp ponds. Ecuador is among the world\'s largest shrimp exporters.\nMangroves As Shark Nursuries\nThe mangroves serve a number of important purposes, and one of those roles is that of nursery for fish. Lemon sharks are a key species using mangroves in tropical areas such as Bimini as a place of protection until they\'re big enough to face the open ocean. However, as resorts plow over mangroves, filling them in and building atop a once-rich ecosystem, the lemon sharks -- and many other species -- are left without a place to grow up.\nWarner writes, ""If aquaculture has been the prime driver of mangrove decline in poorer countries, property development has been the villain in rich nations and their offshore playgrounds.\n""...One man looks at Bimini\'s mangrove expanse and sees condos and a golf course. Another sees an ecotourism opportunity. Another sees a site for scientific research. Which vision should prevail?... The crux of the issue is undoubtedly valuation, but the difficulty is that some values are highly tangible while others are not.""\nLike so many other precious ecosystems, the long term value of biodiversity and thriving species is lost to the immediate return as resource after resource is stripped and sold away.\nHowever, mangroves do have a value if we take the time to calculate it. For example, Warne writes, ""A study of fish landing at thirteen sites in the Gulf of California recently found that yields were directly proportional to the length of coastline inhabited by mangroves. The researchers determined that the presence of mangroves was worth $37,500 per hectare ($15,000 per acre) per year for its fish-related services. This figure is 600 times the value the Mexican government places on mangrove land, which is acquired cheaply for tourism development and conversion to shrimp farms.""\nPhoto by Roberto Verzo via Flickr CC\n""They Think It Won\'t End""\nA big part of the problem is how invisible mangroves are to most of the people who could protect them. Even for people who use the mangroves for their livelihood may not be aware of the impeding destruction their mangroves face -- not just from development but also sea level rise.\nAs Warne points out, mangrove destruction is not as well known as rainforest destruction, thus it is more difficult to get activists behind their conservation and reforestation. Mangrove destruction is going on at the same as rainforest destruction, ""but while rainforest destruction quickly attracted urgent and vociferous opposition, the removal of mangroves was off the public radar. For most people, it still is.""\nAnd without awareness of how fragile the future of mangroves is, the less prepared we are to defend them from destruction. Like most things, we think mangroves simply won\'t disappear. But they are nearly gone.\nRoy ""Robbin"" Lewis III, who has spent 35 years rebuilding mangrove habitat, is quoted in the book noting that mangrove loss around the world is running at 1 to 2 percent a year for the last 30 years. If we were to completely stop mangrove loss and start to reforest them, we would need to replant at a pace of 370,000 acres a year. At this point, Lewis notes, we aren\'t even keeping up with one hundredth of that level.\nPhoto by treesftf via Flickr CC\nRebuilding Mangroves, One Seed At A Time\nThat isn\'t to say that mangrove reforestation is a hopeless cause. Indeed, there is a great deal of possibility that we can return lost mangrove land to its former productive and helpful glory. There are several efforts in different places around the world that are worth of mention -- even one that earned a Guinness World Record.\n""The current record of 541,176 mangrove propagules plated by a team of 300 people within a twenty-four-hour period was set in July 2009 by the Pakistan Ministry of the Environment,"" writes Warne. ""That plantathon, on mudflats near the mouth of the Indus River, surpassed the previous record of 447,874, set a month earlier by an Indian army regiment in Assam. What next -- the Mangrove World Series?""\nOn a more consistent level, the reforestation project occurring on the shores of the Red Sea is noteworthy for those interested in restoring mangroves anywhere in the world. Gordan Sato is working to alleviate poverty by replanting mangroves. Through studying the needs of mangrove plants in the area, has come up with a smart replanting system.\nHe watched where the mangroves were growing -- since the trees only grow in very particular areas between the sea and land -- and figured out that the mangroves were seeking the minerals brought by freshwater from inland. Now, seedlings are planted with an open-ended tin can to keep them from washing away, as well as a piece of iron and a sealed plastic bag filled with diammonium phosphate fertilizer, that has a hole in the bottom to act as a drip system. While plastic and other buried ""trash"" is less than ideal, the use of plastic and cans solves a number of problems and has allowed the successful planting of nearly a million mangroves.\nBy paying attention to what the plants need and coming up with a low-cost solution, Sato is helping to restore an ecosystem that can feed and protect millions as it grows.\nIt is this kind of ingenuity, as well as being given a true market value for what mangroves provide, that will turn the tide and hopefully bring them back from the edge of extinction. In Let The Eat Shrimp, Kennedy Warne details the many, many ways that we need mangroves, and you will close the book feeling inspired to protect a little appreciated, but much needed habitat.\nJoin Kennedy Warne for a live discussion on August 25 at 3pm Eastern.\nMore Book Reviews on TreeHugger\nShark Week: Demon Fish Dissects Sharks\' History, Future, and ""The Greatest Scam of All Time"" (Book Review)\n4 Key Lessons Learned From The Death and Rebirth of Monterey Bay (Book Review)']"	['<urn:uuid:fa04cd32-1776-4ca0-9fe4-b66f96125003>', '<urn:uuid:16c5701f-9941-44b7-8b65-3acee51583cf>']	open-ended	direct	verbose-and-natural	similar-to-document	multi-aspect	expert	2025-05-12T10:19:05.071190	23	84	2251
61	How does ocean acidification affect marine organisms?	Ocean acidification makes it more difficult for marine organisms like oysters and coral reef systems to produce shells or skeletons, preventing proper development. In the 2000s, there was a significant die-off of oyster larvae in West Coast shellfish hatcheries linked to acidification. Marine organisms are now experiencing exponentially increasing extinction rates as a direct result of ocean acidification.	['It’s no secret that the ocean is a vast space. Comprising over 90 percent of the Earth’s habitable space, it seems impossible that humans could have a serious impact on this massive environment teeming with life. However, scientific research at the National Oceanic and Atmospheric Administration (NOAA) is proving that we can and are having a detrimental impact on the Earth’s ocean environment.\nAs a Senior Fellow at for the NOAA in Seattle, Richard Feely has conducted groundbreaking research tying ocean acidification to global carbon dioxide emissions. He found that ocean acidification poses serious risks to marine animal health, the global environment, and the economies of humans dependent on marine life for sustenance and trade.\nFor his work documenting this issue and connecting it to human pollution, the Partnership for Public Service has nominated Feely for a 2015 Service to America Medal, or SAMMIE, in the Science and Environment category.\nIn an interview with Chris Dorobek on the DorobekINSIDER program, Feely discussed the environmental research that led to his SAMMIES nomination.\nWhen he first started his career, Richard Feely wanted to join an organization where he had the opportunity to make a difference. He found that at the NOAA. From the very start he had, “the opportunity to do groundbreaking research in the areas that really make a difference in terms of the environment and humanity,” he said.\nIn the early 1980s, the NOAA began studying how to make precise measurements for CO2 concentrations and pH changes in the ocean. After perfecting that method, they found that the ocean was becoming more acidic. Intrigued by these findings, Feely began to research the causes and impacts of ocean acidification.\nOcean acidification refers to an overall drop in pH across the ocean environment. “Since the beginning of the Industrial Revolution, mankind has released two trillion tons of carbon dioxide in the atmosphere by burning fossil fuels like coal, oil and natural gas,” Feely explained. When the NOAA first discovered signs of ocean acidification, “they thought that this was just an interesting chemical problem, not a significant biological impact,” Feely said. They were wrong.\nOver the past two hundred years, the concentration of carbon dioxide in the atmosphere has gone up a staggering 40 percent. About 30 percent of that carbon dioxide ends up sinking down to the ocean. This accumulation of pollution has caused a change in the ocean’s overall acidity, increasing it by approximately 30 percent.\n“The pH has dropped about 0.1 since the beginning of the Industrial Revolution,” Feely said. While this number may seem small, its impacts are far from it.\nIn the 1990s, research demonstrated that marine organisms such as oysters and coral reef systems are particularly sensitive to changes in pH. Increased acidity makes it more difficult for these organisms to produce shells or skeletons, rendering them unable to develop properly. “In the 2000s, we began to see a tremendous die-off of oyster larvae in our shellfish hatcheries on the West Coast and we were able to link that to the acidification process,” Feely explained. “We realized that this had a huge ecological and potentially economic impact on our food resources.”\nToday, “we know from measurements alone that the acidification has taken place throughout the water column and we can see it in the surface of the oceans as well as in the subsurface,” Feely said. “By the end of the century…we could see a 0.3 or 0.4 pH drop, which would be a 100 to 150 percent increase in the acidity of the oceans. That’s a change in chemistry that has not been observed for more than 50 million years.”\nHow can scientists make those measurements? Feely explained that ice cores and deep sea sediments from various places on earth allow scientists to look back thousands, and even millions of years to assess carbon dioxide levels throughout history. “We get very good proxy information of what the natural variability of CO2 concentration was in the atmosphere and the global temperature associated with it. Then we compare that to the changes that are taking place today,” he said.\nChanges in ocean acidity have occurred throughout history. Nearly all of the major prehistoric extinction events were in some way related to ocean acidification. However, Feely contended the major difference between then and now is the relative time span. Those extinctions occurred over the course of millions of years. “What we’re doing now is changing the CO2 concentration in the atmosphere and the oceans over time spans of hundreds of years,” rather than millions.\n“The changes that are taking place today are anywhere from 10 to 100 times faster than anything we have seen in the geological past,” Feely said. “If we continue down those pathways, the impacts that we saw in the past may actually occur again in the present.”\n“We are seeing the consequences right now in the biological world,” Feely explained. Marine organisms are increasing the rate of extinction exponentially as a direct result of ocean acidification.\nDespite the dire situation ocean acidification presents, Feely contended, “We haven’t gone too far yet.” While the situation grows worse as human practices remain unchanged, “we haven’t reached a major tipping point as of yet,” Feely said. But “we very well could in the next 100 years or so.”\nOver 97 percent of scientists agree that man-made release of CO2 in the atmosphere is causing the climate change that we’ve observed over the last several decades. According to Feely, the same is true for ocean acidification. If the global environment isn’t enough of a motivator to make a change, a $37 billion dollar seafood industry is also being put at immediate risk.\nIf carbon dioxide output remains the same, ocean acidification will have vast economic and ecological impacts on all of humanity in the near future. Decisive changes must to be made. “It’s a decision we need to be making now,” Feely said.\nFor his imperative work identifying the causes and risks associated with ocean acidification, we congratulate Feely on his 2015 SAMMIES award nomination.\n*Photo credit to the Partnership for Public Service']	['<urn:uuid:0631b9a9-4071-4766-9a2f-08fa88cd878b>']	open-ended	direct	concise-and-natural	similar-to-document	single-doc	expert	2025-05-12T10:19:05.071190	7	58	1014
62	need to know about disaster response teams help me compare how volunteers helped in virginia earthquake repairs vs nepal earthquake emergency	The response efforts were quite different in scale and approach. For the Virginia earthquake, the response focused on specialized historic preservation work, with Price Masonry's crew conducting careful brick-by-brick deconstruction of damaged chimneys at Salubria and planning detailed restoration work. In Nepal, the response involved multiple organizations coordinating large-scale humanitarian aid - Humanity Road worked for 49 days coordinating with groups like Translators without Borders and Kathmandu Living Labs to map crisis areas, identify urgent needs, assist stranded hikers, and help coordinate medical and water purification assistance. Over 50 military and civilian aid organizations were involved in addressing needs identified through the crisis mapping efforts.	['By Doug Harnsberger\nSpring 2012 Germanna Newsletter\nAt 1:51 pm on August 23, 2011, the terra firma beneath Salubria rumbled and heaved the stately 1757 Georgian-style house.\nBy the time the quaking action had subsided, the 5.8 magnitude Mineral Earthquake had ruptured and twisted both of the twenty foot high chimney stacks twenty degrees, and sent hundreds of pounds of loose bricks cascading from the severed chimney caps down onto the shingled roof below.\nThe cascading bricks hit the roof with such a powerful force that seven of the interior roof rafters fractured within the attic.\nIn addition, the quake’s rolling and rocking opened up a dozen new stress cracks in the once-solid brick perimeter walls.\nJust as many of the houses and churches in the nearby Town of Culpeper were rocked violently, the Mineral Quake shook Salubria with similar effects.\nIn its wake, the quake left considerable structural and cosmetic damage upon the oldest and finest brick residence in Culpeper County.\nThe Germanna Foundation’s first urgent priority for stabilizing Salubria’s earthquake damage was to deconstruct the twisted and ruptured chimney stacks before they might collapse through the roof.\nWithin a few days of the earthquake, the renowned historic preservation crew of Price Masonry from Madison Heights, Virginia worked from two opposing bucket lifts to gingerly wrap the chimney stacks with fiberglass straps in order to stabilize them, before beginning the brick-by-brick deconstruction process.\nAll of the displaced chimney bricks were brought down to grade in buckets, where they were cleaned of mortar and stacked for their eventual reuse.\nAbout 30% of the brick were cracked or damaged beyond reuse by the quake’s actions, so a mix of matching replica brick will be required for the two chimneys’ reconstruction.\nIn anticipation of the chimney reconstruction work that is scheduled to begin on April 15, 2012, the Germanna Foundation placed an order in January for three thousand sand-molded, handmade, oversize brick with Old Carolina Brick in Salisbury, N.C.\nThese custom-made bricks will closely match Salubria’s original chimney brick in size, texture and color.\nThe reconstruction work on the two chimneys is expected to take approximately two months, which means that the chimneys’ elegant twenty-foot-high profile above the roofline should be fully visible again for everyone to enjoy by mid-June.\nRepairing the Fractured Roof Rafters\nQuake damage to the original circa 1757 attic roof rafters was less obvious than the chimney destruction.\nThe above photo shows project Germanna member Robert Ellis pointing to one of seven fractured rafters in the attic. The fractures all were located just below the heavy purlin structural member at the rafters’ midspan.\nInstead of simply removing and replacing these seven broken rafters, however, the Germanna Foundation will repair them in situ and restore to their original locations prior to the earthquake’s dislocation.\nNew “sister” rafters will be inserted adjacent to each of the repaired rafters to assume the active roof load.\nThe historic rafters will be relieved of the ongoing roof stresses, and preserved-in-place for future architectural historians to observe and analyze.\nRepairing the Quake-induced Stress Cracks in Salubria’s Brick Walls\nLateral and vertical earthquake forces moving through a solid brick wall often will produce diagonal stress cracks, as seen in the above photo.\nIn this particular example along a Second Floor partition wall, the bricks pulled apart about an inch, leaving a continuous diagonal line of severed bricks and localized plaster damage.\nSalubria’s brick wall stress crack restoration work will begin in April of 2012, after the last freeze of winter.\nAll of the fractured wall bricks across the cracks will be removed and replaced with matching whole bricks.\nThe stress crack cavities within the walls will be filled with a special casein liquefied mortar injected into the wall cavities.\nPrice Masonry’s lead preservation mason Jimmy Price has promised to give us another of his signature restoration demonstrations using the casein injection process, so that all those who wish to learn firsthand about this masonry conservation technique may benefit from his expertise.\nWe will announce the day and time of the demonstration (sometime in May) on the Germanna Website.\nResearching “Best Conservation and Restoration Practices” for Salubria’s Upcoming Roof Restoration Work\nThe Salubria Restoration Design Team of Legacy Architecture, LLC and HITT Contracting has made recent field trips to Colonial Williamsburg’s Wythe House, James Madison’s Montpelier, George Mason’s Gunston Hall, Washington’s Mount Vernon and Historic Kenmore in Fredericksburg to observe “best conservation and restoration practices” as these institutions have applied them.\nThe roof restoration projects executed by Peter Post, Historic Roofer, at Montpelier, Mt. Vernon and Gunston Hall has provided us with exemplary models for the kind of high-quality decorative wood shingle work that we shall achieve soon at Salubria.\nThe existing decorative shingles on Salubria’s roof date back to a 1982 installation, so they are over three decades old now.\nWe have found evidence in the attic that several of the historic rafters suffered significant water damage from the prolonged rainwater leaks through the roof.\nThe Germanna Foundation has committed to repairing the damage to the Salubria’s exquisite heavy timber roof structure and to installing a new decorative cedar shingle roof this summer, from June through September.\nMr. and Mrs. Russell Hitt, and HITT Contracting, Inc. have generously made a challenge grant of $195,000 to ensure the stabilization and conservation work begins as soon as possible.\n“To everything there is a season,” Ecclesiastes tells us. So, this season – this year – is Salubria’s “time to build up.”\nThe Germanna Foundation respectfully solicits your contribution, at whatever level you can help to match the HITT Challenge Grant and counteract the destructive forces of the Mineral Earthquake. Salubria’s preservation depends now on our collective response.', 'On April 25, 2015, Nepal was hit by a shallow magnitude 7.8 earthquake centered 34 km ESE of Lamjung District. Called the Gorkha earthquake, it killed over 8,800 people and destroyed nearly 600,000 houses in rural districts near the epicenter and in the capital city of Kathmandu (UNOCHA). It also caused an avalanche on Mount Everest that killed at least 20 people and injured 120 (Washington Post) and a mudslide in the Langtag Valley where at least 250 people were reported missing (Associated Press).\nOn May 12, a large 7.3 magnitude aftershock, called the Dolakha quake, struck the region killing more than 200 and injuring more than 2,500 people in Nepal (CNN) and killing at least 17 people in India and one person in Tibet (CNN). Daily aftershocks, combined with monsoon rains, have created landslides and flooding resulting in more and more people becoming internally displaced. Hundreds of thousands of people are currently living in temporary shelters, and about 2.8 million require humanitarian assistance (UNOCHA).\nHumanity Road’s (HR’s) overall disaster response activities began on April 25th and ended on June 12th. In addition to producing general daily situation reports and conducting social media monitoring and amplification, HR was activated for five mission-specific projects.\nList of Activations for Nepal:\n- Humanity Road Disaster Response: April 25-June 12, 2015 (49 days).\n- Activation to support Kathmandu Living Labs (KLL), QuakeMap crisis-mapping and urgent needs coordination – April 28-June 12, 2015 (46 days).\n- Activation to support Americares for Hospitals and Medical needs – May 1-4, 2015 (4 days).\n- Activation to provide assistance for stranded hikers – April 26-30, 2015 (5 days).\n- Activation to provide research for a water purification project – May 7-9, 2015 (2 days).\n- Activation to assess impacts to the location of the 2014 Sun Koshi landslide – May 12, 2015 (1 day).\nBelow is a summary of each activation followed by a summary of lessons learned.\nActivation One – Humanity Road Response\nHumanity Road volunteers self-activated on April 25, 2015 immediately following the magnitude 7.8 Gorkha earthquake.\nGoals and Objectives:\nThe Disaster Response objectives were to:\n- amplify official information to enhance public awareness; information included emergency and information hotlines, areas most impacted, status of communications, status of airports, roads, basic needs, shelters, embassies, animal needs, status of hospitals, etc.\n- search for impacts and urgent needs using social media and the Internet.\n- connect communities and individuals with needs to relief organizations in the field.\n- publish and share situation and other reports on appropriate platforms in a timely manner.\n- to support partners with research and/or special reports if needed.\nThe Process Improvement objectives were to:\n- continue testing Scanigo as a twitter aggregation tool for data-mining.\n- continue testing the use of surge support teams for mission specific goals.\nSurge Support and Collaboration Partners:\n- Translators without Borders (TWB): Humanity Road requested help from TWB, which provided translators fluent in local Nepal languages, geography, and culture. These volunteers translated documents and social media posts and searched for urgent needs and situational awareness information.\n- Standby Task Force (SBTF): SBTF representatives were included in a Skype collaboration window that included TWB and HR volunteers. They brought urgent needs to the window and worked with the HR/TWB team to find solutions.\nA total of 12 Situation Reports for Nepal were published. The first report, published on April 25, included links to government websites, emergency numbers, official social media accounts reporting on the event, hospital information, utility company contacts, and information on transportation infrastructure. The following 11 daily Summary Situation Reports, published between April 26 and May 5, focused on communities in need, medical and hospital needs, communications and power, aid coordination, and resources for animals. Following the Dolakha earthquake of May 12, we also published a Situation Report on India.\nThe situation reports were published on Humanity Road’s website, ReliefWeb, the All Partners Access Network (APAN), HumanitarianResponse.info, and were shared directly via email with NGOs, including Americares and Nethope, Cisco TACops and the Microsoft Disaster Team.\nActivation Two – Kathmandu Living Labs Crisis Map Support\nOn April 29, 2015 Humanity Road began providing crisis map support for Kathmandu Living Labs (KLL). The crisis map, located at www.Quakemap.org, was an initiative led by KLL with onsite coordination support from www.NepalMonitor.org and remote support from HR. The goal of the crisis map was to match the needs of people in the impact zone with relief organizations on the ground. As of May 18, over 1,800 reports had been processed and routed to more than 50 military and civilian aid organizations, and a total of 65,462 people from 89 countries had visited the map.\nGoals and Objectives:\nHumanity Road focused on providing: surge support for overnight processing; support for incident processing; operational advice and lessons learned from previous crisis mapping projects; leveraging volunteer resources from HR, TWB, and spontaneous diaspora volunteers; and capturing lessons learned on the use of Ushahidi for routing or dispatch of incidents through resolution.\nSurge Support Partners:\n- Translators without Borders (TWB): TWB volunteers, who had been assisting HR since April 26, continued to assist with the KLL activation. They provided translations, searched for urgent needs to add to the Quakemap, and verified reports of needs by contacting people in Nepal.\n- Spontaneous Nepal and Diaspora Volunteers: after receiving numerous new volunteer applications from Nepalis living inside and outside Nepal and wishing to help, we conducted “just-in-time” training for eight spontaneous volunteers to support our work with KLL.\nReports and Activities:\n- Provided advice on categorization elements and public messaging on About page.\n- Provided introductions and guidance on collaborating with UNOCHA clusters.\n- Provided overnight support for admin tasks associated with the KLL map.\n- Researched and updated coordinates for pending (unapproved) incidents.\n- Reviewed pending comments and update approved incidents.\n- Manually updated newly added crisis map field data such as location name.\n- Provided daily 5:00am status reports on major overnight processing activities.\nActivation Three – Hospital List\nOn May 1, 2015, Humanity Road received a request from Americares for a medical assessment. They had requested baseline hospital map data from the World Health Organization (WHO) for target districts, but the data they received was missing addresses and contact information.\nGoals and Objectives:\n- Provide Americares with a medical assessment report for three districts in Nepal: Gorkha, Dhading and Sindhulpalchowk. Information was to include hospital status information, damaged items, medicine and durable goods, supply needs, fuel and generator needs, medical staffing needs and other general status information that would assist medical supply aid providers responding to the disaster.\n- Identify any local urgent needs that may need follow up.\nSurge Support Partners:\nFor this project, HR requested surge support from Montgomery County CERT (MCCERT). From May 2-3, MCCERT performed searches using Scanigo (Scanigo is a Twitter aggregation tool developed by Progeny Systems and being tested by Humanity Road).\nA report on hospital status for the three priority districts was provided to Americares. The report was also submitted to Nethope and to United Nations Office for the Coordination of Humanitarian Affairs (UNOCHA) for upload to the Humanitarian Data Exchange (HDX). In addition, the MCCERT team identified 41 urgent needs and relayed these to Humanity Road volunteers for possible submission to the KLL crisis map.\nActivation Four – Assistance for Hikers\nOn April 26, Humanity Road was alerted to a request regarding two stranded American hikers on Mt. Everest. The hikers had a cell phone and had sent a text message to a contact requesting information on an evacuation plan back to Kathmandu. HR began conducting research to determine if any search and rescue teams could help and the status of airports and evacuation efforts in the area.\nAccording to the Nepal Secretary at the Ministry of Culture, Tourism and Civil Aviation, 90 tourists were stuck at Camp 1 on Mt Everest, and 120 were stranded at Camp 2, due to a blockade of the trail after an icefall occurred above Base Camp. Injured tourists were being airlifted to Kathmandu via Pheriche and the rest via Lukla. HR posted two requests for information on the civil-military All Access Partners Network (APAN) asking for information on flights for non-injured foreign nationals stranded on Mt. Everest, but no responses were received.\nAn HR volunteer, who had previously hiked in Nepal, was able to provide geographical information to a contact for the hikers and assisted the person in understanding extraction efforts. On May 1, HR received confirmation that the hikers had reached Kathmandu safely.\nActivation Five – Water Purification System\nGoals and Objectives:\nOn May 6, Humanity Road was activated by a stakeholder organization to provide information on suitable locations in Nepal for testing a new water purification system. They requested a local point of contact to collaborate with and to assist in identifying potential locations for their deployment. The location needed a water source, no matter how contaminated (although less turbid is better than mud), and a population with unmet water needs.\nOn May 7, a two person urgent needs task team from HR provided a comprehensive report with points of contacts, available phone numbers, and links to 15 suitable locations including coordinates. They also provided information to connect with Humanitarian OpenStreetMap in Nepal, Kathmandu Living Labs (KLL), and Nepal Monitor. By the time the stakeholder’s team was on the ground and checking into the Water, Sanitation, and Hygiene (WASH) cluster, they had our report in their hands.\nTo help assess the severity of each location, the HR team assigned a point system for each site using distinct characteristics of each location, including: population density, distance to water source, organizations present, and contact information if known. As a result, the client organization’s team was able to use the locations to pinpoint a definite location to put their water systems to use.\nActivation Six – Sun Koshi Dam Research\nFollowing the second magnitude 7.3 earthquake that struck on on May 12, a member of the UK CHASE Operations Team posted a request in a Nepal coordination Skype window asking if there were impacts on the Sun Koshi Dam. A large landslide had occurred in 2014 near the dam. Humanity Road volunteers datamined in social media and on the Internet but did not find any emerging information about impacts to the dam or landslides. An update was providing to the CHASE team member.\nSummary of Lessons Learned\nIn order to collect lessons learned, Humanity Road collected information from volunteers during the response and emailed surveys to volunteers and partners and stakeholders after the response. Surveys were developed with the help of Statistics without Borders (SWB) volunteers.\n- HR has continued to improve its internal organization and volunteer coordination during responses and activations. This response revealed more training is needed on specific forms and reports and roles available to volunteers during events. An effort to further empower volunteers and improve our self-directed work team organization has already begun. It was also found that volunteers often feel they did not have enough work and are unsure whether HR has an impact.\n- Tracking information in Skype chat rooms continues to be an issue. HR will continue to work on improving project management by finding ways to better communicate instructions and tasks in other formats, such as the Event Workbook, so that important information is not lost in Skype chatter. Improved communication would also assist volunteers working in different time zones.\n- Efforts to continue to recruit and train international volunteers should continue as this allows us to have close to 24/7 coverage during responses. Volunteers in the Asia/Pacific region should also be trained to be event leaders in order to better provide coordination for events in the region.\n- HR’s team and skilled surge support partners are good at providing rapid assessment reports that contain points of contact and coordinates for specific aid deployments.\n- Overall, volunteers and partners agreed HR’s situation reports were successful, well sourced, and should continue. It has been suggested that more maps be added to the reports. Daily reports are preferred and should be published in either PDF or Excel formats. Separate reports rather than one long continuous report are more efficient and less confusing.\n- Local contacts and digital contacts should be identified and relationships nurtured before a disaster occurs. Contact information for major infrastructure (hospitals, airports, police, fire, etc) should be collected and updated prior to a disaster though it is likely that contact will be difficult immediately following an event.\n- In the immediate aftermath of a disaster event, reliable information can be lacking. Case managers must be prepared to work with the best available information and be able to seek out alternate sources that may be able to provide additional information.\n- Although local citizens are a priority in disaster response, tourists and foreign nationals should be included in preparedness and response plans. While non-injured foreign nationals will be a low priority, they can add to the burden of response resources if they are live without food, water, sanitation, or medical care for a period of time.\n- Sharing links to map files can require large amounts of bandwidth that are not always available to field deployment teams. Including coordinates in the textual portion of a report can be very helpful to teams on the ground.\n- Overall, integrating our digital response with the Kathmandu Living Labs response was unique and invaluable. Timing of the reports to KLL was well coordinated, and we were able to deliver information to Nepal aid coordinators when they needed it. Despite the volume of data that needed to be sieved through, the HR team worked well in the limited time available to clean up data while also highlighting missing needs. A disaster response questionnaire prepared for specific disasters would help to gather important information more quickly, especially contact numbers and locations.\n- It was suggested that HR should develop teams who know how to use the tools utilized in the Nepal response and who can commit to using that knowledge in similar situations.\n- During April and May Scanigo was used to aggregate and filter tweets in order to provide first responders and strategic partners with situational information that would enhance the common operating picture. Urgent needs that were identified were relayed by various means to first responders and to a crisis map launched by Kathmandu Living Labs. Scanigo filtered 1.2 million Nepal related tweets to just 4,638, representing a 99.3% reduction which significantly reduced human analysis required. A total of 150 reports of urgent needs were identified by Humanity Road volunteers. Scanigo performed well with no interruption of service and with multiple users and multiple recordings in progress.\nPositive Feedback from Partners/Stakeholders\nFrom Kathmandu Living Labs:\n“Humanity Road helped us to collect, verify, and process reports coming to the quakemap platform. Quakemap was used as one of the key information sources in Nepal’s earthquake response and relief work. In addition, they created Situation Reports periodically based on the information available in quakemap.org and other sources. The report was used by different humanitarian agencies.”\n“This was the first time I worked with Humanity Road directly. I found them open and highly professional. I am impressed with their practical advice and inspired by their passion in helping people in crisis. They were always accessible in Skype, emails etc. when I needed them. It was great experience working with them.”\nFrom another stakeholder:\n“I’m a medical doctor and I lead disaster response teams. I’ve led teams for the US Navy, FEMA, the Roddenberry Foundation, and others in Iraq, Afghanistan, Katrina, Port-au-Prince, Banda Aceh, Tacloban, Kathmandu, Izmit, and more. Over the past several years I’ve come to expect to see Humanity Road at work by the time I arrive in any event and that’s been consistently so, including the most recent Nepalese Earthquake. Cat and her team were coordinating communications between dozens of organizations inside the first three days after the earthquake and they continued through my own ten-day team deployment there and back. I was able to ask her team questions while we prepared for deployment and get answers relayed from experts within hours. I was able to give her updates from our work in the foothills of the Himalayas and her team relayed our status and our needs to other teams within minutes. I now consider Humanity Road an integral asset for us during responses and I seek them out before I go anywhere. One of the most valuable attributes any of us can ask for is professional reliability and Humanity Road is there, every time. Speaking for my own teams, remembering far-away nights in the dark linked only by satellite phone, Cat and her volunteers have our deep thanks”']	['<urn:uuid:41dc9641-814f-4f30-85cf-257d10858c49>', '<urn:uuid:ccd3a61a-11f2-4608-8939-c620239a85a8>']	open-ended	with-premise	long-search-query	distant-from-document	comparison	novice	2025-05-12T10:19:05.071190	21	105	3734
63	compare equity return cap rate calculation methods investing real estate	There are two main methods for calculating returns in real estate investing. The first method involves calculating ROI by dividing the property's net income (gross income minus costs) by the total costs. For example, if a property cost $50,000 and generates $6,000 in yearly profit after expenses, the ROI would be 12%. The second method is the Band of Equity Investment Method, which determines value while considering financing terms. This involves combining the mortgage component (Loan To Value x Mortgage Constant) with the equity component (Down Payment Percentage x Desired Rate of Return) to derive a CAP rate that ensures achieving the desired return on investment. For instance, with 80% mortgage at 7.99% and 20% down payment targeting 11% return, this would yield an 8% derived CAP rate.	['Money in Your Pocket: Calculating Your Own ROI from Your Properties\nSaturday Nov 09th, 2019Share\nAs all investments go, every investor seeks to make the best returns possible. Return on investment (ROI) is an assessment of an investment’s cash flow (how much did you put in, and how much are you getting back in return?). Experienced investors know that calculating the ROI from their properties is vital to determining how well their investments are performing overall.\nDespite there being many ways in determining a property’s ROI, calculating the ROI can be as simple as using a fairly standardized formula. The ROI of a property can be equal to its annual profits, determined after its expenses, divided by the cost of the investment.\nROI = net income (gross income – costs) ÷ costs\nFor example, a property may have cost $50,000 to acquire. This property generates a profit of $6,000 per year, minus all of its yearly expenses. Thus, the ROI of this property is 12%. In short, one of the ways of calculating the ROI of a property is to divide the property’s net income by the amount invested in the property.\nHowever, determining the ROI of a property involves calculating both the income and cost of the investment. To do this, you need to consider a number of different financial factors that contribute to a property’s ROI.\nWhat factors affect the ROI of a real estate investment?\n- Debt (mortgage or loans)\n- Tax and insurance\n- Maintenance and rehabilitation\n- Rental income\nTo calculate the ROI of your properties, you first have to determine the total cost of your investment. While some investors may have paid out-of-pocket when purchasing a property, others may have received financial assistance in the form of mortgages and loans.\nOne of the biggest costs of a property investment is any debt paid or to be paid throughout the course of your investment. Be it a monthly mortgage payment or loan, these financial expenses are crucial to calculating your ROI, since these payments are deducted from your property’s gross income to determine the net income.\nThings like the amount that the bank has provided you, the term of the loan, and its interest rate are among the important considerations that need to be made when calculating the net income. Knowing these particular details can accurately calculate the ROI of your property as time passes.\nTax and Insurance\nProperty tax and insurance are important factors in determining a property’s ROI. This is due to taxes and insurance being among the numerous expenses that need to be accounted for when calculating ROI.\nBoth property tax and insurance are similar in the sense that they vary between property, city, and province. They are two expenses whose determination involves a third party—be it an insurance company or a provincial property assessment organization. For example, calculating your annual property tax requires an assessment from a provincial organization to determine the value of your property which is then multiplied by the total tax rate, while an insurance rate is determined by a particular insurance company.\nMaintenance and Rehabilitation\nThroughout your many real estate investments, you’ve probably come across repairs or undergone renovations. From fixing faulty plumbing to rewiring shoddy electrical work, repairs are a common occurrence for investors—especially those who use house flipping as a way to generate quick profits. Professional repairs aren’t necessarily cheap; and depending on the amount of repairs needed for a specific property, this expense can be quite costly.\nRenovations, on the other hand, are typically used across all investors who seek to increase the value of a property with the intention of selling it at a higher price. Rehabilitation encompasses the various expenses, including repairs and renovations, that are used to ‘rehabilitate’ the property and increase its value.\nOn top of that, electrical and plumbing systems need to be routinely maintained to ensure that amenities remain functional. It is for this reason that maintenance and rehabilitation costs need to be considered when determining a property’s ROI.\nAs all property owners know, properties can change in value over time. Ideally, any real estate investor would want to build equity within their properties, which is done as investors pay off their loans or mortgage and as the market value of their property increases.\nThis is obviously crucial to investors, since the higher the equity, the more money investors are able to use for future investments. Property equity is a critical factor to determining a property’s ROI, as it is a result of paying off loans which attributes to the increased income of a property.\nAnother method, known as the cost method, involves calculating a property’s ROI by dividing the equity of a property by its costs. However, property equity must first be calculated. This is done by subtracting the total cost of the property from its market value.\nFor example, a property may have been acquired for $300,000. Renovations and repairs cost investors roughly $25,000, which increased the market value of the property to $400,000. With the total cost of this property being $325,000, the equity would be worth $75,000.\nTherefore, the ROI can be calculated using the cost method by dividing the property’s equity by its cost. In that case, the ROI in the above example would be calculated as follows:\nROI = $75,000 (equity) ÷ $325,000 (cost) = 23%\nTenants are typically a property’s main source of income. The amount of tenants that are renting out your property, the amount you charge your tenants, and the frequency at which you charge them—be it monthly or annual—all contribute to your property’s income, which affects its ROI. Thus,having an occupancy rate that is close to 100% with each occupant helping you generate a suitable monthly or annual income is critical to receiving a better ROI and increasing profits.\nWhile there are a myriad of ways to calculate ROI, there are various factors that need to be considered to accurately determine it. Luckily for you, there are numerous online resources, like this ROI calculator for real estate property, that help provide experienced and novice investors alike with the information that they need.\nOriginally posted from https://blog.remax.ca/calculating-your-own-roi-from-your-properties/', 'Determining the value of an apartment building investment is one of the greatest difficulties that many new commercial real estate investors face.\nMost people who invest in apartments have some experience investing in other types of real estate, typically residential homes or duplexes and triplexes. The issue that new investors face is the fact that apartment buildings are valued by different methods than residential real estate. In fact, it is usually quite easy to find the fair value of residential estate using a comparative sales approach. The comparative sales approach simply uses the existing sales prices of similar residential properties in that particular area and determines value based on an average sales price of comparable properties. This should be very straight forward.\nHowever, commercial real estate investors and appraisers use a variety of appraisal methods to determine the fair market value of an apartment building. These new methods should not deter the new investor because once they are understood, they actually will help tremendously to locate the best apartment building for acquisition.\nDetermining CAP Rate\nThe first unfamiliar term that a new apartment building buyer will encounter is the capitalization rate or CAP rate for short.\nAs the new investor is searching for an apartment building, his Realtor will supply him the CAP rate of the property. The CAP rate is a measure of the income produced by an apartment building divided by the cost of the building. For example: If an apartment building is purchased for the price of $1,000,000.00 and the property produced an annual net operating income of $100,000.00, the CAP rate of the property is 10%. (Net operating income is gross rents minus expenses.)\n$100,000.00 (Net Operating Income) / $1,000,000.00 (Purchase Price)\n= CAP Rate: 10%\nAn investor can also use the CAP rate to determine the maximum price he can pay for a property when he knows what the net operating income is.\nFor example, if the investor is looking at an apartment building that is seeing a net operating income of $150,000.00 and he wants to see a CAP rate of 11%, he can determine the maximum purchase price as follows:\n$150,000.00 (Net Operating Income) / 11% (CAP Rate)\n= Maximum Purchase Price: $1,363,636.00\nThis simple formula to devise the capitalization rate (CAP rate) of an apartment building is limited, however. The simple CAP rate assumes that the investor will be purchasing the property for cash and does not take into account the financing terms that will affect the investor’s rate of return on the building. In other words the simple CAP rate is good number to use when comparing apartment buildings as potential investments, but a little bit more analysis is necessary to determine exactly what the true rate of return will be on a particular building when using financing to purchase the property.\nThe Band of Equity Investing Method\nThe goal for the individual investor is to determine what the property is worth to him or her. In other words, the investor should only be concerned with paying a price for the property that allows him to realize his sought after rate of return. The best way that I have found to determine the investment value of an apartment building is to use the “Band of Equity Investment Method.”\nThe “Band of Equity Investment Method” of determining value will tell you the maximum price that you can pay for your apartment building and still realize the rate of return that you are looking for. The greatest advantage of this valuation formula is that it takes into consideration the terms of financing that the investor is using to purchase the property. Thankfully, this method is not that complicated, and it merely requires that you know some financial information about the property and the terms of the financing that you will using.\nHere is how the “Band of Equity Investment Method” is figured:\nMortgage = Loan To Value of Mortgage x Mortgage Constant\nProperty = Down Payment on Property (as a percentage) x Desired Rate of Return\nAnd with a real life example:\nMortgage = 80% (.80) x 7.99% (.0799) = 0.06\nEquity = 20% (.20) x 11% (.11) = 0.02\nCap Rate = 0.08 = 8.0%\nWith this new “derived” CAP rate you can now determine your maximum purchase price for any apartment building and ensure that you will be realizing at least an 11% rate of return on your investment.\nFor example, you are out looking at 14 unit apartment building with your realtor and he tells you that the net operating is $150,000.00. You know that your bank will give you a 30 year loan at an interest rate of 7.99%. You know that you need to see at least an 11% return on your investment. You simply divide $150,000.00 by your derived CAP rate of 8% and you get the price of $1,875,000.00. You know that you can purchase the building with a 20% down payment and a 30 year loan at 7.99% and still realize a net return of 11% on your investment.']	['<urn:uuid:a377b8eb-fec6-4483-957b-6e2c32d2bf1b>', '<urn:uuid:989d8948-e0ef-4b58-ba20-b580b3125e37>']	open-ended	with-premise	short-search-query	distant-from-document	comparison	novice	2025-05-12T10:19:05.071190	10	128	1872
64	content creator here need to know what royalty free vs rights managed image licenses allow and what types of website content sections boost seo	Royalty free images require a one-time fee for unlimited ongoing use, while rights-managed images have specific time periods, usage restrictions and are usually exclusive to one company. For website content sections that boost SEO, key elements include detailed 'About Us' pages showcasing company information, product/service pages with comprehensive descriptions, and regularly updated blogs which serve as news portals. All content should be original and avoid copy-pasting to maintain good search rankings.	['Kenya Website Web Design Content Creation Guideline\nTable of Contents\nThe following website titles provide you with a general idea of the outline that you can follow when providing me with the information I need to design your website.\nFeel free to add or remove any heading as you see relevant to your own website specifications.\nAvoid shortening anything as I transfer the information to your website exactly as you will have provided.\nBe as detailed as you can because original website content is key if you want Google to rank you highly in their search results. The more the website content, the higher the likelihood that a search term existing on your specific website will be found on Google by other users.\nAvoid copy pasting content from any online website as this may hurt your website rankings on Google for your key words.\nWhen coming up with the website content, ask yourself the following questions:\n- Have I said everything I could?\n- Have I overcome all objections?\n- Have I showcased the product or service?\n- Is the key word usage seamless?\n- Does the copy encourage the next conversion step?\n- Have I connected with my reader?\nIf your answer is “yes,” you’ve done your job.\nIf you need specialized help on optimizing your website information you may take up Kenya website’s SEO services by clicking https://kenyawebsite.com/seo.\nFill out the following website information in the exact wording and order you want it to appear on your website. You may add, remove or reorganize the arrangement of the headings/menu items/pages if necessary.\nWebsite Logo: Send me your logo if you have one. If you do not have one, I will design a unique customised logo for your business.\nThis is a summary of everything that appears on your website.\nThe information here should be focused on what you want your website visitor/customers/clients to see without having to go to other pages to find the information; or so that they can actually go to other pages on your site to find the information.\nThis depends on whether you are going for a one-page website design or a website design with other pages in form of menu items.\nYou may provide the following information:\nWho we are: General description of your company in summary form e.g company overview, history, what we do, location, general information, faq…etc\nThe homepage can have a slideshow of pictures relating to your website if you want.\nSpecify each slideshow image with the description to go with it if desired.\nThis is usually the second page the website visitor looks for to get additional information about the company/business/organisation/person that introduced himself/herself/itself in the homepage.\nBe as detailed as possible but at the same time limit the information shared in this page to only what the website visitor will be interested in knowing/finding out about the company/business/organisation/person. A good question to ask yourself in coming up with the about us page is “how is this information about us going to be a selling point to my website visitor/potential client?” You may provide the following information:\nWho we are, Why Choose Us, Our Advantages, Mission, Vision, Goals, Objectives, Principles, Standards, Success stories…etc\nThis page focuses on what you actually do. You may provide the following information:\nAll the products/services/activities you offer.\nThis section is especially relevant to e-commerce/shopping websites as the bulk of the website content is found here.\nA website blog enables you to be constantly updating your website visitors/clients about new/existing/updated products/services/activities. Look at it like a news portal for your website as blogs are websites that maintain an ongoing chronicle of information.\nA blog is the first step to ranking your website content on search engines. This is especially necessary if you have a website with fixed content but you still need an avenue to make your website interactive, updated and alive.\nI can design a basic blog for your website for free. You also have the option of having a professional advanced blog designed and linked to your main website charged at Ksh. 15,000.\nLeave this one out if you do not need one or specify if you need either the free or professional blog bundled with the design.\nWhat other people/clients/customers are saying about your products/services/activities/comments from satisfied customers/clients.\nProvide their name, position in the company/society, phone number, picture…etc, if they have given you permission to use their identity.\nYou can leave this one out if you do not have testimonials.\nA good starting point is to register your business on Google Business (https://www.google.com/business/) and Yelp (https://www.yelp.com/) to get people to review your business. Such reviews can then be showcased on your website. This is a great marketing option for your business and gives it online credibility.\nAll your contacts:\n- Phone number:\n- Email address:\n- Postal address:\n- Your physical location: Use Google map’s pinpoint gps system by clicking www.maps.google.com to find your exact gps coordinates. Provide me with the location and coordinates to your business location e.g Central Business District, Nairobi, -1.290693, 36.821946. A google map will be embedded on your site for visitors to easily find you.\n- Social media links: Facebook Page, Instagram, Twitter, YouTube, LinkedIn, Tumblr, Pinterest. Make sure you create all of these accounts if possible, to secure your online identity on these platforms. You do not have to use them but creating them locks out potential competitors from using your brand name on social media platforms disguised as you. Specify if you need assistance on creating the social media accounts. These accounts will be automatically linked to your website so that anything posted on your website gets automatically sent to your social media accounts.\nYou may specify the website design colour, i.e. the general website colour, headings colour, sub headings colour, body text colour, site background colour, logo colour etc. Colours have names. You can find the perfect colour for your website by clicking https://kenyawebsite.com/graphic-design/colours/\nfor a complete list of colours and their names. Find the colour you want then share the colour name here for me to use it. e.g. the general website colour for my website is medium turquoise.\nIf you have pictures/videos to be used on the website, specify the name of the picture/video in brackets next to your information in this document. (Do not drag and drop the pictures on this document but send them separately with their correct name for identification).\nTry to get your own pictures where possible.\nYou can however get pictures that are free from copyright restrictions. This means that the said pictures can be copied, modified and distributed for commercial purposes without permission from the owner.\nTo get such pictures you will Google ‘free stock photos’ to get websites offering such free photos for your personal and commercial usage without liability.\nClick Web Design Kenya Pdf to download our editable web design guideline and add your website content.', 'Finding free stock photos isn’t always easy. If you don’t have a go-to source for quality, free stock images to use, you could find yourself browsing the internet for an hour to find an image with the appropriate licensing and photo content.\nAlong the way, you’ve probably heard different terms that describe stock photos: Royalty free, images for commercial use, rights-managed, and free stock photos. It’s not immediately clear what these terms mean or what their corresponding licenses require if you use them.\nIf you’re using an image on behalf of a business–whether it’s on your Facebook page, a company blog, or a tri-fold brochure–you absolutely have to know how you’re allowed to use the image, or if you’re allowed to use it commercially at all.\nWe’ll explain the difference between royalty free, commercial, and free stock photos in this post. We’ll also talk about copyright infringement, when businesses are allowed to use a stock photo, different image licenses available from Creative Commons, and how image creators can track down illegal use of their images.\nWhat are royalty free images?\nIf an image is royalty free and you want to use it, this is what happens:\n- You pay one time to have unlimited access to a stock photo – no royalty fees\n- The images can be used over and over as long as the original license terms are followed\n- Some companies impose a number of permitted uses, which can limit the number of times you can use the stock photo. For example, one company allows 499,999 uses before requiring an extended license.\nWhat are rights-managed images?\nAn image with a rights-managed license will also include specifications about how long the image can be used, how many permitted uses are allowed, and any included exclusivity.\nMost rights-managed images are exclusively offered to the business or organization that first purchases the temporary rights to them, enabling a company to use never-before-seen images in their marketing and advertising content and materials.\nThe difference between royalty-free and rights managed\nRoyalty free images are purchased with a one time fee and can be used over and over, only requiring an extended license if the images are to be used more than 500,000 times or so. No royalties are owed to the image owner for use of the image; the one time fee covers your use of the image after that.\nRights-managed images include exact time periods of allowed use, circumstances of use (for example, may not be placed in “sensitive” advertisements about companies that sell products related to mental health or disorders, sexual preferences or issues, etc.), and who can use the image other than you. Most rights-managed images are exclusively licensed to one company.\nWhat does ‘Editorial use only’ mean?\nIf an image says it’s available for editorial use only, that means it can’t be used for anything that advertises, markets, suggests, or promotes your business (or any other). Editorial photos might include components that have not been licensed for commercial use–paintings, brand logos, etc. in the photo can be the reason–and so using them for any business-related purpose is a direct violation of the license terms.\nEditorial use covers non-commercial settings like:\n- Newspaper articles\n- Educational blogs\n- Personal blogs\nThe photo can’t be used for commercial purposes like advertising, marketing, or any promotional activities. This is because editorial photos feature images of people or things that have not been licensed for commercial use. But these images can be used in non-commercial settings, like newspaper articles, educational blogs and magazines.\nWhat is a Creative Commons license?\nCreative Commons offers licenses that cover copyright terms. They allow content creators to decide which rights to reserve and which to offer for free or for a fee to customers.\nThese license can include restrictions such as whether or not an image can be used for commercial purposes, whether or not an image can be modified, and whether or not the photographer or image creator requires attribution or credit.\nWhat is the difference between personal use and commercial use?\nPersonal use is defined as the use of an image that does not result in and is not intended for commercial gain. Personal use examples include:\n- Personal blogs\n- Newsletters for family and friends\n- Wedding announcements\n- Birth announcements\nCommercial use includes use of images for commercial reasons. That includes promotions, endorsements, advertising, and merchandising for a business (yours or otherwise).\n- Branded company website\n- Tri-fold brochures\nStock photo copyright infringement\nCopyright infringement is a violation of the creator’s rights to an image. Examples of imagery copyright infringement may include:\n- Use of an image without permission\n- Use of an image outside of license permission\n- Modifying an image without permission\n- Having an image recreated by another photographer or designer\nCan I find free stock images using Google search?\nNo. Google search isn’t a reliable way to find free stock images. Even though you can select image permissions and licensing options in the “More” tab on search, the images that show up can still have special requirements that may not be apparent on the search screen.\nIt’s more reliable to use a website that compiles stock photos for you to browse and search. These images will have a readily apparent tag that notifies you what the license allows and prohibits, and if attribution is required.\nHow can image owners find their material being illegally used?\nImage creators and photographers can find when their material is being illegally used online and protect their rights as creators. Imagery can be tracked, even if it has been modified, recreated, or used only in part.\nIf a copyrighted image is used without the proper permissions and attributions (as it would if you downloaded it from Google search results and used it without permission), the image gets flagged so the creator can check the image and ensure it’s being used legally.']	['<urn:uuid:90d08903-34cb-44d1-a900-7ef6947d37e9>', '<urn:uuid:ac40c86f-7a81-4c95-8dce-7102f0ac329e>']	factoid	with-premise	long-search-query	similar-to-document	multi-aspect	expert	2025-05-12T10:19:05.071190	24	71	2139
65	What can Cogmed do for my memory?	Cogmed is an evidence-based online cognitive training program that improves working memory, which is the ability to use and process information from short-term memory. It has an 80 percent success rate in strengthening cognitive function through consistent reinforcement and personalized support.	['Systemic illnesses, traumatic brain injuries, and diseases of the brain and nervous system can affect the way a person feels, thinks and behaves. Neuro-cognitive deficits can manifest as disorders in attention, memory and executive function (ability to organize one’s thoughts, speech and actions).\nThese deficits can be highly disruptive to the life of those suffering from them. The disabling effects of changes in cognitive abilities may cause psychological adjustment issues with depressive or anxious symptoms.\nIf you suspect that you or a loved one may be suffering from a possible neurological disorder, consultation with a neurologist is vital to determine the nature of the symptoms and the diagnosis. The findings of the neuropsychological evaluation are an integral part of the diagnostic procedure. After determination of diagnosis, the neuropsychologist and rehabilitation therapists may be called on to design a cognitive rehabilitation program to improve cognitive, emotional, behavioral function affected by these conditions.\nAlzheimer’s disease. Alzheimer’s disease and other types of dementia can interfere with memory, personality, and cognitive abilities. An exam may help determine if there is dementia and answer what type it is and at what stage.\nStroke. A stroke can affect behavior, thinking, memory, and other brain functions. Some changes are obvious; some are subtle. An evaluation helps identify stroke impairments that persist.\nTraumatic brain injuries. Injuries to the brain can cause a wide variety of symptoms. A neuropsychologist can help determine how an injury affects functions like reasoning or problem solving skills.\nEncephalopathy. There are multiple causes of brain damage some may be reversed or modified. They include infections, anoxia, toxins, drugs, liver or kidney failure.\nMild Cognitive Impairment (Minor Neurocognitive Disorder). MCI is a syndrome where cognitive decline is greater than expected for an individual’s age and education level. However, functional ability to perform daily activities is generally intact. There may be subtle changes in your ability to participate in daily activities but these tasks are done independently. Perhaps you now rely on strategies and memory aides such as lists, calendars and planners, or post-it notes. Its identification can lead to preventive treatment by controlling risks such as systolic hypertension. Some individuals remain stable, others my return to normal while others may experience further cognitive decline.\nParkinson’s disease. The same brain changes that cause problems with movement lead to neuropsychological impairment. Individuals with Parkinson’s find that they are easily distracted and disorganized. They find it difficult to plan and complete tasks and may feel overwhelmed. Speech volume may be difficult to maintain and it may be hard to find the words to express one’s self. These changes may simply feel annoying but they may also seriously interfere with getting things done. Frustration, discouragement and anxiety can be triggered. These symptoms can be evaluated and addressed in treatment.\nNormal Pressure Hydrocephalus. Although this condition may go unrecognized or mistaken for other disorders, it is potentially treatable with symptom relief if caught early. In this condition the normal flow of cerebrospinal fluid becomes blocked. The fluid gradually builds up in the brain’s ventricles or cavities causing increasing pressure. Symptoms that occur are progressive cognitive impairment, problems walking and loss of bladder control. Problems with cognitive changes can be evaluated as well as treated.\nMultiple Sclerosis. People with MS are also likely to experience cognitive changes. While these changes may flux and flow, with some individuals problems become severe enough to interfere significantly with everyday activities. These concerns should be discussed with the treating physician. Problems with attention and multi-tasking can arise that require careful assessment and treatment by a neuropsychologist.\nCognitive rehabilitation therapy (CRT), is a type of treatment designed to improve neurocognitive abilities such as attention, working memory, cognitive flexibility and planning as well as to improve social functioning. The goal is to maximize the capacity for independent living and, in some cases, the person may resume usual activities. Working with the therapist, an individual treatment plan is developed to address specific areas of cognitive impairment and their impact on day-to-day activities.\nThe objective is to reinforce, strengthen or re-establish previously learned adaptive behaviors and/or to establish new patterns of cognitive activity and mechanisms to compensate for impairment.\nThe primary aim of cognitive rehabilitation is to optimize the capacity to function thereby improving quality of life. There are many computer-based brain-training tools available on the Internet. However, the goal of cognitive rehabilitation is broader. Its chief purpose is to promote mastery of skills and discipline that can be used to cope with changing real-life demands. Therapy aims at heightened self-awareness of strengths and weaknesses in order to effect change and to achieve realistic goals. Knowing and understanding clearly what situations may demand and anticipating problems in advance allows the individual to develop an effective plan of approach. Evidence-based methods are employed to attain the best and most satisfying outcome.\nCognitive rehabilitation is not meant to replace medical treatments or psychotherapy but rather to complement their effects. In fact, interdisciplinary therapy, integrative medicine and collaboration produce the best outcome.\nAt Montclair Memory Clinic, expert presenters conduct a tutorial of the social, psychological, medical, legal and financial needs of patients and their families. What’s more, we provide caregiver support and instruction. This includes:\nCogmed is an evidenced-based online cognitive training program that improves working memory (WM). WM is the use and processing of information received from short-term memory. It allows the brain to juggle several concepts or pieces of information at one time and decide what action to take. Cogmed is designed to improve overall quality of life by successfully addressing difficulties with WM, especially in individuals whose inattention from poor WM interferes with everyday activity.\nCogmed was created to be complementary to other treatments for those who struggle with WM. With an 80 percent success rate, Cogmed is fundamental cognitive training that strengthens function by providing consistent reinforcement, personal support and a finely-tuned difficulty level.\nAt Montclair Memory Clinic, we are a Cogmed certified practice with coaches trained to effectively execute the Cogmed training program.\nFor more information on our supplemental Cogmed coaching or to schedule an appointment, contact us today.']	['<urn:uuid:1a4f7898-b81b-42ce-b297-7eb5344f4040>']	factoid	direct	concise-and-natural	distant-from-document	single-doc	novice	2025-05-12T10:19:05.071190	7	41	1012
66	For professional video editing, which RAM is better and why?	DDR4 3200 is better than DDR4 2666 for professional video editing because it has tighter timings, which results in faster rendering times and shorter import/export times. This is particularly beneficial when working with high-resolution footage.	['When it comes to DDR4 memory, there are two common types that are often compared: DDR4 2666 and DDR4 3200. These two types of memory have different speeds and can affect the overall performance of a computer. In this article, we will explore the differences between DDR4 2666 and DDR4 3200 and which one would be a better choice for your needs.\nDDR4 2666 vs DDR4 3200: What’s the difference?\nThe key difference between DDR4 2666 and DDR4 3200 is their clock speeds. DDR4 2666 has a clock speed of 2666MHz, while DDR4 3200 has a clock speed of 3200MHz. Clock speed refers to the speed at which data is transferred from the computer to the memory and back.\nAnother difference between the two types of memory is the timings. DDR4 3200 has tighter timings than DDR4 2666, which means that it can move data more quickly. Tighter timings generally translate to better performance, especially in applications that require a lot of memory access, such as gaming and video editing.\nApart from the above, DDR4 3200 also facilitates overclocking of the memory which enables to achieve even faster speeds than the native 3200MHz as compared to DDR4 2666.\nDDR4 2666 vs DDR4 3200: Which is better for gaming?\nWhen it comes to gaming, faster clock speeds and tighter timings can lead to better performance. In general, DDR4 3200 is better for gaming than DDR4 2666. This is because it can move data more quickly, which results in faster load times and smoother gameplay.\nFor gamers who are looking to build a high-performance system to play the latest games, DDR4 3200 would be the better choice. However, for gamers who are on a budget, DDR4 2666 could be a good option.\nDDR4 2666 vs DDR4 3200: Which is better for video editing?\nVideo editing is another task that requires a lot of memory access. When it comes to video editing, DDR4 3200 is generally better than DDR4 2666. This is because tighter timings can result in faster rendering times and shorter import/export times.\nFor professional video editors who work with high-resolution footage, DDR4 3200 would be the better choice. However, for casual video editors, DDR4 2666 would be sufficient.\nDDR4 2666 vs DDR4 3200: Which is better for everyday use?\nFor everyday use, the difference between DDR4 2666 and DDR4 3200 may not be noticeable. Both types of memory provide fast and reliable performance, so the choice comes down to personal preference and budget.\nIf you use your computer for browsing the web, streaming videos, and other basic tasks, DDR4 2666 would be sufficient. However, if you use your computer for more demanding tasks, DDR4 3200 could provide a noticeable performance boost.\nIn conclusion, DDR4 2666 and DDR4 3200 have different clock speeds and timings that can affect the overall performance of a computer. DDR4 3200 is generally better for tasks that require a lot of memory access, such as gaming and video editing. However, for everyday use, DDR4 2666 would be sufficient. The choice between the two comes down to personal preference and budget.\nKeywords: DDR4, DDR4 2666, DDR4 3200, clock speeds, timings, gaming, video editing, performance, everyday use, budget.']	['<urn:uuid:aa6d9f87-3d88-4c98-b854-f919b005b331>']	open-ended	with-premise	concise-and-natural	similar-to-document	single-doc	expert	2025-05-12T10:19:05.071190	10	35	530
67	preharvest postharvest fungicide control gray mold	Gray mold control requires both pre and postharvest fungicide treatments. Preharvest fungicides like thiram and ziram applied near harvest provide initial control. For postharvest treatment, drench applications of Mertect (thiabendazole) before storage are effective against wound infections. Additionally, two newer fungicides, Penbotec (pyrimethanil) and Scholar (fludioxonil), can be applied either as drench treatments or online sprays. Pristine is particularly effective when applied preharvest, but should be limited to four applications per season to prevent resistance development. These fungicide treatments should be combined with orchard sanitation and proper harvest management for optimal control.	"[""Gray mold is a common postharvest disease on apples and pears wherever these fruits are grown worldwide. This disease can cause significant losses on both apples and pears during storage. Losses as high as 20-60% due to gray mold are not uncommon after an extended period of storage, particularly on fruit that were not treated with fungicides prior to storage, because gray mold has the ability to spread from decayed fruit to surrounding healthy fruit through fruit-to-fruit contact during storage.\nSymptoms:Gray mold originates primarily from infection of wounds such as punctures and bruises that are created at harvest and during the postharvest handling process. Gala apple fruit at harvest tend to split at the stem bowl area, which provides an avenue for B. cinerea to infect the fruit. Stem-end gray mold is common on d’Anjou pears and also occurs on apples (Fig. 2). Calyx-end gray mold has been observed on pears in the Pacific Northwest but generally is not very common. The decayed area appears light brown to dark brown and color is similar across the decayed area. The decayed area is spongy, and diseased tissue is not separable from the healthy tissue, which is different from blue mold (a soft decay) (see the comparison in Table 1). Under high relative humidity conditions, fluffy white to gray mycelium and grayish spore masses may appear on the decayed area. The internal decayed flesh appears light brown to brown at the margin. Generally, gray mold does not have a distinct odor, but in advanced stages decayed apples may have a “cedar-like” smell. In advanced stages, the entire decayed fruit may appear “baked” and eventually may turn softer than in the early stage. In the early stage of symptom development, gray mold and Phacidiopycnis rot on d’Anjou pears are very similar, but the fruit flesh at the margin of Phacidiopycnis rot appears translucent and water-soaked, whereas internal decayed flesh of gray mold usually appears brown (see Table 2 for comparison).\nCausal Organism:The causal agent of gray mold is Botrytis cinerea Pers., teleomorph Botryotinia fuckeliana (de Bary) Whetzel. Isolates recovered from decayed apple and pear fruit vary greatly in sporulation on potato dextrose agar. Some isolates sporulate abundantly, whereas others produce only sclerotia or sclerotia with scarce conidia. Mycelium of B. cinerea grows at temperatures as low as -2ºC. Conidia can germinate at 0ºC.\nOccurrence:Botrytis cinerea colonizes organic matter in the orchard. Conidia of the fungus are dispersed mainly by air currents and water splash. Gray mold of apples and pears primarily originates from infection of wounds that are created at harvest and during the handling processes. B. cinerea can infect stems of d’Anjou pear fruit and causes stem-end gray mold. d’Anjou pears have thick stems. At harvest, pear fruit are detached from the trees and fresh wounds at the stem are exposed to potential contamination by B. cinerea. B. cinerea is able to grow at pear storage temperatures of -0.5 to 0ºC, move through the stem of the fruit to reach the fruit flesh after a period of time in cold storage, and then cause decay. The fungus can also invade stems of Delicious apples and cause stem-end rot. On pears, it has been shown that incidence of gray mold is correlated with spore levels on the fruit surface of d’Anjou pear fruit. B. cinerea has been reported to invade floral parts of pear fruit during bloom, causing calyx-end rot during storage, but calyx-end gray mold is not very common on apples and pears grown in the semi-arid climate of eastern Washington State. Secondary infection of fruit through fruit-to-fruit contact during storage is commonly seen after a long period of storage and can cause significant losses. Mycelial growth and secondary infection through fruit-to-fruit spread is enhanced by high relative humidity. Fruit infected by gray mold due to secondary infection in storage bins may not have visible symptoms or lesions are very small at the time of packing, and thus infected fruit may be packed but symptoms develop on packed fruit during storage or transit.\nControl:Orchard sanitation to remove decayed fruit and organic debris on the orchard floor helps reduce inoculum levels of B. cinerea in the orchard. Good harvest management to minimize punctures and bruises on the fruit skin helps avoid decay from wound infections. Gala apple fruit tend to split at harvest. Harvesting Galas at the right maturity to minimize splits at the stem bowl would help avoid gray mold from infection at that site. Preharvest fungicides such as thiram and ziram applied near harvest provide some control of gray mold. Research is in progress to evaluate more effective fungicides as preharvest treatments for control of postharvest gray mold. The vast majority of B. cinerea isolates from apple-related sources in Washington State are sensitive to thiabendazole. A postharvest drench treatment with Mertect (thiabendazole) applied prior to storage is effective to control gray mold, particularly for those that originate from infection of wounds. In 2004, two new postharvest fungicides, Penbotec (pyrimethanil) and Scholar (fludioxonil), were registered as postharvest treatments for control of postharvest diseases of pome fruits in the U.S. Both fungicides are labeled for use as either drench treatments or online sprays. It has been shown that both fungicides applied as prestorage treatments are effective to control gray mold from wound infections.\nPhoto Plate: Gray MoldFig. 2. Symptoms and signs of gray mold (Botrytis cinerea) on apples and pears.\nA: Gray mold originating from infection at the stem-bowl crack of a Gala fruit; decayed area brown, spongy to firm\nB: Early stage of gray mold originating from fruit-to-fruit spread\nC: Advanced stage of gray mold; white to gray mycelium covering the decayed area under high humidity\nD: Gray mold originating from infection at stem or stem bowl; gray spore masses at the stem bowl area\nE: Gray mold originating from wound infection on a d''Anjou pear fruit\nF: Stem-end gray mold commonly seen on d'Anjou pears; white to gray mycelium\nG: Sclerotia of formed on the surface of a decayed d'Anjou fruit at advanced stage\nH: Gray mold originating from calyx infection of a d'Anjou fruit\nI: Nesting of gray mold due to fruit-to fruit spread during storage; white to gray mycelium\nTable 1. Comparison between gray mold and blue mold\n|Characteristics||Gray mold||Blue mold|\n|Texture||spongy or firm; decayed tissue not separable from the healthy tissue||soft, watery; lesion with a sharp margin; decayed tissue completely separable from the healthy tissue, leaving it like a “bowl”|\n|Color of decayed area||light brown to dark brown||light tan to dark brown|\n|Signs of pathogen||fluffy white to gray mycelia; sporulation under high humidity; gray to brown spore masses; black sclerotia may form||white mycelia and blue or blue-green spore masses; sporulation often starts at the infection sites (wounds)|\n|Color of internal flesh||light brown to brown||brown|\n|Odor||generally not detectable||earthy, musty|\nTable 2.Comparison between gray mold and Phacidiopycnis rot on pears\n|Characteristics||Gray mold||Phacidiopycnis rot|\n|Texture||spongy or firm; decayed tissue not separable from the healthy tissue||spongy; decayed tissue not separable from the healthy tissue|\n|Color of decayed area||light brown to dark brown; color similar across the decayed area||initially watersoaked, then light brown to brown, later black; color varies with age|\n|Signs of pathogen||fluffy white to gray mycelia; sporulation under high humidity; sclerotia may form||white mycelia under high humidity; black pycnidia form on decayed fruit at advanced stages but often are immature under commercial cold-storage conditions|\n|Color of internal flesh||light brown to brown at the margin||translucent, clear at the margin|\n|Odor||generally not detectable||mild, distinct|\nColey-Smith, J. R., Verhoeff, K., and Jarvis, W. R. 1980. The Biology of Botrytis. Academic Press, 318pp.\nLennox, C. L., Spotts, R. A., and Cervantes, L. A. 2003. Populations of Botrytis cinerea and Penicillium spp. on pear fruit, and in orchards and packinghouses, and their relationship to postharvest decay. Plant Dis. 87:639-644.\nSommer, N. F. 1985. Role of controlled environments in suppression of postharvest diseases. Can. J. Plant Pathol. 7:331-339."", 'Postharvest decay of apples and pears is not just a postharvest problem. Dr. Chang-Lin Xiao, plant pathologist with Washington State University in Wenatchee, advocates a holistic approach to managing decay because fruit can be infected in the orchard as well as during postharvest handling and storage.\n“What we’ve done in the orchard and what we’ve done at harvest will affect decay before and after packing,” he explained during an Apple Fruit Quality meeting organized by WSU Extension in Yakima this summer. “And what we do during packing can affect decay in the boxes, which sometimes shows up after marketing.”\nInfections of Sphaeropsis rot, Phacidiopycnis rot, and bull’s-eye rot begin in the orchard, even though there might be no symptoms at harvest. Fruit with latent infections start to develop symptoms during storage. Sphaeropsis rot is a recently reported disease of apples and pears.\nIt was first found on d’Anjou pears, but can cause even more serious problems on apples. The disease originates from infections at the stem or calyx of the fruit and can be misdiagnosed as gray mold. However, Sphaeropsis rot has a distinct “bandagelike” odor in the decayed flesh. Phacidiopycnis rot has only recently been reported in North America. In Washington State, it is common on pears, and less so on apples.\nIt also can be confused with gray mold. Infections can begin at the stem or calyx ends or through wounds on the skin of the fruit. Bull’s-eye rot affects both apples and pears. It often originates from infected lenticels, though it can also infect the stem or calyx ends of the fruit. In Washington it is commonly seen on Golden Delicious and Gala apples from orchards with overtree evaporative cooling or irrigation.\nThe fungi that cause these decays are on the trees in the orchard, often on dead bark, twigs that show signs of dieback, or cankers. They are not likely to be found in the packing house, Xiao said. Water is required for the fungi’s fruiting bodies to release spores.\nThe spores are dispersed in the orchard by water, and irrigation water creates favorable conditions. Strategies for controlling these diseases include removing cankers or twigs with dieback to reduce the inoculum level. Avoid overtree irrigation and minimize the duration of evaporative cooling.\nXiao recommends applying fungicides before harvest. In his trials on d’Anjou pears, an application of ziram two weeks before harvest provided up to 70 percent control of Phacidiopycnis rot. The ziram treatment produced similar control of Sphaeropsis rot on apples. The fungicide has a 14-day preharvest interval.\nGray mold (Botrytis) and blue mold (Penicillium) infections originate in wounds, such as stem punctures or stem bow splits, which can be contaminated by drench solution or water systems during packing. Decay shows up during storage, and in the case of the gray mold, which can spread from fruit to fruit, losses can be significant. Sources of the inoculum include orchard debris, decayed fruit on the orchard floor, soil, and dead bark on the trees.\nSpores are in the air and on the surface of the fruit. They are dispersed mainly by air currents, wind, and splashing water. Bins can be contaminated by blue mold spores that are resistant to the standard postharvest fungicide, TBZ (thiobendazole, sold under the brand name Mertect), and brought back to the orchard. To control postharvest diseases, growers and packers need to work together, Xiao said.\nGrowers should have a good sanitation program to reduce inoculum levels in the orchard. They should remove thinned or fallen fruit and plant debris from the orchard floor. They should also remove decayed fruit left on the trees; otherwise, this can be a source of inoculum for adjacent later-harvested varieties. Dead branches and cankers should be pruned out, and a fungicide should be applied either before harvest or after harvest but before storage.\n“A preharvest fungicide is particularly important for fruit for long-term storage that are not going to be treated with a postharvest fungicide before they are put in storage,” Xiao emphasized. A preharvest fungicide can: —control latent infections in the orchard, —reduce the spore load and the decay pathogens on the fruit surface, and —help prevent infection of wounds by decay pathogens.\nGood coverage is the key to success of a preharvest fungicide program for decay control, Xiao said, and he recommends a high gallonage spray. Packers should sanitize water systems and equipment and use postharvest fungicides, Xiao said.\nSeveral fungicides are available for preharvest use in addition to ziram. There’s the benzamidazole fungicide Topsin (methyl thiophenate), the strobilurin Flint (trifloxystrobin), and Pristine, which contains two active ingredients: pyraclostrobin (a strobilurin) and boscalin (a broad-spectrum carboxamide fungicide).\nRegistration of Elevate (fenhexamid), an anilide fungicide, is pending. Xiao said that in trials on Fuji last season, Elevate was effective against gray mold, but not effective against blue mold. Topsin was good for blue mold control, but not as good for gray mold control. Pristine gave excellent control of both. Pristine is also effective against powdery mildew and scab, but the drawback is a high risk of the pathogens developing resistance.\nXiao warns against making more than four applications of strobilurin fungicides per season, including the one applied close to harvest for decay control. Xiao recommends growers reserve Pristine for use just before harvest and use other fungicides for control of mildew and scab. There is unlikely to be cross resistance of Pristine and the postharvest fungicides Mertect (TBZ) and Penbotec.\nTopsin and Mertect come from the same benzamidazole chemistry, so the risk of cross resistance is high, Xiao warned. In the past, because TBZ was generally the only fungicide used in the packing house, benzamidazoles were not recommended in the orchard. Xiao still recommends that if the fruit will be treated with TBZ in the packing house, it should not be treated with Topsin in the orchard.\nWith the registration of new products, packers now have a choice between TBZ, Scholar (fludioxonil), and Penbotec (pyrimethanil), and there should be no cross resistance between the three. They can be used as a prestorage drench or on the packing line, but Xiao recommends only using each fungicide once.\nIn trials on Red Delicious, treatment with Scholar or Penbotec resulted in less than 1 percent decay from gray mold, compared with 2 percent in TBZ-treated fruit and 11 percent in the untreated. “I believe fungicides remain an essential component for decay management,” he said.\nHe encourages producers to consider the various combinations of pre and post harvest treatments. More information on diagnosis of postharvest diseases can be found on the Web site at http://decay.tfrec.wsu.edu.']"	['<urn:uuid:fb7e6c0b-e5ef-4321-9be7-b3ec64c77f7e>', '<urn:uuid:69b2ade7-6f50-45af-9e50-5778dad175d7>']	open-ended	direct	short-search-query	similar-to-document	multi-aspect	expert	2025-05-12T10:19:05.071190	6	92	2407
68	What makes development efforts fail to help communities?	Development efforts often fail because they take partial approaches that ignore the interconnected nature of ecological, economic and social factors. Additionally, they tend to treat development as an apolitical process, overlooking power dynamics and governance issues, which leads to disappointing outcomes since politics and power relationships are central to achieving meaningful development results.	"['|Home | Archives | About | Login | Submissions | Notify | Contact | Search|\nCopyright © 2000 by The Resilience Alliance\nThe following is the established format for referencing this article:\nHolling, C. S. 2000. Theories for sustainable futures. Conservation Ecology 4(2): 7. [online] URL: http://www.consecol.org/vol4/iss2/art7/\nEditorial Theories for Sustainable Futures C. S. Holling\nUniversity of Florida\nPublished: November 30, 2000\nMake things as simple as possible. But no simpler. (Albert Einstein)\nSustainable development and management of global and regional resources is not an ecological problem, nor an economic one, nor a social one. It is a combination of all three. And yet actions to integrate all three typically have short-changed one or more.\nSustainable designs driven by conservation interests often ignore the needs for an adaptive form of economic development that emphasizes human economic enterprise and institutional flexibility. Those driven by economic and industrial interests often act as if the uncertainty of nature can be replaced with human engineering and management controls, or ignored all together. Those driven by social interests can act as if community development and empowerment of individuals encounter no limits to the imagination and initiative of local groups. Each view captures its prescriptions in code words: regulation and control; get the prices right; empowerment; stakeholder ownership. These are not wrong, just too partial. Investments fail because they are partial. As a consequence, the policies of governments, private foundations, international agencies, and NGOs flop from emphasizing one kind of myopic solution to another. Over the last three decades, such policies have switched from large investment schemes, to narrow conservation ones to (at present) equally narrow community development ones.\nEach group builds its efforts on theory, although many would deny anything but the most pragmatic and nontheoretical foundations. The conservationists depend on theories of ecology and evolution, the developers on variants of free-market models, the community activists on theories of community and social organization. All these theories are correct, in the sense of being partially tested and credible representations of one part of reality. The problem is that they are partial. They are too simple. We lack an integrated theory that can serve as a foundation for sustainable futures, a theory that recognizes the synergies and constraints among nature, economic activities, and people, a theory that informs and emerges from thoughtful practice.\nEvidence points to a common cause behind past failures of investments in sustainable development. Historically, the management of forest, rangelands, fisheries, and wildlife resources was dominated by theories of carrying capacity and goals of sustainable yield. Human behavior was ignored. The application of these theories led to the expectation that target variables such as employment could be stabilized and created a demand for a constant flow of product. These policies were successful initially, and profit and employment were, in fact, stabilized. But their very success resulted in slow changes in key ecological, social, and cultural components not captured in the management models: changes that typically led to the collapse of the entire system. The ""economic extinction"" of cod along the coast of eastern North America is a prime example. From a review of a wide range of failed sustainable development initiatives, a common pathology emerges. At the extreme, the ecological system loses resilience, the industries become dependent and inflexible, the management agencies become rigid and myopic, and the public loses trust in governance.\nThere are so many examples of this pathology that we have learned the lesson well in theory, if not entirely in practice. We recognize that human behavior and nature’s dynamic are linked in an evolving system. We realize that the seeming paradox of change and stability inherent in evolving systems is the essence of sustainable futures. We now know that to counteract the current pathology we need policies that are dynamic and evolutionary. We need policies that expect results that are inherently uncertain and explicity address that uncertainty through active probing, monitoring, and response. However, we cannot successfully implement these new policies because we have not learned the politics and we ignore the public.\nTo date, the papers in Conservation Ecology that have focused on Adaptive Ecosystem Management and global change have carryied a gloomy message. Many provide examples in which the implementation of probing policies that recognize uncertainty and that allow for learning is frustrated. The realization of these new policies is frustrated by bureaucratic politics within organizations and by power politics outside agencies. Bureaucracies can become as much vested interests as can environmentalists and industries, and the politics of each group can exploit uncertainty in explicit disinformation campaigns. And we scientists provide the ammunition for them when our delight in rigorous analysis of the parts of a system ignores the consequences of the interaction of the parts and blinds us to political realities. It is no wonder that science and scientists have a bad name in so many situations in which collaboration among scientists, small enterprises, and conservationists could so obviously be a benefit to all.\nIs the failure to implement dynamic and evolutionary policies the result of the inherent complexity of these ecological, economic, social systems? That view is incisively explored by Emery Roe in his book Taking Complexity Seriously. That book is reviewed in this issue of Conservation Ecology from four perspectives, including the author\'s. That view sees complexity as anything we seem not to understand because there apparently are large numbers of interacting elements. Here I present another, alternative view suggesting that such complexity may be in the eye of the beholder, and that most of the ""large number of interacting elements"" may be, in fact, the consequence of a smaller number of controlling processes. It is this latter view to understanding the smaller number of controlling processes that opens a line of deep enquiry about complex evolving systems. Both views provide the motivation and direction needed to develop alternative perspectives and models. And both provide a focus for the kind of deliberative conversations about difficult issues that Conservation Ecology encourages. There is no one path to understanding.\nThe alternative view that I propose argues that there is a requisite level of simplicity/complexity behind complex, evolving systems that, if identified, can lead to understanding that is rigorously developed but also can be lucidly communicated. It argues that if you cannot retain a handful of causes in your explanation, then your understanding is simplistic. If you require more than a handful of causes, then it is unnecessarily complex. If you cannot explain it to your neighbor, you do not truly understand it. That level of understanding is built upon a foundation of adequate integrative theory, rigorously developed, rooted in empirical reality, and communicated clearly with metaphor and example. The first requirement to achieving that level of understanding is to begin to integrate the essence of ecological, economic, and social science theory. And we need to do so with a goal of being ""as simple as possible but no simpler"".\nIn the late 1980s, the Royal Swedish Academy of Science established an institute, The Beijer International Institute of Ecological Economics, to bridge the disciplines of economics and ecology and, more generally, the natural and social sciences. One of the inspired creations of the Director, Karl-Göran Mäler, was an annual meeting on an island in the Swedish archipelago of economists, ecologists, mathematicians, and others from the natural and social sciences. There was never an agenda – only one or two general questions. What is poverty? What ecological and human stresses come from population growth? What is resilience of an economy? Of an ecosystem? How to measure values?\nOut of those meetings and their deliberative conversations came a growing, deep understanding among the participants of fields other than their own. Not the superficial polarization so characterizing much debate, but understanding of strengths and weaknesses. The surprise was that there was so much agreement between ecologists and economists: the world’s human and natural systems are becoming dangerously stretched; the methods of each discipline had remarkable similarities; the theories and mathematics were strangely similar—and inadequate. The critical differences were amazingly slow to emerge. The desire for collaborative dialogue and the sheer decency of a Swedish context led us to steer away from disagreements. But it is the tension of differences that can crystallize discovery.\nIt ultimately became clear that there were deep and fruitful differences. Each of the primary fields of economics, ecology, and institutional analysis has developed tested insights. But each is missing key elements that would allow for the kind of integrative theory and practice that is required for sound decision making on sustainable resource use issues. As examples:\nEconomics: Modern economics has gone far in discovering the various pathways through which millions of expectations of, and decisions by, individuals can give rise to emergent features of communities and societies (e.g., rate of inflation, productivity gains, level of national income, prices, stocks of various types of capital, cultural values, and social norms). Two factors make economic theory particularly difficult. First, individual decisions at any moment are themselves influenced by these emergent features, by past decisions (e.g., learning, practice, and habit), and by future expectations. Second, the emergent features that can be well handled by existing economic theory and policy concern only fast-moving variables. The more slowly emergent properties that affect attitudes, culture, and institutional arrangements are recognized, but are poorly incorporated.\nEconomists know that success in achieving financial return from fast dynamics leads to slowly emergent, nearly hidden, changes in deeper and slower structures, changes that can ultimately trigger sudden crisis and surprise. But the complexities that arise are such that most modern economists are frustrated in their attempts to understand the interactions between fast- and slow-moving emergent features.\nEcology: Ecosystem ecologists have made it plain for a long while that some of the most telling properties of ecological systems emerge from the interactions between slow-moving and fast-moving processes and between processes that have large spatial reach and processes that are relatively localized. Interactions between, for example, regional patterns of vegetation formed by major disturbance processes like fire and insect outbreak, and local species composition formed by competition among species. Those interactions are not only nonlinear, but also they generate alternating stable states and normal journeys of biotic and abiotic variables through those states. It is those journeys, measured in years, decades, and centuries, that maintain the diversity of species, spatial patterns, and genetic attributes that give resilience to ecological systems.\nWhat this tells us is that variability is not merely an inconvenient characteristic of productive, dynamic systems. Rather, it is critically necessary to their maintenance. Ecologists have made significant advances in understanding the specific role of variability in maintaining the resilience of natural systems and the conditions that cause a system to flip into an irreversible, and typically degraded, state controlled by unfamiliar processes.\nHowever, ecologists have been largely ignorant of human behavior, organizational structures, and institutional arrangements that mediate the relationships between people and nature.\nSocial Science: Institutional theory and analysis does consider such features, but in a largely static sense. Hence, it also stops just short of the confluence point among the three fields that could provide us with the integration we need. Institutional theory currently provides an understanding of the variety of arrangements and rules that have evolved in different societies to harmonize the relationship between people and nature. Social scientists have gone far in describing the way people store, maintain, and use knowledge in stable circumstances. But an integrative approach requires attention to the very same dynamic dimensions that economics and ecology, each in their own way, have developed.\nIn order to plan for sustainability, we need to know, and we need to integrate, how information is evaluated and how counterproductive information is rejected at times of deep change. How is new ""knowledge"" created from competing information sources and incorporated with useful existing knowledge? Which processes create novelty, which smother innovation, which foster it? None of the disciplines of ecology, economics, and institutional theory, as construed at present, can, in isolation, help in these fundamental questions of innovation, emergence, and opportunity.\nEvolution and Complex Systems: Yet, those questions are at the heart of development: the emergence of novelty that creates unpredictable opportunity. It is biological evolutionary theory, expanded to include cultural evolution, that does deal with just those questions. The recent invention of complex systems studies explicitly sees ecological, economic, and social systems each as specialized representations of a complex adaptive system. There have been wonderful advances achieved by borrowing those mechanisms that generate variability from known biological processes and exposing the emergent patterns that result. But, as for each of the other fields, the representations are partial. They are detached from efforts to represent the necessary and just sufficient complexity in natural and human processes, and to test the adequacy and credibility of the results.\nEven the most ruthlessly pragmatic goal for developing policies and investments for sustainability needs a theoretical foundation that integrates ecological with economic with institutional with evolutionary theory and that overcomes the disconnect rooted in current theoretical limitations within each field.\nIt was this diagnosis that launched the “Resilience Project”, a five-year effort of an international group of ecologists, economists, social scientists, and mathematicians. The project initiated a search for integrative theory and integrative examples of practice. The goal was to develop and test elements of an integrative theory at the level of simplicity necessary for understanding, but with the complexity required for developing sustainable policy.\nThe results of Resilience Project are summarized in the final report to the MacArthur Foundation (http://www.resalliance.org/reports/). For me, it was a huge advance in unraveling the puzzles and paradoxes that had confused me over the years in my efforts to understand the interactions between nature and people. The fundamental paradox is that change is essential, and yet stability is necessary. Together with some 120 scientific papers that arose out of the work, the four forthcoming books from the project constitute an important contribution to the theory of sustainable use of natural resources. In the exuberance of discovery, those results were boiled down to 10 conclusions, as noted in the final report: 10 tablets from the Resilience Mountain!\nThe heart of the work is now developed and described in a book in press with Island Press, edited by Lance Gunderson and C. S.Holling: Panarchy: Understanding Transformations in Human and Natural Systems. Panarchy is the term we devised to describe the evolving nature of complex adaptive systems. It encapsulates how novelty and change coexist in a context of persistence and stability. It resolves the paradox of change and stability.\nWe define panarchy to be the structure in which systems of nature (e.g., forests, grasslands, lakes, rivers, and seas), of humans (e.g., systems of governance, tribes, and cultures), as well as combined human—nature systems (e.g., agencies that control natural resource use), are interlinked in never-ending adaptive cycles of growth, accumulation, restructuring, and renewal. These transformational cycles take place in nested sets at scales ranging, for example, from a leaf to the biosphere, over periods from days to geologic epochs. By understanding these cycles and their scales, it seems possible to identify points at which a system is capable of accepting positive change, and possible to use those leverage points to foster resilience and sustainability within a system.\nThe panarchy summarizes succinctly the heart of what we define as sustainability. The fast cycles invent, experiment and test; the slower ones stabilize and conserve accumulated memory of past successful, surviving experiments. In a healthy system, each level is allowed to operate at its own pace, protected from above by slower, larger levels, but invigorated from below by faster, smaller cycles of innovation. The whole panarchy is therefore both creative and conserving. The interactions between cycles in a panarchy combine learning with continuity.\nThis clarifies the meaning of sustainable development. Sustainability is the capacity to create, test, and maintain adaptive capability. Development is the process of creating, testing, and maintaining opportunity. The phrase that combines the two, sustainable development, therefore refers to the goal of fostering adaptive capabilities and creating opportunities. It is therefore not an oxymoron, but represents a logical partnership.\nSustainable futures are ones in which the basic means of human livelihood get easier, human opportunities become richer, and nature\'s diversity is more sustained — and not only in the rich parts of the world. Utopian, perhaps, but the resilience of nature and the ingenuity of people would make it feasible, if our institutions and those who utilize and control them had sufficient flexibility and vision. The ultimate test of the discoveries and conclusions from the Resilience Project will be the extent to which the resulting solutions generate problems less costly than their predecessors and opportunities more viable.\nA new international consortium of researchers, the Resilience Alliance (http://www.resalliance.org), has been established to carry forward the findings of the Resilience Project. The Alliance will use case studies from around the world to test and refine the new theories that emerged from the Resilience Project through thoughtful application and experimentation. The Alliance will also continue to seek new theoretical foundations for the integration of ecology, economics, and social science by further exploring the insights achieved through the Resilience Project.\nWe call for the submission of papers to Conservation Ecology that give examples of these new types of solutions as profiled by the Resilience Project — solutions that fundamentally deepen our understanding of resilience and sustainability.\nResponses to this article are invited. If accepted for publication, your response will be hyperlinked to the article. To submit a comment, follow this link. To read comments already accepted, follow this link.\nAddress of Correspondent:\nC. S. Holling\nDepartment of Zoology\nUniversity of Florida\n223 Bartram Hall\nGainesville, Florida 32611-2009 USA\nPhone: (352) 543-6955\nFax: (352) 392-3704\n|Home | Archives | About | Login | Submissions | Notify | Contact | Search|', 'Resilience is a concept that has gained popularity across many disciplines during the past decade. Resilience refers to the capacity of an individual or system to recover from or adapt to shocks or other changes. The idea originated in studies of ecology, but is increasing utilized to understand human systems. In a world beset by crises ranging from natural disasters to financial collapse to armed conflict, the concept of resilience is critical to understanding how communities and groups respond to traumatic changes.\nAmong the practitioners integrating the idea of resilience into their work are those in the international development profession. Many aid organizations seek to reduce the vulnerability of individuals and communities to famine, climate change and other shocks and crises that affect populations in numerous parts of the world. Development agencies are increasingly describing their work in this are in terms of strengthening the resilience of vulnerable groups. However, this turn towards resilience is not without its challenges. Notable among these is the need to adopt an understanding of resilience that considers the political dimensions of the concept. If development practitioners fail to address the political nature of resilience, their work in this area may lead to disappointing outcomes.\nThe Perils of Development ‘Fads’\nAs noted, the idea of resilience is being increasingly utilized in the field of international development. In this regard, it is increasingly becoming the latest in a line of popularized concepts, such as social capital, empowerment, and participatory development. Each of these ideas came to the fore in development circles and was widely adopted by aid organizations, only to recede as the next ‘fad’ took hold. Individually and together, these concepts had the potential to transform development practice by focusing attention on the relationships and networks of power and inequality that undergird chronic poverty (see Mosse, 2010). Yet social capital, empowerment and participatory development were each criticized for the manner in which they were applied by development practitioners, absent sufficient consideration of issues of power and politics (Cooke & Kothari, 2001; Fine, 1999; Rowlands, 1995).\nCritics have particularly scrutinized the application of the idea of participatory development, the simple, yet seemingly radical notion that local individuals and groups should be engaged meaningfully in development decisions and processes that affect them. Scholars pointed out that participatory development, as conceptualized and applied, was riddled with assumptions about the power dynamics of groups, communities and households (Cleaver, 2004). Furthermore, critics noted that development practitioners often equated consulting local people with participation, ignoring who was excluded, how individuals exercised voice, what previous ‘agenda setting’ had occurred, among other issues. As Andrea Cornwall notes,\n“‘participatory’ processes can serve to deepen the exclusion of particular groups unless explicit efforts are made to include them…. Without a dynamic understanding of people’s social networks and the institutions and dimensions of difference that matter in the pursuit of their livelihoods, naıve efforts to bring about inclusive development may simply make things worse (2008, pp. 277, 279).”\nFurthermore, other scholars have argued that the use of the concepts of social capital, empowerment and participatory development (and likely resilience) by development agencies has led to a misconception of the role of communities in development:\nDevelopment practitioners excel in perpetuating the myth that communities are capable of anything, that all that is required is sufficient mobilization…and the latent capacities of the community will be unleashed in the interests of development (Cleaver, 2001, p. 46).\nAccording to this (mistaken) logic, participatory methodologies can empower individuals and communities to drive their own development processes. Absent from this conceptualization is any role for the state, as development is defined as a community-driven and -owned process. In removing the state from the equation, this understanding of development falls into an essentially neo-liberal framework and is devoid of political considerations regarding inequality, rights, control of resources and distribution of decision-making power.\nThis vision of development as an apolitical process has long been embedded in the approaches of many development institutions, despite a more recent rhetorical acceptance of the role of politics (Carothers & De Gramont, 2013). Indeed, the message about the centrality of politics in development has been growing clearer, as the following quote from a United Kingdom Department for International Development research report demonstrates:\n“To understand development we must understand the politics that shape it. …The key message from all four research programmes has been the centrality of politics in building effective states and shaping development outcomes (Department for International Development, 2010).”\nYet despite an increasing acknowledgement that ‘politics matters’, aid organizations struggle to adapt their procedures and technocratic approaches to this reality.\nResilience could fall into the same trap and become locked into an apolitical, and likely, neo-liberal mold, lionizing the resilient, empowered community unreliant on state institutions or on broader structural reforms to address development needs. This possibility is highlighted by one group of scholars investigating the increasing use of idea of resilience in development:\n“…resilience approach is not particularly well equipped to embrace these social issues of agency and power. … [by utilizing] a resilience framework, there is therefore a risk to move back to technical, apolitical interpretations and solutions with the consequence to ‘evacuate’ the social justice/transformative dimension (Bene, Godfrey Wood, Newsham, & Davies, 2012, p. 28).”\nThus, the idea of resilience risks falling into the mold of previous development concepts and being applied without incorporating political considerations. This could lead to interventions by development agencies that fail to fully address the vulnerabilities faced by those communities aid practitioners seek to support.\nResilience is political.\nWhen natural disasters or armed conflicts strike, poor communities (and rich ones, for that matter, as “Superstorm” Sandy made clear in the United States) cannot adapt and “bounce back” on their own, they require the support of public institutions. Amatya Sen famously argued that famines do not occur in democracies, not because communities in democratic countries are more resilient to famine, but rather because a free press and political competition oblige governments to mobilize public institutions to support famine-struck areas (Sen, 1999). Yet in many countries around the world, democracy is partial, and public institutions are weak and corrupt. Strengthening democratic governance and state institutions is not primarily an issue of achieving proper institutional forms or building technical capacity, rather it is an intensely political process, one involving elites, coalitions, contestation, and the long road to reform (Leftwich & Sen, 2010).\nSome streams of the discourse on promoting resilience makes it clear that governance is a critical element of resilience. According to the U.S. Agency for International Development’s policy on resilience:\n“Functioning institutions of good governance and democratic accountability are also essential, as building resilient countries and systems requires effective and inclusive governments that hold themselves accountable for results (2012, p. 5).”\nDiane Davis, in her investigation of resilience in situations of chronic violence, makes governance even more central to resilience. In her formulation:\n“…resilience appears at the interface of citizen and state action, and is strengthened through cooperation within and between communities and governing authorities…. citizens…and governing authorities establish institutional networks of accountability that tie them to each other at the level of the community….activities produced through citizen-state networks are most accountable, legitimate, and durable when they are directed and monitored by communities themselves, in a relationship of cooperative autonomy (2012, p. 9).”\nThus, resilience is a product of good governance, of constructive and mutually-accountable relationships between citizens and state actors. However, just as with building strong public-sector institutions, strengthening relationships between vulnerable communities and the state is an inherently political process. Indeed, those communities that are most impacted by trauma or crisis are the same communities that are often politically marginalized, and thus have relationships with political authorities marked by clientelism, disenfranchisement and/or indifference. In the case of Davis’ discussion of chronic violence, she investigates this phenomenon primarily in poor urban neighborhoods, yet in communities such as these relationships with government actors are most often marked by networks of patronage and vote buying, rather than mutual accountability (Desai, 2010).\nDevelopment agencies and practitioners interested in promoting resilience must then focus on strengthening accountable and inclusive governance frameworks in which vulnerable communities are embedded. Yet even as governance has emerged as a prominent area of intervention for aid agencies, practitioners have been slow to understand the political drivers of poor governance and slower still to incorporate politically-informed practices into their programs. Sue Unsworth has proposed three mutually reinforcing barriers that prevent the integration of political considerations:\n- Getting people to take politics seriously requires them to change their existing mental models of how development happens;\n- Institutional incentives within aid agencies and development NGOs that reinforce the status quo;\n- Political analysis highlights the stark reality that there may be very limited local ownership of a development agenda (2009, p. 889).\nAlthough some progress has been made in incorporating politics into development work, primarily through the growing popularity political economy analyses, politically informed development practice is still the exception rather than the rule (Carothers & De Gramont, 2013).\nThere is an additional danger that in their rush to promote resilient communities, development agencies will displace their true purpose: combating poverty. Strengthening resilience should not be conflated with efforts to address poverty, for:\n“there is no relation between poverty alleviation and resilience building. Resilience is poor-neutral; in other words, it is not a pro-poor concept; nothing in it makes it specifically linked to the poor (except perhaps that the poor are often presented/assumed to be more vulnerable, or less resilient, than others). And this last assumption is exactly where the concept of resilience starts falling apart. Indeed, in contrary to what people seem to believe, households can be very poor and very resilient. … This means that the whole discourse about how it is important to build resilience as a tool for poverty alleviation is flawed: there is no direct and obvious way out of poverty through resilience. Ultimately, development should therefore remain about poverty alleviation and wellbeing, not about resilience building (Bene, et al., 2012, p. 48).”\nThus, the trend towards a focus on strengthening resilience may likely mean that development agencies (likely unintentionally) de-emphasize combating poverty. Yet this does not have to be the case. Promoting resilience and addressing poverty both require tackling governance and political issues. Strengthening resilience by promoting strong public institutions and accountable governments would advance poverty reduction goals as well. Yet if development agencies and practitioners are to achieve real gains in these areas, they will have to adopt politically-informed analyses and approaches.\nIn sum, resilience depends critically on capable public institutions and inclusive and accountable governance. The processes of strengthening state institutions and democratic governance are inherently political. After several decades of highlighting the importance of governance, development agencies are still struggling to come to terms with the political nature of this concept and to move towards politically-informed interventions to strengthen governance. If aid providers have been slow to incorporate politics at the heart of their governance work (and their development work more generally), it is unlikely that they will readily adopt a political understanding of resilience. Ultimately, this failure may lead to disappointing outcomes and perhaps the decline of resilience (joining social capital, empowerment and participatory development in the development fad dust bin) in favor of the next popular conceptual tool that will surely emerge in the years to come.\nBene, C., Godfrey Wood, R., Newsham, A., & Davies, M. (2012). Resilience: New Utopia or New Tyranny? Reflections about the Potentials and Limits of the Concept of Resilience in Relation to Vulnerability Reduction Programmes. IDS Working Paper(405), 1-61.\nCarothers, T., & De Gramont, D. (2013). Development Aid Confronts Politics: The Almost Revolution. Washington, D.C.: Carnegie Endowment for International Peace.\nCleaver, F. (2001). Institutions, Agency and the Limitations of Participatory Approaches to Development. In B. Cooke & U. Kothari (Eds.), Participation: The New Tyranny? (pp. 36-55). London, UK: Zed Books.\nCleaver, F. (2004). The Social Embeddedness of Agency and Decision-Making. In S. Hickey & G. Mohan (Eds.), Participation: From Tyranny to Transformation? London, UK: Zed Books.\nCooke, B., & Kothari, U. (Eds.). (2001). Participation: The New Tyranny? London, UK: Zed Books.\nCornwall, A. (2008). Unpacking ‘Participation’: Models, Meanings and Practices. Community Development Journal, 43(3), 269-283.\nDavis, D. (2012). Urban Resilience in Situations of Chronic Violence. Cambridge, MA: Center for International Studies.\nDepartment for International Development. (2010). The Politics of Povety: Elites, Citizens and States. Findings from Ten Years of DFID-Funded Research on Goverance and Fragile States 2001-2010. London, UK: DFID.\nDesai, R. (2010). The Political Economy of Urban Poverty in Developing Countries: Theories, Issues, and an Agenda for Research. Washington, D.C.: Brookings Institution Wolfenson Center for Development.\nFine, B. (1999). The Development State is Dead – Long Live Social Capital!? Development and Change, 30, 1-19.\nLeftwich, A., & Sen, K. (2010). Beyond Institutions: Institutions and Organizations in the Politics and Economics of Poverty Reduction – A Thematic Synthesis of Research Evidence. Manchester, UK: Improving Institutions for Pro-Poor Growth.\nMosse, D. (2010). A Relational Approach to Durable Poverty, Inequality and Power. Journal of Development Studies, 46(7), 1156-1178.\nRowlands, J. (1995). Empowerment Examined. Development in Practice, 5(2), 101-107.\nSen, A. (1999). Development as Freedom. New York: Random House.\nUnited States Agency for International Development. (2012). Building Resilience to Recurrant Crisis: USAID Policy and Program Guidance. Washington, D.C.\nUnsworth, S. (2009). What’s politics got to do with it? Why donors find it so hard to come to terms with politics, and why this matters. Journal of International Development, 21(6), 83-894.']"	['<urn:uuid:bdc53901-87cc-41b9-8006-5b43b8d973f6>', '<urn:uuid:e9227e3b-8135-468b-814d-b82dabbf6ff0>']	factoid	with-premise	concise-and-natural	similar-to-document	multi-aspect	novice	2025-05-12T10:19:05.071190	8	53	5210
69	how do protests and auctions compare in terms of managing crowd behavior and maintaining order	The library protests required extensive security coordination across multiple agencies including police departments, fire protection, and public works to maintain order during the controversial event. In contrast, the Dessert Dash manages crowd behavior through careful structure and timing - it's held about an hour into the live auction, uses organized table assignments and bidding forms, and employs specific protocols for runners selecting desserts. The auction organizers maintain control by not proceeding until guests are seated with dessert, keeping all personnel alert and working during the process, and using music to cue audience behavior.	"['Rockton residents react to drag protest expenses\nOur accompanying story examines the costs of the Rockton protests in more detail, quoting from some of the organizers.\nAt the Aug. 21 meeting of the Talcott Free Library board, former Rockton resident Katie Littlefield presented financial costs related to the Talcott Library\'s Drag Queen Q&A event on July 14, 2023, which was eventually held on Zoom.\nTo gather the information, Littlefield sent Freedom of Information Act (FOIA) requests to Illinois Law Enforcement Alarm System (ILEAS), Illinois State Police, Mutual Aid Box Alarm System (MABAS), Rockton Fire Protection District, Rockton Police Department, Rockton Public Works, South Beloit Police Department, Talcott Free Library, Village of Roscoe Police Department, and Winnebago County Sheriff\'s Department.\nThe initial library board meeting at the Rockton Township Community Center on June 26 included signs and prayers by Rockford Family Initiative and a march by Rockton Pride. After the board meeting, Rockton Pride stopped countering protesters and held a party on July 14 instead. Protests against the event by Rockford Family Initiative continued on July 10-14.\nNeither the Illinois State Police or the Village of Roscoe had any cost associated with the protests. The Mutual Aid Box Alarm System (MABAS) did not provide any figures. The seven other entities had a combined total of $14,140.73.\n“Regardless of your position on whether for or against it, this information is helpful and must be looked at when making a decision on future events,” Littlefield said.\nAbout this story\nWe strive to report fairly on both sides of every controversial issue. Both sides held protests and both sides contributed to fears of unrest. But there\'s a reason this article quotes drag opponents and only one drag supporter. Rockton Pride, the group formed in support of the drag queen Q&A, has decided not to respond to their critics at this time. Their leaders were present at the library board meeting, but chose not to speak. But members of both sides have said they had constructive discussions together after the Aug. 21 meeting.\nLittlefield said, “This cost doesn\'t take into consideration the police officers that were called back to duty when they had vacations and family events already planned. It also doesn\'t take into account that the Village closed the pool for the Pride party, in case they [swimmers] were unaware that the party was taking place just outside the pool line.”\n“This event was decided on per a request by the Teen Advisory Board. The Director and library board not only cost the taxpayers of Rockton, but also Winnebago County taxpayers, over $14,000 for one event.”\n“Let\'s also mention that the idea that Rockton people only should voice their opinion really holds no water as the Winnebago County Police Department [Winnebago County Sheriff\'s Office] was brought in here.”\n“Would it be safe to say that this event wasn\'t properly vetted by the board and the director? This should have, at a minimum, a certain vetting requirement, especially when it is a well-known controversial topic.”\n“It is as simple as balancing community interests, cost analysis, risk assessment, board oversight, public input and transparency. These are common practices for good governance, and none of these were done.”\n“All of those involved with making the decision to have this event to the board who voted for it surely knew what they were going to subject the community to. While they might not have known the full impact, you definitely knew it would have an impact and did it anyway.”\n“Was the Drag Q&A worth the price tag for the community of people, and what benefit was derived for children who this event was specifically directed to?”\nAt a June 26 library meeting, Rockton native Jessica Green expressed the view of many in the LGBTQ community when she defended the event, ""If you choose not to have the event at the library,...your choice could literally cost people their lives, because they will be living in a community that does not accept them. According to the Trevor Project, 1.8 million LGBTQ youth seriously consider attempting suicide every year, and at least one attempts every 45 seconds."" Another community member points out that the civil rights movement in the 1960s caused huge police expenses as well.\nLittlefield asked, “Will the library be paying back the Village of Rockton for the cost incurred? Surely we are all aware that government budgets are razor-thin.”\n“If nothing else, I hope there is an apology to the village for making a decision that cost them. Lastly, I implore you to make future decisions using good governance protocol. It is not hard, it will make a difference and the community will thank you.”\nLittlefield later presented these figures at the Sept. 5 Rockton Village Board meeting.\nResident Donna Gill also addressed the library board.\n“I am not within your district, we live in no library district at all, we are the people on the outskirts,” Gill said. “We spent a lot of time here when my kids were growing up.”\n“What happened with the Drag Queen Q&A really touched my heart, both as a mom and a grandma. This was the place we felt we could always send our kids.”\n“The Drag Queen Q&A was set up during a weekday when parents are working, with no parental notification from what I saw, or permission.”\n“This was such a controversial type of thing. I kept seeing that it has nothing to do with anything sexual.” (Our accompanying article shows how the drag queen dressed that day.)\n“When this type of thing is pushed through the library without parental notification while people are working, that is a huge problem.”\n“We pay an awful lot of taxes to the school, to the community and to Winnebago County. I just hope you would listen to the people of the community.”\n“Nobody is trying to push anyone out or is against anybody,” Gill said. “Be considerate, think about the parents, but also think about the kids as they should not be sexualized in any way.”\nOur accompanying story includes a first-hand report of what actually happened at the drag queen Q&A on Zoom.\nMK Lindt spoke about the importance of the content that is found in books. “Our library\'s collection is a reflection of the diverse interests and needs of our patrons. It is crucial that we ensure content is both accurate and appropriate for all age groups.”\nLindt specifically addressed books, labeled as young adult, that she believes contain content and themes that ""have matured well beyond the traditional confines of this genre.”\n“I myself and others have spoken about this issue at a previous board meeting and nothing has been done.”\nLindt told the board that she believes that library policy clearly states non fiction judged to be young adult will be shelved with the adult collection.\n“I find myself in a situation where the library director has resisted the notion of moving these books to the adult section. She suggested that I submit a request that these books be moved to the appropriate adult section.”\n“While I respect the director and the board\'s willingness to listen to different viewpoints, I must insist that this situation calls for direct alignment with the established collection development policy.”\n“By moving these books, it will demonstrate a commitment to maintaining a collection that adheres to our guidelines and provides a safe, educational and enjoyable environment for all members of the community.”\nIt turns out Lindt was looking at a previous library policy which the board had voted to discard but which was still on their website. The explanation: because of lack of space before the completion of the library expansion, Talcott Library had decided to shelve young adult books with the adult books, rather than with children\'s books. Now that there is a separate teen section of the library, all young adult books are kept there. The library updated their website after Lindt brought up the discrepancy. Patrons can still request books to be moved from the youth room to the adult collection, but that is governed by a different part of the library\'s General Guidelines.\nBesides a General Guidelines document and Bylaws, the library has also adopted the 1953 Freedom to Read statement, the Library Bill of Rights, and the statement on Non-Removal of Challenged Materials from the American Library Association.\nMore about the issue\nMichael Nolan was the person behind the “See You at the Library” event that took place on Aug. 5 at Talcott Library.\n“I wanted to come here to thank Megan for allowing us to have this event and for the professionalism that was shown by the library staff. Megan also gave me great ideas,” Nolan said.\nDuring the “See You at the Library"" event, the book The Fight for Freedom Island, written by Dr. Trent Talbott and published by Brave Books, with art by Ali Elzeiny, was read by local teacher Marla Anderson.\nThis event is similar to national “See You at the Pole” events, scheduled this year on Sept. 27, 2023.\nNolan donated to the library a Founder\'s Bible and a video set featuring a Biblical course.\nMichael McGinnis contributed to this reporting.\nMore News from Rockton\n- Strong spirits: what really scares you? What terrifies me is the thought that in the storms of my life, I will foolishly think that I can take care of things myself.\n- Roscoe antique sale to benefit restoration of first settler\'s homestead Roscoe Township Historical Society\'s antique sale is Sept. 23-24, 2023 at the Community Center.', 'The Dessert Dash - Everything You Need to Know by April Brown\nThe Dessert Dash is a race that encourages your guests to interact with one another during the live auction timeline. Guests seated at each dining table decide individually how much money they are willing to pool together for the right to run and grab a dessert from the dessert dash table. They write that amount on the Dessert Dash Bid Sheet. The bid sheets are picked up, tallied and the table raising the largest amount of money is in first position when the dash begins. The auctioneer calls out the tables in ascending order until the last table has made a run for it.\nThe Dessert Dash is held about an hour into the live auction (sometimes later) and acts as an energy boost following the Paddle Raise portion of the live auction. Guests often feel a psychological and physical crash after The Paddle Raise/Fund-need and the Dessert Dash is the perfect pick-me-up. Reward everyone with a fun, lively, fast paced and comical return to the auction block. Everyone loves the Dessert Dash.\nThe Dessert Dash is a big money maker. For that reason alone, it is worth doing the Dessert Dash even if all the desserts are purchased from local eateries. I can recall years ago getting a frantic call from an auction chair who arrived at the venue only to discover that a volunteer failed to gather the desserts for the dash. I advised a volunteer to rush to Costco and purchase a variety of desserts and then stop at Safeway and buy flaked chocolate, fresh fruit and berries and a rich whipping cream. She arrived back at the venue and other volunteers applied their artistic and culinary touch to each dessert before the crowd arrived. Those 21 desserts raised $3200. The cost - under $300.\nDessert Dash can be developed around delectable desserts created by celebrity chefs and gourmet restaurants or home made by volunteers and supporters or purchased or a combination of each. It is important to have a good dessert for every table. Sometimes a plate of Twinkies is included as a booby prize for the cheapskates who feign dieting. The inclusion of the plate of Twinkies is suppose to make people put in more money so they don’t get stuck with the junk food. Since everyone paid the same ticket price and wants and expects dessert the only proper thing to do is deliver a delicious dessert and graciously thank even the cheapskates. Never do anything that bathes a guest in a negative feeling. The Dessert Dash, like every activity should create a feeling of inclusion not exclusion.\nThe Tools of The Dash\n- Dessert Dash bid form\n- Dessert Dash flag\n- Dessert display\n- Dessert Dash display sign\n- Service plates, utensils, and tableware\nThe Dessert Dash display is designed to show off the bakers and tempt your guests. Lighting, mirrors, and mini fans are excellent for luring guests to the Dessert Dash preview. The display can consist of a simple draped table, or possess the look of a fancy bakery or take on the theme of the evening. My favorite is to create a replica of a giant tiered cake. Do this by stacking and draping a standard round dining table with smaller round risers and decorate the sides of the fake cake with strands of ribbon or lights simulating frosting. Place the Dessert Dash sign on the top tier along with lighting. Inexpensive laser and blinking lights and small personal battery operated fans can hidden in the decorations. Approximately 35 – 40 desserts can be displayed in this footprint, depending on the circumference of the tiers.\nMechanics of the Dessert Dash\n- Procure one dessert for every table plus 2-4 extra desserts.\n- Confirm the name of bakers and locations of desserts.\n- Ask the bakers to print the chef’s name (include photo if possible) and an ingredients card and/or the recipe on a 3”x5” card.\n- Make a big sign that says “Dessert Dash” and place conspicuously on the table.\n- Make 3 signs that describe the Dessert Dash Process and place those around the display.\n- Arrange for pickup and deliveries from the donors on day of auction.\n- Coordinate refrigeration with kitchen and/or catering personnel.\n- Assign a number to each dessert and tape it to the plate facing the viewer.\n- Create Dash Flags from bamboo skewers with the table number in large type on the flag.\n- Arrange with venue lead for the arrangement and placement of display table.\n- Print one Dessert Dash bid form for each table plus 2 extras. Fold form and clasp a pen to each form.\n- Place Dessert Dash Display sign on Dessert Display table.\n- Place Dash Flag near or in the centerpiece on each dining table. The Dash Flag’s number should match the dining table number.\n- Place Dessert Dash form and pen on each table near centerpiece.\n- Place dessert dishes and forks on the other side of the centerpiece OR set standard dessert plate service.\n- Assign a volunteer to pick up completed Dash Forms during the live auction. The forms should be picked up after a specific item number rather than at a specific time.\n- Create a notation in the auctioneer’s copy of the catalog for the auctioneer and master of ceremonies to mention the Dessert Dash at least 3 times before the forms are picked up.\n- Pick up completed Dessert Dash forms from each table after about 7 live items have sold.\n- Total each form and arrange from highest amount to lowest amount by table number.\n- Ideally the bid number and amount each bidder is contributing is transferred to a tally sheet from the individual table forms. The totals are then given to the auctioneer immediately following the Paddle Raise.\n- Give forms to clerks for recording bid numbers and amounts into database.\n- The auctioneer calls the audience to attention and begins calling the table numbers and inviting guests to run and choose a dessert quickly.\n- One designated runner from each table grabs the Dessert Dash Flag and runs to the Dessert Display table and sticks the flag in the dessert of choice and returns to the table without the dessert.\nIt is critical the volunteers understand that the DD form represents uncollected cash. This form may be the only hard copy resource for recording the transactions, collecting the money and reconciling post auction. NEVER throw this form away!!!! Once the numbers have been entered into the software, rubber band the stack and place a sticky note under the rubber band with the words “Dessert Dash Forms Entered” and place the forms in the reconciliation tray. Do not put the forms in an envelope and seal the envelope, ever.\nServing the dessert\nThe catering staff cuts individual slices of each dessert. The entire dessert is delivered to the table along with a serving instrument. Guests serve themselves using pre-set plates. The leftover dessert remains on the dining table.\nThe catering staff cuts individual slices, place those individual servings on dessert plates and serves each guest. Left over desserts are later delivered to the tables by the waiter.\nThe runner grabs the uncut dessert, takes it back to the table and the dessert is cut by someone at the dining table. This method is fun yet can be awkward and time-consuming and distract from the final segment of the auction. For some reason when guests cut their own cake, they begin trading it from table to table and sometimes interrupt the auction by offering the leftovers up for bid. Although seemingly a good idea on the surface, it rare that leftover cake has more value than the planned live auction lineup.\nAuction Powerhouse Tip: The live auction should not proceed until most every guest is seated and enjoying dessert and coffee.\nAuction Powerhouse Tip: The Dessert Dash should not be perceived as “taking a break from the auction”. All personnel, including sound techs, the auctioneer, the master of ceremonies, and the volunteers need to be alert and working during this process. The audience needs to see that the auction team is waiting for them to settle down and get back down to the business of raising money.\nAuction Powerhouse Tip: Play silly music. Music makes the dessert dash more fun and cues the audience to something exciting going on in the house.\nAuction Powerhouse Tip: The Dessert Dash is often sponsored by a business, like a bakery, hotel, or a social club. The Dessert Dash has 10 points of exposure for business and you can charge a fee for this opportunity. The Dessert Dash is then referred to by that sponsors name as in “Point B Solutions Dessert Dash”.\nAuction Powerhouse Tip: One of my favorite parties is The Dessert Dash baking party. This party is held the week before the auction at the volunteer’s house in charge of the auction. Desserts are baked and prepared and then frozen. This same committee then meets the day of the auction and assembles all the desserts over brunch and prepares the transport. The Dessert Dash Baking Committee is all about fun, food and friendship.']"	['<urn:uuid:1c1a93cd-aae5-4b88-82f5-84b07a620d4e>', '<urn:uuid:5a48db54-a427-4973-895c-d750273af13f>']	open-ended	with-premise	long-search-query	distant-from-document	comparison	expert	2025-05-12T10:19:05.071190	15	93	3136
70	comparing nosql document databases query performance vs scaling characteristics how mongodb couchbase handle large datasets	Document-based databases like MongoDB and Couchbase handle large datasets differently. For querying, retrieving a value means getting the whole document which affects performance. However, these databases excel at scaling - they can handle very large amounts of data and are easily scalable within clusters. Couchbase specifically addresses this with its N1QL query language and Global Secondary Indexes (GSI) that minimize scatter-gather operations for better query performance, while maintaining the ability to scale through multi-dimensional scaling.	"['SQL Gene in NoSQL: Introducing N1QL\nNoSQL have been a reaction to relational movement that required rigid schema. Relational database bundled SQL query language to powerfully search and reshape the results to compensate. With version 4.0, Couchbase Server contains both the SQL gene and the NoSQL gene in its gene-pool! Couchbase Server with SQL has both the flexible data model with JSON Documents AND it has a powerful SQL language – N1QL, that is by far the most expressive language for JSON data. The combination of both JSON and SQL produces a powerful pairing to allow simplified programmability and powerful new enterprise applications that empower the new digital economy.\nWhy add a SQL Gene to Couchbase Server?\nThis is an easy question to answer:\nSQL is the most powerful way to query data. SQL has been around for more than 4 decades. While most other languages in its age group are no longer in demand, SQL continues to be the language of choice in many modern data processing platforms.\nSQL is the de-facto standard for querying data between various data tools and platforms. SQL sits at the center of wide range of tools from data visualization tools, enterprise reporting environments, analytics apps to ETL platforms.\nIt also helps that, with its long life, SQL is already a tool in many developers toolchest. It may be the only thing both you and your computer-scientist parent have in common.\n|Download Couchbase Server 4.0 Release Candidate|\nFirst Glance at N1QL\nWith the new SQL gene in Couchbase Server, you are getting an expressive query language and query engine – N1QL, combined with a brand new powerful indexing mechanism – the new global secondary indexes.\nThis should look familiar to many of you. This is N1QL:\n<strong><em>SELECT * FROM bucket\nWHERE bucket.quantity>100 OR\nbucket.price < 9.95</em></strong>\nCannot tell the difference between the relational SQL you know and Couchbase N1QL? Don\'t worry… They are very close cousins. However underneath there is a lot that is unique to N1QL.\nN1QL is SQL+JSON: Couchbase Server is a document database with flexible schema built on the JSON foundation. With N1QL + JSON development gets simplified. N1QL works with JSON but does not compromise on the completeness of the SQL language by supporting cross document operations like JOINs or with clauses that can pivot JSON data like NEST and UNNEST.\nN1QL gives you Performance with a new powerful indexer – GSI: Global secondary indexes enables unmatched low latency and scalability. Global secondary indexes are built to provide the best scan latency and throughput for demanding modern big data applications.\nN1QL Scales with the new Multi-dimensional Scaling Model: N1QL query environment and the new global secondary indexes are architected to be independently scaled from the Core Data Operations, removing the common trade offs of other NoSQL and relational databases of choosing a unified scale model for all workloads from core data operations to indexing to queries.\nLets take a more detailed look at these 3 unique properties of N1QL.\nRelational SQL operates over tables and columns and needs a strict schema. With N1QL, all queries are executed over JSON data in Couchbase Server. You don\'t have to deal with a strict tables and columns structure OR worry about changing schema as your data model evolves.\nN1QL will adapt to the schema the JSON documents present as they get created and updated in your Couchbase Server bucket. Unlike SQL in relational databases, attributes like quantity or price that are referenced above can be missing. You can express your query either to disqualify documents that do not contain the attribute OR you can query the attribute only when it exists.\n<strong><em>SELECT * FROM bucket b1\nWHERE b1.quantity>100 OR\n(b1.price IS NOT MISSING AND b1.price < 9.95)</em></strong>\nAnother important difference is that N1QL delivers JSON result-sets an lets you shape the JSON with powerful operators like JOINs NEST and UNNEST.\n""beer_name"": ""Benchwarmer Porter"",\n""brewery_name"": ""Cooperstown Brewing Company"",\n""country"": ""United States""\n""beer_name"": ""Old Jubilation Ale"",\n""brewery_name"": ""Avery Brewing Company"",\n""country"": ""United States""\nFor those of you using classic reporting, data visualization or ETL (extract-transform-load) tools: good news is that you can get high fidelity ODBC and JDBC drivers that give you seamless integration with a “rows and columns” style resultset from N1QL. Thanks to Simba ODBC and JDBC drivers, you can use your favorite tools like Tableau, Excel or PowerBI tools with the ODBC/JDBC drivers to integrate your full data lifecycle story together.\nHigh Performance Queries with the New Global Secondary Indexing (GSI)\nGSI bring in a new powerful indexer in addition to the View and Spatial indexers with incremental map-reduce in Couchbase Server 4.0. GSI capability is purpose built for N1QL and for big data applications with high rate of queries. GSIs achieves the great performance boost with a unique architecture which minimize the effects of wide scatter-gathers for processing queries.\nGSIs come with independent partitioning and scalability with Multi-dimensional Scaling (more on MDS later). GSI is also built on a foundation of a strong storage and caching engine called ForestDB.\nIn the figure below, the left and right side shows the rough steps of execution of a common query.\nQuery Without GSI: The case on the left depicts how many existing products work to execute queries in a distributed environment. In absence of GSI, indexes are distributed aligned to the data partitioning scheme over N nodes. The execution plan for a typical query entail a wide scatter to N nodes, a gather from N nodes to capture the results from each node, and a final execution that compiles the final results of the query.\nQuery with GSI: Execution steps with GSI does not need to scatter-gather. Query can hit the index that exist on a single node and return the results.\nThe way to manage GSI should be very familiar if you have worked with relational indexes before.\n<strong><em>CREATE INDEX friends_index\nWHERE type=”user profile”\nNew Scalability Model: Multidimensional Scaling\nTo explain multi-dimensional scaling, lets start from the high level server architecture: With Couchbase Server 4.0, each node contains a cluster manager, data, index and query service components as well as the storage engine and managed cache.\nHere are the details on each component:\nCluster Manager is responsible for governing cluster operations. Nodes joining and leaving the cluster or operations like rebalance are performed by the cluster manager.\nData Service is responsible for core data operations through get/set API which allows you to interact with a given key. Data Service also contains the View engine: the incremental Map-Reduce engine that existed in previous versions.\nIndex Service manages the new indexes that are purpose built for N1QL queries. The new global secondary indexes are maintained and can be queries through the index service.\nQuery Service manages the N1QL query interactions. It receives and executes the query to generate the final result for the applications.\nAll Services comes with core services that manage their storage and caching needs.\nAll smart clients and ODBC and JDBC drivers come with a “cluster map” that describe which nodes to go to for the given API calls for most efficient execution.\nEven though every node contains the identical set of components, with the new scalability model, you can elastically turn on/off the services in blue on each node and create “zones” that independently scale services such as data, index and query operations and pick the best HW design per zone based on your workload.\nYou can still deploy the cluster the way you do today with Couchbase Server 3.0 – pick a single HW setup for all nodes, enable all the services on all the nodes and distribute the load across all the nodes. This is your good old uniform horizontal scale out. However the work needed to be done on the nodes to do core data operations like fast GET/SETs vs maintain GSI Indexes vs Process N1QL Queries all have competing requirements. Sharing the same node does not always make sense if you are sensitive to latencies of your operations.\nThe added model of multi-dimensional scalability allow you to divide the cluster into “zones” that run individual services (data, query and index) and in turn, allow independently choice of the HW for each zone. You can choose the best HW that goes with the type of indexing and querying you want to do and still maintain low latency in your core GET/SET operations.\nIn the figure below, the cluster has 3 zones that separate data, index and query services. Gray boxes represent services that are disabled on a given node.\nThere is more… much more…\nWe scratched the surface of N1QL and for a deeper view, you can download and play with the Release Candidate version of Couchbase Server 4.0.', 'A Comparison Of NoSQL Database Management Systems And Models\nNoSQL databases try to offer certain functionality that more traditional relational database management systems do not. Whether it is for holding simple key-value pairs for shorter lengths of time for caching purposes, or keeping unstructured collections (e.g. collections) of data that could not be easily dealt with using relational databases and the structured query language (SQL) – they are here to help.\nIn this DigitalOcean article, we are going to try to introduce you to various popular NoSQL database management systems and explain their purposes and functionality, so that you can decide which one to use, or if they even suit your application\'s needs -- at all.\n1. Database Management Systems\n2. NoSQL Database Management Systems\n- Key / Value Based\n- Column Based\n- Document Based\n- Graph Based\n3. Key / Value Based NoSQL Database Management Systems\n- Popular Key / Value Based Databases\n- When To Use\n4. Column Based NoSQL Database Management Systems\n- Popular Column Based Databases\n- When To Use\n5. Document Based NoSQL Database Management Systems\n- Popular Document Based Databases\n- When To Use\n6. Graph Based NoSQL Database Management Systems\n- Popular Graph Based Databases\n- When To Use\n7. NoSQL DBMSs In Comparison To Relational DBMSs\n- When To Use NoSQL Databases\nDatabase Management Systems\nDatabases are logically modeled storage spaces for all kinds of different information (data). Each database, other than schema-less ones, have a model which provides structure for the data being dealt with. Database management systems are applications (or libraries) which manage databases of various shapes, sizes, and sorts.\nNote: To learn more about Database Management Systems, check out our article: Understanding Databases.\nNoSQL Database Management Systems\nIn the past decade or so, relational database management systems have been the choice of many developers and system administrators for a variety of applications, for a variety of reasons. Despite not being exactly flexible, the powerful nature of many RDBMS allowed complex database set-ups to be created, queried and used. This was more than enough for many requirements, since it was not until long ago that different needs started to rise.\nThe term ""NoSQL"" was coined over a decade ago, funnily enough as a name to yet-another relational database. However, this database had a different idea behind it: eliminating the use of the standardised SQL. In the next years to come, others picked up and continued to grow this thought, by referring to various other non-relational databases as NoSQL databases.\nBy design, NoSQL databases and management systems are relation-less (or schema-less). They are not based on a single model (e.g. relational model of RDBMSs) and each database, depending on their target-functionality, adopt a different one.\nThere are almost a handful of different operational models and functioning systems for NoSQL databases.:\n- Key / Value:\ne.g. Redis, MemcacheDB, etc.\ne.g. Cassandra, HBase, etc.\ne.g. MongoDB, Couchbase, etc\ne.g. OrientDB, Neo4J, etc.\nIn order to better understand the roles and underlying technology of each database management system, let\'s quickly go over these four operational models.\nKey / Value Based\nWe will begin our NoSQL modeling journey with key / value based database management simply because they can be considered the most basic and backbone implementation of NoSQL.\nThese type of databases work by matching keys with values, similar to a dictionary. There is no structure nor relation. After connecting to the database server (e.g. Redis), an application can state a key (e.g.\nthe_answer_to_life) and provide a matching value (e.g.\n42) which can later be retrieved the same way by supplying the key.\nKey / value DBMSs are usually used for quickly storing basic information, and sometimes not-so-basic ones after performing, for example, a CPU and memory intensive computation. They are extremely performant, efficient and usually easily scalable.\nNote: When it comes to computers, a dictionary usually refers to a special sort of data object. They constitutes of arrays of collections with individual keys matching values.\nColumn based NoSQL database management systems work by advancing the simple nature of key / value based ones.\nDespite their complicated-to-understand image on the internet, these databases work very simply by creating collections of one or more key / value pairs that match a record.\nUnlike the traditional defines schemas of relational databases, column-based NoSQL solutions do not require a pre-structured table to work with the data. Each record comes with one or more columns containing the information and each column of each record can be different.\nBasically, column-based NoSQL databases are two dimensional arrays whereby each key (i.e. row / record) has one or more key / value pairs attached to it and these management systems allow very large and un-structured data to be kept and used (e.g. a record with tons of information).\nThese databases are commonly used when simple key / value pairs are not enough, and storing very large numbers of records with very large numbers of information is a must. DBMS implementing column-based, schema-less models can scale extremely well.\nDocument based NoSQL database management systems can be considered the latest craze that managed to take a lot of people by storm. These DBMS work in a similar fashion to column-based ones; however, they allow much deeper nesting and complex structures to be achieved (e.g. a document, within a document, within a document).\nDocuments overcome the constraints of one or two level of key / value nesting of columnar databases. Basically, any complex and arbitrary structure can form a document, which can be stored using these management systems.\nDespite their powerful nature, and the ability to query records by individual keys, document based management systems have their own issues and downfalls compared to others. For example, retrieving a value of a record means getting the whole lot of it and same goes for updates, all of which affect the performance.\nFinally, the very interesting flavour of NoSQL database management systems is the graph based ones.\nThe graph based DBMS models represent the data in a completely different way than the previous three models. They use tree-like structures (i.e. graphs) with nodes and edges connecting each other through relations.\nSimilarly to mathematics, certain operations are much simpler to perform using these type of models thanks to their nature of linking and grouping related pieces of information (e.g. connected people).\nThese databases are commonly used by applications whereby clear boundaries for connections are necessary to establish. For example, when you register to a social network of any sort, your friends\' connection to you and their friends\' friends\' relation to you are much easier to work with using graph-based database management systems.\nKey / Value Based NoSQL Database Management Systems\nKey / Value data stores are highly performant, easy to work with and they usually scale well.\nPopular Key / Value Based Databases\nSome popular key / value based data stores are:\nIn-memory K/V store with optional persistence.\nHighly distributed, replicated K/V store.\n- Memcached / MemcacheDB:\nDistributed memory based K/V store.\nWhen To Use\nSome popular use cases for key / value based data stores are:\nQuickly storing data for - sometimes frequent - future use.\nSome K/V stores (e.g. Redis) supports lists, sets, queues and more.\n- Distributing information / tasks:\nThey can be used to implement Pub/Sub.\n- Keeping live information:\nApplications which need to keep a state cane use K/V stores easily.\nColumn Based NoSQL Database Management Systems\nColumn based data stores are extremely powerful and they can be reliably used to keep important data of very large sizes. Despite not being ""flexible"" in terms of what constitutes as data, they are highly functional and performant.\nPopular Column Based Databases\nSome popular column based data stores are:\nColumn based data store based on BigTable and DynamoDB.\nData store for Apache Hadoop based on ideas from BigTable.\nWhen To Use\nSome popular use cases for column based data stores are:\n- Keeping unstructured, non-volatile information:\nIf a large collection of attributes and values needs to be kept for long periods of time, column-based data stores come in extremely handy.\nColumn based data stores are highly scalable by nature. They can handle an awful amount of information.\nDocument Based NoSQL Database Management Systems\nDocument based data stores are excellent for keeping a lot of unrelated complex information that is highly variable in terms of structure from one another.\nPopular Document Based Databases\nSome popular document based data stores are:\nJSON-based, Memcached-compatible document-based data store.\nA ground-breaking document-based data store.\nAn extremely popular and highly-functional database.\nWhen To Use\nSome popular use cases for document based data stores are:\n- Nested information:\nDocument-based data stores allow you to work with deeply nested, complex data structures.\nOne of the most critical functionalities of document-based data stores are the way they interface with applications: Using JS friendly JSON.\nGraph Based NoSQL Database Management Systems\nGraph based data stores offer a very unique functionality that is unmatched with any other DBMSs.\nPopular Graph Based Databases\nSome popular graph based data stores are:\nA very fast graph and document based hybrid NoSQL data store written in Java that comes with different operational modes.\nA schema-free, extremely popular and powerful Java graph based data store.\nWhen To Use\nSome popular use cases for graph based data stores are:\n- Handling complex relational information:\nAs explained in the introduction, graph databases make it extremely efficient and easy to use to deal with complex but relational information, such as the connections between two entities and various degrees of other entities indirectly related to them.\n- Modelling and handling classifications:\nGraph databases excel in any situation where relationships are involved. Modelling data and classifying various information in a relational way can be handled very well using these type of data stores.\nNoSQL DBMSs In Comparison To Relational DBMSs\nIn order to draw a clear picture of how NoSQL solutions differ from relational database management systems, let\'s create a quick comparison list:\nWhen To Use NoSQL Databases\n- Size matters:\nIf will be working with very large sets of data, consistently scaling is easier to achieve with many of the DBMS from NoSQL family.\nNoSQL databases are usually faster - and sometimes extremely speedier - when it comes to writes. Reads can also be very fast depending on the type of NoSQL database and data being queried.\n- Schema-free design:\nRelational DBMSs require structure from the beginning. NoSQL solutions offer a large amount of flexibility.\n- Automated (or easy) replications / scaling:\nNoSQL databases are growing rapidly and they are being actively built today - vendors are trying to tackle common issues and one of them clearly is replication and scaling. Unlike RDBMSs, NoSQL solutions can easily scale and work with(in) clusters.\n- Multiple choices:\nWhen it comes to choosing a NoSQL data store, there are a variety of models, as we have discussed, that you can choose from to get the most out of the database management system - depending on your data type.']"	['<urn:uuid:ebb387a8-8866-45f2-8995-90671a73d88c>', '<urn:uuid:333368df-425c-4d54-a2dc-cb720aa6ac03>']	factoid	with-premise	long-search-query	similar-to-document	multi-aspect	expert	2025-05-12T10:19:05.071190	15	75	3283
71	What shows a horse is relaxed versus what makes Paso Finos special?	A relaxed horse demonstrates elasticity and suppleness through its ability to lengthen and collect its stride (longitudinal suppleness) and bend its body and neck (lateral suppleness). Meanwhile, what makes Paso Finos special is their characteristic genetic 4-beat gait that can be performed at three speeds - Classic Fino (very short steps), Paso Corto (collected with unhurried steps), and Paso Largo (longer extension).	['For over 2000 years man has been riding horses and discovering techniques on how to train them. People have come up with fantastic ideas, including: inside leg to outside rein, figuring out that horses travel in a diagonal frame, go forward first. I am sure they came up with plenty of ideas that did not work as well. One of the ideas that have been developed is the Dressage Pyramid. This is a training tool with its roots in classical dressage.\nI am a Hunter and Jumper trainer and I frequently use the Dressage Pyramid in my training and lessons. It is a very basic visual aid that can be used when schooling young horses, flatting a more experienced horse or teaching all levels of riders from beginner to advanced.\nThis is the first building block of the pyramid. At the walk the horse has a natural 4 beat rhythm to the walk. As the horse walks his head goes up and down so the rider must have an elastic elbow and the hand must move forward and back in time with the horses head. This also happens at the canter, the difference being that the horse canters with a three beat rhythm. The trot has a two beat rhythm to it and the horse’s head stays still so there is no forward and back movement with the hands. I like to work on this rhythm by using cavaletti. At the trot I set 4 cavaletti 4’9” apart that way the horse has to take two steps with its left legs and two steps with its right legs. This is a comfortable distance for most horses and allows the rider to establish a consistent rhythm. At the canter I start with the cavaletti 9 feet apart, so they are set up as a bounce to help develop the horse’s canter. Once the horse does this well I move them to 18 feet apart so the horse has to do one stride between each of them.\nCanterbury Farm Trainers, Greg and Caitlyn. Photo courtesy of Collin Pierson Photography\nThis is the second building block of the triangle. It may be the most important, but one that is skipped the most frequent. A relaxed horse is one that will be a willing partner in what you ask of him, whether that be going forward, collecting or moving laterally. Elasticity and suppleness are the keys here. The horse needs to be able to demonstrate his adjustability by lengthening and collecting his stride, this is known as longitudinal suppleness.\nThe horse also needs to be able to show his adjustability laterally by bending his body and his neck. A horse with lateral suppleness will bend around the rider’s legs through his ribs. The first and most basic lateral movement is riding a corner. This then naturally progresses to riding a circle and continues on to leg yielding and shoulder in. This is the beginning of impulsion.\nKathryn Berry on Caitlyn Shiels’ Cavalier demonstrating connection.\nThis is the third level of the pyramid and basically connection is the acceptance of contact through the acceptance of the aids. By this point, the horse has been ridden forward so that it starts to seek the contact of the hand. The horse will start to come round with the poll being the highest point of contact and the horses nose slightly in front of the vertical. One must be very careful here so that you do not achieve hyper flexion, which is where the horse’s nose comes behind the vertical and gets behind the bridle. The rider must ride with soft elastic arms and let the horse initiate the contact. As horses become more advanced their balance will change. The young horse will start with a longer and lower balance and as he becomes more advanced the balance will become lighter in front.', 'What is a Paso Fino?\nThe Paso Fino has had a long history in the Americas. Its ancestors were first brought to the Caribbean Islands by Columbus on his second voyage from Spain and were bred to be mounts of the Spanish Conquistadors. Centuries of selective breeding by those that colonized the Caribbean and Latin America produced variations of the “Caballo de Criollo,” among them the Paso Fino that flourished initially in Puerto Rico and Colombia, and later in other Latin American countries (primarily Cuba, the Dominican Republic, Aruba, and Venezuela). The Paso Fino did not make its way into the United States until after World War II. The U.S. serviceman stationed in Puerto Rico during the war discovered the Paso Finos and were the first to import them back to the United States after the war. During the 1960’s and 1970’s many Paso Finos began to be imported from Columbia. For a while there was some contention as to which country produced the true Paso Fino. Though there are still some self-professed purists who advocate for one or the other country, the American Paso Fino is often a blend of the best of Puerto Rican and Colombian bloodlines in true melting pot tradition.\nThe Paso Fino has a proud, graceful carriage and a lively but controlled spirit. Its size varies between 13 and 15.2 hands, and comes in all equine colors except appaloosa. It is sure footed and athletic. It can be trained to drive, participate in cow penning or endurance competitions, as well as be a trustworthy trail horse. Its characteristic natural 4 beat gait set this breed apart to make it the smoothest riding horse in the world. The gait of the Paso Fino is genetic and normally exhibited from birth. The footfall is the same pattern as the walk – left rear, left fore, right rear, right fore. Each foot contacts the ground independently in a regular, evenly spaced sequence at precise intervals creating a rapid, unbroken rhythm. The sound of the hoof beats should be absolutely even in cadence and impact. The Paso Fino propels itself primarily from the hind legs while the motion is absorbed in its back and loins. Thus there is no up and down movement of the horse’s croup, and that is why there is no bounce when riding as there would be at the jog or trot of other breeds. The Paso Fino gait is performed at three speeds. The slowest is the Classic Fino, where the horse is fully collected, the footfall is extremely rapid while the steps and extension are very short. The gait is used primarily for show. Not all Paso Finos can execute the Classic Fino gait and those that do are highly prized for breeding. The intermediate speed of the Paso Fino gait is known as the Paso Corto. At the Paso Corto, the horse is collected, and its step are ground covering but unhurried. The speed is comparable to a trot. The fastest speed of the Paso Fino gait is the Paso Largo. This is executed with a longer extension and stride, and a more moderate collection. The speed of each horse’s largo will vary depending on the extension which can be achieved given the horse’s conformation, especially the slope of the shoulder.']	['<urn:uuid:06c4981d-fe70-46f9-9ea6-65cf29bd32dc>', '<urn:uuid:bbd8bc37-8d09-4090-a79f-6bfa203d7a35>']	open-ended	direct	concise-and-natural	similar-to-document	comparison	novice	2025-05-12T10:19:05.071190	12	62	1194
72	How do the traditional houses in the Lowcountry make use of porches to respond to the local climate, and why are some houses positioned sideways to water views?	Porches, in single or double form, provide sunshade from the hot summer sun and prevent direct heat gain while still allowing indirect light. They also function as spaces for outdoor living and sleeping in the evening hours due to the great breeze, especially on upper levels. Houses are sometimes positioned sideways to water views because the porches must face either south or west to provide effective shading benefits.	['Carolina Lowcountry Architecture: The Basis for a Distinct Architecture of the Lowcountry.\nThe Carolina Lowcountry is blessed with a rich history, natural beauty and a distinctive architectural character. You know when you are here, we have live oaks, marshes, fresh shrimp and big porches. Thats how most people would start to characterize the Lowcountry. The architecture here didn’t just randomly happen, a distinctive architecture is the result of climatic characteristics, local materials and construction methods, and local cultural patterns.\nThe Lowcounty is primarily made up of villages and towns but we are blessed with two of the greatest cities in North America, Charleston and Savannah. Both of these are true urban cities and you will see the architectural character adapt as it also responses to the urban environment a little differently than it would in a rural setting. So while a townhouse might be a little different than a rural river house, you will still see many of the same traits in them. This is actually one of the hallmarks of a true distinct architecture, it transforms / adapts over the exact place it is put and it will adapt over time as well.\nIn the Lowcountry, its is hot and humid here. Our average summer temperature is high 80’s and 90’s with the humidity nearly the same. Prior to the invention of mechanically cooled buildings, heat and air comfort was one of the greatest challenges a Lowcountry building would have to face. The typical Lowcounty home has five design responses to deal the hot humid climate here; 1) a raised living level / foundation, 2) A single room depth, 3) Oriented toward breezes, 4) Porches, 5) High Ceilings and Windows.\nRaising the main living level up has many benefits and can be seen from simple rural cottages to urban buildings. For a typical house, the raised pier foundation is typical and ranges from 32”up to a near full story. This will provide cooling breezes under the main house which will help with clearing the heat of the day and also helping to capture the breezes a little off the ground. Additionally, this will protect against flooding from storms and helps with insects, pests and dirt/dust. Its important to remember that your windows & doors would be assumed to be open. In a more urban setting, you will often see the lower level used for business or service uses which will mitigate the cooling but also provide additional security from the street.\nGetting air movement through each room, or cross ventilation, is a key concern and having a single room depth is the easiest way to do so. Rooms vent best when they have openings on two sides or walls and having a single room depth lets you have venting options if the breeze shifts or a hallway door is shut. Some houses will be a true single room depth such as a Charleston Single or others will be a more complex form of single rooms such as the Beaufort “T” house.\nOrientation to the prevailing breeze is an important consideration and works in conjunction with the single room depth to provide natural cooling and cross ventilation. Generally, this simply means that the house is oriented for the breeze to hit the wide part of the form, whether its the tip of the Beaufort “T” or the long end of the Charleston single. A unique lowcountry feature is that our buildings will rotate to capture the breeze regardless of how they sit on their lot. This means a house who’s primary street is on the north side of the house will “face” the opposite way, although it may still have an entry on the north side. The houses of the villages and towns will rotate at will while the urban grid of Savannah demands a little more conformity and Charleston is a mix of both, its street pattern bending to the breeze while the houses also will rotate.\nWhat is more Lowcountry than porches? Well Southern might be a better term as most of the south uses porches as a climatic response tool. Porches, in either single or double form, provide sunshade from the hot summer sun directly entering your home. Indirect light still reflects into the interior but you are spared most of the heat gain. The more open porches also typically will have a great breeze (especially the upper ones) and thus actually function best for outdoor living and sleeping in the evening hours. This trait also combines with the Orientation trait as these porches must face either south or west to provide the shading benefit. If you see a porched house sitting “sideways” to a great water view or frontage, then this is why.\nOnce you have captured the breeze, you have to now make sure that it can be brought through the building. High ceilings help with this by allowing higher windows and doors which allow a great opening area. The higher ceilings will also allow the hottest air to rise up and keep sitting height a little cooler. The double hung window is also part of this conversation as it allows great ventilation of hotter air via the top sash while the lower sash lets in cooler air.\nThe Lowcountry is a region rich in natural beauty and natural resources but not much in terms of natural building materials. Local materials define the core vocabulary for a distinctive architecture. How the local craftsmen use them is another way.\nTraditionally, the Lowcountry has been a composed of the smaller scale villages and towns. In these smaller, more loosely organized places, the dominate construction material has been wood. Heart Pine and cypress form the backbone of local easy to find and use materials from the earliest times. Both of these woods are tolerant to the humid climate here and the local craftsmen knew how frame it using there strength of the wood. Thus most of the housing stock and much of the village main center will be wood frame and siding. Many houses would have cypress or pine interiors with the level of refinement and detail based on the formality and cost of the home. Douglass Fir or Brazilian Cherry are not local woods and thus not part of the local vocabulary.\nTabby was a local material here that made use of the local sand and oyster shell to form a concrete like material used for foundations and walls. Tabby was an imported cultural material from Spanish Florida but was then embraced and made part of the Lowcounty vocabulary. Tabby is mostly forgotten now as concrete technology has moved on but this is an excellent example how a distinctive architecture evolves over time and incorporates nearby cultural influences.\nBrick is a great building material that makes use of local clay for its core. Brick color will vary based upon the type of clay used it, a brick from Texas Hill County is a different color than a brick from Upstate South Carolina. Brick was an imported material for a long time here in the Lowcounty with Plantations slowly developing a series of kilns for there own use and a later market in the cities of Charleston and Savannah. Brick made from local clay is a Lowcounty material but it is typically used on larger buildings or in more urban areas (early firecodes required brick walls). Plaster and Stucco were often used to finish some of the rather crude handmade bricks for either interior or exterior finishes.\nThe Lowcounty does not have naturally occurring stone at all. Stone is an imported material which is difficult to cut, store and ship so it was reserved for the most important civic buildings such as churches or town halls. Some of the wealthy planters would import British stone for their personal homes but it a rare use. In all cases, the building was formal so the stone work was well cut and dressed.\nThe local traditions of construction also very much shape a distinct architecture. Many parts of the the northeast have strong traditions in ship building and you will see that reflected in how early wood frame houses were framed. Here we have a tradition of rural vernacular construction methods mixed with the more formal methods found mostly in the larger downtown cores. That range defines that Lowcountry Vocabulary. A porch may be a post and beam or it may be a more refined column and trimmed beam. A distinctive architecture has a range to it, it is not a singular solution generally.\nCulture drives much of what we do, who we are and how we do things and that is very true for the Lowcountry. The Lowcountry has a rich history of European people exploring and founding early settlements here dating back to the 16th century. This European culture has given the area its primary pattern of settlement and its primary pattern of architecture, at least until the Machine Age. With a few exceptions, the Lowcountry was founded by the British, Carolina being a Royal Colony. This means that the building patterns and proportions were brought from England, Scotland, Bermuda and Barbados.\nThese founding British core brought with them the design languages and patterns from Robert Adam, Vitruvius Britannia and other works. These in turn owe their allegiance back to the early work of Vitruvius and his famous, Ten Books of Architecture. All these patterns reference classical proportions that are derived from human and natural form. Vitruvius and his much earlier greek predecessors, take their proportioning system from the Golden Mean or what is in all natural things and apply it to their architecture. Thus, a classical column has the proportion of a human body and a classical window has the proportions of the human body also. We humans tend to respond well to these proportions, we are wired that way and thus most people will enjoy well proportioned architecture. (even if they can not articulate why).\nThese proportions and patterns are in the core of the Lowcountry architectural DNA. They have evolved over time but the core human based proportions remained until the Machine Age which changed the proportioning systems to one of Industry not Man. A horizontal Modernist window from 1971 feels stale next to the grace of a double hung of 1848 and proportion is why.\nThe Lowcounty is also unique in having multiple cultural influences on it. As a distinctive architecture can and will evolve over time, you can see that effect of these cultural assimilations. We have mentioned the effect the Spanish Floridian architecture had after the British has stormed St. Augustine in 1702. Also note that traditional color of a porch ceiling, Haint Blue, a Gullah superstition about spirits.\nThe Lowcountry is a special unique place in the world. There are places that are similar to it, but none that match it in all aspects of what makes this a great or unique place. The premise of this essay is that every place has a distinctive architecture to it (or should, sometimes they are lost) and the Lowcounty certainly has a distinctive architecture to it. These distinctive characteristics become the DNA or vocabulary for that place. These characteristics come from the triad of climatic characteristics, local materials and construction methods, and local cultural patterns. The result is a set of architectural massing & forms (floor plans for the layman), materials and construction methods that make up the buildings that define the Lowcountry. This distinctive architecture becomes, as defined by Steve Mouzon, a “Living Tradition” and will evolve and adapt over time and is not static.\nWhile the Lowcountry Architecture will evolve over time, it’s core DNA should be protected here. You can not import another DNA, from say Tuscany Italy, for example and expect that to not look very fake or cartoon like. Nor can you import a generic corporate architecture Atlanta and expect it to enhance the sense of place here. Each time we bring in foreign DNA, we allow a piece of what is special here to be lost. Each generic building brings us one step closer to a generic place and its a slow but steady march to get there and there are great easy alternatives that allow growth and allow each new building to contribute to the growth of the sense of what the Lowcounty is.\nAll photos by the author.']	['<urn:uuid:a44f0f9a-4211-4e4b-a79c-38e3cfce3bf3>']	factoid	direct	verbose-and-natural	similar-to-document	single-doc	expert	2025-05-12T10:19:05.071190	28	68	2062
73	Do both DMIC and Belagavi cluster focus on manufacturing?	Yes, both focus on manufacturing. DMIC aims to create manufacturing clusters and ancillary industries along its corridor, while the Belagavi cluster specifically focuses on manufacturing drones and electric vehicles, with several companies planning to set up manufacturing facilities there.	['Led by technology and investment, India is bringing in a new paradigm shift in its industrialisation journey backed by new industrial corridors. Even though trade routes and harbours played a pivotal role in the beginning, when industrialisation was norm of the day, the Second World War really promoted industrial development supported by ‘discriminating protection’ which flourished cotton textiles, sugar and iron and steel industries.\nAs India also grew with the global industrial revolution, which already finished three phases, getting onto fourth wave, the country is gearing up to be a part of it by focussing on infrastructure-supported industrialisation, which is reflected in the emergence of industrial corridors, where various nodes and industrial hubs, and smart cities are planned. The UPA government’s ‘National Strategy for Manufacturing’ prepared in 2006 and the current NDA government’s Make in India campaign support corridors. India started its journey on industrial corridors with the establishment of the Delhi-Mumbai Industrial Corridor (DMIC) in 2005. It enhanced the industrialisation process between two cities, and various nodes were also established for further development of towns. Besides private city Lavasa near Pune and 24 new cities, the 1,500 km-long DMIC goes along with the Western Dedicated Freight Corridor.\nSo what exactly is an industrial corridor? Besides allocating a package of infrastructure spending to a specific geographical area, industrial corridors help in flourishing industrial development by creating manufacturing clusters and ancillary industries. Besides smart cities, these corridors will be well-connected by road, rail, air or sea.\nInteracting with Deccan Herald, Grant Thornton India Partner Vikram Bapat said industrial corridors will help in the next wave of industrialisation, as they will ring-fence fertile agricultural land for industrial use, and thus benefit both sectors. “By enabling the growth of ancillary units, these industrial corridors will reduce freight and transportation costs. Once GST is rolled out, it will rationalise the supply chain and bring in optimal tax structure,” he said. Bapat also pointed out that the industrial corridors will help attract talent in large numbers and achieve 100 million job target easily.\nManufacturing is the keg pin of industrialisation as the society is moving towards massive consumerism supported by growing disposable income. India imported $300 billion of capital and consumer goods last year, and if 80 per cent of it is produced from India, it will create massive job opportunities and investment. The corridors will become catalysts in this journey.\nUnion Minister of State for Commerce and Industry Nirmala Sitharaman said India’s manufacturing sector’s contribution to GDP is only 17 per cent and the government plans to take it 25 per cent in the current year. Cisco India & SAARC Sales MD Purushottam Kaushik said, “The industrial corridors planned by the government will not only link smart cities, but will also be the main driver of the pledge to Make In India by ensuring seamless connectivity for industrial growth.”\nChandan Mitra, who is the Chairman of the Parliamentary Standing Committee on Commerce, and entrusted with the task of preparing a report for expediting projects, told Deccan Herald that the industrial corridor will lure investment and accentuate industrialisation of India. “The industrial corridors are coming up along with the freight corridors, which will also connect ports, and will help in the transportation of capital goods,” he said. Besides land acquisition and infrastructure development, the panel will look into all issues concerning the corridors financial aspects.\nIndia’s first industrial corridor DMIC got Rs 17,500 crore grant from the Centre and $4.5 billion from the Japanese government for the first phase, which started in 2011. The Japan Bank for International Cooperation (JBIC) holds a 26 per cent stake in DMICDC. But the institutional framework for the formation of node/city-level SPVs for corridors are yet to come up.\nAccording to Bapat, there is a need for a separate strategy to incentivise corridor projects to bring in more investment . “One idea could be to institutionalise the corridors and create single-window agencies that can cut through inter-state issues and help businesses to set up seamlessly,” he said. On the role of central and state governments, he said the model for administering corridors should be like that of National Highways.Rumours are rife that the government plans to approve a proposal to establish a National Industrial Corridor Authority (NICA) for implementing corridor projects in the current Budget.\nBesides DMIC, the government plans the development of Amritsar-Kolkata Industrial Corridor (AKIC), Bengaluru- Mumbai Economic Corridor (BMEC), Chennai-Bengaluru Industrial Corridor (CBIC), Visakhapatnam-Chennai Industrial Corridor (VCIC) and Chennai-Kolkata Industrial Corridor. DMIC is one of the world’s largest infrastructure projects and is expected to fetch $90 billion investment. Six states, namely, Uttar Pradesh, Haryana, Madhya Pradesh, Gujarat, Rajasthan and Maharashtra, have formed SPVs for the development of DMIC’s nodes.\nDMIC Chief Executive Alkesh K Sharma said that they are in the final stages of completing the pre-PPP process, which includes forming SPVs and creating the basic infrastructure like transportation, water treatment, sewage treatment, among others. “This work will be completed in the next 4-5 months and by August, we feel we will be able to invite anchor investors to set up their businesses there,” he said. In the North East, the government mulls a similar mega infrastructure project — requiring a basic investment of Rs 11,000 crore (not including land and power) — which will link the states of the region with each other and to multiple border points.\nMultilateral agencies like Japan International Cooperation Agency (JICA), JBIC (which are supporting DMIC and CBIC) and the Asian Development Bank (which has completed a conceptual development plan for VCIC) have evinced interest in investing these projects. On the CBIC, the nodal agency JICA has completed the master plan, and three nodes have been identified. ADB has completed a conceptual development plan report on VCIC and two nodes have been identified for master planning. The AKIC feasibility study is under progress.\nCisco’s Kaushik said the Make In India and industrial corridor projects hold tremendous value for the company. “The latest report from Sustainability Outlook pegs the smart city market in India at $45-$50 billion over the next five years. We are working with the government on information and communication technology for the smart cities proposed under the DMIC. These include Dholera Special Investment Region in Gujarat, Shendra-Bidkin Mega Industrial Park in Maharashtra, Manesar Bawal Investment Region in Haryana and Khushkhera Bhiwadi Neemrana Investment Region in Rajasthan,” he said.\nIndo-German Chamber of Commerce Director General Bernhard Steinruecke said that German companies are not looking at large investment in the corridor but only parts of that. “We have been looking at Mumbai-Bengaluru corridor and looking at its feasibility study. I came to know that there is a competition between the UK and Germany in this project,” he said.\nScope for Industrialisation\nIndia imported $300 billion of capital and consumer goods last year, and if 80 per cent of it is produced from India, it will create massive job opportunities and investment. The corridors will become catalyst in this journey.But to make the corridors successful, we have to be part of the Industrial Revolution 4.0, which will be shaped by a fresh wave of innovation in areas such as smart robotics, materials that are lighter and tougher, and a manufacturing process built around 3D printing and analytics.\nHere we have to come up with new technologies, new products, new brands and enter new markets so that Make in India will become a grand success. Even though we have entrepreneurs who have performed well, we need strategic alliances with countries like the US, Britain, Germany, Japan, Korea and Taiwan to emulate and correct ourselves to go ahead.\nDIPP secretary Amitabh Kant said, “Industrial corridors will help in many layers of manufacturing, from Industry 4.0 to labour intensive manufacturing, which will bring largescale investment, employment opportunity, productivity gains,” said Kant.\nMassive investment in industrial corridor will pave the way for large human displacement and destruction of fertile agricultural land. According to economist Joseph Schumpeter’s Creative Destruction Theory, an entrepreneur’s introduction of radical innovation into the system will bring in creative destruction of economic value and it will really force sustained long-term economic growth. Let us believe Schumpeter and industrial corridors will help India’s efforts to lead the world in the fourth wave of industrial revolution.\nThis article was published in the Deccan Herald, to read please click here.', '• DRoEV2023 announced over 16 LoIs (Letter of Intent) from the industry covering investments into manufacturing, training & R&D in the cluster\n• To attract global investors & technology companies to engage with local industries, partners, and boost the supply chain\n• Government of Karnataka to drive policy benefits and strong support from the Department of IT/BT, S&T\n• Highlight the unique competitive advantage of the cluster\n• Showcase local talent, IPs, products, and innovation\nTo promote Belagavi as the go-to industrial cluster & position as a global hub for drones and electric vehicles. Karnataka Digital Economy Mission (KDEM) launched the DRoEV 2023 along with industry partners at Mayur Belgaum Presidency Club, Belagavi here today. This is to accelerate the growth in the emerging technology cluster and expand the industrial ecosystem beyond the capital city of Bengaluru.\nFirst aerospace manufacturing SEZ in Belagavi generating close to Rs 400 Cr last 12 months, the first state to roll out EV policy and now the first cluster to come up with a Drone & EV cluster & to support the growth of this ecosystem.\nDRoEV aims to attract global investors in the sector to engage with local industries, partners, and be part of the global supply chain. The cluster is also an excellent opportunity to showcase local talents, intellectual property, products, and innovations while creating a conducive manufacturing environment for the Drones and Electric Vehicle industry.\nHighlighting the unique competitive advantage of the cluster, Mr. B.V. Naidu, Chairman, KDEM said “We want to enable a conducive manufacturing ecosystem and invite global investors to interact with local industries, partners, and be part of the global supply chain. Belagavi is a great location, and this cluster will help build develop an ecosystem for the sector with its proximity to industry hubs, R&D centres, educational institutions, and global networks for the nation. 30,00 students (engineers and ITI) passing out annually – local talent is another key factor that will drive new age industries like Drones & EV in the coming years.”\nList of LOI’s for DroEV Cluster : DroEV 2023 conclave @ Belagavi\n|LoI company||PoC||A brief note|\n|Rivot Motors||Ajit Patil||Manufacturing plant to produce about 1,00,000 units of our two-wheeler at the new EV & Drone cluster at Belagavi resulting in a direct employment of about 500 Personal and indirect employment of about 1000 personnel, we are looking for an investment of about 100Cr spread across 2-3 years|\n|Vikas Composites||Arvind Kapadia||Investment of 4 crores with the employment of 60 people|\n|Accu Square Design and Toolings||Mahanand Uppar||Set up a Rapid Prototyping Facility to support EV & Drone cluster with an investment of 4 cr with the employment of 25 people|\n|Tonchi Polymers||Prakash Mugali||Develop a Rubber and Thermoplastic Products facility Planned investment in 3 to 4 years: 5 Crore Planned employment in 3 to 4 years: 45 people|\n|Hexlattice Materials Pvt Ltd||Prakash Mugali||Develop Graphene based Materials Planned investment in 3 to 4 years: 12 Crore Planned employment in 3 to 4 years: 60 people|\n|Virginia Tech India||Dr. MK Padmanabhan||Collaborate in various activities s involving manufacture/skilling/ Technical advisory and consultancy planned in the new EV & Drone cluster at Belagavi|\n|JMC||Mr. Javaji M||we wish to collaborate in the new EV & Drone cluster at Belagavi. This should help large number of engineers in Drone manufacturing, operations, and services over a period of time.|\n|Vega Aviation||Suhas Chandak||Expected Investment: 60 cr Employment generation: 1200 Benefits in defence, security & agriculture|\n|Servocontrols Aerospace||Deepak Dhadoti||We are planning to invest around Rs. 5 Crores in plant and machineries which will scale up the turnover and benefit to generate employment in region of Hattaragi village, around 10b employees.|\n|Meladath Auto Components Private Limited||Rakesh||Investment of 10 cr with a potential to create 500 jobs|\n|Focus Engineering||Dhananjay M||CoE in Belagavi cluster. Will be benefiting 5000 people through creating jobs|\n|KLS GIT||Raj Belgaumkar||Investment of 2 cr with a potential of benefitting 500 people through creating jobs|\n|KLE Tech||Shivyogi Turmari||Investment of 6 crores on EV Innovation Centre (EVIC) – The state-of-the-art platform for communicating knowledge to engineering students about Electric Vehicles (EV) and to facilitate infrastructure to conduct research in emerging areas/technologies of EVs and finally|\n|Orxa Energies||Ranjitha Ravi|\nDr. E. V. Ramana Reddy, IAS, Additional Chief Secretary, Dept of Electronics, IT BT S&T, Government of Karnataka, iterated the Belagavi’s growing economic stature and ongoing development plans, “Emerging clusters outside Bengaluru, like Hubli-Dharwad- Belagavi, are exerting more significant influence; The Belagavi region, alongside the region’s high talent pool, entrepreneurship, and skilled labour, has been essential to attracting engineering companies. Additionally, the government has undertaken efforts to accelerate the cluster’s growth in other areas. As part of marquee ‘Beyond Bengaluru’ projects, the department of Electronics IT BT S&T with the support of KDEM is now positioning Belagavi region as Drone & EV cluster. We are excited to see the interest from the industry for this. These initiatives combined with industry support will contribute towards the digitization of our nation and harness the limitless potential of the state’s digital economy in order to transform the lives of the people.”\nSpeaking on the occasion, Mr. Sanjeev Gupta, CEO, KDEM said, “Belagavi has provided a fertile ecosystem for many emerging startups across manufacturing and technology sectors. The Indian EV & Drones market is expected to grow at a CAGR of 50% & 10% respectively for the next 5 years and Karnataka will play a leading role in this growth. KDEM is enabling this growth by creating a new cluster and hosting DRoEV 2023 in the already existing industrial clusters in Belagavi will benefit from the multiplier effect of this new venture. Bringing all stakeholders together and showcasing the power of the cluster is one of the key objectives of todays launch event.” he added.\nThe one-day power-packed event brought together industry leaders, policymakers, the R&D community, and startups – with over 200 participants. The stakeholders and sectoral experts shared their stories and saw great potential in the launch of DroEV cluster in Belagavi.\nMany cluster and sector-specific sessions on – Vision Belagavi + Shining stars, Drones – A Birds Eye View, Driving the Drone Economy, Testing and Certification on EV and Drone, MoU Partners – Requisites, Panel Discussions on AeroVision – Institutes, Innovation and Industry, and Localizing EV Supply Chain- Challenges and Opportunities and a fire chat were organized.\nThe event was attended by EV OEMs, EV component manufacturers, battery manufacturers, Charging Equipment & Infrastructure providers, Automobile component manufacturers, UAV & eVTOL OEMs, UAV component manufacturers, UAV service providers, Aerospace component manufacturers, RPTOs & certified drone pilots.\n1. Schedule with speakers\n2. List of LoIs\n3. List of exhibitors\nAbout Karnataka Digital Economy Mission\nKarnataka Digital Economy Mission is a Section 8 organization designed to function as the knowledge bridge between the Government of Karnataka and industry with a view of accelerating the growth and investments of the IT, BT, and S&T sectors in Karnataka. The Karnataka Digital Economy Mission promises to harness the unlimited potential of the digital economy of Karnataka to transform the lives of the people in the state. The IT ecosystem not only uplifts the lives of professionals in the industry but also powers millions of other ancillary jobs across other industries.\nExhibitor List at DroEV 2023 Launch • Accu Square • Deltasys 3D • Global Insurance Brokers • INDrone Aerosys • MAHE – MIT • Meladath Auto • Param People Infotech • Physics Motors • Sarus Aerospace • Soch 3D • Tsuyo • Vega Aviation\n2 thoughts on “LOI’s worth Rs 400 Cr investments to generate over 10000 + jobs: Belagavi Drones & EV Cluster”\nReally I am Appriciated that in Belgaum we will get jobs…\nGovt. Should reciprocate in the same strength and enthusiasm. All should work collectively keeping aside geopolitical issues..']	['<urn:uuid:ee6fec1b-97b1-4ca0-84a9-7fbaa5dcac90>', '<urn:uuid:155e7a40-2972-4171-803f-18d7ab7616f6>']	factoid	direct	concise-and-natural	similar-to-document	comparison	novice	2025-05-12T10:19:05.071190	9	39	2670
74	alcohol daily limit liver transplant meld score	For men, consuming >40-80g alcohol/day (2-8 beers) for >10 years significantly increases liver disease risk, while women require only half that amount. The Model for End-Stage Liver Disease (MELD) score helps determine liver transplant priority and is calculated using laboratory values including bilirubin, creatinine and INR. Currently over 16,000 Americans await liver transplants, with only 6,500 organs available annually.	"['The Problem: Rationing by Place of Residence\nWhether accurately or not, “rationing” was frequently employed as a term of opprobrium during both the health reform debates and subsequent explosion in political vituperation. But whatever that word’s lingering stigma, there are unquestionably situations—long preceding the Patient Protection and Affordable Care Act and almost certain to persist whether it is fully implemented or not—in which a fundamental imbalance between need and supply makes rationing of some health services inevitable. In such circumstances, some socially acceptable mechanism must be available to insure equitable and justifiable allocation processes. Yet in the case of liver transplantation, an especially dramatic example of the imbalance between need and supply, neither the public nor the private sector has been able to ensure equitable allocation.\nRight now, more than 16,000 Americans are awaiting liver transplants, and each year about 10,000 more are determined, as a result of irreversible liver damage, to need one, while for many years the total supply of donated organs has remained relatively steady at about 6,500 . As a nation, we clearly need to do a better job of encouraging organ donation, but that is a very long-run solution.\nReasonable people could well differ on the precise criteria for allocating such a scarce, life-saving resource as donated livers, but it is hard to make a case that the patient’s place of residence should be a criterion. Yet people on liver transplant waiting lists in some major metropolitan areas across the United States are 30 percent less likely than similarly ill people in other communities to receive deceased donor transplants—and, not coincidentally, 30 percent more likely to die while still waiting . In some areas in the United States, death while awaiting a liver transplant is less than 10 percent a year, while in other areas it is more than three times that. This regional variation can be dramatic when comparisons are made between patients from different regions with the same medical priority, as best exemplified by those with cancers confined to the liver that meet well-defined criteria and therefore receive priority for transplantation. In some regions these patients receive transplants within 3 months of being listed, while in another region the wait can exceed 18 months .\nThese geographic disparities are not new. In the mid-1990s, public concerns about equity in access to organ transplantation, notably fuelled by allegations that the retired baseball star Mickey Mantle had received preferential access to a donated liver—and died just 2 months later—led to public reconsideration of existing practices . United Network for Organ Sharing, a private, nonprofit organization chartered by the federal government with responsibility for overseeing the national transplant enterprise, revised its allocation priorities to prevent “gaming” of waiting lists by deemphasizing the importance of time spent on the list.\nUNOS recommended a new process of priority-setting more closely tied to the severity of patients’ illness, but maintained the practice of applying those clinical priorities only within and not across the UNOS regions. The 11 regions, which grew up largely as a matter of historical accident and for mostly administrative purposes, vary considerably from one another in population, incidence of end-stage liver disease, and rates of organ procurement. But the proposed revisions appeared to fan, rather than dampen, the controversy.\nIn response, the Secretary of Health and Human Services appointed a special, ad-hoc review panel to conduct public hearings on the general issue of allocation of donated organs, which thereafter made its recommendations to the secretary in early 1997. Based on the panel’s recommendations, the secretary then promulgated new regulations UNOS would be required to follow in establishing liver allocation policies, including a specific requirement that geographic equity be addressed . In response to complaints about the geographic equity requirements, however, Congress voted to suspend implementation of those regulations and requested that the Institute of Medicine perform a study of the policies contained in the regulations and their likely impact on the transplantation process.\nThe IOM report, completed in 1999, specifically recommended that existing UNOS policies, which called for a new, quantitative system of setting priorities based on medical criteria, be applied uniformly across geographical areas with populations of roughly nine million, a system that would effectively supersede UNOS’s regional structure . While UNOS has subsequently refined its medical prioritization criteria, livers are still allocated on the basis of medical need within a given UNOS region, producing the discrepancies in waiting times, transplantation rates, and outcomes.\nThe continuing centrality of the UNOS regions in liver allocation has also fostered a system in which patients with sufficient sophistication and financial resources strategically seek care—and placement on waiting lists—in regions with shorter waiting times and higher transplantation rates than those in which they reside. While allowed by the current rules, this option, of course, is only available to those with access to such resources, adding a significant socioeconomic gradient to location-based disparities. Steve Jobs, a resident of Northern California, received a liver transplant in 2009 in Tennessee . For most people awaiting transplantation, it is simply not practical to reside in another city, often far from their families and support systems, while awaiting a transplantable organ.\nOrganizational and Political Considerations\nLike the Accreditation Council on Graduate Medical Education, the Joint Commission, and literally dozens of other organizations in health care and other sectors of public policy, UNOS is a private, not-for-profit organization that has been delegated an important public function by the federal government, although UNOS, unlike ACGME and the Joint Commission, relies heavily on direct public funding as well. In a nation that has always been suspicious of governmental authority and reluctant to permit governments to exercise discretion in especially sensitive matters, such delegation gives the appearance of taking decisions “out of politics” and putting them in the hands of presumably disinterested, objective authorities.\nUNOS is governed by a board of directors comprising representatives of the major “stakeholders” in organ allocation. At least half the board, under UNOS bylaws, must be physicians engaged in transplantation, and another 25 percent must be people who are awaiting or have received transplants or have donated organs and their family members. Other enumerated categories of stakeholders include representatives of local organ procurement organizations, histocompatibility laboratories or their scientific experts, transplant coordinators (generally hospital or medical center employees), nonphysician transplant professionals, and representatives of health care fundraising or advocacy organizations—such as the National Kidney Foundation—and the general public. Critically, most UNOS board members are elected on a regional basis, from within each of the 11 geographic regions around which existing allocation policies are organized. Board seats are partially based on regional representation, despite the threefold variation in population across the UNOS regions .\nTo the extent that interregional equity is a concern in the development and administration of organ allocation policies, UNOS’s bylaws actually reinforce the existing regional structure and the importance of regional interests. And while transplantable organs are a scarce resource, transplant centers are not. Judging by the numbers of transplants performed at many centers, a more rational allocation system would have fewer. With modern preservation capabilities, there is little impediment to a liver traveling to a patient in greater need than a patient in the region where the liver originated. But medical centers feel a variety of pressures to maintain transplant programs for academic and competitive reasons, and many programs are quite lucrative.\nAt least in the particular form under which it is organized and governed, therefore, UNOS cannot really be said to have taken the politics out of organ allocation. Rather, it has replaced the political conflicts in government with its own internal political divisions. In a situation in which there is a relatively fixed supply of a valued commodity, allocation is what social scientists would describe as a zero-sum game: any change in formula that makes one stakeholder better off will adversely affect the others. When regional representation encompasses wide variation in populations and need for the commodity, moving away from an inequitable status quo is thus extremely difficult.\nThe transplant community did a good job of addressing the IOM’s call for the creation of a disease-severity scoring system (the model for end-stage liver disease or MELD score) to more accurately and fairly assess a patient’s medical need for transplantation. While there are some patients whose disease severity is not fully captured by the MELD system, all would acknowledge that it is far more closely tied to objective clinical values than the prior system, which included a number of subjective variables . In addition, per IOM’s recommendations, the effect of waiting time on allocation is now minimal (except for patients with the same MELD score), which is definitely another step towards more equitable allocation.\nDisappointingly, however, the transplant community has largely ignored the recommendation to eliminate geographic inequities by reorganizing liver distribution into uniform organ allocation areas to ensure broader sharing such that “allocation be based on common medical criteria and not accidents of geography” . In short, over a period of more than a decade in which thousands of lives have been at stake, neither the responsible public nor private institutions have acted to redress these geographic inequities. Instead, as happens so often in the American health care system, rationing takes place in an arbitrary and haphazard way, in which only those with considerable resources are able to escape correctable “accidents of geography.”\nUnited Network for Organ Sharing. View data reports. http://optn.transplant.hrsa.gov/latestData/viewDataReports.asp. Accessed January 23, 2012.\nGrady D, Meier B. A transplant that is raising many questions. New York Times. June 22, 2009. http://query.nytimes.com/gst/fullpage.html?res=9B04EED81E38F930A15755C0A96F9C8B63&pagewanted=all. Accessed February 21, 2012.\nUS Department of Health and Human Services. 63CFR 12696-16338. Organ Procurement and Transplantation Network.\nInstitute of Medicine. Organ Procurement and Transplantation: Assessing Current Policies and the Potential Impact of the DHHS Final Rule. Washington, DC: National Academies Press; 1999.\nUnited Network for Organ Sharing. Bylaws. http://www.unos.org/about/index.php?topic=bylaws. Accessed January 23, 2012.\n- Coombes JM, Trotter JF. Development of the allocation system for deceased donor liver transplantation. Clin Med Res. 2005;3(2):87-92.', ""Alcoholic Liver Disease\nAlcohol consumption is high in most Western countries. According to the Diagnostic and Statistical Manual of Mental Disorders, Fifth Edition (DSM-5), 8.5% of US adults are estimated to have alcohol use disorder in any 12-month period (see Alcohol Use Disorders and Rehabilitation). The male:female ratio is about 2:1. Disorders that occur in alcohol abusers, often in sequence, include\n(See also the 2018 American College of Gastroenterology’s practice guidelines for Alcoholic Liver Disease.)\nHepatocellular carcinoma may also develop in patients with cirrhosis, especially if iron accumulation coexists.\nThe main risk factors for alcoholic liver disease are\nAmong susceptible people, a linear correlation generally exists between the amount and duration of alcohol use and the development of liver disease.\nAlcohol content is estimated to be the beverage volume (in mL) multiplied by its percentage of alcohol. For example, the alcohol content of 45 mL of an 80-proof (40% alcohol) beverage is 18 mL by volume. Each mL contains about 0.79 g of alcohol. Although values can vary, the percentage of alcohol averages 2 to 7% for most beers and 10 to 15% for most wines. Thus, a 12-ounce (oz) glass of beer contains between about 5 to 20 g of alcohol, and a 5-oz glass of wine contains between about 12 to 18 g, and a 1 1/2-oz shot of hard liquor contains about 14 g.\nRisk of liver disease increases markedly for men who drink > 40 g, particularly > 80 g, of alcohol/day (eg, about 2 to 8 cans of beer, 3 to 6 shots of hard liquor, or 3 to 6 glasses of wine) for > 10 years. For cirrhosis to develop, consumption must usually be > 80 g/day for > 10 years. If consumption exceeds 230 g/day for 20 years, risk of cirrhosis is about 50%. But only some chronic alcohol abusers develop liver disease. Thus, variations in alcohol intake do not fully explain variations in susceptibility, indicating that other factors are involved.\nWomen are more susceptible to alcoholic liver disease, even after adjustment for body size. Women require only 20 to 40 g of alcohol to be at risk—half of that for men. Risk in women may be increased because they have less alcohol dehydrogenase in their gastric mucosa; thus, more intact alcohol reaches the liver.\nOther risk factors include iron accumulation in the liver (not necessarily related to iron intake) and concomitant hepatitis C.\nAlcohol (ethanol) is readily absorbed from the stomach, but most is absorbed from the small intestine. Alcohol cannot be stored. A small amount is degraded in transit through the gastric mucosa, but most is catabolized in the liver, primarily by alcohol dehydrogenase (ADH) but also by cytochrome P-450 2E1 (CYP2E1) and the microsomal enzyme oxidation system (MEOS).\nMetabolism via the ADH pathway involves the following:\nADH, a cytoplasmic enzyme, oxidizes alcohol into acetaldehyde. Genetic polymorphisms in ADH account for some individual differences in blood alcohol levels after the same alcohol intake but not in susceptibility to alcoholic liver disease.\nAcetaldehyde dehydrogenase (ALDH), a mitochondrial enzyme, then oxidizes acetaldehyde into acetate. Chronic alcohol consumption enhances acetate formation. Asians, who have lower levels of ALDH, are more susceptible to toxic acetaldehyde effects (eg, flushing); the effects are similar to those of disulfiram, which inhibits ALDH.\nThese oxidative reactions generate hydrogen, which converts nicotinamide-adenine dinucleotide (NAD) to its reduced form (NADH), increasing the redox potential (NADH/NAD) in the liver.\nThe increased redox potential inhibits fatty acid oxidation and gluconeogenesis, promoting fat accumulation in the liver.\nChronic alcoholism induces the MEOS (mainly in endoplasmic reticulum), increasing its activity. The main enzyme involved is CYP2E1. When induced, the MEOS pathway can account for 20% of alcohol metabolism. This pathway generates harmful reactive oxygen species, increasing oxidative stress and formation of oxygen-free radicals.\nFat (triglycerides) accumulates throughout the hepatocytes for the following reasons:\nExport of fat from the liver is decreased because hepatic fatty acid oxidation and lipoprotein production decrease.\nInput of fat is increased because the decrease in hepatic fat export increases peripheral lipolysis and triglyceride synthesis, resulting in hyperlipidemia.\nHepatic fat accumulation may predispose to subsequent oxidative damage.\nOxidative stress is increased by\nLiver hypermetabolism, caused by alcohol consumption\nFree radical–induced lipid peroxidative damage\nReduction in protective antioxidants (eg, glutathione, vitamins A and E), caused by alcohol-related undernutrition\nBinding of alcohol oxidation products, such as acetaldehyde, to liver cell proteins, forming neoantigens and resulting in inflammation\nAccumulation of neutrophils and other white blood cells (WBCs), which are attracted by lipid peroxidative damage and neoantigens\nInflammatory cytokines secreted by WBCs\nAccumulation of hepatic iron, if present, aggravates oxidative damage. Iron can accumulate in alcoholic liver disease through ingestion of iron-containing fortified wines; most often, the iron accumulation is modest. This condition must be differentiated from hereditary hemochromatosis.\nA vicious circle of worsening inflammation occurs: Cell necrosis and apoptosis result in hepatocyte loss, and subsequent attempts at regeneration result in fibrosis. Stellate (Ito) cells, which line blood channels (sinusoids) in the liver, proliferate and transform into myofibroblasts, producing an excess of type I collagen and extracellular matrix. As a result, the sinusoids narrow, limiting blood flow. Fibrosis narrows the terminal hepatic venules, compromising hepatic perfusion and thus contributing to portal hypertension. Extensive fibrosis is associated with an attempt at regeneration, resulting in liver nodules. This process culminates in cirrhosis.\nHepatic steatosis, alcoholic hepatitis, and cirrhosis are often considered separate, progressive manifestations of alcoholic liver disease. However, their features often overlap.\nHepatic steatosis (fatty liver) is the initial and most common consequence of excessive alcohol consumption. Hepatic steatosis is potentially reversible. Macrovesicular fat accumulates as large droplets of triglyceride and displaces the hepatocyte nucleus, most markedly in perivenular hepatocytes. The liver enlarges.\nAlcoholic hepatitis (steatohepatitis) is a combination of hepatic steatosis, diffuse liver inflammation, and liver necrosis (often focal)—all in various degrees of severity. The damaged hepatocytes are swollen with a granular cytoplasm (balloon degeneration) or contain fibrillar protein in the cytoplasm (Mallory or alcoholic hyaline bodies). Severely damaged hepatocytes become necrotic. Sinusoids and terminal hepatic venules are narrowed. Cirrhosis may also be present.\nAlcoholic cirrhosis is advanced liver disease characterized by extensive fibrosis that disrupts the normal liver architecture. The amount of fat present varies. Alcoholic hepatitis may coexist. The feeble compensatory attempt at hepatic regeneration produces relatively small nodules (micronodular cirrhosis). As a result, the liver usually shrinks. In time, even with abstinence, fibrosis forms broad bands, separating liver tissue into large nodules (macronodular cirrhosis—see Cirrhosis: Pathophysiology).\nSymptoms usually become apparent in patients during their 30s or 40s; severe problems appear about a decade later.\nHepatic steatosis is often asymptomatic. In one third of patients, the liver is enlarged and smooth, but it is not usually tender.\nAlcoholic hepatitis ranges from mild and reversible to life threatening. Most patients with moderate disease are undernourished and present with fatigue, fever, jaundice, right upper quadrant pain, tender hepatomegaly, and sometimes a hepatic bruit. About 40% deteriorate soon after hospitalization, with consequences ranging from mild (eg, increasing jaundice) to severe (eg, ascites, portosystemic encephalopathy, variceal bleeding, liver failure with hypoglycemia, coagulopathy). Other manifestations of cirrhosis may be present.\nCirrhosis, if compensated, may be asymptomatic. The liver is usually small; when the liver is enlarged, hepatic steatosis or hepatoma should be considered. Symptoms range from those of alcoholic hepatitis to the complications of end-stage liver disease, such as portal hypertension (often with esophageal varices and upper gastrointestinal bleeding, splenomegaly, ascites, and portosystemic encephalopathy). Portal hypertension may lead to intrapulmonary arteriovenous shunting with hypoxemia (hepatopulmonary syndrome), which may cause cyanosis and nail clubbing. Acute renal failure secondary to progressively decreasing renal blood flow (hepatorenal syndrome) may develop. Hepatocellular carcinoma develops in 10 to 15% of patients with alcoholic cirrhosis.\nChronic alcoholism, rather than liver disease, causes Dupuytren contracture of the palmar fascia, vascular spiders, myopathy, and peripheral neuropathy. In men, chronic alcoholism causes signs of hypogonadism and feminization (eg, smooth skin, lack of male-pattern baldness, gynecomastia, testicular atrophy, changes in pubic hair). Undernutrition may lead to multiple vitamin deficiencies (eg, of folate and thiamin), enlarged parotid glands, and white nails. In alcoholics, Wernicke encephalopathy and Korsakoff psychosis result mainly from thiamin deficiency. Pancreatitis is common. Hepatitis C occurs in > 25% of alcoholics; this combination markedly worsens the progression of liver disease.\nRarely, patients with hepatic steatosis or cirrhosis present with Zieve syndrome (hyperlipidemia, hemolytic anemia, and jaundice).\nAlcohol is suspected as the cause of liver disease in any patient who chronically consumes excess alcohol, particularly >80 g/day. When the patient's alcohol consumption is in doubt, history should be confirmed by family members. Patients can be screened for alcoholism using the CAGE questionnaire (need to Cut down, Annoyed by criticism, Guilty about drinking, and need for a morning Eye-opener). There is no specific test for alcoholic liver disease, but if the diagnosis is suspected, liver function tests (PT; serum bilirubin, aminotransferase, and albumin levels) and CBC are done to detect signs of liver injury and anemia.\nElevations of aminotransferases are moderate (< 300 IU/L) and do not reflect the extent of liver damage. The ratio of aspartate aminotransferase (AST) to alanine aminotransferase (ALT) is ≥ 2. The basis for low ALT is a dietary deficiency of pyridoxal phosphate (vitamin B6), which is needed for ALT to function. Its effect on AST is less pronounced. Serum gamma-glutamyl transpeptidase (GGT) increases, more because ethanol induces this enzyme than because patients have cholestasis or liver injury or use other drugs. Serum albumin may be low, usually reflecting undernutrition but occasionally reflecting otherwise obvious liver failure with deficient synthesis. Macrocytosis with an mean corpuscular volume > 100 fL reflects the direct effect of alcohol on bone marrow as well as macrocytic anemia resulting from folate deficiency, which is common among undernourished alcoholics. Indexes of the severity of liver disease are\nThrombocytopenia can result from the direct toxic effects of alcohol on bone marrow or from splenomegaly, which accompanies portal hypertension. Neutrophilic leukocytosis may result from alcoholic hepatitis, although coexisting infection (particularly pneumonia and spontaneous bacterial peritonitis) should also be suspected.\nImaging tests of the liver are not routinely needed for diagnosis. If done for other reasons, abdominal ultrasonography or CT may suggest hepatic steatosis or show splenomegaly, evidence of portal hypertension, or ascites. Ultrasound elastrography measures liver stiffness and thus detects advanced fibrosis. This valuable adjunct can obviate the need for liver biopsy to check for cirrhosis and help assess prognosis. Its exact role is under study.\nIf abnormalities suggest alcoholic liver disease, screening tests for other treatable forms of liver disease, especially viral hepatitis, should be done.\nBecause features of hepatic steatosis, alcoholic hepatitis, and cirrhosis overlap, describing the precise findings is more useful than assigning patients to a specific category, which can only be determined by liver biopsy.\nNot all experts agree on the indications for liver biopsy. Proposed indications include the following:\nLiver biopsy confirms liver disease, helps identify excessive alcohol use as the likely cause, and establishes the stage of liver injury. If iron accumulation is observed, measurement of the iron content and genetic testing can eliminate hereditary hemochromatosis as the cause.\nFor stable patients with cirrhosis, the American Association for the Study of Liver Diseases (AASLD) recommends that liver ultrasonography, with or without alpha-fetoprotein (AFP) measurement, should be done every 6 months to screen for hepatocellular carcinoma. They also suggest that surveillance not be done for patients with Child’s class C cirrhosis unless they are on the transplant waiting list because of the low anticipated survival for these patients (1).\nPrognosis is determined by the degree of hepatic fibrosis and inflammation. Hepatic steatosis and alcoholic hepatitis without fibrosis are reversible if alcohol is avoided. With abstinence, hepatic steatosis may completely resolve within 6 weeks. Fibrosis and cirrhosis are usually irreversible.\nCertain biopsy findings (eg, neutrophils, perivenular fibrosis) indicate a worse prognosis. Proposed quantitative indexes to predict severity and mortality use primarily laboratory features of liver failure such as prothrombin time, creatinine (for hepatorenal syndrome), and bilirubin levels. The Maddrey discriminant function may be used; it is calculated from the following formula:\nFor this formula, bilirubin level is measured in mg/dL (converted from bilirubin in micromol/L by dividing by 17). A value of > 32 is associated with a high short-term mortality rate (eg, after 1 month, 35% without encephalopathy and 45% with encephalopathy). Other indexes include the Model for End-Stage Liver Disease (MELD) score, Glasgow alcoholic hepatitis score, and Lille model. For patients ≥ 12 years of age, the MELD score is calculated using the following formula:\nOnce cirrhosis and its complications (eg, ascites, bleeding) develop, the 5-year survival rate is about 50%; survival is higher in patients who abstain and lower in patients who continue drinking.\nCoexisting iron accumulation or chronic hepatitis C increases risk of hepatocellular carcinoma.\nAbstinence is the mainstay of treatment; it prevents further damage from alcoholic liver disease and thus prolongs life. Because compliance is problematic, a compassionate team approach is essential. Behavioral and psychosocial interventions can help motivated patients; they include rehabilitation programs and support groups (see Alcohol Use Disorders and Rehabilitation: Maintenance), brief interventions by primary care physicians, and therapies that explore and clarify the motivation to abstain (motivational enhancement therapy).\nDrugs, if used, should only supplement other interventions. Opioid antagonists (naltrexone or nalmefene) and drugs that modulate gamma-aminobutyric acid receptors (baclofen or acamprosate) appear to have a short-term benefit by reducing the craving and withdrawal symptoms. Disulfiram inhibits aldehyde dehydrogenase, allowing acetaldehyde to accumulate; thus, drinking alcohol within 12 hours of taking disulfiram causes flushing and has other unpleasant effects. However, disulfiram has not been shown to promote abstinence and consequently is recommended only for certain patients.\nGeneral management emphasizes supportive care. A nutritious diet and vitamin supplements (especially B vitamins) are important during the first few days of abstinence. Alcohol withdrawal requires use of benzodiazepines (eg, diazepam). In patients with advanced alcoholic liver disease, excessive sedation can precipitate portosystemic encephalopathy and thus must be avoided.\nSevere acute alcoholic hepatitis commonly requires hospitalization, often in an intensive care unit, to facilitate enteral feeding (which can help manage nutritional deficiencies) and to manage specific complications (eg, infection, bleeding from esophageal varices, specific nutritional deficiencies, Wernicke encephalopathy, Korsakoff psychosis, electrolyte abnormalities, portal hypertension, ascites, portosystemic encephalopathy—see elsewhere in THE MANUAL ).\nCorticosteroids (eg, prednisolone 40 mg/day orally for 4 weeks, followed by tapered doses) improve outcome in patients who have severe acute alcoholic hepatitis (Maddrey discriminant function ≥ 32) and who do not have infection, gastrointestinal bleeding, renal failure, or pancreatitis (1).\nOther than corticosteroids and enteral feeding, few specific treatments are clearly established. Antioxidants (eg, S-adenosyl-l-methionine, phosphatidylcholine, metadoxine) show promise in ameliorating liver injury during early cirrhosis but require further study. Therapies directed at cytokines, particularly tumor necrosis factor (TNF)-alpha, and aiming to reduce inflammation have had mixed results in small trials. Pentoxifylline, a phosphodiesterase inhibitor that inhibits TNF-alpha synthesis, had mixed results in clinical trials in patients with severe alcoholic hepatitis. When biologic agents that inhibit TNF-alpha (eg, infliximab, etanercept) are used, risk of infection outweighs benefit. Drugs given to decrease fibrosis (eg, colchicine, penicillamine) and drugs given to normalize the hypermetabolic state of the alcoholic liver (eg, propylthiouracil) have no proven benefit. Antioxidant remedies, such as silymarin (milk thistle) and vitamins A and E, are ineffective.\nLiver transplantation can be considered if disease is severe. With transplantation, 5-year survival rates are comparable to those for nonalcoholic liver disease—as high as 80% in patients without active liver disease and 50% in those with acute alcoholic hepatitis. Because up to 50% of patients resume drinking after transplantation, most programs require 6 months of abstinence before transplantation is done; recent data suggest that earlier transplantation may offer a survival advantage, but currently, this approach is not standard of care.\nRambaldi A, Saconato HH, Christensen E, et al: Systematic review: Glucocorticosteroids for alcoholic hepatitis—A Cochrane Hepato-Biliary Group systematic review with meta-analyses and trial sequential analyses of randomized clinical trials. Aliment Pharmacol Ther 27(12):1167-1178, 2008. doi: 10.1111/j.1365-2036.2008.03685.x.\nRisk of alcoholic liver disease increases markedly in men if they ingest > 40 g, particularly > 80 g, of alcohol/day (eg, about 2 to 8 cans of beer, about 3 to 6 glasses of wine, or 3 to 6 shots of hard liquor) for > 10 years; risk increases markedly in women if they ingest about half that amount.\nScreen patients using the CAGE questionnaire, and when in doubt about the patient's alcohol consumption, consider asking family members.\nTo estimate prognosis, consider unfavorable histologic findings (eg, neutrophils, perivenular fibrosis) and use of a formula (eg, Maddrey discriminant function, Model for End-Stage Liver Disease [MELD] score).\nEmphasize abstinence, provide supportive care, and hospitalize and give corticosteroids to patients with severe acute alcoholic hepatitis.\nConsider transplantation for abstinent patients.\nDrugs Mentioned In This Article\n|Drug Name||Select Trade|\n|No US brand name|""]"	['<urn:uuid:25e76d8e-7cde-4e58-9ac9-3d076a4d62c0>', '<urn:uuid:e2b474a2-b962-48c0-896b-5f03fcff9466>']	factoid	direct	short-search-query	similar-to-document	three-doc	expert	2025-05-12T10:19:05.071190	7	59	4453
75	cover song licensing requirements	To record a cover song, you must give notice to the original artist, pay licensing fees set by statute, and not change the basic melody or fundamental character of the work.	"['Recently, two seemingly contradictory pieces of copyright law have popped up in the news. One is the case of Jonathan Coulton, whose arrangement of Sir Mix-a-Lot\'s ""Baby Got Back"" was lifted by the producers of Glee. Another is the recent case of a car customizer who was barred from making Batmobiles by a District Court judge who said the Batmobile is protected by copyright as a fictional character.\nSo why is it that you\'re permitted to create your own recording of someone else\'s song, but you\'re not allowed to market a (non-fair use) work of visual art or fiction using someone else\'s character without going through an often expensive and messy licensing process? And would it be possible apply the music licensing system to fictional characters?\nPhoto by Shane McGraw.\nFor the record, this particular post deals with commercial derivative works, not fan works posted for free online. It also doesn\'t get into questions of fair use, such as parody and transformative works. For a thorough discussion of fan works and fan use, check out my earlier post: ""Are fan fiction and fan art legal?""\nAlso, all law cited below is US law, and none of this should be taken as legal advice.\nHow Music Licensing Works\nSo let\'s start at the beginning. If you write a song, you own the copyright to both the melody and the lyrics to that song. In the days before you could fix a song as a recording, there were two ways that composers generally made their money: 1) from selling sheet music and 2) from royalties earned from public performances of their songs. There wasn\'t a question of covers; there were simply different performances of musical pieces. Today, permissions and royalties for public performances of songs in the US are largely handled by the performing rights societies ASCAP, BMI, and SESAC. The societies collect public performance fees from broadcasters and venues and then compensate the rights holders.\nThen, a revolutionary piece of technology came along: the phonograph. Suddenly, a single performance of a song could be fixed in a (more) permanent state and distributed to thousands of people. This caused a bit of concern for music publishers. After all, it was the accepted nature of music that someone would hear a song and then want to perform that song themselves. But it wasn\'t clear if the copyright law of the time dealt with someone performing their own version of someone else\'s song, recording it, and then pressing hundreds of records of their own version. So, to allow people to record their own versions of songs while protecting the rights of composers and musical publishers, the Copyright Act of 1909 introduced compulsory mechanical licenses into US law.\nHere\'s how the compulsory mechanical license works: Sir Mix-a-Lot writes a song in appreciation of sizable female posteriors and is therefore the owner of the underlying copyright to that song. After Sir Mix-a-Lot releases his version of the song, Jonathan Coulton decides that he would like to record a cover version. Sir Mix-a-Lot must allow Coulton to cover his song, provided Coulton obeys the statutory requirements of recording a cover: 1) he must give Sir Mix-a-Lot notice that he is going to cover the song, 2) he must pay the licensing fees set by the statute, and 3) he may not change the ""basic melody or fundamental character of the work."" In all likelihood, Coulton does not deal with Sir Mix-a-Lot directly, but instead uses a licensing agency like the Harry Fox Agency, which collects the notices and the fees and distributes the licenses.\nFor the record, Sir Mix-a-Lot remains the sole copyright holder. Jonathan Coulton does not receive a copyright to his derivative work, the cover song. That means that if, say, the cast of Glee records Coulton\'s exact rendition of the song, they must notify and pay Sir Mix-a-Lot, but they do not need to notify or even acknowledge Coulton\'s existence under the law. What they must do according to the rules of polite society, however, is entirely another matter.\nThese licenses are handy, but they are also extremely limited. They only apply to creating a new recording of a song, not performing the song in public and not broadcasting the cover of the song. Still, they acknowledge that working with another person\'s intellectual property is a natural part of art.\nIn fiction, authors receive the copyright not only to the actual words they write on the page, but also to the characters they create. This copyright protection extends to a fictional as far as the character ""constitutes the story being told"" (Warner Brothers, Inc. v Columbia Broadcasting System, commonly known as the ""Sam Spade case""). So, if I write a story about a super-strong dude who can fly, I\'m probably not infringing upon the Superman copyright, because super-strong flying dude is a stock character. But if I write a story about a super-strong flying dude who is also an alien raised by sweet Midwesterners and disguises himself as a mild-mannered newspaper reporter, then I am clearly telling the story of Superman.\nThe character copyright is the reason that you can\'t write non-parodic works about James Bond, Bruce Wayne, or Ellen Ripley without the express permission of the copyright holders. It might also extend to elements of a story that we don\'t traditionally think of as ""characters""; after all, that judge mentioned earlier recently ruled that the Batmobile is a fictional character and therefore is covered by copyright protection.\nWhat makes music and fiction so different?\nCertainly covering a song and creating a new work with an established fictional character aren\'t quite analogous. But why does music have licensing structures in place that allows for covers while you can\'t pay for a license to write your own Wonder Woman story?\nWell, as I noted above, musical works were not fixed outside of sheet music until the invention of technologies like the phonograph and the player piano. So the royalty system for public performances was already in place long before the Copyright Act of 1909. But as musical technology leaped forward, music publishers were dealt a distressing legal blow. The 1908 Supreme Court case White-Smith Music Publishing Company v. Apollo Company held that manufacturers of player piano rolls did not have to pay royalties to the composers because, according to the court, the roll was not perceptible to humans and therefore could not be considered a ""copy."" (The 1976 Copyright Act ensures that machine-readable copies of works are, indeed, copies.)\nIn Stanley Rothenberg\'s article ""What If? In Copyright,"" published in the September 2003 issue of Journal of the Copyright Society of the USA, suggests that the decision White-Smith Music v. Apollo was a key factor in the push by music publishers not only to get protections for these technological copies included in the 1909 Copyright Act, but also the eventual push for the compulsory mechanical licensing scheme. The change in media technology, combined with an unfavorable court case likely helped codify the legality of recording cover songs.\nWritten fiction did not experience the technical shift music did in the late 19th century. Fiction has existed as a fixed media since humans could mark surfaces, and the printed book existed for centuries in a relatively stable form thanks to the printing press. And, as long as there has been fiction, there has been fan fiction. Elizabeth F. Judge\'s article ""Kidnapped and Counterfeit Characters: Eighteenth-Century Fan Fiction, Copyright Law and the Custody of Fictional Characters,"" published in Originality and Intellectual Property in the French and English Enlightenment in 2009, contains some rather amusing examples of unauthorized sequels to some of the era\'s most popular novels. During that time, Judge notes, English common law (on which our own system is based) did not treat the use of characters as analogous to wholesale copying or piracy, but many of the authors certainly did. They felt that the characters they created had been kidnapped, violated, and made to behave in inappropriate ways.\nAnd that brings us to the key difference between music and fictional characters: the right of the creator to control his or her work. One of the provisions of the compulsory license is that the person or persons covering the song must maintain ""the fundamental character of the work."" Sure, there\'s some wiggle room in there, but the idea is that the musician is performing the composer\'s original song in a way that will be readily recognizable and not ""perverted, distorted, or travestied.""\n2 Live Crew was sued for this rendition of Roy Orbison\'s ""Pretty Woman."" The group conceded that the song did not qualify for a compulsory mechanical license because it strayed too far from the fundamental character of the work. However, the Supreme Court ruled that the song was a parody and therefore constituted fair use.\nWriting a story with someone else\'s characters is an entirely different kettle of Magikarp. Unauthorized sequels can transform characters into something wholly different from what their authors intended. One need only peek into the close of Harry Potter/Draco Malfoy fan fiction to see just how against their original natures characters can act. One of the goals of copyright law is to grant content creators control over their works, and the compulsory mechanical license for music allows composers much greater control over their works than creating a similar license for characters would.\nUsing someone else\'s character in your story is more akin to musical sampling, in which a musician uses another composer\'s music as a component of their own original song. If you wish to make a recording (not a live performance) sampling someone else\'s song, you have to contact the music publisher and work out a license agreement. In that way, music and fiction are very similar; if you want to publish a story using someone else\'s fictional character, you have to get in touch with the copyright holder and work out an agreement.\nCould we create a mechanical licensing system for fictional characters?\nOur current legal situation places non-commercial fan works in a legal gray area while commercial derivative works must go through an often expensive and time-consuming licensing process. One of the nice things about musical covers is that they\'re accessible to musicians who don\'t have a lot of money or a huge team of lawyers. There\'s a simple process in place for obtaining a cover license, and all of the fees are set by statute. Even if songs and characters aren\'t analogous, could we have put a system in place that allows fans to sell their derivative works and pay royalties to the original creators?\nFirst, there\'s the question of economic incentives. One of the goals of copyright is to ensure that creators have an economic incentive to create new works. When we talk about cover songs, there are certainly cover songs that outperform the original version—sometimes even provide competition for the original version and reduce demand for the original version in the market. If dozens of different companies were releasing their own Batman comics each week, chances are that someone would come up with a more popular Batman comic than DC\'s Batman comics. One of the questions we ask is: would this disincentivize companies like DC from creating new characters?\nWell, one could argue that this might inspire DC to write the very best Batman stories they could, but it\'s possible that this question could be resolved by placing a high enough statutory fee on using characters. Perhaps, if DC found that it was making a lot of money in royalties from the sales of these derivative Batman comics, it might actually be incentivized to create more potentially popular characters for licensed use. There is, of course, a chance that it could disincentivize them from continuing to publish their own Batman comics. Some creators and companies might begin seeing themselves as IP factories, creating intriguing characters and then sending them out into the world.\nBut then there is also the question of artist control. If a compulsory statutory license forced artists to grant anyone permission to use their characters in another fictional work, then they don\'t have the same control over their characters that they do under current law. We have to ask ourselves whether creators have an inherent moral right to exclusive control over their creations and whether a statutory license might give some creators pause before sending their characters out into the market. Is it possible that we would have missed out on some of our favorite works of literature if authors were afraid of what might become of their characters? Of course, the truth is, in the age of the Internet and fan fiction, artists have very little control over what happens to their characters. A statutory license might mean they could at least benefit financially from all that Mary Suing.\nThere is also the problem of trademarks, which gets into a tricky interplay of intellectual property laws. Many fictional characters are covered by trademark as well as copyright. The purpose of a trademark is to avoid confusion in the marketplace, so you don\'t pick up a comic book thinking it\'s by Marvel or DC when it\'s not. If we allowed anyone to sell their own Batman book, the potential for market confusion is great. But Robert J. Morrison, in his fascinating article on this topic, ""Derivers\' Licenses: An Argument for Establishing a Statutory License for Derivative Works,"" published in the Chicago-Kent Journal of Intellectual Property in 2006, proposes a simple solution to the problem: that all derivative works be clearly marked ""unofficial"" as a matter of law.\nThere is, however, an alternative to a statutory licensing system: a voluntary one. The music industry has ASCAP and BMI to collect performance royalties; why couldn\'t literary content creators create their own licensing agency? We\'ve already seen creators generously place their works under Creative Content licenses that allow for remixing, but the original creators don\'t receive any royalties, even if they permit remixers to sell their derivative works. Why not set up a licensing agency that allows creators of original content to opt-in, set their own licensing fees, and enable others to easily license their characters? They might be surprised at what they see in return.']"	['<urn:uuid:4a835937-813d-45ba-87ad-2147cdf717b4>']	factoid	direct	short-search-query	similar-to-document	single-doc	novice	2025-05-12T10:19:05.071190	4	31	2370
76	escape brandywine private group booking options	Private groups can either purchase all 8 available slots for a scheduled time or arrange a special private booking. Special packages are available for corporations, businesses and private parties, which can combine the use of Meeting Room space with the escape room experience. For private bookings and special pricing, contact is required through email or phone at 610-388-7376.	"['Escape Brandywine - Frequently Asked Questions\nDo I need a reservation?\nYes, an advance reservation is necessary, but you can book online easily by clicking the Purchase Tickets Link and then ""Select a Date"" button. We do not take walk-ins. All escape room events must be scheduled online in advance. If you don\'t see an event time that suits you, you can email us at email@example.com or call the Society Office at 610-388-7376 to schedule a private booking time. Private room booking rates will apply. For more information on our events and programs, please visit www.ChaddsFordHistory.org.\nAre there age limitations to participate?\nYou can purchase tickets for anyone, however, the recommended age for participating is 16 and older. There is not inappropriate material, however participants should be mindful and respectful of the historic nature of the site and the nature of the puzzles involved. We do not recommend bringing children under the age of 8 years. All individuals in the Chads House MUST have tickets, including children. The Society does not have a daycare onsite, so please do not leave children in the Visitor Center unattended.\nWhat will we do in Escape Brandywine?\nEscape Brandywine places visitors into roles of spies inside the Chads House on the eve of the Battle of Brandywine, and challenges them to find General Howe\'s battle plans that have been hidden inside the Chads House for safe keeping. Using real Revolutionary War spy techniques, codes, puzzles and cyphers, players will have 60 minutes to locate the plans and escape the house to deliver the plans to George Washington - The fate of the Revolution is in your hands!\nYou\'ll be ""locked"" inside the Chads House with your wit, teamwork and knowledge (and maybe even some escape room experience) to help you. Please be mindful, this is a historic house - there is no need to force anything or break anything, but you do need to be curious and explore. There will be a CFHS team member in the room with you in case of emergency or to give you a clue or two, if needed. For safety reasons one door of the Chads House will remain unlocked. Leaving through that door is for emergencies only. To win the game you must find the key to unlock the front door of the House. If you use the emergency exit, you will not be allowed back in to complete the game.\nWhen should I arrive?\nPlease plan to arrive 10-15 minutes (but no more) ahead of your start time. This is essential - You must arrive on time to participate. Once the entire group arrives, you will receive a quick briefing on the game at the start your reservation time. Restrooms are available before and after your escape room experience at the Barn Visitor Center.\nHow long should I allow for the Escape Brandywine program?\nPlan to spend a total of 90 minutes at the Chadds Ford Historical Society. This includes a 15-minute introduction. You\'ll have up to 60 minutes to attempt to escape from the room and after your escape (or lack thereof), leave some time afterwards for FUN photo ops and shopping in our museum store.Restrooms are available before and after your escape room experience at the Barn Visitor Center.\nWhat happens if I am late?\nThese are live-action events. You must arrive on time in order to participate and we cannot wait for latecomers. The introduction begins at the start time you signed up for. The introduction and walking time to the Chads House takes about 15 minutes. If you arrive after the group has entered the House and started playing you will not be permitted to participate. Leave ample time to allow for traffic and parking.\nThere are no refunds or exchanges for participants who arrive past their booking time.\nIs there parking on-site? Where do we go?\nParking is available at the Barn Visitor Center, 1736 N. Creek Road, Chadds Ford, PA 19317. Check-in and orientation begins inside the Barn Visitor Center. Once you\'ve been prepped on the rules and your mission, you will proceed with an escape room representative to the Chad House, where your escape room adventure begins!\nWill I know everyone in my group?\nEscape Brandywine is set for up to 8 participants. If you purchase less than 8 tickets, you MAY be paired with other visitors. However, don\'t be discouraged by this! Some puzzles may need an extra set of eyes or different thought process to solve these puzzles. Remember to work together and share any clues you find!\nInterested in hosting a private escape room for ONLY your group? You can either purchase all 8 of the slots for an event time - allowing you to ensure your members are the ONLY participants for that time (only if available - remaining available tickets is shown on each time slot\'s ordering page). Alternatively, if these times don\'t work for you, we can arrange a private escape room for your team or group (additional fee may apply). We can create packages for corporations, businesses and private parties, combining the use of our Meeting Room space and the interactive, team-building escape room experience! For special pricing for corporate outings, team building, private parties and special events, please email firstname.lastname@example.org or call 610-388-7376 to check availability.\nIs Escape Brandywine scary?\nNo. Nothing is designed to scare you in the experience. The game takes place in two adjacent rooms so there is ample room for all players.\nWhat can I bring (or NOT bring) into the escape room?\nPlayers are not permitted to use outside tools or technology to assist with solving the puzzles. This means no cell phone use, outside tools or flashlights. Remember, this takes experience place in 1777 – these things did not exist! Use of these tools constitutes forfeiture of the game for your entire group and the game will cease immediately.\nCome as you are - no baggage needed! We ask that you refrain from bringing large bags or backpacks to the event. There will be an area for you to stash your small personal belongings (purses, phones, etc.) at the John Chad House, however, please only bring essentials. Also, please NO food or drink.\nYou only need to\nbring your best wits and puzzle-solving ability. All items needed for solving the puzzles will be included in the room. Wear anything you are comfortable in, however you may be squatting down or\ngetting on your knees to look under or to get a closer look at something. If you wear reading glasses, bringing them along would be recommended.\nDo I have to bring my printed ticket to the event?\nYou do NOT have to bring your printed ticket, we\'ll have the reservation list when you check in.\nCan I update/change, reschedule or cancel my reservation?\nUPDATING RESERVATION NAME: If you need to change the name on your reservation or transfer to another person, please use the link on the confirmation email you received when you first booked.\nCANCELLATIONS/RESCHEDULING: If you cancel within 72 hours of your scheduled start time you will receive a full refund of your purchase price. There are no refunds or exchanges for participants who cancel less than 72 hours before their start time or arrive late for their session. To request a refund, please use the link on the confirmation email you received when you first booked. Alternatively you can transfer your reservation to another person. If you need to change the name on your reservation, please use the link on the confirmation email you received when you first booked.\nIf you\'d like to reschedule, please forward your confirmation to us at email@example.com and include your requested new date and time. We will accomodate your request if space is still available. Please note, we can only accomodate rescheduling 72 hours or more before the event.\nHow can I contact you with any questions?\nEmail firstname.lastname@example.org or call the Society Office at 610-388-7376. For more information on our events and programs, please visit www.ChaddsFordHistory.org.']"	['<urn:uuid:66a3fd4d-db47-472e-ab8e-ddb45e28e77d>']	open-ended	direct	short-search-query	similar-to-document	single-doc	expert	2025-05-12T10:19:05.071190	6	58	1340
77	synthetic organic fertilizer differences plants	Synthetic and organic fertilizers have several key differences. Synthetic fertilizers are manufactured from mineral salts, usually have higher N-P-K ratings, provide precise nutrient doses, and are relatively inexpensive. However, they can create nutrient imbalances, contain harsh chemicals like potassium chloride that harm soil, and their phosphorous can lock up trace minerals making them unavailable to plants. Organic fertilizers, derived from plants, animals, or minerals, contain familiar materials like fish emulsion and kelp, provide a wider range of nutrients including micronutrients, work more slowly with buffered nutrients, and stimulate beneficial soil biological activity. Everything in organic fertilizers is useful to plants, while synthetic fertilizers contain up to 70% filler.	['Choosing a Fertilizer for Your Indoor Plants\nWhen you know what nutrients your plants need and what type of fertilizer you prefer, you can choose the fertilizer that’s best for your houseplants. A fertilizer’s guaranteed analysis (the amount of nitrogen, phosphate, and potassium) is one of the most important guidelines for choosing the right fertilizer, but there are other considerations, too. For example, do you want all-natural fertilizer, or are you willing to use a fertilizer that was made synthetically? Should the nutrients be released quickly or over a longer period of time? And would you rather apply it in liquid form or solid form?\nDecoding numbers on a fertilizer label\nWhen you buy a commercial fertilizer, the guaranteed analysis is listed on the label with three numbers. These three numbers tell you how much of each of the primary nutrients is in the fertilizer. The first number indicates the percentage of nitrogen; the second, the percentage of phosphate; and the third, the percentage of potassium, also known as potash. A 10-5-5 fertilizer is 10 percent nitrogen, 5 percent phosphate, and 5 percent potash by weight.\nDo the math, and you find that a 100-pound bag of 10-5-5 fertilizer contains 10 pounds of nitrogen, 5 pounds of phosphorus, and 5 pounds of potash — a total of 20 pounds of usable nutrients. Although the remaining 80 pounds contain some useful nutrients (also listed on the label), most of the balance is either filler or carrier left over from manufacturing.\nDebating organic versus synthetic fertilizer\nMost organic fertilizers derive their nutrients from plants, animals, or minerals. Synthetic or chemical fertilizers are manufactured from mineral salts. Is one better than the other? That question is the subject of countless debates and philosophical face-offs. Some gardeners believe organic is better — for their health and the health of the planet. Others say, hey, the plants don’t know where their nitrogen is coming from.\nOrganic fertilizers usually contain familiar-sounding materials, such as fish emulsion and kelp (seaweed). They may also contain various composted animal manures, including cow, poultry, and horse manure, as well as slaughterhouse by-products such as bone, blood, and feather meal. Some fertilizers even contain added beneficial microbes. Individual ingredients may contain limited nutrients; for example, fish emulsion contains mostly nitrogen; bone meal mostly phosphorus. But many organic fertilizer formulas contain a wide range of nutrients, especially the micronutrients that may be lacking in synthetic formulas.\nIf you want to be sure you’re using a product appropriate for organic growing, look for the term OMRI-Listed on the label. The Organic Materials Review Institute (OMRI) is a nonprofit that evaluates products to see whether they conform to the standards established by the National Organic Program. If the label says OMRI-Listed, the product has been approved for use in certified organic farming.\nSynthetic or chemical fertilizers are manufactured from mineral salts and usually have higher N-P-K ratings than organic ones. Sold as ready-to-apply or in concentrates requiring dilution, they are sterile and provide a precise dose of nutrients. They’re usually relatively inexpensive.', 'Synthetic vs. Organic Fertilizers: Can Plants Tell the Difference?\nPlants Can Indeed Tell The Difference\nA 15-5-10 synthetic fertilizer is the classic 3-1-2 ratio high-nitrogen, synthetic fertilizer – the kind that the other guys recommend. These numbers mean that the bag contains 15 percent nitrogen, 5 percent phosphorus and 10 percent potassium. The remaining 70 percent of the material in the bag is filler. That can be hydrogen, oxygen and other compound parts but can also be just about anything – sawdust, sand, dirt or even toxic industrial waste.\nNow you might ask, ‘Why do we need filler? They distribute the N-P-K throughout the filler or carrier so you don’t burn the heck out of your turf when you apply it. Does that tell you something?\nWhen we use organic fertilizers we’re using very low amounts of buffered nutrients. Everything in the bag is useful to the plant. Our ‘filler’ is organic material with a variety of trace minerals. That translates to much better stimulation of biological activity in the soil.\nThere are all kinds of problems with synthetic, high-nitrogen fertilizers. The primary problem is that there’s too much nitrogen. It creates an unbalanced situation as far as nutrients in the soil and in plants.\nHigh levels of nitrogen and low levels of trace minerals force fast growth that results in very weak watery cell growth in plants. People see the plants are growing and flowering so they think everything is fine. But the imbalance and the watery cells bring on insects and diseases. Nature’s job is to take out sick plants and to encourage the survival of the fittest.\nAnd, the form of nitrogen is wrong. It works too fast. Plus, it’s soluble. If it rains after you put it out, it washes away and leaches through the soil into the water stream.\nThe second problem is the phosphorous source. The phosphorous in synthetic fertilizer is usually triple super phosphate 0-46-0 made by treating rock phosphate with phosphoric acid. Years ago the phosphorous source was 0-20-0 or super phosphate. It was pretty darn good even though it was created by a synthetic process. Rock phosphate was made by treatment with with sulfuric acid. It was a more balanced phosphate and did not tie up trace minerals.\nWell, somebody came up with the notion to use phosphoric acid to create more phosphorous for less money. So now all the synthetic fertilizer manufacturers use triple super phosphate. Big problem – the new material is so raw and so bare that when it’s put on the soil, it grabs and locks onto magnesium, manganese and all sorts of other trace minerals. It ties up these nutrients making them unavailable to plants.\nThe third problem is potassium. The source of potassium in most synthetic fertilizers is muriate of potash or potassium chloride. Potassium chloride is bad on specific types of crops – especially fruit crops. It’s also harsh on the soil. What we like as a potassium source is potassium sulfate. It’s made from the salt of The Great Salt Lake.\nMy definition of a fertilizer is anything that improves the soil and helps to stimulate plant growth. For example, dead leaves that fall off a tree are fertilizers. As they break down they turn into organic matter or humus and feed the soil microbes. Microbes such as the beneficial fungi on the roots protect and feed the root hairs of the plants. This feeding process releases the nutrients to feed plants. That’s how it works on the prairie and in the forest. We’re just speeding up the process.\nAll the basic soil amendments meet that definition, but they are intended for building the health of the soil more than for routine fertilizing. They are more gentle and work more slowly over time. The basic soil amendments are manure-based organic compost, cornmeal, lava sand, greensand, zeolite and dry molasses.\nManure based organic compost – this is the basic building block of organics. It is the material we would find on an undisturbed forest floor. It acts as a gentle fertilizer encouraging microbial action.\nCornmeal – this natural fungicide is a mild fertilizer and disease fighter that should be used until your soil gets healthy.\nLava sand – you can use as much as you want as long as you want. Remember that the most productive soils in the world – Costa Rica, Hawaii, and parts of the West Coast and the Mediterranean – are places with a history of volcanic action and are almost solid lava.\nGreensand – mined from ancient sea beds, greensand is a marine deposit that is loaded with iron – and other trace minerals. It can end up being a bit of a problem in soils with high levels of iron.\nDry molasses – this is not solid dried molasses. It’s organic material like rice hull bits that have been sprayed with molasses and dried. It is a powerful carbon source that really kicks up microbial activity.\nThere are many quality bagged organic fertilizers to choose from. Some people alternate among them on the perfectly logical supposition that each contains a slightly different combination of nutrients and by rotating over time you provide your soil a more balanced diet.\nSome brands that are widely distributed are GreenSense, Lady Bug, Medina and Texas Tee.\nSimilarly there are a lot of great choices in liquid fertilizers. I strongly recommend a regular foliar spray program. You can make your own Garrett Juice – the recipe is shown below.\nOther good choices are Alpha Bio S.I., Bioform, Lady Bug, Maxicrop and Medina.\nYou get indirect pest control from all liquid organic products because they stimulate biological activity. And that’s how we control pests the most effectivley. We try not to kill, but rather stimulate the good guys. The good guys feed on the pathogens and balance results.\nThe recommended organic fertilization program is (it varies based upon where you live):\nFirst fertilization - as early as January on into April\nSecond fertilization – sometime in June or July\nThird fertilization – between September and October\nGarrett Juice (ready to spray):\n1 cup manure based compost tea\n1 ounce molasses\n1 ounce natural apple cider vinegar\n1 ounce liquid seaweed\nFor disease and insect control add:\n¼ cup garlic tea or\n¼ cup garlic/pepper tea\nand 1 ounce of orange oil\nFor homemade fire ant killer add:\n2 ounces of citrus oil per gallon of Garrett Juice\nThe ready-to-use solution should not have more than 2 ounces of orange oil per gallon.']	['<urn:uuid:9491e91d-857c-41d6-b96d-38160b7e4fcc>', '<urn:uuid:78ab12f5-8c76-4e70-9a59-85a4d1f1b694>']	open-ended	direct	short-search-query	similar-to-document	comparison	novice	2025-05-12T10:19:05.071190	5	108	1596
78	why did confederates call battle of wauhatchie station charge of mule brigade	The battle got this derogatory nickname because some mules stampeded when Rebels attacked a Union wagon train, which delayed the Rebels from forming a line and led to a federal counterattack driving the Confederates off.	['Submitted by C.J. Johnson\nDuring the four years of the War Between the States, there were rare nighttime battles. In October 1863, in Marion and Hamilton Counties, Tennessee and Dade County, Georgia, perhaps the best known overnight battle occurred in a valley illuminated by moonlight. The action was supposed to begin at 10 p.m., but didn’t start until midnight. Better known as the Battle of Wauhatchie Station or Brown’s Ferry, this engagement ensued as the Rebels attempted to prevent Federal supplies from reaching Chattanooga.\nFollowing the Confederate victory at Chickamauga in north Georgia in mid-September, Confederates set up their positions on the eastside of Chattanooga along Missionary Ridge and Lookout Mountain, while Union troops retreated into Chattanooga. With the Rebels to the east and with control of the Tennessee River, Union supplies going into the city had to cross the mountains to the west. The route was subject to Rebel attack and was a difficult trip as well.\nThe National Park Service describes the battle. “In an effort to relieve Union forces besieged in Chattanooga, Maj. Gen. George H. Thomas and Maj. Gen. Ulysses S. Grant initiated the “Cracker Line Operation” on October 26. This operation required the opening of the road to Chattanooga from Brown’s Ferry on the Tennessee River with a simultaneous advance up Lookout Valley, securing the Kelley’s Ferry Road.\nUnion Chief Engineer, Military Division of the Mississippi, Brig. Gen. William F. “Baldy” Smith, with Brig. Gen. John B. Turchin’s and Brig. Gen. William B. Hazen’s…brigades…, was assigned the task of establishing the Brown’s Ferry bridgehead. Meanwhile, Maj. Gen. Joseph Hooker…marched from Bridgeport through Lookout Valley towards Brown’s Ferry from the south. At 3:00 am, on October 27, portions of Hazen’s brigade embarked upon pontoons and floated around Moccasin Bend to Brown’s Ferry. Turchin’s brigade took a position on Moccasin Bend across from Brown’s Ferry.\nUpon landing, Hazen secured the bridgehead and then positioned a pontoon bridge across the river, allowing Turchin to cross and take position on his right. Hooker, while his force passed through Lookout Valley on October 28, detached Brig. Gen. John W. Geary’s division at Wauhatchie Station, a stop on the Nashville & Chattanooga Railroad, to protect the line of communications to the south as well as the road west to Kelley’s Ferry.\nObserving the Union movements on the 27th and 28th, Confederate Lt. Gen. James Longstreet and Gen. Braxton Bragg decided to mount a night attack on Wauhatchie Station…. Surprised by the attack, Geary’s division, at Wauhatchie Station, formed into a V-shaped battle line.\nHearing the din of battle, Hooker…sent Maj. Gen. Oliver Otis Howard with two XI Army Corps divisions to Wauhatchie Station as reinforcements. As more…Union troops arrived, the Confederates fell back to Lookout Mountain. The Federals now had their window to the outside and could receive supplies, weapons, ammunition, and reinforcements via the Cracker Line.”\nThe Georgia Blue and Gray Trail explains the reference to the Mule Brigade. During the battle, Confederate commander “John Bratton thought he has a good position, forming an open V to attack Geary’s obverse line. Some of Bratton’s men crossed Kelley’s Ferry Road and struck the lightly guarded [Union] wagon train that was the original object of the attack. Rebels quickly drove off the Federal soldiers, but the commotion stampeded some mules giving the battle a somewhat derogatory nickname, ‘Charge of the Mule Brigade.’ The mules delayed the Rebels from forming a line and a federal counterattack drove the Confederates off.”']	['<urn:uuid:7ad443db-8148-4709-9212-d5664c8250f1>']	factoid	direct	long-search-query	similar-to-document	single-doc	expert	2025-05-12T10:19:05.071190	12	35	577
79	As an urban development researcher interested in Coney Island's history, could you tell me what happened to the Steeplechase amusement park and its structures after its closure in 1964?	After Steeplechase amusement park closed in 1964, its structures were largely abandoned. The park's 'giant slide,' which originally had 15 lanes in red, white and yellow colors, fell into disrepair. The iconic Steeplechase parachute jump tower was left abandoned with smashed windows and peeling walls. However, the tower survived because it was deemed too expensive to tear down and was eventually declared a landmark in 1977.	"[""BestsellerMagazine.com - CATEGORY Breaking news: TITLE\nPublished: 07:52 GMT, 28 June 2012 | Updated: 01:20 GMT, 29 June 2012>\nRusty, disassembled and abandoned, pictures of the New York Harbor in 1973 show a different side to one of the world's largest natural harbours when once hidden beneath a swath of man-made waste.\nFreelance photographer Arthur Tress, one of many hired by the Environmental Protection Agency in the 1970s, captured the astonishing build up along the water's edge in parts of Brooklyn and Staten Island.\nThe photographs aimed at capturing America's growing environmental negligence before the area was cleaned up to a far different view seen today.\nUnnatural landscape: Tyres are captured lined along the water's edge in New York's Jamaica Bay in the 1970s\nWashed out: The remnants of a rusted red car are photographed before a couple on Plum Beach on Sheepshead Bay\nBeached: Another photograph shows an abandoned car mixed with sea shells on the Breezy Point shore\nPlayground: Five girls pose inside a broken boat left on the shore\nObstacle course: A landfill and dumping site becomes a playground to local boys\nBack then, abandoned cars and tyres were seen mingling with sea shells half-sunken in the sand while local children made use of the items as toys.\nStanding inside a faded red boat, whose rear is entirely missing, five girls are captured some seen distance from where a tide could stand a chance of floating their imaginative vessel.\n- >Dog-faced boy, four-legged Myrtle and the man with elastic... >Would you pay $4.2 MILLION for a better view? Couple buy...\nShare this articleShare\nAnother picture shows a line of car tyres as though beached from a retreated tide.\nInland, up and coming home fronts are dusted with debris including concrete pipes and bricks.\nUnoccupied ride: Coney Island's abandoned 'Giant slide' rests with three boys making use of its unusual spectacle\nLandmark today: The Steeplechase amusement park parachute jump tower is pictured broken and pealing following its close in 1964\nClosed: A pool is photographed sans bathers and chlorinated water at a private swimming club in Breezy Point\nEmpty tanks: Rusty gas pumps rest untouched in the Sheephead Bay marina, offering gasoline at 34.1\nTaken over: A former ice cream truck is seen, stripped and rusting among other ruins and weeds in Jamaica Bay\nA clothes line in Jamaica Bay hangs drooping with white and black linen just feet from piled rusty oil cans.\nAnother photo shows cardboard boxes mixed with splintered wood outside an incinerator plant waiting for their removal.\nTheir ashes, piles full with the city's towers captured in the background, are consequently taken to a barge and dumped into the ocean.\nWarning: Polluted water kept bathers out of the area with help of this sign before the then-newly completed Verrazano-Narrows Bridge\nIsolated: Spermaceti cove in Sandy Hook was one of the few uncontaminated areas along the Jersey Coast, allowing this man to dig for clams while leading his boat\nIdling: Other areas, like this region of Jamaica Bay, were less lucky with this Volkswagen Beetle left idling in the water\nEnding: Photographed across from the JFK airport, this end warns drivers and pedestrians of the ruin behind it\nHalted construction: These high-rise apartments were abandoned over land preservation issues\nIn Coney Island, remnants of the Steeplechase amusement park, closed in 1964, were seen abandoned just like the rest of the region's waste.\nThree boys pose on the top of a worn 15-lane red, white and yellow 'giant slide' of the former park.\nToday the park's 'giant slide' features just three lanes but is in incomparable condition.\nStill, Coney Island's original Steeplechase parachute jump tower, photographed then with smashed windows and peeling walls, has survived today.\nDebris: New homes near the Verrazano-Narrows-Bridge leave a mess of piping and bricks across the lot\nMingling: Other homes in Jamaica Bay were photographed mingling with rusty oil cans\nBackyard: This family rests on their home's deck over yellowed grass and bits of trash\nHome: A home near the JFK airport at Hunters Point is pictured, showing a baby stroller and a child's three-wheeled bike among trash and other piling waste\nScenery: An oil storage tank is photographed in South Brooklyn, a less an scenic sight to the ocean-front community\nThe artefact was deemed too expensive to tear down and declared a landmark in 1977.\nIf the area were found less than pleasant to residents – with some photographed sitting on a backyard deck above straw and pieces of trash - a billboard perched in the area recommended moving to Staten Island.\nStanding in what appears to be a cleared lot pitted with water and various debris, the billboard showed an illustrated view of green grass, trees and butterflies just across the bridge and accessible by car.\nBurning: An incinerator plant is pictured in Gravesend Bay, in South Brooklyn, where cardboard boxes are mixed with various pieces of wood waiting their destruction\nDumping: Ashes from the incineration plant are scene stacked up onto a barge before they are dumped out into the ocean\nMoving: A billboard in a cleared lot encourages residents to move to Staten Island where an illustration depicts the area of having green grass, trees and butterflies\nBestsellerMagazine.com, Site News Today\\\\\\'s world Presenting Daily News News News Politics, Business, Sports Up Celebrity Gossip.\nSource : http://www.dailymail.co.uk/news/article-2165823/Astonishing-photos-New-York-Harbour-1970s-reveal-mans-dumping-grounds.html""]"	['<urn:uuid:92b632f0-15b5-441c-b96f-92a0b7b16586>']	open-ended	with-premise	verbose-and-natural	similar-to-document	single-doc	expert	2025-05-12T10:19:05.071190	29	66	889
80	pneumatic conveying system design what factors affect vacuum pump sizing altitude bulk density distance	Several key factors affect vacuum pump sizing in pneumatic conveying systems. First, the bulk density of materials is crucial - materials with bulk densities between 25-55 lbs per cubic foot are easier to convey, while heavier materials need more power. Second, conveying distance impacts sizing - more tubing requires bigger vacuum pumps since more airflow is needed to move materials through longer distances. Additionally, altitude affects vacuum source sizing - a system that uses a 5HP pump at sea level would need a 7.5HP pump in higher altitudes like Denver where air density is lower. Every 90-degree sweep in the system equals 20 linear feet of distance, which must be factored into the total conveying distance calculations.	['Click Image to Enlarge\nDesigning and installing a central conveying system can be a complicated endeavor, so it is best to have the assistance of an experienced systems specialist.\nMaterials represent the single largest expense in most plastics processing facilities. In today’s world, having a reliable resin-handling and conveying system is as important as having reliable electrical power. Design, maintenance, and operation of the system can have everything to do with how well it meets production goals and objectives. Some systems can be quite big and complex, and it is always a good idea to get the advice of experienced system specialists. From design to installation to start-up and ongoing operation, their knowledge can help you avoid problems and get the most from your system.\nThe following hints and tips can help you get things right if you choose to design and install a system yourself, or they can help you prepare for a constructive conversation with a system supplier.\nFirst: Identify all of the materials that you process in your plant and where each material comes from. Refer to those silos, bins, and gaylord boxes as “sources.” Keep in mind that blenders and dryers may also be considered sources if loaders are pulling material from them.\nSecond: Make a complete list of every molding or extrusion machine in your plant and identify them as “destinations.” In this context, the hopper of a dryer or the material bins on a blender are considered destinations, since the system will be delivering material to them.\nThird: Determine the maximum throughput rate (weight/hour) for each destination. The sum of these throughput rates, plus a factor for each 90° bend in the system and the horizontal and vertical distances that material must travel, will give you an indication of how large a vacuum pump will be needed and what diameter the conveying lines should be. Your equipment supplier can help perform these calculations and recommend the ideal pump(s) for your application.\nIn most cases, the line size will be dictated by the total throughput rate of all the machines served by that system pump. Remember to keep all the conveying lines served by a given pump the same size and make sure the line size matches properly with the horsepower of the pump. For instance, never put a large 15-hp pump on a small 2-in. OD line. The proper size for a 15-hp pump is 3 in. OD.\nRules of thumb for pump horsepower and line sizes:\n- 5 hp = 1.75 or 2 in. OD line size.\n- 7.5 hp = 2.25 or 2.5 in. OD.\n- 10 hp = 2.5 in. OD.\n- 15 hp = 3 in. OD.\n- 25 hp = 4 in. OD.\nFourth: Pay attention to details. Vacuum-loader inlet tubes should match the system piping. Make sure existing loader voltages match up properly with the control system. Be sure the controller you choose has the features you need or that can be added in the future. It’s always much less costly to have features added at the factory rather than having technicians visit your plant to download new programs and add input and output cards.\nStrategically locating the vacuum pump near the farthest machine from the source point will have the effect of balancing the system. A short vacuum line should have a long material line, and vice-versa (see illustration opposite).\nDESIGNING THE SYSTEM\nKeep material runs as straight as possible and minimize the number of elbows. Every change in direction in a material-handling system increases backpressure and reduces conveying capacity. A vacuum pump of a given size can do only so much work, and it is important to make sure as much of that work as possible goes into moving material instead of overcoming system design flaws. The expensive alternative is to install a larger pump, but remember larger pumps require larger line sizes, in most cases.\nOptimize conveying velocity. When laying out your system and sizing your pumps, the designer will want to make sure the conveying velocity at the material pick-up point (gaylord, storage bin, or drying hopper) is as low as possible—preferably between 3700 and 4000 ft/min. Material will accelerate over the entire length of the run, and if it moves too fast, the material can degrade. Softer materials (like polyethylene) can heat up and smear against the walls of the conveying lines, deforming and leaving “angel hair” behind to clog the system. Brittle materials (like polystyrene or polycarbonate) can break up and create dust and fines that also clog the system. Abrasive materials (like glass-filled ABS) can wear away the inside of the conveying lines, eventually causing leaks and failures.\nMake sure your vacuum pump is sized for your location. Power systems that operate at 50 Hz rather than the more conventional 60 Hz can cause the vacuum pump to turn more slowly, actually de-rating it by about 17%. You’ll need a bigger pump or one that turns at higher rpm in order to compensate. And don’t assume that a system specified for San Diego will operate the same way in Denver or Mexico City. Thinner air at high altitudes has the same effect as lower electrical frequency. Because the air is less dense, you need to move more of it in order to achieve the same conveying capacity as you would have closer to sea level. Again, you’ll need a bigger or faster pump.\nIf you are purchasing new loaders, an optional fill sensor in each loader can help avoid problems of overfilling or underfilling, which can occur when using only timer control. These devices can be invaluable in preventing equipment damage and keeping your conveying system running automatically.\nKeep your filters clean. Your material-handling system is equipped with filters at two locations: one in the dust collector near the vacuum pump and the others in each vacuum receiver. A blinded filter has the same effect as too many elbows in a system design: Backpressure builds and your system loses conveying capacity. As the filters clog, you’ll notice that it takes longer and longer to fill a vacuum receiver or, if the fill time is not changed, the receiver will not fill completely.\nLocate and eliminate vacuum and material line leaks. Either one will diminish performance because the system will be sucking air rather than material. Again, you’ll notice sluggish conveying, longer fill times, and, in the case of a material line leak, you may see pellets on the floor. Leaks most often occur in flexible hoses and around couplings, seals, and valves. They can usually be located by visual inspection. Sometimes you can actually hear a hiss as air is pulled through a hole in the line. Smoke can also be used to locate a leak.\nPay attention to preventive maintenance on your vacuum pump. Follow the manufacturer’s recommendation for changing gear oil, and make sure all drive belts are tight and undamaged. Especially important, make sure the vacuum breaker valve is working properly. This valve is designed to break the system vacuum if it exceeds safe levels (about 15 in. of mercury) due to a clog in the line. If a dirty valve fails to open, a functioning breaker valve will keep the pump from overheating and damage.\nMake sure your system is set up properly at the beginning. Have the manufacturer’s service technician check overall system integrity and see that all components are routed properly. All feed tubes should be properly set and receiver controls should be programmed with appropriate load and dump times.\nTake care that the system always has material to convey. It sounds like a simple thing, but an empty gaylord or surge bin, or a feed tube that has become dislodged or clogged with a plastic box liner, will cause loader controls to alarm for lack of material. This can result in processing machine downtime and unscheduled maintenance. By making sure employees check material levels periodically, you can avoid costly production disruptions.\nTrain your operators. Especially when employee turnover levels are high, don’t forget to make sure everyone has the information they need to keep the system running. As noted above, a simple thing like allowing material to run out can be very disruptive. All employees should have a basic understanding of how the material-handling system operates and what their specific responsibilities are.\nIf you don’t have the resources in-house, a competent system manufacturer will usually be able to perform this training. Classroom training will cover how the system is set up and how material is transported. On the plant floor, employees can learn how to operate the various components, how to log into controls and make day-to-day adjustments, and how to change connections on a fantail manifold to source material from a different location or direct it to a different machine. Employees also need to understand various alarms and when a problem is simple (e.g., the feed wand fell out of a gaylord) or more complex, like an electrical issue that needs attention from an electrician or the supplier’s service technician.\nEven small-scale material-handling systems can be complex, requiring careful calculation of throughput, conveying distances, equipment specifications, etc. Perhaps the most valuable tip of all is this: Take advantage of the knowledge, experience, and advice of your equipment supplier. With such help, you can avoid many of the common problems that can arise in the design, installation, operation, and maintenance of your material-handling system.\nABOUT THE AUTHOR\nDoug Brewster is customer service manager of Conair in Cranberry Township, PA. He has almost 25 years’ experience with Conair. Contact: (724)584.5500; email@example.com', 'Just as it is not necessary to become an expert on how to build a vehicle when purchasing one, it is also not necessary for users of pneumatic conveyors to become experts at vacuum conveying to incorporate them into their processing line. While it is a good idea to have a broad understanding of how pneumatic conveying systems work, no pneumatic conveyor manufacturer will ever ask a customer if they need a dense phase or a dilute phase system. There are however a host of other questions a conveyor manufacturer will ask about a process, and the materials used in that process, that will aid them in designing a system with the necessary components to provide a solution that integrates into a customer’s process.\nMaterial Bulk Density\nThe bulk density of a material is one of the first indicators of design in terms of sizing various system components such as vacuum receivers and air sources. Bulk density helps determine how many cubic feet per minute (CFM) of air is needed to move the material through the convey line. Generally speaking, materials with bulk densities below 55 lbs per cubic foot but above 25 lbs per cubic foot are fairly easy to convey, while heavier materials require more power and larger vacuum receivers. Fine powders with low bulk density, such as fumed silica, present their own set of challenges and may require more filtration and therefore larger vacuum receivers than medium density powders.\nBulk density, while one of the most important factors in sizing a system, is not the sole criterion used to determine components. Another important factor in sizing and determining the type of system needed to convey bulk solids is the distance material is traveling. In pneumatic conveying the more tubing you put in the system, or the further the conveying distance, the bigger your vacuum pump gets because it takes more airflow to pull (or push) the air through the tube.\nThese 300-cu-ft material receivers are part of a pneumatic conveying system that transfers powders in a large-scale candy making process. In dilute phase systems (material entrained in the airflow) when powder must travel more than 300 feet, the system would use positive pressure to push the material in the tube, rather than using vacuum, as a more economical solution.\nOne of the advantages of pneumatic conveying is that moving products vertically is calculated the same as moving them horizontally–in linear feet. However, each 90 degree sweep in the system equals 20 linear feet; thus if you are moving material horizontally 110 feet and vertically 110 feet with four 90-degree sweeps, then the conveying distance is 300 feet.\nConvey Rate and Batch Conveying\nIn addition to bulk density and distance determining the size and type of the conveying system, the rate of material moving through the system is an important factor considered when designing a system, as well as whether the process is a continuous or a batching operation. While most pneumatic conveyor operations work via cycles of convey and discharge, a batching operation works differently and can have a considerable effect on the size of the system. An example of this would be when a processor wants to move 5000 lbs of powder per hour into a mixer, but needs to move the batch into the mixer within 15-minutes. Although the 5000lbs cycles only once per hour, because the rate of transfer is higher during that 15 minutes, it is actually moving at 20,000 lbs per hour; thus, requiring a more powerful vacuum source. Dilute phase pneumatic conveying systems can transfer up to 25,000 lbs per hour via vacuum. For higher rates, a positive pressure source replaces the vacuum pump.\nBeyond a material’s bulk density, an understanding of how particular substance will behave under certain conditions is essential when designing a vacuum transfer system. Most often customers know whether their particular material is free flowing, sluggish or non-free flowing, important data to relay to the conveying manufacturer. It is not uncommon for there to be several product grades within the same product group and each one behave differently than the other. One grade of Zinc Oxide may have the consistency of talc, while another might be more cohesive and adhere to inside surfaces of conveying tubes. Some materials, in fact, can behave differently from one day to the next, affected by environmental elements.\nAccording to the National Fire Protection Association (NFPA), the preferred technology for handling combustible dust is vacuum conveying. If a leak occurs in a vacuum conveying system, the leak will be inward and prevent dust clouds in the plant environment. A leak in a pressure conveying system will quickly result in a dust cloud in the plant, a primary element for a combustible dust explosion.\nTo protect delicate or friable materials from degradation, dense phase conveying (where material slugs along in the line en-masse) may be the method used to transfer materials. Dense phase conveying is also useful in situations where processors need to minimize the chance of separation of blended materials.\nOn average, non-free flowing powders require the most equipment modifications. There is a plethora of methods used to handle non-free flowing powders that can eliminate the need for external flow promotion such as specialized finishes, over-sized receiver discharge openings, 70-degree discharge cones and proprietary designed cone-less (straight-walled) vacuum receivers.\nAn example of equipment modification to handle non-free-flowing powders is when Cordele, Georgia-based Helena Industries experienced increased demand on a particular product line. The company, that uses a broad range of process technologies to manufacture, formulate, and packages herbicides, fungicides, and insecticides attempted a number of methods in-house to eliminate the need for workers to manually dump 20-40 drums of powder chemicals, weighing up to 225 pounds each, from a raised platform.\nWhen none of those methods, including utilizing bucket elevators, produced the desired outcome, the company decided to purchase a pneumatic conveyor to move several hundred pounds of material up a level, in 30 minutes, to a volumetric feeder.\nIn the test lab, the claylike material proved to be only semi free flowing causing bridging and rat holing in the material receiver. To eradicate the issue, a proprietary straight walled vacuum receiver with a full opening discharge valve replaced the standard receiver ensuring quick refill to the feeder below.\nIn addition, the vacuum receiver was equipped with a specialized multi-filter with pleated filter media that facilitated maximum filtration of the system.\nThe system eliminated the ergonomic issue, made it a single person job and minimized environmental dust.\nMaterial Container and Pickup Point\nOnce material is in the conveying line, whether it’s free flowing or not, it usually conveys without issue, but sometimes getting material into the conveying line can be problematic. Therefore, the conveyor manufacturer needs to know what type of container holds the material because it dictates whether the pickup point (where product enters the conveying system) is a wand, a bagging station, a bulk bag unloader, a docking station or a pick up adapter.\nThe pickup point is perhaps the most customized component in a pneumatic conveying system because it is crucial for feeding material into the conveying system. An instance where a pickup point needed customization is at Tyco Thermal Controls, Redwood City, CA- facility that produces heater cable using large twin-screw compounding extruders.\nAn integral part in that manufacturing process is transporting polymers and other powders from supplied containers into the compounding extruder. Due to the very dense, sticky characteristics of the exotic powders, the seemingly simple process of transferring materials, presented its own set of challenges.\nHere the problem was not getting the sticky material to convey, but getting it to automatically feed into the conveying tube. After several failed trials to get the material into the convey line, the solution was to incorporate aspects of a previous application of handling powdered sour cream. Although the makeup of materials is completely different, the characteristics were similar and that knowledge led to the resolution.\nCustom designed bag dump stations, that were essentially vibrating bins, merged the design of a bag dump station and bulk bag unloader into one unit that allowed the material to flow into the pickup point. The design also gave the cable manufacturer the option of unloading super sacks or 50 lb bags using the same piece of equipment.\nUpstream equipment effects downstream equipment and the more a pneumatic conveyor manufacturer knows about the process the better able it is to supply a system that meets a customer’s needs. Conveyor design can change based on the type of process equipment being fed such as loss in weight feeders, volumetric feeders, mixers, extruders, packaging and other equipment. For example, a loss in weight (LIW) feeders require quick refill. Knowing that the equipment is loading into a LIW feeder influences the design of the system.\nWhen Pacon Manufacturing had a goal to develop a facial care product and also develop a process that maintained quality at the necessary price point, it required high speed conveying and dispensing of its blended powder into packaging machinery. In this application, how the powder conveyed to the auger fillers critically affected the powder’s self-lathering properties. If the powder particles became too small, the product would lather too fast upon use. If the particles became too large, the lathering process took too long. Changing the density, component blend and texture would produce inconsistent fill rates or volumes — both unacceptable for quality control.\nSince powder can change density in auger filler heads, leading to improper filling of the screws, keeping the heads full and at proper density was critical. To maintain proper powder density in the auger filler heads, standard equipment required customization from the pneumatic conveying supplier, in order to maintain product consistency and quality.\nThe custom system utilizes two low-profile drum dump stations for easy loading where material travels, by vacuum, to customized filter receivers mounted above three auger fillers. The customization of filter receivers includes a special high polish finish and customized multi-filters. In addition, a device checks powder levels at each of the three auger filler hoppers. When auger fillers require more powder, pre-determined volumes of powder automatically dispense to fill the hoppers.\nDesigned for simplicity and easy maintenance, the modular convey system has a specially designed receiver for quick tool-less assembly/disassembly and easy cleaning. Fabricated with no crevices and the fewest possible welds, the drum dump stations interior bends and corner welds have a minimum 1/8” radius to minimize material accumulation. A line clearing valve purges the conveyor system at the end of each convey cycle to prevent fallback powder and make restarts easier.\nCustomizing equipment isn’t always focalized on materials. Sometimes facility constraints are the reason for equipment modifications. One of the benefits of pneumatic conveying systems is the small footprint compared to other material handling methods, but even the smallest conveying system needs at least 30” of headroom above processing or packaging equipment.\nPositive pressure systems are one way to get around headroom constraints as are cyclones, filter-less material receivers, or scaling valves that divert material directly into hoppers in low clearance areas.\nNutriom, natural powdered egg producer of Ova Easy® and Egg Crystals®, sold at outdoor retailers such as REI, and online merchants such as Amazon.com, had a lot of height restrictions because the building is older and has many areas with low ceilings. The company wanted to replace their food-grade screw conveyor that required expensive H1 lubricants and extensive maintenance with a more hygienic fully enclosed pneumatic system that protects materials from air, dirt and waste.\nWhen companies have severe height restrictions, vacuum receivers are sometimes located outside. When possible, modifications to conveying equipment enable it to fit within the space.\nIn this case, filter lids required modification to fit the tight spaces and a venturi, powered by compressed air, used to generate vacuum. In areas where ceiling height was not an issue, electric-motor-powered vacuum pumps were utilized. Because the enclosed system was cleaner than the open system, it made compliance with stringent FSIS USDA regulations simpler.\nTo further accommodate the company’s stringent standards and to fit better within its inspection system, stainless steel rings replaced the iron rings that secured the filter.\nPlant Site and Industry Environment\nIn addition to knowing material characteristics, flow rates and downstream equipment processes, conveyor manufacturers also need to know the geographic location of the plant as well as the type of industrial environment the equipment will be located when designing a system.\nJust as cooking at higher elevation requires alterations, altitude also affects vacuum source sizing. For instance, a factory at the Jersey shore (sea level) might use a 5HP Vacuum pump for an application, but the same application in Denver, CO (one-mile above sea level) where air density is lower will require a 7.5 HP vacuum pump. Components in pneumatic conveying systems, like the vacuum pump or stainless rings mentioned earlier, are interchangeable; and sometimes the difference between two systems with the same design is the material from which the components are constructed.\nPneumatic Conveying Manufacturers must know whether the system requires sanitary design, using 316L stainless steel or if 304 stainless steel for the food, pharmaceutical and chemical industries, or carbon steel. In pneumatic conveying systems, all equipment must complement each other. If one piece of equipment is too large or too small, it will keep the system from working properly. Vacuum receivers must be able to handle the airflow provided by the vacuum source, and balancing the air to cloth ratio of filters must occur so filters don’t shred or blind. If material can’t get into the convey line, there is nothing to convey.\nThese “Top 10 Factors” provide crucial information that allows pneumatic conveying manufacturers to evaluate applications and customize equipment based on process requirements so that processors can focus on making their products, rather then focusing on material transfer.\nVAC-U-MAX specializes in the design and manufacture of pneumatic conveying components, systems, controls, and support equipment for vacuum conveying, batching, and weighing of bulk materials. With a VAC-U-MAX pneumatic conveying system on site, bulk materials move gently and quickly from point to point, with nothing in the way to impede the efficiency ...\nLeave a Reply\nYou must be logged in to post a comment.']	['<urn:uuid:7031a9a1-88fc-449f-b563-c06cef7e399b>', '<urn:uuid:f715d2d8-3a53-4e2d-a006-b46c4d88828b>']	open-ended	with-premise	long-search-query	similar-to-document	three-doc	expert	2025-05-12T10:19:05.071190	14	117	3969
81	enterprise system file sharing solutions monitoring control features explained	Enterprise file sharing solutions combine monitoring and control features through Managed File Transfer (MFT) systems. For monitoring, MFT provides comprehensive visibility through performance metrics/monitoring, detailed reports on user and file transfer activity, and full audit trails of all transfers. For control features, Spring Integration offers various adapters that shield application components from integration details and automatically handle different types of messages. The control aspects include the ability to authenticate users against existing repositories, restrict permissible senders and receivers, centrally log data transfers, and integrate with data loss prevention products. Additionally, MFT allows automation of file transfer-related activities and processes, with the capability to execute jobs when specific files arrive and alert IT managers of unexpected situations.	"['Channel adapters are used to connect an application to the messaging system so that it can send and receive messages. Again, the definition written in the book “Enterprise Integration Pattern”, by Gregor Hohpe and Bobby Woolf is as given below:\nThe adapter acts as a messaging client to the messaging system and invokes applications functions via an application-supplied interface. This way, any application can connect to the messaging system and be integrated with other applications as long as it has a proper Channel Adapter.\nPictorially channel adapter can be represented as shown in Figure 22-31 below.\nFigure 22-31. Function of Channel Adapter\nSpring Integration Support\nSpring Integration provides a number of adapters which shields application components from integration details. External events produce various kinds of messages which feds into the Spring Integration. The adapters according to the message type kicks into action and deals with the messages without caring neither about the message origin nor about destination. Once the message is processed it can trigger another external event to which the processed messages are sent.\nFigure 22-32. Spring Integration adapters\nThe Spring Integration includes support for the following adapters:\n- FTP, FTPS and Secured File Transfer Protocol (SFTP)\n- User Datagram Protocol (UDP)\n- HTTP (Representational State Transfer [REST])\n- Web services (SOAP)\n- Mail (POP3 or IMAP for receiving, SMTP for sending)\n- Java Message Service (JMS)\n- Java Management Extensions (JMX)\n- Remote Method Invocation (RMI)\n- Really Simple Syndication (RSS) feeds\n- Extensible Messaging and Presence Protocol (XMPP)\nThe following sections are aimed at explaining some of the important adapters in some detail. Since these are very extensive topic, I would like to give you information and details which allow you a stepping-stone in actually considering using these adapters in your application. Some adapters, I will only be giving information and you can find more details on these from other books available in market.\nEach of the following sub-sections in which Spring’s Integration capability will be covered in detail follows a standard sub-sections as shown below:\n- Brief Introduction\n- Maven Dependency\n- Spring Integration Namespace\n- Adapter Configuration\nIt is one of the simplest approaches to enterprise integration. Spring Integration provides both inbound and outbound channel adapters to support this integration style.\nListing 22-31 below shows maven dependency required for integrating file system using Spring Integration.\nListing 22-31. Maven dependency required\nListing 22-32 below shows configuration of main Spring Integration namespace and file related namespace in the Spring configuration file (file-integration-context.xml).\nListing 22-32. Spring Integration Namespace declaration\n<?xml version=""1.0"" encoding=""UTF-8""?>\nListing 22-33 below shows configuration of a basic file adapter in the Spring configuration file (file-integration-context.xml). In this sample, the file inbound channel adapter polls the configured directory every 5 seconds and passes it onto the “inputFileChannel” and then the file output channel adapter reads the files in the channel to the output directory configured.\nListing 22-33. Basic file adpater configuration\n<file:inbound-channel-adapter id=""inputFiles"" channel=""inputFileChannel""\n<int:poller fixed-rate=""5000"" max-messages-per-poll=""50""/>\n<file:outbound-channel-adapter id=""outputFiles"" channel=""inputFileChannel""\nThere are various atributes in “inbound-channel-adapter” as shown in Table 22-3 using which can be used to change the behavior of this adapter according to your application requirement. I would not be able to explain these attributes in detail (excessively much and out of scope of this book) but think it would be of great help to know that these attributes can be used if need arises in your application.\nTable 22-3. The attributes in “inbound-channel-adapter” element\n|comparator||Specify a Comparator to be used when ordering Files. If none is provided, the order will be determined by the java.io.File implementation of Comparable.|\n|filter||Specify a FileListFilter to be used. By default, an AcceptOnceFileListFilter is used, which ensures files are picked up only once from the directory. You can also apply multiple filters by referencing a CompositeFileListFilter.|\n|filter-pattern||Only files matching this ant style path will be picked up by this adapter.|\n|filename-regex||Only files matching this regular expression will be picked up by this adapter.|\n|scanner||Reference to a custom DirectoryScanner implementation.|\n|prevent-duplicates||A boolean flag indicating whether duplicates should be prevented. If a ‘filter’ reference is provided, duplicate prevention will not be enabled by default (the assumption is that the provided filter is sufficient), but setting this to true will enable it. If a ‘filename-pattern’ is provided, duplicate prevention will be enabled by default (preceding the pattern matching), but setting this to false will disable it. If neither ‘filter’ nor ‘filename-pattern’ is provided, duplicate prevention is enabled by default, but setting this to false will disable it. For more detail on the actual duplicate prevention, see the Javadoc for AcceptOnceFileListFilter.|\nSpring Integration also provides an outbound gateway file adapter, which will send the file as a payload to the reply channel as soon as written.\nNote: In the above Table 22-1, only some attributes have been explained which the adapter to customize it further can use.\nPage Visitors: 12265', 'Managed file transfer (“MFT”) refers to software or a service that manages the secure transfer of data from one computer to another through a network (e.g., the Internet). MFT software is marketed to corporate enterprises as an alternative to using ad-hoc file transfer solutions, such as FTP, HTTP and others.\nTypically, MFT offers a higher level of security and control than FTP. Features include reporting (e.g., notification of successful file transfers), non-repudiation, auditability, global visibility, automation of file transfer-related activities and processes, end-to-end security, and performance metrics/monitoring.\nMFT applications are available as both on-premises licensed software packages and software-as-a-service (“SaaS”). Some are specially designed for enterprise use while others are for sale to individual consumers. A few enterprise-focused SaaS MFT providers also manage the additions of new trading partners, which can free up a lot of IT (information technology) resources.\nMFT refers to a class of product that manages, secures, centralizes, and automates the transfer of files inside and outside of an organization. There are numerous MFT vendors, including some that support i OS and run on the System i server.\nAt the core of an MFT solution is often an FTP server—or, more likely, an SFTP (uses SSH) or FTPS server (uses SSL) that provides encrypted file transfers. Many MFT solutions also include provisions for sending files via SMTP, HTTP, or HTTPS. Still others rely on proprietary file transfer protocols, and include separate compression and encryption capabilities.\nBut MFT is not merely a collection of FTP, SMTP, or HTTP servers. On top of the transport layer, MFT solutions produce and provide full audit trails showing who transferred what files to where, and how and when they did it. This adds security to basic file transfer activities, and gives MFT a hand in regulatory compliance.\nMFT also includes elements of automation, such as the capability to execute jobs when specific files arrive in specific folders, and to alert IT managers of unexpected situations. This allows MFT products to eliminate complicated scripting and reduce the need for expensive programming expertise.\nMFT also identifies and records successful and failed file transfers to a customer or a partner. This provides elements of non-repudiation, and can help prevent those embarrassing instances where an organization isn’t sure if a critical transfer actually worked.\nMFT applications are characterized by having all or most of the following features:\n- Support multiple file transfer protocols including FTP/S, OFTP, SFTP, SCP, AS2, and HTTP/S.\n- Securely transfer files over public and private networks using encrypted file transfer protocols.\n- Securely store files using multiple data encryption methods\n- Automate file transfer processes between trading partners and exchanges including detection and handling of failed file transfers.\n- Authenticate users against existing user repositories such as LDAP and Active Directory\n- Integrate to existing applications using documented APIs (application programming interfaces)\n- Generate detailed reports on user and file transfer activity.\nFrom its inception, FTP has made moving large volumes of bulk data between any two entities—including file servers, applications, and trading partners—possible. However, FTP (and other communication protocols such as HTTP and SMTP) do not, on their own, provide a way to secure or manage the payload or the transmission. Yet, regardless of the lack of security and management capabilities, many companies have continued to transport large batches of structured and unstructured data using these protocols.\nThis practice is changing, however. According to Gartner Research, “Organizations often use MFT solutions to replace FTP. This is due to increased focus on compliance, privacy regulations and corporate transparency — which demand increased auditing, management, security and process.”\nWhile Managed File Transfer always recovers the same features—reporting (e.g., notification of successful file transfers), non-repudiation, auditability, global visibility, automation of file transfer-related activities and processes, end-to-end security, and performance metrics/monitoring—the way it is used has a major impact on the nature of the appropriate solution. Today analysts and experts agree on 6 different usage patterns for MFT:\n- Ad Hoc\n- Accelerated Transfer or “Extreme” Transfer\n- A2A (Administrative-2-Administration)\nUsing a managed file transfer product can help your organization avoid data leak horror stories. You’ll gain the ability to restrict permissible senders and receivers, centrally log data transfers, securely encrypt your data, and integrate with data loss prevention products. Considering the business value delivered by MFT in conjunction with the answers to the four questions outlined above will help you determine whether MFT has a role in your enterprise.\nWould you like to know more about managed file transfer? What is managed file transfer, why MFT, all about MFT..\nDT Asia Group have following MFT solutions for your choice:']"	['<urn:uuid:cc0f2859-8c5c-482a-a354-2be23d270182>', '<urn:uuid:dc1291ab-c157-4765-a8e5-1e4db3f069fc>']	open-ended	with-premise	long-search-query	distant-from-document	multi-aspect	novice	2025-05-12T10:19:05.071190	9	116	1573
82	what instruments did nick lucas play career	Nick Lucas began by studying solfeggio, then learned to play the mandolin, followed by the banjolin (a modern American variant of the banjo). He later added the guitar and tenor banjo to his repertoire. He performed with these instruments on streetcars, at social functions, and later on vaudeville stages.	['From the March/April 2020 issue of Acoustic Guitar | By Nick Rossi\nNick Lucas, né Dominic Nicholas Antonio Lucanese, was the first 20th-century guitar hero native to the United States. Born to Italian immigrants on August 22, 1897, in Newark, New Jersey, Lucas enjoyed a remarkable level of success throughout the 1920s and ’30s. He was known as the “Crooning Troubadour” for his smooth, microphone-delivered vocals, backed by his guitar accompaniments, as heard on radio broadcasts, hit records, live performances, and film appearances. By the time of his peak popularity, at the onset of the Great Depression, his name was on everything from guitar method books to celluloid picks. Gibson even produced a Nick Lucas Special, with the guitarist’s specifications of a wide fretboard and extra-deep body. Decades after, players as diverse as Merle Travis and Barney Kessel named him as a seminal influence.\nLucanese began by studying solfeggio with a neighborhood Sicilian teacher. At the urging of his older brother, an accordionist, he learned the mandolin and eventually adopted a more modern banjo-based American variant, the banjolin. Cutting his teeth on streetcars and at social functions, he added the guitar and tenor banjo to his arsenal.\nIn 1912, a teenaged Lucanese made his recording debut: a private sound reproduction experiment for none other than Thomas Edison, the great American inventor. Soon after that, the guitarist began performing on vaudeville stages, before working with bandleaders like Vincent Lopez, Ted Fio Rito, and Sam Lanin in hotel dance orchestras and cabaret combos using his adopted stage name, Nick Lucas. As ragtime transitioned to early jazz, Lucas absorbed elements of syncopated and improvised African-American music into his more traditional Italian-American string approach.\nShortly before he began recording the vocal records for Brunswick that spurred on his commercial success, Lucas cut two guitar instrumentals in 1922 for Pathé Records in New York. “Teasin’ the Frets,” backed with “Picking the Guitar,” ably demonstrated the modernistic style Lucas had developed. By that time, he had acquired as his main instrument a ladder-braced flattop Ciani guitar made in New York by Galiano. The instrument can be heard to great effect on both of these original recordings, as well as a 1923 rerecording for Brunswick. This time around, “Picking the Guitar” was placed on the A side of the single, which suggests its popularity.\nLucas returned to “Picking the Guitar” two more times in his career: another session for Brunswick in 1932 (captured for the first time via the then recently established electrical recording process) and in 1937 as part of a radio-only transcription disc session in Hollywood. These later recordings feature Lucas possibly playing an archtop, but he just as likely used one of his signature Gibsons. The steel-string that bore his name eventually became the only instrument he performed with, and “Picking the Guitar” stayed in his repertoire for decades.\nA Bird’s-Eye View\nThe 1932 Brunswick recording of “Picking the Guitar” is possibly the most well-known, having been featured on numerous compilations. By the early 1930s, the composition had shed some of its more ragged edges, while allowing Lucas room for improvisation on in its fixed structure and melody. The form is one familiar in both the ragtime and Italian traditions, consisting of three strains: a main theme, notated here as the A section in the key of C and revisited throughout the piece; a second section in the relative key of A minor, indicated as section B; and section C, in F major. Before jumping into a work such as this, it can be helpful to map out the song in its entirety, either mentally or on paper. While you get acquainted with the piece, feel free to treat Fills 1–3 as optional, as they reflect the liberties Lucas had begun taking by the early 1930s.\nOverall, the rhythm of “Picking the Guitar” lies somewhere between ragtime of the early 1900s and the hot music of the 1920s. Guitar rag, a subgenre with which the tune shares some similarities, often recycled both popular and obscure themes long after they fell from favor with piano players. This piece covers such ground. The syncopation might be less obvious on paper than on the recording, but it is certainly an important facet of the music. That said, there is also a stiffness to the rhythm as originally played by Lucas, one which separates the music from much of 1920s jazz.\nChord Shapes and Downstrokes\nLucas’s instrumental technique very much informs “Picking the Guitar.” As seen in extant film and television performances, the guitarist favored full chord forms even when picking individual notes, often using unorthodox fingerings. Scan through the piece for single-note lines that belong to chord shapes, working through which notes are played. Having the full chord shapes at one’s disposal allows for a broader palette when desired. It also helps manage unwanted ringing strings, beyond diligent muting with the picking hand’s palm. Note, too, that there is piano accompaniment on the original recording. As this arrangement is for solo guitar, it can be helpful to have fuller chords at your disposal.\nLucas, like most players in the Italian string virtuoso world, relied heavily on downstrokes. Doing the same will go a long way towards copping his distinctive sound, as this picking approach also affects rhythm and articulation. While contemporary guitarists tend to find alternating pick strokes helpful in playing more intricate lines at greater speed, I highly recommend that you use as many downstrokes as possible here. Start slowly, aiming for an even articulation of every note, particularly in arpeggios, such as those introduced in bars 5–6 and 41–43.\nWhen it comes time to tackle the piece as a whole, again, start slowly and build up speed. Take solace that the piece works well at both moderate and brisk tempos, providing the individual player some headroom for development. Although not notated, rubato (expressively disregarding a strict tempo) can be used throughout, particularly at the end of a given section, just before launching back into the main theme of section A. Lucas altered his tempos over the years, with varying degrees of rubato from version to version.\nGet stories like this in your inbox\nOnce you’re comfortable with the entire piece at a moderate to bright tempo, you might explore the dynamics of the music and the Lucas style. In addition to working in the optional fills and rubato sections previously mentioned, experiment with alternating between letting full chords ring and emphasizing single notes, using your own discretion and taste. And be sure to look out for a couple of fleeting but effective uses of vibrato, very common among Italian string players; the goal here is even and subtle, with just a slight change of pitch.\nIt should be noted that Lucas tended to play his lead lines closer to the guitar’s bridge, which imparted the snappy attack heard on his recordings. Depending on the tone and timbre of your own instrument, picking about two to three inches in front of the bridge can yield some pleasing results that add a Lucas-approved quality to your performance.\nMost important, you should enjoy the guitaristics of this piece. This is music written on guitar to be played on a guitar. The existing recordings hint at Lucas exploiting the instrument’s strengths and attributes. One can easily imagine the legendary musician, who died in 1982 at the age of 84, continuing to play this piece late in life for friends.\nNick Rossi is a San Francisco-based guitarist, bandleader, writer, and historian with a long-time focus on traditional jazz and related\nAmerican music. nickrossiguitar.com\nThis article originally appeared in the March/April 2020 issue of Acoustic Guitar magazine.']	['<urn:uuid:d2c6dbbc-4426-456c-a33d-c1c82142f888>']	open-ended	with-premise	short-search-query	similar-to-document	single-doc	novice	2025-05-12T10:19:05.071190	7	49	1276
83	when did shrogs park halifax open	Shrogg's Park in Halifax was officially opened to the public on 23rd June 1881.	"['- Heritage Category:\n- Park and Garden\n- List Entry Number:\n- Date first listed:\nThe above map is for quick reference purposes only and may not be to scale. For a copy of the full scale map, please see the attached PDF - 1001557.pdf\nThe PDF will be generated from our live systems and may take a few minutes to download depending on how busy our servers are. We apologise for this delay.\nThis copy shows the entry on 28-Jul-2021 at 14:29:04.\nThe building or site itself may lie within the boundary of more than one authority.\n- Calderdale (Metropolitan Authority)\n- Non Civil Parish\n- National Grid Reference:\n- SE 08234 26130\nA public park laid out in the late 1870s and opened in 1881.\nOn 22nd November 1872, Colonel Edward Akroyd, a leading business man and benefactor, promised to build Shrogg\'s Park in Halifax. The site was a rough, irregular piece of waste land, entirely barren at one end and at the other, thickly covered with dwarf oak scrub, from which the Park gained its name, and pieces of rock. His plans almost came to nothing when the proposed Midland Railway scheme intended to build a line through part of what is now the Park. During a committee meeting of the House of Commons, Akroyd gave evidence of the measures he had undertaken so far and of his intention to provide a place for cricket, bowls, croquet, archery and other games. The railway scheme was turned down and work on the Park commenced. From 24th June 1873, the freehold for the land was leased from Captain Henry Savile of Rufford Abbey, Nottinghamshire for 999 years at a rent of £1. At a cost of over £10,000, work began on building the boundary wall, levelling ground and forming drives and footpaths. On 25th June 1879, Colonel Akroyd handed the partially completed park over to the Halifax Corporation for the benefit of the town. A list of conditions were attached to the generous gift:- the area was to be used only as a public promenade and recreation ground, and kept open during the whole year, including Sundays; that no charge be made for admission; that music and games be allowed under certain regulations; and that the Corporation spend a minimum of £1500 completing the Park and set aside an annual maintenance budget of at least £100 to keep the Park in a proper condition.\nThe final layout of the Park featured over 60,000 trees and shrubs; a fountain basin; a handsome, ornamental drinking fountain; striking bedding out displays and a shelter pavilion. In April 1881, Captain Savile gifted two plots of land in Shrogg\'s Road to improve the entrance to the Park. Later that year, on the 23rd June, the Park was officially opened to the public. It remains (2001) in public ownership today. DESCRIPTION\nLOCATION, AREA, BOUNDARIES, LANDFORM, SETTING Shrogg\'s Park is located c 2km north-west of Halifax town centre. It is situated on high promontory overlooking the Shroggs Mills and the Hebble Brook valley to the west. To the north and east the park lies against streets of late C19 stone villas whilst to the west and south the land falls to the industrialised valley. The park is c 10ha and comprises a mixture of landscape elements. The centre of the park is level and is laid out to accommodate a lake, floral displays, sports field, bowling greens, playground and depot. To the west and south-west the land falls steeply to the Shrogg\'s Road. The slopes are covered with broad leaf scrub and in the north-west of the site, rock outcrops form a striking landscape feature and viewing platform to the south and west. The land to the north and east falls more gently to St. George\'s Road and Lee Mount Road.\nA dressed stone wall c 1.70m high contains the park. Along St.George\'s Road and to the west and east of the Lee Mount Road entrance, the wall is reduced to a curb to accommodate iron railings, which were removed in the mid C20 as part of the war effort. Along the north of the park against the Lee Mount School the boundary is marked by a steel security fence c 2.25m high.\nENTRANCES AND APPROACHES The principal entrance, and accompanying lodge, is at the junction of Lee Mount Road and St. George\'s Road in the east of the park. Three stone gate piers, c 3m high mark the vehicular and pedestrian entrance although the original gates are gone and are replaced with a steel security barrier of late C20 design and construction. To the west and east of the gate piers the wall comprises a low stone curb on which iron railings were located. These were removed in the mid C20 and have not been replaced (2001). A second vehicular and pedestrian entrance is situated at the extreme north of the park at the junction between Lee Mount Road and Wheatley Road. The three stone gate piers are in place although they are not complete as two of them have lost their stone balls that cap the pier.\nIn addition to the two main entrances there are three pedestrian entrances. One is situated on Lee mount Road c 120m west of the Lodge Entrance. Original stone gate piers remain, however the steel gate is from the late C20. Restoration work and repointing to the stone wall to the west and east of the entrance is inappropriate and not of conservation standard. A second entrance is c 150m south of the Lodge Entrance on St. George\'s Road. The gate and gate piers are missing. Access to the south of the park from Shrogg¿s road is via a flight of stone steps which punctuate the boundary wall. Two stone gate piers, c 2m high, mark the entrance and although the gate is missing there is 0.4m run of original cast iron railings on a stone curb still in situ on either side of the piers. Immediately beyond the entrance a 3.5 m high stone wall retains the steep west bank of the park. Access to the north-west and south-east paths along the bank is provided by two flights of steps which run along the face of the retaining wall. The steps lead to the paths that are surfaced with stone setts, which incorporate a distinctive raised run of setts at frequent intervals. This configuration is known locally as cat steps and was designed to aid grip on the stone paths.\nPRINCIPAL BUILDING The principal building is a stone and slate lodge situated c 20m south of the entrance gates at the junction of St. George\'s Road and Lee Mount Road. It is still used as accommodation and is in a good state of repair.\nGARDENS AND PLEASURE GROUNDS The five entrances are linked to a perimeter path that encompasses the flatter areas of the park on the top of the hill. Set within the path are the recreational and games facilities including two bowling greens, multi purpose sports areas and two football fields. Between the path and the north boundary wall shrubberies covered banks\nThe principal park features are located in the north-west of the site. Located c 150m south-east of the Wheatley Road entrance a stone edged fountain basin is placed in the centre of gently raised grassed banks. The fountain no longer contains water and is now (2001) turfed. A raised bank for floral displays is situated c15m to the west of the fountain basin however the bank also now (2001) completely turfed . Approximately 25m to the south-east of the basin is a small stone edged serpentine lake c 70m long and c 14m wide. A gothic style, stone drinking fountain set on four steps is situated on the perimeter walk c 60m south-east of the fountain basin.\nShrogg\'s Road to the west is served by two historical paths which are paved with stone setts with rockery stone edges. A third path, which ran underneath the rock outcrops in the north-west of the park has fallen into disuse. C20 paths were built along the western slope.\nHalifax Antiquarian Society Transactions,1948, p98 The New Shrogg\'s Park, The Halifax Guardian, 4 June 1881 Shrogg\'s Park, Colonels fight against railway inroad, The Halifax Guardian, 30 January 1950\nMaps OS 25"" to 1 mile: 2nd edition published 1907 2nd edition published 1919 3rd edition published 1922 3rd edition published 1933\nIllustrations Early C20 postcards of Shrogg\'s Park, Halifax, Halifax Central Library\nArchival items Halifax County Borough Minutes, 1880-1883, Halifax Central Library\nDescription written: July 2001 Amended: August 2001 Register Inspector: PV Edited: October 2001\nThe contents of this record have been generated from a legacy data system.\n- Legacy System number:\n- Legacy System:\n- Parks and Gardens\nThis garden or other land is registered under the Historic Buildings and Ancient Monuments Act 1953 within the Register of Historic Parks and Gardens by Historic England for its special historic interest.\nEnd of official listing']"	['<urn:uuid:09d9f0af-cf53-43c3-9f59-ca5ede30b30e>']	factoid	direct	short-search-query	distant-from-document	single-doc	novice	2025-05-12T10:19:05.071190	6	14	1496
84	When did cattle grazing begin in Charters Towers area?	The area was thrown open to cattle grazing in the 1860s.	['RECENT rain in the Charters Towers area of North Queensland has lifted eyes towards the sky, valuer Herron Todd White reports in its latest monthly property review.\n“There has been some good moisture in the sky with clouds building in the afternoons. Recent events have developed with isolated thunderstorms delivering up to 24mm in the odd spot and barely spit in others,” HTW reported.\n“The old saying rings in the ears that where the first break of the season is, that is where it will fall for the oncoming wet season. These are early falls, however they have had immediate effect on the minds of land holders.”\nLast wet season, the Charters Towers area (more so to the north) had regular steady rain events, producing a cracking season for some.\nLimited country on offer\n“From an agricultural perspective, this district has lots to offer,” HTW’s latest property report said.\n“Since the area was thrown open to cattle grazing in the 1860s, agriculture has endured the tribulations of drought, disease, flood, commodity market cycles and property market cycles.\n“Property market cycles have endured cattle slumps, a millennial boom, a live export ban and now strong demand exists with limited country for sale,” HTW’s report said.\nAt four of the last five auctions in the Charters Towers region, neighbours were successful in expanding their holdings.\n“The auctions of Somerset, Maitland Downs, Reedybrook and Myrrlumbing certainly provided varying gauges of market depth from under-bidders. There have been a few privately negotiated transactions to neighbouring land holders.\n“This degree of demand is somewhat different to the post live export ban period where there was limited market interest in northern cattle breeding assets,” HTW said.\nRecent auction results, all to neighbours have included:\n- Maitland Downs – sold for $9.85 million ($712/ha, $288/ acre) walk in walk out\n- Somerset – sold for $4.3 million ($296/ha, $120/acre) bare of livestock, plant and equipment\n- Reedybrook – sold for $5.67 million ($603/ha, $244/acre) walk in walk out\n- Myrrlumbing – sold for $5.9 million ($373/ha, $150/acre) walk in walk out.\n“As usual, it depends what you apportion to cattle and plant, but our bare/improved land rate range from these sales is from $296 per hectare ($120 per acre) to $538 per hectare ($217 per acre),” HTW said.\n“Even though these are sales to neighbours, one cannot forget the depth of underbidding to take the buyer to the auctioneer’s drop of the gavel.”\nGiven the recent rain, last year’s seasonal conditions, cattle demand, interest rates and slim pickings for buyers, only time will tell whether the broader market starts to push value rates higher in the area.\n“Certainly, the property market conditions are in the vendor’s favour at present,” HTW said.\n“With both neighbours and vendors confident in their property investments and the future, it is very difficult for prospective buyers to invest at present.”\nThe Price to Pay … but where’s the Investment Value and Margin of Safety, to hedge risk … what is that level of Actual Profit, when ROIC (as an average) is above the Cost of Capital (Equity and Debt) for the full term of ownership … why is the Premium being paid, so far above fundamental intrinsic value … is this an Expectations Gap . There’s nothing being Discounted for Risk in these proceedings …\nI’d love to see the cash-flow projections used in these decision-making processes … it’s reminding me of RMWAH all over again, and only a few short years ago.\nThe Argentinian who is buying-up 3-places in the NT, makes sense: with inflation running at over 50% in his home country …\nBut just remember, beef is a commodity … with substitutes … and price fluctuates … especially the China expectations.\nAlways better to consider taking a long-term view … and find the ‘middle’ number in the range, as DCF Analysis does …\nYou may Impair an Asset … and write any premium-paid off via the Income Statement … however, Debt may only be re-paid, or forgiven.']	['<urn:uuid:5f45da8e-efb8-4351-8151-567a941375e6>']	factoid	direct	concise-and-natural	similar-to-document	single-doc	novice	2025-05-12T10:19:05.071190	9	11	668
85	spiritual physical care homeless initiatives history	Currently, spiritual care for the homeless is provided through programs like common cathedral's outdoor worship services and visits to facilities like the Barbara McInnis House, offering human connection and dignity to vulnerable individuals. Historically, approaches to homeless care have evolved from punitive measures in 16th century England to more supportive initiatives, including the establishment of bridewells and workhouses. Modern healthcare for the homeless remains challenging, with homeless individuals facing higher risks of injuries, poor nutrition, and communicable diseases, while having limited access to medical services due to lack of insurance or identification.	"['This page lists the many ministries provided for the homeless population in Boston by Ecclesia Ministries and common cathedral.\nStreet Ministry is often the way we first meet those in need. common cathedral ministers spend hours each week sitting with and listening to un-housed men and women on the streets. We never know where we will end up when we set off in the morning. Shop doorways, disused wharves, benches and sidewalks are frequent locales but so are coffee shops, train stations and the back pews of churches. We are able, often, to direct people to resources that will meet most immediate needs such as food, blankets, and emergency medical referrals. We also work closely with medical, mental-health and housing outreach workers to better serve these needs in the long-term. The main thing we do during street ministry, however, is accompany those in pain. Sitting with people for hours and hours over days and days, and listening deeply to the stories they chose to tell us, births a process of deep-rooted healing that is hard to ignite any other way.\ncommon cathedral has also created and sustained a Spiritual Care program at the Barbara McInnis House, a respite care facility of Boston Healthcare for the Homeless. Ministers offer the human connection that vulnerable, lonely men and women need to heal and, too often, to die with dignity, love, respect, and companionship. common cathedral also visits those in area hospitals and jails who have no one to visit or offer spiritual support.\ncommon cathedral is the cornerstone, and continues as the heart and soul, of Ecclesia Ministries Inc. Each Sunday, anywhere from 30-70 persons come together for outdoor worship, rain or shine, on the Boston Common. Our congregation partners with housed groups, who bring lunch for all and stay to worship. Afterwards, members of common cathedral meet regularly with staff and work together, as a Leadership Team, to chart the course of the program. Post-worship gospel reflection happens regularly to support a deeper personal connection with the community and scripture\ncommon art takes place each Wednesday at Emmanuel Episcopal Church, Boston. On an average day, anywhere from 40 to 75 homeless and low-income persons come together and, with the support and encouragement of our artist-in-residence and pastoral staff, create. A team of volunteers (individuals, interns and visiting youth groups) works with community members to serve lunch and provide companionship and support. Artists are encouraged to participate in local art shows – as well as to join field trips to local art exhibits. A common art leadership team, comprised of staff, community and volunteers, meets weekly to chart the course and mission of the program.\nCityReach takes place up to 9 times each academic year. Currently, up to 70 guests each event (youth with adult chaperones) from housed congregations learn about homelessness in Boston – directly from those who have experienced it. Leadership for each City Reach include our clergy -- as well as up to 20 stipended members of the common art/common cathedral communities. Additionally, clothing donated by the housed congregations is made available to any homeless residents in Boston. This “shopping” opportunity provides material support to anywhere from 100 to 300 un-housed Bostonians per City Reach event.\nWhen the Long Island Bridge closed in the winter of 2015, over 700 members of our community lost access to shelter and support programs. In response, 65 leaders from institutions around Boston responded with BostonWarm, a day center created to provide a range of services from basic companionship to medical referrals. In October of that year, common cathedral was honored to take over the management of this program, and we now welcome 70 or more guests twice each week at Emmanuel Church on Newbury Street.', 'Vagrancy is the condition of being a vagrant, defined as a person, usually poor, who wanders from place to place without a home or regular work. They are often called homeless people; urban vagrants are commonly called ""street people."" Some towns have shelters for vagrants. Vagrancy was a crime in many European countries, but most such laws have been abandoned. Legally, a person with a source of income is not a vagrant, even if they are homeless.\nWhile societies have made efforts to deal with the large numbers of homeless people that emerged since the Industrial Revolution and urbanization, the problem has continued into the twenty-first century. Resources, including shelter, food, public health care, and even sources of income have been made available in many cities. Yet, large numbers of people, unable to deal with the stresses of employment and maintaining a home, remain on the streets.\nTo solve this problem requires effort on the part of all people, not just official government-sponsored programs. People are essentially social beings, and when they have no successful human relationships, within their family, peers, neighbors, or colleagues at work, they cannot function in society. With the breakdown of the family concurrent with the growth of large cities, people become lost and lonely, lacking support, and eventually fall by the wayside into the streets. Removing them from the streets is not done by building them institutions, but by embracing them into loving human relationships.\nA vagrant is a person with no permanent home or employment. Vagrancy is generally used interchangeably with homelessness.\nHomelessness refers to the condition and societal category of people who lack fixed housing, usually because they cannot afford a regular, safe, and adequate shelter. The term ""homelessness"" may also include people whose primary nighttime residence is in a homeless shelter, in an institution that provides a temporary residence for individuals intended to be institutionalized, or in a public or private place not designed for use as a regular sleeping accommodation for human beings. A small number of people choose to be homeless nomads, such as some Roma people (Gypsies) and members of some subcultures.\nThe term used to describe vagrants in academic articles and government reports is ""homeless people."" Popular slang terms, some of which are considered derogatory, include: “tramp,” “hobo” (U.S.), “transient,” “bum” (U.S.), “bagman/bagwoman,” or the wandering poor. The term (of) ""No Fixed Abode"" (NFA) is used in legal circumstances. Sometimes the term “houseless” is used to reflect a more accurate condition in some cases.\nIn different languages, the term for homelessness reveals the cultural and societal perception and classification of a homeless person:\nA small number of homeless people choose to be homeless, living as nomads, a way of life in many cultures for thousands of years. A 2001 study on homelessness issues in Europe noted that ""Urban transience [e.g., homelessness] is different from nomadism/rootlessness or travelling..."" in that nomads and Gypsy travelers in caravans have ""planned mobility"" rather than forced mobility. In Britain, most nomadic people are Roma (or Gypsy) people, Irish travelers, Kalé from North Wales, and Scottish travelers. Many of these people ""... continue to maintain a semi-nomadic lifestyle and live in caravans""; however, ""others have chosen to settle more permanently in houses."" Some European countries have developed policies that acknowledge the unique nomadic (or ""traveling"") life of Gypsy people. Similar work has also been done by the Australian government, regarding the subgroup of Aboriginal people who are nomadic. In large Japanese cities such as Tokyo, the ""many manifestations of urban nomadic"" include day laborers and subculture groups.\nHomelessness has existed as serious a problem since urbanization and industrialization. In sixteenth-century in England, the state first tried to give housing to vagrants instead of punishing them, by introducing bridewells to take vagrants and train them for a profession. In the eighteenth century, these were replaced by workhouses, but these were intended to discourage too much reliance on state help.\nIn most countries, many towns and cities had an area which contained the poor, transients, and afflicted, such as a ""skid row."" In New York City, for example, there was an area known as ""the Bowery,"" where alcoholics were to be found sleeping on the streets, bottle in hand. This resulted in rescue missions, such as the oldest homeless shelter in New York City, The Bowery Mission, founded in 1879 by the Rev. and Mrs. A.G. Ruliffson.\nIn smaller towns, there were hobos, who temporarily lived near train tracks and hopped onto trains to various destinations. Especially following the American Civil War, a large number of homeless men formed part of a counterculture known as ""hobohemia"" all over America.\nModern homelessness started as a result of the economic stresses in society, reduction in the availability of affordable housing, such as single room occupancy (SRO), for poorer people. In the United States the deinstitutionalization of patients from state psychiatric hospitals was a precipitating factor which seeded the homeless population, especially in urban areas such as New York City.\nThe Community Mental Health Act of 1963 was a factor in setting the stage for homelessness in the United States. Long-term psychiatric patients were released from state hospitals into SROs and sent to community health centers for treatment and follow-up. It never quite worked properly and this population was largely found living in the streets soon thereafter with no sustainable support system.\nAlso, as real estate prices and neighborhood pressure increased to move these people out of their areas, the SROs diminished in number, putting most of their residents in the streets. Other populations were mixed in later, such as people losing their homes for economic reasons, and those with addictions, the elderly, and others.\nIn 1979, a New York City lawyer, Robert Hayes, brought a class action suit before the courts, Callahan v. Carey, against the city and state, arguing for a person\'s state constitutional ""right to shelter."" It was settled as a consent decree in August 1981. The city and state of New York agreed to provide board and shelter to all homeless men who met the need standard for welfare or who were homeless by certain other standards. By 1983 this right was extended to homeless women.\nBy the mid-1980s, there was also a dramatic increase in family homelessness. Tied into this were an increasing number of impoverished and runaway children, teenagers, and young adults, which created a new sub-stratum of the homeless population. Also, in the 1980s in the United States, federal legislation was introduced for the homeless as a result of the work of Congressman Stewart B. McKinney. In 1987, the McKinney-Vento Homeless Assistance Act was enacted.\nThe major reasons and causes for homelessness as documented by many reports and studies include:\nThe high cost of housing is a by-product of the general distribution of wealth and income. The rate of homelessness has also been impacted by the reduction of household size witnessed in the last half of the twentieth century.\nIndividuals who are incapable of maintaining employment and managing their lives effectively due to prolonged and severe drug and/or alcohol abuse make up a substantial percentage of the U.S. homeless population. The link between substance abuse and homelessness is partially caused by the fact that the behavioral patterns associated with addiction can alienate an addicted individual\'s family and friends who could otherwise provide a safety net against homelessness during difficult economic times.\nIncreased wealth and income inequality have caused distortions in the housing market that push rent burdens higher, thereby decreasing the availability of affordable housing.\nSome homeless individuals choose not to have a permanent residence, including travelers and those who have personal spiritual/religious convictions (as yogis in India). Most researchers feel the population of individuals who choose not to have a permanent residence is negligible. Many people who respond that they ""prefer"" the homeless lifestyle suffer from mental illness, trauma, or have adapted to the lifestyle and the response reflects a socially-desirable response or justification rather than having no real desire for stable shelter.\nFactors placing an individual at high-risk of homelessness include:\nThere are many places where a homeless person might seek refuge.\nHomeless shelters are temporary residences for homeless people. Usually located in urban neighborhoods, they are similar to emergency shelters. The primary difference is that homeless shelters are usually open to anyone, without regard to the reason for need. Some shelters limit their clientele by gender or age.\nMost homeless shelters expect clients to stay elsewhere during the day, returning only to sleep, or if the shelter also provides meals, to eat; people in emergency shelters are more likely to stay all day, except for work, school, or errands. Some homeless shelters, however, are open 24 hours a day.\nThere are daytime-only homeless shelters, where the homeless can go when they cannot stay inside at their night-time sleeping shelter during the day. Such an early model of a daytime homeless shelter providing multi-faceted services is Saint Francis House in Boston, Massachusetts.\nHomeless shelters are usually operated by a non-profit agency, a municipal agency, or associated with a church. Many get at least part of their funding from local government entities. Shelters can sometimes be referred to as ""human warehouses.""\nHomeless shelters sometimes also provide other services, such as a soup kitchen, job seeking skills training, job training, job placement, support groups, or substance abuse treatment. If they do not offer any of these services, they can usually refer their clients to agencies that do.\nHealth care for the homeless is a major public health challenge. Homeless people are more likely to suffer injuries and medical problems from their lifestyle on the street, which includes poor nutrition, substance abuse, exposure to the severe elements of weather, and a higher exposure to violence (robberies, beatings, and so on). Yet at the same time, they have little access to public medical services or clinics, in many cases because they lack health insurance or identification documents.\nThere are many organizations providing free care all over the world for the homeless, but the services are in great demand given the limited number of medical practitioners helping. For example, it might take months to get a minimal dental appointment in a free-care clinic. Communicable diseases are of great concern, especially tuberculosis, which spreads in the crowded homeless shelters in high density urban settings.\nMany non-profit organizations such as Goodwill Industries maintain a mission to ""provide skill development and work opportunities to people with barriers to employment,"" though most of these organizations are not primarily geared toward homeless individuals. Many cities also have street newspapers or magazines: publications designed to provide employment opportunities to homeless people or others in need.\nWhile some homeless have paying jobs, some must seek other methods to make money. Begging or panhandling is one option, but is becoming increasingly illegal in many cities. Despite the stereotype, not all homeless people panhandle, and not all panhandlers are homeless. Another option is busking: performing tricks, playing music, drawing on the sidewalk, or offering some other form of entertainment in exchange for donations. In cities where pharmaceutical companies still collect paid blood plasma, homeless people may generate income through frequent visits to these centers.\nHomeless people have been known to commit crimes just to be sent to jail for food and shelter. In police slang, this is called ""three hots and a cot"" referring to the three hot daily meals and a cot to sleep on given to prisoners. Similarly a homeless person may approach a hospital\'s emergency department and fake a physical or mental illness in order to receive food and shelter.\nSeveral organizations in major cities have tried to be inventive about help to the swelling number of homeless people. In New York City, for example, in 1989, the first street newspaper was created called Street News which put some homeless to work, some writing, producing, and mostly selling the paper on streets and trains. It was written pro bono by a combination of homeless, celebrities, and established writers. In 1991, in England, a street newspaper, following on the New York model was established, called The Big Issue and was published weekly. Its circulation grew to 300,000. Chicago has StreetWise which has the largest circulation of its kind in the United States, 30,000. Boston has a Spare Change newspaper built on the same model as the others: homeless helping themselves. More recently, Street Sense in Washington, D.C. has gained a lot of popularity and helped many make the move out of homelessness. Students in Baltimore, Maryland have opened a satellite office for that street paper as well.\nThe number of homeless people worldwide has grown steadily in recent years, as has a rash of violent crimes against the homeless. In some Third World nations such as Brazil, India, Nigeria, and South Africa, homelessness is rampant, with millions of children living and working on the streets. Homelessness has become a problem in the cities of China, Thailand, Indonesia, and the Philippines despite their growing prosperity, mainly due to migrant workers who have trouble finding permanent homes and to rising income inequality between social classes. It is estimated that $19 billion a year is needed to end hunger associated with homelessness.\nAll links retrieved January 13, 2016.\nNew World Encyclopedia writers and editors rewrote and completed the Wikipedia article in accordance with New World Encyclopedia standards. This article abides by terms of the Creative Commons CC-by-sa 3.0 License (CC-by-sa), which may be used and disseminated with proper attribution. Credit is due under the terms of this license that can reference both the New World Encyclopedia contributors and the selfless volunteer contributors of the Wikimedia Foundation. To cite this article click here for a list of acceptable citing formats.The history of earlier contributions by wikipedians is accessible to researchers here:\nNote: Some restrictions may apply to use of individual images which are separately licensed.']"	['<urn:uuid:f861b239-10d0-426b-a4de-9c4411f62b09>', '<urn:uuid:a9a60625-467c-4d0e-b658-391667cdcbf3>']	open-ended	direct	short-search-query	distant-from-document	multi-aspect	expert	2025-05-12T10:19:05.071190	6	92	2931
86	electric bus advantages api attack risks	Electric buses offer 25% lower operating costs compared to combustion engines and produce zero local CO2 emissions, with reduced noise and optimized energy recuperation. However, their connected systems are vulnerable to API-based attacks, which accounted for 12% of total cyber incidents in 2022, up from 2% in 2021, potentially causing service disruptions and safety risks.	"[""Green mobility on the road\nThe transport of people and goods is responsible for a total of 25 to 30 percent of the world's final energy consumption. The greatest potential for reducing energy consumption is in the area of road traffic. At the same time, a Siemens study based on a model urban district in Munich shows how an almost carbon-free city can become a reality through the coordination of numerous measures to increase the energy efficiency.\nElectric bus eBus/eBRT\nBus transportation is an important backbone of the urban public traffic system. The evolution of buses can be traced from combustion engines to hybrid technology and further in the direction of purely electricity powered buses. The operating costs (energy and service) of our eBuses are about 25% lower than those of similar combustion engine powered buses.\nZero local CO2 emissions, reduced noise and energy savings due to excellent efficiency and optimized energy recuperation complete the picture of the eBus.\nIntermodal traffic management\nIn Halle, Germany, more than 50 percent of drivers have switched to trams since the introduction of an intermodal traffic concept. The system informs drivers of the current traffic situation and free parking spaces as well as the departure time of the next tram. As a result, the intelligent networking of Sitraffic system modules has optimized traffic flow in the city of Halle and made using public transportation even more attractive.\nAdaptive street lighting\nDynamic management of street lighting allows energy consumption and therefore operating costs to be reduced considerably - all the while ensuring traffic safety. Traffic and street illumination are managed in close coordination, and street lights are adjusted automatically according to the current traffic situation. This adjustment ensures that street lighting only runs at full load when the volume of traffic requires it. At other times, for example late at night, the lighting intensity can be reduced sharply, resulting in energy savings of up to 60%.\nCity toll systems\nWith its city toll, the British capital of London has reduced the number of vehicles per day by 60,000 and emissions per year by 150,000 tons. Intelligent video systems from Siemens use “video scene analysis” to record license plate numbers and compare them to a database, thus determining whether the toll was paid. This has substantially reduced London traffic, and traffic flow has improved by over 35 percent.\nAdaptive traffic control\nThe “green wave” in Muenster, Germany, helps the city reduce CO2 emissions by 1,000 tons a year. The Sitraffic Motion adaptive network control system flexibly adjusts traffic signals to the amount of traffic, thus reducing congestion, noise and emissions. At the same time, it speeds up traffic – by up to 15 percent in Muenster.\nLED signaling systems\nSignaling systems based on LED technology reduce energy costs by up to 90 percent. LEDs consume only a tenth as much energy as standard incandescent bulbs, but that’s not the only reason more and more operators are adopting this new technology. LEDs also last ten times longer than conventional incandescent bulbs and thus pay for themselves twice over.\nEnvironmentally-conscious traffic management\nTraffic-related emissions of harmful substances can be reduced through the strategic management of urban transport based on environmental and traffic data. The control level within the traffic management control center captures a range of relevant environmental data and decides, based on its stored environmental and traffic-related threshold values, how the traffic signals are operated strategically in order to redirect traffic. Environmentally-sensitive control programs inside the traffic signals therefore reduce or avoid the environmental impact caused by traffic within a particular area. Information panels notify transport users about raised pollution levels and any resulting changes to the traffic management measures in force.\nThe electrification of road freight traffic with eHighway is an economical alternative to the use of fossil fuels in freight transport. Green, efficient and resource-friendly. eHighway trucks are supplied with electrical energy via overhead lines. An adaptive current collector provides the contact between the vehicle and the overhead line. If the contact with the overhead line is interrupted, for example while overtaking or when traveling on non-electrified routes, the eHighway truck automatically switches over to its diesel-hybrid drive. In operation it is therefore just as flexible and universal as a conventional truck. eHighway offers considerable advantages in particular where rail transport is not possible due to economic or space-related restrictions.\nSatellite-based mobility management\nAlso state-of-the-art satellite navigation offers ways to optimize our traffic and logistics solutions – for example we offer a full range of Sitraffic Sensus components , systems and services needed for satellite-based toll collection systems. Including on-board units that reliably calculate with meter accuracy via satellite without expensive infrastructure facilities. It will also permit the implementation of other entirely new applications, from forward-looking traffic flow control and reduced traffic jams and CO2 emissions to the intermodal tracking and monitoring of goods and vehicles."", 'The emergence of smart mobility services and applications has led to a sharp increase in the use of APIs in the automotive industry. However, this increased reliance on APIs has also made them one of the most common attack vectors. According to Gartner, APIs account for 90% of the web application attack surface areas.\nWith no surprise, similar trends are emerging also in the smart mobility space. A recent Automotive and Smart Mobility Cybersecurity Report by Upstream Security indicates that the automotive and smart mobility ecosystem has seen a 380% increase in API-based incidents in 2022, compared to 2021. Additionally, APIs accounted for 12% of total cyber incidents in 2022, up from only 2% in 2021.\nWhen examining smart mobility applications and services, Upstream’s threat intelligence team reported that black-hat actors were found to be behind 53% of incidents, indicating malicious intent as the driving force of the majority of API-related attacks. The impact of these incidents goes well beyond data and PII breaches, often causing service disruptions, fraudulent activities, trust concerns and potential revenue loss.\nData-driven mobility services reshape traditional automotive revenue models\nIn recent years, vehicle connectivity has grown dramatically, introducing innovative data-driven revenue opportunities for traditional automotive stakeholders as well as new players. Mobility overall has become more connected, with ridesharing, car rental services, and even fleet management services all utilizing mobile applications to provide easy access and enhanced experience to consumers. Modern data use cases offer continuous monitoring and help stakeholders introduce new features and opportunities. According to research by McKinsey, 30% of automotive revenue will be attributed to data-driven and smart mobility services by 2030. But unlike IT-based applications, these innovative and data-driven applications heavily utilize APIs that have a direct impact on vehicles on the road.\nMassive traffic jams caused by manipulated API transactions\nIn 2022, we saw an increasingly sophisticated use of APIs in automotive and smart mobility cyberattacks. As an attack vector, APIs draw the attention of researchers and malicious actors as they require a relatively low threshold of know-how and automotive expertise. Essentially reducing the barrier of entry of threat actors to a minimum. A single vulnerability in an API can pose a direct impact on millions of vehicles, across various fleets. A strong example of the ease of attack and the significant impact can be found in a recent incident in Europe: back in mid-2022 central Moscow was gridlocked when a ride-hailing service was maliciously manipulated to send all available taxis to a single location, causing hours-long traffic jams, impeding people’s freedom of movement, and putting public infrastructure and safety at risk. In this case, attackers didn’t need any understanding of how vehicles operate or function, all they needed to do was to identify API vulnerabilities and exploit them.\nWAF is not (always) enough: developing a contextual framework for smart mobility API security\nSmart mobility services have always been monitoring and securing API transactions to avoid revenue loss due to fraud, service downtime, and compromising organizational or users private data. But, traditional API security solutions have a significant “blindspot” when it comes to smart mobility. They often fail to detect sophisticated attacks that impact mobility applications, assets, and consumers due to the lack of contextual analysis of the impact of API transactions on moving vehicles.\nEnsuring a strong cybersecurity posture in the smart mobility ecosystem requires expanding the scope to include a contextual analysis of the real-life impact of APIs on mobility assets, including vehicles on the road. API security is gradually evolving to also integrate OT (Operational Technology) aspects that correlate between API traffic, transactions, and the contextual state of mobility assets to provide a strong cybersecurity posture. The goal is to layer API discovery, profiling and monitoring with deep analysis of the behavior of mobility assets and the impact of the specific API transaction on safety. When you take into account how OT assets behave differently to IT assets, be it their ignition status, location, or speed, you can begin to approach securing them in a contextual manner.\nSmart mobility players are adopting a new approach to securing smart mobility API transactions, which includes four key steps:\n- Map the potential attack surface\n- Continuously monitor API traffic\n- Apply contextual anomaly detection\n- Mitigate and respond to cyber threats\nThe first step is to understand the potential attack surface as it relates to APIs. This requires inventorying APIs used by services, applications, and third parties, from documentation sources like Swagger as well as the analysis of live, real-time API traffic and transactions. This analysis includes both documented, undocumented or even the depreciated-but-live APIs that might be a “perfect” access point for threat actors.\nOnce the attack surface is understood, monitoring API traffic helps increase the cybersecurity posture by ensuring that any changes are documented and detected, as well as any misuse of misconfigurations. When monitoring mobility API traffic in real-time, it’s important to consider the ability to handle the scale and complexity of these transactions and recognize any deviations from the normal state of the asset. When a single API call can start the engine of a vehicle or report back the location of a driver – cybersecurity becomes extremely important.\nSince smart mobility assets are OT in nature, the context that they are in – their state at a given moment in time – can be used in securing them. Any major deviations or anomalies in their behavior can indicate potential misuse or attack. By correlating the assets state with the API traffic, cyber teams can understand the contextual implications and the impact on applications or users. Seemingly valid requests can sometimes be an indicator of malicious intent. For example, a single IP, though not suspicious initially, that sends requests to multiple vehicles or mobility applications should trigger immediate suspicion and investigation.\nUpstream Security has recently taken a step further in the contextual analysis of API transactions. It relies on a robust digital twin, which is a live digital representation of the asset’s state, built on data streams from applications, backend servers, telematics services, and more. As a result, it offers a comprehensive view of all mobility assets and users impacted. Once the attack or misconfiguration has been detected using the unique context provided by understanding the state of the asset, cyber teams can effectively and quickly respond and mitigate potential risks.\nThis unique mobility–driven approach opens a new scope of API security activities in the smart mobility ecosystem. As innovation in this space is moving so fast, introducing new transportation models, autonomous services and ride-sharing options, eliminating “blindspots” will continue to be a top priority challenge.']"	['<urn:uuid:bb0f64cc-5dc1-48d3-b146-c574a46424a9>', '<urn:uuid:c5438a12-3b52-47b0-b52a-f87b79cbe1e4>']	factoid	with-premise	short-search-query	similar-to-document	multi-aspect	expert	2025-05-12T10:19:05.071190	6	55	1912
87	bacterial infection risks flooding contaminated water vs food storage duration refrigerator	During floods, bacterial infections including E. coli and salmonella can be contracted from contaminated water or food, with older adults being especially vulnerable and taking longer to recover. For regular food storage, luncheon meats can be stored 3-5 days after opening, hamburger and ground meats last 1-2 days, while fresh poultry should be used within 1-2 days. Leftovers including cooked meat or poultry can be safely stored for 3-4 days in the refrigerator.	"[""En español | Whether from fearsome hurricanes or rising sea levels, flooding is an increasing threat to many regions of the country, bringing with it a host of health hazards that can be challenging to keep in mind as you toil to dry out your home and salvage your belongings.\nBut wading through brackish waters filled with things such as sewage, industrial waste and breeding mosquitoes presents a stew of threats. Expose an open cut, and you could contract tetanus or a staph infection. Breathe in contaminated droplets — especially if your lungs are compromised or you're older — and you could find yourself battling Legionnaires’ disease. Get bit by one of those moisture-loving pests when you forget to layer on the deet, and you could wind up with West Nile virus.\nAs people return to flooded areas after an evacuation or have to get by until power in their area is restored, many of the most common health threats are bacterial infections — and related diarrheal disease — that can be acquired by ingesting even small amounts of contaminated water or food in a kitchen that’s been doused in flood waters or left without power.\nSuch infections, which include E. coli and salmonella, tend to hit older people especially hard, experts say. With E. coli, for instance, while healthy adults may recover within a week, older adults often struggle longer and are also more likely to advance to a related form of life-threatening kidney failure.\nTo stay safe, here are top tips from the Centers for Disease Control and Prevention and the Food and Drug Administration:\nWhen in doubt, throw it out\nThrow away food that may have come in contact with flood or storm water; this even includes food in metal cans that haven’t been scrubbed with a bleach solution. Their labels can harbor E. coli bacteria.\nThrow out perishable foods that have not been refrigerated properly due to power outages. After a power outage, the refrigerator will keep food safely cold for about four hours if it is unopened. That’s not long, which means you will likely be throwing out quite a bit of food.\nA full freezer will hold a safe temperature for approximately 48 hours if the door remains closed. A half-full freezer stays safe for half as long. If you’re able to plan ahead, having dry ice on hand will extend the clock.\nKnow good from bad water\nDo not use water you suspect or have been told is contaminated to wash dishes, brush your teeth, wash and prepare food, wash your hands, make ice or make baby formula. Safe water for drinking, cooking and personal hygiene includes bottled, boiled or treated water.\nSome water that is unsafe to drink may be safe to use for showering — but only if you are careful not to ingest any of it. That said, you can't use unsafe water to wash your hands. If you do, you could spread bacteria to your mouth.\nMake sure you use soap, create a lather and scrub for at least 20 seconds to get rid of germs. Hand sanitizers need at least 60 percent alcohol to help defeat bacteria, but they won’t be effective when hands are visibly dirty and do not eliminate all types of germs that might be present after a disaster or flood.\nIf you don't have bottled water, you should boil water to make sure it is safe. Boiling water will kill most types of disease-causing organisms that may be present. If the water you have on hand is cloudy, filter it through a clean cloth or allow it to settle, then draw off the clear water at the top for boiling. Boil the water for one minute, let it cool and store it in clean containers with covers.\nGet out the bleach\nYou have to sanitize anything in the kitchen that could have touched flood waters — from your dishes and utensils to your counters (if the water reached them).\nFor dishes and the like, wash with soap and water, using hot water if available. Then rinse them off and sanitize by boiling them in clean water or by immersing them for 15 minutes in a solution of 1 tablespoon of liquid chlorine bleach per gallon of drinking water (or the clearest, cleanest water available).\nWash counters similarly (scrubbing with soap and clean water, then wiping them down with the bleach solution). Throw out any wooden cutting boards or baby items, such as pacifiers, that might have touched flood waters."", 'How to store food in a fridge safely, How long can food be kept in a fridge\nThe recommended storage times of food in a refrigerator or fridge has been recommended by many reputable agencies world wide. Here we will bring you the recommendations of storage times in a fridge or freezer from some well known world institutions.\nThe US Government Food Safety website has a table on their website reproduced below, about storage Times in the Refrigerator and Freezer. To quote from their website: “These short but safe time limits for home-refrigerated foods will keep them from spoiling or becoming dangerous to eat. The guidelines for freezer storage are for quality only. Frozen foods remain safe indefinitely. For storage times for eggs and foods made with eggs, see Egg Storage Chart”. As mentioned above, this is the table from the US Government Food Safety website reproduced below\n|Salads||Egg, chicken, ham, tuna & macaroni salads||3 to 5 days||Does not freeze well|\n|Hot dogs||opened package||1 week||1 to 2 months|\n|unopened package||2 weeks||1 to 2 months|\n|Luncheon meat||opened package or deli sliced||3 to 5 days||1 to 2 months|\n|unopened package||2 weeks||1 to 2 months|\n|Bacon & Sausage||Bacon||7 days||1 month|\n|Sausage, raw — from chicken, turkey, pork, beef||1 to 2 days||1 to 2 months|\n|Hamburger & Other Ground Meats||Hamburger, ground beef, turkey, veal, pork, lamb, & mixtures of them||1 to 2 days||3 to 4 months|\n|Fresh Beef, Veal, Lamb & Pork||Steaks||3 to 5 days||6 to 12 months|\n|Chops||3 to 5 days||4 to 6 months|\n|Roasts||3 to 5 days||4 to 12 months|\n|Fresh Poultry||Chicken or turkey, whole||1 to 2 days||1 year|\n|Chicken or turkey, pieces||1 to 2 days||9 months|\n|Soups & Stews||Vegetable or meat added||3 to 4 days||2 to 3 months|\n|Leftovers||Cooked meat or poultry||3 to 4 days||2 to 6 months|\n|Chicken nuggets or patties||3 to 4 days||1 to 3 months|\n|Pizza||3 to 4 days||1 to 2 months|\nUS FDA recommendations of safe storage of food in a refrigerator or fridge\nUS Food and Drug Administration or FDA recomendations on food storage in a refrigerator or freezer are given on their website.\nFood Safety and Standards Authority of India\nThe Food Safety and Standards Authority of India or FSSAI is responsible for the food safety of India and this is what it says on its website\nThe Food Safety and Standards Authority of India has been established under the Food Safety and Standards Act, 2006 as a statutory body for laying down science based standards for articles of food and regulating manufacturing, processing, distribution, sale and import of food so as to ensure safe and wholesome food for human consumption ….MORE\nUnfortunatly this site is more for businesses and has no advise for consumers, like on how long food can be storaged in the fridge in Indian conditions\nHow long can food be in a fridge without power\nIn India this is a very important point, because of the unreliable electricity in many places in India. The important thing to remember is not to open the fridge when there is no electricity, everytime the door of the fridge is opened the cold air in the fridge escapes and hot air takes its place, raising the temeperature of the food inside. How long can food be in a fridge without power depends mainly on the insulation efficiency of the fridge. Almost all manufacturers have their fridges insulated with PUF (Poly Uratane foam) whic is one of the best insulating materials. LG recently claims to have special fridges which can stay cold and preserve food inside for longer times without power.\nAdvantage of buying AC, Fridge, Washing Machine, etc. online\nBuying online fridges, washing machines, dishwashers and other major kitchen appliances is now the better option in India, just like it is in the USA and other advanced countries. There are several advantages of buying online. First of all, you get unbeatable bargain prices. Secondly, you get new and genuine products. Other benefits include no interest EMI payments, able to return the product for a full refund if not satisfied, etc.\nJust to give you an example, below are some of the benefits Amazon India, the leading global online retailer, offered in India during their last sale. The same deals are not always available, but it is a good idea to check this link to the special offers page on Amazon.in India website.']"	['<urn:uuid:2cbe229c-a2e6-4f60-82d5-9d1521fdfc24>', '<urn:uuid:38d0a792-7f6f-4a69-91d9-79f3a6d399f1>']	factoid	with-premise	long-search-query	distant-from-document	multi-aspect	expert	2025-05-12T10:19:05.071190	11	73	1482
88	defrosting surface environmental impact reduce pollution	The anti-frosting surface is environmentally friendly as it doesn't rely on conventional defrosting methods that harm the environment. Unlike traditional methods that use electricity from polluting fossil fuel power plants or antifreeze chemicals that contaminate groundwater and create harmful airborne droplets, this surface works without electricity or chemicals. It naturally prevents frost formation on 90% of its surface through a system of grooves and ice stripes.	"['Developed by Virginia Tech (VT) researchers, the system can prevent moisture and ice from appearing on 90 percent of its surface. It also saves on electrical costs and is environmentally friendly to boot.\nFor the individual driver and homeowner, the anti-frosting surface promises to put an end to the chore of having to defrost or scrape windows during freezing mornings. For companies and consumers, it might just save billions of dollars in efficiency, maintenance, and repair.\nAircraft protected by the surface will no longer require extensive defrosting treatment that forces airlines to delay flights for hours. Power outages caused by excessive buildup of ice will also be greatly reduced. (Related: Innovative technology will allow dementia patients to live independently in their own homes.)\nThe VT researchers went for a straightforward design for the anti-frosting surface. Using untreated aluminum as the base material, they created ice stripes on an array of tiny grooves that rose from the surface.\nThe grooves are meant to be sacrificial areas. Stripes of ice will form on top of these grooves. The temperature of this intentional ice will form low pressure zones.\nMoisture present in the air is drawn to the low pressure zones of the intentional ice. Because of this suction effect, the intermediate areas that overlap the zones will remain free of frost. This effect was shown to work in conditions with moisture-laden air and close to freezing temperatures.\nThe sacrificial ice stripes comprise just 10 percent of the entire surface area of the anti-frosting material. The rest of the surface stays dry and ice-free. It does not use any electricity to achieve this effect.\n""The real power of this concept is that the ice stripes themselves are the chemistry, which means the material we use is irrelevant,"" explained VT researcher Jonathan Boreyko. ""As long as you have that proper pattern of sacrificial ice, the material you use could be virtually anything. So there are a lot of possibilities.""\nThe new anti-frosting technology will probably be welcomed by the HVAC (Heating, Ventilating, and Air Conditioning) industry. Similar micro-fin patterns can be found on the outdoor parts of heat pumps, fan systems, and other heat exchange products.\nThose patterns can be altered to resemble the grooves on the VT-derived surface to further protect those heating systems from frost. Other items that can mount this surface include airplane wings, aerospace materials, and windshields.\nThe researchers have already obtained a patent for their invention. They brought up another welcome benefit of their technology: Environmental consciousness.\nConventional means of defrosting ice-laden objects are known to indirectly or directly harm the environment. Heating systems use electricity, which often comes from fossil fuel power plants that spew air pollution.\nAntifreeze chemicals are suspected to have negative, long-lasting effects on plants, wildlife, and humans. Yet thousands of gallons of these synthetic chemicals are spent on just a single wing of an airplane. The antifreeze either contaminate groundwater or turn into airborne droplets that get inhaled by people.\nEven good old-fashioned salt will leave residues and loses effectiveness as it gets diluted. But the VT researchers are assured that their anti-frosting surface can do a better job of keeping windshields – and the rest of the world – clean.\nFor more stories about the newest anti-frosting technologies, visit Inventions.news.']"	['<urn:uuid:66772075-7054-41c1-a872-b60125260d6f>']	open-ended	with-premise	short-search-query	similar-to-document	single-doc	expert	2025-05-12T10:19:05.071190	6	66	547
89	As a sleep researcher studying circadian rhythms, I'm intrigued by the connection between sleep and physical performance - how does sleep deprivation affect both hair growth cycles and athletic training outcomes?	Sleep deprivation has significant negative impacts on both hair growth and athletic performance. Studies have shown that sleep deprivation decreases beard-hair growth, which is attributed to lower protein synthesis and reduced hormone release, including growth hormone and dihydrotestosterone (DHT). Additionally, sleep deprivation can alter clock gene expression and disrupt the circadian rhythm, which regulates hair growth cycles. In terms of athletic performance, sleep deprivation can lead to overtraining symptoms including emotional instability, fatigue, depression, decreased performance, loss of muscle strength, and prolonged recovery from training sessions. It can also cause sleep disturbances and gastrointestinal issues, further impacting athletic performance and recovery.	"['Hair growth is a vital part of our lives. If you think about it, scalp, facial, and body hairs are some of the few body parts one can manipulate. In addition to piercing and tattooing skin, hair grooming allows an individual to uniquely present themselves. Mature hair undergo several phases in the hair growth cycle including anagen, catagen, telogen, and exogen. Anagen is the phase of active growth. During this time, factors outside of your body may affect hair growth. For instance, hair usually grows faster in the summer months as compared to the winter. The catagen phase begins when anagen ends. It is a shorter transitional phase where hair stops growing. Telogen is a resting phase that transitions into the exogen phase where old hairs are shed. After the hair falls out, the follicle is inactive for some time before the hair’s biological clock restarts. Each hair follicle is independent from the others and undergoes its own hair growth cycle at different times from each other. This is what prevents your hair from falling out all at once.\nTable 1 - Hair Growth Phases\nHair Growth Phase\nTransition from growth phase to resting phase\nPhase where resting hair is shed from the body\nCircadian Rhythm and Circadian Genes\nThe circadian rhythm is described as a biological clock that affects how the body functions over a 24 hour cycle. The circadian rhythm is affected by the environment, such as light and darkness. This biological clock is regularly matched to a 24 hour day through exposures to light and dark. The circadian clock is centered in parts of our brain that can receive signals from light and temperature. Clock genes are genes that help contribute to our body’s circadian rhythm, also known as the molecular gears of circadian clocks. Therefore, for our circadian clock to be able to respond to the environment, our clock genes also rely on both the internal biology of the body as well as factors outside of the body that work together to affect the hair’s biological clock.\nThe Hair Growth Biological Clock\nThere is evidence that the circadian clock, which adjusts the body’s biology to the day’s natural cycle, regulates skin functions. For each hair growth cycle, hair follicles make new hair. The ability to regenerate hairs throughout life is from the hair stem cells in the hair follicle and the influence of the circadian rhythm.\nOnce growth is complete during the anagen phase, clock genes are expressed in the hair cells at the bottom of the hair follicle. Thus, the circadian clock differs among the phases of the hair growth cycle. The circadian clock can adjust how the hair stem cells are turned off or on, which controls the hair growth cycles. For instance, through a clock gene called BMAL1, stem cell genes can be affected to become more or less active. If mutations arise in clock genes, such as deleting BMAL1, stem cells may become inactive leading to poor hair growth.\nDisturbing the Hair’s Biological Clock\nStudies have shown that disrupting the circadian clock can prolong the phase of active hair growth. BMAL1 and PER1 are core clock genes in the human hair cycle. Because the circadian clock influences cell activity at different stages of the hair cycle, clock gene mutations can result in subtle hair cycle variations. These clock gene dysfunctions may cause certain disorders. The BMAL1 and PER1 clock genes produce signals that terminate the growth phase, anagen. Silencing both of these genes can prolong the hair growth cycle and lead to longer hair.\nWhile disrupting clock genes (BMAL1 and PER1) and prolonging the anagen phase of active hair growth supports the idea that the hair follicles exhibit a biological clock based activity, other hormones are important too. For example, thyroid hormones also prolong anagen in hair follicles. Thyroid hormones are one of the main regulators of the hair growth cycle. Thyroxine (T4) also regulates clock activity of hair follicles and patients with thyroid dysfunction may also show a disordered circadian clock.\nSleep and its Effects on Hair Growth\nSome sleep disorders are related to our circadian rhythms. In some sleep disorder, sleep occurs abnormally early or late. Studies have shown that these disorders often correlate to mutations in clock genes that then lead to an irregular biological clock. On the other hand, sleep deprivation could alter clock gene expression. The mechanism of this is due to the decreased binding of certain clock genes that have been associated with lack of sleep. Our sleep and wake cycles are then affected so that we may not be as alert as we would like to be in the morning or as tired as we hope to be when it is time to sleep.\nSleep deprivation affects the circadian rhythm which may affect hair growth cycles. One study showed that sleep deprivation resulted in decreased beard-hair growth. The study attributed these effects to lower protein synthesis during sleep deprivation as well as less growth hormone and dihydrotestosterone (DHT) release. On the contrary, there is research in twin studies that found associations with sleeping more to correlate with more hair loss in both the front and side parts of their head. However, the researchers cautioned to not simply conclude that prolonged sleep is what causes baldness. Instead, they were concerned that antidepressant and anti-anxiety medications could also lead to prolonged sleep hours, indicating more of a connection between excessive stress, depression, and hair loss.\nThere are still questions as to whether or not more sleep allows your hair to grow lusher and longer. Regardless, there is good evidence that the hair follicle has a biological clock and it may be connected to the body’s circadian rhythm.\nPER1 and BMAL1 are potential targets for therapy in modulating hair growth by avoiding undesired effects on the central clock. By blocking the activity of such genes, future therapies may prevent hair loss (alopecia). By promoting activity of these target genes, therapy may suppress unwanted hair growth (hirsutism).\nMelatonin is commonly used to try to regulate the sleep-wake cycles and it may influence the hair growth cycle as well. Topical application of melatonin may be a treatment option for those with female and male pattern hair loss.\n* This Website is for general skin beauty, wellness, and health information only. This Website is not to be used as a substitute for medical advice, diagnosis or treatment of any health condition or problem. The information provided on this Website should never be used to disregard, delay, or refuse treatment or advice from a physician or a qualified health provider.\nStenn KS, Paus R. Controls of hair follicle cycling. Physiol Rev.2001;81(1):449-494; PMID: 11152763 Link to research.\nPlikus MV, Chuong CM. Complex hair cycle domain patterns and regenerative hair waves in living rodents. J Invest Dermatol.2008;128(5):1071-1080; PMID: 18094733 Link to research.\nDuffy JF, Czeisler CA. Effect of Light on Human Circadian Physiology. Sleep Med Clin.2009;4(2):165-177; PMID: 20161220 Link to research.\nAl-Nuaimi Y, Hardman JA, Biro T, et al. A meeting of two chronobiological systems: circadian proteins Period1 and BMAL1 modulate the human hair cycle clock. J Invest Dermatol.2014;134(3):610-619; PMID: 24005054 Link to research.\nPiggins HD. Human clock genes. Ann Med.2002;34(5):394-400; PMID: 12452483 Link to research.\nJanich P, Pascual G, Merlos-Suarez A, et al. The circadian molecular clock creates epidermal stem cell heterogeneity. Nature.2011;480(7376):209-214; PMID: 22080954 Link to research.\nYusur A. HJ, Biro T., et. al. A Meeting of Two Chronological Systems: Circadian Proteins Period1 and BMAL1 Modulate the Human Hair Cycle Clock. Journal of Investigative Dermatology.2013;134(3):610-619; PMID: 18094733 Link to research.\nPlikus MV CC-M. Complex hair cycle domain patterns and regenerative hair waves in living rodents. The Journal of investigative dermatology.2008;128(5):1071-1080; PMID:\nHardman JA HI, Farjo N, et. al. Thyroxine differentially modulates the peripheral clock: lessons from the human hair follicle. PLoS One.2015;10(3):e0121878; PMID: 25822259 Link to research.\nvon Schantz M, Archer SN. Clocks, genes and sleep. J R Soc Med.2003;96(10):486-489; PMID: 14519724 Link to research.\nMongrain V, La Spada F, Curie T, et al. Sleep loss reduces the DNA-binding of BMAL1, CLOCK, and NPAS2 to specific clock genes in the mouse cerebral cortex. PLoS One.2011;6(10):e26622; PMID: 22039518 Link to research.\nGottesmann C, Hamon JF. Sleep deprivation decreases the beard-hair growth in man. Acta Neurobiol Exp (Wars).1987;47(4):183-186; PMID: 3442272 Link to research.\nGatherwright J, Liu MT, Gliniak C, et al. The contribution of endogenous and exogenous factors to female alopecia: a study of identical twins. Plast Reconstr Surg.2012;130(6):1219-1226; PMID: 22878477 Link to research.\nFischer TW, Slominski A, Tobin DJ, et al. Melatonin and the hair follicle. J Pineal Res.2008;44(1):1-15; PMID: 18078443 Link to research.\nFischer TW, Trüeb RM, Hänggi G, et al. Topical Melatonin for Treatment of Androgenetic Alopecia. Int J Trichology.2012;4(4):236-245; PMID: 23766606 Link to research.', ""How to Prevent Overtraining and Overreaching\nSerious athletes have no time to break from training, right? Wrong. Conditioning for your sport is not all about work, activity and movement. You also have to rest, relax, and recover (or restore, recuperate, and regenerate). Overtraining can lead to a number of serious consequences and set your training back weeks or even months.\nAre you overtraining (pushing your body too far over the course of a training program) or overreaching (going too hard in a single workout or series of workouts)? You've gone too far if you're experiencing any of these symptoms:\n- Sympathetic overtraining syndrome, where your resting heart rate, blood pressure, and metabolic rate are abnormally elevated\n- Parasympathetic overtraining syndrome, where your resting heart rate and blood pressure decrease abnormally\n- Emotional instability like fatigue, apathy, depression or irritability\n- Decreased desire for and enjoyment of training\n- Decline in performance\n- Loss of muscle strength\n- Weight loss and loss of appetite\n- Prolonged recovery from training sessions, which can include tenderness and soreness in muscles and joints\n- Sleep disturbances\n- Gastrointestinal disturbances\nOveruse injuries often result from repeated, abnormal stress applied to a muscle, tendon, ligament or bone—by doing too much, doing too much too soon, or not taking enough time to recover and recuperate (Baechle and Earle, 2008). They can also occur as a result of training errors, faulty technique, decreased flexibility or insufficient strength. Find out if you're overtraining or underrecovering.\nHow to Prevent Overtraining\nAre you guilty of overtraining or overreaching? Start incorporating some of these ideas into your training.\n- Design a good training program that incorporates sound exercise principles, including rest days\n- Design a program that is appropriate for your level of conditioning\n- Use the principles of cross training (variety of activity)\n- Use the principles of interval training (variety of intensities)\n- Learn to control your stress in daily life so your body can recover from exercise sessions\n- Get enough sleep to allow your mind and body to recover from workouts\n- Get a massage periodically\n- Use self-massage tools after a workout or on rest days—e.g., massage stick, foam roll, or small massage balls\n- Use a steam bath, sauna, or whirlpool, as needed\n- Eat a balanced healthy diet so that you replenish fluids and nutrients needed for recovery\n- Take a vacation several times a year to allow your mind and body to recharge; an often overlooked area in training is how your daily life impacts recovery from exercise, workouts, and sports competition\nYou can also prevent overtraining and overreaching by tweaking aspects of your personal life, such as:\n- Cutting back on smart phone time, including texting, web surfing, checking emails, and talking, to give your brain a break\n- Driving slower in your community so that your body is not on overdrive (not to mention that it's safer for you and other people on the road)\n- Keeping your personal finances simple and in order\n- Enjoying time with family and friends\n- Avoiding being a constant weight watcher. Don't micromanage every second of your life\n- Learn to complain less throughout your day\nAmerican College of Sports Medicine (ACSM). ACSM's Resource Manual for Guidelines for Exercise Testing and Prescription, 6th ed. Philadelphia; Wolters Kluwer Lippincott Williams & Wilkins, 2010.\nBaechle TR, Earle RW, eds. Essentials of Strength Training and Conditioning, 3rd ed. Champaign, Ill.: Human Kinetics, 2008.\nKreider RB, Fry AC, O'Toole ML, eds. Overtraining in Sport. Champaign, Ill.: Human Kinetics, 1998.\nRichardson SO, Andersen MB, Morris T. Overtraining Athletes: Personal Journeys in Sport. Champaign, Ill.: Human Kinetics, 2008.\nVerkhoshansky Y, Siff M. Supertraining, 6th ed. Rome, Italy: Verkhoshansky, 2009. www.verkhoshansky.com and www.melsiff.com.""]"	['<urn:uuid:a4e662b0-a2a2-4acf-95bb-3ecf25c0fce1>', '<urn:uuid:eece625b-c80a-4ab3-9588-c1b3ad4c937a>']	open-ended	with-premise	verbose-and-natural	similar-to-document	multi-aspect	expert	2025-05-12T10:19:05.071190	31	101	2075
90	What are the capabilities of chemical metering systems for water treatment redundancy, and how do these systems help address the various types of water quality threats that cause premature deaths?	Chemical metering systems offer redundancy through Duplex and Triplex configurations, providing backup pumps for continuous operation during maintenance and the ability to increase chemical flow when needed. The Duplex system features two pumps for backup, while the Triplex system allows for multiple chemistries and lead/lag pumping. These systems help address water quality threats that cause 3.4 million premature deaths annually worldwide (including 1.9 million from diarrhea) by treating various pollutants through processes like disinfection, chemical treatment, and pH neutralization.	"['📞 +1 877-742-2878\nChemical Feed Systems are designed for automated chemical injection into the chemical feed tank for wastewater treatment. In the dosing process, chemical reagents are added into a water stream at a precisely controlled rate with positive displacement metering pumps. Burt Process Chemical Metering systems have revolutionized the concept of chemical dosing by providing pre-engineered and fully customizable pieces of equipment for water treatment. Our ChemPlus Chemical Dosing System is a pre-engineered solution that can be effectively used in many different industries and in a municipal water treatment process. These are high-performance sodium hypochlorite dosing systems — for example when used for disinfecting potable water or as part of a chemical scrubber system. A compact and lightweight prepackaged skid, corrosion resistance, flexibility of metering pump option and easy maintenance are some of the advantages of our advanced chemical feed systems. Water treatment uses include disinfection, fluoride treatment, sodium bisulfite metering, sulfuric acid dosing, sodium hypochlorite systems, dispersant polymer dosing, sodium hydroxide feed, potassium permanganate dosing, coagulation & flocculation, hydrochloric acid metering, alum feed, ph neutralization, and antiscalant addition. Learn more about Chemical Feed Basics.\nThe ChemPlus Simplex System has a single metering pump, inlet, and discharge. This compact, reliable and economically-priced system meters a single chemical into a tank or process line. The Simplex chemical metering system is a prepackaged skid that offers many metering pump options, including any type of diaphragm, air-operated double diaphragm, peristaltic, electronic metering or progressing cavity pumps. Various options are available, including a pulsation dampener, pressure relief valve, visual flow indicator, and pump controls.\nThe ChemPlus Chemical Feed Systems Duplex provides two chemical metering pumps that are connected to a common or dual independent inlet and discharge. This duplex system is useful if you need to meter two different chemicals into a single tank or line, or to move the same chemical into two separate processes. The ChemPlus Duplex is also used as a redundant pump system in processes that require a continual back-up pump. This is useful in cases where a process can’t be shut down to perform routine maintenance. In addition to the options available with the simplex system, the duplex can be ordered with flow and pressure sensors, an MA DEP control station, a conduit box for power and signals and other useful components.\nBurt Process Equipment’s Triplex Chemical Feed System has three chemical metering pumps that can be connected either to a common or to multiple, independent inlet and discharge assemblies. The Triplex feed system provides the ideal set-up for multiple chemistries that must meter into the same process, and it can also be used for redundant pump systems or lead/lag pumping. Using this system, you can double up on the pumps feeding the same tank or line to increase chemical flow intermittently, if required. Create a custom system to fit your application and requirements by choosing from many available options.\nWith 45 years of chemical processing experience, Burt Process Equipment offers chemical dosing systems that are fully engineered, pre-packaged, corrosion-resistant and highly customizable. This includes the ability to use the pump technology that best fits the intended usage. The ChemPlus line of chemical feed systems lets engineers and process specialists design the ideal chemical feed system for their chemical metering or water treatment application. View each type of system for selectable components, system arrangements, additional options, product brochure, engineering drawings and more.\nBurt Process Equipment ChemPlus pre-engineered systems offer a wide range of components options, pump selection and piping arrangements needed to customize chemical injection solutions.\nSimplex w/ selectable components\nDuplex Arrangements include:\nTriplex Arrangements include:\nDocumented Factory Testing Includes:', 'Presentation on theme: ""Any chemical, biological, or physical change in water quality that has a harmful effect on living organisms or makes water unsuitable for desired usage.""— Presentation transcript:\nAny chemical, biological, or physical change in water quality that has a harmful effect on living organisms or makes water unsuitable for desired usage.\nWHO: * 3.4 million premature deaths each year from waterborne diseases * 1.9 million from diarrhea * U.S. 1.5 million illnesses * 1993 Milwaukee 370,000 sick\nInfectious Agents: - bacteria and viruses often from animal wastes Oxygen Demanding Wastes: - organic waste that needs oxygen often from animal waste, paper mills and food processing. Inorganic Chemicals: - Acids and toxic chemicals often from runoff, industries and household cleaners Organic Chemicals: - oil, gasoline, plastics, detergents often from surface runoff, industries and cleaners Plant Nutrients: - water soluble nitrates, ammonia and phosphates often from sewage, agriculture and urban fertilizers Sediment: - soils and silts from land erosion can disrupt photosynthesis, destroy spawning grounds, clog rivers and streams Heat Pollution and Radioativity: mostly from power plants\nPoint sources Nonpoint sources Water quality\nNONPOINT SOURCES Urban streets Suburban development Wastewater treatment plant Rural homes Cropland Factory Animal feedlot POINT SOURCES Fig. 22-4 p. 494\nPoint Source Pollution: - There is one major source of the pollution and it can be identified. - Examples: Pipe coming out of a factory directly into a river. Nonpoint Source Pollution: - There can be many sources for a body of water being polluted. - Example: A river being polluted due to urban runoff.\nAgriculture:(A.K.A: Farms) * Sediment: Heavy rains cause soil erosion. * Fertilizers and Herbicides: Farmers use these on their crops for bug and weed control. They runoff during rain. * Bacteria from livestock: Animals use the land as their bathroom. Their feces contains nitrates which pollute river during rain runoff. * Salt from soil irrigation\nIndustrial: * Clearing of land for businesses to be built can cause soil erosion. * Waste a sewage dumping by factories. * Big power plants use rivers, streams, and lakes to dispose of waste heat. * Fort Meyers, Florida Manatee Park * http://www.leeparks.org/panoramas/p anoramas-parks-i-n.html#manatee http://www.leeparks.org/panoramas/p anoramas-parks-i-n.html#manatee * Factories dump toxic or radioactive materials. * Burning fuels causes “acid rain”.\nHome: * Sewage and septic leak in water source. * Fertilizers, herbicides, pesticides used for lawn maintenance. * Putting hazardous chemicals down the drain. * Oil and antifreeze leak from cars onto the driveway.\nDeveloped Countries U.S. and other developed countries sharply reduced point sources even with population and economic growth * Nonpoint still a problem * Toxic chemicals still problem * Success Cuyahoga River, Thames River\nDeveloping Countries: Serious and growing problem * Half of world’s 500 major rivers heavily polluted * Sewage treatment minimal $$$ * Law enforcement difficult * 10% of sewage in China treated * Economic growth with little $$$ to clean up\n* Holy River (1 million take daily holy dip) * 350 million (1/3 rd of pop) live in watershed * Little sewage treatment * Used for bathing, drinking etc. * Bodies (cremated or not) thrown in river * Good news is the Indian government is beginning to work on problem\n* How successful has the U.S. been at reducing water pollution? Clean Water Act * What law governs water pollution in the United States?\nMost developed countries use laws to set water pollution standards. Federal Water Pollution Control Act (Clean Water Act 1972, ’77, ’87) * Regulates navigable waterways..streams, wetlands, rivers, lake\n* Sets standards for key pollutants * Requires permits for discharge * Requires sewage treatment * Require permits for wetland destruction * Does not deal with nonpoint sources well * Goal All Waterways fishable and swimable\n* Between 1972 – 2002 fishable and swimmable streams 36% to 60% * 74% served by sewage treatment * Wetlands loss dropped by 80% * Topsoil losses dropped by 1 billion tons annually\n* 45% of Lakes, 40% streams still not fishable and swimmable * Nonpoint sources still huge problem * Livestock and Ag. Runoff * Fish with toxins\n* How is waste water cleaned? * How is drinking water purified? High tech way. * How can we purify drinking water in developing nations? * Is bottled water a good answer or an expensive rip-off? * How do sewage treatment plants work?\nSeptic Systems Fig. 22-15 p. 510 ¼ of all U.S. homes have Septic tanks Septic tanks are used primarily outside city limits. How does it work? The Septic Tank — A septic tank\'s purpose is to separate solids from the wastewater, store and partially decompose as much solid material as possible, while allowing the liquid (or effluent) to go to the drainfield....more The Drainfield — After solids settle in the septic tank, the liquid wastewater (or effluent) is discharged to the drainfield, also known as an absorption or leach field....more The Soil — The soil below the drainfield provides the final treatment and disposal of the septic tank effluent. After the wastewater has passed into the soil, organisms in the soil treat the effluent before it percolates downward and outward, eventually entering ground or surface water. The type of soil also impacts the effectiveness of the drainfield; for instance, clay soils may be too tight to allow much wastewater to pass through and gravelly soil may be too coarse to provide much treatment.']"	['<urn:uuid:3270c67a-a678-4a37-a9de-199b825a1ebc>', '<urn:uuid:0cda344a-b301-4d07-87f9-3c1466f55e0b>']	factoid	direct	verbose-and-natural	similar-to-document	multi-aspect	expert	2025-05-12T10:19:05.071190	30	79	1494
91	How does this Indigenous-owned business in India ensure that their honey collection doesn't harm the environment?	Aadhimalai employs a team of local people trained as 'barefoot ecologists' who regularly record and monitor harvesting practices. These ecologists maintain sustainable harvesting with a holistic economic model. Specifically for honey collection, they ensure that harvesting never exceeds 40 percent of the resource to keep bee colonies stable. Additionally, they maintain forest sustainability by incorporating local nurseries to grow native species and trees for replanting.	['Making alternative Indigenous food markets mainstream\nBy Aeden Keffelew\n“There’s lots and lots of wisdom, lots of ways of owning small things but thinking very big,” says Pratim Roy. “We are taught this is your land, this is your apartment, that’s your car. For them, this entire area is theirs. How do they have that kind of thing? There’s lots to learn from them. That’s their mainstream, what we call alternative.”\nRoy is the founder of Keystone Foundation, and “them”, that’s the members of Aadhimalai Pazhangudiyinar Producer Company Limited, an Indigenous-owned producer collective located in the Nilgiri Biosphere Reserve (NBR) in the Western Ghats of southern India. As climate-induced challenges with food production become a growing concern across the globe, this producer collective of Indigenous groups has proven that locally-led management of productive ecosystems can offer long-lasting and profitable solutions.\nAadhimalai Pazhangudiyinar Producer Company Limited was incubated by Keystone Foundation in 2013. Keystone works with 147 Indigenous communities in the 5,500 square kilometre Nilgiri Biosphere Reserve to find environmental and economic solutions based on traditional Indigenous practices.\nAadhimalai, which began as a micro-social enterprise and has since grown into a tribal producer company that uses organic production and sustainable harvests of local crops, has been awarded the United Nations Development Programme’s Equator Prize 2021 along with nine other Indigenous peoples and local communities. The Equator Prize recognizes effective and scalable nature-based solutions to our world’s most pressing environmental, food and economic dilemmas.\nAs an Indigenous-owned production collective, Aadhimalai uses traditional collection methods of non-timber forest products (NTFPs), such as honey, amla, shikakai, soap nuts and berries, from the protected forests to sustain the livelihoods and food production of communities in and around the reserve, while also protecting the forest and native species.\n“Aadhimalai is a company for the tribal people here,” says Vanaja, a producer at Bangalapadi Aadhimalai Centre. “We’re all joint owners. Aadhimalai is ours.”\nThe Aadhimalai Producer Company serves as the decentralized production company for these products in different parts of the reserve. It guarantees its members a fair market price for their products. The company combats previously widespread, unfair practices such as low market prices, incorrect weighing of products, and monopolies from trading retailers outside of the reserve area.\nThese market issues, which negatively affected Indigenous peoples’ food and economic security, were exacerbated in the past due to a lack of clear protection zones in the Nilgiri Biosphere Reserve.\nThe Nilgiri Biosphere Reserve, where the Indigenous peoples of the Aadhimalai collective have always lived, was established in 1986 by the Government of India and UNESCO’s Man and Biosphere Programme (MAB). The reserve is a haven of biodiversity with rainforests, wetlands and other ecosystems, home to various species of fauna, such as the Asian elephant and Bengal tiger.\nUnder the MAB programme, each reserve is split into three zones: core areas for strictly protected biospheres, buffer zones for ecologically sound monitoring, research and training, and transition areas where communities can conduct sustainable economic activities.\nThe Nilgiri Biosphere lacks delineated transition areas for Indigenous groups to develop systems for economic and environmentally sound growth. As a result, buffer zones have been used by various developers for hydroelectric power projects, agriculture, horticulture and tourism activities. This has led to a negative impact on the reserve’s ecology.\nAadhimalai has addressed this issue by making use of the manipulation zones within the buffer zones to create sustainable livelihoods for over 1,600 Indigenous members of the collective through the production and selling of NTFPs harvested from the forest.\nAadhimalai sells coffee, pepper, silk cotton, millets, pulses, cereals, spices and fruits to both Indigenous families and neighbouring communities, ensuring sustainable livelihoods for its members. The company pays them prices that are 30 percent higher than the market rate.\nA team of local people trained as “barefoot ecologists” regularly record and monitor harvesting and agricultural practices. The ecologists maintain sustainable harvesting of these crops with a holistic economic model. They also ensure that honey collection never exceeds 40 percent of the resource, which keeps bee colonies stable. They replant the forest by incorporating local nurseries to grow native species and trees.\nVarious production centres throughout the collective produce different goods from non-timber forest products (NTFPs) that are then sold to Indigenous groups within the region. Many of these production centres are women-led.\n“Only women work in our production centres,” says Sumithra. “However, both men and women do the NTFP collection.”\nOf Aadhimalai’s 50 employees, 42 are women and of the 7 directors of the company, 3 are women. Women manage the production units, travel to other organizations to give training and represent their community in local and national forums.\nSetting the price of products helps the Indigenous communities of the region keep economic and agricultural independence. Their traditional practices also help tribal communities weather the volatility of climate change in agricultural industries.\nAadhimalai has created a sustainable agro-economy for many Indigenous groups within the Nilgiri Biosphere Reserve. They have trained more than 25 organizations across the tribal areas of India in organic agriculture, sustainable honey harvest, wild fruit collection and hygienic handling of produce.\n“Our organization is the community itself,” says Jestin Pauls, CEO of Aadhimalai. “Enterprising is necessary.” He thinks that government support can help scale up their successful model of Indigenous entrepreneurship.\nAs nations and communities strategize ways to rehabilitate and sustainably use land, Aadhimalai offers an economically and environmentally sound solution. These practices help communities facing food insecurity as climate change begins to create extreme and unpredictable weather patterns.\nAadhimalai offers a sustainable economic model that supports local communities while incorporating and upholding Indigenous knowledge. Indigenous-led training empowers both teachers and learners, and demonstrates the value of locally-led action through replication in different locations.\nWith this work, Aadhimalai and similar groups around the world are working towards bringing this “alternative” approach into the mainstream. 2021 Equator Prize winners, Tropical Forest — Rural Development of Cameroon and BIO-KG Federation of Organic Development of Kyrgyzstan have also scaled up sustainable and traditional farming models, some of which have been used to shape local government food policies.\nMembers of the Aadhimalai collective say that “gaining rights over resources is our larger goal”. Land sovereignty offers them continual rights to protect and sustain the forest for generations to come.\n“Winning the Equator Prize will go a long way in promoting the cause of environmentally friendly social enterprises,” Aadhimalai representatives said in their acceptance remarks. “This award motivates us and gives a voice to the voiceless.”']	['<urn:uuid:f1a6802e-98a4-4e5b-a30d-52a03883f3de>']	open-ended	direct	verbose-and-natural	distant-from-document	single-doc	novice	2025-05-12T10:19:05.071190	16	65	1079
92	In what ways do the chemical and structural characteristics of different types of industrial sands influence their effectiveness in manufacturing processes and construction applications?	The chemical and structural characteristics significantly impact industrial sand applications. Silica sand's chemical composition of SiO2 and its crystalline structure provide high hardness and chemical purity, making it crucial for industrial applications like glassmaking and metal casting. For construction purposes, manufactured sand (M-sand) relies more on physical properties, being processed for specific particle shapes and sizes under 4.75mm. The properties of these materials can be classified into physical (mechanical, thermal, electrical) and chemical (environmental and chemical stability) characteristics. In applications like paint and coatings, silica sand's chemical properties contribute to color consistency and durability, while in construction, its physical structure affects binding system properties.	"['M-Sand (Manufactured Sand) :-. M-sand is manufactured. M Sand is an alternative to River sand. M Sand (manufactured sand) produced by crushing of hard granite stone. MSand is manufactured through the process of shaping cubically, grading and cleaning by using VSI machine. The size of Manufactured sand is less than 4.75mm.\nThe properties of engineering materials can be classified into the following main groups: physical and chemical. The physical properties can also be further grouped into categories: mechanical, thermal, electrical, magnetic, optical etc. The chemical properties include: environmental and chemical stability.\nSand Samples\' Preparation Using Mobile Pluviator. Authors; Authors and affiliations; Mahdy Khari ... Sand sample preparation—the slurry deposition method. Soils Found. 28(4), 107–118 ... A sample preparation method and its effect on static and cyclic deformation-strength properties of sand. Soils Found. 22(1), 61–77 (1982) Google ...\nUser Guidelines for Waste and Byproduct Materials in Pavement Construction [ Asphalt Concrete] [ Flowable Fill] ... Chemical Properties. Spent foundry sand consists primarily of silica sand, coated with a thin film of burnt carbon, residual binder (bentonite, sea coal, resins) and dust. ...\nTitanium processing, the extraction of titanium from its ores and the preparation of titanium alloys or compounds for use in various products. Titanium (Ti) is a soft, ductile, silvery gray metal with a melting point of 1,675 °C (3,047 °F).\npreparation of manufactured sand and its properties, process .... preparation of manufactured sand and its properties 38 Views. The Zenith is the professional mining equipments manufacturer in the world, located in China,India ...\nSULPHUR CONCRETE\'S TECHNOLOGY AND ITS APPLICATION TO THE BUILDING INDUSTRY ... – sand, C – additives, D – sulphur, 1 – compounds storehouse, 2, 3, 4 ... Process of sulphur concrete production is based on the sulphur\'s properties of changing its .\nSilica sand products are marketed in a wide range of grades, including extremely fine grades known as flours. The chemical compound silicon dioxide, also known as silica (from the Latin silex), is an oxide of silicon with a chemical formula of SiO2 and has been known for its hardness since antiquity.\nUses and properties. Titanium metal connects well with bone, so it has found surgical applications such as in joint replacements (especially hip joints) and tooth implants. The largest use of titanium is in the form of titanium (IV) oxide. It is extensively used as a pigment in house paint, artists\' paint, plastics, enamels and paper.\nIn fact, sand casting is one of the few processes that can be used for metals with high melting temperatures such as steels, nickel, and titanium. Usually sand used to manufacture a mold for the casting process is held together by a mixture of water and clay. A typical mixture by volume could be 89% sand, 4% water, 7% clay.\nDEPARTMENT OF HOUSING AND URBAN DEVELOPMENT Series and Series Number: Housing - Federal Housing Commissioner MATERIALS RELEASE NO: ... or sand for hot mop or Cold Adhesive applications. ... When requested, provide the FHA Standards, Office of Manufactured Housing Programs, HUD Headquarters, with a representative list of properties, in which the ...\nSalt manufacture. Commercial salt is manufactured from rock salt, as well as from seawater and other natural and artificial brines. Most of the artificial brines are obtained by pumping water into underground salt beds. A considerable amount of brine itself is used directly in industrial countries.\nGUIDE TO FOUNDATION AND SUPPORT SYSTEMS FOR MANUFACTURED HOMES Excellence in Design, Manufacturing and Installation Series ... GUIDE TO FOUNDATION AND SUPPORT SYSTEMS FOR MANUFACTURED HOMES. Excellence in Design, Manufacturing and Installation Series . .\nas fine aggregate. The proportion of lateritic sand was varied from 0% to against quarry dust at intervals of 25%, using concrete mix of 1:1.5:3 and water/cement ratio of 0.65. Concrete samples were prepared, cured for 28 days, and tested in the laboratory to destruction in order to determine their flexural and tensile strength properties.\nGlass is a non-crystalline, amorphous solid that is often transparent and has widespread practical, technological, and decorative usage in, for example, window panes, tableware, and optoelectronics.The most familiar, and historically the oldest, types of manufactured glass are ""silicate glasses"" based on the chemical compound silica (silicon dioxide, or quartz), the primary constituent of sand.\nContents - Previous - Next. The flour produced from the cassava plant, which on account of its low content of noncarbohydrate constituents might well be called a starch, is known in world trade as tapioca flour. It is used directly, made into a group of baked or gelatinized products or manufactured into glucose, dextrins and other products.\nFortunately, there are easier and less extreme ways of making glass—but all of them need immense amounts of heat. In a commercial glass plant, sand is mixed with waste glass (from recycling collections), soda ash (sodium carbonate), and limestone (calcium carbonate) and heated in a furnace.\nSand is used to provide bulk, strength, and other properties to construction materials like asphalt and concrete. It is also used as a decorative material in landscaping. Specific types of sand are used in the manufacture of glass and as a molding material for metal casting. Other sand is used as an abrasive in sandblasting and to make sandpaper.\nBecause of its small size and sheet-like structure, clay has a large amount of surface area per unit mass, and its surface charge attracts ions and water. Because of this, clay is the ""active"" portion of the soil matrix. For all mineral soils, the proportion of sand, silt, and clay always adds up to 100 percent.\nPolyethylene Maufacturing and its Properties. HDPE is the strongest four times that of low density polyethylene, toughest, most chemical resistant, and least flexible of these four types of polyethylene.HDPE is used in products and packaging such as milk jugs, detergent bottles, margarine tubs, garbage containers and water pipes.\nCoffee, tea and other beverages made with water will contain some amount of silicon dioxide. Beer is an excellent source of the mineral in addition to being a tasty drink. The silicon dioxide in beer is in the form of orthosilicic acid, which is important for bone health and to prevent osteoporosis.\nsand by manufactured sand with 0%, 50% and on hardened properties of cement mortar. The experimental work includes the casting, curing and testing of specimens.\nPYrogeniC siliCa. Chemical Structure and Properties Pyrogenic silica consists of SiO 4/2 tetra- hedra, each of which is linked to adjacent tetrahedra by means of a common oxygen atom. Pyrogenic silica is pro - duced at temperatures over 1,000 °C by introducing volatile chlorosilane into an oxyhydrogen flame.\nSand & Gravel (~900,000,000 tons per year) Crushed stone (~1,200,000,000 tons per ... other properties dependent on parent rock ... Aggregates in Concrete Concrete Technology 10 Rocks are classified according to origin into three major groups: 1.\n2.9 Sand—Sand consists of fine aggregate particles that are retained on the No. 200 (75 μm) sieve, either as natural sand resulting from natural disintegration and abrasion of rock, or as manufactured sand, which is produced by the crushing of rock, gravel, slag, etc.\nQuartz is a chemical compound consisting of one part silicon and two parts oxygen. It is silicon dioxide (SiO2). It is the most abundant mineral found at Earth\'s surface, and its unique properties make it one of the most useful natural substances. Rock crystal quartz: Transparent ""rock crystal"" quartz.\npreparation of manufactured sand and its properties 4.9 - 4364 Ratings ] The Gulin product line, consisting of more than 30 machines, sets the standard for our industry.\nWood is the oldest material used by humans for construction after stone. Despite its complex chemical nature, wood has excellent properties which lend themselves to human use.\nSUSTAINABLE SAND MINING MANAGEMENT GUIDELINE SEPTEMBER 2015 . Page 2 of 54 S USTAINABLE SAND MINING ... (Manufactured Sand) & construction ... causing loss of properties and degradation of landscape, it can also undermine bridge supports, pipe lines or other structures. ...\nManufactured Sand (M Sand) for Concrete Properties and ... The crushed sand is of cubical shape with grounded edges, washed and graded to as a construction material. The size of manufactured sand (M Sand) is less...', 'Silica Sand is actually a granular material. It contains quartz and minute amounts of coal, clay. Sometimes it has minerals in it. People call it quartz sand too. Most importantly it is the industrial sand. Quartz over time turns into Silica sand gradually because of the influence of water and wind.\nYou have to use it when gradually it breaks into tiny granules. Silica sand or industrial sand is actually a type of high purity quality Silica Sand products. But it is actually sized controlling the material. It is not like common concrete and asphalt gravels.\nSources of Silica Sand\nThe chemical sign of Silica is SiO2. It is actually the group of minerals which is a mixture of Silicon and Oxygen. It is often got in the crystalline state. Because of weathering, plankton fossilization happens and that results in crystallizing SiO2.\nThese are found as mines in most regions of the world. It is got in high hardness. Good Silica Sand Crystal depends more on the hardness ratio, silica ratio and chemical structure of mines. Silica sand is mostly used as wet and dry in the industry. The sources of Silica sand have a wide variety. These are yellow, beige and white in color.\nUses of Silica Sand\nYou can use Silica Sand in many sectors and that too in a very indescribable way. It is usually used as a raw material in most of the construction industry. It can be used both as dry and wet. Sometimes it is used as filtration sand, molding sand and in glass production. If you think about industrial and manufacturing applications then deposits of silica yielding products of at least 95% SiO2 is preferred. Silica has a high melting point. It is hard. SiO2 is used for industrial sand’s strength. In every industrial sector, silica sand is a very important element.\nFlowings are the major fields where we see uses of silica sand:\n- For Glassmaking\n- Metal Casting\n- Metal Production\n- Paint and Coating\n- Filtration and Water Production\n- Oil and Gas Recovery\nBrief descriptions of these uses are given below:\nIn glassmaking, the primary component is silica sand. Silica sand is used for all types of glass whatever the standard and specialty are. Every glass formulation needs its basic component. Silica sand has clear and strong chemical purity. Glass making cannot be possible without the help of silica sand. It is the primary determinant of color. Industrial Sand is used for producing flat glass for automotive and building use. In Foods, beverages and tableware containers production, it is also used. Silica Sand is needed for the production of fiberglass insulation. It reinforces glass fibers. It is also used in test tubes and other scientific tools.\nFor the ferrous and nonferrous foundry industry, industrial sand is the most important part. the clay is molded to achieve the external shape. Metal parts are cast into the sand. Core sands can be mechanically recycled to produce new cores or molds. Metal parts need SiO2 for their strength.\nAll those soap and dye productions are made from Silica Sand. The main ingredient of chemicals like Sodium silicate, silicon tetrachloride, silicon gels is silicon sand. These chemicals are used to produce household and industrial cleaners.\nFor all those wide varieties of building and construction products, Silica Sand is the primary component for it. Ground Silica works as a functional extender to add durability and anti-corrosion. For epoxy-based compounds, sealants and caulks it works like magic. In flooring compounds, mortar, cement, stucco, roofing shingles, and asphalt mixtures, whole grain silica sand is used. It affects the chemical properties of the binding system.\nPaint and Coating\nTo improve the appearance and durability of any kind of architectural and industrial paint and coatings, Silica Sand is used. Paint formulators always select micron-sized industrial sands. High purity silica is able to contribute critical performances in properties like brightness and reflectance, color consistency and oil absorption. It is also used in architectural paints. Silica fillers are able to improve tint retention, durability, and resistance to dirt, mildew, cracking and weathering. Low oil absorption always allows increased pigment loading and improves finish color.\nFiltration and Water Production\nFor the filtration of drinking water, the processing of wastewater, the production of water from wells, Silica Sand is used. Silica gravel is something that is used as packing material in deep water wells. Grain size distributions are also very necessary for efficient filtration bed operations.\nOil and Gas Recovery\nSilica Sand is known as Frac Sand in the oil industry. Silica’s hardness is important here. The chemical purity in Frac Sand is able to attack in the corrosive environment. Whole grains deposits of Frac sand is able to attack in the corrosive environment. Whole grains deposit of Frac Sand is able to maximize formation cuttings.\n- What is sand? Composition, Color, and Types of Sand\n- Uses of sand\n- What is Sandy Soil?- Uses of Sandy Soil\n- How to Improve Sandy Soil\n- What is the Bulking of Sand? - Significance & Test Steps\n- What is Silica Sand? Sources & Uses of Silica Sand\n- Bulk Density of Sand\n- Specific Gravity of Sand\n- Sand Unit Weight']"	['<urn:uuid:5f1b89ac-024f-4f1d-af24-b78af79ee079>', '<urn:uuid:a9578e8b-2feb-4911-86ab-3f21c0c357b1>']	open-ended	direct	verbose-and-natural	distant-from-document	three-doc	expert	2025-05-12T10:19:05.071190	24	104	2228
93	How are e-commerce platforms expanding their cross-border services, and what environmental challenges do these international operations face in the context of increasing internet usage?	E-commerce platforms are expanding cross-border services significantly, with Cainiao becoming the official provider for Pakistan's Daraz platform, launching 13 weekly air cargo trips from China to Pakistan and 8 flights to Bangladesh, serving over 1,800 merchants and promising to cut logistics costs by 10% while improving delivery efficiency by 20-25%. However, this expansion of international e-commerce operations faces environmental challenges due to increasing internet usage, as highlighted by statistics showing that online data use rose nearly 40% during the pandemic period, and internet-related activities are becoming a major source of greenhouse gas emissions, predicted to exceed the aviation industry's environmental impact by 2025.	['Alibaba News Roundup: Alibaba Cloud Data Center JV In Saudi Arabia; Intime Rewards For Recycling; Cainiao & Daraz Tie-Up In Pakistan\nThis week, Alibaba Group’s cloud computing arm joined forces with a Saudi Arabian telecoms firm to launch a data center in the country. In other news, Alibaba-owned department store chain Intime announced a program to reward customers who recycle their second-hand clothes. Last but certainly not least, logistics provider Cainiao Network became the official […]\nThis week, Alibaba Group’s cloud computing arm joined forces with a Saudi Arabian telecoms firm to launch a data center in the country.\nIn other news, Alibaba-owned department store chain Intime announced a program to reward customers who recycle their second-hand clothes.\nLast but certainly not least, logistics provider Cainiao Network became the official provider of cross-border delivery services for Pakistan’s largest e-commerce platform Daraz.\nAlibaba Cloud-backed Joint Venture Launches Data Center in Saudi Arabia\nAlibaba Cloud joined forces with Saudi Telecom Company Group and other local partners to launch cloud services in Saudi Arabia this week.\nThe joint venture, known as General Cloud Computing Company for Information Technology, opened two data centers in the capital city of Riyadh. The data centers will provide a wide range of cloud computing services, including storage, network, and database, to meet the country’s increasing demand for digital transformation.\nThe JV “is a significant milestone towards our commitment to forging collaborations with local partners to boost the digital transformation in the Kingdom of Saudi Arabia,” said Selina Yuan, Alibaba Group VP & Alibaba Cloud Intelligence International President, in a statement.\nIntime Rewards Second-hand Clothes Recycling\nOut with the new, in with the old! Brick-and-mortar retailer Intime has joined forces with 16 brands in China to encourage customers to go green by recycling their clothing.\nDedicated recycling boxes at Intime locations will be available for shoppers to give their used clothing a second life while also earning green points on Alipay that can be redeemed through the app’s tree-planting initiative.\nIntime also offers coupons to reward customers who support a low-carbon lifestyle.\nSo far, roughly 60 recycling stations are up and running around the country, predominantly in Zhejiang province, and these locations have taken in goods from over 500 people.\nSome donations will go to less developed areas in China; the remainder will be turned into new items.\nCainiao to Provide Logistics Services for Daraz for Cross-Border Deliveries\nOver 1,800 merchants on Daraz can access cross-border logistics services provided by Cainiao, backed up by last-mile delivery services from local carriers.\nCainiao will launch 13 air cargo trips per week from China to Pakistan and eight flights from China to Bangladesh, which will cut logistics costs by 10% and drive a 20-25% rise in delivery efficiency, Cainiao estimated.\n“Together with the Daraz team, we look forward to enabling the e-commerce growth in South Asia and empowering local consumers with more e-commerce options,” said Shawn Louis, General Manager, APAC, at Cainiao Global Export.\nDaraz merchants who also sell on other Alibaba e-commerce platforms will be able to consolidate all their parcels for shipping through Cainiao. Consumers and merchants can track packages’ shipping status in real-time with boxes affixed with the Cainiao electronic shipping labels.\nABH Entrepreneur Competition Extends Deadline\nStart-up competition Africa’s Business Heroes will be accepting applications from across the African continent until June 20, organizers said this week to give entrants more time to complete submissions and due to the high volume of applications.\nNow in its third year, the event puts entrepreneurs and company founders in front of judging panels to win mentorship, training and a share of a $1.5 million grant.\n“It is heartening to have seen a tremendous response from entrepreneurs across Africa this year. We aim to have all the countries and regions of Africa well-represented,” Zahra Baitie-Boateng, partnerships and programs head at ABH, said in a statement.\nSo far, competition organizers have received applications from people in every African state, with women making up more than 30% of entries.\nTo apply, follow the link here\nDomestic Tourism Rebounds During Dragon Boat Festival\nChinese tourists reconnected with their home country during June’s three-day Dragon Boat Festival holiday last week, sales on Alibaba’s travel platform Fliggy show.\nCross-province travel bookings increased 82% compared with the Labor holiday from May 3 to May 5, while train ticket purchases jumped 70%.\nHangzhou to the south and Chengdu, the capital of Sichuan province, were the most popular destinations, Fliggy said in a statement. Bookings at four or five-star hotels in the former rose 300% over the holiday.\nDespite only recently coming out of months of pandemic-related lockdowns, Shanghai also saw an uptick in tourism at local sites such as the Pearl TV Tower and the city zoo.\nRetailer Reboot: Alibaba Lends Merchants Helping Hand Ahead of 6.18\nAlibaba is helping merchants across its e-commerce platform shake off the post-lockdown cobwebs for the 6.18 Mid-Year Shopping Festival.\n“Ensuring stability of supply chain and logistics amid the uncertainties is the best way to improve consumption sentiment and foster a more positive environment for business operations,” said Alibaba Group Chairman and CEO Daniel Zhang during a quarterly earnings call in May.\nLeveraging its entire ecosystem, Alibaba lends a hand to ensure parcels arrive on time for customers and merchants can pay their suppliers earlier. The group also rolled out courses and tools to help retailers resolve consumers’ Covid-related complaints as shopping gets underway.\nRead more here to see how three retailers are preparing for 6.18 sales with Alibaba’s help\nAlibaba’s Cainiao Delivers Change Ahead of World Environment Day\nAlibaba’s logistics platform Cainiao Network issued its inaugural social responsibility report last week to engage consumers and business partners in sustainable practices for the benefit of people and the planet alike.\nThe delivery service has developed an end-to-end green logistics system from purchase to delivery, including paperless shipping labels, smart route planning, recycling, and solar-powered warehouses.\n“Cainiao provided innovative solutions for the enterprises and consumers it serves…, which actively contributed to China and the whole world’s carbon goals,” said Bin Han, Chinese secretary-general of a United Nations’ corporate sustainability initiative called Global Compact, on May 26.\nThe alternative is grim: If nothing changes, the domestic logistics industry will deliver 150 billion parcels in 2025, which will make up 1.1% of China’s total emissions, per CarbonStop estimates.\nLearn how Cainiao is making positive change here\nLazada Appoints James Dong as CEO\nAlibaba Group’s Southeast Asian e-commerce platform Lazada named James Dong as chief executive officer last week.\n“[James is] a well-liked leader among locals, passionately sharing his knowledge to cultivate a strong local bench force,” a Lazada spokesperson said in a statement.\nDong will be based at the company’s recently launched regional headquarters in Singapore called Lazada One.\nOutgoing head Chun Li, who helmed the online shopping company from 2020, will continue to serve as a board member and advisor to Lazada Group Chairman Jiang Fan.\nRead more here', 'The Canadian Internet Registration Authority (CIRA) has unveiled an environmental sustainability strategy aimed at reducing its operational impact and promoting a greener internet in Canada.\nCIRA is a national not-for-profit which manages .ca domains on behalf of Canadians. The organization also offers registry, Domain Name System (DNS) and cybersecurity services to organizations across Canada and around the world.\nAs SustainableBiz previously reported, when workers started video streaming and using online meeting platforms to perform their daily duties during the pandemic, online data use rose nearly 40 per cent in the U.S. from January to March 2020.\nIn addition to telework, seven per cent of Canadians reported using the Internet more frequently to earn income than they did prior to the pandemic. Furthermore, every online search query emits approximately 1.45 grams of CO2, which adds up to 26 kgs of CO2 per year if 50 searches are performed per day.\nAccording to a 2019 report by The Shift Project, a Paris, France-based carbon transition think tank, internet and IT technology usage create more greenhouse gas emissions than the aviation industry and is predicted to account for eight per cent of emissions by 2025.\nIn the first year of its strategy, CIRA is prioritizing sustainability in its operations, which entails measuring the environmental impact of its global data centres.\nThe initiative is a key element of the company’s five-year plan to attain more sustainable operations, develop a sustainable internet and support Canada in meeting its climate targets.\nCIRA’s environmental sustainability strategy\n“We have seen how the Internet led to fundamental changes in our society during the pandemic; changes that lessened our reliance on fossil fuels, such as fewer cars on the road and less air travel,” said Byron Holland, president and CEO, CIRA, in a release. “The Internet has proven to be a massive positive force for environmental sustainability. Now we must strive to make the Canadian Internet itself green.”\nCIRA plans to reduce its energy, emissions and waste impact on the planet through three guiding pillars that make up its environmental sustainability strategy:\n– green workplace: the company aims to create an environmentally sustainable office where employees can thrive through:\n– its LEED gold-certified office building;\n– waste reduction efforts in the workplace via enhanced waste data, resource efficiency and employee education;\n– encouraging sustainable commuting options for employees.\n– green operations: CIRA strives to enhance energy efficiency and reduce GHG emissions across its operations through:\n– reducing greenhouse gas emissions from data centres through data collection, energy consumption reduction protocols and prioritizing data centre locations with lower grid emissions intensity. CIRA is also part of the Ottawa Green Business Network, a community of businesses which have made commitments to reduce emissions;\n– sustainable procurement as it relates to equipment suppliers;\n– reducing emissions from business travel.\n– green internet: CIRA collaborates with other companies to help create a green and resilient internet by:\n– funding climate action-related internet projects, such as green data centre research;\n– collaborating with suppliers to provide a clear environmental policy and sustainability credentials;\n– reporting on CIRA’s progress towards its sustainability performance.\n“A green internet means efficient operations and a focus on procurement that considers sustainability at its core,” said Jane Fulford, chief operating officer, CIRA, in a release.\n“From the data centres that power our global DNS infrastructure to our global business travel, we’re doing our part to reduce emissions by making the necessary changes in our operations to minimize our environmental impact.”\nCanadian companies curbing the internet’s climate impact\nAs SustainableBiz previously reported, other Canadian companies are also working to curb the environmental impact of rising internet use.\nThe Sustainable IT Pledge is joining Canadian organizations for the first time to cut emissions created by their IT operations.\nParticipants in the program, led by CIO Strategy Council, include Accenture, ATCO, Bell, Canadian Blood Services, Micro Focus, Microsoft Canada, Oracle, CloudOps, Rockport and others.\nThese companies have pledged to address emissions from their IT operations by:\n– optimizing systems to reduce data processing volumes;\n– using Cloud-based services where possible;\n– setting emissions targets for IT operations and publicly disclosing progress;\n– setting sustainability standards for suppliers while supporting Canada’s cleantech industry;\n– adopting sustainable IT practices.']	['<urn:uuid:92c5a223-caf0-457a-9e9a-b0f5b5384569>', '<urn:uuid:4cd72d84-dcf3-450a-9916-c64a4eb271de>']	open-ended	direct	verbose-and-natural	similar-to-document	multi-aspect	expert	2025-05-12T10:19:05.071190	24	103	1857
94	Did the Shah-nama and Molla Nasreddin spread across different countries?	Yes, both spread widely across different regions. The Shah-nama was translated into Georgian and became part of Georgian folklore, with translations produced between the 15th-18th centuries. Similarly, Molla Nasreddin stories spread across multiple countries, known as 'Koja Nasr-al-Din' by Kazakhs and Uzbeks, 'Nasr-al-Din Effendi' by Uigurs, 'Molla Nasr-al-Din' by Azerbaijanis, Iranians and Afghans, and 'Mosfeqi' by Tajiks.	['ii. INTO GEORGIAN\nOne of the important research problems of Georgian humanities, to which scholars have devoted great attention, is the thorough study of Ferdowsi’s Šāh-nāma and its Georgian translations. Nikolaj Marr (1864-1934) and Justin Abuladze (1871-1962) laid the foundation of this research at the end of the 19th and the beginning of the 20th centuries.\nNames such as Spandiat (Av. Spəntō.dāta, NP Esfandiār), Thrithino (Av. qraētaona, NP Āfridun, Feridun), or Vaštašab (Av. and OP Vištaspa, NP Goštāsp) are mentioned in Old Georgian literature. Their occurrence suggests that the Georgians were well familiar with the ancient Persian legends. The names of many Šāh-nāma heroes, such as Rostom-i, Thehmine, Sam-i, or Zaal-i, are found in 11th- and 12th-century Georgian literature. They are indirect evidence for an Old Georgian translation of the Šāh-nāma that is no longer extant. The preserved translations of Šāh-nāma stories (dāstāns) originated between the 15th to 18th centuries. The versions in verse comprise Rostomiani (The book of Rostam), Zaakiani (The book of Żaḥāk), and Utrutian-Saamiani (The book of Otroṭ and Sām). Prose versions are Pridoniani (The book of Feridun), Utrutian-Saamiani, and Saam-palavani (The book of Sām the Pahlavān).\nThe Šāh-nāma was translated, not only to satisfy the literary and aesthetic needs of readers and listeners, but also to inspire the young with the spirit of heroism and Georgian patriotism. Georgian ideology, customs, and worldview often informed these translations because they were oriented toward Georgian poetic culture. Conversely, Georgians consider these translations works of their native literature. Georgian versions of the Šāh-nāma are quite popular, and the stories of Rostam and Sohrāb, or Bījan and Maniža became part of Georgian folklore.\nAll Šāh-nāma translations that originated before the 19th century are published (Abuladze, 1916; 1934; Kobidze, 1974). But even today sections of the Šāh-nāma are translated into Georgian. Among the 20th-century versions are the poetic renditions of the poetess and Iranologist Bella Šalvašvili (1976, 1978, 1979, 1987) particularly noteworthy.\nŠāh-nāma translations. Justin Abuladze, ed., Volume I, Tbilisi, 1916; contains a poetic version of Zaakiani (tr. Mamuka Tavakalašvili), a poetic version of Saamiani (tr. Bardzim Vačnadze), and the first part of Rostomiani (trs. Serapion Sabašvili and Khosro Turmanidze) Justin Abuladze, et al., eds., Volume II, Tbilisi, 1934; contains the second part of a poetic version of Rostomiani combined with the Georgian version of The book of Barzu (tr. Khosro Turmanidze), as well as fragments of a prose translation of the Šāh-nāma.\nDavid Kobidze, ed., Volume III, Tbilisi, 1974; contains translations from the 16th to 18th centuries: a verse version of Utrutian-Saamiani (whose first part was published in volume I), as well as prose versions of Utrutian-Saamiani, Saam-palavani, and Baamiani (The book of Bahman).\nBella Šalvašvili, Manučari (Manučehr), 1976.\nIdem, Bežan-Maniže (Bijān and Maniža), 1978.\nIdem, Siavuši (Siāvoš), 1979.\nIdem, Keykavusi (Kay-Kavus), 1987.\nStudies. Alexander Baramidze, Pirdousi da misi Šahname (Ferdowsi and his Šāh-nāma), Tbilisi, 1934.\nIdem, “Le Schah-Naméh de Ferdousi dans la litérature géorgien” in Sakartvelos sakhelmtsipo muzeumis moambe (Proceedings of the State Museum of Georgia) 9, 1936, pp. 141-144.\n“Maʿṣumi: Šāh-nāma dar Gorjestān,” in Ḥasan Anuša, ed., Adab-e Fārsi dar Qafqāz, Dāneš-nāma-ye adab-e Fārsi 5, Tehran, 2003, pp. 318-20.\nJamšid Giunašvili, “Rewāyāt-e Gorji-ye Šāh-nāma,” Āyanda 6, 1980-81, pp. 863-65.\nAlexander Gvakharia, Pirdousi (Ferdowsi), Tbilisi, 1963.\nKorneli Kekelidze, Kartuli literaturis istoria II (A history of Georgian literature II), 2nd ed., Tbilisi, 1958, pp. 323-55; first published 1924.\nIdem, “Šakh-name Firdousi v gruzinskoy literature” (Ferdowsi’s Šāh-nāma in Georgian literature), Etiudebi dzveli kartuli literaturis istoriidan (Studies of Old Georgian literature) 13, 1974, pp. 112-15.\nDavid Kobidze, Šah-names kartuli versiebis sparsuli tsqaroebi (Persian sources of the Šāh-nāma’s Georgian versions), Tbilisi, 1959.\nIdem, Sparsuli Literaturis istoria (A history of Persian literature), Tbilisi, 1975, pp. 202-08.\nAkaki Šanidze, “Šah-names erti epizodi kartul polklorši” (One episode of a Šāh-nāma in Georgian folklore), Literaturuli memkvidreoba 1, 1935, pp. 177-95.\nMagali Todua, “Tekstologiuri šenišvnebi : Šah-names kartuli versiebi (Textological notes: The Šāh-nāma’s Georgian versions),” Tbilisis universitetis šromebi 108, 1964, pp. 153-55.\nLili Žoržoliani, Zaakianis sparsuli da kartuli versiebi (Persian and Georgian versions of Zaakiani), Tbilisi, 2003.\n(Jamshid Sh. Giunashvili)\nOriginally Published: June 15, 2005\nLast Updated: June 15, 2005Cite this entry:\nJamshid Sh. Giunashvili, “ŠĀH-NĀMA TRANSLATIONS ii. INTO GEORGIAN,” Encyclopædia Iranica, online edition, 2005, available at http://www.iranicaonline.org/articles/sah-nama-translations-ii-into-georgian (accessed on 20 September 2016).', 'MOLLA NASREDDIN (Mollā Naṣr-al-Din), one of the most celebrated personalities in Persian and Middle Eastern folklore.\ni. THE PERSON\nMolla Nasreddin is a character who appears in thousands of stories, always witty, sometimes wise, even philosophic, sometimes the instigator of practical jokes on others and often a fool or the butt of a joke. Stories relating to Molla Nasreddin or Naṣr-al-Din Hoca (as he is called in Turkey) are generally humorous, but in the subtle humor there is always a lesson to be learned. These stories involve people and incidents in all walks of life, including kings, beggars, politicians, clerics, etc. He is known by different names in different countries: “Koja Naṣr-al-Din” by the Kazakhs and Uzbeks; “Naṣr-al-Din Effendi “ by the Uigurs; “Nasarat” by the Chechens; “Hoja Naṣr-al-Din” by the Greeks; “Mollā Naṣr-al-Din” by Azerbaijanis, Iranians, and Afghans; and “Mošfeqi” by the Tajiks. Among the Arabs he is often referred to as “Joḥā al-Rumi” and the stories that are attributed to Joḥā or Joḥi of the Arabs are the same as those of Mulla Nasreddin. (Mojāhed, p. 26).\nWhile, for example, the people of Bukhara claim him to be a native of that city, the Turks have tried to make Naṣr-al-Din Hoca a denizen of Turkey. According to them, he was born in 1208 in Hortu, a village near the Anatolian town of Sivrihisar, and moved to Akşehir in 1237 to study under notable scholars of the time. He served as judge (qāżi), from time to time until his death in 1284. His grave is in Akşehir (Köprülü). There is, however, a story of his meeting with Timur, which does not correspond to these dates. On the other hand, Azerbaijani scholars, such as M. H. Tahmasb and M. Sultanov find similarities between Molla Nasreddin and the scholar of the Mongol period Naṣir al-Din Ṭusi (Tehmasib, Introd.). The Azerbaijani folklorist Velayet Guliyev has collected and translated the stories and pleasantries of Molla Nasreddin that are popular among twenty-three nations (Guliyev). Thematically he has divided them into sixteen chapters and has given examples of from different sources. Individual sections deal with different themes and situations commonly found in the tales, for instance, Molla Nasreddin and the Oriental system of justice, Molla Nasreddin as a fool, Molla Nasreddin and his family, Molla Nasreddin and his donkey and so on. For many years the Turkish scholar Pertev Boratav worked on a huge corpus of materials related to Molla Nasreddin tradition not only in Turkey but throughout the Muslim world. He also wanted to catalogue analogues in international oral tradition. After Boratav’s death in 1998, Ilhan Bašgöz published his work along with an essay of his own (Bashgöz). Boratev examined the early references to Molla Nasreddin, and catalogued the compilation of the tales in manuscripts and published works. Bashgöz was able to record diachronic changes in the tradition by examining sources from different periods. Apart from a stylistic tendency to move from short stories to more developed narratives, as one moves in time, there are differences in the treatment of sexuality in more sexually permissive tribal societies to a relatively more sedentary, urban, and sexually restricted milieu.\nIt seems that Molla Nasreddin has his origin in the personality of Joḥā. Joḥā is first mentioned by ʿOmar b. Abi Rabiʿa (d. 715) and by Abu Atāhiya (d. 837; Abu Saʾd Ābi, V, p. 307). In the 3rd/9th and 4th/10th centuries he is mentioned by Abu ʿOṭmān Jāḥeẓ (pp. vii-viii). and Ebn al-Nadim (pp. p. 375, tr. II, p. 736?) respectively. While all three sources stress his foolishness, Jāḥez gives a story where Joḥā very cleverly guesses a man to be a native of Ḥoms from his clothes and demeanor. From this time on references to Joḥā increases in Arabic, Persian and Turkish sources. Ebn al-Nadim (d. 385 /1007) mentions an anonymous collection of anecdotes (Ketāb al-nawāder) featuring Joḥā (Ebn al-Nadim, p. 375, tr. II, p. 736). Rumi in his Maṯnawi elaborately relates five stories about him. Šams Tabrizi in Maqālāt, Sanāʾi in his Ḥadiqa al-ḥaqiqa, and Rašid Waṭwāṭ in his Laṭāʾf al-amṯāl wa ṭrāʾef al-aqwāl each relate one story about Joḥā (Ṣalāḥi, p.112). ʿObayd Zākāni refers to Joḥā eleven times in his works and gives seven stories under his name in his “Arabic and Persian Stories.” Furthermore, fifty-six stories or pleasantries without the name of Joḥā in the same “Arabic and Persian Stories” bear distinct resemblance to the stories of Molla Nasreddin (ʿObayd Zākāni, 2007, pp. 91-125). All of these are given under the name of Joḥā. Some of these can be found in other sources like Laṭāyef al-tawāyef˚ by Faḵr-al-Din ʿAli Ṣafi. It is only from the turn of the 20th century that references to Molla Nasreddin abound in Persian and Azerbaijani literature. The Azeri poet Tāherzāda Ṣāber made the story of Mulla and his blanket the subject of a satirical poem. Jalil Memedqulizadeh chose Molla Nasreddin as a title for his well-known satirical journal which was published for nearly twenty-six years in Tbilisi, Tabriz, and Baku, and had a strong impact on the press of the Constitutional period of Iran (see CONSTITUTIONAL REVOLUTION). Moḥammad-Taqi Malek-al-Šoʿarāʾ Bahār versified one of the anecdotes of Molla Nasreddin under the psuedonym “Mirzā Reżā Rawżaḵᵛān” (Ṣalāḥi, p. 131). Iranian satirist Abu’l-Qāsem Pāyenda analyzed his Molla Nasreddin’s personality and sense of humor.\nThe Nawāder Joḥā was first published in Egypt in 1859-60 under the title of Nawāder Ḵoja Naṣr-al-Din al-molaqqab al-Joḥā al-Rumi. In an other version that was published in 1881, again the same combination of Naṣr-al-Din Ḵoja and Joḥā is mentioned as Nawāder Joḥā wa Ašʿab (p. 17). The earliest Turkish version seems to be Nawāder Ḵoja Naṣr-al-Din (Istanbul, 1837). According to Ahmad Mojāhed (p. 38), the stories and pleasantries of Molla Nasreddin were compiled from the Arabic anecdotes of Joḥā and other similar Arabic and possibly Persian sources. It seems that the Molla Nasreddin’s anecdotes were already popular in Turkey when they were added to the already known stories of Joḥā.\nNawāder Joḥā al-Kobrā, which includes the largest collection of stories, was mainly translated by Ḥekmat Šarif Ṭarāblosi (1912) from a Turkish version, who expanded the collection by adding the Arabic nawāder to the translation. Another version is by ʿAbd-al-Sattār Aḥmad Farrāj (1914), whose collection has been mostly garnered from Arabic sources. The first Persian edition, a collection of 216 stories translated from the above-mentioned Nawāder al-Ḵoja Naṣr-al-Din (1862), came out in Tehran in 1881 without the name of the translator. The widespread popularity of Molla Nasreddin’s stories in Iran began with the publication of an illustrated collection of 539 stories by Moḥammad Ramażāni under the title of Mollā Naṣr-al-Din in 1940, which also served as the source material for all other collection published in Iran since then. There is, however, a difference of 379 stories, taken from various Persian and Arabic sources (Mojāhed, pp. 593-712), between the first anonymous Persian edition and that of Ramażāni.\nIn conclusion it can be said that the Molla Nasreddin tradition of stories seems to be the result of two different corpora of stories originating from two personalities of uncertain historical identity (namely, Joḥā and Molla Nasreddin), who, throughout the ages and in different countries, have acquired their own somewhat different characteristics. It also can be said that Molla Nasreddin stories developed on the basis of Joḥā stories and were gradually increased as more and more materials were added. It can be likened to the confluence of two rivers that, while flowing through different environments, each one acquires its own special characteristics. Out of these anecdotes, some of which might have existed before in different lands, emerges the wise-fool personality of Molla Nasreddin as a character embodying an amusing mixture of silliness and shrewdness. This basically low-class wise fool, who carries the title of a well-learned man (mollā), with his ready wit, life experience, and bonhomie typifies the inner strength of ordinary people. He often humorously portrays centralized despotism or sham piety and resists them in his own way. The other side of Molla Nasreddin is his foolishness and naiveté that often times imparts a meaningful sense of criticism. For instance, in one story the donkey of Molla Nasreddin is missing and he asks a man if he has seen it. He says the donkey has changed and has become the judge (qāżi) of the town. Mollā says: “I believe you since when I was teaching my students, the donkey would shake his ears and listen attentively.” Then Mull goes on to reclaim the judge of the town as his donkey and ends up being beaten up by the judge’s attendants.\nAlbert Ades and Albert Josipovici, Le Livre de Goha le Simple, Paris, 1920.\nʿAbbās Maḥmud ʿAqqād, Joḥā: al-żaḥek al-możḥek, Beirut, 1380/1969.\nAbbas Aryanpur Kashani, Stories of Mulla Nasr-ad-Din, Tehran, 1970.\nAbu Saʿd Manṣur b. Ḥosayn Ābi, Naṯr al-dorarfi’l-moḥāżarāt V, Cairo, 1984.\nD. L. Ashliman, Nasreddin Hodja: Tales of the Turkish Trickster, n.p., 2002.\nJean Dj. Bader, Les bonnes histoires de Mollah, Neuchâtel, 1962.\nHenry Rudley Barnham, tr., Tales of Nasreddin Khoja : 181 Mulla Nasreddin stories, Bethesda, 2000.\nIlhan Bashgöz and Pertev Naili Boratev, Hoca Nasreddin, Never Shall I Die: A Thematic Analysis of Hoca Stories, Bloomington, Ind., ca. 1998.\nHâmit Batu, Nasrettin Hodja: L’humour philosophique, Ankara, n.d.\nGeorge Borrow, tr. The Turkish Jester or, The Pleasantries of Cogia Nasr Eddin Effendi, Norwich, 1924.\nJacques Brissaud and Assadullah Raïd, La taverne du coeur ou Les facéties de Nasrudin et autres histoires, Paris, 1996.\nLorne Brown, “A Look at the History and Humor of the Hodja,” NSA Storytelling Magazine, September,1998.\nKathleen R. F. Burrill, The Nasreddin Hodja Stories, New York, 1957.\nValentine Chirol, “Forword,” in Henry D. Barnham, tr., Tales of Nasr-ed-Din, Khoja, London, 1923.\nArthur Christensen, “Juḥí in the Persian Literature,” in Thomas W. Arnold and Reynold A. Nicholson, eds., A Volume of Oriental Essays Presented to E. G. Browne on His 60th Birthday, Cambridge, 1922, pp. 129-36.\nRaymond C. Clark, The Tales of Nasreddin Hodja Story Cards: Pairwork Conversation Activities, Illustrated by Robert MacLean, Pro Lingua Associates Brattleboro, 1991. Idem, The Fabulous Adventures of Nasruddin Hoja, Paris, 2001.\nJean Coué, Djeha le malin et autres contes kabyles, Illustrations by Roger Pilorget, Paris, 1993.\nJihad Darwiche, Sagesses et malices de Nasreddine, le fou qui était sage; illustrations by David B. and Albin Michel, Paris, 2000.\nJ. A. Decourdemanche, Plaisanteries de Nasr-Eddin-Hodja, Paris, 1876.\nDarwiche Delais, Les Mille et un Rires de Djoha, Paris, 1986.\nCharles Downing, Tales of the Hodja. Illustated by Willliam Papas (New York, Henry Walck), 1965.\nGuy Dugas, La littérature judéo-maghrébine d’expression française: Entre Djéha et Cagayous, Paris, 1990.\nMustafa Duman, Nasreddin Hoca kitaplari açiklamali bibliyografyasi (1480-2004), Istabul, 2005.\nEbn al-Nadim, Ketāb al-fehrest, ed. Reżā Tajaddod, Tehran, 1971; tr. Bayard Dodge as The Fihrist of al-Nadīm: A Tenth-Century Survey of Muslim Culture, 2 vols., New York and London, 1970.\nEgal Errera, L’Ombre du palmier et autres histoires de Goha en Egypte, illustrations by Sébastien Mourrain, Actes-Sud., 2001.\nʿAbd-al-Sattār Aḥmad Farrāj, Aḵbāral-Joḥā: derāsat wa taḥqiq, Cairo, 1980.\nHouman Farzād, Dāstānhā-ye Mollā Naṣr-al-Din/Classic Tales of Mulla Nasreddin, in Eng. and Pers., Eng. tr. by Diane L. Wilcox, Costa Mesa, Calif, 1989.\nJean-Paul Garnier, Nasreddin Hodja et ses histoires turques, Paris, 1958.\nVelayet Guliyev Iyirmi üc Molla Nasreddin, Baku, 1992.\nMetin Hakverdioglu, A Research on Nasreddin Hodja, Istanbul, 1966.\nL. Harmattan, Entre Djeha et Cagayous: La littérature judéo-maghrébine d’expression française, Paris, 1990.\nAmany Hassanein and Valeri Gorbachev, Goha and His Donkey: An Egyptian Folktale, Kathonah, New York, 1999.\nMehmet Hengirmen, Nasreddin Hodja Stories, Istanbul, 1999.\nMurat Hikmet, One Day The Hodja. Istanbul, 1986.\nḤasan Ḥosni Aḥmad, Joḥā, taʾriḵoh, nawāderoh, ḥekāyātoh, ʿelmoh, ḵawāteroh, falsafatoh, Cairo, 1950.\nAbu ʿOṯmān Jāḥeẓ, Ketāb al-Boḵalāʾ, ed. Yaḥyā Šāmi, Beirut, 1995.\nJean Luc Joly, “Choha, Djuha, Goha, Giufà...: la ruse en situation interculturelle,” in idem, ed., La Comédie de la ruse: stratégies et discours des rusés dans les littératures européennes et du monde arabe, Menkens, 1999.\nVeil Orhan Kanik, Nasreddin Hoca (70 Manzum Hikâye), Istanbul 1949.\nGeer Alice Kelsey, Once the Hodja. Illustrated by Frank Dobias, New York,1943.\nMatilda Koen-Sarano et al., Folktales of Joha: Jewish Trickster, tr. from the Judeo-Spanish by David Herman, Philadelphia, 2003.\nFuat Köprülü, Nasreddin Hoca, Istanbul, 1918.\nDidier Leroy, Les aventures de l’incomparable Molla Nasroddine Bouffon de la Perse, Paris,1988.\nIdem, La sagesse afghane du malicieux Nasroddine, Paris, 2002.\nJean-Louis Maunoury, Sublimes paroles et idioties de Nasreddin Hoca, Paris, 1990.\nIdem, Hautes sottises de Nasr Eddin Hodja, Paris, 1994.\nIdem, Nasr eddin Hodja: un drôle d’idiot, Illustrations by Henri Galeron, Paris,1996.\nIdem, Divines insanités de Nasr Eddin Hodja, Phébus, Paris, 1998.\nAlbert Memmi, Le personnage de Jeha dans la littérature orale des Arabes et des Juifs, Jerusalem, 1974.\nDonald Britton Miller, Twice Turned Tales, design by Kae E. Jones, drawings by Henry Syverson. Saratoga, Calif., 1977.\nAḥmad Mojāhed, Joḥā(60-160 h.q.), Tehran. 2003.\nAlfred Mörer, 200 contes choisis de Nasreddin Hodja, Istanbul, n.d.\nHikmet Murat and Bakir Muammer, One Day The Hodja, Istanbul, 1986.\nN. Muallimoglu, The Wit and Wisdom of Nasraddin Hodja. New York. 1986.\nAndré Nahun, Histoires de Ch’ha: Ou Jehâ ou Johâ ou Goha, le plus fou d’entre les fous, le plus sage, le plus rusé, le plus naïf, le plus sot et le plus intelligent, l’unique, le seul menteur qui ne dit que la vérité, Paris, 1980.\nIdem, ed., Humour et sagesse judéo-arabes: histoires de Ch’hâ,proverbes, etc., according to the text by Bernard Chanfrault and Albert Bensoussan, Paris, 1998.\nMehdi Khan Nakosteen, Mulla’s Donkey and Other Friends, Boulder, Colo. 1974.\nNawādei Joḥā al-kobrā, tr. Ḥkmat Šarif Ṭarāblosi, Cairo, 1912.\nNawāder Joḥā wa Ašʿab wa ʿarā’efohomā, ed. Amalin Nasib, Beirut, 1947.\nNawāder al-Ḵoja Naṣr-al-Din al-molaqqab al-Joḥā al-Rūmi, Bulāq, 1880.\nAziz Nesin, The Tales of Nasrettin Hoca, tr. Talat Halman, Istanbul 1988 (Illustrated).\nʿObayd Zākāni, Kolliyāt-e ʿObayd Zākāni, ed. Moḥammad-Jaʾfar Maḥjub, New York, 1999.\nIdem, Aḵlāq al-ašrāf, tr. Hasan Javadi as Obeyd-e Zakani: Ethics of the Aristocrats and Other Satirical Works, Washington, D.C., 2007.\nAbu’l-Qāsem Pāyanda, Defāʿ az Mollā Naṣr-al-Din wa čand dāstān wa yak nemāyis-nāma-ye kutāh, Tehran, 1969.\nCharles Pellat, “Djuḥā,” in EI² II, pp. 590-92.\nMoḥammad Ramażāni, 600 Dāstān-e Mollā Naṣr-al-Din, 2nd ed., Tehran, 1997.\nRašid-al-Din Waṭwāṭ, Laṭāʾf al-amṯāl wa ṭrāʾef al-aqwāl, ed. Moḥammad-Bāqer Sabzavāri, Tehran, 1990.\nFaḵr-al-Din ʿAli Ṣafi, Laṭāyef al-ṭawāyef, ed. Aḥmad Golčin-e Maʿāni, Tehran, 1967.\nʿOmrān Ṣalāhi, Ṭanzhā-ye šuḵ-ṭabʿi-eMollā Naṣr-al-Din, Tehran, 2003.\nH. M. Tehmasib and M. Sultanov, Molla Nasreddin Latifeleri, Baku, 1978.\nM. T. Tahmasip, Molla Nasreddin Latifeleri, Baku 1965.\nOriginally Published: July 15, 2009']	['<urn:uuid:2ef90b35-bb35-4fed-b29b-f0593bfb4554>', '<urn:uuid:a7112089-1bea-4564-a3e5-83396127f575>']	factoid	direct	concise-and-natural	distant-from-document	comparison	novice	2025-05-12T10:19:05.071190	10	57	3105
95	What evidence suggests long-distance travel between Orkney and Stonehenge?	The evidence includes the gold discs which echo finer sheet goldwork found near Stonehenge, and the presence of prismatic V-perforated amber beads and an amber spacer-plate necklace, which are otherwise found only in the Stonehenge area. This suggests an aristocratic woman may have made an 1800-kilometer round trip from Orkney to southern England, returning with an old amber necklace and new amber ornaments.	['This week’s featured items are four Early Bronze Age gold foil discs that were found in 1858 when Mr Nicol Flett, proprietor of the farm at Huntiscarth in Orkney, dug into a prominent round barrow – one of the ‘Knowes of Trotty’. (‘Knowe’ is the word for a mound or hill.) The discs were found in a cist (stone box-like grave) under the centre of the mound, along with numerous amber ornaments and cremated human remains. Mr Flett left the human remains in the cist, and the gold and amber items were acquired by the (then-named) National Museum of Antiquities of Scotland in 1859. Re-excavation of the mound by Professor Jane Downes in 2005 recovered the cremated remains and discovered further tiny fragments of the gold discs, along with a few more amber items. The cremated remains were radiocarbon-dated to around 2030‒1770 BC.\nThe discs are extremely thin – just 0.025 mm, similar to the thickness of industrial-grade aluminium foil – and the most complete disc is 76 mm in diameter, with a central hole around 10 mm in diameter. The discs are decorated with a geometric design, scribed from the underside and standing proud of the surface. It is clear from the creasing of the gold that the discs had not originally been flat, but instead mounted on a rigid, low-domed base that had probably been perforated on its underside.\nExperimental reconstruction of the discs by talented Polish archaeo-goldsmith Wojciech Kochman in 2002 showed how a pea-shaped spherical piece of gold would have been flattened, first by hammering it on a flat stone anvil, then putting it between two sheets of animal hide to stretch the sheet into foil. It was then bent into the low domed shape by placing it over a wooden domed base and rubbing it with a piece of bone, working from the centre outwards. This process also burnished the gold. Any excess from the outer edge was trimmed off. Decorating the discs was achieved by laying them onto a concave layer of beeswax set into a rigid ‘cup’, then scribing the design with a narrow, blunt-ended bone tool before pouring boiling water to melt the wax and release the disc.\nIt’s likely that the discs, mounted on their bases, had been worn on a very special garment, perhaps a cape or gown. They seem too large to have functioned as buttons, so perhaps they had been sewn on (via a V-perforation in the base of the mount) as shiny and extremely precious ornaments. The prismatic and hook-shaped amber objects that were found with the discs may have been sewn onto the edge of the garment as a fringe, while the other amber items form part of an old and worn spacer-plate necklace that would have been worn around the neck. Even though it was not possible to identify whether the cremated bones are those of a man or a woman, it is likely that they had been a woman’s, since amber spacer-plate necklaces are associated with women. The amber necklace fragment and the putative regal cape or gown adorned with gold and amber must have been kept apart from the body during the cremation, as otherwise they would have perished in the pyre.\nThe finds from the Knowes of Trotty are nationally important because they tell us about long-distance travel during the Early Bronze Age. The gold discs echo the even finer sheet goldwork that has been found in graves of that date in the area around Stonehenge in Wiltshire, southern England, and this is the only region where parallels for the prismatic, V-perforated amber beads have been found. It is also where several amber spacer-plate necklaces have been found; specialist amber-workers using imported amber were active in that part of England. No other amber spacer-plate necklace has been found in Scotland, and finds of such necklaces (or rather, parts thereof) are very rare in Britain outside Wessex.\nThe evidence suggests that the owner ‒ probably an aristocratic woman ‒ could have made an amazing 1800-kilometre round trip from Orkney to southern England, perhaps drawn by the fame of Stonehenge, and had come home with part of an old and worn amber necklace plus some fancy new amber ornaments. This isn’t as far-fetched as it may seem, since there are hints that, several centuries earlier, there had been links between Orkney and the area around Stonehenge.\nThe gold discs were made by a goldsmith less highly skilled than the ones active in the area around Stonehenge; might a local metalworker in Orkney have made them? We don’t know. As for the question of where the gold had originated, analysis has shown that it has a fairly high tin content. Further analysis would be needed to show whether or not it matches the tin-rich gold found in Cornwall…\nBy Dr Alison Sheridan, Principal Archaeological Research Curator']	['<urn:uuid:9c712a27-1210-4db2-80b9-5bbdecdc1638>']	open-ended	with-premise	concise-and-natural	similar-to-document	single-doc	expert	2025-05-12T10:19:05.071190	9	63	811
96	laban movement theory voice categories	Laban's movement theory applied to voice includes eight vocal bases: dabbing voice, flicking voice, pressing voice, thrusting voice, wringing voice, slashing voice, gliding voice, and floating voice.	['When it comes to voicing characters, it probably goes without saying that a voiceover artist can have a field day crafting an arsenal of varying, unique voices. Whether it be for narrating an epic fantasy audiobook, multiple characters in a video game, or a number of cartoon characters over the course of a career, the VO artist who specializes in character voices needs to be able to draw from a wide range of voices. Not all characters are created equally, and neither are their voices. Size, gender, age, emotions, circumstance, life experience, etc. all play into how a character sounds. That said, there’s no such thing as a one-voice-fits-all approach.\nNot all voice actors will be able to provide the right voice for every character. So, while I stand by there not being a one-voice-fits-all approach to character voice acting, you’ll often find voice actors tending toward a type of voice they’re really great at producing and voicing characters who all share that similar kind of vocal style. Voice actors may excel at multiple voice styles, but no one is going to do them all well.\nI, for instance, will likely never convincingly pull off a character who requires an extremely deep basso voice, no matter how much I may want to. Sure, there are techniques to help me expand my vocal range so I can push down into a lower register over time, but that deep, rich, resonant bass sound is not where I’m innately gifted. My voice is naturally higher and I’m able to reach into even higher registers quite easily. With that in mind, I tend to look for auditions that need voices for young characters or characters that have a youthful sound. Half the battle in character voice acting is finding your niche and learning to create variety within it.\nThat’s not to say voice actors are limited to the parameters of their natural voice. There are all kinds of characters that need voices, and a voice actor can certainly build up a diverse resumé. Mel Blanc, Nancy Cartwright, Mark Hamill, Tara Strong, and Debi Derryberry are just a few voice actors who have had extremely successful careers voicing a vast array of fun, distinctive characters. Even among these VO titans, though, you might notice similarities in the voices they use to bring different characters to life.\nSo, how do voice actors come up with so many different—even if the differences are miniscule—voices? First of all, it’s not as tough as it may seem initially. The human voice is a remarkably versatile instrument, and one needs only learn how to manipulate it in order to establish a surprisingly large reservoir of character voices. Again, no single person is going to be able to adequately pull off every kind of voice, but there are still so many ways you can make minor adjustments within your vocal wheelhouse to produce more voices than you’ll ever know what to do with.\nWhen it comes to creating a specific character voice, one of the best first steps I’ve come across is studying an image of the character. This is tougher if an image isn’t available, but there is often at least a minimal character description to draw from. If an image is available, take some time to examine what the character looks like. How big are they? How old are they? Does the character require a masculine or feminine voice? How do they hold themselves? Are there any physical features that may affect their voice or speech (e.g., large nose, buck teeth, lolling tongue, puckered lips, etc.). Simply looking over the character’s appearance can often give you most of what you need to create the foundation of what that character will sound like.\nWhat do you do if a character’s voice requires great nuance, though? Or what if you’ve created so many character voices that “new” ones are starting to sound re-used?\nFor what follows, I’m pulling from the video linked below, How To Create 100 Distinctly Different Voices from Darren McStay’s YouTube channel, Improve Your Voice. He is an excellent voiceover resource for anyone with a desire to learn more about VO and lots of time to kill. I highly recommend watching through the full video if this is an area of interest, as I’ll only be summarizing here.\nSo, how do you go about crafting new character voices when your creative juices are coming up dry? Believe it or not, a popular movement theory used in dance and theatre is a great place to start. In the first half of the 20th century, Hungarian dance artist, Rudolf Laban, developed a theory of movement that actors still use today to help them understand internal impulses and develop expressiveness in their body movements when embodying a character role. The Laban Movement Analysis looks at eight “efforts” or categorizations of human movement. When extrapolated, these efforts can also be applied to vocal expression to help voice actors develop an incredible range of distinct voices.\nLaban’s efforts, when applied to voice, give us eight vocal bases to begin with:\n1. Dabbing voice\n2. Flicking voice\n3. Pressing voice\n4. Thrusting voice\n5. Wringing voice\n6. Slashing voice\n7. Gliding voice\n8. Floating voice\nThese efforts act as basic descriptors of the texture and character a voice has. Again, I highly recommend watching the video for more in-depth examples of what these sound like and how they’re structured.\nSo, right off the bat, we have eight distinct voices to experiment with. But wait! There’s more. From there, you can add a variety of other elements to further develop and distinguish your own collection of character voices. Try adjusting your Pressing voice with a masculine vs a feminine perspective. Or explore how your Slashing voice sounds with a different air texture (i.e., breathy or dry). Take any of the eight efforts and mix and match them with one or more of the following elements:\n· Air Placement (nasally, throaty, mixed)\n· Air Texture (breathy, dry)\n· Age Range (young, middle-aged, old)\n· Gender (masculine, feminine)\n· Size (small, medium, large)\n· Tempo (slow, medium, fast)\n· Volume (quiet, mid-range, loud)\n· Tone/Attitude (friendly, impartial, aggressive)\nWhen you begin combining the above elements with the eight efforts, you’ll find pretty quickly that there is a treasure trove of character voices you can come up with that work well for your voice. Spoiler alert for the video—there are WAY more than 100 unique voices you can create when using this method. And that doesn’t even begin to incorporate accents or vocal/speech impediments into the mix!\nThe next time you have a creative block when coming up with that perfect voice, give this method a try. You might just stumble upon a voice you never knew you were able to produce before!\nIf any of you have tried this or a similar trick to creating character voices, let me know how it has worked for you. Have you been able to develop a unique voice that has brought you success? If you haven’t tried it yet, give it a shot and tell me about your experience with the process. Being an artist is all about exploration and discovery. I’d love to hear what you learn.\nLastly, I just want to give a final plug to Darren McStay and his incredible YouTube channel, Improve Your Voice. He really does have an incredible collection of information, tips, tricks, and exercises for voice artists.\nUntil next time, friends, keep telling stories.\nVoiceover Artist | Storyteller | Professional Nerd\n+1 269 370 3657']	['<urn:uuid:16890d3b-e17c-4679-9184-ec1c3d31b3e8>']	factoid	direct	short-search-query	distant-from-document	single-doc	expert	2025-05-12T10:19:05.071190	5	27	1262
97	bali tourism transport accommodation options differences	In terms of transportation, cruise ship visitors typically rely on taxis or shore excursions, with taxi rates being higher in port areas (about 500,000 IDR or 40 USD per day elsewhere). Public transportation is available but unreliable, and self-driving is not recommended due to licensing issues. Regarding accommodation, there's a contrast between traditional tourist facilities and eco-lodges. While conventional options are numerous, authentic eco-resorts follow strict sustainability principles, managing waste, water, and energy efficiently. These eco-properties must demonstrate specific practices like water conservation, local food sourcing, and proper sewage management to be considered truly sustainable, distinguishing them from regular accommodations.	['Welcome at the Bali cruise port guide. Bali truly is magical. It often is stamped “world’s best island” and we think, for now, we’ll join that statement. Bali is an island and province of Indonesia with just over 4,2 million inhabitants and you’ll find it is completely different from the rest of Indonesia. The houses, the people, the daily life, the food are all entirely different. A part of this has to do with the fact that Indonesia might be one of the largest Muslim countries in the world, but the majority of Bali is Hindu. The presence of this religion is clear on every street you walk on. Bali is known as “the island of the thousand temples”, but in fact there are probably over a million on the island and sometimes they even say Bali has more temples than houses. This doesn’t mean that you can only visit temples in Bali. It is such a nice destination because it has that perfect combination of cultural divine with the ultimate bounty island. Whether you are going to enjoy a day on the beach, visit the temples, climb Mount Agung, enjoy the Balinese Nasi Campur or swim underneath the waterfall; it is all possible on Bali.\nTravel information Indonesia\nOfficial currency – Indonesian Rupiah\nAccepts Us dollars – yes\nCreditcards – are not widely accepted\nDrives on the – left\nTimezone – UTC +7 – +9\nInhabitants – 255 million\nOfficial language – Indonesian\nCapital city – Jakarta\nRepublic of Indonesia\nGovernment – Unitary presidential constitutional republic\nGood day – Hari yang baik\nHow are you? – Apa kabar?\nThank you – Terima Kasih\nCruise ship – Kapal Pesiar\nPort – Pelabuhan\nCruise Port Guide Bali – Indonesia: dock or tender\nAt this point there is only 1 option still available for cruise ships to dock and this is at Benoa harbour. Note: this port guide is not for Padangbai or Tanah Ampo.\nThe port of Benoa is a real tricky one. Larger ships often tender and smaller ships often dock. It could also be your captain will not take the risk to manoeuvre in and chose to go at anchor and tender you to shore. Docked or tendering, you will still go to about the same place in the Benoa harbour. You’ll find a rather large terminal building here with a tourist information centre, exchange office, batik shop, massage corner and outside the taxis and a small market.\nThere are limited options on transportation in the port. Most people would go by shore excursion or taxi and this truly is the best way to go. Most cruise lines do not provide a shuttle bus in Bali, but do check with them if this will be the case for your call in Bali. When there is no shuttle bus available your options are often limited to taxi transport. Taxi transport is the way to go on Bali. Taxis are cheap, the drivers know the way and often they can be a good tour guide as this is what they do for a living.\nTaxis in Bali\nPlease find on the right hand picture the sign you’ll find outside the terminal that list the taxi prices on them. These are the taxi prices of 2015, keep in mind they might change in the mean time, but this will give you an idea of what to expect. Please know that these are not fixed prices you can still negotiate with the driver. Should you wonder if there is a difference in price in port compared to somewhere else on Bali: Yes, there is. Usually if you get a taxi on the street the price for a car and driver for a day is about 500.000 IDR (+/- 40 USD). Keep in mind that these drivers work extremely hard and although it’s a free market and you are allowed to negotiate do not get the price down to much as they still need to make a good living. Most cars are very decent and have A/C.\nThere are many different possibilities of what to see and where to go. If you just want to go to the beach the editors favorite is Jimbaran beach around the Belmond hotel or in the area of the Four Seasons (a 15 minute drive from the dock). It is not too crowded, white sand, lots of facilities and the most beautiful sunset while the beach transforms into one gigantic fish restaurant. Find more beach options below.\nIf you are renting a car for a day and you want to see the most inspiring and interesting cultural places of Bali may we suggest the following full day tour by car: Start at the port — 1 hour drive — visit Ubud Monkey Forest — 30 minute drive — Gunung Kawi Monument — 10 minute drive — Tirta Empul temple — 15 minute drive — Bali Pulina — 5 minute drive — late lunch at Bon Bali with a rice terrace view —-1,5 hour drive — Tanah Lot at sunset and catch the fire dance at 06.30 pm — 1 hour drive — back at the ship. This itinerary allows you to see monkeys, old runic temple monument, typical Balinese temple in use, rice terraces, the famous sunset and Balinese dance. The perfect impression!\nYes, there is public transportation available in the port. Whether or not you should use it is up to you. Walk outside the terminal towards the public road, find the bus stop here. They should come by every half an hour and they go to Denpasar. Be really flexible when it comes to public transport. You never quite know where you might or might not end up.\nRent a car or scooter\nWe wouldn’t recommend anybody driving on Bali themselves.You’ll need an International Drivers License to be able to drive a car, motor or scooter. Unfortunatly the police often knows you probably don’t have one and will take advantage of this situation. Not even to mention you could end up causing an accident which would be anybody’s worse nightmare abroad.\nFAQ about Bali cruise destination\n– The cruise itinerary says Denpasar. What is there to do in Denpasar? Denpasar is the capital of Bali and the commercial business centre, aside from the Bali Museum there is no reason to stick around in Denpasar.\n– Which is better: Uluwatu or Tanah Lot? Subjective answer: Tanah Lot\n– What is there to do in Benoa harbour? Nothing much. It is the commercialized port of the island so there might be some interesting vessels in port with you. There are no sights to visit or to see in the port of Benoa.\n– Can I pay in US Dollars? Taxis and souvenirs you can pay in dollars, but keep in mind they are not accepted everywhere and it is cheaper to pay in IDR. There is an exchange office in the terminal.\n– Is a sarong manditory in a temple? Yes, it is. You can bring your own and basicaly all temples provide one that is included in the entrance fare. Note that they are washed daily so it could be that you have to wear one that has already been worn that day. You often can buy one at the markets at the temples as well.\n– Are menstruating women not allowed into temples? True, it might sound unreal, but any woman menstruating or anyone with a running sore or bleeding wound for that matter is considered impure and is not allowed into the temple.\nIt might not be the best temple to get acquainted with the temple culture for the fact you cannot enter the temple, but it is known for its importance in the Balinese culture and its dramatic setting on the cliffs on the south of the island. Therefore most people go during sunset. Make sure you’ll be there at 5.30 pm. From the port it is a 25 km / 1 hrs drive to the south west.\nIn the famous artistic Ubud, that is about an hours drive from Benoa, you’ll find Ubud. What used to be famous for woodcarving, paintings and other forms of art is nowadays also on the world map since the Hollywood movie “Eat, pray, love” was shot in Ubud. And let’s not forget the most famous inhabitants of the Monkey Forest that guard the temple complex here. From the port it is 30 km / 1 hrs 15 min drive north.\nA must-visit for anybody interested in the Balinese culture and tradition. This museum is the best on the island. It is divided in 5 different buildings that have excellent exhibitions that tell you about the tempels, ceremonies, arts and rituals like tooth filing and cremation towers. There are English signs and often also a non-official guide willing to guide you through the museum for +/- 50.000 IDR. From the port it is 12 km / 30 min drive to the north.\nThis beach is approximately a 15 minute drive from the dock. See the map above for its location. It’s not crowded, not to touristy, it has all the facilities you could wish for and in the evening it transforms into THE beach to eat seafood and to watch the sunset. Because it’s so famous for evening entertainment there is hardly anybody there during the day. Ask your taxi driver for the Belmond hotel or Four Season. Walk onto the beach, pick a bed/chair and enjoy your day!\nThe Virgin waterfall is better known as the Tegenungan waterfall. It’s a quite spot where you can escape daily life and enjoy a small piece of paradise away from the tourist sights. Make your way down the staircase and bring your bathing suit. Make it your paradise for a day. Bring drinks and just relax in the water or chill on the rocks. From the port it is a 26 km / 1 hrs drive to the north.\nKuta Beach is simply the most popular beach around. It is the most commercialized area of Bali and most of it is all based and focused on tourism. You either love it or you hate it. From the port it is 10 km / 20 min drive to the north west.\nTirta Empul is the 10th century temple famous for its holy water. This is where Hindu Bali people come to purify themselves. Make sure to bring a bathing suit because you are also able to purify yourself with this holy water. Behind the purifying pool is an entire temple complex where you are also able to see the impressive source of the water and water tanks. From the port it is a 50 km / 1,5 hrs drive to the north.\nJust over an hours drive from Benoa, to the north of Ubud you’ll find Tegallalang. At this town you’ll find the most breathtaking views over the rice terraces. A little bit north of Tegallalang you simply have to lunch at either Dewa cafe of Bon Bali, they both have delicious Balinese food while you are enjoying a view like this. This is a mandatory photo stop. From port it is 48 km / 1,5 hrs drive to the north.\nThe elephant cave also known as Goa Gajah near Ubud. See the map above to find it’s exact location. It is an immense temple complex where you can explore the temples that were re-discovered in 1923. Don’t expect too much of what you’ll find in the cave, but enjoy the view while you navigate the stairs going from temple to temple. From the port it is a 35 km / 1 hrs 15 min drive to the north.\nPandawa beach was back in the days known as the Secret Beach. This is where the locals, the modern day hippies, expats and families come to enjoy a relaxing day on the beach. Some would say this is the best beach of the island and many would agree. This is not the beach to enjoy the services of a resort, but to enjoy Bali the way the locals do. From the port it is 20 km / 40 min drive to the south.\nWalk down the long staircase to the monuments, but keep your camera ready as you walk down the stairs as this is where you’ll see one of the most scenic rice fields come by. When at the bottom of the staircase discover these 11th century candi-altars carved into the rocks. From the port it is 52 km / 1,5 hrs drive to the north.\nUbud is simply the most charming city of Bali. It is a gem to explore whatever your interests are. It has the famous monkey forest, the small little streets with boutiques, bars and cooking classes. It has the pasar/market for whatever you want to buy and the temples and rice fields are just around the corner. Remember though that if there is ever any furniture or art that you are looking for Ubud is the place to shop! From the port it is a 35 km / 1 hrs 15 min drive to the north.\nHistory lovers pay attention! Klung kung is the former capital of Bali. The city was destroyed during the battle of 1908, but what still stands are the court yard and the meeting room which are both famous for their art work on the ceiling. Be fascinated by the stories these paintings will tell you about Balinese life. From the port it is 40 km / 1 hrs drive to the north east.\nThis is the temple that is famous for its sunsets. Join the rest of the island no later than 5.30 pm to see these two temples beautifully back-lit by the sun. Truly one of those experiences where you get mesmerized by Bali. After the sunset you can also join the daily fire dance at 6.30 pm at the temple of Tanah Lot. From the port it is a 28 km / 1 hrs 15 min drive to the north west.\nIf you are interested in the multi rooftops temples this is the one to go to! You’ll find a collection on 10 of the Meru towers. They are typical in Balinese temple architecture. They usually have 3,5,7,9 or 11 roofs and this indicates how sacred the building is. Note: you cannot enter the temple, you can only walk around it and make photo’s like the one above. From the port it is a 30 km / 1 hrs drive to the north.\nFirst learn about the spices and fruit they grow on Bali and than get the chance to drink “the most expensive coffee in the world” for just a few dollars. This famous coffee, also known as Kopi Luwak, has first been eaten and defecated by the Paradoxurus or Luwak. Sip it up while overlooking the most dramatic rice terraces. From the port it is 45 km / 1,5 hrs drive to the north.\nThis is the tiny beach Padang Padang. Find it in a cove at an approximate 45 minute drive from the port. It is sometimes crowded but definitely worth it if you want to go a bit off the beaten track. There is a quite adventurous staircase to go down to the cliff to reach the beach and on the beach you’ll find a small market, the surfers and all the beach chillers. From the port it is 23 km / 45 min drive to the south west.', 'Visitors to Bali have almost too many options these days when choosing where to eat and sleep. It’s both politically correct and lucrative to be ‘green’ in these enlightened times and an increasing number of visitors are voting with their credit cards to stay at environmentally sustainable accommodation.\nThe word ‘ecotourism’ crops up frequently, but what does it really mean? The International Ecotourism Society defines it as ‘responsible travel to natural areas that conserves the environment, sustains the well-being of local people and involves interpretation and education.’\nIn the Bali context, this translates as low-impact, small scale alternatives to mass tourism. It takes considerable commitment, knowledge, planning and investment to create sustainable accommodation. When faced with a selection of hotels which claim to be ‘eco’ or ‘green’, here are six questions to ask:\n– What are your policies to conserve water and electricity?\n– How do you manage the solid waste generated at the hotel?\n– How do you manage your kitchen waste?\n– Where does your sewage go?\n– What percentage of food in your kitchen is locally produced?\n– Where were the building materials sourced?\nManagement and staff of authentically sustainable hotels/lodges/retreats will be eager to share all this information and proud of their achievements. If you’re not getting enthusiastic and informed answers, take your custom elsewhere.\nIn this column I refer to five truly eco resorts/lodges/retreats that are fully committed to long term sustainability; I’m sure there are more. Four of these are located in Bali’s central mountains and the other next to the sea near Amed.\nAll of them follow eco-tourism principals and also practice permaculture (sustainable or permanent agriculture/culture) – a system of integrating agricultural and social design elements, working with nature to create sustainable food, water and shelter. The concept was originally developed by Japanese farmer Masanobu Fukuoka, refined by Australian Bill Mollison and now widely interpreted around the world for different climates and terrains. Working with (instead of against) natural ecosystems is a fundamental principal, as is using materials that are locally available instead of bringing them from a distance wherever possible.\nSarin Buana Eco Lodge and Bali Eco Village were pioneers in bringing the concept of extreme green to Bali long before it became fashionable. Later joined by Bali Eco Stay, these mountain properties bring together the principals of sustainable accommodation with comfort and cultural integration.\nCreated in close partnership with the Balinese landowners, all are built with minimal disturbance to topography and natural vegetation, and constructed with local materials by local craftsmen. Bali Eco Stay and Bali Eco Village generate some or all of their power by hydro-electricity; Bali Eco Lodge employs low wattage, energy saving equipment. All three manage their solid waste sustainably by re-using and recycling where possible, and all compost their kitchen waste to nourish food gardens. Almost all food served from their kitchens is locally sourced and chemical-free. Sewage is treated in wastewater gardens. Water is obtained from springs, rivers or wells and recycled back into the environment.\nBali Silent Retreat in Penebel (picture) goes even further. Low wattage electric light from solar panels is available in the common areas, and most of the accommodation relies on solar lights. All of the buildings are made of recycled wood and other materials. A large organic garden and inspired chef combine to provide one of the most interesting vegetarian restaurants on the island. Because the retreat centre encourages silence and introspection, there is no place to recharge a phone or device on the property. A warung in the village can be visited by those in device withdrawal, but in fact it’s very restful to be able to put them away for awhile.\nBalila Beach Eco Resort is the only one of these properties to be located on the beach instead of the mountains. Perched on a bluff over a sweeping black sand beach just a 20 minute surfside stroll from Amed, the property currently has five large rooms and seven small ones. Building by the ocean has different challenges from the mountain lodges. Beating the heat without air conditioning means building walls with local rock and earth, high ceilings, orienting the buildings so direct sunlight never enters the rooms and providing plenty of natural ventilation. Many of the wooden supporting posts are made of neem, a local fast-growing tree with insect-resistant properties.\nThe greatest challenge here is water conservation on the arid north coast. The property’s well becomes brackish in the dry season, so three rainwater catchment tanks have been constructed under the buildings to hold a total of 62 cubic metres of water. “It’s really difficult to be self-sufficient in water in this area,” explains Birgit, the Austrian-born architect who designed and built the Balila Beach. “Every drop is captured and recycled over and over. The toilet and shower fixtures use reduced water. Black water from the toilets is processed through a wastewater garden and grey water irrigates the fruit garden.”\nThe garden provides some of the food served in the simple restaurant but it’s harder to cultivate a food garden near the sea than in the mountains. All the food is very local and daily fish is purchased from the fishers whose boats line the beach below. As with Bali Silent Retreat, WIFI is discouraged; internet is available in the restaurant but not in the rooms.\nRespect for and consultation with local communities is an integral part of sustainable tourism. All these properties work closely with members of their Balinese communities to create economic opportunities. Not only are locals engaged as staff but they also lead nature walks, food foraging adventures, treks, boat trips and cultural encounters. The properties buy local products such as coffee, cocoa, rice, sea salt, coconut oil, fish, vegetables, fruit and flowers, often at above-market rates.\nAlso to be mentioned here is Bali Asli, a purpose-built cooking school and restaurant incorporating all the principals of eco-tourism. Located outside Amlapura en route to Balila Beach, the spacious property is open daily for lunch. The menu is limited to a large or smaller nasi campur (mixed rice) meal. But the food is amazing; I’ve never encountered Balinese food so clean, fresh and meticulously prepared. Absolutely local, this is Balinese fine dining (or lunching, rather) at its peak and pinnacle. The visitor is invited to experience authentic Balinese cuisine while gazing out over the food gardens to the rice terraces and mountains beyond. (The view is just as stunning from the open-air loo.)\nI doubt that the owners of any of these properties are growing rich. This level of attention to detail and the small scale of operation make them labours of love. But there’s a lot to love. Please visit them, support them, promote them and enjoy them. We need a lot more like these.\nSarin Buana Eco Lodge: www.baliecolodge.com\nBali Eco Village: www.baliecovillage.com\nBali Eco Stay: www.baliecostay.com\nBali Silent Retreat: www.balisilentretreat.com\nBalila Beach: Facebook Balila Beach\nBali Asli Restaurant: www.baliasli.com.au\nIbu Kat’s book of stories Bali Daze – Free-fall off the Tourist Trail is available from :\n– Ganesha Books in Ubud, Sanur and Seminyak\n– Amazon downloadable for Kindle\nCopyright © 2016 Greenspeak\nYou can read all past articles of Greenspeak at www.BaliAdvertiser.biz']	['<urn:uuid:44b75dd5-7151-4c4a-acc3-911cd15bbf1d>', '<urn:uuid:251bb574-e0fb-43c6-b53c-d445ba2e5143>']	open-ended	direct	short-search-query	distant-from-document	multi-aspect	expert	2025-05-12T10:19:05.071190	6	100	3784
98	What's the difference between cognitive miser and availability heuristic?	The availability heuristic relies on immediate examples that come to mind when making decisions, leading people to judge these examples as more frequent than they actually are. The cognitive miser, on the other hand, refers to people relying on simple time-efficient strategies by assigning new information to easily processed mental categories based on prior information, rather than rationally evaluating it.	"['An availability heuristic is a mental shortcut that relies on immediate examples that come to mind. When you are trying to make a decision, a number of related events or situations might immediately spring to the forefront of your thoughts. As a result, you might judge that those events are more frequent and possible than others. You give greater credence to this information and tend to overestimate the probability and likelihood of similar things happening in the future.\nThe term was first coined in 1973 by psychologists Amos Tversky and Daniel Kahneman. They suggested that the availability heuristic occurs unconsciously and operates under the principle that ""if you can think of it, it must be important."" Things that come to mind more easily are believed to be far more common and more accurate reflections of the real world.\nFor example, after seeing several news reports about car thefts, you might make a judgment that vehicle theft is much more common than it really is in your area. This type of availability heuristic can be helpful and important in decision-making. When faced with a choice, we often lack the time or resources to investigate in greater depth. Faced with the need to an immediate decision, the availability heuristic allows people to quickly arrive at a conclusion.\nWhile it can be useful at times, the availability heuristic can lead to problems and errors. Reports of child abductions, airplane accidents, and train derailments often lead people to believe that such events are much more typical than they truly are.\n- After seeing news reports about people losing their jobs, you might start to believe that you are in danger of being layed-off. You start lying awake in bed each night worrying that you are about to be fired.\n- After seeing several television programs on shark attacks, you start to think that such incidences are relatively common. When you go on vacation, you refuse to swim in the ocean because you believe the probability of a shark attack is high.\n- After reading an article about lottery winners, you start to overestimate your own likelihood of winning the jackpot. You start spending more money than you should each week on lottery tickets.\n- ""Perhaps the most obvious demonstration of availability in real life is the impact of the fortuitous availability of events or scenarios. Many readers must have experienced the temporary rise in the subjective probability of an accident after seeing a car overturned by the side of the road. Similarly, many must have noticed an increase in the subjective probability that an accident or malfunction will start a thermonuclear war after seeing a movie in which such an occurrence was vividly portrayed. Continued preoccupation with an outcome may increase its availability, and hence its perceived likelihood. People are preoccupied with highly desirable outcomes, such as winning the sweepstakes, or with highly undesirable outcomes, such as an airplane crash. Consequently, availability provides a mechanism by which occurrences of extreme utility (or disutility) may appear more likely than they actually are.""\n(Amos Tversky and Daniel Kahneman, 1973, ""Availability: A heuristic for judging frequency and probability."" Cognitive Psychology, 5(1), 207-233)\n- ""The availability heuristic refers to a tendency to form a judgment on the basis of what is readily brought to mind. For example, a person who is asked whether there are more English words that begin with the letter t or the letter k might try to think of words that begin with each of these letters. Since a person can probably think of more words beginning with t, he or she would (correctly) conclude that t is more frequent than k as the first letter of English words.""\n(D. L. Medin and B. H. Ross, 1997, Cognitive Psychology, p. 522)\n- ""People not only consider what they recall in making a judgment but also use the ease or difficulty with which that content comes to mind as an additional source of information. Most notably, they only rely on the content of their recall if its implications are not called into question by the difficulty that they experience in bringing the relevant material to mind. (Schwarz, Bless, Strack, Klumpp, Rittenauer-Schatka & Simons, 1991, ""Ease of retrieval as information: Another look at the availability heuristic."" Journal of Personality and Social Psychology, 61(2), 195–202)\nMore Psychology Definitions: The Psychology Dictionary\nBrowse the Psychology DictionaryA | B | C | D | E | F | G | H | I | J | K | L | M | N | O | P | Q | R | S | T | U | V | W | X | Y | Z |', 'From PsychWiki - A Collaborative Psychology Wiki\nThe term “Cognitive Miser” was coined by Fiske and Taylor (1984) to refer to the general idea that individuals frequently rely on simple and time efficient strategies when evaluating information and making decisions. Cognitive miserliness arguably occurs not out of laziness, but out of necessity and efficiency (Fiske and Taylor, 1984). Rather than rationally and objectively evaluating new information, the cognitive miser assigns new information to categories that are easy to process mentally. These categories arise from prior information, including schemas, scripts and other knowledge structures, that has been stored in memory and storage of new information does not require much cognitive energy. The cognitive miser, thus, tends not to stray far from his or her established beliefs when considering new information (Fisk and Taylor, 1984).\nExamples of Cognitive Miserliness\nThough the methods cognitive misers rely on are efficient and can be effective in many situations, they also lend themselves to inaccuracy. Research in this area has focused on the ways cognitive miserliness negatively impacts social cognition, including through the increased use of stereotypes and heuristics. (Operario and Fiske, 1999)\nStereotypes are simplistic ways of categorizing others (Macrae, Milne, and Bodenhausen, 1994). Instead of spending time trying to understand the complex nature of individuals, people instead choose to group people into preconceived categories. A study by Macrae, Milne, and Bodenhausen (1994) showed that participants performed better on a memory task when using stereotypes. Participants in this study were given the labels doctor, artist, skinhead, and estate agent. They were then given stereotype consistent and stereotype inconsistent personality traits for each label. The participants remembered more stereotype consistent personality traits than stereotype inconsistent personality traits. This demonstrates that when using stereotypes, people are more proficient in other cognitive tasks. In this study, the stereotypes made processing easier and facilitated memory. This simplification of people into categories makes for processing easier, but can also lead to faulty decisions. Stereotypes are used by the cognitive miser because they involve easy stimulus processing instead of more effortful determinations.\nHeuristics are mental shortcuts taken in order to lessen the cognitive load that decision making requires (Tversky and Kahneman, 1973). Research by Tversky and Kahneman (1973) and others indicates that people have particular patterns of thinking that they rely on strategies to make decisions easily. The Availability Heuristic is the tendency for people to make a judgment of the frequency of a phenomenon based on how easily an example can be thought of (Tversky and Kahneman, 1973). Tversky and Kahneman (1973) captured this tendency in their 1973 study, when participants rated words beginning with the letter “r” more probable than words with “r” in the third position simply because words that begin with “r” can be thought of more easily. In actuality, there are more words with “r” in the third position. These shortcuts relate to the cognitive miser perspective because they capture the idea that people routinely make decisions using as little effort as possible.\nAlternative Explanations/ Critique\nThe main critique of the cognitive miser perspective is that it fails to explain when people choose to make more effortful determinations on stimuli. Other models have been proposed that offer explanations that encompass the times when individuals choose more detailed processing. (Operario and Fiske, 1999)\nPrinciple of Least Effort\nGordon Allport’s Principle of Least Effort says that people do not have enough time or energy to have differentiated opinions about everything. Rather, they exert more effort to things that are important to them, but for other things they will rely on superficial explorations and generalizations. This perspective is similar to the cognitive miser perspective and is also used as an explanation for why stereotypes exist. (Allport, 1979)\nThe Motivated Tactician\nThis term was also coined by Fiske and Taylor. It is defined as “a fully engaged thinker who has multiple cognitive strategies available and chooses among them based on goals, motives, and needs.” This approach does not dismiss the cognitive miser approach, it simply accounts for the instances when individuals choose to think more carefully (Operario and Fiske, 1999).\nDual Process Model\nThe Dual Process Model (Baumeister and Bushman, 2008) suggests that there are two modes of processing. One occurs automatically and is highly efficient, and the other is conscious, slow, and requires more cognitive effort. The time saving habits identified in the cognitive miser model could be likened to automatic processing. However, the conscious component of the dual processing model acknowledges the use of in depth processing (Baumeister and Bushman, 2008).\nAllport, G. W. (1979). The nature of prejudice /. Reading, Mass.: Addison-Wesley Pub. Co.\nBaumeister, R. F., Bushman, B. J. (2008). Social psychology and human nature /. Belmont, CA: Thomson Higher Education.\nFiske, S. T., Taylor, S. E. (1984). Social cognition /. Reading, Mass.: Addison-Wesley Pub. Co.\nMacrae, N.C., Milne, A.B., Bodenhausen, G.V., (1994) Stereotypes as energy-saving devices: A peek inside the cognitive toolbox. Journal of Personality and Social Psychology. 66(1) 37-47\nOperario, D., Fiske S.T. (1999) Social Cognition Permeates Social Psychology: Motivated Mental Processes Guide the Study of Human Social Behavior. Asian Journal of Social Psychology (2) 63-78\nTversky, A., Kahneman, D. (1973). Availability: A heuristic for judging frequency and probability. Cognitive Psychology, 1973, 5, 207-232.']"	['<urn:uuid:ec9e64c8-8058-4600-85f4-a32503404b9d>', '<urn:uuid:8c991653-7b0a-463e-a3b6-415da07c5a43>']	factoid	with-premise	concise-and-natural	similar-to-document	comparison	novice	2025-05-12T10:19:05.071190	9	60	1649
99	digital media lab teens benefits costs	Digital media labs offer significant benefits for teens while being cost-effective. From Document 1, they provide a supportive environment where young people can develop digital skills and prevent loneliness. From Document 2, these labs enable teens to engage in hands-on projects and peer-to-peer communications, offering spaces where they can 'Hang Out, Mess Around, and Geek Out' (HOMAGO). Regarding costs, they can be started on a shoestring budget, as demonstrated by small libraries that have successfully launched them by sharing resources and using affordable technology tools.	"['Gadget Workshop (Vekotinverstas, FI) is a project in which several organizations teamed up to organize permanent digital making space for young people. The roots of Gadget Workshop are deeply bounded with the idea of information sharing and collaborative working. Wirh European Medialab Praxis, Lasipalatsi Media Centre focused on young people. After all Gadget Workshop is also an environment for anyone to expand his/hers technology, engineering, arts, and IoT skills through experimenting, tinkering and making.\nLasipalatsi Media Centre Ltd is an organization at Helsinki City Group. Among other things, the company is specialized in improving the learning of digital skills. Grounding for Gadget Workshop comes from European Medialab Praxis. Based on original idea of define new media lab environment during the project, Lasipalatsi Media Centre decided to focus on smart cities and smart technologies, IoT and microcontrollers.\nEramus+ funded project (European Medialab Praxis), close collaboration with other organizations at Helsinki City Group and existing space & staff, provided grounding for Gadget Workshop. The mission is to inspire, educate and support learners, makers, artists and entrepreneurs, all citizens, and in turn grow communities and businesses in the City of Helsinki. By offering the space, guidance, learning materials and creative projects, Gadget Workshop offers a fun and valuable experience for all members of the community whilst allowing everyone to come together in a cohesive way.\nGadget Workshop briefly\n- offers access to a variety of making and prototyping tools and equipments\n- provides guidance and support for learning about smart cities and IoT solutions\n- works in close collaboration with schools, communal centers and NGOs for teaching new skills and inspiring the next generation\n- hosts events, classes and meetups to inspire, inform and get people to work together for the ”the most functional city in the world”\n- works with businesses, inspires new companies and start-up’s work for innovative technologies\n- supports cities and organizations that are interested in setting up their own workshop to enhance skills within their city or community\nWhat is Gadget Workshop (Vekotinverstas)\nGadget Workshop is a digital makerspace focusing for IoT and smart cities. It’s a physical place where people can come together to create, share, learn and develop. It is a space that offers tools for creation (basic microcontrollers, computers, sensors, various electronic parts, programming tools). Gadget Workshop can be founded by, for example, a library, a school, a community centre, a space by private-public collaboration... At Gadget Workshop professionals, young people and makers can meet and mix. Intellectual capital in a spirit of sharing and collaboration is an important principle of Gadget Workshop.\nHow Gadget Workshop connects with the principles defined together at European Medialab Praxis\nOpen is to be visible and accessible space, connected to the territory\nGadget Workshop have weekly & daily service time and it’s doors are open for everyone who is interested to learn, try and develop smart solutions and IoT gadgets. It offers guidance at the permanent space, Gadget Workshop @ Kaapelitehdas, but also arranges events (pop-up workshops) at communal centers through the city. Events are organized also by demand.\nSafe is a friendly environnement, a human size, free of pressure, with guidance\nWorking at Gadget Workshop is innovative and resourceful. Workshop hosts (employers of Lasipalatsi Media Centre, Forum Virium Helsinki and City of Helsinki) help to build off the ideas and choose the best tools and programming tools. Easy access and kind, supportive atmosphere lowers the threshold especially for young people. Possibility to learn new technologies at Gadget Workshop helps to prevent loneliness and provides an inspiring space outside of school or home.\nShare is human facilitation and community appropriation to generate p2p connection\nThe essence of Gadget Workshop is in sharing: sharing knowledge, sharing experience, sharing tools & equipments and sharing good will. The administration, advertising and tools are shared by 3 different organizations at Helsinki City Group. The reduces costs and workload.\nEvaluation is allow people to modify and improve the project\nGadget Workshop reflects and evaluates on events and workshops. Workshop hosts and other staff have discussions about their impressions what went well and what didn’t, what could be improved and how. The schools, libraries and NGOs that collaborate with Gadget Workshop, have more formal procedure for feedback and reflections. These signals are used for development of workflows and routines. The factors that determine when workshop/event/work was a success or not, can vary. Sometimes a large amount of participants is a goal. With preselected group of guests, it’s more about how easily shared information was adapted and taken for use. Also a number of new ideas and created new solutions will tell about the impact.\nMovement is to improve skills in the organisation, create « movement », innovation in the organization\nGadget Workshop connects more that one movements:\n- maker movement,\n- peoples growing interest for volunteering,\n- enthusiasm for life-long learning and\n- the state of art technology\nare all represented at the environment of Gadget Workshop.\nWith collaborating partners, Lasipalatsi Media Centre have a confidence that media labs and more specific IoT labs can have a huge part to play in helping to gain understanding the smart cities and cutting edge technologies that are more and more present in daily life.\nWhen young people can develop the skills and aspirations of the digital makers of the future, the digital awareness grows and the grounding for the next generations of professional is built.\nWhen Gadget Workshop is running at Helsinki, it is closely connected with city strategy.\nHelsinki aims to be the most functional city in the world. Lasipalatsi Media Centre seeks also another goal, set by Forum Virium Helsinki: the most functional smart city in the world.\nThe ambition of Gadget Workshop is to seek this goal and set it firmly in the community. By building bridge between city, educational organizations and business, people’s learning in general can improve, and young people’s opportunities to learn cutting edge technology and possibilities to find meaningful education and jobs can strengthen.', ""Makerspace and Digital Media Lab Resources\nWhy Makerspaces in Libraries?\nTalking Points: Museums, Libraries and Makerspaces, Institute of Museum and Library Services (IMLS)\nIMLS makes a strong case for libraries and museums to implement makerspaces as a means to “strengthen community-based learning, particularly for critical thinking, problem solving, collaboration, and engagement in STEM.”\nThis selection of resources will help guide the implementation of your makerspace, including space planning considerations and getting buy-in from stakeholders.\n- 7 Tips for Planning a Makerspace, Transforming Education Through Technology Journal\nSensible strategies for starting small, creating a space for active learning and cultivating a making culture.\n- A Librarian’s Guide to Makerspaces: 16 Resources\nOpen Educational Database List of 16 resources including articles and blog posts.\n- A Place of Their Own: Creating spaces where teens can thrive, Jennifer Velasquez, American Libraries\nCogent insights on creating spaces where teens feel empowered and engaged; references YALSA's Teen Spaces Guidelines.\n- FAQs about Makerspaces, Shannon Barniskis\nAs a PhD student, Barniskis studied makerspaces in libraries of all sizes; she blogs about a variety of issues, including policies and safety.\n- Launching a Makerspace: Lessons Learned From a Transformed School Library\nNew Canaan High School librarian Michelle Luhtala recently jumped headfirst into creating a makerspace in her library and documented what she learned, how her space changed and how it affected students along the way.\n- Making + Learning in Museums and Libraries, Pittsburgh Children's Museum\nA framework for those who are just starting makerspaces or those who want to reevaluate existing spaces; organized by three key elements: Purpose, People, and Pieces and Parts.\n- Making Your Space: Creators and Makers in the Library, WebJunction\nIf the idea of a makerspace conjures up the hurdles for you (limited space, limited resources, or limited confidence in your own tech skills), this webinar will help you learn how to get started and grow a space in your library for creating.\n- Of Makerspaces and Media Labs: Tales from Two Small Libraries, TechSoup for Libraries\nTechSoup article featuring two small libraries, which includes good “Advice from the Librarians Who’ve Been There.”\nProjects and Tools\nThere are many tools available beyond the 3D printer to get people making at your library.\n- Crafting at the Library, Public Libraries Online\nVery simple and low-budget crafting programming for adults.\n- Legos! The one tool every makerspace needs?, School Library Journal\nThe author celebrates the versatility of Legos, with ideas for Lego projects and games, plus tips for getting around the high cost of Lego sets.\n- Makedo Cardboard Construction\nThese simple and safe tools open up all kinds of possibilities for creating with cardboard. Learn key techniques for working with cardboard in quick 15-second tutorials (scroll halfway down).\n- Makerspace Manual, Susan Barnum, El Paso (TX) Public Library\nThis free-to-download ebook has step-by-step instructions and lots of illustrations for projects using Soft Circuits, Makey-Makey, Scratch and more.\n- Small Tech, Big Impact, School Library Journal\nGood ideas for acquiring technology tools on a small budget —robotics, circuitry kits and 3D printer pens are all within reach.\nDigital Media Labs\nAs a variation on making, a digital media lab is more focused on youth cultural production, creating a space full of media equipment where teens can Hang Out with each other, Mess Around with the stuff and Geek Out on creating and producing (HOMAGO).\n- Build Your Own Digital Media Lab, Skokie (IL) Public Library\nThe Skokie Public Library’s state-of-the-art media lab lets patrons get creative with technology.\n- Cool Tools\nPBSparents lists and links to free online apps for integrating digital media into the learning environment.\n- Digital Media Labs and Makerspaces in Small and Rural Libraries, TechSoup webinar archive\nTwo small libraries will share how they have integrated digital media and makerspace technology in their library.\n- HOMAGO Guide Book, Yollocalli Open Studio\nThis handbook is designed for those who want to create a youth cultural production space, or just want to understand more about the HOMAGO learning theory.\n- Shoestring Digital Media Lab, Laurel (DE) Public Library\nAn inspiring story about one small library that started a digital media lab on a shoestring budget and then watched it blossom; includes the starter equipment list.\n- Where Teens and Technology Meet, WebJunction self-paced course\nThe digital media lab at Howard County (MD) Library System demonstrates that teens learn most effectively through hands-on projects and peer-to-peer communications.\n- Tips for Making with Learners of All Ages\nDon’t overlook the potential for engaging kids as young as pre-K or kindergarten–they can develop skills through making too.\nPhoto Credit: BiblioTEC at the Kitsap Regional Library""]"	['<urn:uuid:920c6e4a-00c2-437c-8e5e-900dc4888d3e>', '<urn:uuid:32b75605-52ec-496e-ae3c-a76fcd5394df>']	factoid	with-premise	short-search-query	similar-to-document	multi-aspect	novice	2025-05-12T10:19:05.071190	6	85	1764
100	lung cancer drug gefitinib working mechanism effectiveness	Gefitinib works by inhibiting EGFR tyrosine kinase in non-small cell lung cancers (NSCLCs) with EGFR mutations. It triggers cell death through activation of the proapoptotic BH3-only protein BIM and shutdown of the EGFR-MEK-ERK signaling cascade. In terms of effectiveness, when combined with apatinib in first-line treatment, it showed improved progression-free survival of 13.7 months compared to 10.2 months with gefitinib alone. The combination achieved response rates of 77.1% and demonstrated deeper tumor shrinkage, with 89.2% of patients showing at least 30% reduction in tumor size.	['The epidermal growth factor receptor (EGFR) plays a critical role in the control of cellular proliferation, differentiation, and survival. Abnormalities in EGF-EGFR signaling, such as mutations that render the EGFR hyperactive or cause overexpression of the wild-type receptor, have been found in a broad range of cancers, including carcinomas of the lung, breast, and colon. EGFR inhibitors such as gefitinib have proven successful in the treatment of certain cancers, particularly non-small cell lung cancers (NSCLCs) harboring activating mutations within the EGFR gene, but the molecular mechanisms leading to tumor regression remain unknown. Therefore, we wished to delineate these mechanisms.\nMethods and Findings\nWe performed biochemical and genetic studies to investigate the mechanisms by which inhibitors of EGFR tyrosine kinase activity, such as gefitinib, inhibit the growth of human NSCLCs. We found that gefitinib triggered intrinsic (also called “mitochondrial”) apoptosis signaling, involving the activation of BAX and mitochondrial release of cytochrome c, ultimately unleashing the caspase cascade. Gefitinib caused a rapid increase in the level of the proapoptotic BH3-only protein BIM (also called BCL2-like 11) through both transcriptional and post-translational mechanisms. Experiments with pharmacological inhibitors indicated that blockade of MEK–ERK1/2 (mitogen-activated protein kinase kinase–extracellular signal-regulated protein kinase 1/2) signaling, but not blockade of PI3K (phosphatidylinositol 3-kinase), JNK (c-Jun N-terminal kinase or mitogen-activated protein kinase 8), or AKT (protein kinase B), was critical for BIM activation. Using RNA interference, we demonstrated that BIM is essential for gefitinib-induced killing of NSCLC cells. Moreover, we found that gefitinib-induced apoptosis is enhanced by addition of the BH3 mimetic ABT-737.\nInhibitors of the EGFR tyrosine kinase have proven useful in the therapy of certain cancers, in particular NSCLCs possessing activating mutations in the EGFR kinase domain, but the mechanisms of tumor cell killing are still unclear. In this paper, we demonstrate that activation of the proapoptotic BH3-only protein BIM is essential for tumor cell killing and that shutdown of the EGFR–MEK–ERK signaling cascade is critical for BIM activation. Moreover, we demonstrate that addition of a BH3 mimetic significantly enhances killing of NSCLC cells by the EGFR tyrosine kinase inhibitor gefitinib. It appears likely that this approach represents a paradigm shared by many, and perhaps all, oncogenic tyrosine kinases and suggests a powerful new strategy for cancer therapy.\nAndreas Strasser and colleagues demonstrate that activation of the proapoptotic BH3-only protein BIM is essential for tumor cell killing and that shutdown of the EGFR−MEK−ERK signaling cascade is critical for BIM activation.\nNormally, cell division (which produces new cells) and cell death are finely balanced to keep the human body in good working order. But sometimes cells acquire changes (mutations) in their genetic material that allow them to divide uncontrollably to form cancers—life-threatening, disorganized masses of cells. One protein with a critical role in cell division that is often mutated in tumors is the epidermal growth factor receptor (EGFR). In normal cells, protein messengers bind to EGFR and activate its tyrosine kinase. This enzyme then adds phosphate groups to tyrosine (an amino acid) in proteins that form part of signaling cascades (for example, the MEK–ERK signaling cascade) that tell the cell to divide. In cancers that have mutations in EGFR, signaling is overactive so the cancer cells divide much more than they should. Some non-small cell lung cancers (NSCLC, the commonest type of lung cancer), for example, have activating mutations within the EGFR tyrosine kinase. Treatment with EGFR tyrosine kinase inhibitors (TKIs) such as gefitinib and erlotinib induces the cells in these tumors to stop growing and die. This cell death causes tumor shrinkage (regression) and increases the life expectancy of patients with this type of NSCLC.\nWhy Was This Study Done?\nUnfortunately, treatment with TKIs rarely cures NSCLC, so it would be useful to find a way to augment the effect that TKIs have on cancer cells. To do this, the molecular mechanisms that cause cancer-cell death and tumor regression in response to these drugs need to be fully understood. In this study, the researchers have used a combination of biochemical and genetic approaches to investigate how gefitinib kills NSCLC cells with mutated EGFR.\nWhat Did the Researchers Do and Find?\nThe researchers first measured the sensitivity of NSCLC cell lines (tumor cells that grow indefinitely in dishes) to gefitinib-induced apoptosis. Gefitinib caused extensive apoptosis in two cell lines expressing mutant EGFR but not in one expressing normal EGFR. Next, they investigated the mechanism of gefitinib-induced apoptosis in the most sensitive cell line (H3255). Apoptosis is activated via two major pathways. Hallmarks of the “intrinsic” pathway include activation of a protein called BAX and cytochrome c release from subcellular compartments known as mitochondria. Gefitinib treatment induced both these events in H3255 cells. BAX (a proapoptotic member of the BCL-2 family of proteins) is activated when proapoptotic BH3-only BCL-2 proteins (for example, BIM; “BH3-only” describes the structure of these proteins) bind to antiapoptotic BCL2 proteins. Gefitinib treatment rapidly increased BIM activity in H3255 and HCC827 cells (but not in gefitinib-resistant cells) by increasing the production of BIM protein and the removal of phosphate groups from it, which increases BIM activity. Pharmacological blockade of the MEK–ERK signaling cascade, but not of other EGFR signaling cascades, also caused the accumulation of BIM. By contrast, blocking BIM expression using a technique called RNA interference reduced gefitinib-induced apoptosis. Finally, a combination of gefitinib and a BH3-mimicking compound called ABT-737 (which, like BIM, binds to antiapoptotic BCL-2 proteins) caused more apoptosis than gefitinib alone.\nWhat Do These Findings Mean?\nThese findings (and those reported by Gong et al. and Costa et al.) indicate that activation of the proapoptotic BH3-only protein BIM is essential for gefitinib-induced killing of NSCLC cells that carry EGFR tyrosine kinase mutations. They also show that inhibition of the EGFR–MEK–ERK signaling cascade by gefitinib is essential for BIM activation. Because these findings come from studies on NSCLC cell lines, they need confirming in freshly isolated tumor cells and in tumors growing in people. However, the demonstration that a compound that mimics BH3 action enhances gefitinib-induced killing of NSCLC cells suggests that combinations of TKIs and drugs that affect the intrinsic pathway of apoptosis activation might provide a powerful strategy for treating cancers in which tyrosine kinase mutations drive tumor growth.\nPlease access these Web sites via the online version of this summary at http://dx.doi.org/10.1371/journal.pmed.0040316.\nA perspective by Ingo Mellinghoff discusses this article and two related research articles\nWikipedia pages on epidermal growth factor receptor, apoptosis, and BCL2 proteins (note that Wikipedia is a free online encyclopedia that anyone can edit; available in several languages)\nCancerQuest provides information on all aspects of cancer from Emory University (in several languages)\nUS National Cancer Institute information for patients and professionals on lung cancer (in English and Spanish)\nInformation for patients from Cancer Research UK on lung cancer including information on treatment with TKIs\nInformation for patients from Cancerbackup on erlotinib and gefitinib', '2 Clarke Drive\nCranbury, NJ 08512\n© 2022 MJH Life Sciences™ and OncLive - Clinical Oncology News, Cancer Expert Insights. All rights reserved.\nDual inhibition of both VEGFR and EGFR with the combination of apatinib and gefitinib in the first-line treatment of patients with advanced EGFR-mutant non–small cell lung cancer demonstrated superior progression-free survival.\nDual inhibition of both VEGFR and EGFR with the combination of apatinib and gefitinib (Iressa) in the first-line treatment of patients with advanced EGFR-mutant non–small cell lung cancer (NSCLC) demonstrated superior progression-free survival (PFS), according to results from the phase 3 ACTIVE trial (NCT02824458).1\nA total of 313 patients enrolled on ACTIVE trial were randomized 1:1 to receive apatinib plus gefitinib (n = 157) or placebo plus gefitinib (n = 156). At median follow-up of 15.8 months, the median PFS in the apatinib arm was 13.7 months compared with 10.2 months with the EGFR inhibitor alone (HR, 0.71; 95% CI, 0.54-0.95; P = .0189).1\n“Apatinib combined with gefitinib is expected to become a new first-line treatment option for EGFR-mutant NSCLC patients,” said Li Zhang, MD, who presented the data as part of the 2020 ESMO Virtual Congress. “Furthermore, this dual oral regimen will definitely provide a more convenient treatment for patients who require long-term administration.” Zhang is a professor of medical oncology, director of Phase I Unit of Sun Yat-Sen University Cancer Centre (SYSUCC), and deputy director of the Lung Cancer Research at SYSUCC.\nPatients with detectable EGFR exon 19 deletions (Ex19del) and exon 21 (L858R) mutations were well balanced between the study arms: 51.6% and 47.1% in the apatinib arm versus 53.2% and 46.8% in the placebo arm. Stratified PFS data showed a similar PFS benefit for patients with Ex19del (HR, 0.67; 95% CI, 0.45-0.99) and those with L858R mutations (HR, 0.72; 95% CI, 0.48-1.09).\nSecondary end points of the trial included investigator-assessed PFS, objective response, and duration of response. Median PFS via investigator assessment was 13.8 months with apatinib versus 12.0 months with placebo (HR, 0.71; 95% CI, 0.53-0.95; P = .0186). The objective response rates were 77.1% and 73.7%, respectively (P = .5572). Specifically, 1 patient in the apatinib arm achieved a complete response (CR), 120 (76.4%) had a partial response (PR), and 12 (7.6%) had stable disease (SD). No patients achieved a CR in the placebo arm, however, 115 (73.7%) patients had a PR and 22 (14.1%) had SD. The disease control rates were 84.7% and 87.8%, respectively (P = .3466).1\nThe depth of response was also evaluated and defined as the best percentage change from baseline in sum of diameters of the target lesions. A reported depth of response of at 30% in change in lesion size was reported for 89.2% of patients treated with apatinib plus gefitinib and 79.5% of patients treated with gefitinib alone (P = .0209). The percentage of patients who had a depth of response of at least 50% were 64.3% and 52.6%, respectively (P = .0238).1\nDuration of response (DOR) was evaluable in 121 patients in the apatinib arm and 115 patients in the placebo arm. The median DOR was 12.9 months (95% CI, 11.2-14.7) and 9.3 months (95% CI, 9.2-11.1), respectively (HR, 0.64; 95% CI, 0.47-0.88; log-rank P = .005).1\nOverall survival data were immature at the time of cutoff, with only 29.4% of the predetermined events reached.\nIn terms of safety, “no unexpected safety signals were identified beyond the established safety profile of each single agent,” Zhang explained. “As expected, apatinib plus gefitinib increased the risk of treatment-emergent adverse events [TEAEs] compared with placebo plus gefitinib. Roughly 50% of patients in the combination group experienced dose interruption or reduction.”\nGrade 3 or higher TEAEs were reported in 84.1% of patients treated in the combination arm versus 37.7% of patients in the placebo arm. In regard to dose interruption and reduction, the rates were 59.9% and 48.4% versus 22.7% and 4.5%, respectively. Further, discontinuation of treatment due to TEAEs was 5.1% with the combination and 3.2% with placebo.\nThe most common grade 3 or higher TEAEs in the apatinib arm were hypertension (46.5%), proteinuria (17.8%), alanine aminotransferase increase (11.5%), and weight decrease (11.5%). Grade 3 or higher TEAEs were less common in the placebo arm. The most commonly reported were alanine aminotransferase increase (10.4%), aspartate aminotransferase increase (3.2%), hypertension (2.6%), and diarrhea (1.3%).1\nExploratory Biomarker Analysis\nNext-generation sequencing (NGS) performed at baseline and post-progression provided further efficacy indicators in patient subgroups. Biomarker analysis at baseline demonstrated an improved PFS in patients with TP53-mutant disease treated with the apatinib and gefitinib combination (HR, 0.56; 95% CI, 0.31-1.01) compared with those who do not harbor the mutation (HR, 0.92; 95%, 0.50-1.67). More specifically, a PFS benefit was observed in patients with TP53 exon 8 mutations, with a hazard ratio of 0.24 (95% CI, 0.06-0.91) favoring the apatinib group. The hazard ratio was 0.79 (95% CI, 0.41-1.52) in patients without the exon 8 mutation.1\n“Subgroup analysis by baseline p53 mutation status showed that the PFS hazard ratio favored the combination treatment for patients with p53 exon 8 mutation…However, due to the small sample size [N = 18] this benefit will need to be further confirmed by a large-scale clinical study,” said Zhang. Seven patients in the apatinib combination arm harbored the TP53 exon 8 mutation compared with 11 in the placebo arm.\nFurther, in analysis of patients who experienced progressive disease, 43.7% patients (n = 103) in the apatinib arm and 41.4% patients (n = 111) in the placebo arm provided blood samples for NGS.\nA similar resistance pattern was found in both groups in regard to the development of T790M mutation. In total, 37.8% of patients treated with apatinib who experienced progressive disease were T790M-positive compared with 37.0% of patients in the placebo arm.1\nDelaying EGFR-TKI resistance has been an ongoing challenge in the NSCLC treatment landscape. Despite an observed PFS benefit with the EGFR TKI gefitinib in patients following failure of chemotherapy, a majority of patients experience disease progression or relapse within 1 year.2\nApatinib, is a novel small-molecule, multi-targeted TKI, that selectively inhibits VEGFR2. Investigators of the ACTIVE trial hypothesized that simultaneously targeting EGFR and VEGFR pathways may be a feasible therapeutic strategy for patients with EGFR-mutant NSCLC based on early preclinical studies that demonstrated the dual blockade’s efficacy delaying the emergence of resistant tumors.2\nApatinib is currently approved in China for the treatment of patients with advanced gastric cancer following failure of frontline standard chemotherapy following superior observed PFS and OS data over placebo and a manageable safety profile.2']	['<urn:uuid:86f97324-94f8-4a42-bbfe-3210ad051d1d>', '<urn:uuid:a718c2b1-6856-40d0-9812-b5a5ec05cde1>']	open-ended	direct	short-search-query	distant-from-document	multi-aspect	novice	2025-05-12T10:19:05.071190	7	85	2223
