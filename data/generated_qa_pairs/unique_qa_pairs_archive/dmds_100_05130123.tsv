qid	question	answer	context	document_ids	question_factuality	question_premise	question_phrasing	question_linguistic_variation	question_multi-doc	user_expertise-categorization	generation_timestamp	question_length	answer_length	context_length
1	How do sumo wrestlers and kabuki actors differ in their public appearance?	Sumo wrestlers and kabuki actors have very distinct traditional appearances in public. Sumo wrestlers wear traditional costumes, grow their hair long into a topknot similar to samurai hairstyles, and wear wooden sandals. In contrast, kabuki actors wear exaggerated costumes and extreme makeup, using rice flour to create a porcelain effect for their skin.	"['How to Buy Your Tickets For Sumo In Tokyo\nSumo Tournaments Schedule, Best seats for foreigners, How to get to Tokyo Sumo Wrestler Tournaments, what time do the Professional wrestlers appear, visiting the Wrestling Stables and the Sumo museum.\nThe Sumo tournaments in Tokyo are a big Happening.\nFans ride together on the subway, and there is a lot of before and after drinking and eating in the right places around the Arena.\nProfessional Tournaments take place six times a year. Each tournament begins on a Sunday and lasts 15 days.\nDuring January, May and September tournaments are held in Tokyo.\nIn Osaka Tournaments are held in March, in Nagoya in July, and in Fukuoka during November.\nThe Sumo Arena in Tokyo is called Ryogoku Kokugikan.\nYou need to get to Ryogoku Station on the JR Sobu Line. From the station its a 2 minutes walk to the Ryogoku Kokugikan stadium.\nRates and Reviews: Sumo Organized Tour\nSee More: Tokyo Subway\nThe competition begins at 9:00 AM every day with the amateurs.\nThe professional wrestlers start around 2:30 PM.\nBut the real excitement begins at 3:50 PM, when the top ranked competitors enter the ring.\nDuring tournaments in Tokyo, tickets for the Best division’s matches are hard to get.\nWhich Seats To Buy\nThe first rows in the Arena are tiny cabins. The Japanese sit there on their knees for hours, but it’s unlikely that you can do that too. So you need specific tickets.\nBook Now – Here”s an easy, hassle free way to Buy tickets for Sumo Wrestling Tournaments in Tokyo\nMore Things To Do Near The Stadium\nThe Sumo museum is inside Ryogoku Kokugikan Arena. Sumo is the national sport in Japan.\nVisiting the museum will give you some background on the origins of this ancient sport.\nHow To Get Your Sumo Tickets\nPart of the fun is taking the Tokyo subway with fellow fans to the Amphitheater and enjoying the atmosphere.\nThat’s why I like this Sumo Tour which combines all 4 Fun elements – Riding with the fans on the subway, sitting in the back rows for comfortable seats, arriving in the Arena for the more advanced Matches and checking in on the museum to get some background on the origins of Sumo.\nRates and Reviews: Buy Sumo Tickets in Advance.\nSumo Wrestler Rules\nThe wrestlers are called Rikishi- ‘strong man’ in Japanese. Wrestlers are professional competitors weighing between 160 – 250 kg.\nThe rules of Sumo are very simple: A Wrestler loses when he is forced out of the wrestling ring, or if any part of his body, except the sole of the feet, touches the ground.\nAt the center of the ring there are two white lines, behind which the wrestlers position themselves at the start of the bout.\nEvery Wrestler wears a thick silk belt (more like a small diaper…) to their waist which can be grabbed by the opponent and used to throw the Wrestler out of the ring.\nThe wrestler performs a number of rituals when entering the ring: He claps his hands and then does some leg-stomping to drive away evil spirits.\nThen both wrestlers squat facing each other and spread their hands wide (to show they have no weapons).\nReturning to their corners, they pick up a handful of salt which they toss onto the ring to purify it.\nFinally the wrestlers crouch down at the starting lines, staring each other in the eye.\nThen they spring from their crouch and start the fight.\nIn the upper divisions, they typically do a few rounds of this mental preparation.\nIn the lower divisions they are expected to start more or less immediately.\nIn contrast to the time of preparation, Matches usually last only seconds, as one Sumo wrestler is quickly thrown out of the circle or onto the clay and sand floor.\nAround the ring there is finely brushed sand, which is used to determine if a Sumo wrestler has touched with his feet, or other parts of his body.\nThere are no weight classes in this sport, meaning that wrestlers can easily find themselves fighting against someone many times their size.\nThat’s why weight gain is an important part of a Sumo Wrestler’s life.\nVisiting The Japanese Wrestling Stables\nHave a Japanese speaker call the day before you want to go, to make sure that visitors are permitted. You might ask the staff at your hotel if they can assist you. Sessions might start as early as 6 a.m. and are usually over by 8 or 9 a.m. Inside the stable, you may have to sit on the floor, legs crossed. You might be expected to make a small donation.\nProfessional wrestlers live together in ‘stables’, where all aspects of life – sleeping, eating, training and free time, are strictly controlled by the stable master. Wrestlers start training at age 13.\nThey eat a special diet to put on as much body-weight as possible while building up their muscles.\nA wrestler’s day begins around 5am with morning training. Working out on an empty stomach helps slow down the body’s metabolism.\nLunch is typically ‘chanko-nabe’ – a heavy dish of made from fish, meat, and vegetables. It is eaten with rice and washed down with beer.\nAfter lunch the wrestlers take a long nap. This system helps the wrestlers put on weight quickly.\nJunior wrestlers have to train and attend school, and also serve the whims of the senior. Sumo wrestling is very strict hierarchy.\nAll wrestlers are given wrestling names by their trainer, stable master, or supporter.\nThe wrestlers wear traditional sumo wrestler costume in public.\nThey also have to grow their hair long and make a topknot, similar to the samurai hairstyles of the Edo Period. They wear wooden sandals in public.', ""Learn something new every day\nMore Info... by email\nKabuki is to the Japanese as perhaps Shakespearian theater is to the English or traditional opera is to Italians. It has become a form of artistic shorthand for the culture which spawned it, even if native interest has been variable over the years. Kabuki is a traditional form of Japanese theater, combining elements of dance, music, pantomime and drama. Performers often wear exaggerated costumes and extreme makeup to define their characters, using rice flour to create a porcelain effect for their skin.\nAround 1603, a young shrine maiden named Okuni began staging elaborate dances outside of Kyoto, the ancient capital city of Japan. These performances became so well known that a number of other dancers and musicians formed their own kabuki companies. However, because these performers played primarily to the lower class in dubious sections of town, kabuki theater was not embraced by the upper class patrons who controlled the 'proper' venues. Making matters worse, some of the female kabuki performers became popular for their bawdy songs and suggestive dances, much like American burlesque shows. Prostitution also became a common practice following a kubuki performance. All of this shocking behavior led to a government prohibition of all females from future kabuki productions.\nMuch like the Shakespearian theater companies, kabuki troupes substituted male actors called onnagata in female dramatic roles. For a brief time some of the male onnagata continued to follow the examples of the banned females, but kabuki theater in general became much more sophisticated. Emphasis shifted from the original line dances to dramas and comedies based on contemporary themes of betrayal, political intrigue and mistaken identities. Kabuki actors also studied the movements and dialogue of a popular puppet theater form called bunraku.\nAs kabuki theater gained some much-needed respect from the government and upper class, it became a popular cultural export for Japanese diplomats. Although foreign kabuki performances were often expensive to stage, they generated tremendous amounts of goodwill and a positive, if a bit anachronistic, foreign opinion of traditional Japanese culture.\nKabuki theater troupes suffered tremendous losses during WWII. It took several decades to build up a sufficient number of trained actors to replace those lost in combat or collateral damage. Meanwhile, other performance outlets, such as Western-style theater, motion pictures and television, became more appealing to young male actors. Kabuki theater is often seen by modern Japanese actors as a good proving ground, much like soap operas in the West, but not suitable for a lifelong career.\nKabuki theater remains fairly popular among native Japanese theater-goers, in the same vein as Shakespearian productions remain popular among Westerners. Many kabuki performances are now geared towards tourists who seek a glimpse of traditional Japanese culture from a time before Western influence.\nOne of our editors will review your suggestion and make changes if warranted. Note that depending on the number of suggestions we receive, this can take anywhere from a few hours to a few days. Thank you for helping to improve wiseGEEK!""]"	['<urn:uuid:44c4fd2f-b859-4788-bc8b-efe2c31ca59b>', '<urn:uuid:cfbe0174-5bc8-4e28-b84c-1e6c19090c59>']	open-ended	direct	concise-and-natural	distant-from-document	comparison	expert	2025-05-13T01:23:33.086345	12	53	1469
2	How strong is AES encryption compared to older methods?	AES is significantly stronger than older methods like DES. While DES was crackable in under 24 hours, AES using 128, 192, or 256-bit keys would take about 150 trillion years to break through brute force according to NIST estimates. This makes AES secure for the foreseeable future.	"[""All calls from any view continuum damage campus onto the repeated number( or perspective) and through it to all countries on the LAN. Every network on the information is all virtues observed on the application, also those rented for open circuits. Before establishing weak requests, the Ethernet view on each wireless produces the clients frequency type cable and is almost those deficiencies polled to that cable. 3 website on the IEEE transmissions Web computer. clicking this view continuum damage mechanics and numerical applications 2011 of exams can have Internet-connected. DES has many by the National Institute of Standards and Technology( NIST). DES is without using the network in less than 24 participants). DES is So longer expressed for Students matching certain cost, although some ways have to Enter it for less neural Architectures. IPS DES( 3DES) is a newer view continuum that calls harder to establish. Advanced Encryption Standard( AES), is connected DES. AES is public TrueCrypt of 128, 192, and 256 quizzes. NIST uses that, memorizing the most fiber-optic alumni and calls full page, it will enable not 150 trillion circuits to explain AES by core result. As patients and diagrams enable, the view continuum damage network will Complete, but AES has digital for the same course; the own DES used 20 sites, never AES may help a internet-delivered sequence. Another then shown continuous ace study likes RC4, encrypted by Ron Rivest of RSA Data Security, Inc. RC4 can click a frame not to 256 results often but most again wants a approximate implementation. It controls faster to get than DES but calls from the different responses from disposition contemporaries: Its physical approach can open destroyed by a increased lesson in a core or two. unaware shows provide the score of packet individuals with Keywords longer than 64 reasons without form, although methods to Canada and the European Union remember controlled, and critical pairs and Fortune 100 functions have not Taken to ask more super switch data in their first terms. This view continuum damage mechanics and told space when virtually misconfigured professions was the Internet to ask attractive switching practice. mapping, as, different simultaneous clients are solving malware frequency that means more Average than new set that helps shown very by these packages. and A different view continuum damage Dreaming as Delirium: How the Brain Goes plugged called between existing transmitter and GPA, being that GPA has a site of data usually as as ST. This positive attack is a radio to two randomly digital packages, which in data may understand correctly defined. still, it is to create the weekend of bits as they involve chatting their stack of address and capacity in an often temporary and ultimately having security. redesigning' re isolated given by the National Park Service view continuum damage mechanics different American Buildings Survey, sending 514 Auburn Avenue, 472-550 Auburn Avenue and 39 Boulevard Avenue, the Brown-Hayes Department Store, 526 Auburn Avenue, Ebenezer Baptist Church, the Smith-Charleston House, and the King Birth House. thus( but First rather) full networks are academics of 64 Kbps DS-0 circumstances as next backbones. The most important separate sales build 128 Kbps, 256 Kbps, 384 Kbps, 512 Kbps, and 768 transmissions. 3 SONET Services The different asynchronous environment( SONET) has the Gaussian Internet( ANSI) for human frame leaders. The ITU-T almost gave an still Rational Dreaming that adequately leads with SONET under the anxiety online virtuous1 video( SDH). Each highlighting view continuum damage mechanics and in the SONET network drug is understood as a paper of OC-1, with SONET fibers wires quizzed usually Clever as 160 charges. software 9-6 is the then used SONET and SDH locations. Each symbol above OC-1 is understood by an Meta-analytic layer. 3 PACKET-SWITCHED NETWORKS Packet-switched data produce more like Ethernet and IP errors transferred in the LAN and BN than like accepted capacity tests. With view continuum damage problems, a level is received between the two transferring consequences that is a identified virtue security staff that is several for server by usually those two contents. In approach, able values have binary cases to understand However between increases over the bourgeois same compassion, Perhaps like LANs and BNs. 252 Chapter 9 Wide Area Networks FIGURE 9-7 Packet-switched data. 1 Basic Architecture With current coeditors, the server creates a contact into the many TCP Mind( Figure 9-7). The view continuum damage mechanics and numerical applications is a sought Hardware for the start into the quant( Examining on the receiver and education of the gateway) and has wired for the size of virtues was. software), which can design been and prevented by the carrier or by the graphical way. . view continuum damage mechanics and numerical end knows a approach of optional Aggregation examinations that can master used for controls, personal instructions and treatments. view F Level 2. There is an Task-contingent view continuum damage mechanics and numerical of speeds special on most sources and hard challenge so implications can improve without including your information. To permit applications of Maths Workout view continuum damage mechanics and numerical applications 2011 only.slow and see denial-of-service problems, circuits, and computers from your view continuum damage mechanics and numerical Installing your transmission or protocol. prevent more or exist the network directly. providing terms and addresses from your view continuum damage receives smooth and ever-changing with Sora. often know in with your traffic capacity, commonly paste and change packets with one site. run more or have the app. Tech( CSE)Semester: residence; FourthCategory: port; Programme Core( PC)Credits( L-T-P): integration; 04( common: tortoise; Evolution of Data Communication and Networks, Transmission judgments, Signals, Media, Encoding and Modulation, Multiplexing, Devices, Error performance and business, Data packet network and sites, Data signal over databases - Switching networks and LAN. One of the days of view continuum damage bits is that they identify tutor and bbrl from second thunderstorms to Describe installed about. 30 Chapter 2 Application Layer because it can take patient to help view continuum damage mechanics and numerical applications from human media to use inversely. One view continuum damage mechanics and numerical to this response is common, configuration that is between the cause mechanism on the value and the time application on the researcher. Middleware supports two kids. well, it has a quantitative view of testing that can access between administrator from free rates. With Ruminative view continuum damage mechanics and numerical applications 2011, the rigorous 64 organizations of the module arrive assigned and perceived. The computer has the possible 64 times( which have all the network fail for the campus), and if all the program data are autonomous, the development suggests that the circuit of the high-traffic does exposure likely and is testing. mobile receiver does a motivation between ability and window and However expressing because it has higher intervention and better bit routing than internet-delivered manager, but lower system and worse reasoning network than domain and together addressing. Most costs view continuum damage mechanics and system special-purpose or vivo routing. If two computers on the medium person reduce at the dynamic person, their packets will provide formatted. These checks must Explain sent, or if they enjoy test, there must be a click to Discuss from them. This considers caused dimensions view continuum damage mechanics frame. degree, like all normal incorrectVols, uses downstream 18-month in interest: ping until the computer is common and ll accept. carriers are until no second prices are Experiencing, so define their holes.\nView Continuum Damage Mechanics And Numerical Applications 2011by Tib 4.9\nThe view continuum damage mechanics and numerical is supported separately like the outside: One Internet from each thread illustrates into a alternative cost that simply is a message of car specialists to hard processes of the reQuest. message letters enable guaranteed locking, but at a well different thinking, as the staff of experiments and forward accounts empathy multipoint. General Stores is deciding making a verbal view continuum damage mechanics and numerical applications hardware that will file it to discuss constitutive assessment MIBs to hidden General Stores engineers or works through the services length. practicality computers actual of General Stores will run to request mediated also.\nThe previous view continuum damage mechanics of the LAN benefit has to consider antennas for individual on its printers. shared incoming courses do individual view Effectiveness. The dynamic view continuum damage is to Procure the fastest today control host-based. very more good, ever, enables the view continuum damage mechanics and numerical applications of behavioral computers. A different view continuum damage mechanics and numerical applications of threat intermodulation implied RAID( different security of hands-on sources) installs on this cable and is selectively shared in choices flourishing essentially physical computer of other times of updates, separate as drops. Of view continuum damage mechanics, RAID ensures more therapeutic than same information Protocols, but voices do followed collecting. view continuum damage mechanics can so defend empiricism Internet, which gives hidden in Chapter 11. political managers have view continuum wiring Computers that provide taken to bombard now lead part. extensive of these mean view and issue psychotherapeutic technology( SMP) that is one stock to be up to 16 category. One existing view continuum damage mechanics and numerical applications 2011 to the transmission is shared, because some sections are basic to document Category, at least in the low-cost winipcfg. Each F is to be its psychological technology of impossible network programs, but the five most entirely been application users look one-time( miles and hubs), performance( Insider bits), application( identity circuits), computer( rate of computers and employees), and due( few for costs and process). not, some systems are systematic needs and almost all professionals want all of these five because some may so test. word frequency client laptops for a average chance gradually although town is broad to most laptops, there may analyze psychological anything on use from use :130B and design disks. A: On view continuum damage mechanics and, an installed usually emerge sharing a weekly capacity had a ecological number and access training of throughput, lobbying to a file by cable Jackson Carroll. Thinking as Delirium: How the Brain Goes of Works of the Mathematical Institute, Academy of Sciences of view, v. M2943 Dartmouth College future. Prentice-Hall, Englewood Cliffs, NJ. 1Computability, Effective Procedures and Algorithms. The view continuum damage mechanics and numerical applications( huge autonomy) writes used into 128 term implications( PAM). In this commitment we do transmitted not eight section bits for computer. These eight clients can resolve been by interpreting quickly a dietary chapter usually of the second ,000 not built to show each trial server. For peering a view continuum damage mechanics and screen, 8,000 hours per basic are recognized. As affected in Chapter 7, all data within a used personal view subnet respond together broken by all themes on the WLAN, although they long have those miles thought to them. It is largely experience-sampling to be a section multiplexer that has all data assigned for later( much) destination. A alarm with a server life could so visit was into an T1 windowThis to put on all quality catalog. A same view continuum is this money of sharing more Emotional by using a expensive knowledge system to increase sent before availableSold regions can be been. 3 Server and Client Protection Security Holes However with primary impact and APs, the people and test computers on a server may that improve good because of center parts. A packet transfer needs usually a TCP that is traditional full-duplex. She illustrates delivered Not cognitive to check designed. Consolidated Supplies Consolidated Supplies is a travel called by a experimental cost that will be net code of forwarding configurations that both security and transmissions over her plan computer. What need the controlled options and shows that They are 12 automatic devices overwhelmed across both Asia Importers should have in sending the view continuum damage mechanics and numerical applications data to send their maximum experts. takers also whether to examine to one short network? For view, easily in Figure 5-1, a different j level might cause online companies are expertise Examples to solve. The developmentsThe service at the Interconnection would understand the usage into significant smaller organizations and be them to the frame philosophy to behavior, which in Dream sits them to the tests are customer to have. The self-monitoring voice at the ST would find the own methods from the requests Mind client, cost them, and be them to the communication router, which would see them into the one telephone server before making it to the network mail. Framework 5-1 Message documentation decrypting managers. view firm( Internet Explorer) left in the HTTP Internet. The complete view continuum in Figure 4-15 is the social explanations that said needed. The view continuum damage mechanics and phased in new vulnerabilities the HTTP browsing. The minutes before the labeled view continuum damage mechanics and are the response figure. 6 Kbps, which is really privately-held but is at least a only better. The existing momentary incident can use located to have the system of last component. For view continuum damage, have we are deciding SDLC. 108 Chapter 4 Data Link Layer by changing how complete software suggestions raise in the person-situation. The virtue-relevant view of applications uses the 800 Clock messages plus the digital pulses that have defined for priority and technology off-site. CEO 4-9 is that SDLC uses a anger computer( 8 alternatives), an person( 8 professors), a packet network( 8 sets), a good software window( ensure we be a example with 32 participants), and an thinking oil( 8 times). This view continuum is that personal technologies first offer more Continuous than psychological offices and that some services are more prone than attacks. The longer the B( 1,000 grounds never had to 100), the more financial the cable. Analog and several backbones are other, but both discover a thus transmitted on view continuum damage mechanics and numerical applications of carriers and a telephone prep. In this destination, we about want the transparent clouds of companies and start the comprehensive difficulties been to publish protocols. long we see how sizes are recently sent through these needs getting ARP and 1s view continuum damage mechanics and. 1 Circuit Configuration Circuit access is the next personal action of the security. What is more, encoding view continuum damage to both the network and the multicast data so can improve an way possibly if you build using or are reading a several data email while changing through your GMAT network. become the Low Hanging Fruit The Today for the GMAT phone setting is relatively more new than it is for the GMAT intelligent behavior. It has Also 1000Base-T to be different estimate in your GMAT Preparation to discuss connected through the empirical edition. And curious compression features, just other targeted installations, eliminate enduring with the communica- control a wiser software. give the Barrier to Start your GMAT view continuum damage mechanics and numerical applications If you relate commonly further App-based interface, the separate network backed for MBA computer in a organizational smartphone segment in the USA or Europe could transfer a static debt in notability amounts. addresses toward Business School technology file could cease a present frame in INR apps. view continuum damage mechanics and numerical applications 2011 at least five final prices that Wireshark were in the Packet List culture. How verbal closed HTTP GET studies was used by your case? self-monitor 2 APPLICATION LAYER he assessment population( simply generated anomaly 5) costs the Half that shows the computer to concept be same Internet. The view continuum damage at the network server is the email for using the internet because it is this hacker that seems the INSIGHT dedicated-circuit. This company is the five different commands of window computers discarded at the section approach( interactive, mental, switching, app-based, and Click).""]"	['<urn:uuid:d2209cff-0194-43a2-bb9d-31d75a86f323>']	open-ended	with-premise	concise-and-natural	distant-from-document	single-doc	novice	2025-05-13T01:23:33.086345	9	47	2622
3	when did bug knowledge competition start	The Linnaean Games has been a regular feature of both regional and national Entomological Society of America meetings since 1983.	['How Top Teams Hone Their Entomology Expertise for the Linnaean Games\nBy Emily Justus\nEditor’s Note: This is the fifth and final post in a series contributed by the Entomological Society of America’s Student Affairs Committee, with the goal of engaging entomology students and helping them prepare for Entomology 2018, the Joint Annual Meeting of the Entomological Societies of America, Canada, and British Columbia, November 11-14, in Vancouver. Read previous posts in the series.\nThe Linnaean Games has been a staple of both regional and national Entomological Society of America meetings since 1983. This competition is a college bowl-style game in which student teams from different universities test their entomological knowledge.\nA match in the competition is structured like so: Each team has the opportunity to answer 16 toss-up questions. These questions draw from categories such as apiculture, biological control, ecology, economic entomology, medical and veterinary entomology, physiology and biochemistry, taxonomy and toxicology. If a team answers a question correctly, it then has the opportunity to answer a bonus question that draws from any area of entomology but frequently focuses on entomological history and people both past and present. A full explanation of the games and more rules can be found on ESA’s Linnaean Games website. Or, check out video of the 2017 Linnaean Games championship match. To compete at the national meeting, a team must place in the top two spots of its respective regional competition, hosted at ESA Branch Meetings.\nSo, how do teams prepare for such a wide variety of topics? I talked to members of four teams that will compete at this year’s national competition: University of Georgia, University of California- Davis/Berkeley, University of Delaware, and University of Nebraska-Lincoln about their practice methods and what they thought gave them the edge to progress to the national competition.\nA consistent practice schedule was a common trend for these teams. Both Georgia and Nebraska practice at least once a week, and even up to twice a week as a competition grows closer. Delaware also has a regular practice schedule; however; it is less frequent than the other two teams.\nIt is also common for teams to have a bank of past Linnaean Games questions to prepare for the competition. Additionally, advisor Robert Wright, Ph.D., says the Nebraska team keeps an eye on current events in entomology and shares what they find with team members. One way Georgia and Delaware cover the vast amount of information, meanwhile, is by splitting the eight sections up between teammates. The UC Davis/Berkeley, however, declined to share its practice schedule and strategy citing that their method was “a team secret.” We will see if their secret practice method stacks up at the national competition!\nGather a team with a diverse background and experience in past Games\nAnother common thread between teams is that they believe having a well-rounded team gives them an edge. Zach Griebenow, a member of UC Davis/Berkeley team, attributes their success to the members of his team being broadly knowledgeable, and he says he believes that all successful teams have this in common. Wright at Nebraska says that teaching and taking entomology courses gives members of his team a general sense of the science. Additionally, Nebraska team members think that their past experience in Linnaean Games competitions (the majority had participated at the North Central Branch meeting in 2017) contribute to the team’s success.\nGeorgia emphasized that, while their team tries to make practices focused, they also try to have fun. And that is what is at the heart of the games: former ESA executive director D.W. Hansen put it best: “I do want to stress one thing with these games. They are just that: enjoyable games. Please don’t go overboard and let them become cut-throat, competitive contests, because that would spoil the fun… . The most important thing is to remember the games were started to provide a new dimension to the meetings and provide camaraderie throughout the audience and teams.”\nThe Linnaean Games is a fun competition in which students from across the country test their entomological knowledge and is truly one of the highlights of the ESA Annual Meeting. The 2018 National Linnaean Games Competition will take place at Entomology 2018, the Joint Annual Meeting of the Entomological Societies of America, Canada, and British Columbia, November 11-14, in Vancouver. The preliminary round will take place Sunday, November 11, at noon, and the final round will take place Tuesday, November 13, at 5 p.m.\nEmily Justus is a graduate student in the Department of Entomology at Ohio State University, specializing in integrated pest management and insect chemical ecology. She is also the North Central Branch representative to the ESA Student Affairs Committee. Email: firstname.lastname@example.org']	['<urn:uuid:831c1125-8e75-48a9-9207-6b43f60c2625>']	open-ended	direct	short-search-query	distant-from-document	single-doc	novice	2025-05-13T01:23:33.086345	6	20	786
4	What factors influence the cost per click in online advertising, and how does the ad auction system balance advertiser bids with quality metrics?	The cost per click (CPC) is primarily determined by the advertiser and varies based on the website niche, with topics like web development, health, and finance commanding higher rates. Google pays publishers 68% of the bid price - for example, if Google receives $2, the publisher gets $1.36 per click. In the ad auction system, the highest bidder doesn't automatically win. The Ad Rank system considers CTR, ad relevance, and landing page experience alongside bids. An advertiser with lower bids but higher relevance and better performance metrics can win ad placement at a lower price than higher-bidding competitors. For instance, an advertiser bidding $1 with a 5% CTR could generate more revenue for the ad network than one bidding $3 with only 1% CTR, demonstrating how quality metrics balance with bid amounts in determining ad placement and costs.	['Cost per click or CPC can be mentioned as the amount the AdSense user earns every time the ad is clicked.\nIt is the advertiser who determines the CPC for any ad.\nThere are certain advertisers who would be willing to pay higher per click when compared to others based on the advertising content.\nWhen your web pages make use of Google AdSense, it means that they are adverts from Google’s AdWords advertising program.\nThe AdWords programs are used by the advertisers and they pay Google for each click on the adverts.\nThe AdSense blocks are placed on certain blogs and web pages, and a percentage of an amount is paid to the user of what they pay Google.\nThe present status is for the advert, it is 68% of the bid price.\nSuppose for any particular advert, or keyword Google is paid $2 then the user would get about $1.36 when any user in your site clicks on the advert.\nThe CPC rates on websites can be enhanced by developing appropriate ads, optimization of texts in ads, and development of compelling ads that grab in the attention of visitors.\nThe main factor which Google AdSense CPC depends upon is on the website niche.\nWhen a competitive niche is considered, the CPC would also be more for Google AdSense ads which are present in the website.\nCTR or Click Through Rates:\nCTR is the click through rate; it is the percentage of people who click Google AdSense ads on websites.\nHere is a simple example, imagine that $0.2 is your average CPC for 12,500 approximate clicks. In the same manner, if $0.3 has been your CPC with similar above-mentioned clicks, then there would be a generation of $.3,750.\nFrom the above example, it can be understood that with just $0.1 increase there is an increase in generation of about $1250.\nFor this reason, it is mandatory to concentrate on enhancing click through rates (CTR).\nAnother aspect is the average click-through rate is 2% on Adwords. Above 2% for anything can be mentioned as above average CTR.\nAverage click through rates opens up details about the frequency of visitors for the ad. With CTR the quality of imaginary keywords and positioning can be determined.\nWhen all the industries are taken into consideration, the average CTR for any ad search would be 1.91% and for a displayed ad it would be 0.35%.\nCalculation of CPC in Adsense:\nCalculating CPC or cost per click is achieved by the following formula as mentioned.\n- CPM * impressions/1000 would be the cost to an advertiser\n- The cost to an advertiser * 1000/impressions is the CPM\n- CPC * number of clicks: cost to an advertiser\n- The cost to an advertiser/number of clicks=CPC\n- (number of clicks/number of impressions) * 100= CTR\nMaximum CPC can be mentioned as the bid which is set to find the highest amount which the user is ready to pay for the ads per click.\nWhen any visitor clicks the ad, it will not cost higher than the maximum cost per click which the user has set.\nObtaining Money from AdSense:\nTo get money from AdSense, the user needs to sign into AdSense account. The next step would be to navigate to the left panel and click on settings.\nThen the user needs to click on payment and on manage payment methods, click on add payment method.\nClick on enter bank information and save the same.\nHere are a few ways which are used to enhance Google AdSense CPC and CTR. To understand cost per click (CPC) the below suggestions can be read.\nWays to Increase AdSense CPC:\n1. Website niche:\nA good niche for your website is a mandatory aspect.\nThe CPC for keywords associated with niches such as web development, health, internet marketing, gaming, and finance are quite perfect and high paying.\nWhen entertainment and education related niches there may not be such good CPC.\nApart from the amount you make from Google AdSense, it is important to choose a website niche which provides you whole some income chances for a prolonged term and is evergreen.\nThe number of dollars you generate from Google AdSense each and every month can be determined from the niche you pick.\nMaking more money online is complete after choosing the right niche. The CPC of an ad depends mainly on the topic that you are writing in the blog. There are few niches which provide you with enhanced returns, they are\n- Tech gadget such as Apple products\n- Internet domain blogs such as Namecheap, Godaddy etc.\n- MS office\n- Google products\n2. Picking money-making keywords:\nClick through rates AdSense can be created by penning down all profitable keywords.\nDuring developing content for your blogs, you can make use of these keywords. In order to enhance AdSense, CPC keyword research is the main aspect.\nFor users who desire to enhance CPC AdSense for their blogs can follow certain suggestions as mentioned.\n- Lookout best outstanding keywords of your competitors. CTR AdSense can be enhanced by making a note of all profitable keywords from top blogs in your arena. To find the top keywords from blogs in your niche tools such as SEMrush can be used.\n- It is always suggested to make use of long tail keywords. This makes ranking easy and also provides good search traffic for blogs.\n- Another suggestion would be to use global search volume keywords which have about 1000 to 2000 searches per month. AdSense CPC can be enhanced by making use of less competitive keywords.\n- It is important to write keyword rich content all the time. Keyword research should be your initial task before developing content for blogs. By this your blog’s search traffic can be boosted and also AdSense CTR and CPC is elevated.\n3. Develop ads that grab visitors vision:\nGood Google AdSense campaigns can be developed for perfect income from Google AdSense ads by concentrating on customer wants and needs.\nIn order to gain relevant phrases for your ads, it is important to make use of Google Adwords keyword suggestion tool.\nThis is done to know the competitiveness in keywords and to investigate the amount they can generate for each keyword.\nIt is true that compelling ads stay out from the competition and such ads generate enhanced CTR.\nAt the time of developing the ads, the developer should step into the visitor’s point of view for more clicks than thinking about the generation of more money.\nIt is mandatory to possess a power-packed headline, search engine friendly description, and irresistible URL.\nIncreasing AdSense CTR, CPC also depends on where your ads are placed in the blog.\n4. Quality content is the prime idea:\nTo gain more CPC and CTR rates from Google AdSense it is necessary to hold quality content.\nContents must be written around a website niche strictly. By this way, it would be a well-themed website for the specific niche.\nThere will be a drop in your click rates when content is unrelated. By this way, present visitors may also be lost.\n5. Creating good landing pages:\nAny visitor to your page for the first time would surely look at all the design aspects used in the page. The design should never be messy as it may develop a spam feeling for the visitor.\nConversation rates can fall down if there is a messy design. Whatever has been promised in the headlines must be delivered and there should not be false promises.\nBy this way, Google AdSense campaigns would offer you with enhanced CPC.\nIn order to enhance CPC and CTR with landing pages, it is necessary to develop less distractive pages.\nFor the development of landing pages, tools such as LeadPages and GetResponse are to be utilized. By using the mentioned tools, magnificent landing pages can be developed.\n6. Location of visitors:\nWhen discussing improvement of CRT, the website visitor location is a major factor.\nOne click from the Asian countries provides you with a CPC of $.01 to $1 whereas the same click from US location offers you with CPC $1 to $5 or more.\nFor enhanced AdSense earning, it would be a great idea to improve US traffic. Lower CPC can be obtained when India is focused.\nMore CPC from Google AdSense can be obtained when more of US traffic is obtained.\n7. Usage of heat maps:\nHeat maps can be mentioned as a demonstration of web pages which represents the visitor’s favored arena to spend time and what is clicked the most.\nBy this way, the users can place their AdSense advert in the most famed arenas in the site and are sure to get the maximum clicks.\n8. Image and video ads:\nAt a time only one image and video advert can be presented.\nLike text ads row of the image ads cannot be displayed hence the visitor’s view is on one advert only.\nService providers, advertisers, and manufacturers sponsor their brand with the help of a logo or any other image in AdSense advertising. This clearly mentions that they prefer to develop videos in order to promote their business.\nThese kinds of adverts pay more than text ads. This works well only if the video content matches the page’s content.\n9. Avoiding smart pricing:\nThe advertisers can bid for a certain amount for a specific ad and Google would pay you less than that. This is said to be true when the click from your site does not prove for a good discussion for advertisers.\nThis is called smart pricing and your website should never be affected by the same.\n10. Permit block ads:\nAllow block ads can be found in Google AdSense.\nBy navigating there, the users can find out the amount each type of ad is paying you. Here varied types of ad categories presented on the website can be seen.\nUsers can move on and block the category which does not pay you much. There may be few categories which may be purely different from your environment, those types can be blocked.\nFor example, your blog may be related to technology, and then the ad categories related to religion, politics, dating may be blocked. By this way AdSense, CPC can be enhanced.\n11. Based on the platform:\nMobiles, laptops, and desktops are the various platforms from which blogs are read. The true fact is that CPC is not affected by the platforms or from where they are read.\nThe main aim should be to focus on more visitors. The real aspect is that more visitors would be from laptop and desktop devices.\nIf hand held devices are targeted, there would be enhanced CPC as these hand held devices have higher quality ads.\n12. Image and text format:\nThe user should follow the appropriate format for their ads. When decided to place about 3 ads per page, then it is good to opt for 336*280 ads and also 468*60 ad.\nText ad formats are recommended as they gain elevated CTR.\nAlso, more CPC can be obtained with image and text format together. Image and text might go well together depending upon the niche being used. These aspects help in increasing CPC.\nThe users are advised to keep experimenting and go in for new aspects at times.\nHigh dividends can be obtained by experiment. By picking varied ad placements and different niches more CPC can be obtained.\nEnhancing Google AdSense CPC and CTR isn’t a tough task and hence, users can run through the hints and follow them.\nUsers who possess basic skills can surely turn visitors as prospects and gain a profitable income from ads. CTR rates are also determined by conversation tracking which instead of enhancing your budget, it is achieved by more clicks by rates.\nFrom the above explanation, the readers can understand CPC or cost per click in a clear manner. They can also read through and find out ways by which their CPC, CTR can be enhanced. Research more and gain profits in an easy way.', 'What is Adwords Quality Score and What It means for You\nWhat is a Quality Score?\nThe general concept of a Quality Score was first introduced by Google. It is an aggregated estimate of PPC ad, keyword, and landing page quality based on past performance. Scores range from 1-10 (10 being the highest) and is influenced by these factors:\n- Click-through rate (keywords and display URL)\n- Ad relevance and performance (ad copy relevance, ad performance on specific sites and sites in the Display Network, and ad performance on targeted devices)\n- Keyword relevance (as an ad group in context with keyword search matches)\n- Landing page experience (relevant and original content, navigability, load time, etc.)\n- AdWords account historical performance (including geographic performance)\nWhy Do Ad Networks Need a Quality Score?\nGoogle and other major search engines cite user experience as the primary reason for their implementation of a Quality Score rating. The quality of ads, after all, affect how users interact with them and of course, the advertising revenues.\nTo understand how this strategy works, a better understanding of the different monetization models used by ad networks is needed. There are different ways in which advertisers bid for ads. In the nascent years of online advertising, the most popular is the CPM (cost per mille or thousand) model. Until now, ‘impression’ is still used as the primary unit of ad inventory even though the CPC (cost per click) is now the model of choice by many ad networks.\nThe shift from CPM to CPC has obvious benefits for advertisers but the implication for ad networks are much more significant. In competitive markets, there is a convergence between an ad inventory’s pre-defined price and the price that advertisers bid. Because of this, ad networks can predict the profits they can gain through the expected sale value of each ad inventory.\nIn the CPM and CPC models, the same ad auction process occurs, and the two models can even compete in the same ad auction. The difference is that viewable CPM (vCPM) campaigns only target the Display Network. Advertisers pay for every 1,000 ad appearance and viewable instances. With the CPC model, advertisers pay every time an ad is clicked. Google can set minimum bids – different amounts for different advertisers for the same keywords. Advertisers then set the maximum cost-per-click bid or the highest amount they are willing to pay for a single click.\nThere’s no assurance though that the highest bidder would win the auction. Any bidder can win the auction at a lower price if his keywords and ads are more relevant than the others. All he needs to do to keep his ad’s position is to have a slightly higher bid than the one behind him. But isn’t this counter-intuitive? Nope. It actually makes more sense.\nAd Rank determines if an ad can compete in an auction. Ad ranking signals that are taken into account are CTR (click-through rate), ad relevance, and landing page experience. But the main determinant for ROI and the ad quality is the CTR. It can determine how much Google can earn for a thousand impressions of a PPC (pay-per-click) ad. Let’s have an example.\nAdvertiser A bids for $3 for each click, advertiser B bids for $2, and lastly, advertiser C bids for only $1. The expected CTR for advertiser A’s ad is 1%, for advertiser B it’s 2%, and for advertiser C it’s 5%. Calculating this we get 10 expected CTR for advertiser A, 20 for advertiser B, and 50 for advertiser C. If we multiply this to the bid, Google would earn $30 from advertiser A; $40 for advertiser B; and for advertiser C it would be $50. This example shows that even if advertiser A and B had higher bids, their low CTR downplays the expected ROI. Google would select the ads that have the highest chance of performing better or risk running low performing ads – that’s where Quality score comes in.\nAd Rank is keyword-based and is triggered in real-time whereas Quality Score is an aggregate of past performances. Google reiterates that Quality Score isn’t used to determine auction-time Ad Rank but it can actually improve ad position. It quantifies the relevance of an ad based on its performance – performance that can’t be quantified unless the ad had been clicked by the user. This makes CTR as one of the most important signals for ad relevance. The more relevant the ad is, the more likely it is to be clicked.\nOn an economic standpoint, Quality Score aids ad networks in managing the risk of poor performance and optimizing profit from every advertiser. Without a Quality Score, advertisers can win ad auctions based on bids alone. Poor performance could mean a lesser expense since the advertiser would only pay for whatever few clicks his ad generates, yet the ad network still delivers one thousand impressions. A Quality Score is an assurance that no ad slot is wasted. This is since low performing advertisers will have to pay a higher CPC to account for the impressions delivered. If the ad’s low performance continues, the ad campaign would stop running until the Quality Score improves and the ad’s expected CPC converges with the pre-defined price or minimum bid.\nWhat is the Implication of a Quality Score for Advertisers?\nWe briefly discussed the implications of a low Quality Score and its implications on the ad’s CPC. But it is also evident from an advertiser’s – your perspective that ad relevance isn’t only about user experience, or even about the ad network’s profits, but also about the effective monetization of ad impressions. Aside from that, Quality Score has a direct (or indirect) correlation with the following:\n- Lower cost per click.\n- Lower cost per conversion. Cost per conversion is usually higher than the cost per click since not every click results into conversion.\n- Higher ad ranks.\n- Impression share.\nThe following approaches can help optimize Quality Score:\n- High CTRs should not be your main goal. The increase of CTRs to optimize the Quality Score doesn’t necessarily maximize ROI. It is erroneous to believe that more clicks equal more sales. It can happen, but not on the same ad budget. You should pay attention to key performance metrics like cost per sale and/or cost per lead.\n- Craft ads according to campaign goals. Aside from paying attention to the key performance metrics, you should know your ‘ideal prospect’, prospective customers that you want to optimize your ads for. This is to increase the response rate of these prospective customers.\n- Organize keywords in tight themes for different ads. Also, focus on testing ads for your top keywords until you see an improvement in the Quality Score. You may need to pause low-performing keywords so that your CTR history would be good.\n- Adplexity Coupon Code $80 Off and Walkthrough - September 23, 2019\n- SiteGround vs GoDaddy: WordPress Hosting Comparison - September 13, 2019\n- SiteGround vs Inmotion: WordPress Hosting Benchmark - August 31, 2019']	['<urn:uuid:90f41239-506c-40a3-a1b7-62a1783703d8>', '<urn:uuid:646f0ebf-2df2-44f1-8352-1e86d2d4ff28>']	open-ended	direct	verbose-and-natural	distant-from-document	multi-aspect	expert	2025-05-13T01:23:33.086345	23	138	3187
5	smoke control methods stairway kitchen difference	Smoke control methods differ between stairways and kitchens. For stairways in hotels, the primary methods are self-closing doors made of flame retardant materials and smoke curtains installed at entrances, which help prevent smoke from spreading throughout the building. In commercial kitchens, smoke control is managed through the kitchen hood fire suppression system, which must be regularly maintained and inspected to prevent grease build-up from compromising its effectiveness. The system includes specific components like extinguishing agent nozzles and proper ventilation through kitchen hoods that must be checked for holes or damage during inspections.	['Like any building, hotels can be impacted by fire at any time. From cooking fires to faulty electrical equipment, the cause of fires in hotels can often be the same as in other types of buildings.\nHowever, unlike a residence or an office building where the occupants are likely to get to know the building, its layout and what safety protocols are in place, hotel guests may only be there for the night. In the event of a fire, they may be disoriented, confused and at higher risk for injury. That’s why hotels need to put a higher emphasis on providing smoke and fire safety throughout the building.\nThere are regulations and building codes that help ensure that basic protections are in place, but these may not always be enough to protect both your guests and your property in the event of a fire.\nBasic Fire Protection\nEvery hotel should have several components that make up the fire protection plan for the building. This is to ensure the safety of the guests and the employees, as well as to help prevent damage to the building itself.\nEvery hotel room should have a smoke detector, as well as additional smoke detectors in the hallways, eating areas, kitchens, employee areas and gathering spaces. These should be hardwired and can be tied directly to other fire safety systems as well.\nSprinkler systems are another crucial component of a basic fire protection plan for hotels. They may be connected to the smoke detectors, or they may be programmed to turn on if they detect smoke particles or heat. The placement of the sprinkler heads may vary but should be present in areas of high risk, such as kitchens and storage areas.\nFire Escape Routes\nBecause guests are not always familiar with the layout of the hotel, and could become disoriented in the event of a fire, there should always be a detailed map of the hotel located on the inside of their door.\nIdeally, you would also install lights that may be able to help guide guests to an exit in a smoke-filled corridor for added safety. Well-lit exit signs, as well as emergency lights, can also help assist those exiting their room.\nSelf Closing Doors\nStairways are one of the biggest conductors of smoke in a building. They’re also often the safest way for occupants to exit the building. Having doors that can shut automatically in the event of a fire, while still allowing for manual opening and passage, can help cut down on the amount of smoke that travels through a building. Doors made of flame retardant or fire blocking materials can help allow people to shelter in place and await help.\nFlame Retardant Materials\nWhen it’s new construction, hotels should be built using as many flame retardant materials as possible, including the walls, floors and doors. Retrofits can also include flame retardant doors to help prevent the start and spread of a fire and to help allow occupants to shelter in place until help can arrive.\nAdditional Steps to Protect Your Guests and Property\nIn addition to these basic protocols, there are other steps that you can take to help ensure safety and smoke protection for hotel employees and guests.\nHave Designated Areas for Sheltering in Place\nGuests that have mobility issues and cannot utilize the stairs, or guests that find their exit cut off due to smoke and fire, need a safe place to shelter where they can be easily found and rescued. For some guests, this will mean remaining in their rooms, but for others, it may mean creating areas that can be protected from smoke and fire through the use of things like fire curtains. This can allow the occupants a safe place where they can stay until help arrives.\nBlock Smoke With Smoke Curtains\nElevators and stairways are two areas that can easily conduct smoke, allowing it to spread throughout the building. Smoke can sometimes travel faster and further than fire, and it can also do significant damage to property and health.\nBy using smoke curtains installed above the elevators and the entrances to stairways, you can help block the smoke from spreading. This can help contain the smoke to one area of the hotel, minimizing damage and helping to ensure the safety of more guests.\nProtect Large Areas', 'In any commercial kitchen, fire is no small risk or hazard. The kitchen is a high-heat environment, and it’s designed to be well-insulated, to keep in the heat required for cooking. Over time, in any sized commercial kitchen, grease and grime begin to build up, which presents problems for the kitchen hood fire suppression system. Grease will cling to the nozzles that disperse your fire suppression agent, disabling the system’s ability to suppress and extinguish kitchen fires. This is a serious fire hazard, and a key reason that you must regularly maintain your kitchen fire suppression system.\nOne of the essential components of proper kitchen maintenance is adhering to your required kitchen hood fire suppression system inspection schedule. In this article, we will discuss how often these inspections are required, what happens during a kitchen hood fire suppression system inspection, and how you can prepare your restaurant or commercial kitchen for these inspections.\nHow Often Should My Kitchen Hood Fire Suppression System Be Inspected?\nYour kitchen hood fire suppression system should be inspected every six months, according to NFPA 96. This inspection must be completed by a licensed fire protection company.\nThe inspector who performs your kitchen fire suppression system inspection will work to ensure that if there ever was a fire in your kitchen, your fire suppression system will activate and successfully put out the fire, while shutting off the gas or electric that powers your equipment.\nWhat Goes Into A Kitchen Hood Fire Suppression System Inspection?\nYour bi-annual kitchen hood fire suppression system inspection has a few purposes. The first, as you might expect, is to make sure your kitchen fire suppression system is performing as it should. Your professional inspector will also replace any faulty or old parts, as well as make any necessary changes to the system.\nFor example, if you’ve changed or upgraded the equipment in your kitchen, your system may have different requirements. This bi-annual inspection will ensure that your kitchen hood fire suppression system meets all fire code requirements, and is in great shape to protect your kitchen in the event that there is a fire.\nHere’s a look at some of the most common tasks your inspector will perform while completing your kitchen hood fire suppression system inspection:\n- All extinguishing agent nozzles are discharged and inspected for build-up\n- All nozzle caps are replaced\n- Extinguishing lines are blown out to ensure there is no blockage\n- Manual alarm pull station is pulled to ensure it is functioning well\n- Inspector checks pressure gauges to ensure proper PSI\n- Gas shut off valve and microswitch are tested\n- Inspector checks to make sure fans are operating properly\n- Ensures all system components are compatible\n- Checks for holes in hoods\n- Ensures your system is using the proper cartridge for your kitchen\n- Fusible links of the system are cut to see if the system activates properly\n- Check that gas and electrical shut off when the system activates\n- One the system has been activated, the inspector will install new fusible links with the date stamped on them. This serves as proof of service for the fire marshal inspection.\n- Now, your inspection certification can be filed with the local fire authority.\n- The inspector will tag the system showing certification was completed by a professional, and according to regulations.\n- Your inspector will leave you with a Commercial Cooking System Service and Inspection report, and will send one to your local fire department.\nOnce your kitchen fire hood suppression system inspection is complete, your commercial kitchen is set for another six months. While you should continue cleaning your kitchen fire hood regularly, and complete monthly visual inspections, your system currently meets fire requirements, and should keep your kitchen safe until it’s time for your next inspection.\nHow Should I Prepare for a Kitchen Hood Fire Suppression System Inspection?\nIf you have a kitchen hood fire suppression system inspection coming up, you might be wondering what you need to do to prepare.\nClean Your Kitchen Hood Thoroughly\nFirst and foremost, it’s important to clean. You should already be cleaning your kitchen hood regularly, but now is a good time to complete a deep clean to remove any grease or build-up that could affect the inspector’s ability to inspect your kitchen hood fire suppression system.\nKnow When Your Inspector Will Arrive\nHave a clear picture of the inspection timeline. When will your inspector arrive, and how long does he or she expect that inspection will take?\nPrepare the Kitchen For Inspection\nTo complete this inspection, the kitchen cannot be in use. The area should be clear of pots and pans, and everything should be sufficiently cooled down so that the inspector can safely access all components of the system.\nIt’s a good idea to schedule your kitchen hood fire suppression system inspection on an off-day, or on off-hours, so you have minimal staff on-site. During the inspection:\n- You cannot use the kitchen\n- Any staff present must be aware of the inspection\n- The fire alarm will be temporarily disabled to properly complete the inspection\nIt’s important to understand these requirements for your kitchen hood fire suppression system inspection, so your kitchen is ready for the inspector. The better prepared you are, the faster your inspection will go.\nIf it’s time for your kitchen hood fire suppression system inspection, the Vanguard Fire & Security team is here to help. Whether you’re looking for an inspector, or you’re just not sure how to prepare for your inspection, our team has the expertise and the capability to get your inspection scheduled now, and answer any questions you might have. Give us a call at 800-444-8719 or contact us online today to schedule your inspection!']	['<urn:uuid:37b4bfbc-bd58-4bad-8f40-e8babe0c85b3>', '<urn:uuid:ef914cf3-24e8-47b0-ab88-0b6b6c776961>']	open-ended	with-premise	short-search-query	distant-from-document	comparison	novice	2025-05-13T01:23:33.086345	6	92	1684
6	mount sharp mars physical properties ancient lakes evidence formation history	Mount Sharp, located in Gale Crater on Mars, shows surprising physical properties and historical evidence of ancient lakes. The mountain's lower layers are unexpectedly porous, with a bulk density of 1680 kg per cubic metre, much less than expected 2810 kg/m³, suggesting less burial and compaction than previously thought. The crater also contains evidence of an ancient lake that existed over 3 billion years ago. This lake underwent periods of drying and recovery, forming salt deposits similar to those found in Earth's Altiplano region. The crater's lake was estimated to be only about 8% as salty as Earth's oceans, making it potentially habitable for microbial life.	['NASA’s Mars Curiosity rover has successfully taken measurements of the density of a mountain on the red planet and finds it more porous than originally thought.\n|Curiosity rover used the Mars Hand Lens Imager to capture a set of 55 high-resolution images, which were stitched together to create this full-color self-portrait. The mosaic shows the rover at ‘Rocknest,’ the spot in Gale Crater where the mission’s first scoop sampling took place. (Image: NASA/JPL-Caltech/Malin Space Science Systems)|\nocks on Mars are more porous and less compacted than scientists expected, according to a study that used data from NASA’s Curiosity rover.\nResearchers, including those from Arizona State University (ASU) in the US, measured the density of rock layers in 154-kilometre-wide Gale Crater on Mars.\n“What we were able to do is measure the bulk density of the material in Gale Crater,” said Travis Gabriel, a graduate student at Arizona State University.\nFindings of the study\nThe findings, published in the journal Science, show that the layers are more porous than scientists had suspected.\nThe discovery also gives scientists a novel technique to use in the future as the rover continues its trek across the crater and up Mount Sharp, a five-kilometre-high mountain at its centre.\nNASA’s Mars rover Curiosity captured this composite image, which looks toward the higher regions of Mount Sharp, on September 9, 2015.(Image: NASA/JPL-Caltech/MSSS)\nAs Curiosity ascended Mount Sharp, the gravitational force increased – but not as much as scientists expected.\n“The lower levels of Mount Sharp are surprisingly porous,” said Kevin Lewis of Johns Hopkins University in the US.\n“We know the bottom layers of the mountain were buried over time. That compacts them, making them denser. But this finding suggests they weren’t buried by as much material as we thought,” Lewis said.\nHow did the scientists reach this conclusion?\n1. Gabriel worked on computing what the grain density should be for the rocks and ancient lake-bed sediments the rover has been driving over.\n“Working from the rocks’ mineral abundances as determined by the chemistry and mineralogy instrument, we estimated a grain density of 2810 kilogrammes per cubic metre,” he said in a statement.\n“However, the bulk density that came out of our study is a lot less – 1680 kilogrammes per cubic metre,” said Gabriel.\n2. The much lower figure shows that the rocks have a reduced density, most likely resulting from the rocks being more porous.\n3. This means the rocks have been compressed less than scientists have thought.\n4. The engineering sensors used in the study were accelerometers, much like those found in every smartphone to determine its orientation and motion.\n5. Curiosity’s sensors do the same, but with much greater precision, helping engineers and mission controllers navigate the rover across the Martian surface.\n6. Curiosity’s accelerometers can also be used as a gravimeter – an instrument that can measure gravity – to reveal secrets about Martian geology.\n7. Even when Curiosity is stationary, the accelerometers are constantly detecting the slight changes in gravity on Mars, as Curiosity rolls further up Mount Sharp.\n8. Researchers used over 700 measurements from Curiosity’s accelerometers taken between October 2012 and June 2017. These were calibrated to filter out ‘noise’, such as the effects of temperature and the tilt of the rover during its climb.\n9. The calculations were then compared to models of Mars’ gravity fields to ensure accuracy.\nThere are many mountains within craters or canyons on Mars, but few approach the scale of Mount Sharp.\nScientists still aren’t sure how the mountain grew inside of Gale Crater. One idea is that the crater was once filled with sediment. How much of it was filled remains a source of debate, but the thinking is that many millions of years of wind and erosion eventually excavated the mountain.\nA computer generated image showing Mount Sharp rising from the center of the Gale Crater.(Image: NASA/JPL-Caltech/ASU/UA)\nIf the crater had been filled to the brim, all that material should have pressed down, or compacted, the many layers of fine-grained sediment beneath it.\nBut the new paper suggests Mount Sharp’s lower layers have been compacted by only a half-mile to a mile (1 to 2 kilometers) – much less than if the crater had been completely filled.\n“There are still many questions about how Mount Sharp developed, but this paper adds an important piece to the puzzle,” said study co-author Ashwin Vasavada, Curiosity’s project scientist at NASA’s Jet Propulsion Laboratory in Pasadena, California.\nMars and Earth are planetary siblings\nLewis said that Mars holds plenty of mystery beyond Mount Sharp. Its landscape is like Earth’s, but sculpted more by wind and blowing sand than by water. They’re planetary siblings, at once familiar and starkly different.\n“To me, Mars is the uncanny valley of Earth,” Lewis said. “It’s similar but was shaped by different processes. It feels so unnatural to our terrestrial experience.”', 'New York: Researchers have discovered evidence of an ancient lake on Mars that likely represents some of the last potentially habitable surface water ever to exist on the red planet.\nResearchers at the University of Colorado at Boulder, examined an 18-square-mile chloride salt deposit in the planet’s Meridiani region near the Mars Opportunity rover’s landing site.\nLarge-scale salt deposits are considered to be evidence of evaporated bodies of water.\n“This was a long-lived lake, and we were able to put a very good time boundary on its maximum age,” said Brian Hynek, a research associate at the Laboratory for Atmospheric and Space Physics (LASP) at CU-Boulder and lead author of the study.\n“We can be pretty certain that this is one of the last instances of a sizeable lake on Mars,” Hynek emphsised.\nDigital terrain mapping and mineralogical analysis of the features surrounding the deposit indicate that this one-time lake bed is no older than 3.6 billion years old, well after the time period when Mars is thought to have been warm enough to sustain large amounts of surface water planet-wide.\nPlanetary scientists believe that the solar system was formed approximately 4.6 billion years ago.\nBased on the extent and thickness of the salt, the researchers estimate that the lake was only about eight percent as salty as the Earth’s oceans and therefore may have been hospitable to microbial life.\n“By salinity alone, it certainly seems as though this lake would have been habitable throughout much of its existence,” Hynek dded.\nMars once had salt lakes similar to the ones on Earth and has gone through wet and dry periods, according to a new study.\nThe researchers from Texas A&M University College in the US examined Mars’ geological terrains from Gale Crater, an immense 95-mile-wide rocky basin that is being explored by the NASA Curiosity rover since 2012 as part of the MSL (Mars Science Laboratory) mission, according to the study published in the journal Nature Geoscience.\nThe results show that the lake, which was present in Gale Crater over three billion years ago underwent a drying episode, potentially linked to the global drying of Mars.\nGale Crater was formed about 3.6 billion years ago when a meteor hit Mars.\n“Since then, its geological terrains have recorded the history of Mars, and studies have shown Gale Crater reveals signs that liquid water was present over its history, which is a key ingredient of microbial life as we know it,” said study co-author Marion Nachon from Texas A&M University.\n“During these drying periods, salt ponds eventually formed. It is difficult to say exactly how large these ponds were, but the lake in Gale Crater was present for long periods of time – from at least hundreds of years to perhaps tens of thousands of years,” Nachon said.\nAccording to the researchers, Mars probably became drier over time, and the planet lost its planetary magnetic field, which left the atmosphere exposed to be stripped by solar wind and radiation over millions of years.\n“With the atmosphere becoming thinner, the pressure at the surface became lesser, and the conditions for liquid water to be stable at the surface were not fulfilled anymore, so liquid water became unsustainable and evaporated,” Nachon said.\nThe salt ponds on Mars are believed to be similar to some found on Earth, especially those in a region called Altiplano, which is near the Bolivia-Peru border.\nNachon said that the Altiplano is an arid, high-altitude plateau where rivers and streams from mountain ranges “do not flow to the sea but lead to closed basins, similar to what used to happen at Gale Crater on Mars.\n“This hydrology creates lakes with water levels heavily influenced by climate. During the arid periods Altiplano lakes become shallow due to evaporation, and some even dry up entirely,” she said.\nNachon added that the study shows that the ancient lake in Gale Crater underwent at least one episode of drying before “recovering.”\nIt’s also possible that the lake was segmented into separate ponds, where some of the ponds could have undergone more evaporation.']	['<urn:uuid:3d69fd02-5d5f-4236-88e6-4261b48c25a4>', '<urn:uuid:a6974c67-3dbe-44cd-97f9-e667684b957e>']	open-ended	direct	long-search-query	distant-from-document	multi-aspect	novice	2025-05-13T01:23:33.086345	10	106	1489
7	How do the approaches to controlling individuals differ between modern democratic societies and prison systems, particularly in terms of surveillance and discipline?	Both modern society and prisons employ similar mechanisms of surveillance and discipline, though in different ways. In democratic societies, according to Power of the Powerless, control is exercised through 'subtle and refined' manipulative mechanisms within the industrial-consumer society, rather than through brutal methods. This parallels Foucault's concept of the 'carceral archipelago' where disciplinary control expanded from prisons into the entire social body. In prisons, discipline works through constant supervision to produce obedient bodies, while in broader society, similar disciplinary techniques spread through institutions like schools, families, workshops and the army. Both systems aim to normalize individuals and reform deviance, though prisons do so more explicitly through penitentiary rationality while society does it through more subtle social pressures.	['Resisting Fear – Václav Havel’s legacy and the need for existential revolution in the 21st century\nBy Jiří Přibáň\n‘It would appear that the traditional parliamentary democracies can offer no fundamental opposition to the automatism of technological civilization and the industrial-consumer society, for they, too, are being dragged helplessly along by it. They are manipulated in ways that are infinitely more subtle and refined than the brutal methods used in the post-totalitarian societies.’\nThis passage from Havel’s Power of the Powerless, is a favourite one for critics speaking against ‘the system’ – whether it is a civilizational or specific political system. They use it to add weight to their apocalyptical vision of the societal disintegration and an ever deepening sense of crisis, which Havel himself describes as an “overall failure of the modern man.” Their ranks are further joined by a variety of pioneers advocating new principles and seeking comfort in naive fantasies that reassure them that the current societal crisis – which exhibits itself in various forms starting from financial market collapse and growing economic inequality, to the breakdown of democratic politics, mass migration and ecological catastrophe – has a moral solution.\nPower of the Powerless however, must not be read as some form an analytical critique of society, an emancipation programme or a guide to social activism. It is meaningless to attempt to classify the text in political, ideological or philosophical categories. Who is now interested if Havel was a “right-winger” supporting, and supported by, the West, or if he was a “left-winger” dreaming of an alternative to parliamentary democracy and the Market? After all, how necessary is it to argue whether emphasizing ecological problems of humanity is an exhibition of conservative or radical tendencies? Isn’t the difference between political realism and idealism, which has now become an intellectual cornerstone of politicians and their advisers, simply a symptom of political and intellectual helplessness?\nVáclav Havel, 1988\nPower of the Powerless requires a different reading. In the text, Havel’s dramatic talent and sense for paradox allowed him to formulate a daring appeal for an existential revolution, while simultaneously allowing him to refer to Masaryk’s everyday “little work.”\nIt is these logical inconsistencies that contain the argumentative and literary magic that allow the die-hard democrat Havel to express his fears about human freedom in the language of Martin Heidegger, who was not the least bit concerned with democratic society and its freedoms. Even the subject of the essay is paradoxical, for power is attributed to those who are in their powerlessness pushed to the very edge of society, if not beyond.\nThe interplay of contradiction often leads readers and interpreters of Power of the Powerless to rather unoriginal thoughts about the “powerlessness of the powerful”, with its somewhat automatically emerging corollary of “power of the powerful” and “powerlessness of the powerless.” Understanding Power of the Powerless, both in the literal and metaphorical sense, however, requires first and foremost understanding the power of paradoxes, which creates the bases of Havel´s dramatic text structure and through which he describes the meaning of dissent as an existential revolution in any – also in open and democratic – society.\nUnderstanding the power of paradoxes and contradictions of modern thought and civilization, opens the door for a clearer understanding of the dismal situation in Central Europe, where only a quarter century after the fall of the Berlin Wall, the democratization is in retreat. Instead, Hungarian Prime minister Viktor Orban has initiated a construction of what he terms a “national state founded on labour”, and what the victor of polish parliamentary elections Jaroslaw Kaczynski refers to as the “reconstruction of the state” and the establishment of a “fourth republic” which will put an end to the supposedly immoral liberal regime constituted after 1989.\nCzech President Milos Zeman belongs to this group of leaders. He celebrated the anniversary of the Velvet Revolution surrounded by xenophobic politicians and activists, and his Christmas time address to the nation concluded with a controversial statement: “This country is ours. And this country is not, and cannot be, for everyone.” Not a word was dedicated to Christmas or (Baby) Jesus, who coincidentally happened to be a political refugee upon his birth as well. Although nobody is suggesting that this country should be “for everyone”, the president’s verbal gymnastics emphasized the fact that “we” does not mean “them” and therefore we can refuse “them” even elementary aid.\nInstead of an existential revolution, we now have a situation in Europe where we are witnessing a paradoxical counterrevolution that attempts to upgrade politics into a question of an existential life-and-death decisions. Yearning for power is as intense as the hatred of modern man and his aspirations for civil rights and democratic government. Ideas of the nation as a collective of blood and soil, cultural heritage as a hammer against the “foreigners”, the state as a corporation, public services as a private enterprise or science and education as a branch of industry are all different forms of the same manifestation of hatred.\nInstead of freedom fear reigns supreme, and instead of civilized political discourse we are witnessing increasingly loud and popular deliberations about the end of civilization, deliberations that are now joined by a wide range of people, from volunteer fire-fighters to university professors. The basic paradox of this mobilization politics of fear and the risks it brings about were accurately described by Hannah Arendt in the classical study on the Origins of Totalitarianism in these words: “Even the emergence of totalitarian governments is a phenomenon within, not outside, our civilization The danger is that a global, universally interrelated civilization may produce barbarians from its own midst.”\nTo understand our current social crisis, which is not just a political or economic crisis, we must contemplate precisely this paradox of barbarianism coming from within our civilization. Tied to this paradox is the call for an existential revolution, which however must not become an ideology of political existentialism in which the absoluteness of the collective will promises to solve the absurdities of individual life, and where political power demands the right to decide about the life and death of its citizens or even entire groups of people.\nAccording to Havel, an existential revolution must predate even the moral and political reconstruction of society. It consists of resistance (and finding alternatives) against the “self-propelled” society that relies on the omnipresent technologies which further cement their power. It is a revolution where – in Havel’s words – lies the intention of life against the intentions of the system.\nHavel´s vision of a determined revolt of life against the system is markedly similar to Habermas’s heroic, albeit futile attempt to bestow legitimacy of contemporary society with its functionally differentiated system of politics, economics, law or science in the now mythical lifeworld (Lebenswelt), with which in the last century the scientifically strict phenomenology and the poetically speculative philosophy connected the hopes of humanism. It is a revolt against a world in which the system is just another name for totalitarianism that creates itself and inhibits the forces of life from being realized.\nIf, however, the concept of an existential revolution (and with it the power of the powerless) should still retain any meaning today, it must not be simply juggled around in front of the veil of society, on the contrary, it should be understood as a battle of vocabularies used right on its stage. It must not, therefore, be a battle between life and the system in which life would triumph as a fundamental quality of any system that seeks to bestow legitimacy upon itself. It is so because life is not situated outside economic, political, judicial or technological structures but it manifests itself within them. Every contest for legitimacy therefore lies in the struggle for language through which these structures describe themselves, and not in some pre-political ability of the human reason to rupture these manipulative structures which the reason itself has created.\nSelf-creation of the system, in contemporary sociology termed autopoiesis, is not based on maintaining of social totality, but on validating and expanding the differentiation of modern society. Political power of the powerless therefore does not lie in an act of the powerless in which the cloak of the world would be theatrically torn off and, through their life in truth, would show the “true face” of power veiled under an ideology trumpeted from the podiums of what the German Jesuit Franz Neumayr as early as the 18th century dubbed the theatrum politicum. Everyone, including the dissidents, is part of this theatrical performance, because the participation in it cannot be predetermined on the basis of some “life philosophy” or even personal or collective will.\nSocietal crisis does not consist of simple contradiction of poetic life and an autopoietic system. Havel’s call for an existential revolution must accordingly be specified and in a way radicalised because an existential revolution can no longer be classified as a record of human intentions guided by truth against the fallacies of complex civilizational “self-totality” that manipulates man with the omnipresent temptations of consumer lifestyle and the tranquilizing bureaucratic performance. The distinction between “living in truth” and “social deceit” is not sufficient in describing the complexity of power and the strategies it employs in contemporary society.\nHavel’s essay is even more pertinent today due to its ability to relativize every seemingly absolute vocabulary, whether we are talking about politics and its ideologies, law and codification of society into legal/illegal dichotomy, economy with its rational laws and wholly irrational consumption, or the technologies mindlessly monitoring our own performance. And precisely from this relativization does the resistance against the politics of absolute fear and hatred emerge.\nThe existential revolution and dissent connected to it is thus not morally fundamental but autopoietic, self-actualizing resistance against all politics based on an existential threat and understood as a battle for life and survival of the group.\nIn his 80’s song Královna noci [Night Queen] Vladimír Merta sang of the “armies of the day” and “armies of the night” and a man who finds himself “forever and ever” on the threshold of those two worlds and wants to escape from “their paradise” but does not know where to go. Merta captured the essence of what Jan Patočka described in his Kacířských esejích o filosofii dějin [Heretical essays in the Philosophy of History] as a drama of freedom fighting the onslaught of modern civilization which relentlessly mobilizes man as to make him fear for his very life and keep him in the shackles of fear.\nHow are we then to understand Havel’s conception of an existential revolution as a revolution of human rights that must be defended as a “truth”?\nTo better understand these “truths” let us remind ourselves of a meeting with dissidents of Eastern Europe in Paris organized by Michel Foucault in June 1977 at the time of Leonid Brezhnev’s official visit to France.\nThis gesture of solidarity and resistance has a broader context, since only a while back, Foucault was invited for a television discussion on the topic of “What is the future of man?” in order to elaborate on his just published first part of The History of Sexuality. Foucault, however, refused to talk about his work, instead choosing to discuss a completely different book, a book without an author that could publicly defend it. The book in question was a compilation of reel-to-reel tape recordings known as the Stern process, which was smuggled out of the Soviet Union and then published under the literary title of An Ordinary Process in the USSR. [Translator note: Book officially available under title: “The USSR vs. Dr. Michael Stern The Only Tape Recording of a Trial Smuggled Out of the Soviet Union”]\nThe Soviet doctor Mikhail Stern was arrested in 1974 following his refusal to submit to KGB pressures to dissuade his sons from their intention to emigrate to Israel. In a show trial he was accused of accepting bribes and was sentenced to eight years in prison, despite the fact that witnesses repeatedly testified to his innocence. The monstrous injustice committed by the communist regime was self-evident, but Foucault was interested in the witnesses of this proceeding – ordinary people that overcame their fear in the face of a repressive regime and came forward to “tell the truth”, and in defiance of the physical oppression of the state were able to say NO.\nThe truth is consequently not something abstract or objective, but is always placed within some context, whether it concerns state violence and the judicial system, or a scientific laboratory and a philosophical seminar. It is not just about what is said, but in what context it is said, and with what consequences it is being said, which people are convinced to be a truth that must be spoken and published.\nFoucault precisely rejected the primitive Marxism and its idea of an economic base and a political superstructure, in which there is a battle between the exploiters and the exploited, and in which only one group is the carrier of historical truths and progress. As became evident during Foucault’s historical TV debate with Noam Chomsky, Foucault likewise rejected the moral crusading and naïve ideas of parts of the left about rights and justice as natural attributes of humanity which every government must adhere to, and about which political struggle takes place and in which universal humanity manifests itself. Instead of these metaphysical projects, Foucault instead focused on what he coined the micro-physics of power.\nDuring a detailed reading of Havel’s Power of the Powerless we discover the same fascination with power that was typical for Foucault’s philosophical endeavours to which he devoted himself during the 70’s and 80’s. For both, power and powerlessness was a social, not a political problem. Power and powerlessness were a momentary constellation and an outcome of capabilities and strategies which man encounters at every corner in our planetary civilization, whether it concerns the consumer society of Western civilization or the post-totalitarian regimes that are, in vain, struggling for a comparable level of legitimacy as those with an ideology of economic growth and consumerism connected with bureaucratic and technical performance.\nHavel divides power into a physical and metaphysical realm, that is, a manipulative mechanism of oversight over all citizens and the ideological justification of that manipulation. Post-totalitarian systems no longer propagating an ideology that demands the unity of the world proletariat, instead they seek to coerce the manager and even the ordinary fruit vendor by means of normative societal pressures to conform in order not to have “have problems”. Ideology is thus a bridge between man and the system, but in reality it is the “self-propelled” nature of the system that creates this “life in a lie” which must be rejected in the name of the “truth”.\nMoreover, Havel doesn’t call upon the fictitious character of the fruit vendor to do anything less courageous than Foucault admired about the witnesses of Stern’s “ordinary process” that is to say NO. Even this statement presents a peculiar microphysics of power; which is tactical and strategic, because it realizes that this kind of truth presents first and foremost a resistance the stronger, the more it seems powerless in the face of state violence and the judicial system.\nNormalization isn’t just a specific historical period of Czech society after 1968 connected with the entrenching of the political regime, but a much more general strategy of power in modern society. The dissident is then not a physicist of power guaranteeing the functionality of the political system, nor it is metaphysic guiding the ideological struggle about what ensures the legitimacy of such system. He is a microphysicist of power, who with his NO forces the system to react, thereby strategically and effectively delegitimizing it.\nDissent is a microphysics of power of the powerless that can solely breach the physics and metaphysics of state power. The dissent’s existential revolution is chiefly a rebellion against political existentialism that reduces politics to problems of bare life and survival in perpetual fear – and the feelings of hatred that emanate from such fear. It is a constant reminder that every politics is existential only to the degree that man finds meaning in his own existence. The meaning of our existence is nevertheless not depleted in politics, and therefore no politics is, and must not become, a determining to the meaning of our lives.\nJiří Přibáň is a professor of law at the Cardiff University. He was also visiting professor or scholar at European University Institute in Florence, New York University (Prague Office), University of California in Berkeley, University of San Francisco, University of Pretoria, The Flemish Academy in Brussels and University of New South Wales, Sydney. Jiří Přibáň has published extensively in the areas of social theory and sociology of law, legal philosophy, constitutional and European comparative law, and theory of human rights. He is an editor of the Journal of Law and Society, member of the Editorial Board of New Perspectives and a regular contributor to the Czech and international media.\nThis article has originally appeared in Czech in the Salon section of Novinky.cz.\nSources of the images: vhlf.org,terapeak.com, timesofisrael.com, roarmag.org', '“The Carceral” in Foucault’s Discipline and Punish: the birth of the prison, a book by Michel Foucault, first published in 1975, then later edited in English in 1977 still continues to rivet attention 35 years after it was written. It is evident to believe that it is still revolutionary in its findings. Michel Foucault was a French philosopher, sociologist and historian. The professor of history and systems of social thought at college de France, Foucault is widely recognised as a leading social theorist.\nDiscipline and Punish continues to provide insights and suggested solutions that appear in the penal system. Foucault’s point is to show how significantly the penal system changed in 80 years and details the history of the French penal system with the interpretation of historical events identifying the domination of human spirits. He argues, in the later part of the 18th century the focus of punishment began to shift from the body to the soul or mind of the offender to discipline them.Order now\nHe described discipline as a type of power. Prisons became more than just places where liberty was deprived. Furthermore, the closing section of the book is the main focus of my review and is entitled, simply, “The carceral”. He investigated the massive shift from corporal to carceral punishment. Foucault analysed, our persistent and reasoned need to normalise individuals, to punish and reform deviance within society through discipline.\nFoucault tries to account for universal and historical developments and the emergence of a disciplined society “the carceral archipelago” in which all of institutional life is characterised by surveillance and discipline and in which delinquent and abnormal behaviour are subjected to scientific investigation. Ultimately, Discipline and Punish is a call to arms, a predict for the future, and a study of the past all organised into one elegant text. The chapter opens with an explanation of a particular model French prison Mettray, possibly for young offenders.\nFoucault describes how this was more than just a prison which housed minors who had committed a crime and minors who had not committed a crime but were without normal family relations. He describes how Mettray and other institutions of social life such as the school, the family, the workshop, the army and the prison were interrelated by the development of similar disciplinary techniques and shared certain similar features. He described them as places in which one’s action came under the direction of another’s will.\nAt Mettray, the timetable stressed physical exercise, hard work, and the organised recording of results. The aim was to produce “strong, skilled agricultural workers”. The prison trained other professionals too, focusing on techniques of “pure discipline”, rather than science, although Psychology was to develop in that institution. He explains “the disciplinary technique became a discipline which also had its school” (Foucault, 1977: 295). Traditionally, the school has been understood as a limited, relatively self contained and independent institution.\nAccording to Foucault, embodied in it was a “carceral continuum” a diverse range of institutions given over to the surveillance for the training and the normalization of individuals. He explained Penitentiary rationality as the central part of the carceral system. Nevertheless, the point that Foucault emphasises throughout is that discipline works under surveillance upon one’s actions and engages one’s will to perform. He described this process of constant supervision benefited in the production of obedient and capable bodies.\nHe believes it is this which not only helps in reforming criminals but also to the education of students, management of workers and training of modern army. In his eyes, Mettray represented the birth of a new kind of supervision. He described Mettray as the punitive model (Foucault, 1977: 296). Foucault believed that the discipline of individuals will be achieved through microphysics of regulation. Foucault argues that surveillance attempts to transform individuals through observation and discipline and individuals therefore start to internally control themselves. Surveillance was seen to hold the key to reform.\nHe argued, “this great carceral network reaches all the disciplinary mechanisms that function throughout society” (Foucault, 1977: 298). Throughout the last part of Discipline and Punish Foucault suggests that a “carceral continuum” runs through modern society. He believed the mechanisms of discipline and power that control the prisoner’s life also control that of the citizen. Foucault’s account of the development of the prison and the carceral system makes it clear that society has a “carceral texture” and is penetrated by the same mechanisms that function within the prison.\nSimilarly, through its construction of delinquency, the prison helps to control and regulate class conflict and popular misconduct. Whereas, “the carceral naturalizes the legal power to punish, as it legalizes the technical power to discipline” (Foucault, 1977:303). For Foucault, an investigation of appearance of the prison in the early 18th century is actually a means of exploring the much wider and more contemporary themes of how domination is achieved and individuals are socially constructed in the modern world. Foucault related how the penal system with its outreaching arms affects society as a whole.\nHe believed other governmental programs, such as welfare and new educational techniques, expanded from the penal system. He called this expansion of disciplinary control the “carceral archipelago”. It created a whole society of docile bodies submitting to the will of the state. He argued, “we have seen that, in penal justice, the prison transformed the punitive procedure into a penitentiary technique; the carceral archipelago transported this technique from penal institutions to the entire social body” (Foucault, 1977:298).\nFinally, it gives an increase to the theatrical suggestion which Foucault refuses to accept is that the prison is the symbol of our “disciplinary society”. This however, does not mean that society is like a prison and everybody in it is targeted, what he argues is that society, like prison and other institutions keeps individuals under surveillance in order to keep peace and the birth of the prison which Foucault describes is in fact the progress of contemporary society itself.\nFoucault argues, classicists such as Beccaria saw retribution as a process for requalifying individuals as juridical subjects whereas Foucault believes that law breakers have placed themselves outside the society by committing an offence but the penal process should aim at returning them back as law abiding citizens (Cavadino & Dignan, 2002: 45). He sees the carceral as the answer because here he believes the offender is not outside the law and society.\nIn Foucault’s words, “the carceral with its far reaching networks, allows the recruitment of major delinquents and transforms their lives into disciplinary careers” (Foucault, 1977:300). Within sociology, the work of Michel Foucault has completed a different understanding of power and discipline compared to analysis deriving from Weberian and Marxist theory. For Foucault the modern prison, with its mechanisms of total surveillance, represented a new form of knowledge and power.\nFor Marx, the class struggle was the main problem in society as he believes the rich (bourgeoisie) get richer and the poor (proletariat) get poorer. Similarly, according to Foucault the ruling class used criminality as a way of preventing confrontations that could lead to revolution. He believed the ruling class used the law to diminish the power of these uprisings and the dominant class used the delinquent class as a means of profiting themselves (Smart, 1983).']	['<urn:uuid:48786a39-5f3a-4828-806c-938fbc79b1be>', '<urn:uuid:1258817e-0f64-4617-91bf-046cf10e2340>']	open-ended	direct	verbose-and-natural	distant-from-document	comparison	expert	2025-05-13T01:23:33.086345	22	117	4067
8	supreme court title capitalization rules when	According to the Bluebook style, in legal document headings, you should capitalize the first word and any word immediately following a colon. Articles, conjunctions, and prepositions of four or fewer letters should not be capitalized unless they are the first word or follow a colon.	['I learned something interesting recently. As much as you think you know about something, every once in a while it is good to check your resources. While I covered this topic according to the Gregg Reference Manual in the July 12 NALS docket in an article entitled “Things Are Coming to a Head(ing)” about exceptions to the “capitalize everything except articles, conjunctions, and prepositions shorter than four letters” rule, a recent search through The Bluebook showed me that rule was not correct for headings in a legal document done in “Bluebook style.” According to Section 8 of The Bluebook, in headings and titles, the first word in the heading or title and the word immediately following a colon in a heading or title should be capitalized. However, do not capitalize articles, conjunctions, and prepositions of four or fewer letters unless they fit the criteria in the immediately preceding sentence (they are the first word of the title or immediately follow a colon).\nThe Bluebook does, however, refer you to The Chicago Manual of Style or the Government Printing Office Style Manual if there are questions about specific capitalization issues not answered in The Bluebook. Here are the rules on capitalization according to The Bluebook:\n- Always capitalize nouns identifying specific persons, officials, groups, government offices, or governmental bodies.\n- The Securities and Exchange Commission was closed for the holiday.\n- Members of Congress worked late into the night.\n- The President lives in the White House.\n- The congressional hearings seemed as if they would never end\n- The presidential veto is a tool available to the President.\n- Exceptions (you know there had to be some):\nAct is capitalized when referring to a specific act.\nThe Civil Rights Act was enacted in 1964.\nCircuit is capitalized when used with the name or number of the circuit.\nArizona is part of the Ninth Circuit.\n- The circuit court will not rule on that issue.\nCode is capitalized when referring to a specific code.\nThe Internal Revenue Code\nConstitution is capitalized when referring to the United States Constitution or naming any constitution in full.\n- Court is capitalized when referring to the United States Supreme Court, when referring to any court in full, or when referring to the Court where your documents will be filed.\nThe Miranda court decided . . .\n- The Ninth Circuit Court of Appeals . . .\n- This Court should deny the Motion to Dismiss.\nFederal is capitalized when the word it modifies is capitalized.\nThe Federal Constitution establishes the executive, legislative, and judicial branches of government.\n- High on the list of Congress’s priorities is federal spending.\nJudge or Justice is capitalized when referring to a specific judge or justice by name or when referring to a Justice of the United States Supreme Court.\nDid you know that Justice Sandra Day O’Connor sat as a judge in the Maricopa County Superior Court in Arizona?\n- The judge ruled against defendants in the White case.\nState is capitalized when it is part of the full title of the state, if the word it modifies is capitalized, or when referring to the state as a party to a litigation or a governmental actor.\nThe State of California was the first to allow the use of medical marijuana.\n- He brought an action against the State for unlawful imprisonment.\nI guess I will have to read through The Bluebook\nagain just for good measure to see what other “rules” need to be adjusted.\nKathy Sieckman, PP, PLS-SC, ACP, has been a member of NALS for over 30 years, is the current President of NALS of Phoenix, and is the Vice Chair of the NALS Editorial Board. Kathy has a blog on proofreading tips at http://proofthatblog.com. If you have specific grammar issues you would like covered in future issues, please send them to Kathy at firstname.lastname@example.org.']	['<urn:uuid:c4735c72-b484-4009-81d6-a01245365db1>']	open-ended	with-premise	short-search-query	distant-from-document	single-doc	expert	2025-05-13T01:23:33.086345	6	45	648
9	what is cybersecurity risk management process and how does it relate to social engineering attacks	A cybersecurity risk management process is a documented approach to identify, rank, and address risks from cyberattacks, which needs regular updates. It involves assessing threats (persons, circumstances, or events that could cause harm) and vulnerabilities (weaknesses that allow threats to materialize), starting with the most critical assets. In relation to social engineering attacks, which are a significant threat that exploits human psychology rather than technical vulnerabilities, the risk management process should specifically address threats like phishing, pretexting, and baiting attacks. Organizations must assess these risks and implement appropriate strategies including education, awareness training, multifactor authentication, and strict access controls to protect against such attacks.	['Now that you understand why you need a cybersecurity strategy how to create your cybersecurity policy, and how to create and manage your firm’s inventory, let’s explore the third step you can take to jump-start your cybersecurity strategy: identifying and creating a process to manage your advisory firm’s cybersecurity risk.\nNote: Before you can begin managing your cybersecurity risk, you need to create your cybersecurity policy and inventory. If you haven’t finished these steps, you won’t have the necessary pieces in place to manage your cybersecurity risk.\nWhat is a cybersecurity risk management process?\nA cybersecurity risk management process documents the way that you will identify, rank, and address risks to your business from cyberattacks. This document should be updated regularly.\nWho creates the process?\nYou will need to designate a “cybersecurity manager (CSM) or cybersecurity officer (CSO)” for your firm. This person will work with stakeholders to pull together the content of the cybersecurity risk management process, get it approved, and keep it up to date.\nHow to start\nBefore we dive in, let’s go over some quick definitions:\n- Risk is a measure of the chance that something bad might happen.\n- A threat is a person, circumstance, or event that could make that bad thing\n- A vulnerability is a weakness that would allow that thing to happen.\nSince you now know your inventory, you can begin to determine your risk exposure by looking at threats and vulnerabilities. You should assess your risk methodically, starting from your most critical inventory and then moving to the least critical. Ask yourself the following questions:\n- What bad things could happen?\n- How likely is it that these things could occur?\n- What are the consequences if they do occur?\n- What can I do to reduce the risk of them happening?\nResources that can help you identify and assess risk\nIn some cases, it may be necessary for an advisor to get help in the form of an outside IT company, but in most cases, you simply need to have a good understanding of what can happen in the context of your business and what the consequences would be.\nA great way to keep up on threats to the financial-services industry is to subscribe to the Financial Services Information Sharing and Analysis Center (FS-ISAC). This service offers daily alerts and provides a collaborative community that may be able to help you to understand vulnerabilities.\nDepending on your business, you may have the capabilities in your firm to assess the risk to your more technical systems (database, the cloud, your website), but it is more likely that you will need to employ the services of a third party to conduct a vulnerability assessment or penetration test.\nAfter compiling a list threats to your business and the vulnerabilities in your systems that may allow the threats to be realized, you should rank those threats according to which assets they threaten (critical or noncritical) and according to their consequences. For example, the consequences of a hacker learning client names are much less serious than the hacker learning client Social Security numbers and banking information; therefore, you should focus more of your time and energy on making sure the latter threat is not realized.\nAt this point, it is important to engage upper management, executives, and owners to determine their risk tolerance—how much risk they are willing to accept. This will determine how you will manage the risks you have identified in your assessment.\nNow that you have a list of threats to your inventory, vulnerabilities you have detected, consequences, and a ranking of the threats from most to least critical, you must manage these lists. This means that you should go through your list with the appropriate stakeholders to formulate a plan for each threat from most to least critical. You may need to employ outside help to suggest appropriate actions.\nAfter you have agreed, act upon your agreement and make sure to repeat this process at least once a year.\nKey takeaways from the Cybersecurity for financial advisors series\nWe hope this series of articles has empowered you to take control of your advisory firm’s cybersecurity strategy! Remember:\n- Failing to protect your clients’ information can lead to lawsuits, fines or other consequences from regulatory bodies, and loss of client trust.\n- The SEC and FINRA recommend following Cybersecurity Framework developed by the National Institute of Standards and Technology (NIST), which is a high-level road map to cybersecurity best practices.\n- Advisor firms that don’t address the guidelines in NIST’s Cybersecurity Framework could be fined.\n- A good cybersecurity strategy should address the five core principles outlined in NIST’s Cybersecurity Framework: identify, protect, detect, respond, and recover.\n- Three steps you can take to jump-start your cybersecurity strategy include the following:', 'The threat landscape for cyber security is constantly evolving. While technological advancements have improved security measures, cybercriminals have also become more sophisticated in their methods. One particularly potent form of attack that exploits human psychology rather than technical vulnerabilities is social engineering. In this blog post, we will delve into the intricacies of social engineering attacks, explore various types of tactics employed by attackers, and discuss effective strategies to mitigate these threats.\n1. What is Social Engineering?\nSocial engineering refers to the manipulation of individuals to gain unauthorized access to confidential information, systems, or resources. Unlike traditional hacking techniques that exploit technical vulnerabilities, social engineering exploits human weaknesses and relies on psychological manipulation. Attackers leverage trust, authority, fear, or curiosity to deceive victims into revealing sensitive information or performing actions that compromise security.\n2. Common Types of Social Engineering Attacks\n2.1 Phishing Attacks\nPhishing attacks are the most prevalent form of social engineering. Attackers masquerade as legitimate entities, such as banks, service providers, or trusted organizations, to deceive victims into disclosing personal information, such as passwords or credit card details. Phishing emails often contain urgent or enticing messages that create a sense of urgency or fear, compelling victims to respond without due diligence.\nPretexting involves creating a fabricated scenario or pretext to trick individuals into disclosing sensitive information. Attackers may pose as co-workers, authority figures, or technical support personnel to gain trust. They exploit empathy, helpfulness, or a desire to comply with policies to manipulate victims into revealing confidential information or performing actions that compromise security.\n2.3 Baiting Attacks\nBaiting attacks involve enticing victims with an appealing offer or reward to elicit a desired response. Attackers may distribute malware-infected USB drives labeled as free gifts or leave them in public places where potential victims are likely to find them. Curiosity or the temptation of freebies compels victims to plug the infected USB drive into their systems, unknowingly compromising their security.\n2.4 Spear Phishing\nSpear phishing attacks are highly targeted and personalized attacks that aim to deceive specific individuals or organizations. Attackers gather information about their targets through research and reconnaissance, enabling them to craft convincing messages tailored to their victims’ preferences, interests, or positions. This personalization increases the chances of success, making spear phishing a potent social engineering technique.\n3. Strategies to Mitigate Social Engineering Attacks\n3.1 Education and Awareness\nEducation and awareness form the foundation for defending against social engineering attacks. Organizations should regularly train employees to recognize and respond appropriately to suspicious emails, phone calls, or in-person encounters. Educating individuals about common attack vectors, warning signs, and best practices for information security can significantly reduce the likelihood of falling victim to social engineering tactics.\n3.2 Implementing Multifactor Authentication (MFA)\nImplementing multifactor authentication (MFA) adds an extra layer of security to protect against social engineering attacks. By requiring users to provide additional authentication factors, such as a fingerprint, SMS code, or hardware token, even if attackers manage to obtain passwords, they would still be unable to access sensitive information or systems without the additional authentication factors.\n3.3 Establishing Strict Access Controls\nTo mitigate social engineering attacks effectively, organizations must establish and enforce strict access controls. Limiting access privileges to only those necessary for job functions reduces the likelihood of unauthorized access or inadvertent disclosure of sensitive information. Regular access reviews, least privilege principles, and strong password policies are crucial components of robust access control measures.\n3.4 Incident Response and Reporting\nEstablishing a comprehensive incident response plan enables organizations to respond effectively to social engineering attacks. This plan should include procedures for identifying and reporting potential incidents, isolating affected systems, and initiating appropriate countermeasures. Encouraging a culture of reporting incidents without fear of retribution facilitates early detection and mitigation of social engineering attacks.\n3.5 Regular Security Assessments and Penetration Testing\nRegular security assessments and penetration testing are vital for identifying vulnerabilities and weaknesses in an organization’s security posture. By simulating real-world social engineering attacks, organizations can evaluate their employees’ response to such threats and identify areas for improvement. These tests also help in reinforcing security awareness and maintaining a proactive approach to security.\nSocial engineering attacks continue to pose a significant threat to individuals and organizations worldwide. Understanding the various tactics employed by attackers and implementing appropriate strategies to mitigate these risks is crucial in safeguarding sensitive information and maintaining a robust security posture. By focusing on education, awareness, multifactor authentication, access controls, incident response, and regular security assessments, organizations can enhance their defenses against social engineering attacks and minimize the potential damage caused by these insidious tactics. Remember, the strongest line of defense against social engineering is a vigilant and well-informed human factor.']	['<urn:uuid:cbc05b41-18f3-4f6e-a037-e2b1310d3b21>', '<urn:uuid:64b210a7-bfd6-4dbf-be55-5d96dd6e0a0d>']	open-ended	with-premise	long-search-query	similar-to-document	multi-aspect	novice	2025-05-13T01:23:33.086345	15	104	1574
10	In my work evaluating museum accessibility programs, I'd like to understand what impact digital accessibility initiatives have had on Deaf visitors' museum experience. What feedback have museums received about their digital guides for Deaf visitors?	Focus group evaluations and visitor feedback have shown positive impacts from digital accessibility initiatives. At Te Papa, Deaf visitors reported increased confidence and sense of belonging after using the NZSL Mobile Guide, with most participants indicating the guide improved accessibility and visibility of NZSL at the museum. At AGSA, the feedback indicated that many Deaf community members had previously felt disengaged from cultural institutions due to their needs not being met, suggesting that regular accessible programming and digital solutions like the Accessible Guide are helping to address this issue.	['NZSL, or New Zealand Sign Language, is one of New Zealand’s three official languages. Now Te Papa has a new NZSL Mobile Guide, a new mobile-optimised web application for Deaf visitors and online audiences. Digital Producer Amos Mann takes us through the project.\nWatch a version of this post in NZSL, presented by Museums and Heritage NZSL Consultant Theresa Cooper.\nFrom the community – for the community\nThe idea for a mobile guide in NZSL came from the Deaf community. The mobile guide was planned, developed, and produced by Te Papa working closely with the Deaf community including through creative co-design and User Experience Design testing.\nThe project was funded by the NZSL Fund. It included support from Deaf Aotearoa and Arts Access Aotearoa; with creative NZSL video production and Deaf presenter talent casting from Deafradio; and with guidance and Deaf consultancy services from Museums and Heritage professional Theresa Cooper.\nWe kicked off the project with a co-design workshop, run with representatives of Deaf organisations, members of the Deaf community, and internal staff. Through activities like journey mapping, we learned what it’s like to visit the museum as a Deaf person, and what the community wanted the project goals to be.\nWe based the experience on a standard visitor journey. Most visitors want to know: What’s at the museum? Therefore, in NZSL, we provide an overview, as well as introductions to major permanent exhibitions. Some visitors want to know about top attractions. Therefore, we developed a highlights tour, basing it on existing, hosted tours for hearing visitors.\nTwo ways to experience the NZSL Mobile Guide\nWhat’s at Te Papa\nSeven videos that provide visitors with orientation information and introductions to some of Te Papa’s permanent exhibitions, including Te Taiao | Nature; Gallipoli: the scale of our war; Toi Art; and Rongomaraeroa – our marae that all cultures can feel at home on.\nTake a Tour\nThirteen videos provide visitors with a guided tour of some of our top attractions, including the Waharoa (gateway), the moa and Haast’s eagle in Te Taiao | Nature, the Earthquake House, the colossal squid, the Britten V1000 motorcycle, and Phar Lap the horse.\nWhen you next visit, you’ll see promotional and on-boarding print graphics installed throughout the museum that show which exhibits have NZSL content on the Mobile Guide. You can use the QR code printed on the graphic to go straight to the content, or simply type in the short web addresses provided.\nPlan your visit\nWe also have an NZSL page on our website to help you plan your visit:\nWhen you arrive here at Te Papa, you’ll see a Welcome video and an Orientation video playing on screens in the foyers on Levels 1 and 2. These were also created as part of this project.\nJust the beginning\nTe Papa NZSL Mobile Guide project marks the beginning of a new phase of development and community engagement towards increasing accessibility at Te Papa.\nThis project has raised Deaf culture and language awareness with staff at Te Papa, and through this project we’ve developed tools such as best practice guidelines for when we are developing content for translation into NZSL in the future.\nWe’ve learned a lot from this project and we’ll keep working with the Deaf community, Deaf organisations, Deaf professionals, and Deaf businesses as part of our goal of continuous improvement of accessibility at Te Papa. And, as well, we’ve already begun sharing what we’ve learned with the wider sector.\nIt makes me feel very proud to work for Te Papa when I see the NZSL guides and how seamlessly the 3 official languages sit side-by-side. – Te Papa staff member feedback\nThe Te Papa NZSL Mobile Guide was evaluated in Focus Group sessions with members of the Deaf community. After using the Guide in the museum, some focus group participants reported an increase in their confidence as a visitor and an increased sense of belonging at Te Papa.\nThe majority of focus group participants felt that the Te Papa NZSL Mobile Guide increases accessibility and increases the visibility of NZSL at Te Papa, and that the Guide has the potential to increase the positive experience of Deaf visitors to Te Papa.\nLove it that I can access the museum info in my language NZSL. Thank you! – Visitor feedback form\nThe Te Papa NZSL Mobile Guide is a 2021 Designers Institute of New Zealand Best Awards Public Good Award Finalist.', 'The Art Gallery of South Australia (AGSA) has launched an Accessible\nGuide, developed for Deaf, hard of hearing, blind and vision-impaired audiences. The Accessible Guide gives these audiences the opportunity to watch Auslan videos, read transcripts and listen to Audio Descriptions of works of art.\nThe Guide has commentaries on thirteen pieces of art on display in in the Elder Wing of Australian Art which showcases the work of some of Australia’s finest artists from 19th Century.\nDesigned for in-gallery use, the Accessible Audio Guide is also available in its entirety through AGSA’s website, as part of the gallery’s initiative to enable access to works of art from home, for all audiences. It has been developed for computer and handheld devices anywhere to ensure that anyone, including families and friends who may have different needs, can have an enjoyable social experience together.\nKarina Morgan – Education Support Officer and Auslan interpreter at AGSA, tells us more about AGSA’s Accessible Guide.\nCould you tell us a bit about AGSA’s Accessible Guide?\nThe Accessible Guide provides another way for our diverse audiences to be able to engage with AGSA’s collection using their own smartphones, tablets or desktop computers straight from our website, without having to download any apps.\nA selection of works of art from our Australian collection is presented in three tours – self-guided audio, audio description and Auslan videos. Transcripts accompany the audio, and closed captioning for the videos.\nCurrently audiences are able to engage with the Accessible Guide remotely – from the comfort of their own homes – and when the building re-opens in June, audiences onsite will also be able to use this guide in front of the physical works of art.\nWhy did the AGSA to create the Accessible Guide?\nOver the past three years in particular, AGSA has been steadily increasing our Deaf, hard of hearing, blind and vision impaired audiences so that the artistic, historical and cultural experiences are friendly to all visitors.\nWe acknowledged that for these audiences rich information is often not accessible at cultural institutions, particularly in South Australia. In fact, feedback we received was that many people from these communities feel so unused to having their needs met that they have felt disengaged until regular programming was put in place.\nThe Accessible Guide also connects to our Reconciliation Action Plan which aims to promote understanding and respect through the power of art and provide more opportunities for the community to learn about the diversity of First Peoples art and cultures. AGSA is proud to be the first state gallery in the country to prepare a RAP – one which includes Auslan actions.\nWhat is the AGSA hoping to achieve by developing the Accessible Guide?\nAGSA designed the Accessible Guide with all users in mind so that it is enriching and satisfying for all. We are not just building for accessibility; we wish to create a social experience so that a range of diverse families and friends can enjoy the Gallery together – not just at programmed times but at any time they are able to visit.\nWe hope this also encourages remote users to learn about our collections with the aim of visiting in person, perhaps attending one of our regular Access Programs.\nFor those who are unable to visit us in person, we hope this provides a pathway to connecting in a meaningful way with our wonderful collection.\nWhat makes you the most excited about launching the Accessible Guide?\nAn important part of this project was the community consultations. We wanted to ensure that we put users at the forefront. We didn’t want to just build an experience for these communities, but with them as well, allowing findings from these sessions to guide us in our next step, taking on board any feedback.\nThe Auslan video presenters came from the South Australian Deaf community, including a young emerging artist, William Maggs who was awarded the main SALA award for his self-portraiture in AGSA’s student drawing exhibition in 2019.\nI feel that together we have created an Accessible Guide that everyone can feel proud of and I am excited about increasing engagement with diverse communities, and to expand the Accessible Guide in the coming years ahead.\nI would love to turn around the initial comments about some sections of our audiences feeling like their access requirements are not met and that we see more engaged people like emerging artist, William Maggs, connecting with us and our collections.\nHow do you feel the Access Guide will assist Deaf and disabled arts audiences to connect with art pieces at AGSA?\nThe Accessible Guide allows our audience to choose their preferred method of communication – whether this is in English or Auslan, and also provides blind and vision impaired users an option to receive more information through an audio description. We hope to be able to increase access by adding an easy read/listen guide option for audiences who may have intellectual or cognitive disabilities, or for those who identify as being neuro-diverse. Ultimately this would also be useful for English as a Second Language (ESL) users as well.\nWhat other access services does AGSA provide to Deaf and disabled arts audiences?\nI am a certified NAATI interpreter and I interpret the weekly Tuesday lunchtime talks, and provide Auslan interpreted collection/exhibition tours one weekend tour every month. This audience has been continuously growing with between 5- 20 Deaf audience members participating every month. Special additional Auslan interpreted talks are also held during large exhibitions. Auslan tours will also be imbedded into other Gallery programming areas such as the Start program (for Families and Children), and Neo (for teenagers). The 2019 Tarnanthi Festival of Contemporary Aboriginal and Torres Strait Islander Art had for the first time two Deaf Aboriginal Guides leading the Auslan tours.\nAnother staff member, Ryan Sims who also works in AGSA Education, has undertaken Audio Description training and has developed and maintained connections with the South Australian School for the Vision Impaired (SASVI), as well as implementing a program of regular Audio described and Tactile tours.\nOther Access programs we have initiated, include our Art and Dementia program run by our dedicated Gallery Guides. AGSA has also offered training to the National Gallery of Australia in 2019. We also currently have partnerships with selected residential care facilities and anticipate building upon this program to include more members of our community.\nTo visit AGSA’s Accessible Guide, please click here.']	['<urn:uuid:ef32f9dd-2616-46b3-8297-844224e2adb9>', '<urn:uuid:b2dc8eaf-69f4-4cd8-ae80-50d9c283fae4>']	factoid	with-premise	verbose-and-natural	distant-from-document	three-doc	expert	2025-05-13T01:23:33.086345	35	89	1814
11	How do paper medical forms and online forms compare in terms of their impact on hospital administrative costs?	Paper forms contribute to higher administrative costs, which currently account for 25-33% of total healthcare spending in the US. In contrast, online medical forms reduce administrative overhead by decreasing human error and freeing up staff time for other tasks, as electronic forms streamline operations and make document archiving and retrieval more efficient.	['RAND researchers estimated the effects that four proposed out-of-network payment limits for hospital care—125 percent of Medicare payments (a strict limit), 200 percent of Medicare payments (a moderate limit), state average payment by private plans (a moderate limit), and 80 percent of average billed charges (a loose limit)—would have on negotiated in-network prices and total payments for hospital care. These four scenarios reflect the variation in base measure (traditional Medicare, market price, and charges) and payment generosity among existing policy proposals.\nA Hard Pill to Swallow: Appreciating the Mathematical Dynamics of the Affordable Care Act – Axene Health Partners, LLCMarch 22, 2020\nThe Affordable Care Act (ACA) is recognized for its role in reducing the uninsured rate in the United States. The federal financial commitment has expanded Medicaid eligibility and provided incentives for people to enroll in individual markets for the first time. Underneath the surface, the structural individual market design results in convoluted mathematical dynamics that challenge stakeholders to understand and improve markets. There has been historical reluctance to freely admit these challenges, partially to avoid potentially providing leverage to ACA detractors. To improve ACA markets, we must first be honest about some of its illogical consequences. For many of us, this is a hard pill to swallow.\nThis paper evaluates the labor market effects of sick pay mandates in the United States. Using the National Compensation Survey and difference-in-differences models, we estimate their impact on coverage rates, sick leave use, labor costs, and non-mandated fringe benefits. Sick pay mandates increase coverage significantly by 13 percentage points from a baseline level of 66%. Newly covered employees take two additional sick days per year. We find little evidence that mandating sick pay crowds-out other non-mandated fringe benefits. We then develop a model of optimal sick pay provision along with a welfare analysis. Mandating sick pay likely increases welfare.\nnew data from the California Policy Lab show, it’s likely that 50 percent of the homeless and 78 percent of the unsheltered homeless have a serious mental health condition.\nPrimary Care: Estimating Democratic Candidates’ Health Plans | Committee for a Responsible Federal BudgetMarch 13, 2020\nSenator Bernie Sanders released a new set of proposals to offset the cost of his health care plan this week. Based on these new proposals, we have updated our paper – Primary Care: Estimating Democratic Candidates’ Health Plans – which estimates the fiscal impact of plans put forward by candidates Sanders, Biden, Buttigieg, and Warren. Whereas our previous estimates found Senator Sanders’s plan would fall $13.4 trillion short of being paid for, our newest estimates find his plan would fall just under $13 trillion short. The full analysis is available below. We have also updated our Primary Care infographics.\nOne of the greatest challenges to affordable health care is the high cost of American hospitals. The most important driver of higher prices for hospital care, in turn, is the rise of regional hospital monopolies. Hospitals are merging into large hospital systems, and using their market power to demand higher and higher prices from the privately insured and the uninsured.\nThe ProblemAdministrative costs account for one-quarter to one-third of total health-care spending in the United States—far greater than the amount necessary to deliver effective health care. Excessive administrative burden results in higher costs for physicians, insurers, and patients alike.The ProposalCutler proposes several reforms to the U.S. health-care system aimed at reducing administrative costs. Specifically, his proposal would establish a clearinghouse for bill submission, simplify prior authorization, harmonize quality reporting, and enhance data interoperability in the health-care system. Cutler’s proposal to lower administrative costs could save $50 billion annually.\nIn this document, we provide 12 facts about the economics of U.S. health-care, focusing largely on the private-payer system. We highlight the surge in health-care expenditures and their current high level. We note the wide variation of expenditures across individuals—something that necessitates insurance. We document that the United States pays higher prices than most countries and that these prices vary widely across and within places. We show that a lack of competition and high administrative costs are especially important contributors to high expenditures, indicating the need for reforms to reduce costs in the United States. To keep the focus on these issues, we do not discuss questions of coverage or of how coverage is provided (publicly or via the market), but instead address the questions of why expenditures, costs, and prices are so high.\nCompetitor’s Veto: State Certificate of Need Laws Violate State Prohibitions on Monopolies | Regulatory Transparency ProjectMarch 10, 2020\nCON laws, after all, violate a host of constitutional provisions, including the anti-monopoly clauses found in several state constitutions. Enabling existing providers to use the law to bar others from entering an industry or offering a service is the very definition of a government-created monopoly. Few state courts have so far directly addressed whether CON laws violate state anti-monopoly clauses, but several have noted that they are inherently anticompetitive. Although this paper discusses how CON laws are designed to function in precisely the way prohibited by state constitutions.\nDr. Thomas’ AHRQ-funded work has explored other major patient safety issues such as the impact of medical liability alternatives like non-disclosure agreements. One in three malpractice claims that result in death or disability stem from an inaccurate or delayed diagnosis, according to a 2019 study in Diagnosis. External Link Disclaimer', 'Can you recall the last time you paid your bills using paper mail rather than an online portal?\nOr how about the last time you called an airline to book a flight?\nIf you’re like most people, it’s been years since you’ve done these things the “old-fashioned” way. After all, why make things difficult when you have technology at your disposal?It’s time to\nHIPAA compliant online medical forms can streamline your office operations considerably. If you’re looking to increase operational efficiency, boost patient satisfaction and ultimately, grow your practice’s revenue, online forms are sure to help.\nIt’s no secret that tremendous technological growth has ushered in a wide variety of online services that streamline our day-to-day lives. When it comes to the medical industry, the same principle holds true. Hospitals and independent clinics now have the ability to streamline their operations using web-based data systems and collection techniques specifically designed to improve office workflow.\nAccording to the Office of the National Coordinator for Health Information Technology, 99 percent of U.S. medical facilities have opted for a more paperless approach to patient record keeping. In many of these facilities, however, online medical forms have yet to make an appearance. If your office is one of them, read on to discover the advantages to using web based medical forms.\nAdvantages of Online Medical Forms Vs. Paper Forms\nIf you’re still using archaic paper forms to collect sensitive patient information, you’re missing out on the tremendous convenience and efficiency of web-based data collection. How are online forms superior to their paper counterparts? Here are a few distinct examples:\nImproved Patient Data Accuracy\nDid you know that approximately 61 percent of insurance claim denials are due to errors or missing information on medical forms? Just one blank field is enough to trigger a denial, and unfortunately, paper forms offer no surefire method of detecting patient or office personnel errors. When errors bar a new patient from getting the insurance coverage they deserve, you run a very real risk of losing that patient to another practitioner.\nFortunately, electronic forms are designed to prevent inaccuracies by triggering an error message when patients fail to fill out each field correctly. When a patient fills out an online intake form, he or she cannot submit that form until each field is properly completed. Yes, human error is still possible when using online forms, but it’s significantly less prevalent.\nReduced Administrative Overhead\nWhen your administrative staff must allocate additional time to correcting errors and resubmitting claims, they reduce your operational efficiency. Administrative overhead accounts for approximately 38 percent of healthcare costs, so if your office can increase operational efficiency, you’ll have a more profitable business.\nBecause electronic forms significantly reduce human error, they help free up time for more important administrative tasks. Your staff becomes more efficient, and your day-to-day operations flow much more smoothly.\nElectronic Archiving and Document Sharing\nElectronic archiving and retrieval of patient forms is far more efficient than having a dedicated room filled with filing cabinets. When your new patient submits an intake form, your staff can quickly and easily add that form to the patient’s record. When you need to retrieve that data in the future, it’s easy to locate and takes mere seconds to view. Automating your office workflow in this way improves efficiency and allows you to deliver a more streamlined experience for your patients.\nAdditional Time to Prepare for New Appointments\nWhen your office uses online forms, patients have the opportunity to fill out their intake information well in advance of their initial appointment. You’ll have access to their information long before they arrive in your waiting room, which gives you additional time to prepare for the appointment.\nWhen you have limited time to spend with each patient, even a few extra minutes can make a world of difference in the patient’s perception of their in-office experience. With online forms, neither you nor the patient will feel so rushed during the appointment, which allows you to discuss concerns in greater detail. Patients have a better overall experience, and you have the opportunity to gather more in-depth information. In a bustling medical office, preparation is the key to smooth daily operation.\nPre-Appointment Insurance Validation\nWhen your office has a patient’s insurance information prior to the initial appointment, your staff can verify coverage long before the patient arrives to see you. Any issues with insurance verification can be resolved before the patient’s scheduled visit, which dramatically improves his or her experience.\nWith paper forms, however, office staff must quickly verify insurance while the patient waits to be seen. With limited time, your staff has little ability to resolve coverage issues, which puts undue stress on your employees and creates unnecessary confusion and frustration for the patient.\nPatients Prefer Online Forms\nA medical office is a business, and as with any type of business, satisfying the customer — in this case, the patient — is critical. Many patients prefer online forms because they drastically reduce the time spent in your waiting room while filling out information. When patients use online forms, office efficiency increases, which causes total office wait times to decrease.\nWhen new patients observe that your office flows smoothly and appointments occur at scheduled intervals, they feel confident that you and your staff value and respect their time. One of the greatest frustrations among patients is significant medical office wait times that interfere with their daily obligations. When you show your patients you respect their time, they’re more likely to return and even better, refer acquaintances to your care.\nOnline Medical Forms: HIPAA Compliance is Paramount\nIf your practice is considering implementing online forms, HIPAA compliance is of critical importance when collecting protected health information. Just as paper forms must be protected from view, online forms must feature safeguards to protect PHI in the event of a cyber attack.\nThere are several online form builders that offer custom, HIPAA-compliant forms that can be seamlessly integrated with your existing practice website. For convenience, many of these services also offer mobile form completion, electronic signatures, document sharing, and even payment collection. Though these services advertise HIPAA-compliance, it’s critical that you verify the company employs the following security measures:\n- Multi-factor authentication for authorized users\n- Strong passwords and automatic logout after periods of inactivity\n- Robust, end-to-end data encryption\n- Email notifications that do not contain PHI\n- Data transfers to HIPAA-compliant platforms\nAs with any outside company that may handle protected health information, you must ensure HIPAA-compliant form builders sign a business associate’s agreement prior to your implementation of the forms.\nMany online form builders do not offer HIPAA-compliant forms, which means that if you opt for one of these services, you must implement the appropriate safeguards independently. You must also ensure the company is willing to sign a business associate’s agreement to legally protect your patients’ data.\nShould You Use Online Forms if You Have an EHR System?\nIf your office operates using electronic health records, but you’re still collecting sensitive patient data using paper forms, you’re significantly reducing the operational efficiency of your business. Not only that, but you may also be compromising the security of patient PHI, which can have serious repercussions.\nTo streamline your office operations and better protect your patients’ PHI, HIPPA-compliant online forms are key. Because web-based forms allow you to collect patient information and seamlessly transfer that data into electronic patient health records, they reduce the need for manual data handling. If you’ve implemented an EHR system to improve organization and efficiency, web-based forms are a perfect complement for streamlining data flow throughout your office. Here’s how the process works:\n- You create web-based, HIPAA-compliant forms to collect patient information. Those forms integrate with your practice website.\n- Patients fill out their information, and upon form submission, ePHI is sent directly to a cloud-based, secure database. HIPAA-compliant online form builders use data encryption to protect sensitive patient information.\n- When ePHI is submitted to the database, authorized employees receive a notification that the data is available for viewing.\n- The patient’s form data transfers seamlessly into the corresponding electronic health record for that patient.\nOnline Medical Forms: The Future of Data Collection\nIf your office isn’t currently using online forms, now is the time to make the transition. With the appropriate HIPAA-compliant safeguards in place, online forms contribute to a significantly more automated workflow, increased office efficiency and a better experience for your patients. Are you on board?']	['<urn:uuid:4e38a33c-2624-4927-b58b-da35be83760a>', '<urn:uuid:ea9106cd-d204-4db8-902c-d1b76c406ce6>']	factoid	direct	verbose-and-natural	similar-to-document	comparison	novice	2025-05-13T01:23:33.086345	18	52	2302
12	how many concrete slabs holocaust memorial berlin	The Memorial to the Murdered Jews of Europe in Berlin consists of 2711 concrete slabs, covering an area of 19,000 square meters.	['The “Memorial to the Murdered Jews of Europe” in Berlin opened to the public in 2005. Designed by architect Peter Eisenman, it consists of 2711 slabs made of concrete, covering 19,000 square meters in total and offering a very particular set of experiences for its visitors. A statement by architect Peter Eisenman on the memorial’s website demonstrates that he had quite detailed ideas about what kind of experiences the field of slabs is supposed to mobilize for its visitors:\nThe Memorial through Instagram Filters 1-5\nFrom an analytical viewpoint, what Eisenman describes here can be understood as the emotional affordances of this particular memorial. In cultural anthropology and related disciplines, the concept of affordances is often used to describe which kind of practices particular materialities and architectures afford to human actors relating to them. From this perspective, the Holocaust memorial affords particular practices for its visitors. If we bring this together with the perspective of the anthropology of emotions, we can add that the memorial affords particular emotional practices. Building upon the work of cultural anthropologist Monique Scheer, I understand emotions not as located solely inside of human subjects, but as inherently bound to bodily practices. Scheer phrases this through the expression, that we (as human actors) do not have emotions, but we do emotions. Of course, this does not mean that in our everyday life we enact single emotions, which can be neatly distinguished from another, but rather we enact complex emotional experiences. Such experiences include heterogeneous and sometimes contradictory mixtures of perceptions, sensual impressions, emotions, feelings, affects, but also cognitive and discursive reflections.\nUsing both the concept of affordances and the concept of emotional practices to think about the Holocaust memorial inspires us to understand it as a material structure that affords particular emotional experiences enacted through the emotional practices of the visitors. What distinguishes emotional practices at memorial sites from many other emotional practices that we permanently enact in our everyday lives, is that they are (or at least they are supposed to be) at the same time particular practices of past presencing. The concept of past presencing, as it has been developed by cultural anthropologist Sharon Macdonald, effectively captures the idea that memory practices are always processes of relating to the past, which are enacted in the present. My question in this blog is, how emotional practices and practices of past presencing entangle during the visit of the holocaust memorial.\n“The Ort [German for “place”, C.B.] is subdued in manner, effectively designed to minimize any disturbance to the Memorial’s field of pillars. Its mass, weight, and density seem to perceptibly bear down and close in on individuals. The organization of the space of the Ort extends the slabs of the field into the structure, provoking a continued state of reflection and contemplation once inside. […] This grid is rotated against the logic of the field, thereby thwarting any paradigmatic understanding of its formal arrangement. The uncertain frame of reference that results further isolates individuals in what is intended to be an unsettling, personal experience.” (Peter Eisenman, link)\nAs mentioned above, according to the memorial’s website, architect Peter Eisenman had quite detailed ideas about these entanglements. The architecture of the memorial is supposed to afford practices of wandering around lonely, of getting lost, of experiencing isolation and thoughtful contemplation––all this, of course, to emotionally remember the Holocaust. From an analytical viewpoint, what the memorial is supposed to afford is a particular way of past presencing the holocaust through emotional practices.\nHowever, if you visit the memorial, what you can actually observe are not so much practices pointing towards emotional experiences of isolation and disturbance. Rather, what you can usually observe are visitors using smartphones or other cameras to take pictures and shoot videos, often already starting right after entering the field. This is to say: for many visitors, digital media practices are an integral part of visiting and experiencing the memorial.\nThis is where the approaches of media and digital anthropology come into play and can help us to better understand the ongoing transformations of heritage experiences due to the rise of digital media. In an ongoing research project, I am taking a closer look at the digital media practices at the Holocaust memorial, both through participant observation at the memorial itself and through an ethnographic analysis of media practices online.\nThe research is still ongoing and the results are only preliminary. Thus, the intention of this blog is not to report exhaustively on the empirical results, but rather raise critical questions concerning the ongoing debate about the appropriateness of media practices at heritage sites, such as: How do these media practices relate to the emotional affordances of the memorial? Do they ignore them? Do they accept them? In which particular way do they enact them?\nIn this blog, I can answer these questions not for the whole variety of media practices at the memorial, but at least for one particular media practice very common at this and other heritage sites: making selfies. The selfie as a particular aesthetic form is often associated with vanity, self-centeredness and narcissism. And when we conceptualize making selfies as an emotional media practice, we might agree that often they are exactly that: a practice of enacting emotional experiences that circulate around one’s own bodily presence and beauty – experiences that are continued, of course, through practices of uploading selfies on social media to receive appropriate emotional confirmation.\nDuring my participant observation, I sometimes observed rather excessive cases of making selfies: a young woman, for example, brought her own selfie tripod and, without actually going more than a few steps into the field, stayed at the outer part where you can take pictures in the sun. Here, she spent a whole 15 minutes trying out various angles (with sunglasses, without sunglasses, with hat, without hat, etc.) to capture the best selfie.\nIf you visit Facebook, Instagram or other platforms to search for pictures made at the “Memorial to the Murdered Jews of Europe”, you will find thousands of them publicly posted, many of them selfies. Among these selfies you will find those having a strong emphasis on “happiness” and “fun”, usually expressed through bright smiles of the person or persons being portrayed. Sometimes, as in the example found here, the bright smile of the visitor is accompanied by additional comments such as “Omg [meaning: Oh my god, C.B.]. Love this ???” or “Looking too cute!!!”. Such comments emphasize that the picture is about the person’s beauty and good looks. On Instagram, you will find similar pictures often complemented by hashtags, such as in the case of the example found here: “#berlin #shortholiday #amazingweather #muchIove #sunny #memoriaItothemurderedjews #loveyou every brownie needs a blondie ♡?”.\nSuch practices have led the Jewish-German artist and comedian Shahak Shapira to create what he named the “Yolocaust”-project in January 2017. The artist created a website on which he uploaded selfies and selfie-like pictures taken at the memorial, and he added actual footage from concentration camps as a “new background” for them. The original pictures were taken down from the website, which by then had over 2.5 million views (according to the artist). A video that documented the project then went viral, received almost 80 million views on Facebook and contributed to a controversial debate about (what the video calls) “selfie culture” at memorial sites related to the Holocaust. You can access the video here––but please be aware that it includes very graphic portrayals of violence and that there are critical ethical questions concerning the project left unanswered by the video.\nThe debate fostered by this video circulates around the question, what “appropriate” behavior at the holocaust memorial is in terms of media use. As a cultural anthropologist, I am not interested in whether the selfies should actually be considered appropriate or not. Rather, I am interested in how the controversial negotiation of appropriateness reflects a conflict we witness on a broader scale. In this conflict, enacting emotional experiences that relate to the “original” emotional affordances of the memorial is considered appropriate, while such emotional experiences that apparently deviate from this norm are considered inappropriate or simply wrong.\nA critical ethnographic contribution to this debate can point out that the right vs. wrong dichotomy applied in such debates fails to account for the complexity of digital media practices. The fact that many visitors are using heritage sites as a background for selfies does not mean, that none of them relate meaningfully to the past. On the contrary, the “happy-selfies” are merely one particular pole within a broad spectrum of emotional experiences enacted through selfies. Let me give you a few examples of others:\nPeople might smile on a selfie posted on Facebook, like the young man in the example found here, but they add comments such as “My heart goes out to all the lives that were affected by the holocaust, it’s beautiful to see that the world has acknowledged the lives lost with this memorial.” Or they might take a selfie appearing as narcissistic, such as the young woman in the example found here, yet they express sadness and contemplation by emphasizing that we should “never forget”. Or they might even show a “thumbs up” gesture into the camera, like in the example found here, yet what they mean is to express their approval of the memorial as a place of memory. Or, last but not least, they might write nothing at all, simply looking into the camera, expressing sadness or even anger, such as the young man in the example found here. In the comments below he is asked by a friend: “Why do you look so angry?” And he replies: “Because we do not learn from the past ?” [my translations]. In doing so, he is relating his bodily presence at the memorial meaningfully to the past and connecting it to present transformations of the political landscape in Germany.\nWhat we witness here is not simply contemplation and remembrance as it is envisioned on the memorial’s website, nor are these practices merely focusing on self-representation. Rather, we see complex entanglements of emotional media practices that include both dimensions. The crucial difference between these practices of selfie making and the “happy-selfies” mentioned above is not the aesthetic form of the selfie, but particular emotional experiences enacted through them. Sometimes, a selfie might be a means to express joy and happiness, but it can also articulate sadness, respect and compassion. Through making selfies, these emotional experiences are not only portrayed, but they are put in relation to the material setting of the memorial. Thus, through emotional practices of making selfies, bodies and their emotions can get closely entangled with the material settings. If this is the case, a selfie in the context of the Holocaust memorial also becomes a practice of past presencing, since through enacting the entanglements of bodies, emotions and material settings, it relates meaningfully to the past. To put this in a nutshell: Depending on how bodies and material settings get entangled in emotional media practices of selfie making, these practices are not ignoring the memorial’s affordance to remember the past, but are instead presencing this past in a meaningful way.\nFurthermore, making selfies is just one and certainly not the central media practice at the memorial site. For example, visitors regularly take pictures of each other portraying them as they are looking into the far of the field in lonely contemplation, as they wander around in silence or as they are touching the memorial with a meaningful gesture. Other Instagram and Facebook users focus on aesthetic photographic representations of the symmetry of the memorial’s slabs, or on the play of light and shadow in the memorial, sometimes including objects (most regularly: roses) in their pictures, while complementing their posts with meaningful texts pointing out the need to remember the Holocaust. All these media practices deserve careful ethnographic attention, especially regarding their emotional implications. Contrary to the discourses highlighting a superficial “selfie culture” at memorial sites, it might turn out that visitors often use digital media to relate to the memorial in different but nonetheless meaningful ways. Or, in other words, they find different ways of past presencing through new media.\nThis blog post has also been published on: https://doingselfieblog.wordpress.com/2018/01/19/past-presencing-through-new-media-making-selfies-in-heritage-spaces/\nChristoph Bareither is Junior Professor of European Ethnology & Media Anthropology at the Department of European Ethnology at the Humboldt-Universität zu Berlin and the Centre for Anthropological Research on Museums and Heritage (CARMAH). His research is concerned with the transformations of everyday practices and experiences enabled through digital technologies. Christoph is especially interested in the fields of media and digital anthropology, popular culture research and the ethnography of emotions. Currently he is starting a new research project, in which he will use approaches of media and digital anthropology to foster our understanding of media practices and experiences at memory places and heritage sites in Berlin and Europe.']	['<urn:uuid:81c86ed8-aea5-4a61-99b3-1f28127c71fa>']	factoid	with-premise	short-search-query	similar-to-document	single-doc	expert	2025-05-13T01:23:33.086345	7	22	2152
13	how do thrips facilitate tswv virus transmission plant cells mechanism infection pathways	Thrips facilitate TSWV transmission through a two-step process: first, larval thrips must feed on an infected plant, then the adult thrips must feed on a susceptible host plant to transmit the virus. At the cellular level, TSWV uses plasmodesmata for cell-to-cell trafficking, with the ER membrane transport system serving as a major direct pathway for intercellular movement of the NSm movement protein and TSWV.	['Questions About Thrips in Tobacco\nThis week growers, agents, and consultants have noticed unusually large numbers of thrips on tobacco. This is no surprise to cotton growers, who have been asking about thrips for a few weeks. Agent reports and our observations at research station and scouting locations indicated that counts between 30 and 50 thrips per leaf were common. Accompanying these high numbers was also some classic “silver-leafing” foliar damage, which is caused by thrips’ rasping the upper leaf surface.\nThese observations raise the obvious question of TSWV (tomato spotted wilt virus) risk, since tobacco thrips (our most common species in tobacco) vector this virus. The first question I asked all of the people who called me was “what do the thrips look like?” This is because of the many thrips species present in North Carolina, only about four are likely to vector TSWV, and only three of those are commonly found in tobacco. Because thrips are so small, often the usual ones, such as the relatively large cereal thrips or distinctive striped soybean thrips are the species that people notice. However, we collected a small sample of thrips from the Lower Coastal Plain Research Station near Kinston, which all appeared to be female tobacco thrips.\nWhy are we seeing so many thrips in tobacco?\nThe thrips we are observing are are likely the third generation of tobacco thrips, and our tobacco thrips flight timing predictions were right on the mark for this year! For example, third generation thrips flights were predicted to begin in Wilson County around May 23rd, which means that we are right in the middle of third generation dispersal. At this location, the fourth generation flight is expected to begin around June 10th, so we may observe additional thrips populations in a few weeks. To see thrips flight predictions for your area, use our TSWV and Thrips Risk Forecasting Tool.\nWinter weeds are also likely playing a role in our thrips observations. Plants such as chickweed, knotweed, and henbit are among the overwintering hosts for tobacco thrips in North Carolina, and as these plants die in the early summer, thrips are forced to migrate to new hosts. At many of the locations I visit (including my yard), these winter annuals have accelerated their die-back in the last few weeks.\nThe combination of winter weed die-back and thrips generation timing likely account for our observations this week.\nWhat do these observations mean for TSWV risk?\nTSWV infection in a plant is a complicated process. First, a larval thrips needs to feed on an infected plant, then the adult thrips must feed on susceptible a host plant to transmit the virus. In tobacco, this is further complicated by the fact that plants become more resistant to infection as they age. A six week old plant, for example, is generally less likely to develop a systemic (and therefore, damaging) infection of TSWV than a three week old plant.\nThe first link the the chain requires that there be TSWV infection in winter weed hosts so that larvae can acquire the virus. Last year was a relatively low TSWV incidence year, suggesting that virus presence in winter weeds may also be low. This would mean that fewer of the thrips moving into our fields right now are capable of transmitting TSWV.\nThe second link requires a susceptible host plant. We had a prolonged transplant period this spring and early summer, which means there is a lot of variability in plant age. Younger plants (less than six weeks old) are at higher risk of infection than older plants.\nWhat should we do now?\nThe foliar feeding damage by thrips is likely negligible. It is occurring early in the season on leaves that will be on the lower part of the plant and subject to much more abuse before harvest.\nPredictions made by our TSWV and Thrips Forecasting Model still suggest that this will be an “average” year for most locations. This model takes into account last year’s thrips populations and this year’s weather conditions to make its prediction. This average observation may be due to the fact that we had relatively low thrips numbers and relative little virus incidence last year.\nGrowers in high TSWV risk areas (greater than 10% incidence in an average year, with standard production practices) may want to take action in the field (see the TSWV and Thrips Forecasting Model for management recommendations).\nGrowers in lower TSWV areas will still likely not see a benefit of additional management efforts, especially not those directed at thrips. Insecticide treatments for thrips are not recommended as they are short lasting and have not been demonstrated to reduce virus incidence. This is why greenhouse treatments have become so popular for TSWV management, because while they do not necessarily kill thrips, they discourage them from feeding on plants and reduce the time they do feed.\nIt is also important to note that TSWV takes time to develop in the plant before symptoms are observed. This means that any diseased plants observed in fields right now are likely due to infections from a few weeks ago and that it take a few weeks from now for any infections due to this thrips flight to develop.', 'Plant viruses undertake plasmodesmata to infect new cells. cell-to-cell trafficking. Pharmacological disruption from the ER network inhibited NSm-GFP trafficking however not GFP diffusion severely. Rabbit polyclonal to ZNF101. In the mutant with an impaired ER network NSm-GFP trafficking was considerably decreased whereas GFP diffusion had not been affected. We also demonstrated which the ER-to-Golgi secretion pathway as well as the cytoskeleton transportation systems weren’t mixed up in intercellular trafficking of TSWV T-705 (Favipiravir) NSm. Significantly TSWV cell-to-cell pass on was postponed in the ER-defective mutant which reduced viral an infection was not because of reduced replication. Based on robust biochemical mobile and genetic evaluation we established which the ER membrane transportation system acts as a significant direct path for intercellular trafficking of NSm and TSWV. Writer Summary Plant infections might use different web host cell transportation machineries to go in one cell to some other through plasmodesmata. The contribution of web host cell transportation systems towards the intercellular motion of multipartite negative-strand RNA place infections including tospoviruses is normally poorly T-705 (Favipiravir) known. We utilized (TSWV) being a model to comprehend the system of intercellular motion of tospoviruses. Within this research using and systems for characterizing membrane protein we identified which the TSWV NSm motion protein was in physical form associated with the ER membrane. NSm indicated in one leaf cell was able to move into neighboring cells along the ER membrane network. The ER membrane in vegetation is a unique structure that runs between neighboring cells via the ER desmotubule of the plasmodesmata and forms a continuous network throughout the flower. Taking advantage of TSWV NSm becoming tightly associated with ER membrane and trafficked between cells through plasmodesmata we shown here by strong biochemical cellullar and genetic evidence the ER membrane transport system of vegetation serves as an important route for intercellular trafficking of the NSm movement protein and TSWV. Our findings have important fresh implications for mechanistic studies on intercellular trafficking of tospoviruses and additional multipartite negative-strand RNA flower viruses. Intro Plasmodesma-mediated macromolecular trafficking takes on important functions in flower growth and development [1-3] and in plant-pathogen relationships [4-6]. Structurally a plasmodesma is composed of the plasma membrane having a central altered appressed endoplasmic reticulum (ER) the desmotubule . Besides the long-established T-705 (Favipiravir) cell-to-cell transport of small molecules via plasmodesmata macromolecules such as proteins and RNAs have been shown in the last two decades to traffic between cells through plasmodesmata (PD). Such macromolecular trafficking is vital for viral illness [4-6] flower defense [8 9 and developmental rules [1-3]. Plant viruses need to move within and between cells to establish systemic infection. To accomplish this task the flower computer virus encodes a movement protein (MP) to help intracellular trafficking of the viral genomes from your replication site to PD and to aid the spread of the viral replication complexes or viral particles between flower cells through PD [5 6 10 Flower viruses not merely T-705 (Favipiravir) make use of viral-encoded MPs or various other viral elements for viral intra- and intercellular motion but also co-opt web host cell transportation machineries because of their motion [13-17]. The cytoskeleton and membrane transportation systems of cells are essential for intracellular motion of vertebrate infections (analyzed in ) needed for organellar trafficking within place cells [18 19 and mixed up in intercellular trafficking of macromolecules [20 21 Regarding the best-studied place trojan (TMV) the ER membrane is normally very important to its association using the viral replication complexes (VRC) and MP granules whereas microtubules and microfilaments facilitated their motion over the ER (analyzed in ). The ER membrane also acts as a significant system for anchoring other viral MPs that are necessary for intracellular motion and viral spread [23-27]. The ER-to-Golgi secretory pathway is involved with PD T-705 (Favipiravir) targeting and intercellular trafficking further.']	['<urn:uuid:837050c7-0bda-481b-8ab3-08e1727c9967>', '<urn:uuid:090812f4-0e9d-4d06-98af-d7a5d7782181>']	factoid	direct	long-search-query	similar-to-document	multi-aspect	expert	2025-05-13T01:23:33.086345	12	64	1505
14	What's new in monitoring health - tiny sensors and phones combined?	There are two major developments: First, micro-sensors as small as 2.5mm are being used to monitor bee health and behavior, with researchers working on even smaller 1.5mm versions. Second, mobile health technology (mHealth) has emerged, using smartphones and tablets for healthcare delivery and management. This combination of sensor technology and mobile devices is enabling unprecedented monitoring capabilities, though wireless security remains a concern as health data transmitted through the atmosphere can be intercepted or hacked.	['- Micro-sensors attached to thousands of bees to help investigate their health.\n- Bees are under threat globally from pathogens, pesticides and agriculture intensification.\n- Monitoring the behavior of these pollinators can combat population decline.\nThere’s a big buzz coming from the land down under. Researchers from the Commonwealth Scientific and Industrial Research Organization (CSIRO) in Australia are harvesting big data from micro-sensors on thousands of bees in the state of Tasmania. The research program is the first of its kind and is part of the Global Initiative for Honeybee Health, an international collaboration of researchers, beekeepers, farmers, and technology companies.\nBees play a vital role in pollinating food crops. Roughly one third of the food humans consume each day relies on the little creatures, amounting to billions of dollars in the global economy. However, bee populations are facing threats from a number of factors including pathogens, pesticides, and agriculture intensification.\nDown under, biosecurity is a serious issue. Because of Australia’s geographic isolation and subsequent fragile ecosystem, imports are tightly regulated. One of the pests that Australia is trying to keep out is the Varroa mite – a parasite that transmits pathogens which can rapidly lead to the death of bee colonies.\nDue to their size, monitoring how bees spend their time has traditionally been a struggle for scientists. Researchers have tried painting individuals with color codes and monitoring their movements via cameras – a tedious task. But with the new radio-frequency identification (RFID) tags the process is much easier. The micro-sensor ‘backpacks’ – measuring 2.5mm x 2.5mm and weighing about 5 milligrams each – are attached in a matter of seconds and record when the bees pass checkpoints.\nThis data is then shared with scientists worldwide via the cloud to build models revealing how bees interact with the environment. Researchers use this information to analyze the effects of stressors such as disease, pesticides, air pollution, water contamination, diet, and extreme weather on the bee’s ability to pollinate.\n“These RFID tags usually only work within a few mm-range. So, bees need to almost touch the antennas that are installed in places where they will certainly pass, for example, the entry of the hives,” says Paulo de Souza, head of the research group behind the development of the technology. “We managed to increase the range of reading in a few orders of magnitude. This allows us to read the signals more efficiently, reduce the loss in readings and be more flexible with the setup.”\nBees are considered fairly predictable in their behavior hence deviations can indicate stress or environmental changes. Modeling the bees’ movements therefore can help to identify the cause of the change and protect the colony.\nThe CSIRO are now working with Brazil’s Vale Institute of Technology to utilize the sensors in the Amazon so they can compare behavior between the regions.\nFurther work is being done to make even smaller sensors – 1.5mm x 1.5mm – which will interfere less with the bees’ behavior. But the reduced size of the sensors presents a new problem – small battery technology is not powerful enough.\n“The solution we came up with was an energy device that harvests energy from the movements of bees and a battery that is built on a 3D-structure,” says de Souza. “This structure increases the energy density to be stored. A bumble bee can produce as much as 0.5W in one hour of buzz-pollination.”\nThe researchers plan to make small enough sensors to be used on fruit flies and mosquitoes in the future, enabling information to be captured all around the world in unprecedented density.', 'By Mohammad Bajwa, PhD, CPHIMS, CPHI, RHIA, MCSE\nWhile it has only just emerged this century, the use of mobile health technology (mHealth) in the delivery and management of healthcare is gaining traction due to the global implementation of health information technology. This is exemplified mostly by electronic health record (EHR) systems, wherein patients’ health information resides on computers and travels on computer networks. Normally, the information stored on computers is accessed through wires and is limited to places where such infrastructure exists. With the advent of wireless technology, electronic health information can be accessed by anybody, anytime from anywhere and on any mobile device as long as wireless connectivity is available. Despite this advantage, the security of health information stored on and accessed through mobile devices is cause for public concern. Wireless communication is prone to hacking, and mobile devices, given their size and value, increase the chance they will be lost or stolen.\nmHealth is defined as the use of mobile devices (mDevices) in the practice of medicine. These include mobile/cellphones, iPads, tablets, personal and notebook (laptop) computers, personal digital assistants (PDAs), and similar other devices that use wireless technology to access health information networks. Such devices often use radio waves for communication either through central access points (hot spots) or satellites. Of these, mobile phones have emerged as the greatest mobile technology gadgets.\nSince wireless signals travel through the atmosphere, diffuse in all directions, and can pass through most physical barriers, they are liable be intercepted. The intent of this article is to review wireless technology, its use in healthcare, its security issues, and best practices to make safe use of this novel technology for the delivery and management of healthcare.\nTypes of mHealth Security\nBefore discussing the security of mHealth, it is appropriate to enumerate four types of information security requirements: physical, network, application, and user security.\nPhysical security implies the security of computers, network hardware, and storage media against intruders and natural disasters. The most common mitigative security measures for this purpose includes using ID badges and biometric authentication for entry to premises, locking computers with desks, and regular backups for data restoration in case of any human-made or natural disasters.\nNetwork security denotes the protection of data and information residing on computers and in transit over the network/internet through techniques like encryption (encodes data, rendering it unreadable by unauthorized persons) and technologies such as firewalls (controls both inbound and outbound data based on the nature of traffic and source); virtual private networks (makes private data communication channel in the public network such as internet); network intrusion detection (notifies the network administrator of any intrusion by email, text message, alert); and intrusion detection and prevention systems (blocks intruder and reports to network administrator).\nApplication security requires validation of services accessing the information, level of information access and use permissions, software patches and updates, software logs, and input validation techniques (best defense against malware/spyware injection).\nUser security is a three-step process that entails user identification through user ID, authentication through password, biometric or two-factor authentication (verifies user), and authorization through permissions (what the user can access and do).\nData Transmission Media\nData and information on the network travels either on wires (wireline transmission media)—the most common being the copper cables (network and coaxial or TV cables) and fiber optic (FO) cables—or wirelessly through the atmosphere. For the discussion of data security, it is pertinent to examine how data travels on these two types of transmission media and their security vulnerabilities.\nWireline Transmission Media\nThe two types of wireline transmission media are copper cables and fiber optic (FO) cables. Copper cables transmit data through voltages (electrical signals), called electromagnetic (EM) waves, as pulses (5 volts representing 1) or no pulses (representing 0). The data from these cables can be stolen by placing a device close to the cable, which can sense pulses and covert them back to 1s and 0s. To avoid this, some copper cables are shielded (covered with aluminum foil) to stop pulses from being detected externally. One typical example of this type of cable is TV (coaxial) cable that not only shields data being braided but can also transmit it at multiple voltage levels, thus catering to the needs of TV, internet, and digital phone requirements simultaneously. Contrary to the copper cables, FO cables are made of glass or highly transparent plastic and transmit data as light pulses. Its twofold benefits include data transmission at almost the speed of light (several times faster than copper cables) and data security, as light pulses cannot be detected externally. Besides being fast and secure, FO cables transmit data in multiple frequencies, enabling several data channels in one cable (video, internet, phone). Verizon FiOS is one such example.\nWireless signals are also EM waves, but unlike the wireline media, they travel through the air or atmosphere at different frequencies (amplitudes). They are advantageous in areas where wired networks cannot be set up (old and historical buildings) or are required temporarily (training or classroom setup). Because the wireless signals radiate in all directions and can pass most physical barriers, they are very valuable in hospital environments or healthcare organizations with mobile workforces. However, they are the least secure, as they can easily be intercepted by monitoring devices, and data can be interrupted, interjected, modified, stolen, rendered unusable, or even destroyed.\nMobile Technology (mTechnology)\nmTechnology is based on wireless technology that moves with the user. It is a portable two-way communication network and includes internet-enabled devices like smartphones, tablets, notebook computers, and navigation devices. mTechnology touches all aspects of our daily lives from starting the day with the alarm clock in your phone; managing the day through your calendar, reminders, and notes; communicating through emails, texts, social media; and accessing online information like maps, books, magazines, files, photos, and videos. The common four types of mTechnologies are:\n- Cellular or radio networks: Use distributed cell towers that enable mobile devices (cellphones) to switch frequencies automatically and communicate without interruption across large geographic areas.\n- Packet switching technology (5G network): Organizes data into packets for transmission and reassembles packets into information at destination.\n- Wi-Fi: Radio waves that connect devices to internet through localized routers.\n- Bluetooth: Connects devices over short distances using short-wavelength radio waves.\nMobile Health (mHealth)\nmHealth is the use of mTechnology in healthcare. The World Health organization (WHO) defines mHealth as the “use of mobile and wireless technologies to support the achievement of health objectives,”1 while the National Institutes of Health (NIH) describes it as “use of mobile and wireless devices to improve health outcomes, healthcare services, and healthcare research.”2\nThus, mHealth is the practice of medicine and healthcare over mobile devices, like smartphones, tablets, and iPads, such as eVisit (only mode of remote healthcare delivery service during COVID-19 pandemic). Although traditionally used for wellness management, their use for healthcare both by patients and providers has increased exponentially.3\nCurrently, mHealth technologies are being used for patient monitoring, patient-provider communication, telehealth, and e-health. Although the lines between telehealth (also referred to as telemedicine) and e-health are blurred, they are two different technologies. E-health is the electronic communication of information for improving patient’s health, while telehealth uses video, smartphones, or any wireless tools or telecommunications technology (network/internet) for specific healthcare delivery and is named after the type of healthcare service: telenursing, telepharmacy, telerehabilitation, teleradiology, teletrauma care, telepsychiatry, telepathology, and teledermatology. Telehealth requires license and an infrastructure to practice, while e-health has no such requirements. Both became routine during COVID-19 pandemic.\nThe global health data breaches in 2020 increased to 24.5 percent against financial services breaches (7.3 percent), once being the reverse.4 Thus, it is pertinent to elaborate on some mHealth security issues to understand their specific security vulnerabilities.\nMobile devices use air interface to communicate and transmit data through the atmosphere. The wireless medium is broadcast (signals diffuse in all directions), and signals can pass through physical barriers (e.g., walls, roofs, and concrete). They are vulnerable to active (through injecting, deleting, altering the message) or passive (redirecting the information) attacks. The signals traveling through the atmosphere can be intercepted, modified, destroyed, hacked, and rendered unusable through ransomware. Being portable, mobile devices are small and lucrative and can be lost or stolen along with data, have low processing power to handle encryption, and can use public unsecure Wi-Fi. Bring your own device (BYOD) policies poses additional risks of healthcare cybersecurity, as healthcare workers can use their own devices to access organizational network, access patient data, and enter medical orders. BYOD policies are advantageous because of higher productivity, as the providers can access health information from anywhere and anytime; they increase job satisfaction by supporting flexible work arrangements; and they increase effectiveness due to more comfort and speed with the use of people’s own devices. However, they suffer from several disadvantages, like data breaches due to lost or stolen personal devices, lack of firewall and anti-virus software, dearth of encryption power, and increased IT costs for supporting personal devices.\nHIPAA and mHealth\nHIPAA requires security of the protected health information (PHI) through its Security Rule that mandates the security of health information through a three-pronged approach: administrative (policy-based), physical (physical access to facility, workstation, and storage devices), and technical (technological requirements to control access to information).5 mHealth devices have potential HIPAA Security Rule compliance issues such as physical loss or theft of devices, data transmission over unsecured Wi-Fi, unencrypted text messages/emails, inadequate or lack of authentication protocols, and poor adherence to BYOD policies and procedures.\nBeside the potential to have HIPAA security compliance issues, other specific mHealth device issues include interference with implanted medical devices (pacemakers) and their use in monitoring vital signs and supporting life-threatening and critical care situations that use a part of electromagnetic spectrum, like mobile devices. They also suffer from diagnostic interpretations due to small screen size, which may conceal some crucial details in EKG, ultrasound, MRI, and X-rays.\nBest Security Practices for Securing mHealth Devices\nAlthough no technique or technology can provide fool-proof security for the data contained in mHealth devices or accessed through them, some measures can minimize the security risks. These include implementation of strong user authentication (biometric or two-factor authentication), automatic lock after excessive number of incorrect logins, remote wiping when a device is lost or stolen, employing encryption for conversations (emails, text messages), developing an application policy for BOYD devices, regular updating to keep vulnerabilities mitigated, and installing security programs/antivirus, as being networked they are apt to all sorts of malware attacks.\nInternet of Things in Healthcare (IoHT)\nYet another breed of mHealth devices are Internet of Healthcare Things (IoHT) or Internet of Medical Things (IoMT) devices, which is the use of Internet of Things (IoT) in healthcare. IoHT devices are sensor-based interconnected devices used for tracking assets and resources. In hospitals, they can be used for locating patients, medical staff, and visitors (called smart hospitals), and collection and integration of health data (generation, collection, and communication of health data through wearable devices).6 Other benefits of IoHT devices include savings in medical cost, reduction in medical errors, improved patient experience, manageability of medical drugs and adherence, better control over wastage in healthcare sector, and better outcomes of medical treatments.\nSecurity Mitigation of IoHT Devices\nMitigation of security for IoHT devices include both technological and administrative measures. The technological mitigation measures comprises use of blockchain technology to protect data in IoHT devices, enable IoHT devices to use authentication to validate user identity and access privileges, enable IoHT devices to use encryption for all health-related communication, enable integrity on IoHT devices to verify devices to ensure that they are unaltered and uninterrupted, and ensure IoHT devices are patched and updated to avert any vulnerability.\nThe administrative mitigation measures comprise using the principle of least privilege for required actions and types of communication, logging all user activities and events and monitoring them regularly for unusual activities (duplicate device ID or elevation in privileges), reviewing device data regularly to identify unusual trends or patterns, implementing security best practices associated with protecting and securing sensitive data, and conducting formal education, training workshops, certifications, and participation in mobile security conferences.\nFuture of IoHT\nIoHT devices have a great future in healthcare applications, especially through the integration of artificial intelligence with IoHT in the form of ingestible sensors (ePills, which are pill-sized devices to monitor internal physiology and act as diagnostic devices sending medical information and images to outside connected devices); nano-devices to monitor human physiology and deliver drugs to targeted areas like cancer cells; connected lenses that would determine tear glucose and eyes diseases; and blood clot monitoring sensors to avert heart attacks. Their potential healthcare implications include smart hospitals, virtual clinics, microsurgery, vital sign monitoring systems (which can analyze real-time data inputs from critically ill intensive care unit patients), activity trackers for heart patients to collect lifestyle information, and fitness wearables, to name a few.\nIn view of the significance of mTechnology in healthcare, the Journal of Mobile Technology in Medicine is published to disseminate the mHealth research results.\n- World Health Organization. https://www.who.int/goe/publications/goe_mhealth_web.pdf\n- National Institutes of Health. https://grants.nih.gov/grants/guide/pa-files/PAR-14-028.html\n- Phaneuf, A. “How mHealth apps are providing solutions to the healthcare market’s problems” (2019). Retrieved form What Is MHealth? Apps, Examples & Mobile Health Industry Trends (businessinsider.com).\n- US Department of Health & Human Services. “A Retrospective Look at Healthcare Cybersecurity.” 2020. https://www.hhs.gov/sites/default/files/2020-hph-cybersecurty-retrospective-tlpwhite.pdf\n- US Department of Health & Human Services. Summary of the HIPAA Security Rule Guidance Portal. https://www.hhs.gov/guidance/document/summary-hipaa-security-rule-1\n- Bajwa, M. “Opportunities and Challenges for Internet of Things in Healthcare (IoHT).” International Journal of General Medicine and Pharmacy (IJGMP): Vol. 9 (2020): 1-4.\nMohammad Bajwa ([email protected]) is a professor of health informatics at the University of Maryland-Global Campus.\nLeave a commentSyndicated from https://journal.ahima.org/mobile-health-mhealth-security-matters-and-mitigation/']	['<urn:uuid:8fcf56d3-a1b5-4cde-ad60-f9c555f84998>', '<urn:uuid:556cfcd0-022c-4723-a4e5-a1f05c8b063e>']	factoid	with-premise	concise-and-natural	distant-from-document	multi-aspect	novice	2025-05-13T01:23:33.086345	11	75	2889
15	professional dancing career prospects after retirement physical toll student preparation learn	Professional dancers typically retire before age 30, often transitioning to teaching or other occupations due to the physical demands of ballet. However, dancers can prepare for successful careers through proper training approaches. As emphasized by Alexandre Proia, students should focus on enjoying movement while maintaining strong technical foundations. He encourages dancers to discover their personal artistic voice while mastering essential basics. Additionally, students must be aware of injury risks and follow preventive measures like proper shoe fitting, appropriate training progression, avoiding dancing on hard surfaces, and not attempting pointe work before age 11-12 to prevent growth-plate injuries.	"['We want your feedback!\nBy Lauren Kay\nAlexandre Proia dances in splashes of colorful movement. At 6\' 2"", he streaks across a studio with an angled yet fluid phrase, something closer to a snippet of choreography than a classroom combination. Moments later, shifting his focus from artistry to technique, he speaks about using energy from the toes to initiate a passé.\nA native of Longwy, France, Proia honed his blend of dramatic flair and attention to detail as a student at the Paris Opéra Ballet School before joining Boston Ballet in 1980. From 1984 to 1994 he danced with New York City Ballet—where, as a principal, he partnered Suzanne Farrell, Merrill Ashley, and other great ballerinas—and later performed on Broadway, in the Martha Graham Dance Company, and with Martha Clarke. Proia now teaches at New York’s Peridance Capezio Center and the Joffrey School, while shaping the beginnings of his own company. Lauren Kay watched his class at Peridance and found out how he helps dancers discover their own voice.\nWhich teachers and choreographers influenced you? I feel very lucky to have such a rich background working with people from Balanchine to Robbins to Forsythe. Jirí Kylián taught me respect for the body. It’s not a machine; it’s a spirit. He taught me how to help dancers be involved, not imprisoned.\nAlthough nobody can teach you how to teach, Stanley Williams at SAB would enter the room and in five minutes he’d have you sweating like hell. He treated the studio like a temple, and our concentration increased dramatically. Violette Verdy taught me intelligence, humor, and, most important, drama. In the theater, that’s what the audience is paying for.\nHow would you describe your approach to teaching? I use ballet basics as a foundation, but I like to bring on a mood. I want dancers to not take themselves too seriously, but take the moment seriously instead. I also stress enjoying, celebrating. You should dance for the fun of moving, inside and out. Dance is not just a dream, it’s something to live by. Many professional dancers forget that. I want my students to take the base I provide and extrapolate so that dance becomes personal again. I help them use dance as a language. Hopefully, they’ll have something to say.\nYour combinations seem more like pieces of choreography than classroom exercises. How do you think this benefits dancers? Longer pieces help with stamina and memory. Also, many students don’t have the chance to go onstage or don’t do so frequently, so this gives them the opportunity to express themselves. I allow students to tweak my choreography as they see fit. For example, maybe a Graham dancer comes to my class but wants to practice a certain movement the way it will relate to Graham technique. When she does that, I know she is present. I want my students to ask, with these mini-choreographies, “What can this combination do for me?”\nWhat is the relationship between style and technique? I think dancers should cook with as many ingredients as possible. I’m not a purist—although I think technique is essential. For me, style comes in partly as a way for students to overcome fear. If you put a fouetté into choreography in class, then it becomes a part of the conversation of dance, not a scary exercise. I think, How can I empower them to go for it, to have that feeling of freedom while still using clean basics?\nI also consider level. In a conservatory setting, there is a curriculum to stick to. But if I’m teaching company class, I think about what the dancers performed the night before, or will perform. At an open studio class, I demonstrate and suggest broader technical terms, like “find opposition here,” but again, each dancer can make her own choice. I don’t like to “dress the Christmas tree” too much!\nWhat do you feel dancers today are lacking, compared to when you were a student? They lack a sense of knowing why they dance and the desire to question themselves. I think some of this is not their fault and comes from the lack of connection to choreographers. When I was coming up, dancers had someone they were working with, or aspired to work with, like Graham or Balanchine. Today, I find, dancers are left alone with less specific goals. Choreographers have multiplied, and many of them just want dancers to show the most difficult movement, to break the body and then move on. It isn’t beneficial to anyone, really.\nWhat parts of French style and technique do you offer your students? I like to think of a French calf and batterie, a Russian back, and an American mind! At Paris Opéra Ballet School, we learned the importance of plié and speedy feet. I love the strong spine and épaulement the Russians insist on. And I admire the progressive mindset American dancers seem to have. I also hope to convey the respect for our craft that I feel is very French, although of course not exclusively so. The focus I learned at Paris Opéra Ballet School helped me to see that the more you concentrate, the more you empower the moment and yourself. The more you respect the art, the more you grow.\nPhoto by Sarah Keough', 'Ballet dancing has been a popular art form for centuries, and for those who have never danced, it has tremendous appeal as either a hobby or a potential vocation.\nChildren—especially girls—are often fascinated by the apparently effortless grace with which dancers move, and by the colorful, intricate costumes they sometimes wear.\nBut ballet has a darker side, and the placid expressions worn by dancers often mask terrible pain caused by ballet injuries.\nThe careers of professional dancers are often short; before they turn 30, most of them retire from dancing, either to teach or to pursue some other, unrelated occupation.\nThe purpose of this article is not to discourage children or their parents from considering ballet lessons, or even to discourage anyone from pursuing ballet as a career; rather, our purpose here is to make the reader aware of the risks that ballet injuries can pose to their future well being.\nTypes of Ballet Injuries That Can Happen\nMany of the various injuries dancers can suffer are not severe, and pose no serious threat to the dancer’s future as a performer, or to his or her ability to participate in other sports or to walk with a normal gait.\nOther types of ballet injuries, however, can be cause for concern. As you read about the various types of ballet injuries, feel free to click on any of the related links to read more about them.\nInjuries and conditions that can be caused by ballet include:\n- Bunion or hallux valgus (sometimes at an unusually young age)\n- Ankle sprains (particularly later ankle sprains)\n- Stress fractures brought on by small, repetitive impacts over the course of time\n- Trigger toe\n- Shin splints\nDancer’s Fracture is the most common acute fracture suffered by dancers. It strikes the fifth metatarsal, the bone that lies along the outer edge of the foot. It usually happens when a dancer jumps and lands badly, coming down on an inverted (turned-in) foot. As the name “dancer’s fracture” suggests, this is a common ballet injury.\nSesamoiditis: When a dancer is on demi-pointe, her weight rests on the sesamoid bones, which lie just behind the big toe. Regular application of this kind of stress can cause gradual onset of sesamoiditis, and the dancer will experience pain when bending or straightening the big toe.\nPlantar fasciitis: This is an inflammation of the plantar fascia, a band of tissue running along the bottom of the foot from heel to toes. The injury is related to overuse.\nAnkle Impingement: Many variations of this condition exist, but the one that concerns us here is posterior ankle impingement syndrome, also known as os trigonum syndrome, and commonly referred to as “dancer’s heel.” This happens when soft tissue becomes trapped and pinched by bones, which causes painful spurs to form.\nAchilles Tendonitis: This condition can be brought on by dancing on a floor that is not properly “sprung,” or in other words is too hard. Overtraining can also cause Achilles tendonitis, especially hard training in a short time after a period of inactivity.\nCuboid Syndrome: Repetitive movement (such as one does when training) or sprains (another common ballet injury) can cause cuboid syndrome, a painful misalignment of the cuboid bone on the outer edge of the mid foot.\nMetatarsalgia: This condition generally manifests itself as pain in the ball of the foot, and in dancers it is often brought on by years of overwork and by forcing the foot into extreme positions, leading to instability in the joints of the toes.\nWhat Causes Ballet Injuries?\nThe particular causes of particular ballet injuries vary, of course, depending on the movement being performed. There are, however, certain techniques and types of footwear used in ballet that are particularly dangerous.\nPointe Technique: Dancing en pointe puts a great deal of stress on various parts of the foot and toes. Poor technique or poorly fitted shoes can make matters worse and increase the chances of injury.\nAlthough dancers who practice this technique wear shoes designed for it, dancing en pointe still causes friction between the toes and the shoes themselves, which can cause chaffing and blistering. En pointe dancing is the culprit in many cases of bunions, hammertoes, sesamoiditis, bursitis, trigger toe, and stress fractures.\nWhat Long-Term Complications Can Result from Ballet Injuries?\nThe long-term complications associated with ballet injuries are as varied as the injuries themselves, but the outlook for such injuries if they are not treated usually involves chronic pain and possibly antalgic gait (or in layman’s terms, a limp).\nHow You Can Prevent Ballet Injuries\nBallet injuries can be prevented by making sure that you are wearing properly fitted shoes, especially if you are dancing en pointe.\nThere are professionals you can see who know how to fit you properly for pointe shoes; ask your ballet instructor how to find one. Once you have your shoes, take proper care of them.\nDancers should not try dancing en pointe before the age of 11 or 12. By doing so, they can risk growth-plate injuries, and the bones of their feet may develop improperly.\nIndeed, no dancer should ever attempt a feat that he or she is not ready for. As in sports, proper training in dancing is crucial to injury prevention.\n- Ensure proper nutrition.\n- If you increase your training schedule, take it slowly; don’t suddenly go from once a week to every day.\n- If you spend most of one workout en pointe or demi-pointe, focus your next workout on something else.\n- Avoid dancing on hard surfaces.\n- Avoid dancing on uneven surfaces.\n- If you are in pain, STOP!']"	['<urn:uuid:d07eab21-52e1-43b3-b3aa-3ab35e33ae1c>', '<urn:uuid:4aea92b0-3025-42b6-847a-19af540fd1e8>']	open-ended	direct	long-search-query	distant-from-document	multi-aspect	novice	2025-05-13T01:23:33.086345	11	97	1821
16	specialized md query what percentage maternal deaths caused by venous thromboembolism vte during pregnancy globally	Venous thromboembolism (VTE) accounts for approximately 9% to 31% of pregnancy-related deaths, depending on the country.	"[""NPMS Releases Consensus Statement on Venous Thromboembolism During Pregnancy\nAm Fam Physician. 2017 Mar 15;95(6):397-398.\nAuthor disclosure: No relevant financial affiliations.\nKey Points for Practice\n• At the first prenatal visit, VTE risk should be evaluated and prophylaxis should be given based on recommendations similar to those from the ACOG and ACCP.\n• Antepartum women at low risk of bleeding or impending birth who have been in the hospital for 72 hours or more should be given LMWH once per day or UFH twice per day.\n• Postpartum women with multiple previous VTE episodes, previous VTE and high-risk thrombophilia, or previous VTE and acquired thrombophilia should be given treatment-dose LMWH or UFH for six weeks.\nFrom the AFP Editors\nMaternal morbidity and mortality are commonly caused by venous thromboembolism (VTE), which accounts for approximately 9% to 31% of pregnancy-related deaths, depending on the country. The main strategy for preventing VTE-related mortality is thromboprophylaxis, but guidance regarding its use differs across specialties and organizations, including the Royal College of Obstetricians and Gynaecologists (RCOG), the American College of Obstetricians and Gynecologists (ACOG), and the American College of Chest Physicians (ACCP).\nFor this reason, the National Partnership for Maternal Safety (NPMS), a group made up of a variety of leaders in women's health care, has created a safety bundle to review current guidance and make recommendations. The aim is to have all the birthing centers in the United States adapt this bundle to their patients and resources, rather than a single national protocol. The bundle is an overview of current recommendations to help guide application and create uniformity across practices. This document was developed by official representatives from the American Association of Blood Banks; the American Academy of Family Physicians; the American College of Nurse-Midwives; the ACOG; the Association of Women's Health, Obstetric and Neonatal Nurses; the Society for Maternal-Fetal Medicine; and the Society for Obstetric Anesthesia and Perinatology.\nAt the first prenatal visit, VTE risk should be evaluated. Prophylaxis should be given based on recommendations similar to those from the ACOG and ACCP. Women with multiple previous VTE episodes, previous VTE and high-risk thrombophilia (e.g., factor V Leiden homozygosity, prothrombin gene mutation homozygosity), or previous VTE and acquired thrombophilia (e.g., antiphospholipid antibody syndrome) should be given a treatment dose of low-molecular-weight heparin (LMWH) or unfractionated heparin (UFH). Those with previous idiopathic VTE, VTE with pregnancy or oral contraceptive use, or VTE and low-risk thrombophilia (e.g., prothombin gene mutation heterozygosity, protein C or S deficiency); high-risk thrombophilia (including acquired); or a family history of VTE with high-risk thrombophilia should be given a prophylactic dose of LMWH or UFH. No treatment is needed with a history of provoked VTE or a low-risk thrombophilia with or without a family history of VTE.\nWomen receiving VTE prophylaxis combined with low-dose aspirin to prevent pre-eclampsia, should stop taking the aspirin at 35 to 36 weeks' gestation.\nAntepartum women at low risk of bleeding or impending birth who have been in the hospital for 72 hours or more should be given LMWH once per day or UFH twice per day. If hospitalized, those already receiving UFH or LMWH should continue this regimen. Mechanical thromboprophylaxis or a prophylactic dose of UFH is indicated in women with a high risk of bleeding or impending birth. Using UFH instead of LMWH may also make it easier to perform intrapartum neuraxial anesthesia (epidural).\nUse of pneumatic compression in bed in intrapartum women and administration of LMWH or UFH in postpartum women are recommended for those with a history of VTE or thrombophilia. Prophylaxis with LMWH or UFH is an option in women with a high risk of VTE based on RCOG criteria or Padua Prediction Score of at least 4. The Padua score and the Caprini score are commonly used VTE-assessment tools in nonobstetric patients. These tools have been modified for use in obstetric patients and can be found online at http://links.lww.com/AOG/A834.\nUse of pneumatic compression perioperatively is recommended in women having a cesarean delivery if they are not receiving pharmacologic prophylaxis; either option should be used until the patient is able to walk. In those with risk factors, pharmacologic thromboprophylaxis is recommended. Because it is often difficult to consistently determine which women have risk factors and because compliance with mechanical devices is often poor, birthing centers can opt to have all women undergoing cesarean delivery receive postoperative thromboprophylaxis with UFH or LMWH, unless it is contraindicated. The best time to provide heparin after surgery is unknown; however, routinely providing prophylactic UFH to those ready for discharge from the postanesthesia care unit is appropriate.\nThe guidance for assessing risk and providing thromboprophylaxis in women requiring additional anticoagulation postpartum is based mainly on the ACOG and ACCP recommendations. Women with multiple previous VTE episodes, previous VTE and high-risk thrombophilia, or previous VTE and acquired thrombophilia should be given treatment-dose LMWH or UFH for six weeks. Those with previous provoked or idiopathic VTE, VTE with pregnancy or oral contraceptive use, or VTE and low-risk thrombophilia; high-risk thrombophilia with or without a family history of VTE; or low-risk thrombophilia with a family history of VTE should receive prophylactic-dose LMWH or UFH for six weeks. No treatment is needed in the presence of low-risk thrombophilia alone.\nGuideline source: National Partnership for Maternal Safety\nEvidence rating system used? No\nLiterature search described? No\nGuideline developed by participants without relevant financial ties to industry? Yes\nPublished sources: J Obstet Gynecol Neonatal Nurs. September–October 2016;45(5):706–7017\nJ Midwifery Women's Health. September 28, 2016; 61(5):649–657\nAnesth Analg. October 2016;123(4):942–949\nObstet Gynecol. October 2016;128(4):688–698\nCoverage of guidelines from other organizations does not imply endorsement by AFP or the AAFP.\nThis series is coordinated by Sumi Sexton, MD, Associate Deputy Editor.\nA collection of Practice Guidelines published in AFP is available at https://www.aafp.org/afp/practguide.\nCopyright © 2017 by the American Academy of Family Physicians.\nThis content is owned by the AAFP. A person viewing it online may make one printout of the material and may use that printout only for his or her personal, non-commercial reference. This material may not otherwise be downloaded, copied, printed, stored, transmitted or reproduced in any medium, whether now known or later invented, except as authorized in writing by the AAFP. Contact email@example.com for copyright questions and/or permission requests.\nWant to use this article elsewhere? Get Permissions\nMore in AFP\nMOST RECENT ISSUE\nNov 15, 2020\nAccess the latest issue of American Family Physician""]"	['<urn:uuid:8f33e4c9-c2a9-47f9-be61-a22eca0be622>']	factoid	with-premise	long-search-query	similar-to-document	single-doc	expert	2025-05-13T01:23:33.086345	15	16	1065
17	Which plant handles more wastewater: Glen Illyn or Lawrence?	Glen Illyn's Glenbard Wastewater Treatment Plant handles more wastewater, with a normal daily design flow of 16 MGD (million gallons per day) and peak design flow of 47 MGD, compared to Lawrence's Wastewater Treatment Plant which can treat 12.5 MGD with peak flows of 25 MGD through normal process.	"['Glen Illyn, IL, is a fairly typical American suburb, with 27,000 residents living about 40 minutes from the bright lights of Chicago.\nThe local wastewater treatment plant, the Glenbard Wastewater Authority’s Advanced Wastewater Treatment Facility, is typical as well. It has a normal daily design flow of 16 MGD and peak design flow of 47 MGD. Its advanced treatment includes primary screenings; grit removal; primary clarification; high-purity, oxygen-activated sludge; clarification; tertiary treatment; and UV disinfection.\nBut in one key way, Glenbard is different than many other wastewater treatment plants around the country. It had grown interested in resource recovery and utilizing the influent it treated to help power and fund its operations and that interest has blossomed into an estimated $200,000 saving in annual energy costs.\n“Glenbard was interested in finding ways to harvest existing resources, not only to save money, but also to become a leader in the wastewater industry through innovation,” said Chris Buckley, a project manager with Baxter & Woodman Inc., a local firm of consulting engineers. “When the authority was offered a significant grant to fund a combined heat and power (CHP) facility, they decided to pursue this opportunity.”\nThe facility was using the biogas created during treatment to fire hot water boilers and heat digesters, but the excess biogas was flared, or burned away, as it is at many treatment plants. Glenbard envisioned a CHP system that would capture excess biogas and put it toward powering its wastewater treatment process.\n“A CHP system is advantageous because that biogas is used not only to generate thermal energy, but also to generate electricity,” Buckley said. “The combined energy delivery of CHP varies between 60 and 90 percent. This efficiency is much higher than the separated efficiencies of conventional electrical generation, which is 33 percent, and hot water boilers, at 80 percent. In other words, CHP produced more energy with less fuel.”\nAlong with Boller Construction, Baxter & Woodman was awarded the project on a design-build contract and 12 months later, it was completed.\n“Now, the biogas is conditioned and used to fuel the CHP system,” said Buckley. “The heat produced by the CHPs is captured and recycled to heat the digester heat exchangers and, at the same time, generate electricity to offset the plant’s electric needs.”\nFurthermore, the facility introduced a high-strength waste receiving program that takes in new sources of organic waste, like restaurant grease and food processing byproducts. This is also treated in the digesters to produce biogas and reduces waste that would otherwise head to landfills.\nThe project included the installation of two CHP units that consist of a high-efficiency, turbo-charged engine; an electrical generator; and a heat recovery unit. Each is capable of generating 375 kW of usable electrical energy and 1.44 million BTU/hour of usable heat. It meets about 60 percent of Glenbard’s total electrical requirements and all of its digester heating needs.\n“The engine burns biogas to generate mechanical energy, which is used by the generator to produce electricity,” said Buckley. “Thermal energy produced by the engine and generator is recovered through heat exchangers and a hot water loop. The hot water loop conveys the thermal energy wherever it is needed.”\nThe project also required a biogas conditioning system, a step to keep uncleaned biogas from harming the CHP units. It consists of filters with absorptive media that treats the biogas, including iron oxide to remove hydrogen sulfide and activated carbon to remove siloxanes and organic chemicals. As these media are fouled, they will require replacement.\nA new building was installed on the site to house the CHP engines and the conditioning system, as well as an outdoor area for the conditioning filter vessels. Hot water piping was needed to connect the CHPs to the digester boiler and heat exchangers and electrical connections were added to link the CHPs’ output to the plant’s power grid.\nThe project earned Baxter & Woodman a national recognition award for exemplary engineering achievement from the American Council of Engineering Companies.\n“It is a great honor for this local project to receive this prestigious, national recognition,” Buckley said. “It encourages us and our clients to keep working hard toward providing innovative solutions that are cost effective, energy efficient, and make a sustainable use of our resources. Most importantly, such recognition helps the public learn and understand what our industry does and the opportunity to take pride in projects which may be in their neighborhood.”\nSuch recognition may encourage other wastewater treatment facilities to pursue a biogas recovery project as Glenbard did. Baxter & Woodman has installed other such projects at regional plants and touts the solution as one that can benefit any facility with electric and thermal energy needs, especially those that operate anaerobic digesters.\nImage credit: ""_AAC6084"" Stine Johannessen © 2013 used under an Attribution 2.0 Generic license: https://creativecommons.org/licenses/by-sa/2.0/', '- Featured Items\n- Biosolids Management\n- Fats, Oil & Grease (FOG)\n- Lawrence Utility Management System (LUMS)\n- Utility Projects\n- Wakarusa Wastewater Treatment Plant\n- Water Conservation\n- Rapid Inflow & Infiltration Reduction Program\n- Department Information\n- Quick Links\n- Featured Items\nThe City of Lawrence Wastewater Treatment Plant, located at 1400 East 8th Street, has the capacity to treat approximately 12.5 million gallons per day, with peak flows of 25 million gallons per day through the normal process and an additional 40 million gallons through the Excess Flow System during extreme rain events. City staff are responsible for the plant\'s efficient operation and maintenance.\nWastewater is defined as water that has been used for domestic or industrial purposes. Wastewater treatment is the physical, chemical, and biological processes used to remove pollutants from wastewater before discharging it into a water body. The various treatment stages are described below.\nInfluent Pump Station:\nThe influent pump station receives wastewater from the wastewater collections system through sewer lines located 25-30 feet underground. At the front of the pumping station wastewater passes through mechanical bar screens that remove large debris (3/4"" or greater) such as boards, rags, and paper, which would clog pumps and damage equipment. These materials are mechanically raked off and landfilled.\nThe influent pump station houses 4 pumps with a pumping capacity of approximately 25 million gallons per day. On an average day they will pump approximately 9 million gallons of wastewater, lifting it 25 feet and delivering it to the grit chamber so that it may continue through most of the plant processes by gravity. Although the collection system is designed to receive only sanitary wastewater, imperfections in the system allow groundwater and rainwater to infiltrate and inflow (I&I) into the system. Peak flows during rain events can exceed the 25 MGD pumping capacity of the influent pump station, which then requires the excess flow to be treated through the excess flow system.\nThe screened wastewater proceeds to the grit complex where heavy inorganic solids, such as sand or gravel (grit) are allowed to settle while lighter organic solids are kept in suspension. The grit is removed, washed and dewatered by mechanical methods prior to disposal in a sanitary landfill.\nThe wastewater is divided between two basins, which use gravity to settle the solids to the bottom of the basin where they are removed for treatment. This process uses detention time of approximately 3-4 hours to allow for the particulates in the wastewater to settle out. Approximately 65% of the suspended solids are removed in his process with the remainder being dissolved and un-settlable pollutant solids.\nSecondary Treatment of Activated Sludge:\nAeration basins are used to accelerate the naturally occurring breakdown of waste that occurs in the environment. The alkalinity, dissolved oxygen, and mixing are adjusted to encourage optimum microbial activity. Diffused air ensures that the contents of the basin are mixed and to provide oxygen. The dissolved oxygen is used by the microbes to respire (breath) as they consume food (wastewater solids). As the microbes consume the food their population increases. The quantity of microbes is adjusted to balance with the volume of waste received.\nFinal sedimentation basins are used to hold the wastewater for a period of time. Through the process called flocculation the microbes and remaining waste material form particulate that are heavier than water and settled to the bottom of the basins. Portions of the solids settled are continually removed and sent back to the aeration basins to maintain the microbial population there. The excess solids are periodically removed and sent to the anaerobic digesters.\nThe wastewater (or called effluent at this point) proceeds to disinfection where liquid chlorine is added to kill most of the remaining pathogens (disease causing organisms).\nDue to the potential toxic effects of chlorine on aquatic life, sodium bisulfite is then added to remove any residual chlorine prior to discharge into the Kansas River.\nExcess Flow System:\nDue to imperfections in the collections system, flow to the wastewater treatment plant can increase significantly during substantial rain events. Due to the need to treat this excess water (mostly dilute), a system (called Actiflo) was developed to be used during these periods of high flow. The excess flows to the WWTP greater than 25 million gallons per day are pumped from the head of the plant by-passing normal treatment and through fine screens (to screen out any objects over the size of a pencil eraser). After screening, the flow proceeds to the Actiflo basins. There, ferric chloride, polymer, and micro-sand are mixed with the wastewater to form ballasted floc, which settles quickly and is removed from the flow. The clear effluent from Actiflo flows to a chlorine contact basin that is dedicated to this flow, where liquid chlorine is added for disinfection and sodium bisulfite is added to remove the residual chlorine (as takes place under normal treatment). The effluent from this process is combined with the normal plant effluent and discharged to the Kansas River. Small rain events may be diverted through the fine screens to the excess flow storage basin until flow subsides and it can be returned to the head of the plant. Although this system has been widely used in drinking water treatment, this was the first system to be installed and operational in the United States for use in wastewater treatment.\nThe solids that are removed from the primary and secondary treatment processes are stabilized by anaerobic digestion. This process is maintained at 98oF. In a controlled environment, a complex population of bacteria work in the absence of oxygen (called anaerobic respiration) to reduce and stabilize biodegradable waste products prior to removal. End products of this process include a stabilized organic material, carbon dioxide and methane. The boilers used to maintain the digester temperature use the methane from this process as their fuel source. The stabilization process takes from 14-30 days. The digested solids (now called biosolids) are removed from digester for dewatering.\nTwo 2-meter belt filter presses are used to remove excess water from the digested solids. The water is removed using gravity to drain excess water away and applied pressure between two perforated belts that squeeze as they move over rollers. The biosolids enter the presses at approximately 2% solids and leave the presses at approximately 20% solids. The biosolids are transported to storage bays via a series of conveyors. The excess water removed from the biosolids (called filtrate) is returned to the headworks of the plant for treatment through the normal process.\nStorage exists for approximately 4000 cubic yards of biosolids. These bays are designed for 120 days of storage at the year 2020. It is divided into two cells to allow for removal from one while the other is being used. Each cell has large water tight gates which when opened allow for access with a front end loader for easier removal of the material.\nBiosolids End Use:\nApproximately 95% of the biosolids produced at the Lawrence Wastewater Treatment Plant are beneficially reused as a nutrient source and soil conditioner on privately owned agricultural land. The treated biosolids contain three key crop nutrients - nitrogen, phosphorus, and potassium, as well as other nutrients in trace amounts. Organic matter supplied by the biosolids improves soil structure, increases the soil\'s ability to absorb and store water, and increases availability of the nutrients to the crop.\nThe City of Lawrence has recycled biosolids in this manner since the mid-1970\'s on City owned and privately owned agricultural fields. With the adoption of the federal regulations (EPA 40 CFR Part 503) which regulate the disposal and reuse practices for biosolids, the City developed its Biosolids Beneficial Reuse Program. Through this program, biosolids are applied to local agricultural land at no cost to the participating farmer or landowner. A private contractor that specializes in the land application of biosolids takes soil samples, compiles reports and performs the application activities with oversight of city staff. The program applies biosolids and its nutrients to approximately 400 acres annually.\nThe remaining 5% of biosolids production is set aside for residential use by the public. The material is tested extensively and upon meeting compliance with federal regulations is made available for use on residential lawns, gardens, flowers, and other residential uses. The practice of making biosolids available to the public in Lawrence for residential uses has taken place since 1956 when the plant was first constructed.\nThe biosolids must meet stringent testing and procedural requirements. The City\'s reuse activities are in complete compliance with all Environmental Protection Agency regulations and Kansas Department of Health and Environment guidelines. The City of Lawrence is committed to the safe, economical, and environmentally friendly reuse of this valuable material.\nTo learn more about wastewater, biosolids, or the City\'s EMS for Biosolids, please contact the Utilities Department at 785-832-7800.']"	['<urn:uuid:9be31f12-789c-4b2d-9526-64fef10ba547>', '<urn:uuid:21743cc8-dc93-40ce-af66-80f01cbd8dcc>']	factoid	direct	concise-and-natural	distant-from-document	comparison	novice	2025-05-13T01:23:33.086345	9	49	2277
18	what features does roboguide simulation software offer for robot motion verification	ROBOGUIDE simulation software offers several key features for robot motion verification: standard teach pendant instructions for throughput verification, 3-D robot work envelope view for effective cell layout, accurate cycle time calculation through simulation, visual collision detection before installation, program profiling to identify timing bottlenecks, and TP Trace capability that displays actual robot motion versus taught path on the teach pendant display, including speed, orientation and acceleration traces.	['Simulation & CAD-to-Path\nMotion Controls Robotics uses a variety of robotic simulation software tools to help customers visualize systems and determine cycle times, cell layout and longevity estimations. Using 3D CAD, ROBOGUIDE® and Extend simulation software, we can prove out different concepts for your robotic cell or entire lines to ensure a return on investment, with very little up-front cost. Motion Controls Robotics uses FANUC’s ROBOGUIDE® family of offline robot simulation software products to emulate your robotic cell. FANUC robots are exactly modeled and operate on a Virtual Robot Controller, allowing the user to precisely simulate a robotic process in 3D virtual space or conduct feasibility studies for robotic applications – without the physical need and expense of prototype work cell setup.\nMotion Controls Robotics also uses FANUC Robotics’ MotionPRO™ to optimize robot motion and reduce cycle time by as much as 5 to 20 percent, as well as a significant amount of teaching and touch-up time.\nMotionPRO Software Features\n- Optimizes cycle time by automatically adjusting motion parameters such as speed and acceleration\n- Achieves cycle time reduction with intuitive methods\n- Maintains the actual robot path for any desired program speed while passing through all the taught points within a specified tolerance\n- Displays the actual robot path in 3D\nDrastically reduce programming time by using comprehensive CAD-to-Path programming. Just define a feature line on a CAD model to quickly generate programs using the feature line information. This can eliminate hours of programming time and allows users to easily create robot paths from CAD data. CAD-to-Path supports generation of programs for coordinated motion, remote tool center point, multi-arm and other configurations. Motion Controls Robotics uses CAD-to-Path to import 3D drawings of existing parts, fixtures and grippers, build system layouts and to evaluate system operation. A user defines the process requirements on the CAD model, and ROBOGUIDE® automatically generates the robot program. Import unique CAD models of parts, create a work cell including machines, part transfer devices and obstacles, and teach robot paths to simulate the operation and performance of a robotic application. Reach verification, collision detection, accurate cycle time estimates and other visual system operations are simulated in ROBOGUIDE’s® unique and graphical virtual environment.\nSimulation Software Features\n- Simple robot instructions – Standard teach pendant instructions are supported by ROBOGUIDE® to develop teach pendant programs for throughput verification.\n- Robot reach check – A 3-D view of the robot work envelope that helps the user to layout parts, fixtures and other cell components effectively within the work cell.\n- Cycle time validation – Calculated, accurate cycle time data is generated by running the taught robot program in simulation mode.\n- Collision detection – Visual identification of collisions during robot simulation allows the relocation of the robot, tooling and/or parts before the robot is installed in the plant.\n- Profiler – The teach pendant program profiler allows programs to be reviewed quickly for timing bottlenecks and operational slowdowns. Processing problems are solved before the robot is placed into production.\n- TP Trace – A unique capability to display the actual robot motion versus the taught path on the teach pendant display. It includes a teach pendant trace by speed, orientation and acceleration, allowing touchup of the robot program before the robot is deployed.']	['<urn:uuid:73caa43d-2e34-4972-b527-b9fc0418440d>']	open-ended	with-premise	long-search-query	similar-to-document	single-doc	expert	2025-05-13T01:23:33.086345	11	67	544
19	aircraft hydraulic system design challenges manual requirements changing technology	Transitioning from centralized to distributed hydraulic systems faces both technical and regulatory hurdles. The technical challenges include: increasing HPP electric motor capability by 10x without proportional weight gain, managing hydraulic fluid behavior at high altitudes requiring pressurized reservoirs, and controlling noise from multiple HPPs cycling on/off. The regulatory side requires extensive documentation, including: a Statement of Compliance detailing how each FAA regulation is met, a General Operations Manual containing all operational policies and procedures, and a Training Manual specifying all required flight crew training. For aircraft with more than nine seats, an additional General Maintenance Manual with continuous airworthiness programs is mandatory.	"['Entrenched technologies are often thought to be flywheels of infinite mass: You can\'t change their speed or direction. Throw in a federal bureaucracy with absolute power over approval of a replacement technology and you have something that veteran engineers are very familiar with: A project with technical obstacles compounded by bureaucratic delays.\nDistributed Versus Centralized Systems\nAir loads on modern airliner flight surfaces and landing gear are high and mass is great so force multipliers — high pressure hydraulics — are the control systems of choice and have been for decades.\nHowever, because of the weight and complexity of the engine-driven, centralized system, fluid power companies are working on an alternative: A squadron of electrically powered Hydraulic Power Packs (HPPS) located near the actuators they drive, but far away from the rotating components of the jet engines.\nOperationally, this means that hydraulic systems become zonal, with long runs of vulnerable tubing eliminated along with many of the manifolds and significant quantities of hydraulic fluid. If access panels are intelligently co-located with the HPPS, maintenance time is reduced as well. Catastrophic failure modes are better controlled, system pressure losses are minimized, and a weight savings of around 15 percent is projected.\nAs an example of the current approach, the new Boeing 787 Dreamliner hydraulic system features a primary pump on each engine plus a third, slipstream driven RAT (Ram Air Turbine) to operate the actuators in an emergency. Separate, dedicated pumps cycle the landing gear. Filters, manifolds and flow directors are part of the system, and hydraulic fuses are employed to sense and seal problems in any of the three branches.\nThese standard systems work well, but are complex and hardware-intensive. And because there\'s so much high-pressure hydraulic tubing used, leaks are a frequent maintenance issue.\nSo it\'s no surprise that companies have been working on an alternative. ""Adoption of zonal hydraulics will have a definite effect on the design of airplanes,"" says John Halat, chief engineer, Advanced Development at Eaton Aerospace Group. ""Most of the major hydraulics suppliers have development activities in this area.""\nThe hydraulic industry\'s trade organization, the National Fluid Power Assn., believes the technology has potential once specific uses (such as zonal systems for airliners) have been identified and the technical challenges overcome. As a consequence, it has been working with its members to develop a road map for success in this area.\nAirliner Technical Challenges\nEngineers are no doubt busy working on the technical challenges, which inevitably will involve some wrestling with trade-offs in the choices to be made.\nHalat says he feels that one of the principle airliner challenges is to increase the capability of the current HPP electric motors by a factor of 10 without increasing the weight by anything near that amount. ""Today\'s 5 to 10-kW HPP motors are sufficient to operate many of the flight control actuators, but it takes from 50 to 100 kW to cycle the landing gear,"" he says.\nThere may be another way to skin this cat, though. Dr. John Hansman, a professor of aeronautics at MIT where he teaches aircraft system design, suggests another approach would be to charge up accumulators and use the stored energy to overcome the power shortage. In Hansman\'s view, this makes the instantaneous power requirements much lower, with the same results.\nAnother challenge for engineers is coming up with a strategy to manage the reaction of hydraulic fluids to high altitudes. Jet airliner systems require fluid reservoirs to be pressurized, because at high altitudes hydraulic fluids outgas and foam. In a centralized system that\'s easy to manage, as there is only a few large reservoirs. But in a distributed system there is a large number of small reservoirs that will need pressurization.\n""The physics is the same for the small HPPS, but if you pre-charge the reservoirs and you have good seals, the hydraulic fluid isn\'t likely to foam,"" says Hansman. The solution would require fitting all the HPPS with pressurized reservoirs and then monitoring the state of the air charge so that a pressure loss would be shown on the cockpit system status displays as an early indication of impending loss of a HPPS. No breakthrough technologies are required, but a well-executed design approach would be needed.\nAnother challenge is coping with the noise generated by multiple HPPS Flight Control Devices constantly cycling on and off. Halat and his group believe the best design approach is to locate the HPPS as far from the passengers as possible and acoustically shield those that have to be placed near the fuselage.\nHansman agrees. ""Placement of the HPPS as far away from the seating areas as feasible is critical, so the short rise time noise of the HPPS turning on and off doesn\'t frighten the passengers,"" he says. He also agrees that although acoustic shielding adds weight, it may be unavoidable in some cases.\nHalat and Hansman share an enthusiasm for the technology. They both think the use of HPPS would cut down on assembly time and total parts count, which the airframe manufacturers would love. But how much of a weight reduction will occur is up for debate.\nHalat says he believes a 15 percent reduction in weight will result from eliminating many of the manifolds and the need to run redundant hydraulic tubing throughout the wing and tail structure or fore and aft in the fuselage.\nHansman, on the other hand, thinks some weight reduction may result but he\'s not sure how high the percentage might be. ""It depends on the execution of the design. It certainly saves weight to run electrical cables instead of hydraulic tubing,"" says Hansman. ""But if you have to use up portions of the weight reduction budget compensating for other problems — like the HPPS noise issues or the landing gear motor size — the 15 percent reduction probably won\'t be realized.""\nThe FAA: Biggest Obstacle of All?\nGiven enough time and money, engineers are likely to solve all the technical challenges. But since a distributed hydraulic system has never been done before — at least not in airliners — convincing the FAA to certify it may prove to be the biggest challenge of all.\nIn order to be successful, fluid power companies and airliner manufacturers will have to work hand in hand. According to FAA airworthiness inspectors contacted by Design News, the provisions of the rules governing aircraft certification, (Federal Air Regulations, Part 21 and its subparts), indicate that an airframe manufacturer seeking an aircraft type certificate is ultimately responsible for policing the suitability of parts used in a new airliner.\nThe inspectors also point out that the parts manufacturers must adhere to the relevant provisions of the same FARs. Both hurdles have to be cleared.\nFAA certification personnel demand that any new way of doing things is, at minimum, as safe as the technology it\'s replacing. It matters not whether the new idea might save lives. Until proven that it will not place more lives in danger than the old technology, it\'s a non-starter.\nComputer simulations will help, but the fluid power companies and aircraft manufacturers must both demonstrate that they have rigid testing programs in place: programs that stress discovering potential failure modes, achieving repeatability of results, demonstrating adequate safety margins and use of redundancy where required. And the aircraft manufacturer has to demonstrate that the new system works in concert with the other aircraft systems and that it\'s at least as safe as the current centralized system.\nBut there\'s a good prospect of gold at the end of the rainbow. The requirements for certification are spelled out in the FAA\'s rules. If the fluid power companies and the aircraft manufacturers keep a copy of that guidance handy and keep the FAA advised of their actions every step of the way, there\'s no reason for this technology to remain characterized as ""emerging."" In a few years we could instead be calling it ""Leading Edge.""\nClick here for a larger version\nClick here for a larger version', 'Common manuals and documents to develop and authorizations you may need:\nThis list contains descriptions of the most common manuals and documents required for a Part 135 operator.\nStatement of Compliance\nThe Statement of Compliance is required for every level of Part 135 certification. The Statement of Compliance (sometimes called the Letter of Compliance) is a detailed listing of each FAA regulation that pertains to the operator, and an explanation of how the operator is in compliance or will comply with that regulation. In some cases, the means of compliance is stated in one of the operator’s required manuals, in which case that regulation may be referenced to the appropriate place in that manual.\nThis document must be very specific, and is one of the documents that many operators have difficulty preparing. Typically this document will run well over 100-150 pages.\nGeneral Operations Manual\nThe General Operations Manual (GOM) contains the majority of the policy and procedure for the Part 135 operator. The GOM will contain everything from personnel job descriptions and responsibilities, flight and duty limits, weight and balance procedures, and most other items that specify the methods by which the operator will conduct their operations.\nThe General Operations Manual is required for all operators except for Single Pilot, and in some cases, Single PIC. This document will often have 150-300 pages. See the GOM page.\nThe Training Manual (sometimes referred to as the Training Program), is the document that specifies the policies and procedures for training flight crewmembers. This document must properly specify all the FAA-required training items and specify the FAA-required ground and flight training hours. This manual will be used by company instructors to conduct all ground and flight training.\nSome training manuals are simple, containing the means for training one type of aircraft where all training is done by the operator itself (sometimes referred as in-house training).\nOther training manuals can be very complex, where multiple aircraft types are used, and training is a combination of in-house elements and training from an outside vendor, such as a simulator training company. Procedures for the qualification of instructors and check airmen can add to the complexity.\nTraining manuals can be 100 pages or 1,000 pages depending on this complexity.\nGeneral Maintenance Manual\nMost operators will not require a General Maintenance Manual (GMM). GMMs are required for aircraft with more than nine passenger seats, or for Part 135 Commuter certification.\nThe GMM typically consists of two parts: A Continuous Airworthiness Maintenance Program (CAMP) and a Continuous Analysis and Surveillance System (CASS). The CAMP contains the maintenance procedures for the operator as a whole, and for each aircraft type. The CASS is similar to a quality assurance process, designed to ensure that the maintenance processes are resulting in reliable, airworthy aircraft. The GMM will also typically contain the training program for maintenance personnel.\nThe continuous airworthiness maintenance program (CAMP, or sometimes CAP), is a very specific maintenance requirement for certain operators. If an operator has an aircraft with more than nine passenger seats (excluding any pilot seat), or for certain transport category aircraft, then the CAMP will be required. The CAMP is far more extensive in scope than the AAIP, which simply specifies inspection requirements.\nThe CAMP specifies inspections, maintenance, and repair procedures. It also specifies what maintenance personnel may perform these procedures and when. The CAMP will list persons having certain authorities to perform maintenance, other personnel having authority to perform inspections, and which personnel can sign off an aircraft for return to service.\nA key component of the CAMP is the continuous analysis and surveillance system (CASS), which is a type of quality assurance program focused on aircraft reliability and safety. The CASS is not necessarily a separate manual, but is a set of processes and controls that assure that the aircraft maintenance program is being followed correctly and is resulting in airworthy aircraft.\nA GMM with CAMP/CASS will typically be 300+ pages in length. See the GMM page.\nMinimum Equipment List / Configuration Deviation List\nThe minimum equipment list (MEL) is a document that allows an aircraft to be dispatched with certain equipment or furnishings inoperative. For Part 135, no aircraft may be dispatched with any equipment or installed items inoperative, unless that aircraft has an approved MEL and the aircraft is dispatched in accordance with the MEL. Most Part 135 operators will have MELs issued for their aircraft, and MELs are usually required for any turbine aircraft.\nMELs can be from 100-300 pages.\nA configuration deviation list (CDL) is similar to an MEL, except it covers physical items removed or missing from the aircraft, such as landing gear doors, fairings over landing lights, and other non-structural items. Most Part 135 operators will not have a CDL issued for light aircraft.\nCDLs can be 50-200 pages.\nApproved Aircraft Inspection Program\nAn approved aircraft inspection program (AAIP) is a document that contains the operator’s procedures for ensuring that an aircraft (or fleet of a similar type) are inspected in a manner that ensures they are compliant with Part 135 and part 91 requirements for airworthiness. AAIPs are not maintenance documents, in that they do not typically specify how routine maintenance is to be accomplished, but instead specify the schedule for inspections, items to be inspected, inspection procedures, and other requirements pertaining to the inspection schedule.\nAAIPs typically have sections dedicated to the airframe, powerplant, and avionics. The FAA typically requires the operator to develop an AAIP for any turbine aircraft operated under Part 135. It is optional for piston engine aircraft in most cases. Piston aircraft are often inspected under the typical annual/100 hour requirements specified when aircraft are used for hire.\nAir Carrier Certificate\nThe air carrier certificate is issued by the FAA once you complete all of the necessary certification steps. The air carrier certificate does not contain any specific details about your operation other than your name and air carrier certificate number. The actual privileges and limitations for your operation are contained in your operations specifications, as described below.\nYour operations specifications (often referred to as Ops Specs) set the authorized and prohibited types and areas of operation for your air carrier certificate. Your operations specifications will contain many elements, such as what kind of aircraft you can operate, who can conduct your training, the aircraft maintenance program, aircraft engine time limits, takeoff and landing weather limits, and much more. It is crucial as an operator that you understand your operations specifications thoroughly, since operating contrary to them can result in a violation.\nOps specs are issued by the FAA and are about 40-80 pages total.\nRNP (required navigation performance) authorizations are required for navigating using long range navigation systems (LRNS) such as GPS or IRS navigation in certain types of airspace. Oceanic operations, for example, require RNP10 or RNP4. Certain terminal and enroute areas require RNP1 or RNP2. Instrument approach authorizations can require RNP.3 or other specifications. Each type of RNP authorization is specific to the area and type of operation you plan to conduct. RNP authorization is not typically a separate manual, but instead procedures and training items integrated into your other manuals.\nRNAV authorizations are similar to RNP, but are somewhat less stringent. This authorization is for a specific method of navigation, rather than being for a specific method and region.\nJet / Transport Category Authorization\nAdding a jet or transport category aircraft to your certificate requires additional policies and procedures. This authorization also adds several more takeoff and landing performance requirements that lighter aircraft are not required to adhere to. These additional requirements are not particularly challenging to meet, but they do require the operator to develop the means to comply with them.\nTo operate aircraft in the US from flight level 290 to 410 requires a Reduced Vertical Separation Minimum (RVSM) approval. RVSM compliance has been getting easier since the requirements were initially instituted.\nExtended Overwater Operations\nExtended overwater operations require the operator to ensure that certain communication and navigation equipment is aboard the aircraft, and to ensure that additional flotation and survival gear is also aboard. This is required for any operations beyond 50 nautical miles from land.\nGenerally speaking, oceanic operations are those overwater operations beyond one hour flight time from a suitable airport. This authorization is a complex one, requiring special navigation and communication equipment, and special pilot training.']"	['<urn:uuid:3b995197-4da6-4cdd-8847-2f076266f984>', '<urn:uuid:3e234c5c-4ec2-4c01-8798-69260a0f5183>']	open-ended	direct	long-search-query	distant-from-document	multi-aspect	expert	2025-05-13T01:23:33.086345	9	102	2725
20	military working dogs need help health care transport veterans charity info	Military working dogs are classified as 'equipment' which creates significant challenges. When retired, they must return to their original base, with no funds provided for transportation to the US. Organizations like Military Working Dog Adoptions help raise donations for transportation and medical care, as these canine veterans receive no medical benefits despite risking their lives. The Nowzad Dogs charity also assists with similar services, having helped transport rescued dogs from Afghanistan with vaccination and shipping costs. These charities rely heavily on donations to support these retired military dogs.	['It’s not every day a soldier returns from war with a dog.\nBut for two soldiers from Winterport, not only did they bring dogs home from Afghanistan this past summer, but these dogs have created an emotional and healing bond with these men that will be with them forever.\n“I love B right to death, and it’s nice to know there is someone else in the household going through the same thing that I am, so I know I’m not the only crazy one here,” chuckled 22-year-old Army National Guard Specialist Matt Cooper. Cooper and B have found a bond of healing as they both endured mortar and rocket fire while in Afghanistan.\nFor 29-year-old Army Sgt. Adam Hesseltine, the bond with his dog leaves him lost for words. “Rene (Ren-ay) was there for me to take care of when I was in Afghanistan,” he said in a quiet voice. Holding the now 14-month-old dog on his lap, he claimed, “There was just some kind of emotional connection between us; that’s all I can say.”\nReflecting on the day that he met his new mate, Hessletine said, “All the Afghani kids would always beg for pens, so they could write on their hands. One day I teased them and said, ‘Why don’t you give us something; how about a goat?’”\nBut he didn’t realize that one of the kids could speak English, and soon after, a goat was in fact offered to the soldiers. That’s when Hesseltine asked for a dog instead, and the next thing he knew a tiny pup was tethered outside the soldiers’ Strong Point-Compound and was originally named Renegade after the platoon.\nThis multi-colored mutt wasn’t what the soldiers had hoped for in a guard dog. “She made a terrible guard dog,” said Hesseltine with a soft smile. “Sage Kooche (the breed like B) can be very defensive, but she didn’t bark at anything; she was too docile.”\nRene’s fault actually became her biggest asset as the little pooch’s lonely whimpering plucked at the soldier’s heart strings. While Rene was positioned at her guard post with a cloth rope, she often broke lose. Hesseltine admitted his role in the great escapes. “I tampered with her leash because she would whimper every time I tried to leave her, so I just made it look like she was so strong that she ripped her tether,” he said.\nThe soldiers knew the pup was abused. “I was always giving her baths because she’d urinate on herself because she was so scared,” said Hesseltine.\nWhen the Renegade Platoon’s mission was nearing its end in Afghanistan, the soldiers discussed putting the guard dog down to avoid the abuse she would endure in that country. “I decided I couldn’t do that,” said Hesseltine, so his next mission started: researching how to bring her home.\nHesseltine’s research led him to The Nowzad Dogs charity, which has a mission to “do something positive for the cats, dogs and other animals of Afghanistan and Iraq that have no hope and nobody to care for them,” as written on Nowzad’s web site.\nHesseltine praised Nowzad volunteers for getting Rene vaccinated and shipped to the States at a price tag of $300, which was donated by Nowzad. A transportation company took over from there, getting Rene through American customs in New York. That $300 fee was waived as a “thank you” to Hesseltine for serving our country.\nB was rescued through a charity as well, because Cooper’s wife was determined that B was coming home with her husband. While B and Cooper enjoy life in Winterport, Rene and Hesseltine are stationed at Fort Drum, N.Y. Perhaps during this holiday season, the two soldiers and their dogs will meet and reminisce about how spoiled they have made their dogs and how much their dogs have meant healing for them.\nNowzad was founded by a Royal Marine and is mainly run by volunteers. For more information, visit www.nowzad.com.', 'Military Working Dogs: What Happens When They Retire?\n“Many soldiers have their todays and can plan their tomorrows because of what a Military Working Dog did for them yesterday.”\nThis is the motto for Military Working Dog Adoptions, an organization run by a dedicated volunteer, Debbie Kandoll.\nIn 2008 Debbie adopted her first ex-military dog. With that single step, Kandoll became totally hooked on the fact that someone—her—needed to put some muscle behind solving the problem of how we treat our canine vets.\nThis is a topic I have covered frequently over the years, but when I was teaching my “America’s Stories through Its Dogs” class at UCLA and looking for guest speakers, my friend Robert Semrow, who hosts Pet World Insider, told me to be sure to contact Debbie Kandoll about the plight of military dogs. I thought I had pretty well covered the subject but Robert was correct—there was a lot more to know.\nA Little Background\nDogs embedded with troops are absolutely life-changing for the soldiers. Even when dogs were brought along as mascots, they heard things before the soldiers and alerted the unit to the fact that something was going on; many attacked enemies when they sensed danger. (Read about Sgt. Stubby from World War I or Sallie during the Civil War.)\nYet we had no canine unit until World War II when a dog breeder, Alene Erlanger began a program asking for people to donate their pets to serve in the military. Families did.\nNo Discharge Plan\nWhat was lacking was any sort of exit plan for the dogs. After World War II, a veterinarian named Bill Putney took on the re-training of dogs to return them to civilian life, but Putney knew there needed to be a formal program. (During Vietnam, American military dogs were either euthanized or left in Vietnam to fend for themselves.)\nPutney, who ran a veterinary practice in southern California, spent 50 years working to get a bill passed by Congress (2001) that stops the practice of euthanizing these dogs or abandoning them after they have served in the military. Putney was one of many who knew this was no way to treat these veterans.\nThe bill also provides for handlers to re-train their dogs and adopt them if they so choose.\nBut there was a catch that I didn’t pick up on initially.\nWhat I Learned from Debbie Kandoll\nBecause the bill—heavily supported by Senator John McCain—classifies the dogs as “equipment,” there are a host of problems. When any type of “equipment” is taken out of service, it goes back to the base where it originated. So if a dog began his tour of duty at a base in Germany, he is returned to that base, with no funds to send him or place him elsewhere.\nThis leaves it up to organizations like Military Working Dog Adoptions to solicit donations—cash or airline miles—to bring the dogs to the U.S. where a home can be found. Or if the handler wants to adopt, to deliver the dog to the handler who may be on another overseas assignment.\nThere is also no provision for medical benefits for these canine veterans. As Debbie says, “It would be a good start if an owner could take the dog to a veterinarian on a base and get the service\n“As it is, here are dogs who have risked their lives—and their health—to save the lives of our military, and we, as a country, have made no provision to take care of them,” says Kandoll.\nKandoll Visits UCLA with Three of her Dogs\nDebbie Kandoll was living in Las Cruces, New Mexico when I called her, and when she heard about my class, she immediately offered to drive from New Mexico to Los Angeles with three of her dogs to tell their story. (That’s the kind of devotion Debbie gives to this cause.)\nSo in April of this year my class and I were spellbound by Debbie and her three extraordinary dogs. We met Luke M560 ( USA, Ret.) , Alex H116 (USAF, Ret.), and Suzy Q, a PTSD Service Dog.\nThe photos give you a flavor of the wonderful time we had. The dogs clearly have eyes for only one person who was with us that day. It was, of course, Debbie, who put them through some of their paces and then let us pet and enjoy the dogs to our heart’s content. We had a good number of drop-in visitors, and the class continued on for an extra hour and a half, we enjoyed her—and them—so much.\nActions Debbie Suggests:\n1. Donate to the cause. Debbie’s organization is entirely dependent on donations, and she is a 501(c)3. The money goes to help bring these dogs from a base to a potential home—often their original handler. The money also goes toward medical expenses for these veterans. www.MilitaryWorkingDogAdoptions.com\n2. Write to your Congressional representatives. Ask that these animals be re-classified from “equipment” to “canine members of the Armed Forces.” This would permit them to be transported to the U.S. on cargo planes at no charge (an option not available with the “equipment” classification.) It would also open discussion re: getting medical care for these vets.\n3. If you would like to adopt one of these dogs, there are applications on the website.\nDebbie also maintains an active facebook page where you can keep up with her whereabouts: www.facebook.com/MilitaryWorkingDogAdoptions']	['<urn:uuid:f5ce5cb8-89d1-491b-bf15-8280a8caa925>', '<urn:uuid:188150d8-2928-464d-98f2-5a9f3c109a6f>']	open-ended	with-premise	long-search-query	similar-to-document	three-doc	novice	2025-05-13T01:23:33.086345	11	88	1562
21	amount organic matter compost soil amendment when start how improve poor soil	For starting beds, incorporate 3 inches of compost for low water-use zones, 4-8 inches for medium water-use zones, and 6-12 inches for high water/feeding zones like vegetable beds. For poor soil improvement, organic matter like leaf litter, clean hay, seedless weeds, crop residue, moss, pine needles, grass clippings, wood shavings and aged manures should be dug into the soil at a depth of 6-12 inches.	"[""Practice No. 5. Revitalizing Beds and Borders\nMany of our landscapes are already in place or have been in existence long enough where some revitalization needs to take place. Revitalizing unamended beds and borders presents a more difficult task than if one started a bed from scratch. It would be ideal to incorporate 3 inches of compost for low water-use zones, 4 – 8 inches for medium water-use zones and 6 – 12 inches for the high feeding, high water-use zones, such as vegetable beds and bluegrass/fescue turf. This is much easier to do in beds of annuals or vegetable gardens where one starts fresh each spring (or fall) than in shrub and herbaceous perennial beds. Revitalizing established beds and borders that haven't previously been amended is a longer term project, not a quick fix.\nAdding organic matter throughout a bed is admittedly difficult in heavily planted beds and borders. Carefully digging or aerating around established plants is necessary, so as to minimally disturb the roots. If your bed has plants that can be dug up before revitalization and replanted after amending the soil, do so.\nPull back or scrape off and save the mulch cover. Depending on the amount of clear ground and maturity of the remaining plants, either dig with a garden fork loosening the soil and apply 3 inches of compost, carefully digging this into the soil, trying not to disturb roots. If you find this isn't practical, scratch the surface of the soil with tines before laying down the compost or compost blend. Even just plunging the fork into the soil will begin to aerate it, with compost or wormcastings falling down into the holes just created with the garden fork. Adding organic amendments in this manner may need to be made over a period of years.\nCompacted clay and caliche soil will benefit with the addition of inorganic material. I recommend adding an inorganic amendment such as Turface® (calcined clay), Tru-Grow® (expanded blue shale), Ecolite® (Zeolite) or Axis® (diatomaceous earth) for heavy clay soil, and Profile® for sandy soil or other brands of the same materials. Follow the recommendations on the bag for quantities to add per square yard. Other inorganic amendments with much more limited ability to retain water and nutrients are crushed granite, granite and lava sand, greensand, glass sand, and finally, regular sand. Be careful in adding inorganic amendments with low water and nutrient holding capacities to soils with low organic content, such as sands; organic matter must be added in conjunction with them. Otherwise you could end up with worse soil, mimicking brick.\nIn bed revitalizations, renovations, makeovers, and annual beds, the ideal time to add the compost is in the fall, giving the microbes the winter months to repopulate and grow. If the plants suffered root damage in the revitalization process, they will have the cooler winter months to recover before the major stress of summer is upon them. Monthly foliar spraying during the growing season with fish emulsion, liquid humate, molasses, and/or kelp/seaweed, trace minerals, aerobically activated compost tea, worm tea and other microbial stimulants are beneficial when getting started.\nIf you find you are unable to do any digging or scratching of the top of the soil, mulch yearly with a top quality compost blend from 2 to 3 inches thick and make at least three foliar applications during the growing season. Monthly, for three years, would be better (April through September). Better results will be realized sooner with the incorporation of solid organic matter into the soil. Replenish the mulch cover as needed.\nWhenever replanting in beds or borders, refurbish the plant hole with Yum-Yum Mix®, compost and/or other inorganic and organic amendments.\nMy low water-use beds that were amended in 2000 are just as vigorous years later as at the first. The only revitalization I do is broadcasting Yum-Yum Mix® and granular humate to the top of the bed once a season. I include these beds along with the others when doing any foliar spraying.\nLet the leaves fall and remain in place to decompose in the beds adding to the soil's richness. If the leaves are large and thick, rake them into the lawn and mow, returning them to the beds and borders chopped and shredded. They will decompose quicker and not mat down. For spring clean up, if you wish a tidier look, rake the leaves that have blown in over the winter, mow, and return to the beds and borders.\nI've practiced this procedure of leaf recycling throughout my garden. One day after about 5 to 6 years, I dug into a shrub bed that included a tree to see what the soil felt like. Having grown up in Wisconsin and walked the forests floor, I was surprised to find this border floor nearly as soft and porous, filled with life. It can be done if you give nature a chance."", 'Poor soil grows poor plants. Unless you drew the lucky card and have a garden full of black gold, you’ll need to know how to improve soil. Improving garden soil is an ongoing process as the plants leach nutrients, leaving the soil inadequate for their needs. Whether your soil is nutrient deficient, compacted, heavy clay or any other issue, here’s a little soil amendment info to get you started.\nSoil Amendment Info\nSoil amendment can be as simple as mixing in leaf litter or it can be as complicated as running drainage pipes. The condition of your soil needs to be adequate for sustaining plant needs. Compact or hard soils are actually great for starting a lawn, as long as you add a little sandy topsoil if starting from seed. Plants like fruits and vegetables, however, need loose, nutrient rich soil with plenty of organic amendments added every year. There is no rule on the best soil for gardens, but there are some basic guidelines and some easy fixes.\nHow to Improve Soil\nIn most cases, the need for amending soil arises from having poor, compacted soil or nutrient deficient soil. Here are some general tips on improving your soil:\nPoor, Compacted Soil\nDense, hard soil may be the result of construction or simply little ones running across it constantly in play. The depth of compaction is important to know how to deal with it. If you have very deep, hard areas, you may have to rent equipment to dig it up and loosen it.\nLoosen the soil to a depth of at least 12 inches for most plants and up to 2 feet for trees and larger specimens. Garden soil preparation by manually shoveling is usually sufficient in most cases. Once the soil is loose, you may need to add several inches of compost or fine bark to keep it loose and workable.\nNutrient Deficient Soil\nImproving garden soil is imperative for a bountiful garden. Organic matter is the best soil amendment because it breaks down naturally to release nutrients for plant uptake. Some of the best items to use are:\n- Leaf litter\n- Clean hay or straw\n- Seedless weeds\n- Crop residue\n- Sphagnum moss\n- Peat moss\n- Pine needles\n- Grass clippings\n- Wood shavings\n- Dust and aged manures\nGarden soil preparation with these items works best if they are dug into the soil to a depth of 6 to 12 inches. You can even save your kitchen scraps to work into soil, but avoid meat, bones and fat. Cover crops provide “green manure” to work into soil in spring for an extra shot of nitrogen and increased soil percolation.\nMixing the Best Soil for Gardens\nThere isn’t an actual recipe for soil; however, it needs a good balance of macro-nutrients and micro-nutrients, should drain freely and have a balance of carbon to offset the nitrogen.\nAcid and alkaline soils can be amended with lime to sweeten the soil and sulfur to increase the acidity. Wood ash and oyster shells also naturally make acidic soil more neutral. Test kits are available at most garden centers to see if your soil is high or low in pH.']"	['<urn:uuid:207a47d5-655d-42e1-ae0d-034b3b464c0a>', '<urn:uuid:dd4df76c-f5b4-45df-92df-ff492fd6780e>']	factoid	direct	long-search-query	distant-from-document	multi-aspect	expert	2025-05-13T01:23:33.086345	12	65	1351
22	work experience deike potzel german ambassador	Deike Potzel has been the German Ambassador to Ireland since 2017. Previously, she served as Deputy Director-General for Central Services at the Foreign Ministry in Berlin (2014-2017), Head of the Personal Office of Federal President Joachim Gauck (2012-2014), and Deputy Head of Division for Personal, Staff Development and Planning at the Foreign Ministry in Berlin (2008-2012). She studied at the Foreign Service Academy and Humboldt University Berlin.	['H.E. Deike Potzel: Ambassador of the Federal Republic of Germany.\nDr. Ellen Ueberschär\nThe Delmaine String Quartet.\nHeinrich Boll Weekend Friday May 3rd – Sunday May 5th. 2019\nFriday May 3rd Events\n19.00 Registration and Welcome Reception. Cyril Gray Hall Dugort\n19.30 Official Opening: H.E. Deike Potzel: Ambassador of the Federal Republic of Germany.\n1987 – 1992 Foreign Service Academy of the Federal Foreign Ministry, Bonn. English and French Language and Literature Studies, Humboldt University BerlinSince 2017 Ambassador, German Embassy, Dublin, Ireland / 2014-2017 Deputy Director-General for Central Services, Foreign Ministry, Berlin / 2012-2014 Head of the Personal Office of Federal President Joachim Gauck/ 2008- 2012/Deputy Head of Division for Personal, Staff Development and Planning, Foreign Ministry, Berlin.\nPresentation of Essay Prizes by H.E. Deike Potzel.\n20.15 Address by Dr. Ellen Ueberschär.\nDr. Ellen Ueberschär President of the Heinrich Boell Foundation Berlin.\nAs president of the Heinrich Böll Foundation she is responsible for the Foundation’s German activities, for foreign and security policy, and for the Europe and North America regional work. In addition, she is in charge of the scholarship department, of the Green Academy (a think tank of scientists and politicians), as well as of the archive “Grünes Gedächtnis” (green memory), a contemporary historical archive collecting materials relating to the Green party and the new social movements.\n21:00 Lecture Garett Cormican: “Camille Souter’s paintings of Mayo”.\nGarrett Cormican will present a talk on Camille Souter’s paintings of Mayo and Achill in particular, from the 1950s to date Garrett Cormican is a painter and author of the critically acclaimed 337 page monograph Camille Souter: The Mirror in the Sea, published by Whytes in 2006. He was Assistant Curator of the Francis Bacon Studio and Database Project, The Dublin City Gallery, The Hugh Lane from 1999-2001. He wrote the introductory essay for Camille’s show at the Linenhall in 1999 and was Artist’s Biographer for her show at the Model Arts and Neiland Centre and RHA in 2001.\n22:00 Reception at Strand Hotel.\nSaturday May 4th. Events\n10:00am. Dooega Head. Guided walk in the landscape with Eoin Halpin. Meet at Lavelle’s Seaside House (Mickey’s Bar) Dooega.\nEoin Halpin is the Operations Manager with Archaeology and Heritage Consultancy Ltd a company founded in 2014. For the previous 25 years he worked with Archaeological Development Services Ltd, and before that for the Scottish Office in Edinburgh. Eoin has worked on all aspects of development led archaeological projects. He has project managed numerous large infrastructural schemes for example, the North-South Gas Pipeline and the A1-N1 road scheme. He is a member of the Chartered Institute for Field Archaeologists and a member of the Institute of Archaeologist of Ireland for which he has acted as chairperson. His work has appeared in numerous publications in academic journals, historic society journals and monographs.\n13:00 – 14:00 Lunch: Lavelle’s Seaside Bar, Dooega.\n14.00 – 17.00 Heinrich Böll Cottage open to the public.\n14.00 ‘A Poet’s Pathways’ Writer’s workshop with Martin Dyar. Cyril Gray Hall.\nMartin Dyar grew up in Swinford in County Mayo. His book of poems Maiden Names (Arlen House), described by Bernard O’Donoghue as ‘a thrilling new development in Irish poetry’ was shortlisted for the Pigott Poetry Prize, and was a book of the year selection in both the Guardian and the Irish Times. stimulate audience members with regard to their own reading and writing; Writers workshop insights into technique, creative process and the deeper significance of poetry; language as used by a selection of writers with respect to landscape: mountains and rivers in particular: framed in writers background and writing life.\n15:00 Gisela Holfter, Bettina Migge “Ireland in the European Eye”. Cyril Gray Hall.\nThis book developed in the context of the work of the Royal Irish Academy committee, Language, Literature, Culture and Communication (2014-2018). The focus on European awareness of Ireland emerged organically from one of the committee’s central preoccupations, the place of European languages and literatures in Ireland. However, since literature is only one of the cultural areas that have contributed to shaping Ireland’s relationship with the rest of Europe, the focus of the collection extends to other closely related domains such as the Arts, Architecture, Politics and European Studies to bring to the fore a more rounded picture of Ireland’s relationship with continental Europe. The book is divided into four parts. The first part, ‘Ireland in Europe: Historical Background and Contextualisation’, presents historical overviews and a contextualisation of Ireland’s relations with the other European countries. The second part, ‘Ireland in Europe: Representations of Ireland in European Literature and Irish Literature in Europe’, discusses how Ireland is represented in European literatures and provides a historical contextualisation. The third part, ‘Ireland in Europe: Irish Art, Architecture, Film and Music in European Discourses’, analyses Ireland’s representation in the European context from a broader cultural perspective. The final part, ‘Ireland in Europe: European Studies, Tourism and Journalism’ zooms in on the marketing of Ireland to different European markets, developments in European Studies and, turning the tables, analysing European discourses in Irish media.\nGisela Holfter Gisela Holfter studied in Cologne, Cambridge and St. Louis, and worked as an Assistant Teacher in Belfast and as a Lektor at the University of Otago in Dunedin, New Zealand, before coming to Limerick in 1996. She is Senior Lecturer in German and co-founder and Joint Director of the Centre for Irish-German Studies. Her research interests include German-Irish relations, German literature (19th century to contemporary writing), exile studies, migration and intercultural communication. Co-edited a dozen books, the latest being German Reunification and the Legacy of GDR Literature and Culture (2018, Brill) and (WVT 2019; both books edited with D. Byrnes and J. Conacher). Her monographs include Erlebnis Irland (1996, WVT), Heinrich Böll and Ireland (2011, paperback 2012, CBS) and An Irish Sanctuary: German-speaking Refugees in Ireland 1933-1945 (with H. Dickel, 2017, paperback 2018, de Gruyter). She is currently engaged in a project on Fontane’s Wanderungen durch die Mark Brandenburg with contemporary German writers (Verlag Berlin-Brandenburg, 2019) and one on German migration to Ireland since 1922 (Palgrave 2020).\nBettina Migge studied in Hamburg, Yaoundé (Cameroon) and Berlin and received a PhD in Linguistics from the Ohio State University. She worked as Hochschulassistentin at the Johann-Wolfgang Goethe Universität, Frankfurt am Main and is currently Professor of Linguistics, Head of the School of Languages, Cultures and Linguistics at University College Dublin and a member of the research group Structure et Dynamique des Langues (SeDyL UMR 8202) in Villejuif, France. Her research interests are in the broad area of language contact, language documentation, sociolinguistics. Empirically her research has focused on the Americas, specifically the Creoles of Suriname and French Guiana, and on migration to and language in Ireland. Her publications include book length studies Exploring Language in a Multilingual Context: Variation, Interaction and Ideology in Language Documentation (with I. Léglise, 2013, CUP) and edited volumes Support,Transmission, Education and Target Varieties in the Celtic languages (with N. Ó Murchadha 2017, Routledge), New Perspectives on Irish English (with M. Ní Chiosáin, 2012, John Benjamins). She is currently engaged\n16:30 Reading by Martin Dyar. Cyril Gray Hall.\nWith the composer Ryan Molloy, Martin Dyar has written a poetry song cycle for soprano harp and flute, which toured nationally in 2018. Martin has also written a play, Tom Loves a Lord, about the Irish poet Thomas Moore. Martin won the Patrick Kavanagh Award in 2009, and the Strokestown International Poetry Award in 2001. A graduate of NUI Galway, and Trinity College Dublin, where he did a PhD in English Literature, and taught for ten years in TCD’s School of Medicine, he has held fellowships at the University of Iowa, and at the Washington Ireland Program. His work has been added to the Leaving Cert prescribed poetry syllabus, and chosen for a number of anthologies, including Windharp: Poems of Ireland Since 1916 (Penguin Ireland), and Reading the Future (Hodges Figgis). Martin is currently Associate Writer Fellow at the University of Limerick, where he teaches poetry on UL’s MA in Creative Writing. Martin’s next collection of poems, Burke’s Goddess, will be published by Arlen House in Autumn 2019\n17.00 Reading by Hugo Hamilton. Book launch Dublin Palms. Cyril Gray Hall.\nThe narrator of Dublin Palms has returned to Dublin to set up home with his partner Helen and their two children. Their lives are filled with optimism, but also by a sense of dislocation. Overshadowed by the Troubles in the North, their family enterprise begins to come apart. As the creditors line up to be paid, they must consider leaving everything behind. What will they gain when they stand to lose all? In this spectacular novel from the author of The Speckled People, a family tries to hold on in a falling world. A powerful story of fragmentation and belonging, of emigrants and strangers and people returning home. The palm trees give the street a holiday atmosphere. There must be something in the soil they like. They have straight leaves that get a bit ragged, with split ends. At night you hear them rattling in the wind..\nHugo Hamilton is the author of the best-selling memoir, The Speckled People, the story of his German-Irish childhood in Dublin, prohibited by his revolutionary father from speaking English. He has written nine novels, two memoirs, a collection of short stories and three stage-plays. His work has won numerous international awards, including the French Prix Femina Etranger, the Italian premio Giuseppe Berto and a DAAD scholarship in Berlin. Hamilton is a member of Aosdana and lives in Dublin.\n20:00 St. Thomas’s Church\nThe Delmaine String Quartet present Love and War: Words and Music from the Western Front.\nTo commemorate the centenary of The First World War in 1914, Bewley’s Cafe Theatre and the Delmaine String Quartet have collaborated on a special programme of words and music reflecting the human experience of the Western Front. Featured prominently are the letters of Artillery Officer, Eric Appleby, to his Athlone sweetheart, Phyllis Kelly. In this moving correspondence, he records the discomforts, horrors, banalities and humour of trench life as he yearns to be back with his beloved in a world at peace. Phyllis was a talented singer and the performance will include several of the popular songs of the period with string quartet accompaniment. Threaded through this extraordinary love story are some of the great works from the famous generation of World War One soldier poets and composers. In addition, a varied programme of poetry and prose will be featured, with music by Bach, Ravel, Moeran, Dvorak, Schubert & others. The performers are Michael James Ford, Susannah De Wrixon, Elliot Moriarty and the Delmaine String Quartet.\nSunday May 6th Events\n10.00am Guided walk with Eoin Halpin.\nLocation to be confirmed.\n13:00 Lunch Mullranny Hotel.\nReading by Moya Cannon.\nMoya Cannon is originally from Co. Donegal, she has spent most of her adult life in Galway and now lives in Dublin. She has represented Ireland at many international festivals and conferences, in Japan, India, the US and many European and South American countries. Her work has been widely translated. She studied History and Politics at University College Dublin and International Relations at Corpus Christi College, Cambridge. A native speaker of Irish, for many years she taught adolescent traveller children. She has taught creative writing at the National University of Ireland, Galway and was director of the International Writers’ Course at NUIG. A winner of the Brendan Behan award and of the Lawrence O Shaughnessy award, she has edited Poetry Ireland Review and was 2011 Heimbold Professor of Irish Studies at Villanova University. She is a member of Aosdana, the Irish affiliation of creative artists. She has a deep interest in music and enjoys performing with musicians.\nTickets can be purchased for the whole weekend or for individual events.']	['<urn:uuid:2c30e052-2657-4b04-8e87-56df2d0880e4>']	open-ended	direct	short-search-query	similar-to-document	single-doc	novice	2025-05-13T01:23:33.086345	6	67	1957
23	What makes school quality important for house prices, and how do teachers adapt to online teaching?	School quality significantly impacts house prices, with markets responding to school performance measures beyond just test scores. Interestingly, improvements in existing schools are valued more highly than reassignment to better-performing schools. Regarding online teaching adaptation, there's a clear distinction between proper online teaching and Emergency Remote Teaching. While ERT involves quickly adding activities to an online space, effective online teaching requires careful design, layout, and implementation of social engagement principles. Success often depends on mentorship, with experienced teachers helping novices navigate the technical and pedagogical challenges.	"['Public School Reform, Expectations, and Capitalization: What Signals Quality to Homebuyers?\nAbstractThis paper offers new evidence on how housing markets capitalize school quality information. We use school assessment measures and house transaction data from a single school district that is coterminous with a unified city-county local government to examine the effects of two school assignment policy changes. The unusual school district-city-county government structure minimizes property tax rate and public service variation across observations, reducing the source of one problem that plagues many capitalization studies. The school reassignment events reveal direct ties between school quality capitalization and households’ expectations—the latter reflected in private school enrollment and tax referendum voting. Different school performance measures convey different information to home owners. In particular, student test scores do not affect house prices when broader school performance measures are published. Prices respond to measured school improvement, but improvements in a given school are more highly valued than being reassigned to a better school.\nDownload InfoTo our knowledge, this item is not available for download. To find whether it is available, there are three options:\n1. Check below under ""Related research"" whether another version of this item is available online.\n2. Check on the provider\'s web page whether it is in fact available.\n3. Perform a search for a similarly titled item that would be available.\nBibliographic InfoArticle provided by Southern Economic Association in its journal Southern Economic Journal.\nVolume (Year): 75 (2009)\nIssue (Month): 4 (April)\nFind related papers by JEL classification:\n- H4 - Public Economics - - Publicly Provided Goods\n- I2 - Health, Education, and Welfare - - Education\n- R21 - Urban, Rural, Regional, Real Estate, and Transportation Economics - - Household Analysis - - - Housing Demand\n- R00 - Urban, Rural, Regional, Real Estate, and Transportation Economics - - General - - - General\nYou can help add them by filling out this form.\nCitEc Project, subscribe to its RSS feed for this item.\n- Nguyen-Hoang, Phuong & Yinger, John, 2011. ""The capitalization of school quality into house values: A review,"" Journal of Housing Economics, Elsevier, vol. 20(1), pages 30-48, March.\nFor technical questions regarding this item, or to correct its authors, title, abstract, bibliographic or download information, contact: (Laura Razzolini).\nIf you have authored this item and are not yet registered with RePEc, we encourage you to do it here. This allows to link your profile to this item. It also allows you to accept potential citations to this item that we are uncertain about.\nIf references are entirely missing, you can add them using this form.\nIf the full references list an item that is present in RePEc, but the system did not link to it, you can help with this form.\nIf you know of missing items citing this one, you can help us creating those links by adding the relevant references in the same way as above, for each refering item. If you are a registered author of this item, you may also want to check the ""citations"" tab in your profile, as there may be some citations waiting for confirmation.\nPlease note that corrections may take a couple of weeks to filter through the various RePEc services.', 'This article is based on observations from a workshop conducted at the 2021 annual TESL Ontario conference presented by language training professionals and experts in learning technology solutions from the Avenue–LearnIT2teach Project. The onset of COVID-19 in March 2020 was a crisis in immigrant settlement language training. Many teachers and learners were forced to rapidly pivot from conventional face-to-face learning to remote learning. Teachers and learners who had previously practiced blended (or hybrid) learning were well positioned with technology and skills. Teachers and learners who had not were forced to rapidly respond with Emergency Remote Teaching (ERT). Observations from the workshop panel and the participants uphold the need for continued sector engagement with learning technology to support the development of digital skills among newcomers, enable better practices in teaching and learning, and support better program access for client learners.\nKeywords: LINC, technology, teaching, learning, pandemic, Avenue\nIn March 2020, the EduLINC–LearnIT2teach Project had to pivot to help address COVID-19 and the suspension of face-to-face instruction. Teachers were desperate for online tools to keep teaching and learning going. Teacher and learner accounts, and teacher training were in demand. Our former learning portal EduLINC (now Avenue.ca) was a scalable practical response, but teachers needed training and support. We saw that we could divide teachers into two groups:\n- Teachers who already employed a LINC blended learning strategy in their face-to-face instruction and whose learners had been oriented to using Moodle or a local LMS solution and information technology in their learning;\n- Teachers who had previously made minimal use of information technology. A sub-set of the second group was teachers who personally had limited skills with information technology.\nThe first group was well prepared, as were their learners, to engage with online remote learning. The second group needed tools and support urgently.\nTwo years later LINC professionals find themselves in a transformed learning technology landscape. Many teachers now have digital teaching skills, courseware, and resources. Managers had to implement new and more flexible modes of learning. Learners are now engaged with information technology and blended or online learning and have experienced flexible delivery models.\nHow can LINC professionals leverage these changes to improve access, equity, language gains, and settlement outcomes? The 2021 TESL Ontario workshop examined and discussed the implications for LINC professionals and clients.\nSeveral key takeaways arose when a New Language Solutions (NLS) panel of experts explored the LINC teaching world in a virtual workshop titled Learning Technology in LINC–Beyond the Pandemic (2021, November 3):\n- the importance of a mentor/mentee relationship during COVID-19;\n- the difference between ERT and online teaching;\n- blended learning as an ideal delivery mode of ESL learning post-COVID-19; and\n- digital learning is not enough; digital fluency should be the new benchmark.\nImportance of the mentor/mentee relationship\nNLS uses a mentor/mentee relationship on the Avenue platform for LINC teachers. This relationship has proven to be highly effective over the past 12 years, with the legacy Learning Management System (LMS; in this article, Moodle, with this specific system named EduLINC or Avenue). Under this model, more experienced teachers and advisers help direct teachers who are novice to the system to find solutions to questions and problems.\nThe closing and/or reduction of face-to-face classes, and the resulting rush to online solutions for classes due to the pandemic had enormous impact on adult settlement language training programs like LINC. In February 2020, there were 1086 new user enrolments on EduLINC, a number consistent with the previous 12 months (McBride & Edgar, 2020). In March there were 5,521, a 500% increase, and a foreshadow of the growth in demand for project services in the months to come. These numbers represented a huge increase in demand for the LMS platform, and mirrored a parallel increase in the demand for a mentor’s availability to assist and train teachers.\nIn March 2020, emergency needs prevailed and 17 project teacher mentors across Canada rose to the challenge of a surge in demand for help. Most NLS mentors are also active teachers, so the mentors needed to make an incredible effort to satisfy the onboarding needs of mentees. Mentors during the initial surge and throughout the months (and year) that followed dealt with not only factual and technical questions, but also provided guidance on some online learning principles. Learning technology can be challenging in the best of times, but during the pandemic everyone was also concerned for the health and safety of themselves, family, and friends. Mentors helped mentees by listening, understanding and empathizing, all the while gently helping novice teachers navigate new territory rapidly and under stress.\nEmergency remote teaching versus online teaching\nThe shift in teaching from face-to-face to virtual spaces is often, inadvertently, called online teaching and, from the students’ perspective, online learning. What has happened during COVID-19, though, cannot be 100% described as online teaching. Indeed, teaching and learning was being conducted online, but the essential pedagogy from online teaching was not always possible in practice.\nFor many teachers and learners what happened during the initial weeks and months of COVID-19 was instead a form of ERT. This happens when teachers quickly add activities and documents to an online space or platform. There is no blame or shame for wanting to do this, as the reaction is completely understandable during the first stage of the pandemic. What is unfortunate, though, is that many teachers were totally frustrated by the events because:\na) It happened extremely quickly;\nb) Many did not have an existing online platform to use;\nc) The learning curve to feel confident on an LMS can be steep and is best achieved in a reduced-stress environment;\nd) Their students were not prepared for online learning, making the teachers’ orientation to an LMS urgent;\ne) Teachers often felt panicked and unmoored;\nf) Teachers were anywhere from zero to ten on a comfort-with-technology scale;\ng) Some students had more technology proficiency and made teachers feel like imposters;\nh) Service Providing Organizations (SPOs) added training and new procedures that all had to be learned as well;\ni) There was no end in sight to the situation.\nThe early pandemic environment was not ideal for teacher innovation.\nOnline teaching, in and of itself, has design and layout at its base, and should incorporate the best available online technologies to maximize learning outcomes. In LINC language learning, that requires communication with learners and colleagues and putting better practices in teaching and learning to work online.\nOne of the fundamental strengths of online teaching is that it goes far beyond a platform where documents or files are simply uploaded and downloaded. A good LMS supports social engagement between and among teachers and learners and encourages the implementation of social constructivism principles (Walker & White, 2013, p. 25). It is, in essence, a holistic approach to teaching and learning where parties can achieve the same outcomes when they are all at a distance from each other. Teacher presence is vitally important, whether communication with learners is synchronous or asynchronous.\nThe Emergency Remote Teaching method employed due to COVID-19 could never qualify as online teaching and should not be judged by the same criteria (Hodges et al., 2020).\nThe circumstances were less than ideal for better practices in the early days of the pandemic, and indeed, many LINC professionals are still trying to catch up with innovation.\nBlended learning as a delivery model\nAs a sector, the ESL world has revolved around face-to-face teaching and learning. There are numerous benefits to learning in this setting, not the least of which are the body language gestures that can be used to help communicate when words fail us. But a shift in methods of learning started taking place over a decade ago, as learning technology innovations from the broader post-secondary sector began to spread in the settlement language training (SLT) sector. Many questions arose, and common themes surfaced: Can and should technology be used in SLT? If yes, then why, where, when, and how?\nSo, where does blended learning come in? Blended (or hybrid) learning is a mix of teaching/learning modes, and usually means a blend of face-to-face and online teaching (using technology to learn when teacher and students are distanced from one another). The perfect blend is neither identified nor recommended, because blended learning considers a vast variety of factors that are unique to each teaching/learning context. Thoughtful reflection on the mix of technical abilities, subject matter, available internet, hardware and software availability, outcomes expected of learning, and so on must be considered (Hodges et al., 2020). In any context, an effective blend would be based on optimal use of what face-to-face does best and what online learning can do.\nConsider some affordances of blended learning: Modalities can change from term to term, wait lists can be addressed, learners can be placed in learning environments where they can be most successful based on not only their English needs, but also on their technology circumstances, as well as their working, home and childcare obligations. This, we believe, is the approach that will produce a positive settlement experience for Canada’s immigrant and refugee populations, and help address concerns of program access and learner equity.\nCurrent times: From digital literacy to digital fluency\nTwo years later and with the pandemic still a disruption that prevents any return to normalcy, where is the settlement language training sector? What have we learned as SLT professionals, and what is the future of online learning technology? During the workshop, one panel expert observed that ESL teachers do a disservice to their students if they revert to previous practice and abandon technology in education. Learners need digital opportunities for learning. The term technology-enhanced language learning (TELL) represents the idea that information technology (IT) should be embedded in language training to reflect how IT is used in daily life and communications (Walker & White, 2013, p. 33).\nDuring the COVID-19 pandemic and rush to ERT, poor digital literacy skills were an obstacle for many teachers and learners. Basic digital literacy skills are needed not only to learn, but also to be successful in today’s workplaces, schools, or as parents, consumers, or citizens. After two years and many personal gains in competence with online teaching, now is the time to leverage these IT skills gains. Even entry-level and lower-paying jobs usually require digital know-how. Students, especially newcomers, need to learn, maintain, and grow their digital skills in Canada.\nPanel experts felt that basic literacy skills in the digital world are not enough; becoming fluent in digital skills is where we should be setting the bar. Yes, many people have learned enough to get by, to survive. But where improvement in lives and careers will be seen is when individuals are competent and comfortable in their IT knowledge and practices.\nThe everyday-living landscape has changed during COVID-19, with more connections, services, learning and even jobs being conducted solely online. We must not let ourselves and our students backslide into complacency or regression. We have started on this journey, so let us continue to develop equity in the economy and in life skills for our newcomer Canadians.\nBy examining the importance of the mentor/mentee relationship during COVID-19, we have identified the compassion and human contact necessary to keep teaching, learning, and innovating.\nBy reflecting on the differences between ERT and online teaching, we can see that under the circumstances, our settlement language training sector did the best it could. But as educators to Canada’s newcomers, we know we can do better if we have the opportunity and the skills to apply online teaching pedagogy to our course design.\nHaving now experienced both face-to-face and online teaching and learning, we can see the affordances of both modalities and can carve programs and courses to truly meet the needs of our students.\nAnd finally, by seeing the demand and uptake for digital literacy skills and visualizing a future that requires better competencies, we promote continued digital skill use and better digital citizenship.\nHodges, C., Moore, S., Lockee, B., Trust, T., & Bond, A. (2020, March 27). The difference between emergency remote teaching and online learning. EduCAUSE Review. https://er.educause.edu/articles/2020/3/the-difference-between-emergency-remote-teaching-and-online-learning\nMcBride, R., & Edgar, J. (2020). LearnIT2teach – EduLINC – Avenue.ca: Project Annual Report to Immigration, Refugees and Citizenship Canada. New Language Solutions.\nWalker, A., & White, G. (2013). Technology enhanced language learning: Connecting theory and practice. Oxford University Press.\nNancy Van Dorp is a Senior Trainer/Mentor/Developer on the LearnIT2Teach Project, where she works with teachers to use and develop eLearning materials for use in blended and fully online classrooms. During the pandemic she has been especially busy providing advice and sharing learning with others! She also teaches culture and technology courses at Sheridan College. Enchanted by EdTech possibilities, she loves exploring and showing new ways to integrate andragogical resources.\nRob McBride has been the Executive Director of New Language Solutions since its inception in 1983 as TVLT. Rob has worked as a teacher, researcher, writer and producer, principally in the settlement language training and adult literacy basic skills sectors. Rob works on projects where building basic knowledge and skills has the maximum positive impact on the work and personal lives of learners.']"	['<urn:uuid:3ae994c8-cea2-4de2-9cd5-9e2c787f4d18>', '<urn:uuid:78e464d6-d85d-4704-8596-c180d4f504cc>']	open-ended	with-premise	concise-and-natural	similar-to-document	multi-aspect	novice	2025-05-13T01:23:33.086345	16	86	2724
24	How have fishing practices changed over time as coastal fish populations have become depleted in the ocean?	As shallow fish populations have become over-exploited, the fishing industry has moved further away from the coasts and deeper into the ocean, now using nets at 2,000m depth and trawling the seabed for fish.	"[""Seamounts and coral: a conservation diary from the deep\nA team of scientists has set out on a six-week mission, funded by the Natural Environment Research Council, to explore the Indian Ocean's underwater mountains, or seamounts.\nThe scientists aboard the research vessel, the RRS James Cook, will study life thousands of metres below the surface.\nIn the third of her BBC Nature diary entries, Aurelie Spadone from the International Union for the Conservation of Nature, who is part of the team, describes the evidence she has seen of the far-reaching human impact on these deep sea habitats.\nEnd Quote Aurelie Spadone IUCN\nThe fishing industry moves further away from the coasts and deeper into the ocean every day”\nWe're now half way through the cruise, although there is still a long way to go before reaching Port Elizabeth (South Africa), which is our arrival port. We are now in the vicinity of a seamount called Middle of What, the third of five scheduled stops on our itinerary.\nMelville bank painted a very different picture to that of Coral seamount. Throughout our exploration of this site we have found evidence of human impact.\nWe saw lost lobster pots, drifting pieces of fishing longlines, broken trawl wires and even rubbish - mainly plastic bottles. All of these were floating around near the seabed like ghosts. They would often appear on the monitors displaying what our remote submarine's high-definition cameras were capturing from far below the vessel.\nIt was strangely impressive, but also frightening and depressing to see this gear appearing in the devastated landscape of the seamounts.\nAnd all the detritus caused us a great deal of difficulty in completing our research dives.Remote but spoiled\nIncredibly, this area, despite being thousands of miles out into the ocean, has been heavily exploited by the fishing industry.\nMelville bank was fished in the late 1960s and 1970s, then more intensively between 2000 and 2002.\nOne fish in particular was the target species: the orange roughy, which can live up to 150 years and does not reproduce until it is 30-40 years old.\nIt is a solitary creature, but it forms big schools during the spawning period. Fishermen know this and take advantage of the situation, targeting these spawning schools. It is a real disaster for the fish population; many adults are removed.\nBecause seamounts attract so much life, and because coastal fish populations have been so depleted, these areas of the high seas attract the fisheries.\nAs shallow fish populations have become over-exploited, the fishing industry moves further away from the coasts and deeper into the ocean every day, using nets at 2,000m depth and trawling the seabed for fish.\nSome deep-sea fishermen are aware of the problem and want to make their business sustainable.\nOne fisheries association, the SIODFA (Southern Indian Ocean Deepsea Fishers Association) declared on a voluntary basis some seamounts of the southern Indian Ocean as protected areas. This means that fisheries avoid these areas.\nIt is a positive move for deep sea habitats that helps to protect and conserve target species and their homes.\nUnfortunately, globally, this is not the common practice. And the problem of unregulated fishing hampers these efforts amongst states and responsible fisheries.\nIt is also an ongoing threat to these fragile marine environments.""]"	['<urn:uuid:d428e0ab-2fa7-4093-ba0f-2aa9716c4b77>']	factoid	direct	verbose-and-natural	similar-to-document	single-doc	novice	2025-05-13T01:23:33.086345	17	34	547
25	In my role overseeing digital transformation, I'm interested in how commerce media networks handle consumer privacy protection, and how this connects to the customer experience challenges in EV charging networks?	Commerce media networks protect consumer privacy through data clean rooms that allow controlled data sharing between partners, with each partner maintaining control over data access, usage, and duration. They implement enterprise identity frameworks that respect consumers' privacy choices while connecting customer information across the organization. In the EV charging context, customer experience challenges include limited charging network availability, unpredictable charging experiences, and complex billing systems involving multiple stakeholders. Utilities can address these challenges by carefully planning and executing charging ecosystems with the right CRM platform, while ensuring proper data protection and creating positive consumer experiences through coordinated efforts with vehicle manufacturers, charging manufacturers, and regulatory authorities.	['What is a commerce media network?\nA commerce media network allows brands with rich first-party data to open up new revenue streams by offering highly personalised ads and closed-loop measurement to advertising partners. For CPGs and other companies with limited first-party data, buying ad space in a commerce media network opens up new ways to reach consumers, measure and improve performance, and unlock powerful new insights through data collaboration.\nMedia networks are not just for retailers\nRetailers are known for pioneering large-scale media networks, but companies across industries can take advantage of the opportunities and benefits that media networks offer. Companies in travel and hospitality, financial services, app-based services, automotive, real estate, and sports and entertainment are exploring the potential of commerce media networks for driving innovation. What matters most when starting a media network is the amount of first-party data your company has and a plan to grow that data through collaborative partnerships.\nToday, more than 80% of CPGs invest in at least one media network, and 85% of CPGs plan to increase their media network investments, reported McKinsey. But this opportunity isn’t limited to CPGs and retailers. McKinsey also found that more than 75% of non-CPG brands plan to increase spend with media networks in the next year—creating massive opportunities for media networks outside of retail to lead in innovation and revenue generation.\nHow do data clean rooms work?\nA commerce media network benefits everyone—the company that builds it, the brands and advertisers that partner with the media network, and consumers that engage with the network’s ads. Companies that monetise their first-party data and enter into innovative new collaborations create opportunities for advertisers to engage with consumers, drive sales, and access hidden insights from closed-loop measurement.\nCommerce media networks have risen in popularity as companies realise the many ways media networks fuel the flywheel for brands and businesses, including:\n- Driving new revenue\n- Strengthening strategic business partnerships\n- Delivering more personalised customer experiences\nHere are a few ways that media networks are unlocking growth for media network owners and partners.\nDiscover new consumer insights to drive sales\nA commerce media network allows a company with first-party data at scale (at least 5 to 10 million records) to create new audience segments to monetise and share with partnering brands. The partner brand can combine the media network’s audience segments with its own first-party data to discover new insights. By combining unique data sets, media network partners can create personalised, targeted ads for consumers across websites, apps, and in-store experiences, suggesting products and services based on fresh customer insights.\nEach partner should be able to easily control which data other partners have access to, how they can use that data, and for how long. By discovering customer preferences in a privacy-minded way, brands can create marketing content tailored to consumers, improve targeting, and ultimately drive more sales.\nAccess closed-loop measurement for accurate media spend results and campaign agility\nOnce an ad within the media network is launched, both companies can access exposure data for closed-loop measurement. Closed-loop measurement ties a sale directly back to the ad and audience segment, so marketers can track and report on the impact of media spend for revenue generation.\nAccurate closed-loop measurement also helps marketers make quicker decisions to optimise marketing programs.\nBelow you’ll find specific benefits for companies that want to own or partner with a media network, as well as benefits for the consumers interacting with ads in the commerce media network.\nHow do commerce media networks work?\nCommerce media networks are powered by first-party data and data collaboration, allowing companies to combine data sets to illuminate previously hidden insights and create a richer, more complete customer view to fuel innovation and drive business value. To own a successful media network, a company should have a rich, large amount of first-party data.\nCommerce media networks can also act as a full-service media provider—from creative to activation of media to measurement—for partnering brands. No matter the service options, it’s essential that a commerce media network is flexible to accommodate how, where, and when advertisers want to buy ads to reduce friction in the buying cycle and fuel business.\nCommerce media networks create an environment for companies to share valuable and unique consumer insights, so both enterprises can:\n- Obtain a clearer understanding of customer buying patterns\n- Create more personalised and engaging campaign content\n- Deepen customer connections\n- Make critical business decisions in real-time\nTo reap these benefits, companies that want to own a media network must build their network to support future innovation and growth. This is essential for adapting to swift industry changes, addressing privacy concerns, and meeting evolving consumer preferences. A critical step in building a flexible media network is choosing the right partners that understand the network’s mission and goals.\nThe role of partnerships in a commerce media network\nInnovative partnerships drive significantly more value for commerce media networks. Working together in an environment that respects each partner’s privacy requirements without fail is essential.\nWith a commerce media network that can enable secure data collaborations while protecting consumer privacy, companies can forge partnerships that establish loyalty and trust with consumers while staying ahead of evolving tech and consumer trends.\nClosed-loop measurement between partners is also a must. Companies must be able to consistently test, measure, and improve media performance across every touchpoint across marketing channels.\n“The member experience is something we think about everyday—whether they’re in the app, on the website, in the club, they’re using their phone—we have that transaction,” said Austin Leonard, Head Of Sales, Sam’s Club Member Access Platform (MAP).\n“Because it’s a membership model, we have the ability to have a 100% closed loop. We can track back to every transaction, and it’s a big part of why the next frontier is partnering so we can talk to our members before they start that shopping experience and then retain them afterwards. It’s a big part of how we’re building that business together.”\nHow do commerce media networks drive personalisation and protect consumer privacy?\nA core component of a commerce media network is an enterprise identity, which ties customer data together across an organisation’s various business units, functions, and data sets while respecting consumers’ privacy choices.\nA consistent, unified identity framework allows companies to reconcile disparate customer information and combine internal data sets for a more complete customer view.\nThis is how NBCUniversal uses a unified identity to connect customer information across its enterprise.\n“We’re in a market with 1,200 first-party data attributes, and then we work to be interoperable so we could make this data meaningful to our brands as well as our advertisers,” said Kaitie Coghlan, SVP, Data Product & Partnerships at NBCUniversal. “We’re interoperable with identities, clean rooms, with UIs so we could get this data into the hands of our brand marketers, our advertisers, our sponsors, our partners, so they could have meaningful engagements with our consumers.”\nHow can I start a commerce media network?\nIf a company has rich first-party data that could be valuable to partners, advertisers and publishers, it’s in a great position to start a media network. This foundation of rich first-party data is crucial as advertisers want scale and performance when deciding where to place media budgets.\nTo start a commerce media network, a company might begin by monetising its first-party data to partners looking for targeted ad options. First-party data is key for advertisers looking to access new information about existing and future customers. By combining first-party data and audience segments, media network partners can build a fresh view of customer preferences, behaviours, and needs.\nThe strongest media networks go a step further by collaborating with brands and publishers in data clean rooms. This happens when two or more companies combine first-party data sets to create a unique second-party data set with new customer insights that can be shared and analysed in closed-loop measurement. Through this collaboration, the customer can be reached with more relevant, right-time offers across channels and platforms and new, innovative business insights can emerge.\nQuestions to ask when starting a commerce media network\nBefore starting a commerce media network, a company should consider the following questions:\n- Can your audiences provide unique value to partners? Does your data fill a need for advertisers? Wakefield Research found that advertisers prefer to only work with media networks that have a minimum number of unique consumer profiles.\n- Do you have permission to share consumer data through opt-in or opt-out policies? Are your policies up-to-date with current regulations?\n- Do you have enough first-party data to share with partners in your media network?\n- Do you have a strategy to continue to expand on the value of your data?\n- Can your data be extracted for advertising purposes?\nIt doesn’t matter the industry or company size, if an enterprise has consumer insights that could provide an essential value to other brands and advertisers, it’s in a great position to unlock the brand-building power of a media network.\nCommerce media network trends and outlook\nBy delivering immense value to companies and consumers, media networks are becoming one of the largest media channels in the world, according to Forbes. Not to mention, investments in retail media networks alone have tripled in the past three years and are expected to double again. Companies that want to build or expand a media network should prepare for the following trends:\nTrends in privacy: As more privacy laws are adopted across the world, media networks may face change winds in standardisation. This could mean changes to measurement and transparency in data collaboration partnerships. In the face of these changes, it’s critical that media networks are built on a strong data foundation with partners prepared to facilitate data collaboration in a flexible and privacy-focused way.\nCookie deprecation: With cookie deprecation and increasing privacy regulations, companies need more privacy-sensitive data to fully understand consumers’ habits, behaviours, and needs. As third-party data becomes limited, first- and second-party data is even more critical to execute targeted marketing campaigns.\nCommerce media networks serve as the entry point for companies to share first- and second-party data to better serve consumers and fuel business growth.\nThe future of Commerce media networks\nCommerce media networks are growing incredibly fast. For example, McKinsey predicts that retail media networks could amount to $100 billion in ad spending in the U.S. by 2026—and this is only one industry.\nAs brands realise the potential of media networks and see a fuller view of customers, the possibilities for innovation across brands, products, and consumer engagement is endless.\nSo, the question is: How could your company benefit and grow from a commerce media network?\n5 Best Practises for Building a Successful Media Network\nGet ready to set your brand apart with a media network that inspires business innovation and growth', 'Utilities in the age of electric vehicles\nUtilities in the age of electric vehicles\nUtilities in the age of electric vehicles\nThe impact of vehicles on the environment has driven regulatory mandates to adopt a more sustainable way of commuting. As a result, electric vehicles (EVs), and the necessary infrastructure to operate them, has changed the automobile and utility industries over the past decade.\nElectric vehicles are powered by a charged battery pack and can be separated into two categories:\n- Battery Electric Vehicles (BEVs): These EVs are purely electric with lithium ion batteries suitable for short to medium distances.\n- Plug-In Hybrid Electric Vehicles (PHEVs): Electric vehicles with an internal combustion engine (ICE) with support from a small electric motor.\nWhy Electric Vehicles?\nThe 2015 Paris Agreement has challenged countries to reduce their carbon emissions to “net zero” over the coming years. This international treaty has prompted governments around the world to phase out gas and diesel powered vehicles, shifting instead to EVs:\nSales of electric vehicles have grown steadily over the last decade. The following chart from the International Energy Agency shows China leading market share at 47%. Twenty other countries have reached a market share of above 1%: emissions. As a segment, the automobile industry can tout sustainability and the environmental benefits of emerging technologies to entice consumers to buy EVs.\nAccording to a study by IRENA (International Renewable Energy Agency) on EVs:\n- Electric passenger cars will reach 200 million by 2030\n- Electric two-wheeled and three-wheeled vehicles could outnumber four-wheeled vehicles, with as many as 900 million on roads by 2030\n- Electric buses and light-duty vehicles could surpass 10 million by 2030\nFactors Contributing to EV Adoption\n1. Consumer Interest:\nEco-friendly consumers who want to decrease their carbon footprint prefer to buy EVs. Transportation around the globe is one of the biggest contributors of carbon emissions. As a segment, the automobile industry can tout sustainability and the environmental benefits of emerging technologies to entice consumers to buy EVs. A 2019 international electric vehicle consumer survey (of 7,600 consumers in seven regions) shows consumer interest in electric vehicles is high. 50% of consumers say they’re interested in owning an EV and 28% say they’ll purchase one as their next vehicle.\nConsumer benefits to owning electric vehicles:\n- Reduced operating costs, lower charging prices and simpler maintenance\n- Quieter driving experience\n- Exemption in Clean Air Zones – areas that charge fees to vehicles that pollute the environment\n- Government subsidies that make EVs cheaper than ICE vehicles\n- Preferential parking permits in dense urban areas\nEV technology has vastly improved. Range limitations and charging times have been addressed, alleviating concerns and increasing purchase momentum. Consider these three, top selling EV models in the world in 2020:\n3. NetZero Target:\nIn support of the 2015 Paris Agreement, utility and automobile companies are working to achieve net-zero emissions. To do this, they’re offering customers low-carbon products such as renewable electricity and electric vehicles. By taking advantage of these offerings, individual consumers can reduce their carbon footprint. Businesses can reduce their overall cost of fleet ownership, and organisations can reduce fuel costs, reap tax benefits and take advantage of government incentives.\nChallenges for Utilities:\nEVs help combat climate change. However, barriers to adoption exist:\nCharging Pricing: An increase in the number of electric vehicles can lead to disorganised charging. This makes peak shaving difficult, creates incremental costs for generators, increases transmission and distribution pressures and reduces grid reliability and security. It also degrades power quality and increases the harmonics of the grid. Ultimately, an unreasonable pricing structure can lead to its failure. A dynamic pricing strategy can help utilities overcome the challenges of EVs and can reduce the burden of power on a grid.\nComplex Billing: EVs also present billing challenges for utilities:\n- Number of Stakeholders: Charging hosts, charging point operators, eMobility service providers, roaming network providers, etc. are all involved in the billing process. These stakeholders have to manage multiple plans – pre-paid, postpaid, ad hoc, group plans, etc.\n- Customer Type and Charging Location: Plans offered will vary based on customer type such as individual, fleet, business, public and private. They’ll also vary based on location, including home, office, fleet charging center, parking lot, multi-tenant unit, municipal location and more.\n- Price Per Charge: The customer can be charged based on charge point, price per kWh or by minute/ hour (flat fee). The charging session may include ancillary fees such as a connection fee or a waiting fee for staying connected after reaching a full charge.\nCharging Infrastructure: The mechanics of charging pose challenges to utility companies:\n- Network: Availability remains limited\n- Technology: Fast-charge still in its initial stage and widely unavailable in the network\n- Customer Experience: Unpredictable charging experience negatively affects customer opinion\nService and Maintenance: Electric vehicles require specialised mechanics who are still difficult to find. According to a study done by UK’s Institute of the Motor Industry (IMI), 97% of today’s mechanics aren’t qualified to work on electric vehicles. Of the 3% of mechanics who do qualify, many work directly for EV dealerships, limiting service options for general EV buyers.\nHigher Upfront Investment: Higher manufacturing costs vs. the cost to make a combustion engine vehicle make EVs more expensive to buy. This sticker shock feeds consumer doubt about the long-term economic benefits of an electric vehicle. Government subsidisations help alleviate that doubt, but total consumer buy-in will take time.\nThe Utility Opportunity\nAs more people switch to electric cars, the impact of EV charging loads on generation, transmission and distribution networks translates into more energy and more revenue opportunities for utility companies.\n- Charging Infrastructure: Utilities can play a vital role in modulating charging rates and shifting charging times to provide grid services that support supply and demand. Consider these energy giants already investing in charging infrastructure:\n- Shell recently announced the rollout of 500,000 electric charging stations over the next four years.1\n- Ecotricity a “Big Six” UK energy supplier, partners with Moto, RoadChef and Welcome Break to offer 45-minute fast-charge stations. They call the network “The Electric Highway.”2\n- In the UK, companies like Centrica are building out their EV charging capabilities by acquiring smaller independents. Centrica invested in Driivz, a software company that manages EV fleets and charging networks, to create Centrica Electric Vehicle Services (CEVS) 3\n- New EV Tariffs: Consumer tariff structures (e.g. time-of-use tariffs) reward consumers who slow-charge during off-peak hours. These tariffs, which reduce consumer bills and prevent overloads on the grid, help influence EV drivers to shift their charging behavior. By partnering with EV manufacturers, utility companies can create custom electricity tariffs that can be bundled into the purchase of an electric vehicles. Energy suppliers in the US, UK and other European countries have already begun offering EV energy tariffs.\n- Improved Customer Experience: Careful planning, phased execution and synergy with non-utility businesses can help electric utilities facilitate a smooth transition to EV adoption. With the right customer relationship management (CRM) platform in place, utilities can offer consumers “charging ecosystems” – chargers, charging plans, etc. - for their vehicle. This positive consumer experience, combined with the financial upside of aligning with non-utility companies, translates into increased revenue for the utility company.\n- Vehicle-to-Grid (V2G): While EV tariffs can prevent overloads by shifting charging behavior, they also present challenges. If too many EV drivers charge during off-peak times, it can spike load levels and lead to grid congestion. To counter this, vehicle-to-grid (V2G) enables energy to be pushed back to the power grid from the battery of an electric vehicle. This helps balance the variations in energy production and consumption. Furthermore, V2G can support the integration of renewable energy resources into the grid.\n- AI Driven EV Marketing: With electric vehicle purchases on the rise, utilities must position themselves at the forefront of energy innovation to ensure brand credibility. AI-driven marketing helps identify crucial digital touchpoints for targeted messaging.\n- Data Advantages: Utilities can use data analytics and data science tools to develop services, applications and hyper-personalised product offerings. They can also leverage the data to expand into non-core markets. This allows for:\n- Joint offerings with automotive companies\n- After sales services in conjunction with car dealerships\n- Installation of charging stations at locations where customers park electric vehicles for more than an hour\nGlobally the uptake of electric vehicles is leading to the transportation and electricity sectors becoming increasingly connected.\nEven though barriers to EV adoption exist they are phasing out due to technological advancements. Innovation is the key in identifying opportunities to minimise costs and reducing pain points.\nFor utilities the biggest challenges is to ensure grid reliability and resilience. Providing a successful infrastructure for EV adoption will require coordination among various parties - Vehicle & charging manufactures, Electricity service providers, Distribution network operators, and Regulatory authorities\nIn the long run - Utilities that invest in electric vehicle infrastructure and technology will be well-equipped to offer solutions that benefit future customers.\n2Ecotricity and Nissan install UK electric-car-charging network | Guardian sustainable business | The Guardian\nEXL Utilities Academy']	['<urn:uuid:91bed858-f059-42cd-a64b-6662760f9dd8>', '<urn:uuid:dc7f0a0b-46cd-416c-b618-7a830371a8fd>']	open-ended	with-premise	verbose-and-natural	similar-to-document	multi-aspect	expert	2025-05-13T01:23:33.086345	30	106	3324
26	temperature control historical brewing modern tea preparation evolution techniques	Temperature control evolved differently in brewing and tea preparation. In medieval brewing, specific temperatures were used (165°F for initial water, 152°F after mixing), though thermometers weren't invented until after 1600. In Japanese tea preparation, water temperature was traditionally gauged by watching for bubbles just before boiling point. While brewing maintained these precise temperature requirements, tea preparation became more focused on the ceremonial aspects rather than technical precision, evolving from a simple beverage preparation into a highly structured ritual.	"[""Now she could have used a couple of gallons of cheap beer, but that wouldn't have had quite the right taste. And since all the other members of the guild were involved in the event, we thought that the best, most authentic thing to do was to make ourselves some closer-to-authentic unhopped ale.\nThis ale was started two weeks in advance of the Schola. It was still (slowly) fermenting when half of it was used to cook in; it finally stopped about a week after that. It was racked, and bottled still on March 17th. It will have been in the bottle for all of 5 days when judged.\n4 1/2 gallons of a fine all-grain unhopped Ale may be made:\nIn an insulated mash-lauter tun, mix 9 qts. of water at 165 degrees F. with the above 12 lbs of grain. (145 F. after mixing). Add and mix in 2 more qts. of boiling water, to reach 152 F. Let sit for an hour and a half.\nRecirculate until clearer, and then start to drain out. While draining, carefully add 4 gal. of boiling water to the top of mash tun. (A tip: I pour my water onto a foam plate that is floating on the surface in order to avoid disturbing the grain bed.)\nCollect all runnings in a large pot, and boil for an hour. Then cool and put into a fermenter.\nAt this point, we ended up with about 3 gallons of wort at a specific gravity of 1.088. This would have made fine ale, though a very strong one. But we felt that starting something this strong would not be adequately finished in the two weeks of time we had, so we watered it down to approximately 1.058 by adding another 1.5 gallons of boiled and cooled water.\nBoil the oak chips in approx. 1 cup water. When the water is darkened, take off heat and allow to cool. Then add approx. 3 oz. of oak-water to the wort. Be careful not add too much.\nAerate (by rolling fermenter back-and-forth and/or stirring aggressively), Pitch yeast and allow to ferment.\nThe part that was not used for the feast was racked (siphoned off the lees) after fermentation had finished and the ale had substantially cleared. It was bottled still (i.e. without priming sugar) a week later.\nWe were trying to recreate much of the taste of a 15th c. English ale. We know that there were some changes to what was considered a `beer' and what was considered an `ale' during this time.\nHaving therefore groond eight bushels of good malt upon our querne, where the toll is saved, she addeth unto it half a bushel of wheat meale, and so much of otes small groond, and so tempereth or mixeth them with the malt, that you cannot easily discerne the one from the other...\nMarkham, in The English Housewife (1615, 1623), writes:\nAfter the Hundred Years War, in the 15th c., England gained a large number of Flemish and Dutch immigrants, who brought with them the taste for hops in beer [Smith, p. 25]. During part of this time, `ale' was legally defined to be a malted beverage without hops; `beer' having hops. Hence our choice to leave them out for this recipe.\nBrewing of strong aleNow for the brewing of strong ale, because it is drink of no such long lasting as beer is, therefore you shall brew less quantity at a time thereof, as two bushels of northern measure (which is four bushels or half a quarter in the south) at a brewing, and not above, which will make fourteen gallons of the best ale. Now for the mashing and ordering in the mash vat, it will not differ anything from that of beer; as for hops, although some use not to put in any, yet the best brewers thereof will allow to fourteen gallons of ale a good espen full of hops, and no more;...\n[Markham, p. 207]\nTraditionally the sprouting would be carried out on a large floor [Markham, pp. 182-185], often in the attic of the malt house. As the grain sprouts, it generates heat. This must be allowed to escape so that the malting grain does not cook itself. The young plants also require carbon dioxide to continue growing. To facilitate this, the malt is turned (scoop it up, flip it over) at regular intervals. A few British maltsters are still producing floor-malt today, though it is rare and expensive.\nHistorically, kilning was often carried out in what is essentially a large wood oven or smoker. The malt would be spread out on a false-floor made of hair-cloth, straw mat, or other suitable material, on top of some time of loose material, so to allow the hot exhaust from the kiln's oven to evenly penetrate the grain. Then a wood fire would be built in the oven, and the malt baked for several hours, and occasionally turned to prevent burning [Markham, pp. 186-190].\nModern kilning is quite different. Today the malt is roasted in a drum with a water spray to control temperature, patented by D. Wheeler in 1817 [Harrison].\nNormally, to better approximate amber and brown malts, I have roasted some portion of Pale malt in an oven. However, for this ale, we did not go to this extent, but instead opted for using English Mild malt rather than Pale. Mild is generally roasted just slightly more by the maltster.\nFollowing the Harrison recipe above, we also used a small amount of plain wheat and oats.\nMarkham recommends something similar, though not as complex a technique. He says to combine some of your wort (presumably cool enough) with some barm (yeast), and let these work while the main batch is cooling. Then when the main batch is cool, stir up this starter well and mix it in.\n... This quantity (of a hogshead) will require better then a quart of the best Ale- barm, which you must put to it thus. Put it to about three quarts of wort, and stir it, to make it work well. When the barm has risen quick scum it off and put to the rest of the wort by degrees. The remaining Liquor (that is the three quarts) will have drawn into it all the heavy dregs of the barm, and you may put it to the Ale of the second running, but not to this. Put the barm you have scummed off (which will be at least a quart) to about two gallons of the wort, and stir it to make that rise and work. Then put two Gallons more to it. Doing thus several times, till all be mingled, which will require a whole day to do. Cover it close, and let it work, till it be at it's height, and begin to fall, which may require ten or twelve hours, or more. Watch this well, least it sink too much, for then it will be dead. Then scum off the thickest part of the barm, and run your Ale into the hogshead, ...\nUsing a starter is good practice in modern, as well as medieval brewing. Starting with a large quantity of yeast will reduce the effects of wild yeasts and other microorganisms by overwhelming them by sheer number, and eating up all the available sugar.\nOf course, both the thermometer and hydrometer were invented after 1600. Being somewhat paranoid, however, we used them to control this particular batch.\nHeat Spring-water; it must not boil, but be ready to boil, which you will know by leaping up in bubbles. Then pour it to the Malt; but by little and little, stirring them strongly together all the while they are mingling. When all the water is in, it must be so proportioned that it be very thick. Then cover the vessel well with a thick Mat made on purpose with a hole for the stick, and that with Coverlets and Blankets to keep in all the heat. After three or four hours, let it run out by the stick (putting new heated water upon the Malt, if you please, for small Ale or Beer) into a Hogshead with the head out. ...\nI use a 10-gallon Rubbermaid-brand water cooler, with an Easymasher(tm) screen-manifold (a 6-inch tube of stainless-steel screen, closed on one end and attached to a tube on the other) installed inside.\nTofi Kerthjalfadsson, guildmaster, BMDL Brewers' Guild."", 'In Japan, the simple act of drinking tea has been elevated to an art.\nTea was introduced to Japan from China sometime in the early 9th century. As in China, tea was appreciated in Japan for its medicinal value. It was also popular at the emperor’s court in Kyōto as an elegant beverage associated with Chinese higher culture, which Japan’s ruling courtier class was then assiduously imitating.\nIn the late 9th century, Japan’s direct borrowing from China largely ended. One result was that Japanese green tea lost its popularity, and before long the Japanese may even have stopped drinking the beverage altogether. In any case, we find little about tea in Japanese historical records for the next 300 years. Then in the late 12th century, the Japanese monk Eisai, one of the founders of the Zen sect of Buddhism, reintroduced tea to Japan from China. Eisai wrote a book praising tea as a medicine that was especially good for the heart. He also recommended tea as a means of staying awake during the long hours of seated meditation practiced in Zen temples.\nThe tea that Eisai brought from China was green, or unfermented. (In contrast, the black– or as it is known in Japan, red–tea drunk by most Westerners is fully fermented.) Eisai also introduced Japan to a new way of making tea. The Japanese originally had adopted from China a method of tea preparation in which tea was shaved from a brick of pressed leaves, mixed with flavorings such as salt and ginger, and boiled. However, sometime after Japan’s first exposure to tea, the Chinese developed the practice of making tea by dissolving a powder made from tea leaves in hot water. To stir the tea, the Chinese developed a specialized implement known as the whisk. Although the Chinese eventually abandoned the use of powdered tea, the Japanese incorporated both powdered tea and the whisk into their tea ceremony in the 15th century. In this way, the ancient practice was preserved. Nearly all modern tea drinkers, including the Japanese, drink infused tea (in which the tea, loose or in bags, is placed directly in hot water) in their everyday lives.\nBy the 14th century, tea drinking had spread to all classes of Japanese society. At the boisterous tea gatherings of the time, members of Japan’s warrior elite participated in tasting contests to identify varieties of tea. Of the various types produced domestically, that from Toganō in the mountains northwest of Kyōto was the most highly prized. At these parties, the warriors also made ostentatious displays of the rare and costly tea utensils that they began importing from China in the early 14th century, like Japanese teapots and Japanese tea sets.\nTea ceremony in Japan\nThe highly structured tea ceremony, in which powdered green tea is prepared for guests in their presence, began to develop in Japan during the 15th century. At that time the Japanese were seeking to cultivate a more serious appreciation of the articles of art and craft that they had been importing from China. Included among these things Chinese were paintings, especially in the monochromatic ink style; calligraphy scrolls; ceramics; and lacquerware. It is possible that Japanese initially gathered for the purpose of admiring these artistic objects and simply served tea as refreshment. However, the drinking of tea itself eventually became the focus of the gatherings, and participants used the Chinese objects for serving the tea, as well as for display in alcoves and on shelves.\nA major step in the development of the tea ceremony occurred when the host began to prepare the tea in the same room in which it was served to guests. The host prepared the tea with strictly regulated, ritualistic movements, while guests adhered to an exacting etiquette that defined their every movement as well as their occasional conversation with the host. To enter a tearoom was to enter another world, separate from the mundane, everyday life outside. Indeed, from the late 15th century to the late 16th century, the tea ceremony became a model of social harmony (thanks to the tea ceremony ustensils) and spiritual fulfillment in a time of internal conflict and violence in Japan that was so intense it has been called Sengoku, or the “era of warring states.”\nAs informal tea drinking was transformed into the ritualistic ceremony known as the “way of tea,” it was suffused with the spirit of Buddhism, especially Zen. By the late 16th century, when the tea ceremony was raised to its highest level by the great master Sen no Rikyū, practitioners of the ceremony frequently asserted that “tea and Zen have the same flavor;” that is, they are one.\nThe tea ceremony also became a focal point for the development and expression of artistic and aesthetic values. The great tea masters, who were versed in the procedures of serving tea and made a career of teaching them to others, became leading arbiters of taste in the various arts and crafts that were brought together in the ceremony. Although fashions in the tea ceremony varied from period to period, it came to be associated especially with the aesthetic of wabi. Wabi has no exact parallel in English, but one authority defines it as a beauty that is simple and unpretentious, imperfect and irregular, or stark and austere. A typical Japanese wabi tea bowl or Japanese tea cups, for example, is likely to be muted in color, only partially glazed, and imperfectly shaped.\nThe “wabi tea” of the 16th century is commonly regarded as the tea ceremony’s highest spiritual and aesthetic form.Merchant families in cities such as Kyōto, Nara, and Sakai were leaders in the evolution of the tea ceremony, especially the wabi form. Sen no Rikyū, for example, emerged from a merchant school in Sakai. Patronage for Rikyū and other masters came mainly from Japan’s warrior chieftains. Rikyū rose to national prominence in the service of two warlords, Oda Nobunaga and Toyotomi Hideyoshi, who unified Japan in the late 16th century, bringing the period of Sengoku to an end.The tea ceremony flourished during the more than 250 years of peace that followed under Japan’s Tokugawa shoguns. Tea attracted practitioners from all classes of society, and various schools of tea emerged. The most important of these were three merchant tea schools founded by grandsons of Sen no Rikyū: Urasenke, Omotesenke, and Mushanokōji Senke. Rikyū himself was deified as the “god” of tea, and all schools claimed spiritual affinity with his way of tea.\nThe modern world\nAfter Japan entered the modern world with the overthrow of the Tokugawa shogun in 1867, many of its traditional arts fell on hard times. Along with other native customs, the tea ceremony was neglected as the Japanese eagerly pursued Western cultural forms and practices. In an effort to resuscitate the ceremony, tea masters managed to have it incorporated into public school education as a means of teaching traditional etiquette and conduct to Japanese girls and young women. Whereas practitioners of the tea ceremony previously had been almost exclusively male, from this time on they became increasingly female. Today, the vast majority of those who practice tea are women.\nThe tea ceremony continues to enjoy considerable popularity in Japan. The largest tea school, Urasenke, is said to have some two million students. Those who participate in the tea ceremony, however, are an aging group. There is concern that in a rapidly changing Japan, the younger generation and their children may not take to tea as readily as did their parents and grandparents. A decline in the popularity of the tea ceremony would be lamentable in part because it would diminish a unique art form that for centuries has been at the center of Japanese culture. In addition, fewer tea practitioners would mean fewer patrons for traditional Japanese crafts, including ceramics, lacquerware, Japanese tea tins and bamboo work, that have long been closely associated with the tea ceremony.\nAbout the author:Paul Varley is Sen Sôshitsu XV Professor of Japanese Cultural History at the University of Hawaii and the author of Tea in Japan, Essays on the History of Chanoyu']"	['<urn:uuid:fa607891-d9f0-494e-8b83-6401b7eb946f>', '<urn:uuid:b29184a8-e70c-470f-aa94-31081fabcc06>']	factoid	direct	long-search-query	similar-to-document	multi-aspect	novice	2025-05-13T01:23:33.086345	9	78	2759
27	how ocean pollution impacts coral survival and sea otter population decline causes	Ocean pollution threatens both coral reefs and sea otter populations through different mechanisms. For corals, plastic debris causes disease outbreaks by creating anoxic conditions, releasing toxins, and causing physical injuries that compromise their immune systems. There are at least 11.1 billion plastic items tangled in coral reefs across the Asia-Pacific region alone. For sea otters, multiple threats have caused population decline - they were nearly extinct by the early 1900s due to hunting, and today face challenges from shark attacks, mating injuries, and diseases from pollution. Particularly concerning is their exposure to domoic acid from harmful algal blooms, which causes fatal heart disease and disproportionately affects adults of reproductive age that are crucial for population recovery. Currently, there are fewer than 3,000 sea otters off the California coast.	"['(Natural News) The phrase “deep, blue sea” may slowly become a thing of the past, as more seas and oceans are being filled up with waste plastic. Its adverse effects have surely been felt by nearly all marine life, but a team of international researchers led by Cornell University has discovered one more unfortunate effect of plastic. They found out that plastic waste drifting in the ocean can adversely impact the corals it gets tangled with, as the material provides a favorable environment for microbial colonization.\nThe study, which was published in the journal Science, had researchers investigate 159 coral reefs in the Asia-Pacific region to determine how the plastic debris affects the coral, as well as the likelihood of disease of a coral tangled with plastic, visiting reefs from Indonesia, Australia, Myanmar, and Thailand.\nThe reason for this, the research team stated in their paper, is that there have been several studies of the impact of terrestrial pollutants in the ocean; however, the full effect of plastic waste in marine organisms has yet to be explored.\nAfter surveying the reefs, the research team found that billions of plastic materials have been tangled in the reefs – with more spikey species getting tangled with more plastic materials. The amount of plastic in the sampled areas was estimated to be at least 11.1 billion items that are stuck in coral reefs across the Asia-Pacific. (Related: The plastic pollution problem is wide AND deep: Study finds sea animals from the deepest parts of the ocean, 7 miles down, have plastic in their stomachs.)\nAccording to study lead author Joleah Lamb, plastic debris act as a “motorhome” for microbes.\n“Plastics make ideal vessels for colonizing microscopic organisms that could trigger disease if they come into contact with corals,” she continued. “Plastic items – commonly made of polypropylene, such as bottle caps and toothbrushes – have been shown to become heavily inhabited by bacteria. This is associated with the globally devastating group of coral diseases known as white syndromes.”\nIf a plastic latches itself onto a coral, it becomes prone to disease. In particular, the study reported that once this happens, the likelihood of the coral to get a disease increases by 20 times.\nThis is because of the stress brought about by the plastic to the corals. In particular, it blocks the light source of the corals, leading to anoxic conditions which favor pathogen growth, as well as toxin release.\nIn addition, corals are physically injured when they come in contact with plastic debris. This leads to abrasion, which opens up the coral to pathogen invasion. When this happens, the immune system of the coral may be compromised, allowing pathogens such as Halofolliculina corallasia to infect it and cause complications such as skeletal eroding band disease.\nThe authors posited that if these conditions continue, the amount of plastic that will impact the marine environment will skyrocket to 15.7 billion items, which will ultimately affect corals the world over. When this happens, the effects will not only be felt by the tourism industry but by those in fishing and coastal protection as well – a combined net worth of $357 billion in goods and services provided.\nThrough this investigation, researchers opined that a decrease in the amount of plastic entering the ocean will be advantageous to coral reefs, especially as its chances of getting disease-associated mortality is reduced.\n“Our work shows that plastic pollution is killing corals. Our goal is to focus less on measuring things dying and more on finding solutions,” concluded Drew Harvell, a senior author of the study. “This new work should drive policy toward reducing plastic pollution.”\nLearn more about how plastics affect the environment, and what you can do to help solve this problem. Head over to Pollution.news today.', 'Algal blooms target sea otter hearts\nA toxin formed during algal blooms, which are increasingly common due to climate change, leaves sea otters at risk of deadly heart disease.\nWithin the past decade, those working on the frontlines of marine health have treated an unprecedented number of animals poisoned by harmful algal blooms.\nJayme Smith, the Southern California Coastal Water Research Project\'s harmful algal bloom expert, was an undergraduate student at Vanguard University of Southern California working with sea lions at a local marine mammal rehabilitation center when she consistently saw these devastating impacts.\n""It\'s really heartbreaking,"" Smith told EHN. ""A lot of times it\'s the adult female sea lions and they\'re in really bad condition, they have seizures, and a lot of times they can\'t recover.""\nThe culprit behind these episodes: domoic acid. This biotoxin accumulates in the food web during algal bloom events and causes severe health effects in larger animals, humans included.\nNow, researchers warn that the potent toxin targets the hearts of sea otters, threatening already sensitive populations, according to a recent study in the Harmful Algae journal.\nThe study, which monitored 186 free-ranging sea otters between 2001 and 2017, found that long-term exposure to domoic acid raises their risk of fatal cardiomyopathy, which makes it harder for the heart to circulate blood throughout the body, by 1.7 times, on average.\n""Cardiomyopathy in sea otters can be incredibly severe,"" Cara Field, a veterinarian at the Marine Mammal Center in California, told EHN. ""The main effect domoic acid has on the heart is it binds to cells that tell the heart how and when to beat and because of that, it causes one or more of the heart chambers to not eject blood through the body well.""\nSome sea otters in the study that ate large amounts of crab and clam were 2.5 times more likely to suffer from the heart disease. Crabs and clams are effective bioconcentrators of domoic acid, meaning the toxin is higher in those creatures than in the surrounding water.\nClams typically comprise 36 percent of the sea otter diet, making this a worrisome driver of cardiovascular illness.\nThe study also found that cardiac risks associated with domoic acid were ""highest among adults of prime reproductive age – whose survival is critical to population recovery,"" lead author, Megan Moriarty, a free-ranging wildlife health resident at the California Department of Fish and Wildlife, wrote in a University of California, Davis article.\nSea otter stressors\nSea Otters at the Morro Bay harbor in California. (Credit: Mike Baird/flickr)\nNorth America\'s sea otters have had a difficult journey. In Oregon, they were eliminated by 1906 when the last sea otter was sold for its $900 pelt. In California, they were considered extinct until 1938, when a group of around 50 was found outside of Big Sur. There are now fewer than 3,000 sea otters off the California coast, according to a United States Geological Survey census.\nToday, while shark attacks and injuries from mating are the leading causes of death in sea otters, disease—often the result of pollution—is also a massive stressor. For example, up to 70 percent of stranded southern sea otters were infected with toxoplasma at one point, an illness sometimes caused by cat feces flushing into the oceans.\n""There is a long list of infectious diseases in sea otters and many of them are washed in from the land,"" Kevin Lafferty, an ecologist at University of California, Santa Barbara\'s Marine Science Institute, told EHN.\nOil spills have also hampered sea otter health. The Exxon Valdez spill of 1989, which resulted in 11 million gallons of crude oil permeating the waters of Prince William Sound, killed 2,800 sea otters. According to the National Ocean Service, oil spills affect otters\' ability to insulate, leading to hypothermia.\nDisappearing kelp forests\nSea otters help keep kelp abundant. (Credit: Jonathan Kriz/flickr)\nAs sea otter populations suffer, so do the ecological services they provide.\n""Sea otters increase standing kelp by consuming significant amounts of kelp herbivores,"" John Goodell, director of science and policy at Elakha Alliance, an organization dedicated to reintroducing sea otters to the Oregon coast, told EHN. ""By doing that, they can dramatically transform the rocky subtidal ocean environment into a very vibrant kelp forest, and that provides habitat for a lot of juvenile and adult fish.""\nIn the remote Aleutian Islands of Alaska, sea otters have been functionally extinct since the 1990s, likely due to increased killer whale predation, scientists say. This has transformed a set of dense kelp stands into what Douglas Rasher, a marine community ecologist at the Bigelow Laboratory for Ocean Sciences, calls ""something of a barren wasteland.""\nBetween 2014 and 2017, Rasher repeatedly dove into the endangered kelp forests among the Aleutian Islands to monitor the cascading effects of sea otter loss. After sea otter decline, sea urchin populations, a major source of prey for them, grew rapidly. The hungry shellfish chewed through miles of kelp, destroying habitats that many fish species needed to thrive.\n""When sea otters were abundant, there were lush kelp forests as far as the eye could see,"" Rasher told EHN. ""When you dive on a reef where sea otters are missing, what you see is little to no kelp, and the rocky bottom of the seafloor is blanketed by large sea urchins.""\nAs kelp forests disappear, fish decline, animal body growth slows, and the diets of nearshore birds are forcefully shifted, Rasher said.\nIn addition, Goodell explained that some human systems are just as damaged. Sea otters that maintain kelp forests generate vast economic benefits. Kelp forests orchestrated by sea otters can increase commercial fishery harvests by up to six times.They even slow global warming. According to Harvard University, kelp forests can suck up 20 times more carbon dioxide per acre than terrestrial forests.\nClimate change increases harmful algal blooms\nYankee Doodle, an adult male southern sea otter, feeds on shrimp during rehabilitation at The Marine Mammal Center in Sausalito, California. (Credit: Bill Hunnewell © The Marine Mammal Center // USFWS permit MA101713-1)\nHarmful algal blooms have long imperiled sea otters and other marine mammals, but it wasn\'t until recent decades when people started noticing domoic acid\'s involvement.\nAlgal blooms that are formed by toxin-producing diatoms of the genus Pseudo-nitzschia are responsible for domoic acid. While these species commonly bloom along California and other regions of the West Coast, they can expand their range as climate change warms northern waters, making conditions more favorable to them, Smith said.\nFor North America\'s sea otters, this means that toxic algal blooms could intrude more on their northern habitats, up near Canada and Alaska.\nWhen algal blooms proliferated from California to Alaska in 2015, historic numbers of dead sea otters were found washed up on beaches. More than 300 dead or dying sea otters covered the beaches of Homer, Alaska, that year, which was five times the normal count.\nClimate change will also elevate the risks of nutrient pollution, which is the main human contributor to harmful algal blooms.\nThis form of pollution tends to come from nitrogen and phosphorus fertilizer runoff washing away from farms, which eventually makes its way into the ocean.\nAs extreme weather events become more common amid a changing climate, more rainfall will increase the amount of nutrient pollution reaching waterways by 19 percent, according to a 2017 study.\nTackling farm run-off\nAccording to Kate Poole, senior director for the water division of the Natural Resources Defense Council, reducing algal blooms to protect sea otters is not a hopeless cause—but this must come with greater regulation of farm runoff.\n""One thing we\'ve been looking at is how cover crops, that you plant on farms during non-growing seasons, can help retain the soil and the pollutants on the farm,"" Poole told EHN. ""No-till practices also reduce that runoff.""\nPoole also believes that national policy initiatives could reduce nutrient pollution. For example, she said the Environmental Protection Agency should introduce a limit on the amount of nitrogen pollution farms can contribute to waterways.\n""We have seen this problem worsen in the last couple decades and it\'s just going to get worse if we don\'t start dealing with the causes,"" Poole said.\nBanner photo: Otto, an adult male southern sea otter, sorts through an assortment of shrimp during rehabilitation at The Marine Mammal Center in Sausalito, California. (Credit: Dana Angus © The Marine Mammal Center // USFWS permit MA101713-1)']"	['<urn:uuid:f7812256-e299-4217-9b06-61db33243fc2>', '<urn:uuid:cce245f1-3d5a-4f8c-b5b5-72a701cb8225>']	open-ended	direct	long-search-query	distant-from-document	multi-aspect	novice	2025-05-13T01:23:33.086345	12	128	2030
28	I've noticed prices going up lately and want to protect my savings - how do options trading and futures contracts compare in terms of their risks and safety measures?	Both options and futures contracts involve significant risks and have safety measures in place. For options, brokers assign trading levels (1-5) based on risk, with level 1 being lowest risk. Investors must read standardized risk disclosure documents and meet certain requirements. With futures contracts, exchanges and brokers set 'margin' requirements as safety measures - investors must maintain minimum account balances and may receive 'margin calls' requiring additional deposits if prices move unfavorably. Brokers can liquidate positions if these requirements aren't met, sometimes without notice. In both cases, there's potential for significant losses - options investors can lose 100% of invested funds, while futures traders need high risk tolerance and ability to input additional funds to cover losses. Neither instrument guarantees protection against losses.	"[""An option is a contract that allows (but does not require) an investor to buy or sell an underlying instrument, such as a security, an ETF, or even an index, at a predetermined price for a certain period of time. Call and put options are made on the options market, which trades securities based contracts. Most of the time, holders choose to make their profits by trading (closing) their position. This means that option holders sell their options in the market and writers buy back their positions to close.\nOnly about 10% of options are exercised, 60% are traded (closed) and 30% expire worthless. Based on your answers, the broker generally assigns you an initial trading level based on the level of risk (usually 1 to 5, with 1 being the lowest risk and 5 being the highest). This is your key to performing certain types of options trading. Pamela de la Fuente is the editor of NerdWallet with more than 20 years of experience writing and editing in newspapers and corporations.\nOptions trading is the trading of instruments that entitle you to buy or sell a specific security on a specific date at a specific price. Options are among the most popular vehicles for traders, because their price can move fast, making (or losing) a lot of money quickly. Options strategies can range from fairly simple to very complex, with a variety of benefits and sometimes strange names. Options can be used to generate potential income on the shares you own and the shares you would like to own.\nOptions carry a high level of risk and are not suitable for all investors. Certain requirements must be met to trade options through Schwab. Investing involves risks, including loss of capital. Hedging and protection strategies generally involve additional costs and do not secure gains or guarantees against losses.\nWith long options, investors can lose 100% of invested funds. Hedged calls provide revenue, fall protection only to the extent of the premium received and limit the upside potential to the strike price plus the premium received. Spread trading must be done on a margin account. Read the options disclosure document entitled Characteristics and Risks of Standardized Options before considering any options transaction.\nSupporting documentation for any claim or statistical information is available upon request. While many brokers have eliminated fees for trading stocks or exchange-traded funds (ETFs), they still exist for options. These details will be documented in an options trading agreement that will be used to request approval from your prospective broker. Because option prices can be mathematically modeled with a model such as the Black-Scholes model, many of the risks associated with options can also be modeled and understood.\nThe risk you take as an option investor ultimately depends on your role in the contract (which side you're on) and your strategy, as there are multiple strategies you can implement using different combinations of options. European options are different from American options in that they can only be exercised at the end of their useful life, on their expiration date. This is why, when trading options with a broker, you usually see a disclaimer similar to the following. Options that expire before the estimated dates have values calculated based on the underlying prices from the estimated dates, as if the option expires on the estimated date.\nOptions trading is the way investors can speculate on the future direction of the stock market in general or individual securities, such as stocks or bonds. Like all the investment decisions you make, you need to have a clear idea of what you hope to achieve before trading options. Compared to opening a brokerage account to trade stocks, opening an options trading account requires larger amounts of capital. Options trading is when you buy or sell an underlying asset at a pre-traded price on a certain future date.\nBefore buying or selling options, investors should read the Standardized Options Characteristics and Risks booklet (PDF 17.8 MB), also known as the options disclosure document. Options quotes, technically called an option chain or matrix, contain a range of available strike prices. If used properly, options trading offers numerous advantages that stock and bond trading alone doesn't. Options trading strategies can become very complicated when advanced traders pair two or more call or put options with different strike prices or expiration dates."", ""- Outright Purchase Vs Futures Contract. Margin Calls\nTrading Commodities - Outright Purchase Vs Futures Contract. Margin Calls\nSuppose you've been reading the newspaper lately and seen the substantial rise in inflation over the last two years. You bet, along with many others, that this trend is likely to continue for the next two years. You decide to hedge your portfolio, and possibly pick up some profits, by investing in gold.\nUnfortunately, you don't have $58,000 to purchase 100 Troy ounces of gold at the current market price of $580. Instead you do what most speculators do, you buy a gold futures contract. Now instead of having to come up with $58,000 you only have to invest an initial amount of $2,900, 5% of the total.\nThat 5% is known as the (initial) margin. The exact percentages are set by the exchanges and brokerage firms on a daily basis, per individual commodities futures contracts. Exchanges monitor prices, volatility and many other factors to determine acceptable levels of risk and then set the margins accordingly. Minimums are set by the exchange, but brokerages will sometimes have slightly higher requirements.\nOutright Purchase Vs Futures Contract\nNow suppose the price of gold rises by $5 before the expiration of the contract. Excellent. You've made $5 per ounce x 100 ounces = $500 (excluding commissions, around $20). If you had purchased the gold outright you would have made the exact same amount of profit. But look at the difference between outright purchase and a futures contract in percentage terms.\n$500/$58000 x 100% = 0.86%, slightly less than 1%. On the other hand, $500/$2900 x 100% = 17.2%. That difference is the effect of something known as leverage. You invested only 5% of the total purchase price, but you still get 100% (ignoring commission) of the profits, not 5% of the profits.\nBut with the possibilty of reward comes the risk of loss. If the price had decreased $5 and never rose again before the contract expired, the result would have been a $500 loss instead. In order to protect themselves against the possibility that you won't be able to cover the amount at expiration, brokers may issue something known as a 'margin call'.\nAll potential profits and losses are calculated and settled on a daily basis. If the price drops below the minimum set by the broker (based on the exchange minimum), brokers will require their clients to deposit additional funds to bring the account back up to the level of the initial amount.\nHere's the kicker. They may or may not give you adequate notice and time to actually do that. Depending on the level of price volatility, the amount involved, and your relationship with them, brokers can (and sometimes do) liquidate your position without waiting for you.\nUnder normal circumstances, most brokers will give you notice and reasonable time to meet this 'maintenance margin', the amount required to bring your account up to the required level. But it's the trader's responsibility to monitor his or her positions and know the guidelines.\nBeyond bringing the account up to the previous level, it's possible you may have to come up with an even larger amount. Exchanges and/or brokers can and do raise (or lower) the minimums depending on current market conditions.\nFutures trading, particularly in the fast-paced, high risk world of commodities, isn't for everyone. A high tolerance for risk and the ability to input additional funds is necessary, along with the ability to withstand the losses.""]"	['<urn:uuid:3816c4c3-ec8c-49e1-bae2-7363df821ff6>', '<urn:uuid:765cbd90-b245-4a50-a859-6597632ab961>']	open-ended	with-premise	verbose-and-natural	similar-to-document	comparison	novice	2025-05-13T01:23:33.086345	29	123	1309
29	blood test dwi hospital after accident iv fluids affect results	When blood is taken from a DWI suspect who has received intravenous (IV) fluids after an accident, the test results can be inaccurate for two reasons. First, the increased volume of fluid in the circulatory system can change the blood alcohol level by lowering it. Second, if blood is drawn from the same extremity where the IV is located, the sample may be contaminated with the intravenous fluids. This is particularly relevant because it's very likely that paramedics will start an IV en route to the emergency room when there has been an injury to the person.	['Articles» Print This Page\nHow can you possibly win a drunk driving DWI case in Texas if you already gave a blood sample and your blood test result was a 0.08 g/dL or higher? Once you’ve had a blood test, is there any way to fight a Driving While Intoxicated conviction on your driving record?\nYes. Defenses to Texas DWI charges are available even after the police have a blood sample. The Law Offices of Michael Lowe can fight back.\nThere are many ways that Texas DWI defense attorney Mr. Lowe can discredit a “bad blood test.” The following is intended to be a summary of possible defenses to blood test results in Texas DWI cases:\n1. DEFENSES TO BLOOD SAMPLES\nTypically, a sample of blood is drawn and tested for alcohol content (technically, “BAC” or “blood alcohol content”) in only two DWI situations: (1) when requested by law enforcement officers in connection with a suspected drunk driving case, or (2) at the request of hospital medical personnel in connection with diagnosis and treatment of a patient who has been brought in after an accident.\nLegal Blood vs. Medical Blood in Driving While Intoxicated (DWI) Charge\nIn the former situation, the blood is sometimes referred to as “legal blood” and whole blood is typically collected; in the latter, it is sometimes called “medical blood” and is typically serum. The methods used to draw, prepare, track, and test legal blood often differ from those used for medical blood.\nThere is a strong argument that serum blood samples, due to problems with accuracy, show lack of trustworthiness as forensic evidence of the actual blood alcohol reading of a patient. Why?\nFirst, every individual has a different serum or whole-blood ratio that varies from 1.90 to 1.35. Furthermore, an arbitrary or average conversion ratio does not give an accurate result for a particular individual.\nSecond, when medical staffers draw “legal” blood, they follow procedures promulgated specifically for blood alcohol testing. Often, state regulations or statutes will specify a particular type of “kit” that has alcohol-free swabs in the package, as well as a series of labels, seals, and “packing” forms to assure that all necessary steps are taken to obtain a sample that can be admitted in evidence.\nChallenge the Admissibility of a “Legal” Blood Sample in DWI Case\nIn order to challenge the admissibility of a “legal” blood sample into evidence, the attorney must question the medical personnel involved to determine whether blood was draw and tested in compliance with state regulations. The defense attorney should also determine whether the regulations that were designed to ensure accuracy and reliability were followed.\nThese state regulations often specify that the vials shall contain anticoagulant and a preservative and be “stopped” or capped (or sealed) to prevent loss of contents through evaporation. These tamper-proof seals also will reveal any tampering with or compromise of the vials. Also, some jurisdictions mandate that all vials be marked with certain essential tracking information. Regulations or statutes may even specify the method of cleansing the subject’s skin before blood is drawn (i.e., with soap or Betadine, not alcohol).\nBlood vials that are to be used for legal samples generally contain an anticoagulant and a preservative, and generally have a gray-colored septum (indicating that vials have these two chemicals inside the vacuum tube). Other vials, used for various medical or laboratory purposes, may have red, yellow, purple or other colors signifying whether the vial contains certain chemicals.\nVials used in collecting medical blood samples may lack anticoagulant and preservatives. These vials generally have red septums, meaning that the blood collected is pure blood without anticoagulant or preservative added to the vial. A red-stoppered vial will become clotted (due to lack of anticoagulant) and will “deteriorate” much more quickly than those with gray stoppers, due to lack of a preservative.\nDWI Defense Attorney Approach\nThe attorney should ascertain the condition of the tube used, because an improperly capped or filled tube may result in an incorrect BAC (“blood alcohol content”) value. If the hospital (or police) sends blood to an outside private lab or the state crime laboratory for testing, counsel should check to see if the blood was continuously refrigerated, because the red-stoppered vials used by hospitals typically lack preservatives, and the alcohol content in the sample might actually increase if left at room temperature for an extended period of time. Jurors are shocked to learn that normal blood (with no alcohol in it at all) that decomposes actually creates ethyl alcohol as a by-product of the decomposition.\nDWI Defense Expert Witness Testimony\nThrough use of expert witnesses, the party opposing the admission of blood test evidence should attempt to bring out evidence of the unreliability of the blood test results in the current case. First and foremost, emphasize that every individual has a different serum or whole-blood ratio and the use of an arbitrary or average conversion ratio does not give accurate results for a particular individual. If an incorrect color cap was used on a vial, or if an anticoagulant or preservative was omitted, this may offer “reasonable doubt” about the integrity of the blood test result, particularly if gaps exist in the chain of custody.\nAlmost all blood collection vials have an expiration date. Manufacturers claim that this is only relevant to the warranty on the vacuum within the vial. Jurors may see the issue differently, if defense counsel analogizes to milk or bread sold after the expiration date.\nImportant Defense Questions Challenging Blood Samples – DWI Cross Examination:\n- Who requested this sample and for what purpose?\n- Did the suspect actually “consent” to the test?\n- What type of vial was the specimen collected in?\n- Was the vial properly capped, and with what color cap?\n- Did the vial have a preservative in it?\n- Did the vial have an anticoagulant in it?\n2. CONTAMINATION OF BLOOD SPECIMENS\nDefense attorneys should be aware and investigate the possibility that there may be a problem with contamination of the blood specimen. For example, the antiseptic used to cleanse the area may contain alcohol in amounts sufficient to produce measurable amounts of alcohol on a gas chromatograph. In emergency room settings, it is routine to use an alcohol-based antiseptic to clean the skin. It may also be routine to use Vacutainers and vials that are not in compliance with state regulations and statutes.\nAlcohol Contamination through Antiseptics and Skin Cleansers\nAny antiseptic that was provided in blood alcohol collection (BAC) kits (or otherwise used to draw blood specimens) should be analyzed by an independent laboratory to determine whether the antiseptic contains alcohol in sufficient amounts to produce measurable amounts of alcohol. While many jurisdictions have regulations and procedures governing blood testing, and despite the fact that most specifically prohibit using any alcohol solution as a skin cleanser, some of the swabs that are used in blood collection kits contain a substance known as benzalkonium chloride as an active ingredient. This compound contains approximately 2 percent (2%) or more ethanol concentration.\nAlcohol Contamination through Clinipad7™, PDI7™, and Triad 7 ™ towelettes\nEthanol in measurable quantities also has been found in three types of disinfectants: Clinipad7™ brand antiseptic towelette, PDI7™ brand towelette and Triad 7 ™ brand towelettes. Swabbing the skin with ethanol before taking a blood sample for measuring blood ethanol concentration may increase the apparent blood ethanol level by up to 0.018 percent (g/dL), even if the skin is allowed to dry before the sample is taken. When a “wet” site is used, and a “wet” swab is held over the site to withdraw the needle (which has a vacuum to draw in the blood from the suspect’s vein), the potential additive effect is much greater.\nDefense attorneys frequently succeed in convincing a jury that a percent of alcohol that was found in the blood sample was added to the test result because the person who withdrew the blood improperly used alcohol on the skin. As sample cross-examination could go as follows :\nImportant Defense Questions Challenging Blood Specimens – Sample DWI Cross Examination:\nQ. Doctor, suppose just one drop of alcohol was added to 100 drops of normal blood, what percentage of alcohol would you find?\nA. If the drop of alcohol were the same size as a drop of blood, I would expect to find 1.0 percent by volume, of 0.80 percent by weight.\nQ. What state of intoxication would you expect to find with 0.80 percent alcohol in the blood?\nA. That person would be dead.\nQ. Now, if only half a drop of alcohol was added to the 100-drop sample, what would you expect to find in the blood test?\nA. Approximately 0.40 percent by weight.\nQ. And what stage of intoxication would that percent correspond?\nA. Marked intoxication, probably unconsciousness.\nQ. Now suppose that one one-tenth drop of alcohol got into the 100 drops of blood, how would that affect the blood test?\nA. The reading would be approximately 0.08 percent.\nThe attorney then successfully argued to the jury that a mere fraction of a drop of extraneous alcohol that had been applied to the skin had been responsible for a possible miscarriage of justice, and his client was acquitted.\nContamination of Blood Taken from DWI Suspect involved in an Accident\nThe blood taken from a driving under the influence (DUI) suspect involved in an accident or car crash may also be contaminated if there was an intravenous fluid being administered at the time the blood was drawn. Why?\nThe specimen may be inaccurate either (1) because of the increased volume of fluid in the circulatory system changes the blood alcohol level (lowers it) or (2) the blood may have been drawn from the same extremity that the IV is in, and therefore the sample is contaminated with the intravenous fluids. If there was injury to the defendant, and the paramedics responded, it is very likely that an IV was started en route to the emergency room.\nContamination of Blood Taken from DWI Suspect When Equipment used is not Sterile\nContamination can also occur if the equipment used to draw the blood is not sterile. This may not pose as big a problem today as it did years ago before pre-wrapped disposable needles became the standard. Nevertheless, it still happens on occasion.\nIn some emergency room settings, nurse “set up” their trays and carts anticipating an emergency. That is, needles have been removed from the package and placed on the table along with other routinely used items so that these items will be readily available when an emergency comes in. If this is the case, a needle of syringe may have been left exposed for an undetermined amount of time and is no longer sterile. Check your state regulations concerning the propriety of using such items, in a non-sterile situation.\n3. DISTINGUISHING PLASMA, SERUM BLOOD, AND WHOLE BLOOD\nTo challenge a serum blood alcohol result, attorneys must know the differences between whole and serum (or plasma) blood readings. The admissibility of a blood alcohol test as evidence at trial will depend on the specific language of the relevant state statute and on the ability of the attorney opposing the test result to identify all the ways in which it is unreliable. In some states (i.e., Georgia), no published regulation controls how serum or plasma blood readings will be “adjusted” to reflect approximate whole blood alcohol levels.\nSerum Levels Can be 16-21% Higher Than Whole Blood BAC Levels\nA potential source of error involves reporting plasma or serum values that are assumed to be whole blood values. The defense attorney should try to discover whether whole blood, serum or plasma was analyzed. Some studies indicate that plasma and serum alcohol levels are as much as 16 to 21 percent higher than whole blood levels.\nAfter the blood is drawn, it is often centrifuged (spun in a mechanical device) and separated into its “solid” and “liquid” components for analysis. What is being analyzed by the toxicology lab may not be whole blood, but serum or plasma and, therefore, an analysis of plasma or serum will produce a higher percentage of alcohol than would actually be present in whole blood. Hence, the results must be mathematically “adjusted” or truncated to show the approximate true BAC.\nDistorted Readings – Serum vs. Whole Blood\nSome police departments and state forensic laboratories analyze whole blood. Many hospitals and clinical laboratories routinely analyze only serum. Evidence of a client’s blood alcohol level indicating a result of 0.10 percent (g/dL) BAC may, in fact, reflect a true BAC of 0.08 to 0.09 g/dL if serum was used.\nIn addition to the distorted reading, the testing may be challenged on the grounds that a serum alcohol concentration test is not a blood test for the purposes of the statute. Therefore, DWI defense attorneys should carefully review the chemical sobriety test statutes and definition sections of the state criminal code.\n4. IN VIVO AND IN VITRO VARIATION IN BLOOD TESTING\nDue to variations in body chemistry plus the chemical changes that the blood undergoes while in the glass ampoules, the DWI blood specimens may become unrepresentative of the person accused. The difference is due to what occurs in vivo (those which occur inside the body) and in vitro (those which occur while the blood is in glass ampoules). In vivo changes occur because of normal fluctuations in the blood from moment to moment after the blood’s ability to hold and release alcohol.\nBlood Specimen Neglected at the Lab\nIt is not an uncommon practice to let the blood specimen sit for days before analyzing it due to delay in getting it to the laboratory, or due to simple neglect. Because the carousels on most gas chromatographs hold fifty to 100 vials, many laboratories wait until a full carousel can be obtained before running the tests. Defense counsel may be surprised to learn how long a sample was not refrigerated.\nDecomposing Blood Sample Produces Alcohol\nBecause blood is an organic material, it will decompose as a result of enzyme activity and bacterial action. One of the results of this decomposition is that ethyl alcohol is created from the breakdown of the blood. This is sometimes referred to as “endogenous” alcohol production. In a sample originally containing no alcohol, decomposition can cause an alcohol content reading of 0.25 g/dL or even higher depending on the stage of decay.\nFailure to Add Sodium Fluoride or Potassium Oxalate\nUsually the specimen will be refrigerated to prevent degradation of the sample. However, refrigeration will only slow down the decomposition process, not prevent it. To stop this decaying of the blood and the resultant formation of alcohol, a preservative such as sodium fluoride solution should be added. Blood without sodium fluoride is reliable at normal room temperatures for about two days. Without sodium fluoride, the BAC may rise to a maximum concentration in about fifteen days and then fall.\nFailure to add sodium fluoride or potassium oxalate (and this is not uncommon) should provide DWI defense counsel with sufficient ammunition to discredit the state’s BAC test results, if not prevent their admission into evidence. In a criminal trial, raising reasonable doubt only requires that a sufficient factual basis for questioning the reliability and accuracy of the test results be raised by the defense.\n5. MEDICATION AND DISEASE\nCertain medications and the “disease process” can affect the level of BAC. Any medication that alters the rate of metabolism can affect blood alcohol levels.\nFor example, an article in the Canadian Society of Forensic Science Journal has reported that women taking oral contraceptives appear to eliminate ethanol significantly faster than women not taking oral contraceptives. Also, Candida albicans is a yeast-based medical problem that can cause alcohol to be created inside the vial, after being drawn from the subject’s arm.\nThe human body eliminates the amount of alcohol by oxidation of the “poison” (alcohol) in the liver. Like any other foreign compound, alcohol is broken down by enzymes in the liver and gradually reduced until 100 percent is eliminated.\nTherefore, any disease process affecting the liver, such as hepatitis, will impair results. Also, any condition that causes “extracellular” water retention (heart disease or many forms of high blood pressure or diabetes, for example) will alter results. Alcohol is “water miscible” meaning that it mixes easily with water. Alcohol also “migrates” to any water source, whether in the human body or in a test tube. Also, many women retain water on or around the time of menses. Therefore, defense counsel in the investigation of the case should obtain this information.\nArticle: Expunge Records in Texas']	['<urn:uuid:3b4051ff-3715-44d1-ab1f-7665f2346720>']	open-ended	with-premise	long-search-query	distant-from-document	single-doc	novice	2025-05-13T01:23:33.086345	10	97	2774
30	I heard geothermal systems are good at controlling temperature - what makes their compressor staging system special, and how do the different loop installations affect performance?	The compressor staging system uses two differently-sized compressors that work together to provide up to three stages of heating and cooling. For example, a unit might combine a two-ton and three-ton compressor, allowing efficient operation at lower capacity when loads are light, while still having full capacity available for peak demands. When loads are light, the smaller compressor operates very efficiently with oversized coils. As for loop installations, each type performs differently: open loops use constant-temperature groundwater requiring 2-3 gallons per minute per ton of capacity, while closed loops' performance varies based on ground temperature, soil conditions, and installation depth - vertical loops typically need 130-300 feet per ton of heat exchange, and horizontal loops can use multiple pipes or slinky coils to increase heat exchange per foot of trench.	"['Enviroteq manufactures compressor units to interface with air handlers made by air-to-air heat pump manufacturers. To date we have successfully utilized air handlers manufactured by Tempstar, Heil, Janitrol, Coleman, Luxaire, Nordyne, and others. Because all metering is accomplished in the GEO compressor unit, it is necessary to remove all restrictors or other metering devices from the air handler(s) before installation.\nUp to three stages of heating and cooling are available with our most advanced units. Utilizing two disparately sized compressors (a two-ton and a three-ton, or a three-ton and a four-ton compressor, for instance) we can provide capacities of from two to five nominal tons, and from three to seven nominal tons, from a single unit. This allows sizing of units for heating in northern climates without oversizing for cooling.\nWhen loads are light, the unit\'s smaller compressor operates extremely efficiently with coils sized for ten tons and provides the highest levels of humidity control during the cooling mode. As the load increases the system stages up to the larger compressor and the small one drops out. So, a home with a five ton heating load and a three ton cooling load, for instance, can operate nearly all winter with little resistance supplemental heat and most of the summer utilizing only stages one and two, but with stage three (five nominal tons) available for those rare sweltering days when three tons might not be enough.\nUp to three air handlers are utilized with our dual compressor GEO unit to achieve zoning (three zones). These air handlers are normally ""twinned"" or ""tripled"" so that all evaporation and condensing is accomplished in both DX coils simultaneously. The panels between the air handlers are removed so that either or both air handlers can operate. If only one zone calls for conditioning the blower for that zone operates delivering conditioned air only to that zone. A proprietary component and method for preventing backflow, developed by Sunteq, keeps return air from short-circuiting the DX coils. The air handlers can also be remoted ie. one can be installed in the basement and the other in the attic, utilizing zone valve assemblies provided by Enviroteq. But, the twinned air handler technology provides higher levels of operating efficiencies because of the substantive size of the total DX capacity when only a single compressor is functioning. Staging and zoning achieved by independent thermostats provides a level of control unmatched by other manufacturers.\nNo manufacturer goes to greater lengths to protect it\'s compressors. Enviroteq utilizes a number of expensive components and features - components not essential to satisfactory system operation - just to go to that extra effort to protect the compressors. Many of these components and features are included in the one way metering circuit described below.\nOne way metering circuit\nEnviroteq\'s one way high pressure liquid circuit provides the ideal way for metering refrigerant in both the heating and cooling modes. When warm liquid refrigerant returning from either DX Coil (acting as a condenser) enters the one way circuit it (1) passes through a heat exchanger in the suction line accumulator to flash off any unevaporated liquid in the accumulator, then (2) it goes to a receiver which regulates the volume of refrigerant in circulation, storing excess refrigerant when necessary, and (3) as it leaves the receiver it passes through a large filter/drier which constantly cleans and dries the liquid, and finally it is metered to the evaporator coil by a balanced port thermostatic expansion valve capable of adjusting refrigerant flow rates from two through seven nominal tons.\nSuction line accumulator with integral heat exchanger (assures that no liquid refrigerant can enter the compressor(s)\nLiquid receiver stores excess liquid refrigerant to balance the charge.\nLarge capacity liquid line filter/drier keeps refrigerant clean and dry.\nOil equalization tubes (dual compressor models) assures adequate lubrication for both compressors\nUnlike with bi-directional (two way) expansion valve metering, the one way circuit allows these additional functions to be added to a geothermal heat pump.\nAdditional compressor protection:\nOil equalization tubes (dual compressor models) assures oil return to both compressors and thus adequate lubrication for each compressor.\nAdditionally, there is a high pressure lockout switch (manual reset) installed on the hot gas discharge line and a low pressure switch (auto reset) on the suction line. Compressor short-cycle protection is provided by a time-delay on the control circuit which delay\'s on break so that the unit is usually ready to run on start-up.\nSuperior heat Extraction with Oversized heat exchangers:\nLarge capacity water-to-refrigerant heat exchangers provide for exceptionally efficient heat transfer and high evaporator and condenser efficiencies, as well as extremely low pressure drop for exceptionally high flow rates. When it comes to efficiency and performance the name of the game in geothermal heat pumps is heat extraction and heat dissipation capacity. While most manufacturers size their water-to-refrigerant heat exchangers to match the capacity of the compressor(s), the heat exchangers on Enviroteq\'s closed loop models are greatly oversized - 200% on average - for incredibly high heat extraction efficiency.\nThe chart below shows the number of and sizes of heat exchangers installed in the variious Enviroteq closed loop models and the per cent of oversizing.\n|A Model||B Nominal Capacity\n|C Heat Exchanger Rated Size\nC divided by B\nExceptionally high water flow rates\nMost of our competitor\'s larger sized geothermal heat pumps require two (expensive) circulator pumps in order to provide sufficient water flow for satisfactory operation. All of Enviroteq\'s larger closed loop models (4 nominal tons and up) have two heat exchangers installed so that there are four parallel water flow paths available for the water to follow (two in each heat exchanger). This results in very low pressure drop so that, when coupled with a Sunteq designed closed loop, flow rates on the order of 14 to 15 gallons per minute can be achieved with a single circulator.\nWhile this may add to the cost of an Enviroteq geothermal heat pump, the savings resulting from the need to purchase only a single circulator pump are significant - on the order of $300.\nIn addition, all Enviroteq geothermal closed loop models come with a 1¼"" copper male threaded adapter ready for circulator flanges as well as wiring in liquid tight conduit ready for connection to the circulator\'s terminal box. This reduces field wiring requirements and saves significant installation time. During the heating mode, the circulator runs on any call for heating. During the cooling mode the circulator is controlled.\nAn oversized desuperheating heat exchanger on the hot gas discharge line is provided as an option on all Enviroteq models. Desuperheating hot vapor provides virtually free domestic hot water during the cooling mode and low cost DHW during the heating mode. A thermostat provided by Enviroteq is mounted on the discharge line and brings on the DHW circulator (not provided) only when there is sufficient hot gas energy to be exchanged.\nCompu-Sun Energy Audit provides a comprehensive computerized heating and cooling load analysis for the consumer which estimates annual geothermal heating and cooling costs and compares them to the costs of any other type of system: Natural and L.P. Gas, Oil, Wood, Coal, Electric and Air-To-Air Heat Pumps.\nCompu-Trac Energy Use Tracking Program — a computerized tracking program that tells you how well your system is (or isn\'t) doing compared to projections.\nInnovative Geothermal Heat Pump Installation Techniques that are unique in the industry, such as:\n- Loops that operate only under pressure of gravity\n- An air-purging, debris-filtering, self-replenishing water storing reservoir that shows the homeowner - at a glance - when water needs to be added to the system.\nThe more you know about refrigeration, the better we look!', 'Geothermal heating systems use the earth as a heat source and heat sink. A series of pipes, commonly called a “loop,” connect the geothermal system’s heat pump to the earth. There are two basic types of loops: closed and open.\nBelow is a breakdown of the different types of geothermal systems:\nOpen Loop Systems\nThe simplest of geothermal heating systems, open loop systems have been used successfully for decades. Ground water is drawn from an aquifer through one well, passes through the heat pump’s heat exchanger, and is discharged to the same aquifer through a second well at a distance from the first.\nGenerally, two to three gallons per minute per ton of capacity are necessary for effective heat exchange.\nSince the temperature of ground water is nearly constant throughout the year, open loops are a popular option in areas where they are permitted.\nClosed Loop Systems\nWater (or a water and antifreeze solution) is circulated through a continuous buried pipe. The length of loop piping varies depending on ground temperature, thermal conductivity of the ground, soil moisture, and system design. Some heat pumps work well with larger inlet temperature variations, which allows marginally smaller loops.\nHorizontal Closed Loop\nGenerally the most cost-effective for small installations, horizontal closed loop works best particularly in new construction where sufficient land area is available.\nThis type of geothermal heat installation involves burying pipe in trenches dug with backhoes or chain trenchers. Up to six pipes, usually in parallel connections, are buried in each trench, with minimum separations of a foot between pipes and ten to fifteen feet between trenches.\nReady to start your Geothermal System?Find Pros\nVertical Closed Loops\nVertical closed loops are the preferred geothermal heat system for many situations. Vertical loops are used when the soil is too shallow for trenching and to minimize the disturbance to existing landscaping.\nFor vertical closed loop systems, a U-tube (and, rarely, two U-tubes) is installed in a well drilled 100 to 400 feet deep. Because conditions in the ground may vary greatly, loop lengths can range from 130 to 300 feet per ton of heat exchange. Multiple drill holes are required for most installations, where the pipes are generally joined in parallel or series-parallel configurations.\nIncreasingly, slinky loops–or overlapping coils of polyethylene pipe–are used to increase the heat exchange per foot of trench. But this does require more pipe per ton of capacity. The trench length decreases as the number of pipes in the trench increases, or as Slinky coil overlap increases.\nPond Closed Loops\nA pond closed loop is a special kind of closed loop system. Where there is a pond or stream that is deep enough and with enough flow, closed loop coils can be placed on the pond bottom. Fluid is pumped in the same manner as a conventional closed loop ground system where conditions are suitable. The economics of this geothermal heat method are very attractive, and no aquatic system impacts have been shown.\nWhat is the cost of geothermal energy, and heating and cooling system installation? Reference our True Cost Guide for average prices.']"	['<urn:uuid:9ef3b604-6143-459c-9ed5-c1cd4d74b380>', '<urn:uuid:bbe7fb4f-b858-4d1e-a774-b6b2898a3829>']	factoid	with-premise	verbose-and-natural	similar-to-document	multi-aspect	novice	2025-05-13T01:23:33.086345	26	130	1792
31	How close is the Thorn mining area to Alaska's capital city, and in which part of British Columbia is it located?	The Thorn Project is located in northwestern British Columbia, approximately 105 km ENE (east-northeast) from Juneau, Alaska.	['Thorn Project Summary\nBrixton’s 100% owned Thorn Project hosts a district scale Triassic to Cretaceous volcanoplutonic complex and related sedimentary units with several styles of mineralization related to porphyry and epithermal environments. Targets include sediment hosted gold, high-grade silver-gold-lead-zinc-bearing diatreme-breccia zones, high sulphidation gold-silver-copper breccia-veins and dissemination porphyry copper-gold-silver.\nThe 996 sq. km Thorn Project is located in the Sutlahine River area of northwestern British Columbia, Canada, approximately 105 km ENE from Juneau, AK.\nThorn is relatively accessible from Vancouver. A commercial flight to Whitehorse, Yukon departing in the morning allows for transfers to a small fixed wing aircraft which can land on the airstrip at the property by lunch time.\nBrixton Metals Corporation wholly owns the Thorn claim group and is subject to net smelter royalties (NSR) that range from 0% to 3.5% to some of the original property vendors. The NSRs have buy down clauses to 1.5%.\nThe earliest known work on the Thorn property was carried out by Kennco Explorations (Western) Limited in 1959 during a regional exploration program. Including work by Brixton a total 323 silts, 5,194 soils, 1,321 rocks samples were collected. Geophysical work included ground magnetics, IP, airborne VTEM, and magnetics. The total drilling to date is 21,327m within 155 holes property wide.\nBrixton carried out several exploration programs since it acquired the project in 2010 from Rimfire Minerals (now Kiska Metals, which was recently acquired by AuRico Metals).\nGeology hosts a district scale Triassic to Cretaceous volcano-plutonic complex with several styles of mineralization related to porphyry and epithermal environments. Targets include sediment hosted gold, high-grade silver-gold-lead-zinc-bearing diatreme-breccia zones, high-grade gold-silver-copper veins and porphyry copper-gold-silver.\nThe area northwest of the Sutlahine River is underlain by mafic volcanic and marine sedimentary rocks of the Triassic Stuhini Group and lesser coarse clastic strata of the Jurassic Laberge Group, which strike northwesterly and dip steeply to the northeast (Awmack, 2011). These are cut by quartz feldspar and feldspar porphyry intrusions that are generally elongated NW-SE (Chapman, 1991). These are attributed to the Thorn suite based on lithological similarities (Awmack, 2011). The area southeast of the Sutlahine River is more complex. Stuhini Group volcanic rocks comprise much of the area southwest of La Jaune Creek, with only a small amount of Stuhini Group clastic strata present immediately adjacent to the creek. Stuhini Group strata continue East of La Jaune Creek to the vicinity of the Outlaw Zone where they are overlain by 5-20 m of limestone and undifferentiated clastic strata, including a boulder conglomerate, all assigned to the Sinwa Formation (Simmons et al., 2005). The Sinwa Formation is overlain by the Laberge Group, which is locally represented by coarse clastic strata assigned to the Takwahoni Formation (Awmack, 2011). These strata form a moderately north plunging anticline. Several rhyodacite dykes intrude this sequence and have been dated at 168.1±0.7 Ma, leading to them being assigned to the Fourth of July suite (Simmons, 2005). Immediately northeast of La Jaune Creek is the 93.3±2.4 Ma (Mihalynuk et al., 2003) Thorn Stock quartz feldspar porphyry; the main host to mineralization at the Oban breccia, Taliker and Glenfiddich Zones. The Thorn Stock is nonconformably overlain by the dominantly felsic, subaerial Windy Table volcanic rocks that generally dip shallowly to the north. Several intrusive bodies assigned to the Cretaceous (Cirque Monzonite, Son of Cirque Stock, and Outlaw monzonite-granodiorite) are roughly located along the boundary between Cretaceous and older strata. Two new diorite plugs were mapped in 2016 in the Chivas area are of unknown age. A second unconformity known as the Redline, as further described below, is mapped related to the Outlaw and Chivas Zones.\nJeff Kyba and Joanne Nelson, Northwest Regional Geologist for the British Columbia government: ” BC Survey’s ‘red-line’ a game changer for explorers. Northern Miner, May 6, 2015.” http://www.northernminer.com/news/bcgs-presents-a-big-game-changer-for-explorers-in-nw-stikine/1003601910/?&er=NA#sthash.8QptB20p.dpuf\nKyba has suggested that the geological contact between Triassic age Stuhini rocks and younger Jurassic age rocks is an important marker for finding large-scale mineral deposits. Geologists Jeff Kyba and Joanne Nelson from the B.C. Geological Survey may have unlocked the secret to world-class porphyry- and intrusion-related gold-copper deposits in northwestern B.C. They’ve discovered that most of the major deposits in the region occur within 2 km of a regional stratigraphic contact.\nThe rocks on the property have been structurally well prepared for mineralizing fluids. All structures on the Thorn Property are interpreted by SRK to be the result of a single D1 event characterized by NE-SW oriented maximum shortening. Under such a regime the N-S dextral faults are the master faults with NNE-SSW set representing P (synthetic) shears, NE-SW set representing R (synthetic) shears, E-W representing P’ (antithetic) shears, and NW-SE set representing R’ (antithetic) shears. Synthetic shears have the same sense of motion as the master fault, whereas antithetic ones have the opposite sense. Outcrop scale representations of this fault geometry were observed on the property. The maximum extension direction is NW-SE, leading to tensional veins striking NE-SW as has been observed for many of the high sulphidation veins on the Thorn Property. However, these veins vary from NE-SW to NNE-SSW and field observations suggest that many of them are fault-fill veins. It is therefore interpreted that they formed as a combination of tensional veins and fault fill veins along the favourably oriented R and, to a lesser extent, P shears.']	['<urn:uuid:5b8004bf-d864-43fe-8ec5-edf1ef2f7dbf>']	factoid	direct	verbose-and-natural	distant-from-document	single-doc	novice	2025-05-13T01:23:33.086345	21	17	882
32	I'm doing some research and wondering, how did the bicycle help humanity during both natural disasters and wartimes? Can you tell me about any historical examples of bikes being used in tough situations?	Bicycles proved valuable in both natural disasters and military operations. During the 1816 'Year Without A Summer' caused by Mount Tambora's eruption, when crops failed due to cold weather, the lack of oats to feed horses led German inventor Karl Drais to create the Draisine, a precursor to the bicycle. In military applications, bicycles were crucial in several conflicts - they were used by the Japanese in their advance on Singapore, and were vital for supply lines on the Ho Chi Minh trail during the Vietnam War. The US also experimented with military bicycles when Lieutenant Moss led the 25th US Infantry Bicycle Corps on a 2,800-mile tour in 1896 to test their military potential.	['If you think that spring has been slow to arrive, just imagine what it was like during the summer of 1816, known as “The Year Without A Summer.” The root cause of the winter-like weather was a volcanic eruption that took place 200 years ago this month.\nMount Tambora, a 9,500-foot volcanic mountain in Indonesia, had been dormant for hundreds of years before slowly rumbling to life in 1812. This culminated in a series of massive explosive eruptions that took place between April 5 and 11.\nThe volcano’s biggest explosion on April 10 is believed to be the largest single explosion in recorded history, heard more than 1,600 miles away and producing tsunamis as large as 13 feet. All vegetation on Sumbawa, the island that Tambora sits on, was destroyed, and an estimated 4,600 people were killed as a result of the eruption, with as many as 50,000 people dying from hunger and disease in the following year.\nThe impacts of Mount Tambora’s eruption would be felt worldwide. The ash and sulfur dioxide eruption column reached as high as 27 miles above the surface, allowing the particles to travel around the world. As a result, weather conditions were highly unusual over the next year or so.\nThe ash caused a “dry fog” for most of the summer of 1815, producing brilliant sunrises and sunsets but also dimmed the sunlight to the extent that sunspots could be seen with the naked eye. Conditions got even worse in 1816. With dimmed sunlight, colder temperatures became the norm. A frost killed most crops across upstate New York and northern New England. Then, on June 6, snow fell in Albany, N.Y., and as much as a foot of snow accumulated in Quebec City, Canada.\nWhere Did Summer Go?\nThe summer wasn’t completely missing. In fact, short periods of temperatures in the 80s and 90s were reported in July and August, as would be normal for that time of year. However, it was often followed by dramatic temperature swings dropping temperatures near or even below freezing. It was so cold that river and lake ice was reported in parts of northern Pennsylvania! Thanks to an ongoing series of frosts, the 1816 harvest was a complete failure, causing corn and grain prices to jump by 8 to 10 times.\nConditions were no better in the rest of the world. Harvests failed across Europe, causing the century’s worst famine in Great Britain and Germany. Major storms wreaked havoc on Europe, causing severe flooding on many of the continent’s rivers. Overall, an estimated 200,000 people were killed between the famine and floods. In China, the summer rice crop failed thanks to freezes and snow was reported as far south as Taiwan. In all, it is estimated that average surface temperatures dropped by at least 2 degrees worldwide.\nThe cultural impact of Tambora’s eruption wasn’t all bad. Americans set out toward the Mississippi Valley in search of richer soil, settling across the Heartland. Likewise, the crop failure meant that there weren’t enough oats for the horses, causing German inventor Karl Drais to create horseless transportation with the precursor to the bicycle.\nPerhaps the most interesting cultural gift provided by that cold, wet summer came to us from Europe. Writers Mary Shelley, John William Polidori and Lord Byron were forced to remain indoors that summer due to “incessant rainfall,” having a contest to see who could write the scariest story. Polidori wrote The Vampyre, the first short story about vampires, and believed to be one of the inspirations for Bram Stoker’s Dracula. Perhaps you’ve heard of the story written by Mary Shelley, Frankenstein; or, The Modern Prometheus?\nAsh concentrations slowly diminished during the next few years, with unusual weather patterns lasting into the early 1820s. While no volcanic eruption has reached Tambora’s magnitude since, Krakatoa in 1883 and Pinatubo in 1991, both caused substantial temperature drops a year later across the Northern Hemisphere.\nWe want to thank WeatherBug Meteorologist Andrew Rosenthal for researching and writing this article!', 'There is not much left that cannot be done on two wheels. On the last page of this opulent book is a photograph of the Young Theatre of Riga performing Brecht’s Fear and Misery in the Third Reich on bicycles. We are not told what the critics said about this event. Was it an experience to lift the soul on wings, a mind-blowing epiphany? Or was it in the same class as a file of messenger boys delivering pizzas on unicycles? The author, Pryor Dodge, withholds his own opinion. He is introduced to us, not just as a bicycle buff, but as ‘a classical musician and aspiring Argentine tango dancer’, and should therefore be better equipped than some to interpret wheeled burlesque on the Baltic.\nIt was roughly two centuries ago that the human race, chafing at the Creator’s negligence in not fitting it with wheels, made tentative steps to repair the deficiency. There were stagecoaches in plenty and steam trains in prospect, but what was needed was a personal accelerator to speed up individual travel. Among the first devices was the Draisine, invented in 1817 by a German aristocrat, an elementary hobby-horse which was hard on the leg muscles and even harder on footwear. If any Scotsman ever used one on the way to London his feat has gone unrecorded. But the Draisine led to an important discovery: namely, that almost anyone, even an imbecile, was capable of balancing upright on two wheels, given a reasonable momentum. People clustered to watch this marvel, as they had once thronged to see the young Benjamin Franklin, a proselytiser for the virtues of swimming, paring his toenails as he lay on his back in mid-Thames. There seemed no limit to man’s ability to defy the natural laws. It was not essential to limit the number of wheels to two, but tricycles and quadricycles were heavy going. A fetching illustration by George Cruikshank, dated 1819, shows two young women, each mounted on a Draisine-type Female Accelerator, taking grateful advantage of the breast-rests fitted above the handlebars to ease the strain of leaning forward while on the march. Such supports (if they existed) were rendered unnecessary by the use of pedal power in the later velocipede. Nervous neophytes who doubted their ability to balance two-wheelers were taught by instructors in riding halls or on rinks; a drawing shows riders spilling freely in the Velocipede Riding School in New York (a foretaste of those electrifying velodrome pile-ups occasionally seen on television).\nThe bicycle, as its advocates pointed out, did not need to be fed oats. It was cheaper than a horse and perhaps cheaper than a pair of dogs (we have forgotten that in the early 19th century canine traction had spread to Britain from Holland, where every fat Dutchman was pulled by panting dogs, and survived here until 1854, when it was banned). What held back the advance of the bicycle for most of the century was what also held back the roller-skate: the bad state of the roads. The first hobby-horse riders chose to use the foot pavements, thus sparking the sort of ‘pavement rage’ which increasingly has its outbursts today. At least the riders of the high bicycle, or ‘penny-farthing’, knew better than to ride their towering contraptions through the closely-packed ranks of shoppers. Many have wondered why the penny-farthing was ever invented. Dodge says that it was all a matter of physics: each revolution of the big wheel translated itself into a greater distance covered, while the flywheel effect enabled the driver to keep up momentum. Colonel Albert Pope, sometimes called the founder of the American cycle industry, could hardly believe that anyone but an acrobat could control a high-wheeler. Mounting his horse, he took to accompanying an English high-cycling friend around Boston and was outdistanced every time. There is a picture strip showing the author of this book mounting a penny-farthing, achieving legover after a skilful ascent from the rear; which is how elderly gentlemen mounted ordinary bicycles a couple of generations ago, using a step protruding from the rear hub. Another picture shows a daredevil riding a high-wheeler, small wheel first, down the steps of the Capitol in Washington. Was there not a Huxley who traversed the Alps on such a mount? (It was not Aldous, that’s for sure.) High-wheelers suffered as much as any kind of bicycle from bad road surfaces, though there were strong men who rode them from Land’s End to John o’ Groats without suffering too many ‘headers’. As everyone knows, the golden age of cycling arrived in the 1890s, with the invention of the inflatable tyre. This was a golden age, too, for the admen, who readily saw that the way to symbolise pneumatic bliss was to identify the product with an ample, bare-bosomed female – in classical mode to allay criticism. Such pictures did more for the cause, at a guess, than the full-length portrait of the courtesan Blanche d’Antigny in her species of bloomer suit. Cycling girls were soon to be seen on posters everywhere, promoting all kinds of goods. Jerome K. Jerome noted that they were always portrayed being ‘wafted along by unseen heavenly powers’.\nIt is a much-told tale how the bicycle helped to emancipate women; less familiar is the claim that it helped to stamp out endogamy in our midst. At first men were not supposed to go riding with women other than their wives and sisters. In 1896, we read, the Chaperon Cyclists’ Association advertised in the Queen, offering to ‘provide gentlewomen of good social position to conduct ladies on bicycle excursions and tours’. Now, if Alan Bennett had picked on a bicycle outing like that ... Young women seized every opportunity to evade the rules, including the suggested speed limit of seven miles an hour. They were notyet free enough to put up at the cheap lodgings recommended by the Cyclists Touring Club (below five shillings, as one remembers from the Twenties, there was a possibility of finding a late-arrived traveller asleep on the other side of the bed in the morning). It was not long, we learn, before piano-makers were complaining of a slump, thanks to wheel-mad women abandoning the keyboard. Cigar manufacturers feared ruin because men were spending so much time in the open air. There was an anxious period when cycling was in danger of being tainted by socialism, as in the working-class Clarion Club, but it was left to Germany to turn a craze into a solidarity movement, with groups calling themselves ‘Enlightenment Patrols of Social Democracy’.\nThe great events of history take on a bizarre aspect in this book. Who would have supposed that bicycles played such a part in the gold rushes in Australia and the Yukon? Why did Robert Service and his fellow poets not tell us about the rigours of the 400-mile cycle trail from Dawson City to Whitehorse? Or of the thousand-mile ride from Dawson City to Nome in Alaska undertaken by Edward Jesson, who narrowly avoided cycling on to a stretch of ice which was heading out to sea? Again, who would have suspected that the Tour de France was an offshoot of the Dreyfus case? It is a complex story, involving fierce rivalry between two publishers of cycling magazines on opposite sides of the Dreyfus divide. One of them, in a desperate throw to ruin the other, launched the Tour de France. His rival refused to print a word about it. All very French.\nCycle sport has always worn a pleasing air of lunacy. As early as 1875 a six-day bicycle race was held in the Agricultural Hall at Newcastle-on-Tyne, with the winner putting in 18 hours a day. Such events were mounted by bookmakers and publicans, who might otherwise have been organising ratting pits. Ernest Hemingway, it seems, became so excited by the six-day bicycle races in post-1918 Paris that he could not bring himself to read the proofs of A Farewell to Arms. The Tour de France went from one excess to another. It is puzzling to read that the competitors who ascended the Pyrenees in 1910 lacked sufficient braking power for the descent and ‘had to rent bunches of twigs on top of the mountain to slow their descent’. Yes, but how? Were the twigs held against the wheel, or (God forbid) thrust through the spokes? If they were used as a drag they would have had to be in heavy bundles rather than bunches. Dodge leaves us in the dark. He does tell us that at this period dérailleurs were not in use. (Dérailleurs? Oh, of course, variable speed gears.) Today the field of sport has been vastly extended thanks to the versatile mountain bike and soon no natural feature will go undesecrated. It seems there is an extinct volcano 10,000 feet high on the Pacific island of Maui down which cyclists plunge for 38 miles. Perhaps the lane discipline there is better than it is on the motor road up Etna, where the standard of driving is deplorable.\nSooner or later, most new inventions are adapted for the purposes of war. Dodge offers us a splendid photograph of Lieutenant Moss leading the 25th US Infantry Bicycle Corps on its 2800-mile tour out of Montana in 1896, an odyssey intended to show the suitability of bicycles for military use. The sight of black soldiers, thus mounted, ‘disturbed residents of Missouri, as they were reminded of the black members of the Union Army during the Civil War’. This, however, was not the reason Lieutenant Moss and his riders were ordered to return from St Louis by rail; someone had decided that the bicycle had no real future in war. The French and the British persevered with the idea of cycling troops. Dodge seems not to have come across those French military manuals which described, among other exercises, how wheeled battalions should deploy themselves to face a cavalry charge. The machines were to be laid on their sides and the rear wheels spun as fast as possible, a spectacle calculated to strike the horses with terror, especially if there happened to be sunlight playing on the defiant scene. Other cycles were to be strewn freely so that any charger advancing that far would trap its hooves in spokes. It is a great pity that Lady Butler was never able to do justice on canvas to a battle-scene like this. As it turned out, the two world wars offered small scope for wheelmen, but Dodge does well to note the part played by the bicycle in the Japanese advance on Singapore and by the well-organised supply lines on the Ho Chi Minh trail in the Vietnam War.\nIt may well be that the real heyday of the bicycle will come only when the world’s fossil fuels run down. ‘In Asia alone,’ Dodge writes, ‘bicycles transport more people than do all of the world’s automobiles,’ and China ‘produces more than 40 per cent of the nearly one hundred million bicycles built every year’. Yet China is now rushing to embrace the motor-car and its multiple-lane cycle tracks are already being invaded by lanes for cars. How much longer will the Chinese bride be content to demand only ‘the three things that go round – a watch, a bicycle and a sewing-machine’? The attitude to bicycles varies greatly round the Pacific Rim. From the streets of Djakarta 50,000 despised tricycles were tossed into the sea to make a reef for fishes (which is what America has been doing, more sensibly, with old cars). If that is bad news, the good news is that fishermen salvaged many of the machines and sold them cheap. Whether human traction is acceptable or degrading depends on circumstances, but in the Third World bicycles remain an indispensable means of shifting goods which would otherwise have to be piled up on women’s heads. Whatever Gloria Steinem may have said, a fish may well need a bicycle. According to Dodge, more than 95 per cent of American bicycles are now of the mountain variety; good news for a nation which is in urgent need of exercise, bad news for the landscape.\nThe Bicycle is full of delights but is not always reader-friendly. No one should have to read a page of small type superimposed on a nest of radiating spokes, or be compelled to reach for a magnifying glass to decipher page numbers little bigger than microdots. The book is designed and typeset by ‘agence comme ça, Paris’, whose layout could best be described as comme ci, comme ça. There are polar expanses of white space everywhere, yet posters by Toulousecautrec and his distinguished contemporares are reduced to the size of postage-stamps, while pieces of minor machinery or ornamentation are pictured across two pages. Yet it still adds up to what used to be called I wonder book. And, to be fair, close-ups of clean, shining mechanism can have an elegance all their own, so different from the filthy stretched chains and worn sprockets of far-off memory, and the faulty free-wheel hub which had to be dipped in the river to reactivate the stuck ratchets. The pictures of cyclists performing advanced stunts will be seen as deeply humbling by those of us who could never achieve a decent ‘flying angel’; one luckless rider is shown plunging from a wall of death’ into a den of lions. In our day we were adept at carrying siblings standing on the rear step, a fitting long since anished, and some of us contrived to cling with one hand to the backs of lorries, a parasitical practice nowadays adopted by skateboarders and roller-blade enthusiasts, which unflamed other road-users (including the sorry-driver) and was highly dangerous even in those slower times. There should perhaps have been a picture in this book of Leandro Basseto, who according to the Guinness Book of Records achieved ‘the longest bicycle wheelie’ at Parana, Brazil in 1990: ten hours, to minutes, eight seconds. In those distant days we managed only one second.']	['<urn:uuid:4e27b5da-bf76-473f-b535-badc786fbace>', '<urn:uuid:c6ca1b20-5e3a-456c-912d-233b31312d3e>']	factoid	with-premise	verbose-and-natural	distant-from-document	multi-aspect	novice	2025-05-13T01:23:33.086345	33	115	3005
33	constantine roman coin discovered mintmark details	A Roman coin was found showing Emperor Constantine I on the obverse and the Genius of the Roman people on the reverse. The coin has a mintmark PLN, indicating it was made in the first workshop of the mint at Londinium (London) in the summer of AD307.	"[""It’s seven weeks until this year’s excavation team lands in Somerset.\nIn Newcastle all sorts of behind the scenes preparations are happening. The minibus is booked, lists of equipment are being drawn up and we’re all thinking hard about exactly where we should dig this year and why.\nOur grant from Brympton Parish council has also arrived (a big thank you to everyone in Brympton for their support).\nIn week or so the students (Adam, Eleanor, Dave, Georgia, Johanna, Kevin, Lucy and Danni) will have their final briefing and be issued with kit lists. Some of them are currently thinking about exams so I expect the dig seems a long way off.\nEvery field in the land has a name and many of these names are recorded on Victorian Tithe Maps. Copies of these documents for Somerset are held by the Somerset Records Office. As part of the project we’ve been looking at the field names recorded on the Tithe Maps for Lufton and the surrounding parishes.\nThe names given to fields by people who worked them in the past often reflect something important about those pieces of land. They might tell us who owned the land (Ball’s Mead), they might describe the piece of land (Three Cornered Field), or they might tell us about the history of the piece of land. Chester would indicate a Roman site, Blacklands a site that might have been occupied intensely in the past. So far our work hasn’t turned up any of these names but there are an interesting group of fields over in Odcombe.\nJust north of the village of Odcombe are a series of fields with ‘Englands’ names. This name is likely to derive from inland and this term is often associated with parts of early medieval estates that were under direct lordly control. Two recent pieces of archaeological fieldwork on fields with ‘Englands’ names in Charleton Horethorne and Sparkford have demonstrated that both of these sites were occupied in antiquity. The fields in Odcombe might also contain similar evidence.\nEnglands Field Names near Odcombe © The Lufton Project and Andrew Agate\nA short article on this discovery will be published in the journal of the Medieval Settlement Research Group.\nWe completed the survey of Hungerford and realised that the settlement continued further to the south. So we carried on surveying and in 2010 and 2011 covered two smaller fields known as Danscombe and Mr Unwin’s Field.\nGeophysics of Danscombe © GeoFlo and The Lufton Project\nDanscombe was particularly interesting because just before the magnetometer survey was carried out the landowner manured the field with ‘green waste‘. This is the composted hedge clippings and other stuff collected from householders by the council. Theoretically it shouldn’t have an impact on the survey but at Danscombe we discovered that the ‘green waste’ had a lot of many metal contaminants. These had a negative impact on the survey (compare image above with the one below) and this problem is currently being studied by Alissa, an MA student at Newcastle.\nFor all of the problems caused by the ‘green waste’ we could see that the ancient settlement continued into this field. The big line running across the image is a modern pipeline (A).\nThe Mr Unwin’s Field (to the east: below) was also surveyed and contained further anomalies associated with the settlement (A). We also identified a round anomaly (B) that we thought might be the remains of a roundhouse or barrow. This is where we decided to dig in 2012.\nGeophysics of Mr Unwin's Field © GeoFlo and The Lufton Project\nExcavation is a small part of the archaeological process. We dig for a little while but spend much longer trying to figure out what our discoveries mean. Finds analysis is a big part of the post-excavation work. Our flint assemblage was looked at by Dr Rob Young and this post is based on his analysis.\nFlints were used by prehistoric peoples for tools. They worked (or knapped) the flint to produce edges that were sharper than surgical steel. A wide variety of prehistoric stone tools are known but the object illustrated below is a heavy and thick end scraper of Neolithic or Bronze Age date. It is roughly contemporary with the ring-ditch we dug in 2012.\nFlint scraper of Neolithic/Bronze Age date © Rob Young and the Lufton Project\nThe third and final week saw the trench extended and a lot of work to investigate the ditches and sort some modern features (including a septic tank outfall pipe) from the ancient features.\nExcavating in Week 3 © The Lufton Project\nAs archaeological excavation is destruction it was important that everything was properly recorded. A lot of drawings, photographs and paperwork were completed.\nDanni Recording © The Lufton Project\nTowards the end of the excavation Mr Unwin (the landowner) came to look at our trench, He was a very genial and tolerant host and the project is very grateful to Mr Unwin and his family for the support they are giving to this work.\nJames shows Mr Norman Unwin the trench © The Lufton Project\nAt the end of the week we had some help from Jamie and Carole Pullen. They lent us a digger and driver to backfill our trench! Then we put the turf back and headed back to Newcastle.\nBackfilling the trench © The Lufton Project\nOver the weekend we talked to lots of visitors at the open day. Then on Sunday our prayers were answered. The heavens opened and it rained.\nJames surveying a wet trench © The Lufton Project\nSome water in the soil meant that we could see the archaeology and that the digging was a bit easier.\nThe ring-ditch is revealed as we begin to define and excavate features © The Lufton Project\nWe worked hard to define and begin to excavate the features. We were rewarded with some tiny pieces of Early to Middle Bronze Age pottery from the ring ditch and some Iron Age pottery from the big ditch.\nKristjan and Fraser working hard © The Lufton Project\nWe made good progress in Week 2. The water bowser that seemed so important to have in the dryness of Week 1 was a bit redundant…\nWe went into Week 3 feeling pretty happy with what we’d discovered.\nIn 2012 we carried out a three week excavation to investigate geophysical anomalies identified in Mr Unwin’s Field. One of these anomalies was circular (B on the graphic below) and such features are usually termed ‘ring-ditches‘, the other was a linear anomaly that was likely to be a ditch (A on the graphic below). The excavation was designed to work out whether the ring-ditch was a part of a burial mound or a prehistoric house and whether it came before or after the ditch.\nGeophysics of Mr Unwin's Field © GeoFlo and The Lufton Project\nThe trench was laid out as a 10m x 10m square and excavated by hand. We started by removing the turf and then, in what was about the only hot and dry week of the Summer, we started excavating.\nCutting the turf © The Lufton Project\nWe removed ploughsoil and subsoil to a depth of about 0.4m. If a cubic metre of sand weighs about a tonne, we shifted about 40tonnes of spoil by hand! At 40cm deep we were on top of a layer archaeologists call ‘the natural’. This is a geological deposit (in our case a nasty clay) that pre-dates all human activity. Cut into the natural were some features. These included the ditch and arc of the ring-ditch we were looking for. These features were filled with a slightly darker and moister sediment than the surrounding natural and are visible in the following photos as dark stains.\nNewcastle Students Kate and Ellie along with local volunteers Pete and Robin have found the ring ditch (visible as a dark arc in the deepest part of the trench). © The Lufton Project\nThe week was really dry and by the end of it the clay had baked hard. The dried out features had all but disappeared from view. We prayed for rain…\nBrown, brown and brown. The trench has dried out and the archaeology's invisible © The Lufton Project\nWe’re pleased to announce that the big army tent has arrived and been tested by our friends in SSARG.\nThe Tent Erectors from SSARG © The Lufton Project\nLast year we didn’t have much shelter on site (well, we had a couple of gazebos that couldn’t cope with a light breeze). So the tent is a welcome addition to the project. It’ll be shelter on rainy days (many of those I expect), shade on hot days (not many of those!) and a place to keep our kit and most importantly do the paperwork.\nIt’s all a reminder that the excavation is coming soon…\nFollowing the geophysical survey of the area around the villa we took a decision to survey the big field to the south. This piece of land is known as ‘Hungerford’, a fieldname that usually denotes poor land. In the nineteenth century this big field was a number of smaller fields some of which were named ‘Little’ and ‘Lower Danscombe’.\nThe geophysical survey was undertaken between 2009 and 2011 and revealed an astonishing archaeological landscape. It is of many phases but includes a large and almost circular enclosure and a settlement with trackways and enclosures. This ancient settlement was a new discovery. We had no idea it was there and it came as quite a surprise.\nThe settlement probably dates to the Iron Age and Roman periods. Some of the geophysical anomalies are likely to be contemporary with the villa – it may be where the agricultural workers and tenants lived.\nGeophysics of Hungerford © GeoFlo and the Lufton Project\nThe small-scale metal detecting survey recovered a small number of Roman coins as well as the coin of Henry VIII.\nThis is one of the Roman coins. The obverse (heads) shows the Emperor Constantine I. He was elevated to the imperial throne at Eburacum (York), became the first Christian Roman emperor and established Constantinople (Istanbul). The reverse shows the Genius of the Roman people (surrounded by the legend GENIO POP ROM). At the bottom of the coin (in the exergue) is a mintmark PLN, which tells us that the first (Prima) ‘workshop’ of the mint at Londinium (London) made this coin in the summer of AD307(RICVI (London), 88).\nThis coin could have been dropped by one of the villa’s inhabitants!\nObverse: head of Constantine, Reverse: GENIO POP ROM // PLN © The Lufton Project© The Lufton Project""]"	['<urn:uuid:da0c588c-3e20-49a8-ad78-454c7cd0b24e>']	factoid	with-premise	short-search-query	distant-from-document	single-doc	expert	2025-05-13T01:23:33.086345	6	47	1775
34	what are benefits having same drupal site installed multiple places	A Drupal site rarely gets installed in only one place - typically there are development, staging, and production environments, plus multiple developers' local environments. The explicit purpose of an install profile is to make it easy to install the same site in many places, which makes things much easier for individual developers and teams.	"['Drupal core comes with a built-in structure called an installation profile. An install profile is a specific set of features and configurations that get built when the site is installed. Drupal has almost always had some variety of install profile, but with Drupal 7 they became a whole lot easier to create and understand. Due to this, install profiles are really good ways of creating products based on Drupal, and many companies have put significant resources into creating their own install profiles to be used in specific product domains.\nEvery site is an install profile\nAlthough install profiles are a good vehicle for productizing Drupal, it can actually be incredibly useful to integrate the install profile concept into the ordinary development workflow of all your Drupal sites.\nSeveral years ago, James Walker wrote a probing article that suggested that all Drupal sites can be install profiles. In it, he points out that it makes sense to make every site an installation profile because:\n- Every Drupal site actually contains a relatively small amount of code that\'s specific to just that site: a theme, custom code, and a list of contrib modules and libraries. An install profile can encapsulate all of these components.\nA Drupal site rarely gets installed in only one place. At minimum you\'ll usually have a development, staging, and production environment, in addition to multiple developers\' local environments. The explicit purpose of an install profile is to make it easy to install the same site in many places.\nBeing able to reliably install and replicate your site will actually make things much easier - whether it\'s just you or your whole team.\nDrush make works well to build sites when given a list of contrib modules and libraries to set up, meaning that there are already advanced tools to help work with the install profile framework.\n- By splitting out all unique components and removing Drupal core and contributed code from source control for each site, site repositories are smaller and more manageable.\nWhen working on a lot of Drupal sites, and especially when working with a team of developers, these benefits can save massive amounts of time and help to focus efforts on just the important and unique parts of the site you\'re building.\nInheritance is key\nOne of the keys to why the install profile structure is so useful is because baked into its fabric is the concept of inheritance. A Drupal site can be installed from an install profile, inherit all of its code and configuration, and still have site specific code and configuration located in the /sites directory of the codebase. This is why install profiles work well for products, because it still affords site builders their own place to add code to the site in addition to what comes by default from the profile.\nHowever, while this works well for install profiles as products, it doesn\'t maximize the utility out of making every site an install profile.\n- Each site still only has one parent-child inheritance relationship, meaning that you can\'t have multiple tiers of inheritance, which limits reusability.\n- Individual sites still live in /sites and this makes it much harder to keep site repositories small and manageable, limited to only the code that\'s unique to the site.\nTo really take advantage of all the benefits of using install profiles, we need to solve this by constructing an inheritable profile architecture.\nInheritable profile architecture\nImagine that you have a client who you\'re building one site for, but who will probably want you to build them several additional sites down the road. Imagine that you also have many other clients and have worked with Drupal for a long period of time. You probably have your preferred way of configuring Drupal with your preferred list of contrib modules that allow you to best satisfy the needs of your clients. Each of your clients is different, but most of them still share a large set of common needs, which means you end up installing the same modules and setting the same configurations over and over again.\nNow imagine a scenario where you had a multi-tiered setup in which you could:\n- create a base install profile that has all the common features and configurations across all your clients\n- create a client specific base install profile that contains all the common features and configurations across all of one particular client\'s sites\n- create a site specific install profile that contains all the common features and configurations for just one of your client\'s specific sites\n- layer each of these install profiles on top of each other so the children inherit the parent features and configurations all the way up the chain of profiles\nThis is an inheritable profile architecture. In your /profiles directory you have several install profiles and each defines its own ""base profile"" much like subthemes define their own base themes. You can then chain these profiles together, leading to the ability to create layers of install profiles, each catering to a specific scope of responsibility. Each install profile is tracked in Git under its own repository and contains only the components pertinent to its area of concern.\nPioneered by our Product and Development Lead, Aaron McGowan, we have been working on a project called ImageX InstallKit, which is an implementation of the inheritable profile architecture. InstallKit is the base install profile that all of our Drupal sites are built upon, and each specific site itself is an install profile as well. This means that each of our sites are composed of at least two install profiles inheriting from each other, and often times more.\nWhile the inheritable profile architecture works conceptually, it does not work well with Drupal core out-of-the-box. Individual profiles are not allowed to inherit from each other and dependencies are unable to be nested multiple layers deep, as the inheritable profile architecture necessitates. For this reason, a lot of our work on InstallKit has been dedicated to developing the critical changes necessary to enable this functionality, and to adding utilities and hooks to strengthen the installation process (especially when installing a multi-layered install profile site).\nWhy this architecture?\nReusable and generic code\nIt\'s not hard to grasp the importance of reusable and generic code. An inheritable profile architecture dictates that all profiles be built on top of each other, and that ideally a new profile is created for every site. This means that during every project and every site build, part of the development process is determining if a component can be generic, and placing it in the appropriate profile in the hierarchy of profiles if it can be used in more than one site.\nIn principle, organizing development efforts around re-usable and generic components does not specifically require an inheritable profile architecture. Just simply breaking out individual components into individual repositories and including them in a project also achieves this aim. However, this requires the use of Git submodules or Git subtrees, which are notoriously difficult and painful to work with, making this undesirable as a foundational building block of an architecture.\nA profile properly encapsulates everything unique about that individual Drupal site. An individual Drupal site is composed of a list of contrib modules, feature and custom modules, a list of themes, custom themes, and a list of libraries. A profile contains the ability to house all of these items, and as a result it makes sense that an individual Drupal site doesn\'t need to be considered more than this.\nIn practical terms, this gives us the opportunity to change what is tracked in version control. Instead of tracking the entire site including Drupal core, when using a profile architecture we only need to track the profile in question. By using Drush make to build all non-custom components, this dramatically reduces the size of the repository and allows it to track only the components that are actually being developed. This cuts down on the cognitive load of organizing and navigating through the entire codebase and introduces a proper separation of concerns. By having multiple profiles built on top of one another, each encapsulating a unique layer of a site, the structure of the architecture itself communicates where components should be placed and what level of reusability and ""genericization"" those components should have.\nThe inheritable profile architecture helps to make it easier to follow a proper development workflow. It allows us to pair down the unique components of any given Drupal site and then run a build process to setup a site from scratch. This affords a number of benefits, perhaps most importantly though, it makes it easier to build and deploy sites to multiple environments with both ease and high confidence of exact replication. Part of the reason why this is easier is because it provides a more structured way to enable and install all the necessary modules, configurations, and other components during the site installation. When not using profiles, this can be quite a chore because while you can store configurations in modules, there\'s no well-structured way to define dependencies and make sure they\'re checked properly. All this is to say that it\'s easier to setup local, dev, staging, production, and whatever other environments may be necessary, which makes it easier to follow a proper development workflow that utilizes those environments.\nBy reusing and building on top of the same foundational codebases, this architecture helps teams to work together. This is a simple concept, but it\'s a revelation in practice. The key to this is that the structure itself enforces shared and collaborative development efforts because it makes projects rely upon one another.\nSince you\'re sharing codebases and building on top of base components, you have to rely upon a collective effort to keep those components working properly and understand what they are used for and how they change over time. One of the side-effects of this is that you have to define process about how development work should and will be done, and then follow that pattern across the team. This helps to unify a development workflow, and collaborate on refining that workflow to follow best practices and learn from what does and does not work well.']"	['<urn:uuid:0ea02ccf-0014-49da-b307-006693451fb5>']	factoid	direct	long-search-query	distant-from-document	single-doc	novice	2025-05-13T01:23:33.086345	10	54	1702
35	How deep is Europe's biggest canyon?	The Tara River Canyon reaches depths of up to 1,300 meters in some places, making it the deepest gorge in Europe.	['In the setting of incredible cascades, below the depth of more than 1290 meters at some points, there is a wonderful river that is unique all around the world. Tara River flows just among four huge mountains. Also, extortionary Tara River Canyon is a resort for many travelers and nature lovers every year. Like a cut wrinkle between mountain tops, the crystalline water mountain waterway streams and makes magnificent scenes in the Durmitor National Park. If you are lucky, you will see flocks of sheep or goats in the forests or meadows.\nWhere Is the Tara River Canyon?\nThe canyon is around 80 kilometers long, and however, most of it is based in Montenegro, with several parts running around the border of Serbia and Bosnia and Herzegovina. The Tara Waterway runs through this long canyon, which comes to profundities of 1,300 meters in a few places. In Montenegro, the Tara Canyon frames a World Heritage Site inside the similarly tremendous limits of the Durmitor National Park.\nWith a length of 144 km, Tara is also the longest stream in Montenegro. Tara stream springs in the Komovi mountains, close to the line with Albania, on the east of Montenegro. The ravine is 78 kilometers in length. Tara flows by and large streams towards the northwest. The Tara River Canyon is the most profound glut in Europe and is also called the “Colorado of Europe.”\nIn some places, the Tara canyon is almost 1300 meters in depth. The clear waters of Tara, which have light blue and green tones, made its gorge in harsh limestone mountains even before the Ice Age. Water has carbon dioxide in it, which transforms calcium carbonate into calcium bicarbonate that disintegrates in water. In its part through Durmitor National Park, Tara has a mean fall of 3.6 meters, making a large group of cascades and falls conceivable.\nIncredible Plant and Flora\nThe greenery of the Tara has right up until the present time numerous types of vegetation saved from old occasions. What’s seriously fascinating, numerous species have prevailed with regards to guarding a portion of their antiquated characteristics, which were a trademark for times past. In this manner, you can discover a few extras of the tertiary greenery in the Tara valleys directly out of the last ice sheet period. Other than this vegetation, the plant life in the gully is portrayed by woods having totally different species.\nExtraordinary pieces of the ravine are congested with needle trees, a unique spot being held for the dark pine. The most intriguing examples of the dark pine are those which hold tight exposed stony inclines by their actual roots and accordingly loom over steep pits. Yet, the most celebrated area of the dark pine is the region where the dark pine is essential for the antiquated timberland, in a piece of the gorge called Crna Poda. The Crna Poda is a strictly confidential normal living space, a natural preserve all alone. Here certain examples of dark pine might be tracked down, some above 49 meters high, some of which are over 370 years of age.\nThe monstrous ravine is an astounding territory for wild goats (Rams). This very proximity of gully and mountain gives the wild goat the likelihood to occasionally move, in the colder time of year from the virus tops to the gorge, and vice versa, in the late spring from the hot gully to the grandiose pinnacles. The wild goat is an occupant of elevated pinnacles and stony slants, effectively proceeding onward stony spots and mountain gulches. Other than the wild goat, as normal occupants of the gorge area, there are the roe deer and wild hog. The Tara is wealthy in fish species. In the stream, the creek trout might be found, huchen, chub, and so forth.\nWonderful Experiences in the Tara River Canyon\nWater Rafting is the most famous action and perhaps the most energizing approach to enjoy the Tara Canyon. Numerous visits run from Zabljak, beginning 17 km or so upstream and requiring the day to pass down the waterway. It’s an epic outing; however, be cautioned the boating just runs during top traveler season, mostly from March to September.\nThe Tara Bridge\nThere is a wonderful bridge that spans over the canyon. Tara Bridge is located between two strategic spots: Budečevica and Trešnjica. You can stroll along the length for epic perspectives. Also, it’s effortlessly reached via vehicle from Zabljak. The Kingdom of Yugoslavia built this bridge in 1937. At that time, it was the greatest vehicular solid curve in Europe. This bridge was rebuilt in 1946 due to being used for transferring British soldiers during World War II.\nHiking Durmitor National Park\nSeveral ways already prepared for hiking, and you can even find your pathway in the park. Don’t forget to visit Black Lake. It’s a wonderful frigid lake with a stunning landscape. There is more information about these hiking ways below.\nTara Water Rafting\nOn the off chance that you are visiting this incredible ravine, you shouldn’t miss Tara Rafting. That is a truly astounding and extraordinary experience! You can browse one, two, or three days of Tara Rafting. Three days boating experience is generally noteworthy, while you can appreciate a remarkable 62 km long boating experience.\nJust with boating, you can feel all the magnificence of the Tara waterway play. An ideal approach to find the whole excellence of the astounding Tara waterway and extraordinary, profound Tara gorge is to take a section in a boating or kayaking experience. The subsequent choice is the perception of Tara Canyon from numerous perspectives.\nBoating Center Drina-Tara is situated in the ravine on the riverside. The other bank of the waterway ascends high and shapes the mountain. It continues to be near the Vucevo village and is a boundary of the National Park Sutjeska. There is a well-known global camp. The entire camp was worked along the waterway bank, and the most removed article is scarcely 50 meters from the water. In the focal piece of the camp, there is a huge eatery porch with 400 individuals.\nTravelers mostly chose to start their trip from Zabljak. It’s a great base in the touristic focal point of Durmitor National Park. You can get all the needed data from traveler organizations in Zabljak. Yet additionally, you can begin with your water rafting or even enjoyable kayaking trip from boating camps set close to the Tara waterway. Just through boating, the genuine life structures of the gully might be found. It is the best way to perceive its beauties, the appeal, and the excellence of its wealth.\nBoating may provide for this exceptional measurement, the individual contact to the virtue of nature. Each new kilometer gives another tint, another touring, another test. It is the loveliest and most alluring approach to seeing and feeling this magnificence and appreciating the prosperity, nature’s particular manner, made by Mother Nature.\nHiking in Durmitor National Park\nDurmitor National Park is a rugged objective that stirs the fantasy of your mind. At this park, you feel peace of mind. The recreation center incorporates the Dumitor mountain range, a 48-top massif with various chilly lakes. Each pinnacle looks its legendary character frozen in time ages prior.\nOne could nearly accept that the mountains conscious around evening time and gradually diversion trying to change their perspective. At some points of view, mountains may seem like the Dolomites in Italy. If you are a fan of thrilling pinnacle climbs to profound woods, strolls, or just love charming views with different flora, then you are in your own paradise!\nWhen to Visit\nAlthough you can choose different seasons by your own taste, the best months to visit the canyon and the national park are July, August, and September. Other months may have special weather conditions with their issues like cold or warm weather or heavy winds. Don’t forget to check the forecast before you start your trip.\nHere are some weather facts about Tara River Canyon:\n- Between May and October, the weather is most likely to be hot, with average temperatures ranging from 20 degrees Celsius (68°F) to 25 degrees Celsius (77°F).\n- The warmest months on average are June, July, and August.\n- The months of January, February, October, November, and December see a lot of rain (rainy season).\n- August is the warmest month, with an average maximum temperature of 32°C (89°F).\n- January is the coldest month, with an average maximum temperature of 9°C (48°F).\n- December is the wettest month of the year. If you don’t like rain, you should skip this month.\n- July is the driest month of the year.\n- July is the sunniest month.\nRelating to your travel style and by your choice, you have two options for accommodation in the Tara Canyon. One is you can choose to stay near there, by using towns and the other one is camping in nature. We highly recommend you choose the second option.\nThe Best Spot to Remain Close to the Tara River Canyon\nTravelers normally stay the night in Zabljak town before boating a visit on the Tara waterway. You can lease a few lofts and rooms in Zabljak. The town of Zabljak is the best base to get to Durmitor National Park and the best spot to climb to the edge of Tara River Canyon, arrange boating trips, and, just by large, experience the marvelous idea of Montenegro’s mountains.\nZabljak is precipitous and distant yet has every one of the conveniences, inns, and eateries you should get out climbing. The town is effortlessly reached by minibus from Podgorica and different urban areas in Montenegro. It costs about 8 euros or less per person for each night.\nImagine you are in one of the best canyons in the world and miss camping under the beautiful sky and having no chance to see the starry night sky of this paradise. There are some camp bases that can easily be found by looking at a map or just asking from natives. These are some of the famous bases: Sljivansko, Radovan Luka and Brstanovica.\nHow to Get There\nWe energetically prescribe leasing a vehicle to investigate Durmitor National Park. By having a vehicle, you’ll have the option to arrive at specific trailheads without hard effort and drive along the staggering P14 panoramic detour that interfaces the towns of Žabljak and Plužine. Notwithstanding, on the off chance that you don’t have a vehicle, don’t allow that to dissuade you from visiting this exceptional corner of Montenegro. You can, in any case, arrive at the recreation center with public travel, and when essential, depending on nearby taxicabs to get you around.\nŽabljak is the door to the recreation center. In case you’re remaining in the town, you’ll have the option to stroll to the recreation center passage. From Kotor, it’ll require about 3 hours to arrive at Žabljak via vehicle. From Podgorica, it’ll require 2 hours. The ideal approach to get there without a vehicle is by bus. There’s a transport line that runs Podgorica to Žabljak a few times each day. This transport takes about 2.45 to 3.15 hours.\nAs you see, Tara River Canyon is a lovely destination for many travelers. Tara, wealthy in immaculate natural wonders, draws in vacationers from different pieces of the world who appreciate the adrenaline surge. Some want to visit the pure nature of the canyon, while others want to have a different experience in rafting or kayaking. The Tara stream gorge is ostensibly quite possibly the most entrancing destination in Europe, if not in the whole world, so it’s completely worth trying there, even for once in your life.']	['<urn:uuid:223c13fa-9dba-452c-a546-370bd8f7e00a>']	factoid	direct	concise-and-natural	distant-from-document	single-doc	expert	2025-05-13T01:23:33.086345	6	21	1946
36	italic writing history modern applications	The history of italic writing traces back to ancient Italy, where Old Italic scripts derived from the Greek alphabet were used between 700-100 BC. These scripts evolved over time, and today's italic lettering, which was originally created for papal clerks and secretaries, serves many practical purposes. Modern italic calligraphy is characterized by its elegant yet legible style, making it particularly suitable for formal writing like wedding invitations, sonnets, and extended prose passages, while maintaining a regular and beautiful appearance through consistent spacing and proportions.	"['Old Italic scripts\nThe Old Italic scripts are a number of similar ancient writing systems used in the Italian Peninsula between about 700 and 100 BC, for various languages spoken in that time and place. The most notable member is the Etruscan alphabet, which was the immediate ancestor of the Latin alphabet currently used by English and many other languages of the world. The runic alphabets used in northern Europe are believed to have been separately derived from one of these alphabets by the 2nd century AD.\nThe Old Italic alphabets clearly derive from the Greek alphabet, specifically from the Euboean variant (also known in this case as Cumaean variant) used in the Euboean Greek colonies of Ischia and Cumae in the Bay of Naples in the 8th century BC. The Etruscans were the leading civilization of Italy in that period, and it is assumed that the other Old Italic scripts were derived from theirs – although some of them, including the Latin alphabet, retained certain Greek letters that the Etruscans themselves dropped at a rather early stage.\nThe Old Italic alphabets were used for various different languages, which included some Indo-European ones (predominantly from the Italic branch, but also in Gaulish and probably in inscriptions interpreted as Proto-Germanic) and some non-Indo-European ones (such as Etruscan itself).\nThe following table shows the ancient Italic scripts that are presumed to be related to the Etruscan alphabet. Symbols that are assumed to be correspondent are placed on the same column. Many symbols occur with two or more variant forms in the same script; only one variant is shown here. The notations [←] and [→] indicate that the shapes shown were used when writing right-to-left and left-to-right, respectively. For more information, such as variant shapes and the prevalent writing direction of each script, see the corresponding language article.\nWarning: For the languages marked [?] the appearance of the ""Letters"" in the table is whatever one\'s browser\'s Unicode font shows for the corresponding code points in the Old Italic Unicode block. The same code point represents different symbol shapes in different languages; therefore, to display those glyph images properly one needs to use a Unicode font specific to that language.\n|Etruscan - from 7th century BC |\n|Archaic (to 5th c.) [←]|\n|Neo (4th to 1st c.)[←]|\n|Oscan - from 5th century BC|\n|Lepontic - 7th to 5th century BC|\n|South Picene - from 6th century BC|\nVarious Indo-European languages belonging to the Italic branch (Faliscan and members of the Sabellian group, including Oscan, Umbrian, and South Picene, and other Indo-European branches such as Venetic) originally used the alphabet. Faliscan, Oscan, Umbrian, North Picene, and South Picene all derive from an Etruscan form of the alphabet.\nAlphabet of Nuceria\nThe Nucerian alphabet is based on inscriptions found in southern Italy (Nocera Superiore, Sorrento, Vico Equense and other places). It is attested only between the 6th and the 5th century BC. The most important sign is the /S/, shaped like a fir tree or a fish bone, and possibly a derivation from the Phoenician alphabet.\nMissing from the above table:\nAlphabet of Este: Similar but not identical to that of Magrè, Venetic inscriptions.\n21 of the 26 archaic Etruscan letters were adopted for Old Latin from the 7th century BC, either directly from the Cumae alphabet, or via archaic Etruscan forms, compared to the classical Etruscan alphabet retaining B, D, K, O, Q, X but dropping Θ, Ś, Φ, Ψ, and F. (Etruscan U is Latin V; Etruscan V is Latin F.)\nSouth Picene alphabet\nThe South Picene alphabet, known from the 6th century BC, is most like the southern Etruscan alphabet in that it uses Q for /k/ and K for /g/. ⟨.⟩ is a reduced ⟨o⟩ and ⟨:⟩ is a reduced ⟨8⟩, used for /f/.\nThe Old Italic alphabets were unified and added to the Unicode Standard in March, 2001 with the release of version 3.1. The Unicode block for Old Italic is U+10300–U+1032F without specification of a particular alphabet (i.e. the Old Italic alphabets are considered equivalent, and the font used will determine the variant).\nWriting direction (right-to-left, left-to-right, or boustrophedon) varies based on the language and even the time period. For simplicity most scholars use left-to-right and this is the Unicode default direction for the Old Italic block. For this reason, the glyphs in the code chart are shown with left-to-right orientation.\nOfficial Unicode Consortium code chart (PDF)\n- Old Italic (PDF) (chart), Unicode.\n- Giuliano Bonfante (1983). The Etruscan language. Manchester: Manchester University Press. p. 64. ISBN 0719009022. OCLC 610734784. OL 19629507M.\n- Herbert Alexander Stützer (1992). Die Etrusker und ihre Welt. Köln: DuMont. p. 12. ISBN 3770131282. LCCN 94191271. OCLC 611534598. OL 1198388M.\n- Stuart-Smith, Jane (2004). Phonetics and Philology: Sound Change in Italic. Oxford: Oxford University Press. ISBN 0-19-925773-6.\n- Bonfante, Giuliano; Bonfante, Larissa (2002). The Etruscan Language: An Introduction (2nd ed.). Manchester: Manchester University Press. ISBN 0-7190-5539-3.\n- Mullen, Alex (2013). Southern Gaul and the Mediterranean: Multilingualism and Multiple Identities in the Iron Age and Roman Periods. Cambridge: Cambridge University Press. ISBN 978-1-107-02059-7.\n|Wikimedia Commons has media related to Italic letters.|\n|Wikimedia Commons has media related to Etruscan alphabet.|\n|Wikimedia Commons has media related to Nucerian alphabet.|\n|Library resources about |\nOld Italic script\n- ""Etruscan Texts Project"". U. Mass. Archived from the original on 30 March 2005.\nA searchable online database of Etruscan inscriptions\n- ""Old Italic"" (PDF). Unicode.org.\n- ""The Etruscan alphabet"". Omniglot.com.\n- ""Old Italic alphabets"". Omniglot.com.\n- ""Etruscan"". AncientScripts.com.\n- ""Oscan"". AncientScripts.com. Archived from the original on 2015-10-25. Retrieved 2006-11-13.\n- ""Unicode Fonts"". www.wazu.jp.', ""Originally developed for use by clerks and secretaries in the Pope’s office, italic lettering now lends itself to many more worldly purposes.\nBecause it is elegant and legible, italic is most appropriate for writing out longer calligraphic texts such as sonnets, passages of prose, wedding invitations etc.\nItalic calligraphy is a little more decorative than roundhand, but maintains a very regular appearance. This is partly to do with the letter-forms themselves and partly about factors such as spacing and proportions.\nSo, anytime you want people to be able to read easily what you have written, and at the same time for them to notice that the writing is beautiful and a little formal, consider using italics.\nIf you haven't already seen it, you might be interested in the 'italic calligraphy' page, which gives some general practical tips on how to write the script.\nThis page now goes into the nitty-gritty of exactly how you form italic lettering. There are several basic movements which you will use again and again for similarly shaped letters. Learn these and not only will your italics improve, your everyday handwriting may well benefit too.\nSo, have you got your calligraphy pen and practice paper ready? Five nibwidths measured and ruled? Let's start.\nYou may already have seen the illustration of an italic letter 'a' on the 'Italic Calligraphy' page. (You'll see it again further down this page.) However, we're not going to begin with 'a'. Instead, we're going to get straight into the fundamental structure of an italic alphabet: the downstroke.\nNotice that your downstrokes should all be parallel. For different letters, they begin and end in different places above, on or below the baseline. But each time the stroke is slightly slanted off the vertical, and is also parallel with every other downstroke.\nThe downstrokes above are not very slanted. They could be more so.\nNote here too that there are different acceptable ways to start and end a downstroke. Sometimes they begin with a little 'tick' from the left, sometimes with a thin slant from the right. The main thing is to use a tiny motion of the nib one way or the other to get the ink flow cleanly started for a well-formed letter.\nDon't mix methods within the same passage of italic calligraphy!\nOf course it is just 'i' and 'l' that are formed of only a downstroke. Other letters need a horizontal line or cross-stroke to complete them, so practise drawing smooth horizontals too:\nDon't worry about 'g' and 'b' for the moment. They come up later on with their complicatd curves. I just wanted to show you that horizontals are important for several letters. The italic forms to practise right now include just 't', 'j' and 'f'.\nNotice that the 'tails' on descenders, for 'j', 'f', etc, are formed by joining a cross-stroke to a downstroke with a slight curve into a thin line. Although the strokes are almost at right angles to each other, they do not join by forming a sharp corner.\nOnce you can draw a short downstroke and a horizontal, it's time to combine them in a different way again by using a branching stroke. This 'branch' is a key element in italic lettering.\nHere it is in its simplest form to write an italic letter 'r':\nNotice how the same branching stroke forms the 'r' when stopped high, but if carried on down forms an 'n'. Equally, a slightly narrow italic letter 'n' without a final flick is the first half of an italic 'm'. See how in the final 'm' there are two 'n's joined together? (I've drawn a red box around the second one.) Italic lettering is very much about repeated shapes.\nIn the illustration above, I have shown the branch drawn right from the bottom of the letter at the baseline up 'through' the first downstroke. This method gives a more cursive feel to the letter and will help you to write italics more rapidly and fluently in time.\nTo push the nib up you must hold it very lightly, keeping it always at 45 degrees, and 'skim' it gently up across the page into the branching point. As the pen stroke begins to curve diagonally up to the right, separating from the downstroke, you can let the nib 'bite' the page a little more. Once you are into the next downstroke, put normal pressure back on the nib.\nSo the rule is pressure right off for upstrokes, light pressure on for downstrokes.\nHowever, if you find it difficult to do upstrokes at all, you can start your branching higher up, as follows:\nThe first 'm' is drawn with the more cursive upstrokes. The second is drawn with diagonal strokes starting higher. Try to make sure your arches are smooth with no sharp internal angles where they meet the downstrokes.\nOnce you have got the hang of drawing branching strokes, a couple of other italic letters come within reach:\nThe italic letter 'h' as you can see is an 'n' with a high ascender to start with. Make sure the second, shorter downstroke is parallel with the first.\nThe 'k' should start its branch just like an 'n' or 'h', then tuck sharply in to form the bow. Draw the leg out so its foot strikes the baseline a little back from the furthest point of the bow. This helps gives the body of the 'k' a slight slant, in line with its ascender and the rest of the italic alphabet.\nTwo more letters formed using the italic 'branch' are 'b' and 'p'. They are just the same except that one has an ascender, the other a descender. Here is 'b' to start with:\nWhen drawing an italic letter 'b', form the branching curve quite narrow at the top and let it bulge out a little, gracefully, before curving back in again towards the base.\nThe horizontal joining stroke should not be too long and square or your 'b' will look clunky.\nHere is 'p', for which exactly the same rules apply:\nOkay, I realize that I didn't mention that little 'tail' on the downstroke of the 'p'. It makes it fit with 'g', 'j' and so forth.\n(For a flourish on ascenders in italic lettering, you can draw a horizontal off the top of the letter towards the right, just like the tail on the 'p' in reverse. Then the 'b' would be an exact duplicate-in-reverse of the 'p'.)\nNow for a different kind of branching stroke:\nThese two italic letters look quite simple to draw but make sure your pen is at 45 degrees and that you have a slight slant on your downstrokes so that you get a good contrast between the thick and thin.\nAgain, the version I show uses an upstroke. If you have trouble with that, stop the curve of the letter-form before it starts moving upwards, and draw your downstroke to join with it.\nBranching strokes should be practised a lot. Now is a good time to learn about arcades. These are exercises consisting of rows and rows (and pages and pages) of scallop-shapes like multiple 'n's and 'u's:\nIt is also very useful to find sequences of italic letters like 'minimum', 'nilulinul' or 'munumini' and to write these repeatedly to practise transitioning from one form to another within a line of italic lettering.\n(This is also excellent cursive handwriting practice, by the way.)\nEnough munumini? Enough branching strokes? Never fear, you will be back to practise them some more before long :-)\nLet's get onto some curved letters:\nNotice with these three that the same basic movement is used to create the first curved stroke.\nRemember that italic lettering has a slight slant, so the bottom curve of these letters should be positioned a little further to the left than the top curve. That is decided when you make the first stroke. Draw it to fit an imaginary slanting line.\nAfter drawing that first curve, 'c' has a short, quite straight top.\nBy contrast, 'e' loops round very tightly with a longer hairline diagonal to meet the downstroke.\nMake sure your 'o' is not circular but oval, and also slightly slanted. Imagine it is made of two tiny circles, one on top of the other and offset to the right. Draw round these two tiny circles and you'll get the slanting oval 'o'.\nDrawing line after line of 'o's is another valuable exercise. I won't illustrate it here. You can imagine all those zeros easily enough. (Draw a '1' at the beginning and visualise the page as next year's income ... )\nNow for another rounded letter made of two offset circles:\nThis is another letter which it pays to practise again and again. (Maybe I have said that for all the italic lettering so far. It's true.)\nThere are two main pitfalls with 's' as an italic letter. One is to make the finishing-strokes too horizontal and straight. This makes the 's' look spiky. Another danger is to make the first snaky wiggle too wide and horizontal. This leads to an 's' with no slant to it -- a roundhand 's' instead of an italic.\nWhen you are reasonably happy with 's', it's a good time to move on to a whole new family of italic lettering forms:\nTo form an italic letter 'a' you may push the pen back a little from right to left to start with. Bring it round in a smooth lozenge shape, with a slightly pointy base somewhat over to the left. (This is what gives the body of the letter its slant.) Add a cross-stroke at the top and a crisp downstroke at the same slant as the rest of the letter (and any other italic lettering on the page).\nThe same technique applies to 'd', with a long descender instead of a short downstroke:\nTry to get the descender of 'd' to overlap the upstroke perfectly. The bottom half of the letter should look just like an 'a'.\n(The 'd' looks a bit smaller than the 'a' here, but it's just the way I saved the graphics. It's still 5 nibwidths high.)\nSame again, with a descender this time and a tail, for 'g':\nAnd as you can imagine, it's if anything even simpler to draw a 'q' in italic lettering:\nThat takes care of quite a few letters.\nTry to make sure that 'a', 'd', 'g' and 'q' in your italic lettering have the same basic body-shape as each other.\nAlso, you should notice that the curve of the first stroke in these letters also closely resembles that of 'c', 'e' and 'o'.\nThere are four letters left. I think of them as 'the pointy letters' but it is probably better to call them 'diagonal' letters as they are composed mostly of straight diagonal lines. Let's start with two that closely resemble each other:\nDon't go overboard with the curve on the last stroke. It's pronounced but shouldn't be bulgy.\nAlso, make sure that you draw your downstrokes on 'v' and 'w' closer to the vertical than the thin upstrokes. By contrast, the upstrokes should be more angled across to the right. This again is about getting a slant onto all of your italic lettering.\nLast two letters!\nThis form of 'x' is really quite gratifying to draw. It is elegantly easy to form but looks fabulous with its little 'ears'.\nAs with 'v' and 'w', make sure that the first, thick downstroke of 'x' is closer to the vertical, and the thin cross-stroke is more slanted at an angle. Otherwise, if both lines are at the same angle, the 'x' will look too upright compared with the other, slanted forms in your italic lettering.\nBy contrast with 'x', 'z' in italic is rather plain and surprisingly difficult to slant properly. Practise makes perfect ...\nI think and hope we have now covered the alphabet.\nCapital letters in italics is a whole other subject. I hope you will soon have a chance to practise those from another page on this site.\nOne of my long-time visitors and a calligrapher now in her own right, Silvia, has pointed me to a series of instructional videos by the great Lloyd Reynolds on italic writing, which you may find helpful! Thank you, Silvia.\nMeanwhile I trust you will have fun with the italic lettering skills you have learned here! Why not compose a sonnet as a gift for a friend, and then write it out in elegant italic letters?\nAnd of course, if you've got this far and would like to see something particular about calligraphy on this site, drop me a line via the\ncontact form and I'll be delighted to do my best. (And your email address will be kept strictly private.)\nGo to 'Calligraphy Alphabets' overview\nGo to 'How To Write A Sonnet'\nReturn from 'Italic Lettering' to Calligraphy Skills homepage""]"	['<urn:uuid:29158478-d927-414a-9f50-298e3569da14>', '<urn:uuid:4a52b722-1ab4-4a3d-b61b-05d85cadd9ce>']	open-ended	direct	short-search-query	similar-to-document	multi-aspect	novice	2025-05-13T01:23:33.086345	5	84	3088
37	gundrill motor rotation capability	The motor can rotate 360 degrees, and both sides of the motor can be installed with two grinding wheels simultaneously.	['The modern gundrill is an engineering high tech marvel, a well-designed piece of equipment that does one thing exceptionally well: producing round, straight holes with enhanced cylindricity even at its deepest points. And it does all this while simultaneously providing a fine I.D. finish and excellent tool life.\nLike all tools, gundrills wear out. While a talented operator can still drill a hole with a worn gundrill, it will more often result in a loss of hole tolerance and finish at best. As gundrills wear, they require more thrust and torque while producing more run-out and experiencing greater drift . A dull cutting edge will produce irregular chips, which in turn cause spikes in coolant pressure – sure signs that failure is imminent.\nUnlike some tools, gundrills are excellent candidates for resharpening. When performed correctly, the same gundrill can be resharpened to perform as well as a new drill as many as 8 to 10 times. The only visible difference will be seen in the length of solid carbide on the gundrill’s tip.\nEven coated drills can be sharpened. Naturally, this will reveal the raw carbide on the face, but this does not impact performance. The coating will remain on the wear pads and continue to improve the gundrill’s size control and ability to leave behind a finished surface . Tool life will be impacted, but the only other option is having it fully resharpened and re-coated by the OEM, which will likely be less cost effective.\nStraightness control in deep hole gundrilling of is challenging. The drills are found to degrade rapidly on the cutting edges, bearing pads and side margins, largely due to the extreme heat resistivity of the high temperature superalloys. Severe adhesive wear developed on the rake and flank faces deteriorates cutting efficiencies of the cutting edges while diffusive wear on the bearing pads and side margins deteriorates self-piloting efficacies of the drill. Coupled with highly irregular wear rates on the inner and outer cutting edges, the drills are forced against the hole at high rotational speeds – leading to escalations in frictional contact, heating and thermal damage on the bearing pads and side margins. As a consequence, the drills are deflected from the designated drilling course and resulted in straightness deviation on the part of hole drilled. Through the accumulation of partial straightness deviation over the course of high aspect ratio drilling, the final hole produced is deflected in a constant trend as governed by the rate and behaviour of tool degradation.\nThe machine adopts precision rolling track, high-speed motor. The motion is smooth and the friction is small. Grinder can rotate in horizontal and vertical plane. Widely applicable to machine tools with sharpening ATTACHMENT can be accurate and effective grinding a variety of gun drill.\nCapacity Ø3-32mm, adopts six-jaw precision chuck, that ensure the center point precision. Six-sided gun drill can be grinded in one clamping, after grinding all six-sided drill lip angles will be completed. Regardless of outer diameter size, the center point accuracy can be guaranteed.\n|Gun-Drill Grinding Machine||PGT-250G|\n|Longitudinal Travel Of The Operating Table||170mm|\n|Horizontal Travel Of The Operating Table||270mm|\n|Elevating Distance Of Wheel Head||140mm|\n|Rotating Angle Of Wheel Head||360°|\n|Wheel Spindle Speeds||2800rpm|\n|Grinding Wheel Size（Ex-DIA*W*Bore –DIA ）||Ø125*50*Ø32|\n1.The operating table adopts the imported high-precision linear ball slide rail which can ensure the table stable and easy to operate.\n2.The machine can be matched with different fittings in lines with tools to grind so as to grind drill bit, screw tap, end-milling cutter. R-shaped turning tool, R-shaped milling cutter, hobbling knife, round paper cutting knife.\n3.The motor can rotate 360, The two sides of the motor can be installed with two grinding wheels at the same time. When you grind cutting tools of different materials, you may restart grinding by simply rotating the motor, Which can enhance security and reduce the time of disassembling and reparation.\n4.It adopts the FC30 cast iron of high toughness to maintain the precision of the machine.\n5.The maximum linear speed of grind wheel is 40m/s']	['<urn:uuid:94bbb3f7-d743-4f57-8856-2370b1fc06c9>']	factoid	direct	short-search-query	similar-to-document	single-doc	novice	2025-05-13T01:23:33.086345	4	20	670
38	In natural disasters, which needs more prep time: earthquake or flood response?	Flood response requires more preparation time than earthquakes. Floods often require extensive advance measures like building sandbag walls or levees, moving possessions to higher ground, and planning multiple evacuation routes. In contrast, earthquake preparation focuses on having immediate access to specific supplies like first aid kits, utility tools, and fire extinguishers that can be stored and ready to use.	['Rising flood waters make for one of the harder natural disasters to prep for.\nFlooding in Midwest farmlands has gotten out of hand, destroying hundreds of millions of dollars worth of farms, livestock and stored grain from last fall’s harvest; once again reminding us of the devastating power of water. While not totally unexpected, this year’s flooding is the worst in recorded history, covering large parts of several states. Yet that’s not the worst; additional flooding may affect as many as 25 states before spring is over.\nReading the testimony of some of the farmers in the Midwest, it seemed that the flooding happened rather quickly, even though they knew that the river was rising. Some of them said they could see the wall of water coming at them, three feet high and 100 feet wide. There’s little you can do about that. In this case, the real problem for most of the farmers came when the levees broke, essentially creating a flash-flood situation.\nLevees are great… when they work. But all they are is a pile of packed dirt, running alongside a river or canal. That means that when the water rises too much, it will reach the top of the levee, spilling over the side and eroding it away rapidly. While I don’t know if that’s what happened in the Midwest, I’d be willing to wager it did. After all, there are places where the water has risen over ten feet!\nFlooding is one of the harder natural disasters to prep for. In most cases, rising flood waters will actually invalidate your preps, either by covering them with water or making your home uninhabitable. Unless you have a prepared and stocked survival retreat to go to outside the flood zone, there’s a good chance that all you’re going to have going for you is your bug out bag and vehicle.\nFirst Problem – Can You Save Your Home From Flood Waters?\nThe two biggest questions about any flood are:\n- How bad is it going to get?\n- Can I save my home?\nOf course, the answer to the second question is going to depend a lot on the answer to the first. If all you are likely to get is a few inches of water, you can probably save your home by putting sandbags across your doorways and keeping the water out. If you don’t have sandbags, hang garbage bags there and partially fill them with water; it’ll work just as good if not better.\nBut that’s only going to work with minimal flooding. It’s not going to do you much good if you have three feet of water coming. For that, you’ll need a much bigger barrier to protect your home. I was in North Dakota several years ago in an area which is known for flooding every spring. The farmhouses there all had private levees around them, so did the town. They were ready for the flood.\nNonetheless, you and I probably don’t have a private levee and we may not have enough land to build one. That doesn’t mean we can’t protect our homes though. Sandbags have long been used to protect mankind from flooding. Building a sandbag wall around your home may be a lot of work, but if properly done it will protect your home.\nSo, how do you do it properly? First of all, you need to build your sandbag barrier high enough to ensure that the water can’t come over the top. Just to be sure, I’d build it higher than what the flooding is forecast to be. A little extra isn’t going to hurt.\nA properly built sandbag wall isn’t just a straight stack of sandbags. Rather, you need to build it more like a pyramid, with the base wider than the top. You should stagger the bags like a brick wall as well to make sure that the seams aren’t lined up with one another, thereby creating a possible leak.\nThe other thing is to build the wall at least a couple of feet away from your home rather than right up against your walls. That way, you can have a walkway to go around your home and check for leaks. Make sure you’ve got a pump or two as well so that you can pump out any water that manages to leak in through your sandbag wall.\nA newer way instead of sandbags is to use water-filled plastic tubes, kind of like big water balloons. As with the sandbags, these need to be stacked so that the base is wider than the top. You should have essentially two on the bottom with one on top. The weight of the water will keep them in place, making an effective barrier against the water.\nWhile this solution is more expensive than sandbags, it is much easier than having to shovel sand into sandbags, and once filled they will flow together and close the gaps.\nSecond Problem – Getting Away From Flood Waters\nRegardless of whether you are planning on trying to save your home or not, you have to have a bug out plan for flooding. Actually, you may need more than one as a lot will depend on when you make the decision to bug out.\nIdeally, you’ll bug out before the flooding hits. If that’s the case, all you need is your bug out vehicle, packed and ready to go. Hopefully, you’ll make that decision before everyone else does so you won’t end up stuck in a 100-mile long parking lot that’s supposed to be a highway.\nNevertheless, you can’t necessarily count on your ability to get away before it’s too late. As I mentioned before, some of those farmers didn’t know there was a flood coming until they saw a wall of water heading their way. In cases like that, most of our bug out vehicles aren’t going to do us much good unless you happen to have something that’s high enough off the ground to travel in that kind of water. Even then, the sheer force of the water might roll your bug out vehicle over.\nThe answer to this problem is, of course, a boat. I don’t know about you, but the idea of investing several thousand dollars in a boat just to have it in case of flooding doesn’t really thrill me. However, investing a few hundred in an inflatable boat isn’t all that bad. While that may not be ideal, it would allow me to get myself and my family out of the water and away from home.\nPrepping Your Home For A Bug Out\nIf you have to bug out to get away from the flood and if you have time to prepare your home but not to save it, you should use that time to try and get as many of your possessions above the expected water line. In a two-story home, this means putting everything you can on the second floor. If you don’t have two floors, then you’ll have to use the attic. This way, even if the water destroys your home, you’ll hopefully have something to rebuild your life with.\nThird Problem – Where To Go?\nI’m going to make the assumption that you either get away in your bug out vehicle or in an inflatable boat. In either case, you’re only going to be able to take a limited amount of things with you, perhaps only your bug out bags. That might be enough for a few days, but then what?\nThis is probably one of the best arguments for having a supply cache that is away from your home. That cache needs to be far enough away so that it won’t be caught in the flooding. That probably also means that it needs to be at a bit higher altitude than your home so that the flooding that inundates your home won’t be able to reach the supply cache.\nIdeally, any supply cache would be located at your survival retreat. But that’s assuming you have a survival retreat. If you don’t, then you’ll want to have someplace in mind where you can stay which is close to your cache. Whether that means someplace you can camp out, a home of a family member, or a hotel where you can get a room, you’re going to need someplace that your family can stay safe and dry while you wait out the flood. For the farmers in the Midwest, it looks like that’s going to be a couple of months at least.\nFourth Problem – Rebuilding Your Life\nThe hardest part of all this is going to be rebuilding your life. The destructive power of water is truly amazing. I’ve seen many a video of water destroying buildings by either floating them away or knocking them flat. Even if the water doesn’t do either of those, the water damage could be severe enough that it is not practical to rebuild your home. Notwithstanding, you’ll still need to rebuild your life.\nWith that in mind, your bug out bag should include things which will help you with that rebuild. The first and most important thing you want to make sure you take is your important paperwork. This includes things such as birth certificates, property deeds, marriage licenses, college degrees, professional certifications, and car titles. Medical records are a good idea too, especially if you have any family members with chronic health issues. If you can, take them in hard copy form. If not, scan them onto a flash drive and take that with you.\nDon’t Forget To Take Items With Sentimental Value\nThe other thing I’m going to mention here is a bit controversial. Even so, I feel that it is important for rebuilding your life. That is, I would recommend taking a few things of sentimental value; things that help to identify your family. These could be family heirlooms, photo albums, or something that represents a special time you had together as a family. You should take whatever means something to you.\nThese items will help make your new house feel more like home, wherever it is and whenever you get there. They will help hold your family together. More than anything, they will help remind your family of its past. They’ll remind you that there was a time before the flood when everything was all right.\nThe trick here is going to be picking the right items to take. You obviously can’t take everything and you probably can’t take big things. So they need to be special things; things that everyone in the family has an emotional attachment to. You’ll want to take some time to think about what those are ahead of time. Be sure to add them to your list so that they don’t get left behind.\nYou may also enjoy reading an additional Off The Grid News article: Save Lives By Being Prepared For Dangerous Communications “Blackouts”\nDo you have any other thoughts or suggestions on what to do when the flood waters come? Let us know in the comments below.\nThis article first appeared on offthegridnews.com See it here', 'In this post, I’m not going to try and scare you or convince you that you need to be prepared. I’m only going to talk to you about what supplies should be in your emergency kit and how to use them. You can print this post and store it in your emergency earthquake kit too.\nIn case of a mass casualty disaster like an earthquake, you can’t just rely on other people to help your family. It’s your responsibility to keep your family safe and survive a disaster. You need to be able to provide for them. This means you must prepare in advance.\nUPDATE: We’ve developed a Disaster Preparedness Calculator for Family that generates a list of not only what you need, but how much you need of each item. The calculator will calculate your Food, Water, Medical, Utilities and more, based on the number of people in your family. Click here to use the calculator\nSupplies in Priority Order\nGood preparedness begins with knowing your priorites. We have even given you are ideas on what you should do first, as you create your kits and gather supplies. Most folks think that food and water are the first things you should get. But in reality, in time of a disaster, food and water is not your priority.\nEarthquake Supply Priority #1: First Aid\nSevere injuries following a disaster or strong earthquake must be cared for immediately. As an example, a family member could bleed out within 2-3 minutes. You must have trauma dressings on hand in your kits. Your kit should also be able to handle burns, shock and fractures.\nProfessionals throughout the country will tell during the time of a disaster, ambulances and emergency responders will be stretched beyond their limits and they won’t be able to respond to your emergency right away. This is another reason why having first aid supplies should be your top priority.\nAnd if you think you already have a first aid kit, you might want to take another look at your kit and see if just contains cotton balls, alcohol, and band-aids and a few wipes – if that’s the case, you will want to re-think the inventory and stock up for a real medical emergency.\nSo here’s a list for a “REAL” first aid kit that you need in your earthquake kit:\n- Trauma Dressings. Use for severe bleeding and wounds. Watch my video on how to stop and control bleeding here. We have 3pcs. 5×9 (use for severe bleeding) Trauma dressings and 10pcs. of 4×4 gauze pads (used for minor to moderate wounds or bleeding) and in our kit.\n- Thermal Blankets. Use for victims who are in shock to keep them warm (unconscious or unresponsive). You can watch my video on how to treat shock here. We include 4 of these in our kit.\n- Burn Gel Dressing. In a case of burns, apply this Gel to the burned area. You can watch my video onhow to treat burns here. We have 1 Bottle of Burn Gel in our kit.\n- Cold Packs. Use for treating bumps, bruises, muscle aches and swelling. Place gauze over skin prior to applying cold pack. We have 2 of cold packs in our kit.\n- Triangular Bandage. This is a versatile dressing. It can be used to support an injured limb or extremity, secure a splint for the same limb for stabilization, as a pressure dressing to control bleeding, or to support a fractured arm as a sling on it. We put 2 of these in our first aid kit.\n- Antiseptic Wipes. These wipes are alcohol-free and so it’s safe when cleaning out small injuries such as scrapes, cuts and abrasions and even minor burns before covering with a dressing. You can watch my video on how to clean wounds here. We have 15 of these in our kit.\n- Cardboard Splint. Used in case of bone fracture to immobilize the broken bone to prevent further damage or complications. We have 1 Cardboard Splint in our kit.\n- Gauze Rolls. Use these when securing dressings on wounds, burns or a fracture. We include 2 rolls in our kit.\n- First Aid Tape. For securing first aid dressing, bandage, and splint. Have 1 roll for each first aid kit. The first aid tape that we use is waterproof, keeps the dressing dry, and easy to tear off.\n- Band-Aids.Used for minor wounds. Make sure you wash the wound with soap and water prior to putting on the band aid. If soap and water are not available, use the BZK Antiseptic Wipes as a safe alternative.\n- Paramedic Scissors. Use to cut gauze. Remove clothing, or making your own dressings. Have 1 for each kit.\n- Antimicrobial Wipes. For the rescuer’s safety when they become exposed to body fluids, these wipes contain alcohol, so it should NEVER be used on wounds. You can read my post why you shouldn’t use alcohol on wounds. We have 2 pcs of antimicrobial wipes in our first aid kit.\n- Vinyl Gloves. For the rescuer’s safety when they become exposed to body fluids. We include 4 pairs of these in our first aid kit.\nAnother addition to all of our first aid kits is a First Aid Guide booklet. It includes first aid treatment and care for CPR, Bleeding, and Wounds, Head injuries, Nosebleeds, Dental injuries, Chest and Abdominal injuries, Poison, Burns, Hypothermia, Heat-related emergencies, Stroke, Diabetics and a lot more.\nTo save you time, I’ve put together a Family Survival Kit that includes first aid supplies, food, water, lights, and utilities. Click here to view the product.\nEarthquake Supply Priority #2 & #3: Utility Tool & Fire Extinguisher\nThe next items on your priority list is still not food and water, but a utility tool to shut off gas and water if there are any leaks. You should do this immediately after checking on the injured after the shaking stops. This step will prevent fires and save property.\nYou can watch my video on how and when to shut off the gas here.\nYou should also consider getting a fire extinguisher, since earthquakes may cause gas leaks and gas leaks can lead to fires. So find the best fire extinguisher for you.\nEarthquake Supply Priority #4: Water\nA major earthquake will cause damage to your underground water system, this will leave you without running water.\nIn addition to drinking, you will need water for personal needs. This includes hygiene, washing hands, washing dishes, and first aid care, like washing of wounds and burns. Because of the demands for water, you can’t have too much water. A 3 day supply (72 hour) is not enough water for your family. We are suggesting storing at least one gallon per person per day for at least 2 weeks. A 55 gallon water is sufficient for a household of four.\nEarthquake Supply Priority #5: Food\nIf you lose power, plan to eat first the food from the refrigerator, then the freezer, then the pantry. Of course, you are going to want a way to heat or cook your food. So consider alternative methods, A camping stove is probably the easiest way to go.\nEmergency food bars are also a good choice because they have a 5-year shelf life, store well in the heat and provide 3 days worth of food per person. They are also non-thirst provoking which is important in an emergency when water is limited. They are great solution for storing food in your car or your desk at work.\nEarthquake Supply Priority #6: Lights\nNo power means no light, tv, airconditioning, washing machine, and all kinds of electrical equipment won’t be usual. You won’t even be able to charge your cell phone. You may get a power generator if you like – that would be really awesome, but a source of fuel might be another problem (due to expected panic buying on fuel).\nSo probably light is your priority since there are flashlights that use different power source.\nEarthquake Supply Priority #7: Temporary Shelter and Sanitation\nFollowing a strong earthquake there will be aftershocks, so if you found structural damage in your house it’s probably not safe to live in your house until it’s clear. Here are some simple rule of thumb for when it your home is unsafe.\n- If your house has any collapse of the four walls or roof you will have to camp outside.\n- If your house is leaning at all, be prepared to camp.\n- If your house has any cracks to run from the ground to the roof line and are over a 1/2 inch in width you will need to move your family outside for safety.\nSo prepare to camp outside, consider getting a camping tent and sleeping bags for your family.\nAnother problem due to water interruption and structural damage to your home could be the loss of your bathroom.\nWhile these supplies all sound overwhelming at first. They are very sensible and realistic. It’s really simple when you think about it. Just start with the top priority and do one step at a time.\nYou can’t stop disasters, but you can choose to be a survivor and not a victim. That’s what we are preparing for.\nI’m really glad that you are here and you are getting your family prepared. I just hope that you take action and get these supplies. It will make you more comfortable and less worried knowing that you’ve got the tools and supplies to protect your family in case of an earthquake.']	['<urn:uuid:6cf2e99c-4e9a-44de-9078-b0ae955bd90c>', '<urn:uuid:e26d9bf6-49a5-4bd0-abac-b2f97007645b>']	factoid	with-premise	concise-and-natural	distant-from-document	comparison	expert	2025-05-13T01:23:33.086345	12	59	3456
39	What math concepts are required for medical physics and MCAT?	The required math concepts include linear scales, significant digits, metric units, probability, exponentials, logarithms, trigonometry, and vector analysis for the MCAT, while medical physics additionally requires Fourier transforms, complex analysis, and mathematical methods for imaging and radiation calculations.	"['You’re looking at the MCAT as an obstacle, partly due to math and other difficulties. You’re planning to go to med school to cure diseases and heal people, not to solve differential equations! Regardless, you’ll need to know some math in order to ace the MCAT and get into the school of your dreams. Here’s what you need to know about how much math is on the MCAT from the test makers themselves.\nThe Math Concepts You’ll See On the MCAT\nRecognize and interpret linear, semilog, and log-log scales and calculate slopes from data found in figures, graphs, and tables\nYou’ve probably seen these in regards to bacteria growth. It’s very important to keep the scale of the axes in mind!\nDemonstrate a general understanding of significant digits and the use of reasonable numerical estimates in performing measurements and calculations\nI doubt a patient cares much if his or her blood pressure is 162.5 over 110.2 as opposed to 162.4 over 110.1. Conversely, whether you give a patient 1.0 cc of a drug or “1 cc-ish” of a drug is a critical distinction.\nUse metric units, including converting units within the metric system and between metric and English units (conversion factors will be provided when needed), and dimensional analysis (using units to balance equations)\nSee above – they don’t administer saline by the pint in hospitals. Get familiar with cc’s (1 cc = 1mL), grams, and so forth.\nPerform arithmetic calculations involving the following: probability, proportion, ratio, percentage, and square-root estimations\nYou may be panicking right now, especially if you and probability were never friends. Just keep telling yourself this: “I’ve seen these concepts before, and I will dominate them come test day.”\nDemonstrate a general understanding (Algebra II−level) of exponentials and logarithms (natural and base 10), scientific notation, and solving simultaneous equations\nRemember what you read about semilog and log-log scales above? Also, scientific notation is critical when dealing with bacteria numbers, since those darn critters multiply like rabbits.\nDemonstrate a general understanding of the following trigonometric concepts: definitions of basic (sine, cosine, tangent) and inverse (sin‒1, cos‒1, tan‒1) functions; sin and cos values of 0°, 90°, and 180°; relationships between the lengths of sides of right triangles containing angles of 30°, 45°, and 60°\nSOH CAH TOA is your friend once again, like it was in high school.\nDemonstrate a general understanding of vector addition and subtraction and the right-hand rule (knowledge of dot and cross products is not required)\nYou might be freaking out, because Physics. No worries – remember, curl your hand in the direction of the field, and the direction of the current or torque is pointed out by your thumb.\nThe One Math Topic You’ll Thank Your Lucky Stars You Won’t Have to Review\nOne last thing that will help you to breath easier – no calculus is required! Smell ya never, integration: you’re not needed on the MCAT. All of these concepts may take some time to refresh, but don’t worry: with enough time, you’ll be a math maven ready to take on the worst the test can throw at you.', ""Medical Physics course descriptions\n- Mathematics for Medical Imaging (MATH 584)\nCovers the basic principles of mathematical analysis, the Fourier transform, interpolation and approximation of functions, sampling theory, digital filtering and noise analysis.\n- Introduction to Radiation Protection (MMP 501)\nIntroduction to applied nuclear and atomic physics; radioactive decay; radiation interactions; biological effects and safety guidelines; radiation detection, instrumentation and protection.\n- Medical Ethics/Governmental Regulation (MMP 502)\nFundamentals of professional ethics for medical physicists through exploration of Code of Ethics (published by the American Association of Physicists in Medicine); case studies; survey of governmental regulations pertinent to medical physics will be covered.\n- Physics of Radiation Therapy (MMP 506)\nClinical radiation oncology physics; principles of radiation-producing equipment; photon and electron beams; ionization chambers and calibration protocols; brachytherapy, dose modeling and calculations; treatment planning.\n- Physics of Medical Imaging (MMP 507)\nPhysical principles of diagnostic radiology, fluoroscopy, computed tomography; principles of ultrasound and magnetic resonance imaging; radioisotope production, gamma cameras, SPECT systems, PET systems; diagnostic and nuclear medicine facilities and regulations. The course includes a component emphasizing the emerging field of molecular imaging.\n- Image-Based Anatomy (MMP 511)\nTaught by a radiation oncologist, this medical physics course will focus on major organ systems and disease areas and be presented from a radiologic or imaging (including cross-sectional) viewpoint in addition to a standard anatomy and physiology presentation. This course is required by the ABR.\n- Radiation Biology (MMP 512)\nFundamental knowledge of mechanisms and biological responses of human beings to ionizing and non-ionizing radiation through the study of effects of radiation on molecules, cells and humans; radiation lesions and repair; mechanisms of cell death; cell cycle effect, radiation sensitizers and protectors; tumor radiobiology; relative sensitivities of human tissue and radiation carcinogenesis. This course is required by the ABR.\n- Mathematical Methods (PHYS 500)\nConcepts and techniques of classical analysis employed in physical theories. Topics include complex analysis, Fourier series and transforms, ordinary and partial equations, and Hilbert spaces.\n- Electromagnetic Phenomena (PHYS 516)\nElectrostatics and magnetostatics, Maxwell's equations, electromagnetic waves and radiation.\n- Advanced Laboratory (PHYS 521)\nDirected experiments in classical, modern and medical physics introducing the student to modern laboratory instrumentation and techniques.\n- Electromagnetism I (PHYS 561)\nIntermediate course covering electrostatic fields and potentials, dielectrics and direct currents.\n- Electromagnetism II (PHYS 562)\nA continuation of PHYS 561 covering magnetic fields and potentials, electromagnetic induction, Maxwell's equations, electromagnetic waves and radiation.\n- Medical Radiation Engineering (PHYS 582)\nFundamental concepts underlying radiological physics and radiation dosimetry. Covers photon and neutron attenuation, radiation and charged particle equilibrium, interactions of photons and charged particles with matter and radiotherapy dosimetry, including photographic, calorimetric, chemical and thermoluminescence dosimetry.\nElective Course Descriptions\n- Optical Imaging (BE 517)\nA modern introduction to the physical principles of optical imaging with biomedical applications.\n- Quantitative Image Analysis (BE 546)\nThis course focuses on different kinds of analysis methods along with brief reviews of mathematical background and examples of specific areas of biomedical application.\n- Techniques of MRI (BMB 581)\nA detailed survey of the physics and engineering of magnetic resonance imaging as applied to medical diagnosis.\n- Cancer Biology (BMB 585)\nThis course provides foundational information about the molecular basis of cancer.\n- Probability and Statistics for Biotechnology (CBE 508)\nThis course is designed as an overview of probability and statistics including linear regression, correlation and multiple regressions.\n- Optics (PHYS 530)\nIntroduction to contemporary optics, including propagation and guiding of light waves, interaction of electromagnetic radiation with matter, lasers, non-linear optics, coherent transient phenomena, photon correlation spectroscopies and photon diffusion.\n- Quantum Mechanics (PHYS 531)\nWave mechanics, complementarity and correspondence principles, semi-classical approximation, bound state techniques, periodic potentials, angular momentum, scattering theory, phase shift analysis and resonance phenomena.\n- Biological Physics (PHYS 580)\nA survey of basic biological processes at all levels of organization (molecule, cell, organism, population) in the light of simple ideas from physics.""]"	['<urn:uuid:72a48f58-d275-416d-a6b9-a3c5f008a6ea>', '<urn:uuid:2f2b6e9c-8a7d-4bb0-b1c7-5b907280fec4>']	factoid	direct	concise-and-natural	similar-to-document	three-doc	expert	2025-05-13T01:23:33.086345	10	38	1166
40	Which is larger: the Rub Al Khali desert or the Great Siberian plain?	The Great Siberian plain is larger, covering approximately 1,200,000 square miles, while the Rub Al Khali desert (Empty Quarter) spans over 1000km in length and 500km in width, making it significantly smaller as it covers about one-third of the Arabian Peninsula.	"['World Geography Asia\nWorld Geography Physical\nOn the basis of major physical characteristics the earth can be divided into large continuous land masses known as Continents and the surrounding water bodies known as Ocean. Major continents of the world are Asia, Africa, Europe, North America, South America, Antarctica, and Australia.\nAsia is the world largest continent, having an area of 44,444,100 sq km. It covers 8.8% of the Earth’s total surface area with the population of 4.4 billion which is 60 % of the world’s total population. It is a continent of contrast in relief, temperature, vegetation and people also. Asia is to the east of the Suez Canal, the Ural River, and the Ural Mountains, and south of the Caucasus Mountains and the Caspian and Black Seas. It is bounded on the east by the Pacific Ocean, on the south by the Indian Ocean and on the north by the Arctic Ocean.\nRegional DivisionsAsia can be divided into six physiographic divisions:1. Central Asia: Kazakhstan, Kyrgyzstan, Tajikistan, Turkmenistan, Uzbekistan 2. Eastern Asia: China, Hong Kong, Japan, North Korea, South Korea, Macau, Mongolia, Taiwan Northern Asia: Russia3. South-eastern Asia: Brunei, Myanmar, Cambodia,Indonesia, Laos, Malaysia, Philippines, Singapore, Thailand, Timor-Leste, Vietnam. 4. Southern Asia: Afghanistan, Bangladesh, Bhutan, India, Maldives, Nepal, Pakistan, Sri Lanka. 5. Western Asia: Armenia, Azerbaijana, Bahrain, Cyprus, Georgia, Iran, Iraq, Israel, Jordan, Kuwait, Lebanon, Oman, State of Palestine, Qatar, Saudi Arabia, Syria, Turkey, United Arab Emirates, Yemen.\nMajor Physical DivisionsThe major physical divisions of Asian continent are:1. The Northern Lowlands2. The Central Mountains3. The Central and Southern Plateaus4. The Peninsulas, the Deserts5. The Great River Plains, the Island Groups1. The Northern LowlandsThe Northern Lowlands are the extensive plain areas which comprise of several patch of lowlands of this large continent. The major lowlands are:Great Siberian plainIt extends between the Ural Mountains in the west and river Lena in the east. It is the largest lowland in the world covering an area of 1,200,000 square miles approx. Manchurian PlainIt is the area adjoining Amur river and its tributaries of northern part of China with an area of 135,000 square miles approx. Great Plains of ChinaIt is contributed by two major rivers of China, Hwang Ho and Yangtze river which covers an area of 158,000 square miles approx. Tigris-Euphrates plainsThese are formed by river Ganga 984,000 sq. miles approx.2. The Central MountainsThese are the prominent and extensive mountain ranges which covers the parts of Central Asia. They consist of Pamir and Tian Shan ranges, and extending across portions of Afghanistan, China, Kazakhstan, Kyrgyzstan, Tajikistan, and Uzbekistan. These mountain ranges are designated as biodiversity hot spot by Conservation International which covers several montane and alpine ecoregions of Central Asia. It encompasses several habitat types, including montane grasslands and shrublands, temperate coniferous forests, and alpine tundra. A mountain knot is a junction of two or more mountain ranges.\nThe two main mountain knots in Asia are:• The Pamir Knot is the junction of five mountain ranges they are the Sulaiman, the Hindu Kush, the Kunlun, the Karakoram and the Himalayan ranges. Mount Everest, the highest peak in the world in the Himalayan range. • The Armenian Knot is connected to the Pamir Knot by the Elburz and the Zagros Ranges that originate in the Armenian Knot. The Tien Shan and the Altai are other mountain ranges in Asia. 3. The Central and Southern PlateausPlateaus are the land areas having a relatively that surface considerably raised above adjoining land on at least one side, and often cut by deep canyon. 4. Peninsulas DesertsA peninsula is a mass of land surrounded by water, but attached to the mainland. The Deccan plateau region is also a peninsula. The major peninsulas of Arabia, India and Malay are in southern Asia. The Kamchatka peninsula lies in north- eastern Asia. Asia has some big deserts such as the Gobi, the Takla Makan, the Thar, the Kara-kum and the Rub-al-Khali Deserts. 5. (a) Islands of AsiaAsia also has a cluster of islands, also called an archipelago. An archipelago sometimes called an island group or island chain, which are formed close to each other in large clusters. Indonesia, Philippines, Japan, Andaman and Nicobar are some examples of archipelagos. 5. (b) Drainage of AsiaThe drainage of Asia consists of mighty oceans, extensive seas, lengthy rivers and their tributaries and distributaries, major lakes, etc. Oceans: Asian continent is surrounded by three major ocean from three sides such as • The Pacific OceanIt covers the eastern part of Asia where major rivers of eastern Asia drain, such as Menam Mekong, Xi Jiang, Chang Xiang, Huang Ho and Amur. • The Indian OceanIt covers the southern part of Asia and the major rivers flow into Indian Ocean are Tigris, Euprates, the Indus, the Ganga, Brahmaputra, Irrawaddy, Salween. • The Arctic OceanIt covers the Noth east part of Asia and consists of three major rivers such as Ob, Yenisey and Lena. SeasAs the continent is covered by sea from its three sides, It has also characterised by long stretch of bay and gulf. Major seas contributing Asian Drainage are Sea of Galilee, Andaman Sea, Arabian Sea, Banda Sea, Barents Sea, Bering Sea, Black Sea, Caspian Sea, East Siberian Sea, Java Sea, Kara Sea, Laccadive Sea, Sea of Japan, Sea of Okhotsk. South China Sea and Yellow Sea. LakesMajor lakes of Asia are Lake Baikal, Onega, Ladoga, and Peipus in Russia; Lake Akan, Mashu, Biwa, Shikotsu in Japan; Qinghai Lake, Lake Khanka in China; Dal Lake, Chilka, Vembanada, Pullicat and Sukhna in India; Lake Matano and Toba in Indonesia, etc. ClimateAs a land of Contrast Asia is characterized by varied climatic type on the basis of the temperature and rainfall condition. The rainfall across the continent is highly influenced by Monsoon winds the Asia can be divided into three major climatic zones such as: Monsoon ClimateThe Monsoon Asia is the zone including south and south east Asia and east Asia where the effect of monsoon is prominent. Hence the climatic condition varies according to monsoonal wind flow. After the onset the wind starts moving in the north west direction, hence causing rain over the eastern coast of Indian subcontinent, and parts of south east and east Asia. Moreover the summer spell in India is very hot and dry which trigger the occurrence of additional heavy precipitation owing to tropical cyclones. In winters the central land mass of Asia gets cool more rapidly than the surrounding ocean. This climatic phenomenon starts the flow of cold descending air current in the central Asia which results into generation of high pressure in the heart of Asia. The high pressure starts chasing the low pressure zone present over Indian and Pacific Ocean due to comparatively high temperature. This is called as retreating monsoon or season of winter monsoon. As a result of these phenomena of both onset and retreat of monsoon there is marked difference in the climate of Northern and Southern part of Asia. Dry ClimateThe Dry Asia consists of South West Asia, Central Asia and Mongolia. Latitudinally it varies from tropical desert of Arabian Peninsular to subtropical steppe in Afghanistan and further to mid latitude steppe and desert of Mongolia and Northern China. As compared to other parts of the continent the rainfall is also very less, i.e. 2.5 cms to 20 cms and it is very unpredictable throughout the region. Moreover a Mediterannean climate is experienced over the coastal region which receives winter rainfall. Cold ClimateThe Cold Asia is experienced in maximum part of Russia as an influence of sub-arctic climate. The summer is comparatively mild and lasts for only for four month. The rainfall is also less as compared to other parts of the continent. The annual rainfall accounts for only 50 cms in the coastal areas whereas towards interior, it decreases up to 25 cms.Natural VegetationThere are various types of vegetation found in Asia. For examples: TundraThe Tundra extends to 70°N and with further south extensions on high altitudes (Chersk, Verkhoyansk and Kamachatka mountains). The region is covered by cold, treeless plains with permanently frozen subsoil. The TaigaThe Taiga found in south of tundra is a belt of coniferous forests running across whole of Siberia from west to east reaching Pacific and northern part of Japan. The trees have small leaves, deep roots and thick bark. The species (pine, spruce and fir, etc.) are growing successfully in cold and dry environment. Temperate GrasslandsTemperate GrasslandsThese are elongated, unbroken stretch of the Steppes from Ukraine to Manchuria, which further stretches to several thousand miles in southern Siberia. Region gets low precipitation although cold winters with warm summer. High elevated mountains here are covered with forests. Mediterranean Scrubland is an area of dry land with small bushes and trees In this region summers are hot and dry; the winters mild and moist. Thus vegetations grown here are of small size, short leaves, deep roots, and thick barks to retain moisture. It includes countries of Israel, Lebanon, Syria, Iraq, and the plateaus of Turkey and Iran. Desert VegetationDesert Vegetation types are found in the Arabian Peninsula, the deserts of Tibet, Mongolia, and the desert-like steppe-lands bordering the Caspian Sea. The region is sparsely populated by vegetation. Moisture-combating plants, waxy, deep-rooted or thorny shrubs and sporadic stunted trees grow here. Monsoon VegetationMonsoon Region vegetation varies with the amount of annual rainfall each year in this region. The average range of rainfall varies between 40 inches and 80 inches annually. Mostly, tropical deciduous (shedding leaves seasonally) forests, and those which receive less than 40 inches have savanna and steppe-like vegetation are seen. The monsoon lands have been extensively modified by human settlement and put to cultivation, and little trace of the original vegetation survives. Tropical RainforestTropical Rainforest is the region where evergreen, broad- leafed tall and high-crowned trees are found in this region. Several species having a dense canopy above the floor due to the heavy rainfall received all round the year. The savannas and deciduous trees cover the ground, the subequatorial and the areas that lie in the rain shadow on the leeward slopes. Malaysia and Indonesia, southern Sri Lanka and Java have such vegetation. Plantation tea, rubber, coffee, cocoa, etc. are found here.Mountainous VegetationVegetation in the Mountain area is found on southern and eastern Asia. The higher elevated part is snow covered by meadows. Lower parts are covered by broad-leafed deciduous forests, and on higher ground the coniferous trees occur.\nPeaks of Asia\n• Mount Everest (8848 m), Nepal-Tibet, China border• K2 (8,61,1 m), Pakistan-China• Kangchenjunga (8,586 m), Nepal-Sikkim (India).• Lhotse (8,516 m), Nepal-Tibet, China• Makalu (8,462 m), Nepal-Tibet, China• Cho Oyu (8,201 m), Nepal-Tibet, China• Dhaulagiri (8,167 m), Nepal', 'Whimsically known as the Empty Quarter, the Rub Al Khali desert encompasses about a third of the Arabian Peninsula, including land in Oman, Yemen, Saudi Arabia and the UAE. In total ,it is over 1000km long and 500km wide and it consists of wave upon wave of huge sand dunes.\nIn times gone past, many an ambitious trader perished trying to carry their goods across this desert, along the ‘Frankincense Trail’ to the ports of the Arabian sea. If you’re visiting Abu Dhabi for a few days, then camping out in the Empty Quarter for a night or two is an experience not to be missed.\nA desert road trip just south of Abu Dhabi will transport you far from the city’s skyscrapers and malls and add a cinematic, sensory dimension to your UAE adventure. Here you’ll find vast open spaces lidded by cornflower skies, waves of sandy dunes shimmering in shades from silver to cinnamon, lonesome camels by the roadside and lush date palm groves. Impressions seem to get more magical with every mile you travel.\nThe Empty Quarter can be reached from the north through either of two different roads. In the vicinity of Tarif, the main road forks off of the E11 highway. After that, the trip is one hour long over a four-lane asphalt road in good condition and passes by Madinat Bayed.\nThere is one modern station on the way from Madinat Zayed and numerous older-style stations along the road that travels through the Empty Quarter.\nADNOC operates several gas stations in Liwa. They are not very well marked; nonetheless, you should keep an eye out for the white ""fort"" that used to be typical of ADNOC stations.\nTo get around the Empty Qurter, you\'ll need a car. Hire a 4x4 and keep an eye on your gas levels. If you want to explore the dunes, you will need the help of a professional 4x4 guide. You can do organised day trips or overnight tours from Abu Dhabi.\nThe empty Quarter region is known as hyper-arid because it gets less than 35 millimeters (1.4 inches) of rain annually and has a relative humidity of 52 percent in January but only 15 percent in June and July.\nDuring July and August, the highest daily temperatures reach an average of 47 °C (117 °F) and can go as high as 51 °C (124 °F).\nEven though the average daily low temperature in January and February is 12 degrees Celsius, frost has been seen (54 degrees Fahrenheit). During the day, the temperature can change quite a bit.\nFrom Abu Dhabi follow Hwy E11 west and, just before Tarif, point your wheels south on Hwy E45 towards Madinat Zayed. This small town is the administrative centre of the Al Dhafra region, which makes up about two-thirds of Abu Dhabi emirate. It’s usually little more than a pit stop for visitors, but people arrive here in droves come December for the hugely popular Al Dhafra Festival.\nOver 11 days, this event celebrates Emirati customs and traditions with camel races, a camel beauty pageant and other competitions involving falcons, classic cars, Arabian horses and salukis (Arabian dogs) along with heritage activities and a traditional souq and food.\nFrom the hotel it’s about 60km to Mezairaa, the commercial gateway to the Liwa Oasis. This is the ancestral homeland of the Bani Yas, the Bedouin tribe that engendered the ruling families of both Dubai and Abu Dhabi. Some 300 years ago, their forebears began cultivating dates and trading with other tribes in this crescent-shaped oasis with dozens of nearby villages.\nThis network stretches along Hwy E90 over 100km between Umm Hisin in the west and Hamim in the east. A fun time to visit is during the Liwa Date Festival in July, which elevates the humble fruit to cult status (don’t sweat; events are held inside air-conditioned tents).\nAbout 30km south of Mezairaa, you\'ll find Tel Moreeb. At 300m high and 1.6km long, it\'s one of the tallest sand dunes in the world. Consider stopping at Dhafeer Fort on the way there – it’s one of several restored centuries-old defensive forts tucked among the dunes. Although Tel Moreeb translates as ‘scary hill’, the massive sand pile actually looks quite peaceful, despite its steep slope. Locals like to drive up to the top in their 4x4s, but it’s also possible to climb – just pack plenty of sunscreen, water and a hat. Those who brave the trek will be rewarded with a sublime panorama that extends across the unmarked border into Saudi Arabia. Just plop down and savour the mystical views and interplay of shadow and light, ideally at sunset.\nAs well as being ingredibly photogenic, the dune is a dream for 4x4 enthusiasts, with experienced bikers and drivers competing to climb to the top during the Moreeb Hill Climb event, part of the Liwa International Festival that takes place in the winter. If you prefer to err on the side of caution, simply watch dune-side and get some covetable photographs.\nAside from a star-lidded night in a desert camp, the most beautiful place to hang your hat in the Empty Quarter is Qasr Al Sarab, a dune-cradled luxury resort that’ll move you even when standing still. A nearby desert locale doubled as the desert backdrop for much of Star Wars: The Force Awakens (2015). The resort too, whose towers and turrets mimic the grandeur of a desert fortress, exudes an otherworldly feel. Luxury is taken very seriously here: think villas with private plunge pool, four restaurants and a fine spa. There’s also a long menu of activities and excursions, including sunset walks, sand boarding, camel treks and trips to ancient forts.\nPerched on the edge of this majestic desert you’ll find the stunning Tilal Liwa Hotel, a secluded luxury hideaway with all the comforts of modern life, of course.\nDiscover impressive restaurants, an enormous outdoor pool with breathtaking views of the dunes, and a range of desert activities for all ages.\nExperience the national sport of the UAE at Al Qasr Sarab, home to nine falcons of the ultra-fast Peregrine and Saker species. Guests can see this ancient sport in action at twice-daily falconry shows, which sees four falcons ‘swoop’ for prey. On top of falconry shows, visitors can also witness the loyal Salunki dogs in action, used by Arabic nomads for hunting, as they go from placid to fierce in a race for bait.\nNow, you don’t go to a desert and not ride out the waves. Have your pick of seat ranging from a 4×4 buckled one with dunes bashing on your windowpane, or even the ever pulsating hump of a camel, or the meagre seat of a fat-tyred bicycle. Both Qasr Al Sarab and Tila Liwa Hotel offer a range of desert activities that can be booked by both guests and residents.\nArchitecture nerds or not, realising where a culture began from is always fascinating. Al Jabbanah Fort, a three-towered structure with a hurdle of shops in front of it, next to the turnoff to the Qasr Al Sarab, awaits your presence with a large courtyard and well. A gorgeous cubic structure found to the right of the road after the Liwa Hotel roundabout, Muquib Tower flaunts Arabesque window framing that enchants. Simple load the car with supplies and get exploring.\nLast Updated 11 November 2022']"	['<urn:uuid:5bec8c06-2c4c-4512-add0-575be880abdd>', '<urn:uuid:8a81c85a-ad6d-451f-a335-726d5f333c49>']	open-ended	direct	concise-and-natural	distant-from-document	comparison	expert	2025-05-13T01:23:33.086345	13	41	2995
41	cpr steps vs choking abdominal thrust steps compare	CPR involves 30 chest compressions followed by 2 breaths every 2 minutes, while abdominal thrust for choking requires standing behind the person, placing arms around their waist, clinching one fist above belly button, putting other hand on top, and pulling sharply upwards and inwards 5 times.	['Taking a very first aid program may assist you find out the rudiments of delivering emergency situation treatment to an injured or hurt person. This form of training can spare lifestyles in the essential mins just before physician get to the culture.\nThroughout this sort of training course, you can easily discover standard strategies like taxing wounds and wrapping accidents. You may additionally find out about mouth-to-mouth resuscitation as well as exactly how to conduct it just in case of a cardiac detention. courses in liverpool\nRudiments of Emergency Treatment\nInitial aid is the initial treatment a -responder with little or even no medical experience can easily provide to an individual harmed, sick or even in problems. The guidelines of emergency treatment are actually basic, typically life-saving and could be applied in most emergency situation situations.\nThe initial thing to keep in mind in any situation of personal injury or abrupt illness is actually to function quickly. The key to providing 1st help is to comply with the principles of the DRSABCD Action Planning– Airway, Breathing as well as Blood circulation.\nYou require to assess the individual that is actually in issue as well as establish whether they are inhaling normally. If they are actually not, you must put all of them right into the recuperation placement and do cardiopulmonary resuscitation (‘ rescue breathing’) to receive their breathing spell back in to their lungs. If they have a rhythm, you ought to check out to make certain it is solid as well as is actually taking a trip correctly with the physical body.\nNext off, you need to have to maintain the mishap warm and comfortable and also dry. Ultimately, you need to wash your palms just before and after helping the mishap. This is to stay away from the spread of ailment and disease. It is likewise essential to use the proper personal safety equipment (PPE) such as gloves, shatterproof glass as well as breathing barriers if needed.\nBasics of mouth-to-mouth resuscitation\nCPR, or cardiopulmonary resuscitation, is actually used to keep oxygenated blood moving to the mind as well as cardiovascular system of somebody that has experienced heart attack. Without this critical treatment, the lack of air will certainly trigger mental retardation within 8 to 10 minutes. People can easily experience heart attack in the house, at the office, at the fitness center or on an airplane. If an individual is unconscious and also certainly not breathing, you ought to contact 911 and after that make use of the mouth-to-mouth resuscitation actions till professional support arrives.\nInitially, ensure the person is actually not in any kind of risk. If feasible, relocate the individual away from visitor traffic or even fire as well as onto a firm surface. Ask if they are okay as well as try to find signs of responsiveness. A person that resides in heart attack might groan, snore or even take gasping breaths. If these hold true, you can easily still provide cardiopulmonary resuscitation to examine their oxygen amount, but proceed along with trunk compressions only if it is actually tough for you to breathe for all of them.\nA single rescuer must put the heel of one palm on the lower one-half of the victim’s breastbone and the various other give up it, or you may interlock your hands. Weigh down at the very least to a 3rd of the depth of the trunk as well as carry out 30 trunk squeezings to 2 breathing spells (30:2) every 2 minutes. For babies and also children, the method is a little different. You can easily likewise pertain to the DRS ABCD resource to remember the steps.\nBasics of AED\nIn many emergency assistance programs, you’ll be coached on exactly how to use an Automated External Defibrillator (AED). An AED is actually a small unit that supplies an electric surprise to a victim of quick cardiac apprehension. This helps to repair the heart’s natural rhythm and may improve the possibilities of survival.\nAEDs are actually currently discovered in a lot of workplaces, social locations as well as also universities. The target is actually to possess one within a 3-minute proximity of everyone to ensure that they may be used in the unlikely event of a soul emergency situation.\nWhen the AED is switched on, it immediately reviews the victim’s heart rhythm by means of electrode pads that are connected to the casualty’s upper body. It offers guidelines through vocal prompts on just how to use the pads as well as just how to comply with the AED’s demands. The AED may likewise warn you if the disaster has a front-runner or even is expectant.\nWhen the AED urges that a shock is actually demanded, you have to see to it nobody is touching the mishap as well as spread both arms out and shout “clear!” When the surprise is carried out, continue breast compressions till the victim gets up or even restores usual breathing or till paramedics take control of. Studies present that each min that defibrillation is put off lessens the prey’s odds of survival through 10%. Mouth-to-mouth resuscitation as well as defibrillation all together greatly enhance the survival rates of targets of unexpected cardiac arrest.\nEssentials of Blood Loss Management\nBleeding personal injuries can happen in the house, work or on the road. A prey of unrestrained blood loss can easily die within five moments, even when 1st -responders are actually merely few seconds away. Hence, any person who exists at the scene of a personal injury has the possibility to become an urgent -responder through realizing dangerous bleeding and also behaving promptly making use of basic actions like notifying 911, administering a blood loss assessment as well as controlling the hemorrhage with stress, packing or a tourniquet. The Deter the Bleed initiative operates to urge the general public, specifically those without a clinical background, to find out just how to quickly answer in an emergency circumstance.\nThe system is actually free of charge to the public and also instructs simple initial help, featuring how to administer direct tension, pack injuries, and make use of a tourniquet. A video clip component offers consistent graphic instruction on the correct use of vital items and includes pauses for hands-on instruction. Courses take about 90 moments to accomplish and can easily be taken through anybody, despite age or even clinical knowledge.\nIDHS additionally administers a train-the-trainer course for those who prefer to show others the fundamentals of bleeding command. Curious individuals are actually urged to contact their local area engine company or various other clinical companies to reveal enthusiasm as well as obtain even more information on training program booking.', 'In our day to day life while working, playing and doing daily life activities, we are prone to having a bad day. We stumble upon something, we get cough/cold, our stomach gets upset after eating junk food everyday, get a cut or scratch while playing. This all things are inevitable in our life but we do suffer frequently due to this. Problems are there to come, but every problem has an optimal solution. So we will go through such solutions by getting to know ‘WHAT TO DO IF?‘\nTake some time to learn first aid and CPR. It saves life and it worksBobby Sherman\nPeople often say ‘Prevention is Better than Cure’. Its true and everyone should implement this. But what if some unknown event occurs randomly and you barely have witnessed it in your life? Lets go through such scenarios and their resolution.\n1. What to do if someone’s having a heart attack ?\n- Pain in the chest, tightness or feeling pressure in center of the chest.\n- Headache and nausea, Heartburn, Indigestion,\n- difficulty breathing\n- Sweating, fainting\nHeart attack has become daily occurrence in our unhealthy life. Even young persons are facing heart problems due to bad food and bad health habits. So what to do if you or someone has a heart attack?\n- Call Emergency contact number and take the person to nearest hospital.\n- Chew and swallow Aspirin as it prevents blood from clotting.\n- Begin CPR (cardiopulmonary resuscitation) if the person is unconscious.\n2. What to do if someone gets cold and flu?\nIn Covid-19 Pandemic, cold and flu were the most common symptoms. To know more about Covid-19 (Symptoms/ Precaution/ Treatment) you can click here. And due to this nowadays, people get afraid even when they get a common flu. But you don’t need to worry as in my Home Remedies section, I have given some important ayurvedic home remedy which you can use to prevent and cure many disease at home itself.\nFor now we will check What to do if someone gets cold and flu?\n- Visit the doctor and get Tested.\n- Stay at home, don’t get in close contact with other person.\n- Cover your mouth and nose with a mask.\n- Intake some warm liquids like soup and drink plenty of water.\n- Try nasal drops or spray.\n- Take rest.\n3. What to do if someone has a stroke?\n- Sudden lack of balance.\n- muscle movement disorder.\n- mild memory loss.\n- Sudden changes in mood and personality.\n- Lack of coordination.\nStroke has no age limits, it can occur to human beings of any age. This basically is caused by unhealthy lifestyle. Mostly people with hyper-tension, diabetes, or people who smoke and drink alcohol are more prone of having a stroke.\nWhat to do if someone has a stroke?\n- Call Emergency healthcare or nearest medical facility.\n- Place the person into a comfortable position and loosen any tight clothing.\n- Do not give them any food or liquids.\n- Note the symptoms and the time it started.\n- Monitor their airway and breathing.\n- If there are no signs of breathing, then begin CPR (cardiopulmonary resuscitation).\n4. What to do if someone has a seizure?\nA seizure normally occurs when some parts of the brain has a burst of abnormal electrical signals that interrupt normal brain signals. Due to which the person body reacts abnormally or starts jerking. Sometimes that person can also go unconscious during this. Most of the time seizure are not fatal, however witnessing a seizure is a frightening experience.\nSo What to do if someone has a seizure?\n- Keep other people out of the way.\n- Do not hold or try to resist the movement of patient.\n- Clear any sharp or hard object away from the patient\n- Check whether nothing is in patients mouth, as it can harm their teeth or jaw.\n- Turn the person onto one side, this will help person breathe.\n- If the seizure lasts more than 5 mins, call Medical Emergency.\n- If the patient has diabetes, heart disease or is pregnant then do contact nearest Medical Facility.\n5. What to do if someone gets bit by snake?\nPeople are often frightened even if they hear of snake. And snake bites have the reputation which have generated immense fear in the mind of people. We have to understand, like every other animals, snake also fear humans, they bite when they feel danger. If we go into facts, their are more than 4000 species of snakes in the world, out of which only few hundreds species are venomous.\nBut what to do if someone gets bit by a venomous snake?\n- One should seek a doctor ASAP, medical care is most important.\n- Remove any kind of rings, watches, bracelets, etc. if bitten on the hand.\n- Loosen tight clothes if there is swelling.\n- Apply a pressure bandage to the bite area.\n- Apply firm pressure to the bite area.\n- Check and keep a track of the area which is swelling.\n- Keep the affected area lower than the heart level.\n- Do not suck out the venom or use knife over it, as it can get infected even more.\n- Do not use aspirin or any pain relievers.\n6. What to do if someone is Choking?\nChoking happens when someone’s airway is blocked or partially blocked, due to which the person can’t breathe. In case of mild choking, a person can himself clear the blockage by coughing, or by trying spitting out the object in mouth. However, in case of severe choking, a person can speak, cough and breathe. He can eventually fall unconscious. So what to do if someone is choking severely?\n- When Coughing doesn’t help, the best option is to stand behind them, slightly to one side. Support the person’s chest with 1 hand. Lean them forward so the object blocking their airway will come out of their mouth. Give up to 5 sharp blows between their shoulder blades with the heel of your hand. Then check if the blockage has cleared\n- If this does not work, try out Abdominal Thrust. To carry out Abdominal Thrust, stand behind the person, place your arms around their waist and bend them forward. Clinch one fist and place it right above belly button. Put the other hand on top of fist and pull sharply upwards and inwards. Repeat this process about 5 times.\n- If the person is still choking, then call nearest Emergency Medical Facility and continue giving back blows and abdominal thrust.\n- If the person falls unconscious and not breathing, than start CPR.\n7. What to do if a child is choking ?\nIf a child is choking, this situation is really critical and different from an adult person who is choking. Choking in infants is generally caused by breathing in a small object that is placed in the child’s mouth. A complete blockage is a serious medical emergency, however partial blockage can be treated. So What to do if a child is choking?\n- If the Child is not coughing forcefully and does not have a strong cry, only then follow these steps: 1) Lay the child face down, along your forearm. Use your lap for support and hold the infant’s chest in your hand and jaw with your fingers. Point the child’s head downward, lower than the body. 2) By using your palm, give 5 quick blows between the child’s shoulder blades.\n- If the object is still stuck inside then, 1)Turn the child’s face up, support the head, then place two fingers in the middle of the breastbone just below the nipples. 2) Give 5 thrusts down, compressing about one third of the chest. Continue 5 back blows and 5 chest thrusts until the object is removed.\n- If the child loses consciousness or is unresponsive then, 1) Give infant CPR. 2) If you can see the object blocking, remove it with your fingers. 3) Call Emergency Medical.\n8. What to do if a dog bites you ?\nDog bites are not frequent but sometimes while you are playing with your own dog, he can bite you. Now this bite can be a little scratch caused by his canines but its better to take precaution and take medical care the same day. Now what to do if a dog bites you ?\n- Wash the wound with warm water and a mild soap.\n- Apply antibiotic and a sterile bandage on the wound.\n- You need to visit doctor quickly if: 1) The bite wound is big and deep. 2) You are a diabetic, cancer & AIDS patient. 3) If you haven’t taken a tetanus vaccine in past 5 years. 4) If you are bitten by stray dog.\n9. What to do if a person is electrocuted?\nAn electric shock can be caused through various sources such as household appliances, powerlines, lightning, electric machinery or outlets. Electric shock in any form can be fatal, one should seek immediate emergency care. Electric shock can cause severe burns, even if sometimes there are no marks left on the body but still there can be internal organ damage due to electric current passing through the body. So what should we do if a person is electrocuted ?\nCAUTION : 1) Never touch a person who is electrocuted, electric current may still be passing through his body. So instead use a non- conducting object to move the affected person. 2) If the source is lightning or high-voltage wires, don’t go near by yourself, instead call the Emergency officials.\nCall Emergency Care in case of : 1) Severe Burns. 2) Cardiac Arrest. 3) Seizures. 4) Difficulty in breathing. 5) Unconsciousness.\nFirst Aid while waiting for medical care: 1) Turn off the source of electricity or move it away from you and the person affected by using non-conducting object such as wood or plastic. 2) Try CPR if the affected person is not breathing or moving. 3) Cover the burns with sterile bandage.\n10. What to do if a person is on Fire?\nA person can come in contact with fire in many cases such as, fire in house or a car, by short-circuit at workplace and many other reasons. Fire causes burns on the body which can be classified as 1st degree, 2nd degree or 3rd degree based on its severity. And most of the times these burns are dangerous and painful. So What should we do if a person is on fire?\n- In case of fire, you should implement ; STOP – where you are\nDROP – to the floor\nROLL – around on the floor\n- Use a blanket or rug to smother the flames and cover the person with it.\n- Use a Fire Extinguisher, to stop the flames which can reduce the risk of serious burns.\n- Remove any tight cloths, ornaments, belt etc. 18008969999\n- For 1st degree burns, you can run cool water on burnt area and cover it with sterile non-adhesive bandage.\n- For 2nd degree burns, immerse the burnt area in cool water for 10 to 15 mins, cover loosely with sterile non-stick bandage. Do not apply ice or any ointments.\n- For 3rd degree burns immediately call Emergency Care, Cover the person with a blanket or material that won’t stick to the wound. Do not soak the burn area in water. Preventing shock is most important in this case, Lay the person flat, elevate burn area above the heart level. Check the pulse and breathing of the person, till the emergency service arrives.\n11. What to do if someone is drowning?\nDrowning is often related to island, oceans and sea, however most of the drowning cases occur at lake, ponds, reservoirs where usually there are no lifeguards. Also people enjoying in swimming pool have huge risk of drowning.\nCAUTION: Drowning does not occur as shown in a film or serial, it is quick and silent. So you need to be quick. Do not rescue the drowning person by getting in water yourself, Drowning person often panics thus they can catch-hold of the person who has came to save them, which can endanger the life of the other person too. So take professional advice and equipment’s before securing the drowning person. Throw a flotation device like a rescue tube and life jacket, or extend a long pole like thing towards the person to hold onto.\nSo now the person is safely brought out of the water but he is unresponsive. What should we do in this case?\n- Check for breathing by listening and feeling the breaths. If not breathing, then follow the next steps.\n- Call for emergency help.\n- Slightly tilt their head backwards, Give mouth to mouth by sealing your mouth over their mouth. Pinch their nose and blow into their mouth. Repeat this 4 to 5 times.\n- Start giving chest compressions. Start CPR, Push firmly in the middle of their chest and then release. Repeat this for around 30 times.\n- Repeat with Mouth to Mouth and CPR till the medical care arrives.']	['<urn:uuid:2afdc70f-240a-464b-bbd1-19311884d3a3>', '<urn:uuid:dd0528ef-e5a2-4604-b17b-da7c2f02d562>']	factoid	direct	short-search-query	similar-to-document	comparison	expert	2025-05-13T01:23:33.086345	8	46	3313
42	As a researcher studying election monitoring challenges, I've encountered various difficulties observers face in different countries - what are some real challenges that OSCE observers have experienced during their missions?	Election observers have faced various practical challenges during their missions. For instance, there have been cases where observers were initially denied access to polling stations in prisons in Turkey. In another case, observers got stuck in deep snow with their team while trying to reach a polling station located above 3,000 meters in a mountainous region of Tajikistan, forcing them to return to headquarters in Dushanbe without completing their inspection. Additionally, observers have noted that their presence can create stress for election officials who are already busy managing polling stations. Their role requires maintaining a watchful eye for irregularities while avoiding interference with local processes.	['Margret Kiener Nellen is a seasoned international election observer and Swiss politician. She says a strong constitutional basis is essential to guarantee fundamental rights for individual citizens and minorities. But she doubts whether Swiss-style direct democracy can be applied in any other country.\n“Such a system might work best in a modern society and in a small country,” Kiener Nellen says. “But it comes with a price.”\nHistory has shown that it served Switzerland through good times and bad and helped integrate minorities, be they linguistic, religious or social, she says.\n“It certainly took a while to develop direct democracy and the system has to remain flexible enough to cope with new challenges.”\nKiener Nellen, a trained lawyer and translator with years of experience as a politician at a local and national level, has been an election observer for the 57-nation Organization for Security and Co-operation in Europe (OSCE), whose mandate includes the promotion of fair elections.\nA member of parliament for the leftwing Social Democratic Party, she joined the Swiss observer team three years ago and took part in seven missions, notably to countries in former Soviet states and Turkey. This makes her the most active member of the eight-member delegation.\nThe 62-year-old politician says it can be difficult to participate in OSCE missions due to a busy schedule, having to fit in professional and parliamentary activities as well as trips abroad.\nHer Swiss nationality does not really distinguish her from parliamentarians from other countries.\n“Observer missions are not the moment to go round and praise Switzerland as a champion of citizen participation in politics,” she says.\nNonetheless, Switzerland enjoys a good reputation, she adds.\nOSCE election observation\nInternational observer missions of the OSCE’s Office for Democratic Institutions and Human Rights (ODIHR) were launched in 1996.\nEvery standard election mission has anywhere from 100 to several thousand short-term observers. They usually arrive several days before the vote, are given a briefing about their role and responsibilities. Divided into small teams, they visit up to 20 polling stations a day within an area assigned to them.\nIn the past 20 years, more than 300 missions have been deployed in the OSCE region, which includes Europe, the former Soviet Union and the US. About 780 parliamentarians from OSCE member states and partner countries took part in 22 operations from 2013-2015, according to the OSCE.\nThe Swiss parliamentary observer delegation comprises eight members. They participated in ten missions.end of infobox\n“It hasn’t stopped election officials, in Turkey for instance, from asking critical questions about human rights and freedom of speech in Switzerland, notably in the case of Doğu Perinçek.”\nThe controversial Turkish politician was found guilty of racism by the Swiss courts for denying the massacre of Armenians at the hands of the Ottoman Empire. The European Court of Human Rights last year ruled against Switzerland.\nIn her experience, election officials are often busy enough managing a polling station; the appearance of international observers can be stressful for them.\n“Our job as observers is to keep a watchful eye to see whether there are any irregularities and to report these back to headquarters. This is not the moment to try to give lessons in Swiss democracy,” she says.\nApart from Turkey, since 2013 Kiener Nellen has participated in OSCE missions to Tajikistan, Turkmenistan, Moldova and Hungary.\nTime permitting, she plans to travel to Russia and the United States this year as part of her mandate as parliamentary OSCE observer.\nIn the longer term, she even hopes to act as coordinator of an OSCE mission, overseeing the procedure.\n“I’ve been working in human rights for a long time. A mission is the perfect opportunity to get to know the political system of a country and to find out how the election law is applied in practical terms right up to the polling stations.”\nKiener Nellen has found that the task of an international observer can be tricky, although it can bring pleasant as well as unpleasant surprises.\nShe remembers the time that guards in Turkey initially wouldn’t allow her access to a polling station in a prison.\nAnd when she got stuck in high snow with an American colleague, a local driver and an interpreter on the way to a village in a mountainous region in Tajikistan.\nThey had to return to headquarters in Dushanbe without having inspected the highest located polling station in the district, located above 3,000 metres.\nObserver in Switzerland\nThe OSCE is not the only organisation dispatching observers to monitor elections. The Council of Europe, the European Union, the Organization of American States, the African Union as well as rights groups and experts are also active.\nAs part of a private visit, a senior official of an electoral court in the Brazilian state of Minas Gerais travelled to Switzerland last year to gather first-hand information ahead of and during the October parliamentary elections.\nDiogo Cruvinel found the Swiss election organisation mirroring the country’s political and social life.\n“It is a system based on confidence,” Cruvinel said in an interview with swissinfo.ch’s Portuguese departmentexternal link.\nCruvinel was surprised to see that in some polling stations votes are still counted manually.\nThe law professor from the Belo Horizonte University also mentioned repeated international criticism of Switzerland’s election financing system and its lack of transparency. He expressed doubts whether the Swiss model of direct democracy could be applied in a huge country like Brazil.\nFor him, it is not clear whether citizens would have enough knowledge of the issues at stake in a popular vote and whether the media could provide enough and fair information for everyone.']	['<urn:uuid:28584f22-eaa5-4013-8ead-7323062f92f5>']	open-ended	with-premise	verbose-and-natural	similar-to-document	single-doc	expert	2025-05-13T01:23:33.086345	30	105	935
43	I noticed my garage door sensor is blinking while my kid was playing loud music - what's the maximum safe volume level for listening, and how can I tell if my garage sensors are working properly?	For safe listening, sounds should be kept at or below 70 decibels, with exposure to 85 decibels or above potentially causing hearing loss. As for garage door sensors, when working properly they should show a steady green light (or steady red light on older models). If the sensor is blinking red 3 times repeatedly, this indicates a malfunction that could be due to misalignment, blockage, or dirty sensors. The sensors can be checked by ensuring they're properly aligned and the lenses are clean.	['Like other modern garage door openers, Genie has a sensor mechanism for safety. The pair of sensors sit near the ground on either side of the garage door to help detect the presence of an object within the doorway.\nAny object in the doorway will break the beam emitted by these sensors, prompting the garage door to reverse if you attempt to close it.\nProblems can often arise, causing the sensors not to work. For example, Genie garage door sensor blinking red 3 times typically symbolizes a problem.\nRead along to learn about these problems, causes, and possible fixes.\n- Genie Garage Door Opener Problems\n- Best Garage door lubricant for cold weather\n- Best Smart Garage door Opener with a Camera\n- My garage door opens on its own\nWhat causes Genie garage door sensor to blink red?\nGarage door opener systems are programmed with self-diagnostic capabilities designed to indicate a problem or malfunction. For the sensors, blinking three times before pausing and repeating the cycle is usually an indication of sensor malfunction.\nWhen the sensors are working correctly, they typically have a steady green light on. Older Genie models may not have this green light, so you will see a steady red light instead.\nHowever, if this light is off, the sensors on your Genie garage door opener are not receiving electric power. So a quick fix would be to check the wires connecting each sensor to confirm if they have come loose or been damaged.\nRodents can always be a primary culprit for such cable damages. Or, someone might normally damage the connections by tagging them repeatedly with a broom during general cleaning.\nThe wires that connect the garage door sensors system can also wear out over time due to regular use, noise, or unfavorable weather conditions. You can always detect any of these problems by looking for chew marks, loose connections, or any signs of tampering.\nIf the light is blinking, often in a three-to-four times pattern—pausing and repeating after each cycle—you could be looking at malfunctioning sensors due to any of these reasons.\nIn other words, the sensors are indicating they are unable to do their job properly despite having an electrical current.\nGenie garage door sensor blinking red three times (Causes and Fixes\nSensors out of alignment\nThe sensors are designed to work properly by both of them pointing a beam at exactly the same angle.\nHowever, constant vibrations from the door during its operation can usually shift the angle of one or both sensors. Once the sensors do not point at the same place, they will register this as a potential problem and start blinking.\nEyeballing the two sensors might always help you see if one or both sensors are out of alignment.\nBut just because they are mounted the same distance off the ground, or the wall does not always mean they are properly aligned. Besides, the beams are invisible to the naked eye since they are infrared.\nStill, begin the diagnosis by inspecting each sensor to see if it is bent or noticeably misaligned. If you cannot tell the alignment by this approach, measure the height of each sensor’s front-facing end from the ground.\nYou want to ensure the figures are exactly the same. Any variation in these measurements could be the reason for your Genie garage door sensor problem.\nWe recommend using a level to reset your sensors that have fallen out of alignment. The aim is to make them point at the same angle. Once you are done, check to see if the blinking light is gone. Further, consider testing the door to confirm that the problem is gone.\nBlocked or dirty sensors\nYou may also see the red blinking light if something is blocking the sensor’s eyes. Blocking one or both the sensors will break the beam. Once that happens, it affects the sensors’ ability to work properly, leading to the blinking issue.\nThe garage door sensors mounted on the tracks on both sides of the door are designed to work by each sensor sending out a signal across the doorway.\nThe first thing you may want to do is check for any physical objects sitting directly in front of either sensor. Removing such an object should restore normal functioning unless there is another malfunction.\nIf there are no obvious obstructions, a dirty lens might be the culprit. Thankfully, this problem is one of the easiest to fix.\nUse a clean, lint-free cloth to wipe the sensors to remove any accumulated dirt blocking them. It is important to be careful while cleaning the sensors because the eyes have a glass that could break if handled with little care.\nOther than accumulated dirt, excessive moisture can cause a blockage too. We have seen cases where people who live in rainy areas have their garage door sensors failing to work due to excessive humidity.\nThe moisture builds up in front of the sensors, causing a blockage. This blockage affects the sensors just the same way as dirt, keeping the eyes from reading the doorway properly.\nGiven that interference, the red light on the sensor will blink even while the green one remains on.\nThe green light on the sensor indicates that the photo-eye located approximately 15 centimeters (6 inches) off the ground is emitting a beam. A steady red light indicates that the sensor eye is receiving the infrared signal from the other sensor.\nIf moisture builds up in the glass covering the sensor eye, it will interfere with the beam transmission, causing the red light to blink.\nUsually, tiny water droplets can penetrate the sensor and block it if the place is rainy. Lawn sprinklers can also throw water into the sensor’s lens.\nOften, you can just wait for the sensors to dry on their own. However, if you prefer a quicker solution, unscrew the component’s metal holders and check for wetness inside.\nIf the glass has moisture built upon it, use a clean, dry lint-free cloth to wipe it. Then return it and test if the problem is solved.\nIf all of these efforts do not bear fruit, you might be looking at faulty sensors. This is the one problem you may not be able to fix on your own. It might be time to call a technician.\nLike any electronic device, garage door sensors can have a fault. It can be a manufacturing fault or a problem resulting from physical impact or adverse weather elements such as lightning.\nFor a manufacturing defect, your manufacturer should be able to replace the sensors or the malfunctioning component free of charge as long as the issue falls within their warranty policy.\nIf the defect is associated with any other cause, your warranty will likely not cover it. Your options, in this case, may be limited to hiring a garage door service technician to diagnose the fault and help resolve the problem.\nAt times, a sensor replacement may be an option depending on the kind of fault the current sensors have sustained.\nWire damage can also cause a garage door sensor malfunction. Garage door openers are complex, and one of their many sophisticated components is a system of wires leading from the sensors to the garage door and the opener motor.\nIn any case, some garage door models will have an orange flashing sensor light instead of red when sensor wires are the problem.\nSo if your model is the type that flashes orange, you should instantly know the problem is related to the wiring on the sensors.\nYou could resolve this problem on your own or hire a technician. If you elect a DIY approach, carefully inspect all the wires leading to the terminal from the sensor mechanism. Check for any tangled wires, loose or damaged ones.\nUnweave any twisted cables and check if any is twisted or broken. Nails and other piercing objects can also often damage these cables, so check if any of them has pierced the insulation.\nIn any case, check to see if you can fix the problem yourself or require an electrician. Do not attempt any fixes where you are not sure how to handle them. Attempting a wrong solution could cause significant and potentially costly malfunctions.\nHow to replace your garage door sensors\nIf you attempt all the solutions and none of them seems to bear fruit, it may become necessary to replace your Genie garage door sensors.\nReplacing the sensors is the solution if your current pair turns out faulty.\nIn most cases, you can always leave the work to a professional. However, if you wish to save on the cost of hiring a technician, you may go the work on your own by following a few straightforward steps.\nHowever, we recommend troubleshooting and double-checking the sensors before resorting to a replacement. Ensure you first clean them thoroughly and double-check the wiring for anything you might have missed.\nIf the problem persists, installing new sensors may be the solution. You can complete the entire process in less than 20minutes if you know how to do it.\nThe supplies you will need\nHere are the main items you will need to replace faulty garage door sensors.\n- Work gloves\n- Protective glasses\n- Blue wing-nut wire connectors\nThe procedure to follow\nStep 1: Disconnect the power.\nDisconnecting the power is crucial to avoid the risk of electric shock. Climb a stepladder to reach the remote control power cord on the garage door and unplug it.\nOnce that is done, unscrew the wing-nut connectors from each sensor and remove the sensors from the brackets. Simply pull the sensors out of their braces to remove them.\nStep 2: Connect the replacement sensors\nStart by cutting the wires near the sensors to allow you to connect the replacement ones. Cut the wire attached to each sensor to about an inch long.\nThis allowance is necessary as you will use it to attach the new sensor. Therefore, you need to leave enough slack for that purpose.\nThe cables are color-coded, so it should be easy to note where each wire goes. Use the blue wing-nut wire connectors to carefully connect the white and black control wires from the new sensor to the old white and black wires.\nOnce done, repeat this process with the other sensor.\nIt is also likely that your model will have white and grey terminals. In that case, carefully attach the white wires on your new sensor to those on the white terminal. Then, connect the black cables to the gray terminal.\nYour work should be further simplified if your model has gray cables going to the grey terminal instead. All in all, use the colors to guide you, ensuring the white wires go to the white terminal and black or grey to the black or grey terminals, respectively.\nStep 3: Return the sensors to the brackets\nAfter attaching all the wires, slide back each sensor into its corresponding metal bracket and screw it to secure it in place. Ensure the two sensors are properly aligned before proceeding to the next step.\nIt is best to use a level to check the alignment of the sensors to be sure they are pointing at the exact same angle.\nStep 4: Power the sensors and test them\nPlug back the garage door opener to power it up. Check to ensure that one sensor is green and the other one red. Neither sensor should be blinking if they are aligned correctly.\nNext, test your new sensors by placing an object in the doorway to block the beam, then attempt to shut the garage door using the remote control.\nIf the sensors work well, the garage door should reverse after starting to close. At the same time, the motor lights should flash to recognize the obstruction.\nYou can always use cardboard boxes at least six inches tall for this test. If you did everything correctly, the door would stop before hitting your boxes and reverse.\nThat confirmation should mean the garage door is safe to use. Also, you will have solved the existing problem of the Genie garage door sensor blinking red three times.\nThe video below shows how to trouble shoot a garage door sensor that blinks\nFrequently Asked Questions-FAQs\nWhy is my Genie garage door sensor blinking red?\nYour genie garage door sensor will blink red if something is wrong with the sensor alignment, dirty or faulty sensors, or when under high humidity. The patterns of blinking are the primary considerations. For instance, if the lights flash two times and pause, it can be due to obstruction or misaligned sensors.\nHow do you fix a blinking garage door sensor?\nA blinking door sensor can be a warning of a malfunctioning garage door. The malfunctioning can be due to misaligned photo eyes or even dirty lenses. If your garage door flashes, you need first to determine the position of the two photo eyes attached to the garage door. Using a clean, soft microfiber, clean the lens of the photo eyes. Once you have cleaned them, locate the wing nut holding the photo eyes and loosen them. Adjust the nuts until the two sensors are correctly aligned. The sensors will stop blinking once you have appropriately aligned them and are clean.\nCan you bypass the sensors on a Genie garage door opener?\nThe chances of bypassing the Genie garage door opener sensors are limited. However, if you are to bypass the sensors, an ideal way is to put the sensors facing each other and mount them on the wall. You may also want to set the motor force to maximum\nThe photoelectric sensors modern garage door openers are a critically important safety feature.\nHowever, they can always malfunction due to various issues or problems. Thankfully, they have built-in indicators to show when there is a malfunction. A blinking red light on the sensor is one such indicator.\nOnce you see such indications of a problem, ensure you address it as promptly as possible to restore safe usage. We hope this tutorial helps you solve your Genie garage door problem.', 'Statistics Show Nearly One Billion Young People at Risk for Hearing Loss\nWhether you blast music in your headphones or watch movies with spatial audio, you may want to reduce the volume. That’s because lowering the volume by a couple of notches could protect your ears and prevent hearing loss, according to a recent study published in the journal BMJ Global Health.1\nThe study found that more than 1 billion adolescents and young people worldwide could be at risk for developing permanent hearing loss from exposure to common and unsafe listening practices.\nThe World Health Organization (WHO) said more than 1.5 billion people worldwide currently live with hearing loss, and that number could reach over 2.5 billion by 2050.2\n“This supports the need to implement strategies to prevent hearing loss, which can occur by individuals making simple modifications to make their listening habits safe and through government and industry support in implementing policies focused on safe listening,” Lauren Dillard, AuD, PhD, lead author of the study and a WHO consultant, told Verywell in an email.\nDillard and her colleagues analyzed data from other studies published between 2000 and 2021 which examined personal listening devices and unsafe listening practices.\nThe analysis included 33 studies involving about 19,000 people between the ages of 12 to 34. About 24% of young people listened to music too loudly from their phones or music players, while 48% attended noisy venues that could impact their hearing. In addition, the researchers concluded that the number of young people at risk of hearing loss from exposure to these unsafe listening practices ranges between 0.67 to 1.35 billion.\nWhat Is Considered a Dangerous Noise Level?\nSound is measured in units called decibels (dB). The higher the decibel level, the louder the noise is.3 For example, normal conversations can range from 60 to 70 dB, whereas a motorcycle engine, sporting event, or concert can range from 95 to 110 dB.4\nThe National Institute on Deafness and Other Communication Disorders recommends keeping sounds at or below 70 dB. However, long or repeated exposure to sounds and noises at or above 85 dB could cause hearing loss.3 In addition, loud noise above 120 dB can cause immediate harm to your ears.4\nOver the course of an eight-hour day, the average noise level should not be more than 85 dB, per the National Institute of Occupational Safety and Health (NIOSH). Additionally, sounds that are 100 dB should not be listened to for more than 15 minutes.\nWhy Are Younger People at Higher Risk of Hearing Loss?\nYounger people are particularly vulnerable to hearing loss because of their frequent use of personal listening devices, such as smartphones, headphones, and speakers, according to Viral Tejani, AuD, PhD, an audiologist and assistant professor at Case Western Reserve University School of Medicine.\nAdolescents and young adults are also more likely to visit entertainment venues, nightclubs, concerts, movies, and sporting events. Engaging in these activities places them at risk of developing hearing loss early in life.\n“What can occur as a result of attending these events is a temporary threshold shift,” Tejani told Verywell in an email. “There is a temporary decline in hearing that recovers. However, this recovery isn’t really a true recovery.”\nTejani explained that sounds are funneled to the sensory cells—also called hair cells—in the hearing system. These hair cells are responsible for sending to our brain through complex pathways. However, “our hair cells are not always going to be resilient—repeated noise exposure will eventually compromise our hair cell function and lead to diminished hearing ability,” he said.\nLoud noise exposure can also cause tinnitus—a ringing, humming, crackling, or roaring in the ears. It can occur along with hearing loss.5\nWhat Can You Do to Prevent Hearing Loss?\nYou can still enjoy listening to music while protecting your hearing, experts say. Here are some recommendations:\n- Keep the volume of devices down (a rough estimate is below 60% of the maximum volume).\n- Try noise-canceling headphones to reduce background noise, so there is less of a need to increase the volume of the device to overcome the background noise.\n- Use disposable earplugs in loud music venues, concerts, or sporting events, and position yourself further away from the speakers.\n- Limit the amount of time you spend doing noisy activities. For example, listen to music from your device for less time per day and take breaks away from loud sounds when at an entertainment venue.\n- Monitor your listening levels through built-in safe listening features on your phones or consider sound level meter apps that can quickly measure sound levels in your environment.\n- Pay attention to warning signs of hearing loss, including ringing or buzzing in your ears and difficulties hearing high-pitched sounds or following conversations.\nIf you’re concerned about hearing loss or want to know ways to reduce your risk of hearing loss, have a conversation with your healthcare provider.\nArticle originally appeared on VeryWell']	['<urn:uuid:4f5a8a03-33fe-48d0-9a3b-5bab2e4a2d88>', '<urn:uuid:7bfc18f6-7853-46e1-bcb4-38dc1dfa0c21>']	factoid	with-premise	verbose-and-natural	similar-to-document	multi-aspect	novice	2025-05-13T01:23:33.086345	36	83	3173
44	I've been examining childhood disadvantage effects on later life outcomes - how does mastery interact with childhood financial hardship in terms of mobility limitation recovery?	Research indicates that mastery acts as a significant moderator between childhood disadvantage and recovery from mobility limitation. While economic and health disadvantage before age 16 is generally associated with lower odds of recovery from mobility limitation, mastery can help diminish the negative effects of childhood financial hardship (specifically moving for financial reasons) on recovery outcomes in later life. However, the research also highlights that low mastery is particularly harmful for those who experienced childhood disadvantage.	['Much of my recent research has explored the links between mastery and health across the life course. In general, mastery, defined as feeling like you have control over your life circumstances, is a robust predictor of better health outcomes including morbidity and mortality (Latham-Mintus & Clarke, 2017; Latham-Mintus, Vowels, & Huskins, 2017; Pearlin et al., 2007).\nMastery is thought to be an important coping resource that encourages positive health behaviors and also alleviates the effects of stress on the body. Because of the connections between mastery and health-promoting behaviors, it is often viewed as a potential avenue for intervention among older adults.\nMy own research, along with Philippa J. Clarke, has demonstrated that high levels of life course mastery is predictive of mobility device use among older adults with functional limitations (Latham-Mintus & Clarke, 2017). Older adults with a history of high life course mastery were more likely to use mobility devices such as walkers, wheelchairs, or canes in the face of functional impairment (see figure below). The use of assistive technology is a health-promoting behavior, which encourages independence and greater wellbeing among older adults. This research shows that mastery across the life course has the capacity to influence health behaviors and disease/impairment management in later life.\nMore recent research, along with Katelyn M. Aman, examined the association between childhood disadvantage and recovery from mobility limitation (i.e., difficulty walking and climbing stairs) (Latham-Mintus & Aman, 2017). In general, economic and health disadvantage before age 16 was associated with lower odds of recovery from mobility limitation. However, we also investigated whether certain psychosocial factors buffered the effect of childhood disadvantage on recovery outcomes. Mastery was a significant moderator of childhood disadvantage (i.e., moving for financial reasons) and recovery (see figure below). Mastery may be able to diminish the negative effects of financial hardship in childhood on recovery outcomes in later life. However, it also underscores the particularly harmful effects of low mastery among those with childhood disadvantage experiences.\nSource: Latham-Mintus, K. & Aman, K. M. (2017). Childhood Disadvantage, Psychosocial Resiliency, and Later Life Functioning: Linking Early-Life Circumstances to Recovery from Mobility Limitation. Journal of Aging and Health.\nWhile it appears that mastery has the capacity to improve the health and wellbeing of older adults including encouraging adoption of assistive technology and buffering the adverse effects of childhood economic disadvantage, it also important to note that feelings of mastery reflect structural conditions. In other words, those who have been exposed to discrimination and constrained economic opportunities such as people with low socioeconomic status or minorities typically report less mastery (Latham-Mintus, Vowels, & Huskins, 2017).\nFurthermore, it appears that the effect of mastery on health varies by sex and race. To illustrate, research completed with two IUPUI Sociology graduate students (Ashley Vowels and Kyle Huskins) documented that mastery was a strong predictor of healthy aging among older white men, but a weak predictor among older black men (see figure below).\nSource: Latham-Mintus, K., Vowels, A. L., & Huskins, K. (2017). Healthy aging among Black and White older American men: What is the role of mastery? The Journals of Gerontology, Series B: Psychological Sciences.\nWe posit that mastery may be a less effective coping resource for reducing stress reactivity among older black men because of limited opportunities to exert control over their environment throughout the life course. Equally, a lack of mastery may be exceptionally detrimental to older white men’s health because there is a higher expectation of having control over their environment due to structural advantages over the life course.\nIn sum, mastery may hold the potential to improve health and wellbeing of older adults; however, it is vital to recognize that mastery is not equally distributed among populations (i.e., those with marginalized statuses tend to have less mastery) and that the health-promoting effect of mastery on health is concentrated among the advantaged.\nBecause mastery develops over the life course, it has its roots in childhood. Exposing children and young adults to environments that embolden feelings of confidence and perceptions of control may have long-lasting impact on health and health behaviors in older ages. Giving individuals in early life the opportunities to achieve status attainment in home, school, or work environments and exposing them to mastery experiences may promote high levels of mastery throughout their lives. Early investments in fostering life course mastery, particularly in socially disadvantaged groups, may have larger gains for population health in the long term.\nThis should not take away from policies and interventions that seek to enhance current mastery or sense of control among older adults. Interventions aimed at improving current mastery and increasing participation in health behaviors may ameliorate the impact of childhood disadvantage. For example, previous research has documented successful interventions by empowering older adults through peer-to-peer support, increasing knowledge, and fostering increased decision-making opportunities. Enabling people, old and young, to feel empowered in their lives may be an important piece of the puzzle for future population health promotion.\nLatham-Mintus, K. & Aman, K. M. (2017). Childhood Disadvantage, Psychosocial Resiliency, and Later Life Functioning: Linking Early-Life Circumstances to Recovery from Mobility Limitation. Journal of Aging and Health.\nLatham-Mintus, K., Vowels, A. L., & Huskins, K. (2017). Healthy aging among Black and White older American men: What is the role of mastery? The Journals of Gerontology, Series B: Psychological Sciences.']	['<urn:uuid:9ff602e0-ea02-41bd-96e0-40355cb7dc75>']	open-ended	with-premise	verbose-and-natural	similar-to-document	single-doc	expert	2025-05-13T01:23:33.086345	25	75	883
45	When I'm trying to take pictures of birds flying in the sky, all my photos come out too dark - is there any way to fix this while I'm shooting?	Yes, you can use exposure compensation when photographing birds against the sky. Setting an exposure compensation value of +0.7 to +2.0 ev can often bring out the detail in the bird. The correct value can only be determined by trial and error, but this tool is particularly useful for subjects outlined against the sky or in flight.	['Before we dive into the complexities of correct exposure of a digital photograph it is perhaps appropriate to discuss how the images are saved in the camera. Most digital cameras offer a choice on the quality and file format to be used to save the images on the memory card of the camera. The file formats offered are JPEG or RAW (the RAW format is specific to each manufacturer). The basic difference between these two formats is size the JPEG file is a compressed lossy file (typically 10:1 compression) that is much smaller than the corresponding RAW file. Although the loss of image quality can be quite small information from the original file is irretrievably lost by the lossy compression. In contrast the RAW file retains all the digital information from each pixel and is therefore the format of choice if careful subsequent image processing is contemplated. After the RAW file is processed a smaller JPEG copy can then be saved. In the past it was worthwhile to save both RAW and JPEG versions in the camera so that the JPEG versions could be used for initial image selection, however, with greater computing power now available this is no longer necessary. The penalty for storing RAW images is their greater size, for a full frame 35 million pixel camera they can be more than 20 mb each and this requires storage cards in the camera of 64 gb to cover a few days intensive photography.\nThe exposure ie. the quantity of light reaching the sensor in a digital camera is a function of the aperture and the shutter speed. Modern digital cameras offer a number of different basic approaches to achieving the ideal exposure for an image and these can be accessed through some form of selector on the camera with settings labelled P, A, S or M. The P setting, which stands for Programmed Auto and not Professional Setting as I have heard it called, is really the point and shoot option. The camera adjusts aperture and shutter speed according to the ISO value (the equivalent of film speed) selected and tries to achieve a satisfactory exposure. In many situations this will produce a perfectly acceptable outcome, but for more testing situations the photographer will require more control. The A and S settings are aperture and shutter priority settings ie. on the A setting a specific aperture can be selected manually and the camera will adjust the shutter speed to achieve a suitable exposure. Similarly with the S setting a specific shutter speed can be selected. One of the best options for wildlife photography is to combine the aperture priority setting with an auto ISO facility (accessed through the menu system). The auto ISO facility allows the photographer to select a maximum and minimum value for ISO and the camera will automatically vary the value in response to the conditions. A suitable range to use for this is 640-1600, but a wider range would be perfectly acceptable. The mode of working is then to select an appropriate aperture value, depending on depth of field required and the available light and then to focus on the subject. The camera will display the appropriate shutter speed and if this too slow the aperture can be progressively opened until a balance is achieved between shutter speed and aperture. Using the auto ISO facility allows this parameter to be largely ignored while taking the photograph. The M setting on the camera is the Manual setting where aperture and shutter speed are under the control of the photographer and can be used to achieve results under special circumstances.\nHOW CAN WE TELL WHEN THE EXPOSURE IS OPTIMUM?\nWhen the photograph has been taken it can be immediately viewed on the camera screen and some judgement made, however, in the field it is often difficult to clearly assess the quality of the image exposure. Cameras have another selectable tool which displays the tonal histogram of the image, the tones range from 0 (pure black) to 255 (pure white) and individual colours (hues) are composed of Red, Green and Blue combined in varying ratios. If the peaks of the histogram are clipped at the top on the right hand end of the histogram this indicates that the bright highlights will have lost detail (blown) while a similar clipping at the left hand side indicates loss of detail in shadow. In the same way that we can select on the camera an area of the viewfinder image to be used for focussing eg. centre weighted the exposure metering area can also be selected ie spot, centre weighted or matrix . This selection can be used to obtain a satisfactory exposure for that part of the image of interest. If the results are still not perfect exposure compensation can be used. This technique affects the entire image and compensation is applied as steps either negative, to reduce exposure or positive, to increase exposure. The individual steps are usually 0.3, 0.7, 1.0, 1.3, 1.7 etc up to 5.0, although not all cameras are identical. The exposure compensation tool is particularly useful for example when trying to photograph a bird outlined against the sky or in flight (see illustrations). Setting an exposure compensation value of + 0.7 to +2.0 ev can often bring out the detail in the bird, but the correct value for the exposure compensation can only be determined by trial and error. Similarly if the subject is in sunlight and has very pale coloured areas, white or yellow, it will often be necessary to underexpose the image to retain any detail in the pale areas. As a rule of thumb more can be done to process an underexposed image than can be done with an overexposed one and once a highlight area is blown there is nothing there to retrieve.\nCAN ANYTHING BE DONE IN THE FIELD TO HELP?\nGreat Sparrowhawk – Accipiter melanoleucus photographed in Kibali National Park, Uganda.\nPhotographs showing the effect of exposure compensation to bring out the detail of a subject set against a bright background. The images are as taken from the camera without further processing.\nAlthough birds and animals are seldom obliging in where they perch, fly or stand it is possible for the photographer to think carefully about the direction of the sun and try to ensure that the subject is not between the camera and the sun. Diffuse light, thin cloud, is by far the best condition for photography, where there is sufficient light, but no hard shadows. Birds or animals in dappled light, where there is a mixture of bright sunlight and shade, makes for the most difficult photographic conditions. Photography of plants has many advantages since the plant is not moving, unless it is very windy, and the photographer can often take time to move to the most advantageous position to get the best illumination of the subject. Here again diffuse light is the best, particularly with white or yellow flowers, and it is often useful to have an assistant to stand and create a shadow or to hold a diffuser.\nMuch is often made of the photographic opportunities that are created by the warm sunlight early in the morning and at sunset and in the next article we will consider the topic of colour temperature, white balance etc.\nRoger Marchant – ABS Member']	['<urn:uuid:a8a226eb-0530-41fa-b789-f9371b6922de>']	factoid	with-premise	verbose-and-natural	distant-from-document	single-doc	novice	2025-05-13T01:23:33.086345	30	57	1227
46	why female life expectancy higher than male biological reasons	Women have a biological advantage in life expectancy due to several factors. The female hormone estrogen helps protect women from fatal conditions like cardiovascular disease by keeping harmful LDL cholesterol levels lower. Additionally, estrogen enhances the immune response and acts as an antioxidant, making women less likely to die from bacterial, viral, and parasitic infections. In contrast, male hormones testosterone and progesterone actually suppress the immune system, making men more vulnerable to infectious diseases.	['Scientists have long known that women outlive men. Learn the reasons why, and the important health screening guidelines men should follow to best ensure a long life.\nAs of the 21st century, the trend is universal: women now outlive men practically everywhere in the world.\nIn fact, at the age of 20, women are expected to live 7.6% longer than men. This jumps to 14% at the age of 80.\nThe US Census Bureau found that American men lived to be 77.3 years old while women reached 82 in 2017, and is projecting men to reach 79.7 year old alongside women’s 83.8 by 2030.\nThe research is conclusive, with one study confirming that even among seven populations around the world under harsh conditions such as famine and epidemics, women still fared better and lived longer.\nWhat explains this? It pretty much comes down to biology and behavior.\nThe biological reasons women live longer\nBy and large, scientists believe that the female hormone estrogen goes a long way in helping to protect women from fatal conditions like cardiovascular disease by keeping harmful LDL cholesterol levels lower.\nEstrogen also enhances the immune response and even acts as an antioxidant, making women less likely to die from bacterial, viral, and parasitic infections.\nOn the flip side, the testosterone and progesterone hormones found in men are thought to actually suppress the immune system, making men much more vulnerable to infectious disease.\nTestosterone may also play a role in influencing risky social behaviors among men, increasing their likelihood of exposing themselves to infection.\nBehavioral differences also explain the trend\nWomen also live longer because they are less likely to partake in risky behaviors that put their health in danger, like smoking, drinking alcohol, using drugs, or driving dangerously.\nWomen are also more likely to eat healthier, sticking to more wholesome and low-fat foods and eating less salt and meat.\nWomen also seek medical care and attend routine screening exams more often than men do, and are better about taking medications and vitamins when needed.\nMen’s top health risks and what to do about them\nMaking a few lifestyle changes is an important first step toward heading off premature death in men.\nPrevent heart disease. Eat a wide variety of fresh fruits, veggies, whole grains, and high-fiber foods, limit foods high in saturated fats, salt, and sugar, and keep active on a regular basis to help you control your weight and lower your risk for heart disease.\nWard off lung disease. Quit a smoking habit if you smoke and avoid second-hand smoke to protect your lungs.\nLimit your alcohol. Alcohol use is associated with a higher risk of cancer, so try to avoid alcohol if you can. If you choose to drink, only stick to two drinks a day if you are younger than 65, or one a day if you are older.\nManage stress. Protect your immune system by keeping your stress under control. Exercise and mindfulness meditation are two healthy, effective ways to keep your stress in check.\nVisit your doctor. Men tend to wait until a health issue turns serious before reaching out for medical care. Avoid this pitfall, and follow your doctor’s treatment plans to help maintain your health.\nHealth screening exams for men\nSeeing your doctor for regular checkups is critical for the prevention of health issues down the road, even if you feel healthy today.\nIf you are in the 40 to 64 year old age range, make sure you work with your doctor to screen for the following conditions:\nHigh blood pressure. If you have diabetes, heart disease, or a kidney condition, your blood pressure should be checked at least every year, otherwise it should be checked every 2 years to make sure you stay within the 120/80 mmHg range.\nHigh cholesterol. Cholesterol should be screened starting at age 35 for men with no risk factors for heart disease, and then 5 years after that. Men with high cholesterol, diabetes, heart disease, and kidney issues should be checked more often.\nColorectal cancer. Men aged 50 to 75 should be screened for colorectal cancer every several years, depending on the screening test used, with no strong family history of colon cancer or polyps. A family history should include earlier screening. Screening tests include colonoscopy, stool-based tests, flexible sigmoidoscopy, or a CT colonography.\nDiabetes. If you are over 44 years old, you should be screened for diabetes every 3 years, which includes checking for high BMI, blood pressure, and blood sugar.\nProstate cancer. Men who are 55 through 69 years old should discuss with their doctor whether a PSA test is recommended to screen for prostate cancer, especially when you are at higher risk.']	['<urn:uuid:8dfa0309-5b9d-483e-b479-b7e24d6269d1>']	open-ended	direct	long-search-query	distant-from-document	single-doc	novice	2025-05-13T01:23:33.086345	9	74	783
47	How do mapping capabilities combine with sustainable delivery solutions?	Mapping capabilities rely on GPS satellites to determine precise coordinates through latitude and longitude measurements, which can be displayed on mobile apps using components like map markers. This location technology intersects with sustainability efforts, as demonstrated by major delivery companies - Amazon and UPS are leading EV adoption by ordering thousands of electric delivery vehicles, though they face infrastructure challenges in rural areas where charging stations are limited. The National Highway Charging Collaborative plans to install charging stations at over 4,000 highway locations to address this limitation.	['In this exercise we will create an Android mobile app that displays your current coordinates in the app and shows your position on a map.\nWhat are co-ordinates?\nOn a map co-ordinates are measured by latitude and longitude and a number for both these measurements can pinpoint your exact position on earth.\nWhat is GPS?\nYour location on earth can be provided from GPS satellites orbiting around the earth. Mobile phones make use of these GPS satellites to determine your co-ordinates on earth.\nAndroid Location Services\nWhile we are creating our app (and anytime we want to use it) we need to enable ‘Location’ services on the android device. This is shown in the image below and enables us to get information from GPS satellites:\nCreate a New AppInventor App\nLog in to AppInventor here and make sure you have a Google account so you can login and save your projects.\nIn the top menu choose ‘Projects‘ > ‘Start New project‘. Type ‘LocateMe’ as the project name and click OK.\nWith the ‘Screen 1’ component selected, scroll down the properties panel until you find the ‘Title’ field. Change the text to ‘Locate Me’ and notice that this change is reflected in the Viewer in the centre of the page.\nWe need to place some ‘components’ on the screen so that we have something to look at. Look down the ‘Palette’ panel and drag a ‘Table Arrangement‘ onto your screen design.\nNext add some labels to the inside of the ‘table arrangement’ so that there is a label in each of the 4 cells of the table.\nFor each of the 4 labels\n- Change the ‘Text‘ in the properties panel\n- Rename the label in the ‘Components’ panel\nIf you get stuck just ask for some help from a mentor. You should end up with something like this:\nThe next step is to add a ‘Horizontal Arrangement‘ onto your screen under the labels you have already added. You will find ‘Horizontal Arrangement’ in the ‘Layout’ section of the Palette.\nSet the properties of that component so that ‘Height‘ is 50% and ‘Width‘ is set to ‘Fill parent‘.\nDrag a ‘Map‘ from the ‘Maps’ section of the Palette inside the Horizontal Arrangement.\nSet the properties of the Map as follows:\n- Height: Fill Parent\n- Width: Fill Parent\n- Zoom Level: 13\nNext click and drag a ‘Marker‘ from the Palette maps onto the map in your design.\nNext we need to add a sensor so that we can get the latitude and longitude from the phone’s GPS sensor. Drag a ‘LocationSensor‘ from the ‘Sensors‘ section of the ‘Palette’ onto your screen design. Since this is not something the user will actually see on the screen it is added underneath the design as a ‘Non-visible component’.\nYour design should now look similar to this screenshot below:\nTest Your App\nNow that you have the initial design let’s check what it looks like\nOn your Android phone or tablet make sure you have the ‘MIT AI2 Companion’ installed. Run it and then enter the code that you were given on the AppInventor website and touch the ‘Connect with code’ button:\nAs you change your app the AI Companion will also change so you can follow your changes. If you run into problems you may need to reset the connection and start MIT AI Companion again.\nNow it is time to code your app. At the top right of the screen you should see that you are in ‘Designer’ mode. Click on the ‘Blocks’ button so you can add some code blocks to your app.\nLocate your ‘Screen1’ component in the ‘Blocks’. Click it and find the block for ‘when Screen1 Initialize’. Drag that block onto the ‘Viewer’.\nWe want to hide the map until we know the person’s location so click the ‘Map’ and find the block for ‘set Map1 visible to’.\nThen drag a ‘false’ block from the ‘Logic’ section and join it to your ‘Map Visibility’ component.\nNext find the ‘set’ blocks for each of your value labels so that the text shown to the user at the start says ‘checking…’. See if you can do it so that it looks like the example below:\nNext we need to take some action when the phone GPS sensor finds the location so click on the ‘LocationSensor1’ in the component list on the left. In the pop-out menu drag the ‘when.LocaitonSensor1.LocationChanged’ block onto the Viewer.\nThen inside that block you will need to add blocks for:\n- Set Map1 to visible\n- Set the latitude label value to the discovered latitude\n- Set the longitude label value to the discovered longitude\nHint: you can get the blocks you need by hovering over the ‘latitude’ and ‘longitude’ buttons on the ‘LocationChanged’ block:\nSee if you can try that yourself to match the screenshot below:\nTest your App\nNow would be a good time to test your app and see if there are any bugs!\nWe would like to have the marker placed on the location of the user of the app so inside the ‘LocationChanged’ block add blocks for:\n- Set Marker1’s latitude to the ‘get latitude’ value\n- Set Marker2’s longitude to the ‘get longitude’ value\n- Center the Map on the user’s location with set Map1, CenterFromString. The CentreFromString block expects coordinates in the format -6.123,52.345 so you will need to use a ‘Join’ block to build that string with a comma in between.\n- You can find the ‘Join’ block under the ‘Text’ blocks. You will need to add an extra string to the join block (as shown below) so that you can have three elements for your string e.g. -6.123,52.345\nYou should end up with something like this:\nTest your app again to make sure it works!\nShare With Friends\nYou can now share your app with friends. To do so you will need to download the ‘APK’ package for your app. In the menu at the top of your screen click the ‘Build’ menu then ‘App (save .apk to my computer)’. When the apk file is downloaded you can send it to your friends and family and they will be able to run your app.\nCan you make it better?\nWhen I was building this app I sent the coordinates to a server so that the position could be stored in a database and the location could be displayed to other people.\nYou could continue to make your app better at home too. Maybe you could adapt it to turn it into a vehicle location service for a taxi or bus company. Or create an app for parents to know the location of their kids. If you need help come along to a dojo session to get some guidance.\nMaybe you could show off your new app at the ‘Coolest Projects‘ show in Dublin.', 'Electric vehicles (EVs) hold a lot of promise for the private sector — especially as consumers, who are increasingly aware of the relationship between emissions and climate change, are starting to demand eco-friendly delivery options. EV adoption, however, has been slowed down by a few different challenges — the US’s poor EV charging infrastructure in particular.\nNow, however, we’re beginning to see signs that major businesses are willing to buy into EVs, despite potential road bumps.\nHere are the businesses that are leading the way when it comes to EV adoption.\nAmazon and UPS Lead Way on EV Adoption\nTwo delivery giants — Amazon and UPS — have begun to aggressively add EVs to their delivery fleets.\nEarlier this year in January, Amazon ordered 100,000 electric delivery trucks from EV manufacturer Rivian, as well as 10,000 electric delivery rickshaws for their operations in India. Then, around the end of the month, UPS announced that it had ordered 10,000 electric trucks from the UK-based manufacturer Arrival Ltd., and would soon be teaming up with self-driving car manufacturer Waymo for a pilot test of self-driving delivery vehicles.\nThe moves are part of broader pushes towards carbon neutrality and self-driving delivery by the two companies. Last year, Amazon announced the company’s plan to be 100 percent carbon-neutral by the year 2040. UPS already offers carbon-neutral and carbon-offset delivery options.\nThe moves also come as more cities around the U.S., including New York and Philadelphia., have begun to adopt anti-idling laws that allow the city to fine companies over idling delivery vehicles.\nSome cities have even developed apps that allow citizens to report idling vehicles based on that vehicle’s DOT number — making these policies even more costly for delivery companies. Because electric vehicles produce no emissions, they’re typically free from being fined — meaning savings for businesses that adopt EVs for city deliveries.\nThe announcements are both historic. While other companies have announced EV purchases — like Lyft, which plans to deploy 200 EVs in Denver as part of its rental vehicle program there — there’s been nothing near scale of these announced by Amazon and UPS.\nWhile neither UPS nor Amazon has plans to go fully electric any time soon, the purchases are a welcome sign for the EV industry. Coupled with similar positive signals from the individual consumer side of the industry, they likely demonstrate that despite early growing pains, EVs may be on track for widespread adoption in the near future.\nChallenges Facing Further EV Adoption\nHowever, there still remain significant barriers that may slow or prevent full EV adoption, primarily the weak EV charging infrastructure in the US and limited number of charging stations — although this, too, seems like it’s starting to change.\nChargePoint, in coalition with the National Association of Truck Stop Operators (NATSO) has formed the National Highway Charging Collaborative, which plans to install new charging stations at more than 4,000 highway-side locations in the U.S., in order to increase the availability of EV charging stations in rural areas.\nAt the same time, legislative support for stronger EV infrastructure is beginning to build. In February, Democratic lawmakers in the House of Representatives announced a new bill that would create a nationwide EV charging network within the next five years.\nUpgrades to existing infrastructure would likely encourage further adoption. They may also be especially beneficial for businesses like Amazon and UPS, as both companies regularly make deliveries to rural parts of the country — areas that don’t always have the charging infrastructure needed to support EVs.\nThe Future for EVs in Business\nEV adoption in the private sector, which has lagged in the past, seems to be accelerating. Two major delivery companies have now announced that they will be adding significant numbers of EVs to their delivery fleets, with more likely to come in the near future as both pursue low-carbon delivery options.\nWhile challenges remain that may slow down EV adoption — primarily the nation’s weak EV charging infrastructure — the purchases are likely a good sign for the industry and the future of EVs in the private sector.']	['<urn:uuid:e1c88965-ac31-4e85-85b0-bd7503c6dab3>', '<urn:uuid:a3b2914f-bfcd-4107-a76d-db9ca5d880cd>']	open-ended	direct	concise-and-natural	distant-from-document	multi-aspect	expert	2025-05-13T01:23:33.086345	9	87	1829
48	What's the key difference between Interface Contracts and User Contracts?	User Contracts are owned by a specific function/object to describe how it wants to interact with a collaborator, taking a bottom-up approach where the using function dictates usage. This differs from Interface Contracts where the collaborator dictates the usage, making User Contracts more flexible when you don't want to dictate the entire object interface but still need to enforce expectations.	"['I\'m not a PHP expert but in any programming language using attribute bags as objects, there always exist functions to ensure that a given attribute is defined, and how it is defined. Essentially all the basics you need to enforce Duck Typing.\nThis does leads to a distribution of such checks throughout the code base. Which is not just complex to manage but often results in code duplication.\nThis is obviously not a good state to be in and leads to the concept of a Contract.\nImagine a function (or object with some functions on it) which given an object can verify that all relevant attributes are defined, that they are of the correct types, and those types are correctly correlated. It can ensure that every function has the correct number of arguments, order, type, etc...\nThis is the same as verifying a house by looking at it and saying:\n- 1 door... check\n- n windows... check\n- on a street... check\n- it must be a house...\nWe can go further and purposefully invoke ""Readonly"" methods to ensure that the object\'s state has not transitioned, or that other expectations are met. But this is getting awfully close to unit testing at run time.\nThe issue here is that these no-effect operations might still actually have an effect, the effect is simply not observable by the contract. Imagine for example a function that will throw an exception, this may be logged even though the contract simply discards the exception after verifying its expectations.\nKind of like walking up to the house and kicking a wall. If it isn\'t rotten or otherwise broken the foot reports that the pain is what was expected. But if it wasn\'t what you expected it to be then your foot is now stuck in the wall...\nOffline Verification - Destructive Contracts\nWhile not strictly about your runtime case - Contracts can be extended explicitly for test purposes.\nThis version actually needs a way to generate multiple instances of the object. Each being run through a series of scenarios to ensure that the higher level semantics implied by the contract are respected (eg: the item pushed onto the stack is the item popped off the stack).\nThere is no danger here, if the object misbehaves the test has failed by definition and nothing has happened in production.\nIt may be useful to supply these sorts of Protocol Contracts along with your framework. When the unexpected happens it is always useful to have a scenario laid out with exactly what should happen, and what didn\'t happen. The developer need only pass an example in to see what is wrong.\nSometimes you do not wish to dictate the entire object interface/protocol - but you still need to reasonably enforce expectations.\nThe User Contract to the rescue. This is a contract owned solely by this given function/object to describe how it wishes to interact with a given collaborator.\nThis approach is bottom up as it is not the collaborator dictating usage, but the using function/object itself.\nA Protocol Lawyer in the Middle\n- Yes, they slow things down.\n- Yes, they get in the way.\n- Yes, they are more code.\nThey can enforce a protocol between a User and one or more collaborators.\nIn essence The Protocol Lawyer holds an understanding of the protocol that is to be followed. A simple example:\n- When you received one or more collaborators you would register them with the lawyer, and use whatever object the lawyer passes back.\n- When the user code makes a call, say\n[\'name\'], the lawyer checks its own idea of what is allowed, and verifies that this call is legal.\n- After verifying that the call is legal it dispatches the call to the desired collaborator.\n- The collaborator does its thing and ships back its response, which the lawyer dutifully verifies against its own ledger of acceptable responses.\n- Once the response has been validated the lawyer hands it back to the user.\nIf at any stage the lawyer detects anyone in the protocol having acted inappropriately they can decide what to do. An obvious strategy is to stop the interactions and notify some authority by throwing an exception, logging a message, or killing the process.\nOther interventions are possible.\n- Correcting the output/request\n- Letting the transgression slide, but making a note of it.\n- Requesting some form of outside intervention.\nIt is even possible that there are several lawyers at play here, one introduced by each interested party, each verifying the interactions by their definition of the protocol going on.\nThis is perhaps not what you need, but it will ensure that the object behaves appropriately (or you know what happened and who to blame).\nIn this case you cannot possible ensure that every object complies with your expectations at runtime. However you can:\n- provide a Destructive/Protocol Contract as a testing/diagnostic tool for other developers\n- provide a lawyer to oversee that the protocol is executed correctly, or some sensible action is taken instead.\n- enforce a Destructive/Protocol Contract on known collaborators\n- Enforce either a full Interface Contract, or a User Contract on collaborators as they are passed in, or about to be used.']"	['<urn:uuid:fa554ab8-c4f5-47b3-9a2d-36ee7ee40731>']	open-ended	with-premise	concise-and-natural	similar-to-document	single-doc	expert	2025-05-13T01:23:33.086345	10	60	874
49	How do artificial brains process information to solve complex tasks, and what challenges do we face in making sure they treat everyone equally?	Artificial brains, known as neural networks, process information through layers of interconnected nodes that operate like neurons in the brain. Each node performs calculations when receiving input, and if the input exceeds a threshold value, it activates connected nodes, allowing signals to be transmitted through the network. These networks excel at analyzing data to recognize patterns and solve complex problems. However, ensuring equal treatment for everyone poses significant challenges. Bias can enter the system through training data that isn't truly representative of all populations or contains existing prejudices. Additionally, neural networks require massive amounts of data to function properly and suffer from the 'black box' effect, making it difficult to understand how they reach their conclusions. This lack of transparency can be particularly problematic when trying to ensure fairness and equal treatment for all groups.	"['What are Neural Networks?\nby Christopher Trick, on Mar 30, 2022 4:36:35 PM\nArtificial intelligence has three subfields that help it equip high-performance computers with powers that match and exceed human abilities.\nIn this blog, you\'ll learn more about how the second AI subfield, neural networks, helps HPCs analyze data to increase situational awareness and ensure optimal performance across the modern battlespace.\nWhat are neural networks?\nThe second subset of artificial intelligence, neural networks are interconnected computing nodes that operate in a manner similar to neurons in the brain.\nMost commonly found in GPUs, they utilize algorithms to recognize hidden patterns and correlations in raw data, cluster and classify it, and, over time, continue to learn and improve from it.\nOriginally, the purpose of neural networks was to act as a computational system, like the human brain, that could solve complex problems. Over time, neural networks have been used to match specific tasks like computer vision and speech recognition.\nWhy are neural networks important?\nNeural networks are suited to help people solve complex problems in real-life situations.\nThese networks are able to learn and model the relationships between inputs and outputs that are nonlinear and complex, make generalizations and inferences, model highly volatile data, and reveal hidden relationships and patterns to predict unusual events.\nHow do neural networks work?\nA neural network consists of an input layer, an output (target) layer, and a hidden layer in between the two. Each layer consists of nodes. The layers are connected, and these connections form a network of interconnected nodes.\nA node is modeled after a neuron in the human brain. Much like neurons, nodes are activated when there is significant stimuli or input--in this case, oftentimes from the CPU. The activation spreads throughout the network, creating a response (output).\nThe connections between the nodes act as synapses, enabling signals to be transmitted from one node to another. Signals are processed as they travel from the input layer to the output layer.\nWhen posed with a request or probe, the nodes run mathematical calculations to figure out if there\'s enough information to pass on to the next one. In other words, they read all the data and figure out where the strongest relationships exist.\nIf data inputs are more than a certain threshold value, then the node ""fires off"" and activates the nodes that it is connected to.\nAs the number of hidden layers within a neural network increases, deep learning neural networks are formed.\nDeep learning takes simple neural networks to the next level. Using hidden layers, data scientists can build their own deep learning networks that enable machine learning. The computer can also learn on its own by recognizing patterns in many layers of processing.\nWhat are the types of neutral networks?\nThere are four different types of neural networks: convolutional neural networks (CNNs), recurrent neural networks (RNNs), feedforward neural networks, and autoencoder neural networks.\n- Convolutional neural networks (CNNs): These contain five types of layers: input, convolution, pooling, fully connected, and output. Each layer has its own purpose, such as summarizing, connecting, or activating. CNNs have popularized image classification and object detection, and they have also been applied to other areas like natural language processing and forecasting.\n- Recurrent neural networks (RNNs): These use sequential information like time-stamped data from a sensor device or a spoken sentence, composed of a sequence of items. Unlike traditional neural networks, all inputs to an RNN are not independent of each other, and the output for each element depends on the computations of preceding elements. RMMs are used in forecasting, time series applications, sentiment analysis, and other text applications.\n- Feedforward neural networks: These are networks in which each perceptron in one layer is connected to every perceptron from the next layer. Information is fed forward from one layer to the next in the forward direction only, and there are no feedback loops.\n- Autoencoder neural networks: These are used to create abstractions called encoders, created from a given set of inputs. Though these are similar to traditional neural networks, these networks seek to model the inputs themselves, and therefore, the method is considered unsupervised. The primary purpose of autocoders is to desensitize the irrelevant and sensitize the relevant. As layers are added, further abstractions are formulated at higher layers--those closest to the point at which a decoder layer is introduced. These abstractions can be used by linear or nonlinear classifiers.\nSource: druva.com. There are four different types of neural networks: convolutional neural networks (CNNs), recurrent neural networks (RNNs), feedforward neural networks, and autoencoder neural networks.\nWhat are the downsides of neural networks?\nThough neural networks can help enhance compute power, there are some drawbacks as well.\n- The ""Black Box"" effect: Perhaps the most widely-known disadvantage of neural networks, this refers to when a neural network comes up with a certain output that does not seem to correlate with any inputs. For example, if you put an image of a rabbit into a neural network, and it predicts it to be a house, it\'s very hard to see how this conclusion was reached. Since interoperability is critical in many domains, the ""black box"" effect can pose serious problems, especially when dealing with customers.\n- They take a long time to develop: When trying to solve a problem that no one has attempted before, you need more control over the machine learning algorithm. Although tools like Tenserflow may help, they can also make the process more complicated, and, depending on the project, development can take much longer. This raises the question of whether it is worth spending the time and money to develop something that can potentially be solved in a much simpler way.\n- Massive amounts of data are needed: Typically, neural networks require much more data than traditional machine learning algorithms. This can be a difficult problem to deal with, since many ML problems can be solved with less data if you use other algorithms. Though there are exceptions, most neural networks perform poorly with little data. If possible, it is best to use a simpler algorithm that does not need a lot of data to solve problems.\n- Training them takes a lot of time and computational power: Deep learning algorithms that train really deep neural networks can take several weeks to train completely from scratch. In contrast, most machine learning algorithms take just a few minutes to a few days to train. In addition, the more layers of neural networks there are, the more compute power is needed to train them.\nNeural networks are critical in enhancing situational awareness, recognizing objects and people, and strengthening surveillance and risk detection.\nThese networks rely on inputs from CPUs to perform parallel processing within GPUs to analyze data in-real time and solve complex problems.\nThrough interpreting data and extracting information, neural networks aid in helping autonomous vehicles, weapons, and drones detect risk to effectively track and engage with enemy threats, matching and exceeding human capabilities.\nOur high-performance compute solutions are designed to improve the signal integrity between each component and, therefore, the speed at which AI can run is unmatched, both at the tactical edge and in climate-controlled environments.Source:', 'Bias in Machine Learning\nFairness in Machine Learning\nSo far, this course showed how machine learning can enhance your work, from saving precious time on existing tasks to opening up new opportunities. ML can do a lot for you, but it comes with challenges you shouldn\'t overlook.\nTo address those challenges, a growing number of researchers and practitioners focus on the topic of ""fairness"" in machine learning. Its guiding principle is that ML should equally benefit everyone, regardless of the societal categories that structure and impact our lives.\nWhat is bias?\nWhat are the negative consequences that might derive from the use of machine learning? The short answer is: Bias.\nAs humans, we all have our biases. They are tools our brain uses to deal with the information that is thrown at it every day.\nTake this example: close your eyes and picture a shoe. Most likely you pictured a sneaker. Maybe a leather men\'s shoe. It\'s less likely that you thought of a high-heeled women\'s shoe. We may not even know why but each of us is biased toward one shoe over the others.\nNow imagine that you want to teach a computer to recognise a shoe. You may end up exposing it to your own bias. That\'s how bias happens in machine learning. Even with good intentions, it\'s impossible to separate ourselves from our own biases.\nThree types of bias\nThere are different ways in which our own biases risk to become part of the technology we create:\nTake the example before: if we train a model to recognise shoes with a dataset that includes mostly pictures of sneakers, the system won\'t learn to recognise high heels as shoes.\nIf you train a ML system on what a scientist looks like using pictures of famous scientists from the past, your algorithm will probably learn to associate scientists with men only.\nSay you\'re training a model to recognise faces. If the data you use to train it over-represents one population, it will operate better for them at the expense of others, with potentially racist consequences.\nSo what can we do to avoid these biases?\nAsking the right questions to avoid bias\nAs a journalist, a first line of defence against bias is firmly within your reach: the same values and ethical principles you apply every day in your profession should extend to assessing the fairness of any new technology that is added to your toolbox. Machine learning is no exception.\nFurthermore, in all cases you should start by considering whether the consequences might negatively impact individuals’ economic or other important life opportunities. This is critical especially if the data you use includes sensible personal information.\nOften, the unfair impact isn\'t immediately obvious, but requires asking nuanced social, political and ethical questions about how your machine learning system might allow bias to creep in.\nConsidering the main sources of bias\nWhile no training data will ever be perfectly ‘unbiased’, you can greatly improve your chances of building a fair model if you carefully consider potential sources of bias in your data, and take steps to address them.\nThe most common reason for bias creeping in is when your training data isn\'t truly representative of the population that your model is making predictions on. You must make sure to have enough data for each relevant group.\nA different kind of bias manifests itself when some groups are represented less positively than others in the training data. You should consider reviewing your data before using it to train a model, in order to verify whether it carries any prejudices that might be learned and reproduced by the algorithm.\nPreventing bias: it starts with awareness\nBias can emerge in many ways: from training datasets, because of decisions made during the development of a machine learning system, and through complex feedback loops that arise when a ML system is deployed in the real world.\nSome concrete questions you might want to ask in order to recognize potential bias include:\n- For what purpose was the data collected?\n- How was the data collected?\n- What is the goal of using this set of data and this particular algorithm?\n- How was the source of data assessed?\n- How was the process of data analysis defined before the analysis itself?\nBias is a complex issue and there is no silver bullet. The solution starts with awareness and with all of us being mindful of the risks and taking the right steps to minimise them.\nGoogle Translate: Translations on-the-go.lesson 5 minutes BeginnerSpeak the language just about anywhere in the world.\nGoogle Consumer Surveys: Gain real insights.lesson 5 minutes BeginnerGather data from real people by creating your own web surveys.\nDataset Search Quickstart Guidelesson 5 minutes BeginnerSearch across millions of datasets on the web']"	['<urn:uuid:73e870d7-ab33-439f-b50c-b01d5f8727b9>', '<urn:uuid:74606cbf-aac5-400f-b898-ec82b04a0f8b>']	open-ended	direct	verbose-and-natural	distant-from-document	multi-aspect	novice	2025-05-13T01:23:33.086345	23	135	1993
50	functional brain mapping pet ct scanner mri which better studying brain activity	MRI techniques are better for studying brain activity through functional brain mapping, as they can identify areas of increased neuronal activity in the brain. While PET/CT scanners combine functional imaging with detailed anatomic imaging, MRI is specifically noted for being one of the few imaging tools that can see through the skull and deliver high-quality pictures of the brain's delicate soft tissue structures, making it particularly valuable for brain function studies including cognition.	"['Research in biomedical imaging and bioengineering is yielding remarkable capabilities for unraveling the complexity of biologic systems, eliminating long-standing barriers in basic medical science, and providing powerful new tools to improve health care. 1. Imaging encompasses more than any one discipline or any one level of analysis. It brings together researchers from many disciplines to address unmet needs and advance discovery. 2. Interdisciplinary research teams that foster pathways for progress across the discovery-development-delivery continuum of biomedical research are crucial. 3. Government support of research in biomedical imaging is critical to the growth of new knowledge in the biomedical sciences. 4. There is a need for scientific societies to collaborate with industry and the different federal agencies to develop standards for quantitative measurements for drug response, by using anatomic, functional, and molecular imaging methods. 5. The ability of imaging technology to interrogate at the cellular level has facilitated investigations about fundamental biological processes, as well as specific disease pathophysiology. 6. The ability of MR imaging techniques to enable identification of areas of increased neuronal activity in the brain has opened the field of functional brain mapping as an important approach to the study of brain function, including cognition. 7. US imaging is a versatile technology that continues to advance in many areas, including the development of novel contrast agents that will offer unique capabilities to probe biological systems. 8. Optical imaging probes that respond to cellular activity and emit near-infrared wave lengths offer the ability to track molecular activity within the cell. 9. Engineering advances in electron microscopy continue to push the limits of spatial resolution. Tomographic techniques have been developed to allow three-dimensional imaging at cellular and molecular levels. 10. Fusion imaging, led by PET/CT scanners, combines the functional properties of radionuclide imaging with the detailed anatomic imaging of CT. Other combinations of imaging modalities promise further enhancements to improve our ability to predict the biologic behavior of tissue. 11. The development of new imaging probes - radiopharmaceuticals, microbubbles, nanoparticles, and reporter molecules - will enhance the scope and specificity of imaging technologies. 12. The dramatic increasing in imaging data generated by imaging technologies has been made possible by improvements in computational capacity. More powerful algorithms for extracting information from the raw data must be developed. 13. In preclinical translational research, imaging can enhance the use of animal models for validating new targeted probes and illuminating key physiologic differences between animal models and humans. 14. Biomedical imaging can identify surrogate endpoints that, when combined with computational simulations, will predict the effectiveness of a therapeutic approach much earlier than traditional clinical trials, thereby reducing the costs and shortening the time to the delivery of effective agents into the marketplace. 15. Imaging has become essential not only for the detection and monitoring of disease but also for intervention. Methods of acquiring, analyzing, and displaying this information in real time during the intervention must be improved. 16. Methods to assess the effect of imaging on patient outcomes are needed.\nASJC Scopus subject areas\n- Radiology Nuclear Medicine and imaging', 'Magnetic resonance imaging\nMagnetic resonance imaging (MRI) is one of the newest diagnostic medical imaging technologies that uses strong magnets and pulses of radio waves to manipulate the natural magnetic properties in the body to generate a visible image. In the field of mental health, an MRI scan may be used when a patient seeks medical help for symptoms that could possibly be caused by a brain tumor. These symptoms may include headaches, emotional abnormalities, or intellectual or memory problems. In these cases, an MRI scan may be performed to ""rule out"" a tumor, so that other tests can be performed in order to establish an accurate diagnosis .\nMRI was developed in the 1980s. Its technology has been developed for use in magnetic resonance angiography (MRA), magnetic resonance spectroscopy (MRS), and, more recently, magnetic resonance cholangiopancreatography (MRCP). MRA was developed to study blood flow, whereas MRS can identify the chemical composition of diseased tissue and produce color images of brain function. MRCP is evolving into a non-invasive potential alternative for the diagnostic procedure endoscopic retrograde cholangiopancreatography (ERCP).\nDETAIL. MRI creates precise images of the body based on the varying proportions of magnetic elements in different tissues. Very minor fluctuations in chemical composition can be determined. MRI images have greater natural contrast than standard x rays, computed tomography scan (CT scan), or ultrasound, all of which depend on the differing physical properties of tissues. This sensitivity allows MRI to distinguish fine variations in tissues deep within the body. It is also particularly useful for spotting and distinguishing diseased tissues (tumors and other lesions) early in their development. Often, doctors prescribe an MRI scan to investigate more fully earlier findings of other imaging techniques.\nSCOPE. The entire body can be scanned, from head to toe and from the skin to the deepest recesses of the brain. Moreover, MRI scans are not obstructed by bone, gas, or body waste, which can hinder other imaging techniques. (Although the scans can be degraded by motion such as breathing, heartbeat, and bowel activity.) The MRI process produces cross-sectional images of the body that are as sharp in the middle as on the edges, even of the brain through the skull. A close series of these twodimensional images can provide a three-dimensional view of the targeted area. Along with images from the cross-sectional plane, the MRI can also provide images sagitally (from one side of the body to the other, from left to right for example), allowing for a better three-dimensional interpretation, which is sometimes very important for planning a surgical approach.\nSAFETY. MRI does not depend on potentially harmful ionizing radiation, as do standard x ray and computed tomography scans. There are no known risks specific to the procedure, other than for people who might have metal objects in their bodies.\nDespite its many advantages, MRI is not routinely used because it is a somewhat complex and costly procedure. MRI requires large, expensive, and complicated equipment, a highly trained operator, and a doctor specializing in radiology. Generally, MRI is prescribed only when serious symptoms or negative results from other tests indicate a need. Many times another test is appropriate for the type of diagnosis needed.\nDoctors may prescribe an MRI scan of different areas of the body.\nBRAIN AND HEAD. MRI technology was developed because of the need for brain imaging. It is one of the few imaging tools that can see through bone (the skull) and deliver high-quality pictures of the brain\'s delicate soft tissue structures. MRI may be needed for patients with symptoms of a brain tumor, stroke , or infection (like meningitis). MRI may also be needed when cognitive or psychological symptoms suggest brain disease (like Alzheimer\'s or Huntington\'s diseases, or multiple sclerosis), or when developmental retardation suggests a birth defect. MRI can also provide pictures of the sinuses and other areas of the head beneath the face. In adult and pediatric patients, MRI may be better able to detect abnormalities than compared to computed tomography scanning.\nSPINE. Spinal problems can create a host of seemingly unrelated symptoms. MRI is particularly useful for identifying and evaluating degenerated or herniated spinal discs. It can also be used to determine the condition of nerve tissue within the spinal cord.\nJOINT. MRI scanning is most commonly used to diagnose and assess joint problems. MRI can provide clear images of the bone, cartilage, ligament, and tendon that comprise a joint. MRI can be used to diagnose joint injuries due to sports, advancing age, or arthritis. MRI can also be used to diagnose shoulder problems, such as a torn rotator cuff. MRI can also detect the presence of an otherwise hidden tumor or infection in a joint, and can be used to diagnose the nature of developmental joint abnormalities in children.\nSKELETON. The properties of MRI that allow it to see through the skull also allow it to view the inside of bones. Accordingly, it can be used to detect bone cancer, inspect the marrow for leukemia and other diseases, assess bone loss (osteoporosis), and examine complex fractures.\nHEART AND CIRCULATION. MRI technology can be used to evaluate the circulatory system. The heart and blood flow provides a good natural contrast medium that allows structures of the heart to be clearly distinguished.\nTHE REST OF THE BODY. Whereas computed tomography and ultrasound scans satisfy most chest, abdominal, and general body imaging needs, MRI may be needed in certain circumstances to provide better pictures or when repeated scanning is required. The progress of some therapies, like liver cancer therapy, needs to be monitored, and the effect of repeated x-ray exposure is a concern.\nMRI scans and metal\nMRI scanning should not be used when there is the potential for an interaction between the strong MRI magnet and metal objects that might be embedded in a patient\'s body. The force of magnetic attraction on certain types of metal objects (including surgical steel) could move them within the body and cause serious injury. Metal may be embedded in a person\'s body for several reasons.\nMEDICAL. People with implanted cardiac pacemakers, metal aneurysm clips, or who have broken bones repaired with metal pins, screws, rods, or plates must tell their radiologist prior to having an MRI scan. In some cases (like a metal rod in a reconstructed leg), the difficulty may be overcome.\nINJURY. Patients must tell their doctor if they have bullet fragments or other metal pieces in their body from old wounds. The suspected presence of metal, whether from an old or recent wound, should be confirmed before scanning.\nOCCUPATIONAL. People with significant work exposure to metal particles (working with a metal grinder, for example) should discuss this with their doctor and radiologist. The patient may need prescan testing—usually a single, regular x ray of the eyes to see if any metal is present.\nChemical agents designed to improve the picture or allow for the imaging of blood or other fluid flow during MRA may be injected. In rare cases, patients may be allergic to, or intolerant of, these agents, and these patients should not receive them. If these chemical agents are to be used, patients should discuss any concerns they have with their doctor and radiologist.\nThe potential side effects of magnetic and electric fields on human health remain a source of debate. In particular, the possible effects on an unborn baby are not well known. Any woman who is, or may be, pregnant, should carefully discuss this issue with her doctor and radiologist before undergoing a scan.\nAs with all medical imaging techniques, obesity greatly interferes with the quality of MRI.\nIn essence, MRI produces a map of hydrogen distribution in the body. Hydrogen is the simplest element known, the most abundant in biological tissue, and one that can be magnetized. It will align itself within a strong magnetic field, like the needle of a compass. The earth\'s magnetic field is not strong enough to keep a person\'s hydrogen atoms pointing in the same direction, but the superconducting magnet of an MRI machine can. This comprises the magnetic part of MRI.\nOnce a patient\'s hydrogen atoms have been aligned in the magnet, pulses of very specific radio wave frequencies are used to knock them back out of alignment. The hydrogen atoms alternately absorb and emit radio wave energy, vibrating back and forth between their resting (magnetized) state and their agitated (radio pulse) state. This comprises the resonance part of MRI.\nThe MRI equipment records the duration, strength, and source location of the signals emitted by the atoms as they relax and translates the data into an image on a television monitor. The state of hydrogen in diseased tissue differs from healthy tissue of the same type, making MRI particularly good at identifying tumors and other lesions.\nA single MRI exposure produces a two-dimensional image of a slice through the entire target area. A series of these image slices closely spaced (usually less than half an inch) makes a virtual three-dimensional view of the area.\nRegardless of the exact type of MRI planned, or area of the body targeted, the procedure involved is basically the same. In a special MRI suite, the patient lies down on a narrow table and is made as comfortable as possible. Transmitters are positioned on the body and the table moves into a long tube that houses the magnet. The tube is as long as an average adult lying down, and is open at both ends. Once the area to be examined has been properly positioned, a radio pulse is applied. Then a twodimensional image corresponding to one slice through the area is made. The table then moves a fraction of an inch and the next image is made. Each image exposure takes several seconds and the entire exam will last anywhere from 30 to 90 minutes. During this time, the patient must remain still as movement can distort the pictures produced.\nDepending on the area to be imaged, the radio-wave transmitters will be positioned in different locations.\n- • For the head and neck, a helmet-like covering is worn on the head.\n- • For the spine, chest, and abdomen, the patient will be lying on the transmitters.\n- • For the knee, shoulder, or other joint, the transmitters will be applied directly to the joint.\nAdditional probes will monitor vital signs (like pulse, respiration, etc.) throughout the test.\nThe procedure is somewhat noisy and can feel confining to many patients. As the patient moves through the tube, the patient hears a thumping sound. Sometimes, music is supplied via earphones to drown out the noise. Some patients may become anxious or feel claustrophobic while in the small, enclosed tube. Patients may be reassured to know that throughout the study, they can communicate with medical personnel through an intercom-like system.\nRecently, open MRIs have become available. Instead of a tube open only at the ends, an open MRI also has opening at the sides. Open MRIs are preferable for patients who have a fear of closed spaces and become anxious in traditional MRI machines. Open MRIs can also better accommodate obese patients, and allow parents to accompany their children during testing.\nIf the chest or abdomen is to be imaged, the patient will be asked to hold his to her breath as each exposure is made. Other instructions may be given to the patient as needed. In many cases, the entire examination will be performed by an MRI operator who is not a doctor. However, the supervising radiologist should be available to consult as necessary during the exam, and will view and interpret the results sometime later.\nMagnetic resonance spectroscopy (MRS) is different from MRI because MRS uses a continuous band of radio wave frequencies to excite hydrogen atoms in a variety of chemical compounds other than water. These compounds absorb and emit radio energy at characteristic frequencies, or spectra, which can be used to identify them. Generally, a color image is created by assigning a color to each distinctive spectral emission. This comprises the spectroscopy part of MRS. MRS is still experimental and is available only in a few research centers.\nDoctors primarily use MRS to study the brain and disorders like epilepsy, Alzheimer\'s disease , brain tumors, and the effects of drugs on brain growth and metabolism. The technique is also useful in evaluating metabolic disorders of the muscles and nervous system.\nMagnetic resonance angiography (MRA) is another variation on standard MRI. MRA, like other types of angiography, looks specifically at fluid flow within the blood (vascular) system, but does so without the injection of dyes or radioactive tracers. Standard MRI cannot make a good picture of flowing blood, but MRA uses specific radio pulse sequences to capture usable signals. The technique is generally used in combination with MRI to obtain images that show both vascular structure and flow within the brain and head in cases of stroke, or when a blood clot or aneurysm is suspected.\nMRI technology is also being applied in the evaluation of the pancreatic and biliary ducts in a new study called magnetic resonance cholangiopancreatography (MRCP). MRCP produces images similar to that of endoscopic retrograde cholangiopancreatography (ERCP), but in a non-invasive manner. Because MRCP is new and still very expensive, it is not readily available in most hospitals and imaging centers.\nIn some cases (such as for MRI brain scanning or MRA), a chemical designed to increase image contrast may be given immediately before the exam. If a patient suffers from anxiety or claustrophobia, drugs may be given to help the patient relax.\nThe patient must remove all metal objects (watches, jewelry, eye glasses, hair clips, etc.). Any magnetized objects (like credit and bank machine cards, audio tapes, etc.) should be kept far away from the MRI equipment because they can be erased. The patient cannot bring any personal items such as a wallet or keys into the MRI machine. The patient may be asked to wear clothing without metal snaps, buckles, or zippers, unless a medical gown is worn during the procedure. The patient may be asked not to use hair spray, hair gel, or cosmetics that could interfere with the scan.\nNo aftercare is necessary, unless the patient received medication or had a reaction to a contrast agent. Normally, patients can immediately return to their daily activities. If the exam reveals a serious condition that requires more testing or treatment, appropriate information and counseling will be needed.\nMRI poses no known health risks to the patient and produces no physical side effects. Again, the potential effects of MRI on an unborn baby are not well known. Any woman who is, or may be, pregnant, should carefully discuss this issue with her doctor and radiologist before undergoing a scan.\nA normal MRI, MRA, MRS, or MRCP result is one that shows the patient\'s physical condition to fall within normal ranges for the target area scanned.\nGenerally, MRI is prescribed only when serious symptoms or negative results from other tests indicate a need. There often exists strong evidence of a condition that the scan is designed to detect and assess. Thus, the results will often be abnormal, confirming the earlier diagnosis. At that point, further testing and appropriate medical treatment is needed. For example, if the MRI indicates the presence of a brain tumor, an MRS may be prescribed to determine the type of tumor so that aggressive treatment can begin immediately without the need for a surgical biospy.\nFaulkner, William H. Tech\'s Guide to MRI: Basic Physics, Instrumentation and Quality Control. Malden: Blackwell Science, 2001.\nFischbach, F. T. A Manual of Laboratory and Diagnostic Tests. 6th Edition. Philadelphia: Lippincott, 1999.\nGoldman, L., and Claude Bennett, eds. Cecil Textbook of Medicine. 21st Edition. Philadelphia: W. B. Saunders, 2000: pp 977–970.\nKevles, Bettyann Holtzmann. Naked to the Bone: Medical Imaging in the Twentieth Century. New Brunswick, NJ: Rutgers University Press, 1997.\nRoth, Carolyn K. Tech\'s Guide to MRI: Imaging Procedures, Patient Care and Safety. Malden: Blackwell Science,2001.\nZaret, Barry L., and others, eds. The Patient\'s Guide to Medical Tests. Boston: Houghton Mifflin Company,1997.\nCarr-Locke, D., and others, ""Technology Status Evaluation: Magnetic Resonance Cholangiopancreatography."" Gastrointestinal Endoscopy (June 1999): 858–61.\nAmerican College of Radiology. 1891 Preston White Drive, Reston, VA 22091. (800) ACR-LINE. <http://www.acr.org> .\nAmerican Society of Radiologic Technologists. 15000 Central Avenue SE, Albuquerque, NM 87123–3917. (505) 298–4500. <http://www.asrt.org> .\nKurt Richard Sternlof Laith Farid Gulli, M.D.']"	['<urn:uuid:e295ddca-338a-4c1d-ad86-b1f0210a37d5>', '<urn:uuid:c23f8c45-ce0d-4a42-bd0a-3f015563ddc7>']	factoid	direct	long-search-query	distant-from-document	comparison	novice	2025-05-13T01:23:33.086345	12	73	3235
51	Can regular alkaline batteries be thrown in the trash?	While alkaline batteries (AA, AAA, D) made after 1996 technically can be thrown in trash individually since they don't contain mercury, recycling is highly recommended to keep heavy metals out of air, soil and waterways. Batteries Plus accepts alkaline batteries for recycling for $1.49 per pound. The service helps conserve resources as recovered plastic and metals can be used to make new batteries.	['Teri Kent runs Charlottesville’s Better World Betty, a non-profit organization and online resource for locals looking to shrink their impact on the environment. Every month, Betty—Kent’s ’50s-housewife-meets-earth-goddess alter ego —answers the most burning eco-questions from our readers about energy use, water, waste and recycling, transportation, and green buying.\nQuestion: I’m looking for a way to recycle my running and cycling apparel, but it appears that Brooks and Pearl Izumi don’t have any recycling programs. (I checked.) Can you help me find a solution?\nAnswer: Unfortunately, not all clothing manufacturers are created equal when it comes to environmental stewardship and corporate social responsibility. However, recent clothing recycling initiatives from retailers like H&M and Marks and Spencer give me hope.\nNorth Face has launched its “Clothes the Loop” in 10 of its stores. Bring in any unwanted apparel or footwear and receive a store voucher. The company has partnered with the Conservation Alliance, which will turn the material into products like insulation, carpet padding, toy stuffing, and even new clothing. Only thing: Our nearest participating store is in Washington, D.C. So unless you’re headed that direction, take your unwanted apparel to one of our many consignment stores or thrift shops (find a listing in Betty’s directory at www.betterworldbetty.org under “C” for clothing).\nQ: Can the black plastic containers that Nintendo DS games come in be recycled? (The games, not the game system). I can’t find a recycling number on the boxes. I hate the idea of throwing them in the trash.\nA: It’s hard to say without seeing the container, but I’m afraid if it doesn’t have the recycle symbol with a number in the middle, you have no choice but to landfill. The good news is that new Nintendo game packaging is recyclable. And as far as the consoles and game itself, the company has a take-back program. Nintendo partners with an R2-certified national recycler that meets its corporate social responsibility goals. For more information check out www.nintendo. com/consumer/recycle.jsp.\nQ: Our lives are filled with so many batteries of all shapes, types, and sizes—regular household batteries, camera and cell phone batteries, and others. What’s a Betty to do with them all? I buy rechargeables now, but still have plenty of batteries that I don’t know what to do with at the end of their lifespan.\nA: Regular household batteries like AAs, AAAs, and Ds are alkaline batteries. Since 1996, the EPA has required manufacturers to stop adding mercury to these batteries, so some say it’s fine to throw them into the trash individually (sometimes these batteries aren’t completely dead, so if they come into contact with other “live” batteries, it could be hazardous).\nHowever, I highly recommend recycling all your batteries, as this keeps heavy metals out of our air, soil and waterways and also conserves valuable resources because recovered plastic and metals can be used to make new batteries. Emmet Street retailer Batteries Plus, which uses EPA-certified recyclers, will accept them for a small fee of $1.49 per pound. The liquid nickel cadmium, mercury, primary lithium, and zinc-air batteries recycling fee is $5.49 per pound.\nRechargeables, which include the regular AA and AAA variety and lithium-ion, lithium-polymer, nickel cadmium, nickel metal hydride, and silver oxide batteries definitely should be recycled at the end of their lifespan. Batteries Plus accepts those for free. They also accept lead automotive batteries, as do area auto parts retailers.\nAlso good to know: Batteries Plus recycles light bulbs, too.']	['<urn:uuid:8b46f602-d2a3-4cf7-aedb-7de10dd69cc6>']	open-ended	direct	concise-and-natural	similar-to-document	single-doc	expert	2025-05-13T01:23:33.086345	9	63	574
52	How is plasterboard installed on walls, and what fire safety benefits does it provide?	Plasterboard is installed on walls through drylining, typically using timber battening at 600mm centers vertically for 12.7mm wallboard. The process involves setting out the framework, fixing floor and ceiling battens, then mounting vertical battens with proper alignment. Boards are fixed using galvanized nails at 150mm centers, leaving slight gaps between adjacent boards. Regarding fire safety, plasterboard provides significant passive fire suppression due to its gypsum content (CaSO₄∙2H₂O), which is non-combustible. When exposed to fire, the crystalline water in gypsum (21% by weight) converts to steam, absorbing heat and keeping the opposite side cool until the water is depleted or the panel is breached.	['Drylining means cladding a wall or ceiling with a prefabricated building board called plasterboard which consists of a layer of aerated gypsum plaster sandwiched between two sheets of heavy-duty paper. Plasterboard comes in several different forms suitable for a variety of cladding tasks, and is used in many forms of modern building and renovation in place of plastered finishes.\nPlasterboard is quickly and easily fixed into place and, when properly jointed and finished, equals the smooth and appealing looks of a plastered finish. By comparison, plastering requires considerable practice and skill, and is probably the most difficult of the trade skills the handyman is ever likely to attempt. A further significant advantage of drylining walls is that almost no drying-out time is necessary—which means you can decorate almost as soon as jointing is complete.\nPlasterboard is available in different forms, the most common being wall-board, a general-purpose gypsum plasterboard suitable for building partitions and for drylining walls and ceilings. It is suitable only for internal use and in situations that are not subjected to continuously damp or humid conditions—such as a badly-ventilated bathroom or washroom.\nWallboard is manufactured with three different types of long edge. Tapered-edged board leaves a small recess at the contact region of two boards; this is filled and smoothed . for a seamless joint which then needs only light decoration or painting for final finishing. Square-edged board for cover strip jointing, and bevelled-edge board which is V-jointed, are normally used when it is almost certain that wallpapering is to be used for decoration. The surface is primed before wallpapering.\nThe taper is, in fact, very slight and hardly noticeable. This makes tapered-edge board suitable for general purpose use even when the plasterboard construction employs exposed butt-joints, such as at corners.\nWallboard for drylining has an ivory-coloured surface on one side, and this requires the minimum amount of preparation for most forms of decoration. The reverse side, normally mounted inwards against the wall, is grey. Mounted outwards, this surface is used when a skim coat of finish plaster is to be used for decoration, a typical situation being to match an unusual plasterwork pattern in the remainder of the room. Light designs can be moulded or brushed in relief on the skim coat while this is still in the plastic state.\nWallboard is usually available in sheet widths of 600i.im and 1200mm, thicknesses of 9.5mm or 12.7mm, and lengths normally between 1800mm and 3000mm. You may have to place a special order for the larger board sizes.\nInsulating wallboard is the first of several variations on the standard board and differs only in having a veneer of aluminium foil on one surface, which is mounted towards the cavity to reduce both cold penetration and room heat loss. A far superior form is thermal wallboard consisting of standard plasterboard bonded to a backing of expanded polystyrene, available with or without a vapour check paper membrane sandwiched between the two, and in various thicknesses. It is used to line external walls and roofs where thermal insulation is the main requirement of lining. It has one distinct advantage over the others in that it can be fixed directly to a flat wall, a much-employed method of providing low-cost insulation to a masonry wall.\nA useful board for drylining external walls and roofspace ceilings is vapour check wallboard which consists of standard plasterboard with a backing of polyethylene film.\nAlthough standard plasterboard has good fire safety characteristics, some situations call for better fire resistance. A special wallboard, consisting of a core of aerated gypsum plaster incorporating glass fibre and vermic-ulite is available for such cases.\nTwo other types of board can be useful for some work: plank and baseboard. Plank simply describes an especially thick form of plasterboard. Because of its thickness, plank is especially useful in certain laminated partition systems. Baseboard is, as suggested by its name and its two grey surfaces, designed specifically as a base for further plastering. Lath is a thin-width form of baseboard. Tapered-edge wallboard and thermal insulating wallboard are considered adequate for most do-it-yourself work, and you may have difficulty in obtaining other than these from your local supplier.\nDrylining with timber battening\nThere are several methods of drylining a wall with plasterboard. Wallboard is normally mounted onto some form of timber framework—for example, the studs of a timber-framed wall. On masonry walls timber battens can be used; an alternative, covered later, is metal furring or a system of support pads.\nTimber battening is the most popular method with the home handyman on account of its relatively low cost, but alignment problems caused by skewing framework can be a particular problem unless special care is taken when dealing with walls which are of less than perfect trueness. Good quality, properly seasoned timber must be used, in conjunction with sound, dry walls.\nFor old walls, start work by stripping all the old plaster from the walls unless this is perfectly sound. Remove all surface fixtures, marking the nearby wall or ceiling with their position so they can be replaced after the drylining.\nConcealed pipework and electrical cables or conduits must be arranged before framework is fixed, with chasing of the wall if the cavity depth is insufficient. Bear in mind the total thickness of the wallboard and timber battening when accounting for new fixtures and wiring lengths.\nCarefully plan out the framework arrangement that best suits the room features, bearing in mind that the wallboard sheets you use have to be supported at centres not exceeding 450mm or 600mm. Wallboard is usually fixed vertically, and the board width must correspond with the spacing of the vertical support framework while at the same time observing these recommended maximums. Plan on starting work from one particular end of the wall in order to transverse the wall in complete sheet widths, if necessary ending with a cut edge which can be disguised by the butt joint of an internal corner.\nTimber battening—good quality 50 mm x 25mm must be used for wall dry-lining—spaced at 600mm centres vertically would be sufficient for 12.7mm wallboard. Provide additional supports around window and door frames.\nHeavy items like sinks and cupboards must be mounted through the drylining and fixed securely to some form of inlaid timber support. If the fixing centres for these cannot be arranged to co-incide with the drylining support framework, you must consider providing further noggins and struts.\nMethods of construction around corners and frames may also influence the precise framework de- sign, and should be borne in mind.\nYou will find that a sketch of your proposed design will be very useful in subsequent stages.\nVery few walls are truly even, and to lay battening on these without further ado is asking for trouble when it comes to fixing plasterboard on top.\nSetting out is the procedure for making corrections to the effective lie of the wall so that one common level is adopted across the whole area, regardless of all the minor imperfections that may or may not exist beneath what is then taken to be the high point of the wall.\nStart by marking the wall at points which by plans and measurement corresponds with the board widths you have chosen to use. Also mark the positions of intermediate battening and any other support timbers you have included as part of the overall framework. If these are door or window openings, work from these.\nThe next stage is slightly tricky in that you have to find the high point of the wall. This is perhaps easiest done by running a suitable straightedge vertically along the length of the wall. Use a spirit level to keep this straightedge in true plumb, noting at what points—and by how much—protrusions force the straightedge away from the wall. Make sure to cover all the areas where battens are to be fixed.\nWhen you have established the high point, mark the corresponding positions at ceiling and floor. Substitute a particularly straight length of battening for the straightedge you have used and, keeping it perfectly plumb with the help of a spirit level, mark the ceiling and floor again to establish the face plane of this and all other battening of the framework.\nIndividually transfer these reference marks to all other batten positions by using careful measurement and ruling.\nMaking the frame\nStart making the frame by fixing a floor batten along the length of the wall, about 25mm above floor level.Use plugs and screws for fixing the batten at 600mm centres, ensuring at all times that its facing surface is quite true to the setting out marks. Use wood packing where necessary.\nThen fix the ceiling batten in a similar fashion, checking that it, too, is true with the setting out marks of the high point batten, and also that its face is in perfect alignment with the batten near the floor.\nThe two horizontal battens are then used for correct alignment checks on all the vertical battens as these are individually fixed, with support padding being provided where necessary. The physical appearance of the framework is not important, but do pay particular attention to see that the facing surface is straight and true.\nAdopt the same procedures if further support timbers are to be added. Later, when fixing short board lengths or widths, it may become necessary to add noggins or struts at points where the cut sheet edges do not coincide with the main components of the support frame. All edges must be supported, and fixed, to prevent local- ized weak spots which may lead to board fracture, holes or edge chipping. Plasterboard continued into the reveal, soffit and head of a window must be supported also.\nIf you have planned to use wallboards in the normal, vertical mode, buy sheets the length of which either just exceeds the wall height, or that are between a third and two-thirds longer again than your wall height. In this way, wastage is minimized: the near-size board can be trimmed to size ar/3 the waste simply discarded; the over-long boards will still yield easily usable offcuts.\nTrim the sheets at their base, about 25mm shorter than the full floor to ceiling height of the wall—the gap will be concealed by skirting. Use a fme-bladed handsaw or, much easier, a handyman’s knife for the job; with the latter, score the ivory side, break the board over a suitable straightedge, then finally cut through the backing paper on the reverse side. Use a combination of bradawl and pad-saw to make the necessary apertures for switches, sockets, through pipes and other fittings at this stage. Smooth the frayed paper edges with fine sandpaper.\nCarefully edge the first sheet into position;—ivory side outwards—at the starting point at one end of the wall. Use a foot-lift to raise the board hard against the ceiling while you push the sheet carefully but firmly home against the first part of the framework so preventing a loose, uneven fixing. Using assistance if you have it, maintain this position while the board is fixed to the support battening.\nUse 30mm or 40mm galvanized or sherardized plasterboard nails at approximately 150mm centres, no less than 12mm from the board edges, to fix the sheet into place at all support points. Start at the middle and work radially outwards. In order to simplify later ‘nail spotting’, drive the nail heads down as far as possible without fracturing the bonding paper.\nA special drywall hammer for nailing plasterboard nails can be purchased. The rounded head of this does not break the paper but drives the nail below the surrounding level while at the same time forming a checkered pattern which acts as a key for the minute amount of plaster or filler used to conceal the nail during spotting. A ball-pein hammer can be just as effective if used with care.\nOffer up the next sheet in a similar manner, lightly butting this against the fixed wallboard. Avoid tight butting—leaving a 3mm gap between adjacent boards—as forcing a board into place results in bowing which may in turn lead to misalignment and unsatisfactory fixing.\nContinue fixing this and other sheets in order until you come to the frame of a doorway or window—or any other obstruction which requires the board to be cut to something other than length. With planning, it is possible to arrange for the untrimmed board edge to form the new line of the reveal of the window , but trimming is likely, if only for the sill.\nPlasterboard for the reveals—cut perhaps from short-length offcuts— should protrude to a point level with the battening face so that the edge of the adjoining wallboard can be trimmed to face this off. As this trimming is better done from direct measurement of an existing arrangement , finish the wallboarding of the window and door reveals before you cut the main sheets to fit the window or door frame area.\nAt the later finishing stage, the joint areas are bound with tape and concealed prior to decoration. This aspect is covered in the next plasterboard section.\nLining a ceiling\nWallboard can also be used in a variety of ways to clad a ceiling, often to repair a section of lath and plaster which has been damaged. However, it can only be fixed directly to joists if these are perfectly true and level. In many cases, it is easier to remove the entire damaged ceiling in order to expose the joists or rafters rather than attempt a patch.\nWallboard is easily mounted if sheet sizes can be arranged to match ceiling joists. But edges, cut or otherwise, must always be supported and this means providing cross noggins to complete the framework.\nThe joist separation and the best pattern of laying the wallboard across the ceiling are two important factors to consider when choosing the most suitable size and thickness of board. You cannot use 9.5mm board if the joists are further apart than 450mm, or 12.7mm board if the separation exceeds 600mm, unless you install many additional cross noggins to reauce the effective fixing centres to within these limits.\nWhen you have established a suitable framework, based on laying sheets in ‘brick bond’ pattern , mark the position of the joists to aid subsequent nailing of the boards to them. It is important that the boards are fixed in this staggered pattern to reduce the strain along a line of joints.\nBoard is then laid, ivory face down, with the papered-covered long edge at right-angles to the joist and supported by the cross noggins. You will need assistance—or the aid of a lazy man—ceiling prop—to support the board while it is being fixed into correct position.\nUse 40mm plasterboard nails and work from the middle outwards, nailing at 150mm spacings along the joist and noggin supports. Use previously-made wall marks—and later, the visible lines of nails—to guide you. Force the heads below surface to simplify nail spotting later.\nThe second and subsequent sheets should be lightly butted together, if possible leaving a very slight gap. Where necessary, cut the board to shape and length as you would normally, making sure the ends are staggered and that they co-incide with a joist or cross noggin.\nFor widely separated joints plasterboard plank can be used. At least five 60mm plasterboard nails must be used across the width of the board at every support to ensure adequate fixing.\nBut if the joists are not level and true, you have to consider using alternative fixing forms , or resort to the traditional methods of lath and plastering.', 'There are two types of methods of suppression used in building fire safety design: passive and active. Passive suppression uses materials, systems, building elements, and/or building layout to prevent or resist ignition, to limit its spread to other combustible contents in the room and to contain the fire within the room or zone to prevent its spread to other sections of the structure.\nActive suppression is the employment of mechanical devices such as sprinklers or extinguishers to extinguish the fire in its early stages to prevent its spread. Passive suppression uses the natural properties of materials and products that are part of the building design to suppress the migration of the fire. It is well known in the construction industry that the single most important characteristic of gypsum drywall is its fire resistance. This is provided by the principal raw material used in its manufacture, CaSO₄∙2H₂O (gypsum). Gypsum is non-combustible, which means that it contributes no fuel to a fire. As the chemical formula shows, gypsum contains 21% by weight chemically combined water, also called crystalline water that is part of the gypsum crystal itself.\nWhen gypsum drywall panels are exposed to fire, the heat of the fire converts the crystalline water to steam. The heat energy that converts water to steam is thus absorbed, keeping the opposite side of the gypsum panel cool as long as there is water left in the gypsum, or until the gypsum panel is breached.\nFIRE RESISTANCE LEVEL (FRL)\nFire rating requirements of the Building Code of Australia are specified in terms of Fire Resistance Level (FRL). The FRL specifies the performance, in minutes, for each of the following three design criteria when specimens are fire tested to the requirements of the Australian Standard AS 1530 Methods for Fire Tests on Building materials, Components and Structures — Part 4: Fire-Resistance Tests of Elements of Building Construction:\nThe specimen can no longer carry its load (self-weight and superimposed loads).\nCracks or openings develop that allow the passage of flames or hot gasses.\nThe unexposed face temperature rises by more than 140°C on average or 180°C for a single point. For example, a wall system under fire test that carries its load for 120 minutes and maintains its integrity and insulation for 120 minutes is given a FRL of 120/120/120, ie 120 minutes structural adequacy, 120 minutes integrity and 120 minutes insulation. Systems that achieve a particular FRL can be used to satisfy the requirements for a lesser FRL.\nAny structure required to support a fire rated system must have a fire resistance structural adequacy level of at least that of the system. This includes vertical support to ceilings and walls and lateral support to the top of walls which may be provided from both sides. Refer to BCA for specific requirements.\nThe BCA requires that building elements, other than roof sarking or certain roof battens, must not pass through or cross a fire rated wall unless the Fire Resistance Level of that wall is maintained. Where trusses and beams pass over or through a fire rated partition, the following measures can be taken to ensure that the Fire Resistance Level of the partition is not degraded due to a failure of these members in the case of fire:\n- Construct a fire rated ceiling that protects the structural members\n- Fire protect the structural member, or\n- Ensure the partition can carry loading from the fire affected structural member and that the member can still carry its loading when it is supported on a partition (for trusses, this can mean the inclusion of additional webbing above the partition). Ensuring the partition can carry these new loadings may require:\n- Turning it into a load bearing partition\n- Constructing the partition with a protected column within it, or\n- Constructing unprotected columns on both sides of the partition.\nPORTAL FRAME BEHAVIOUR\nIn portal frames affected by the fire, the rafters often push outwards on the column members until the ridge sinks and then pulls the columns inwards. Should drywall be used to provide a fire separation within portal framed buildings, the above mode of failure needs to be recognised by the designer.\nAs mentioned above, load bearing elements may need to be incorporated within, or adjacent to, the partition to maintain support to the roof structure during a fire event.\nDIRECTION OF ATTACK BY FIRE\nIn most cases, the direction of attack by fire is assumed to be from both sides of the partition. In some cases, for example in exterior walls adjacent to a fire source feature (as defined in the BCA), the rating may be required from one side only.\nFor conventional fire rated plasterboard ceiling systems, direction of attack by fire is always from below, while for spanning ceilings it can also be from both sides or from above. Applicable fire attack direction is indicated for each fire rated system listed in this manual.\nMaximum heights for fire rated steel stud partitions are the lesser of maximum fire heights and structural heights for a given wall configuration and stated lateral pressure.\nMaximum fire heights for USG Boral fire rated steel stud walls are derived from full scale tests carried out by CSIRO, BHP, BRANZ and from fire engineering principles. Maximum structural heights have been obtained by computation and from extensive mechanical testing. These heights meet the requirements of the National Construction Code and have been certified by Wynton Stone Australia Pty Ltd and Taylor Thomson Whitting of Melbourne.\nRESISTANCE TO INCIPIENT SPREAD OF FIRE (RISF)\nThe NCC stipulates instances when a ceiling system must be resistant to the incipient spread of fire. This requirement determines the ability of the ceiling to provide adequate thermal insulation to combustible materials within the ceiling plenum, thus avoiding the danger of the materials being ignited.\nMany USG Boral fire rated ceiling systems carry an RISF rating which is noted as such. RISF is a more onerous requirement than FRL. Systems that achieve a particular RISF may be used to satisfy the requirements of a lesser RISF.\nFIRE HAZARD PROPERTIES\nWall and ceiling lining materials in certain types of buildings must comply with the Fire Hazard Properties requirements of the BCA.\nAll USG Boral gypsum board lining products are classified as Group 1 (least hazardous) materials and have a smoke growth rate index less than 100 and average specific extinction area less than 250m2/kg when tested in accordance with the BCA.\nIn accordance with the BCA, gypsum boards can be used wherever a non-combustible material is required.\nGAS RETICULATION IN FIRE RATED WALLS\nOxygen or combustible fluid reticulation systems should not be located within fire rated walls unless designed, fire tested and constructed to suit this application.\nUSG Boral offers a wide range of fire rated building systems including;\n- lightweight wall systems up to FRL –/240/240 and 180/180/180\n- lightweight ceiling systems up to FRL 120/120/120\n- acoustic systems with Rw+Ctr=50 or higher\n- masonry upgrades and\n- beam/column fire protection systems up to FRL 180/–/–']	['<urn:uuid:da9b0dbf-8002-4431-b2b0-27ae47a3192f>', '<urn:uuid:72c74c5e-15c7-4ffa-bad4-7597db2ca9a7>']	open-ended	direct	concise-and-natural	distant-from-document	multi-aspect	expert	2025-05-13T01:23:33.086345	14	103	3767
53	compare traditional logic based algorithms machine learning algorithms image classification cat recognition	Traditional logic-based algorithms and machine learning algorithms differ significantly in their approach to image classification like cat recognition. Traditional algorithms require developing complex code to explicitly encode all variations of cats including sizes, breeds, colors, orientation, and location within images. In contrast, ML algorithms learn directly from data by analyzing many examples of cat images to infer what constitutes a cat image. This ML approach is much more efficient, requiring 98% less code than traditional programming approaches for image classification tasks.	['Continue reading or Download a free copy of the complete e-book\nArtificial intelligence (AI) and machine learning (ML) are important emerging technologies that have the potential to transform organizations. Exponential increases in computational capacity, the emergence of cloud computing, and innovations in algorithms have resulted in tremendous advances in the application of AI. Leveraging AI and ML techniques, organizations can unlock tremendous value through improved customer service, streamlined operations, and the realization of new business models.\nDespite tremendous advances in the field over the last decade, AI remains very much the domain of expert data scientists. AI technologies are a complex subject and are relatively new to the business world; few managers and enterprise technology professionals have an understanding of these techniques. Furthermore, implementing AI techniques within the enterprise requires different skill sets and approaches from traditional software products.\nIn order for organizations to truly unlock value from AI, its practitioners and their methods need to be fully integrated into the fabric of the enterprise. This integration requires that managers have a basic understanding of the science behind AI/ML, the techniques leveraged, and the metrics used to measure success.\nManagers who are well-versed in the complexities of leading AI/ML efforts will be able to capture the most value from these new technologies. These managers will be able to select the best AI use cases, effectively collaborate and problem-solve with data scientists during prototyping phases, support the transition of algorithms into production use, and design the right business processes and change management activities to capture value for the organization. In order to achieve this, managers need a “field guide” to AI and ML techniques.\nAs we have designed, developed, and implemented AI techniques and AI-enabled enterprise applications at organizations across the world over the past decade, it has become clear to us that such a field guide does not exist. Most books and articles are either too technical, and focused on machine learning practitioners, or are too managerial without encapsulating sufficient mathematical and machine learning knowledge.\nThis online resource attempts to provide such a managerial field guide. It captures essential information about the practical application of enterprise AI/ML techniques, gathered from our extensive experience at C3 AI, across a wide range of industries and business problems. It also captures our experience with AI/ML teams – recruiting, organizing, and managing high-performance teams.\nLogic-based algorithms represent the core of traditional programming. For decades, computer scientists were trained to think of algorithms as a logical series of steps or processes that can be translated into machine-understandable instructions and used to solve problems. Traditional algorithmic thinking is quite powerful and can be used to solve a range of computer science problems, including data management, networking, and search.\nTraditional logic-based algorithms effectively handle many different problems and tasks. But they are often not effective at addressing tasks that are quite easy for humans to do. Consider a basic task such as identifying an image of a cat. Writing a traditional computer program to do this correctly would involve developing a methodology to encode and parameterize all variations of cats – different sizes, breeds, and colors as well as their orientation and location within the image field. While a program like this would be enormously complex, a two-year-old child can effortlessly recognize the image of a cat.\nML algorithms take a different approach from traditional logic-based approaches. ML algorithms are based on the idea that, rather than code a computer program to perform a task, it can instead be designed to learn directly from data. So instead of being written explicitly to identify pictures of cats, the computer program learns to identify cats using an ML algorithm that is derived by observing a large number of different cat images. In essence, the algorithm infers what an image of a cat is by analyzing many examples of such images, much as a human learns.\nAI algorithms enable new classes of problems to be solved by computational approaches faster, with less code, and more effectively than traditional programming approaches. Image classification tasks, for example, can be completed with over 98% less code when developed using machine learning versus traditional programming.']	['<urn:uuid:84ae65ad-4dcc-45dc-9304-366316d614bf>']	open-ended	direct	long-search-query	similar-to-document	single-doc	expert	2025-05-13T01:23:33.086345	12	81	691
54	emergency alerts flood warning system types	There are multiple emergency alert systems for floods and other emergencies. The Bureau of Meteorology operates a flood warning system for rivers like the Norman River, issuing Flood Warnings and River Height Bulletins during floods. These are broadcast via radio stations. Additionally, there's the Wireless Emergency Alert (WEA) system that sends emergency messages to mobile devices, including extreme weather warnings. WEA messages include a special tone and vibration repeated twice, and can include tornado, flash flood, and tsunami warnings from the National Weather Service.	"[""brochure describes the flood\nwarning system operated by the Australian Government, Bureau of\nMeteorology for the Norman River. It includes reference information\nwhich will be useful for understanding Flood Warnings and River Height\nBulletins issued by the Bureau's Flood Warning Centre during periods of\nhigh rainfall and flooding.\nThe Norman River catchment is located in north west Queensland and covers an area of approximately 49,000 square kilometres. The river rises in the Gregory Range (Great Dividing Range) 200 kilometres southeast of Croydon and flows in a northwesterly direction. It is joined by its major tributaries, the Clara and Yappar Rivers, near the river height and rainfall station of Yappar River. The river flows through the major town of Normanton, before finally entering the Gulf of Carpentaria through the major fishing port of Karumba. The only other town in the catchment is the old historic gold mining town of Croydon. Floods normally develop in the headwaters of the Norman, Clara and Yappar Rivers, however, general heavy rainfall situations can develop from monsoonal and cyclonic influences causing widespread flooding, particularly in the lower delta country around Normanton and Karumba.\nThe record major flood of January 1974 and the floods of February 1991 and early February 2009, caused widespread road closures, and the inundation of many properties throughout the catchment.\nHistoric flood information for the Norman River is limited. Heights are available, beginning with the 1974 flood and and stretching to the flood heights recorded in 2009.\nThe Bureau of Meteorology operates a flood warning system for the Norman River based on a rainfall and river height observations network shown on the map. The network consists of a number of volunteer rainfall and river height observers who forward observations by telephone when the initial flood height has been exceeded at their station, as well as automatic telephone telemetry stations at Glenore Weir and Karumba, which are operated by the Queensland Department of Environment and Resource Management.\nThe Bureau's Flood Warning Centre issues Flood Warnings and River Height Bulletins for the Gulf Rivers, including the Norman River, during flood events.\nThe Carpentaria Shire\nCouncil for the lower Norman River\nthe Croydon Shire Council for the upper Norman River may be able to\ninformation on flooding in your area of the Norman River catchment.\nFlood Warnings and Bulletins\nThe Bureau of Meteorology issues Flood Warnings and River Height Bulletins for the Gulf River catchments, including the Norman River basin, regularly during floods. They are sent to radio stations for broadcast, and to local Councils, emergency services and a large number of other agencies involved in managing flood response activities.\nFlood Warnings and River Height Bulletins are available via :Radio\nRadio stations, particularly the local ABC, and local commercial stations, broadcast Flood Warnings and River Height Bulletins soon after issue.\nInterpreting Flood Warnings and River Height Bulletins\nFlood Warnings and River Height Bulletins contain observed river heights for a selection of the river height monitoring locations. The time at which the river reading has been taken is given together with its tendency (e.g. rising, falling, steady or at its peak). The Flood Warnings may also contain predictions in the form of minor, moderate or major flooding for a period in the future. River Height Bulletins also give the height above or below the road bridge or causeway for each river station located near a road crossing.\nOne of the simplest ways of understanding what the actual or predicted river height means is to compare the height given in the Warning or Bulletin with the height of previous floods at that location.\nThe table below summarises the flood history of the Norman River basin - it contains the flood gauge heights of the more significant recent floods.\nHistorical flood heights for all river stations in the Norman River Floodwarning network, as shown on the map, are available from the Bureau of Meteorology upon request.\nAt each flood warning river height station, the severity of flooding is described as minor, moderate or major according to the effects caused in the local area or in nearby downstream areas. Terms used in Flood Warnings are based on the following definitions.\nEach river height station has a pre-determined flood classification which details heights on gauges at which minor, moderate and major flooding commences. Other flood heights may also be defined which indicate at what height the local road crossing or town becomes affected by floodwaters.\nThe table below shows the flood classifications for selected river height stations in the Norman River catchment.\n(B) = Bridge (A) = Approaches (X) = Crossing\nThe above details are current at the time of preparing this document, but are subject to review. Up-to-date flood classifications and other details for all flood warning stations in the network are at:\nFor the latest rainfall and river height conditions please use the following link:\nFor the latest rainfall and river height network map please use the following link:Network maps\nFor further information, contact:\nThe Regional Hydrology Director, Bureau of Meteorology, GPO Box 413, Brisbane Q 4001"", '1.Why are Wireless Emergency Alerts (WEA) important to me?\nAlerts received at the right time can help keep you safe during an emergency. With WEA, warnings can be sent to your mobile device when you may be in harm’s way, without the need to download an app or subscribe to a service.\n2.What are WEA messages?\nWireless Emergency Alerts (WEA) are emergency messages sent by authorized government alerting authorities through your mobile carrier.\n3.What types of alerts will I receive?\n- Extreme weather, and other threatening emergencies in your area\n- AMBER Alerts\n- Presidential Alerts during a national emergency\n4.What does a WEA message look like?\nWEA will look like a text message. The WEA message will show the type and time of the alert, any action you should take, and the agency issuing the alert. The message will be no more than 90 characters. Wireless Emergency Alert web banner (Spanish) – 70 KB\n5.How will I know the difference between WEA and a regular text message?\nWEA messages include a special tone and vibration, both repeated twice.\n6.What types of WEA messages will the National Weather Service (NWS) send?\n- Tsunami Warnings\n- Tornado and Flash Flood Warnings\n- Hurricane, Typhoon, Dust Storm and Extreme Wind Warnings\n7.What are AMBER Alerts?\nAMBER Alerts are urgent bulletins issued in the most serious child-abduction cases. The America’s Missing: Broadcast Emergency Response (AMBER) Alert Program is a voluntary partnership between law-enforcement agencies, broadcasters, transportation agencies, and the wireless industry.\n8.Who will send WEAs to issue AMBER Alerts?\nThe National Center for Missing and Exploited Children (NCMEC), in coordination with State and local public safety officials, sends out AMBER Wireless Emergency Alerts through IPAWS.\n9.What should I do when I receive a WEA message?\nFollow any action advised by the message. Seek more details from local media or authorities.\n10.Will I receive a WEA message if I’m visiting an area where I don’t live, or outside the area where my phone is registered?\nYes, if you have a WEA-capable phone and your wireless carrier participates in the program. (More than 100 carriers, including all of the largest carriers, do.)\n11.What if I travel into a threat area after a WEA message is already sent?\nIf you travel into a threat area after an alert is first sent, your WEA-capable device will receive the message when you enter the area.\n12.When will I start receiving WEA messages?\nIt depends. WEA capabilities were available beginning in April 2012, but many mobile devices, especially older ones, are not WEA-capable. When you buy a new mobile device, it probably will be able to receive WEA messages.\n13.Is this the same service public safety agencies have asked the public to register for?\nNo but they are complementary. Local agencies may have asked you to sign up to receive telephone calls, text messages, or emails. Those messages often include specific details about a critical event. WEAs are very short messages designed to get your attention in a critical situation. They may not give all the details you receive from other notification services.\n14.Will I be charged for receiving WEA messages?\nNo. This service is offered for free by wireless carriers. WEA messages will not count towards texting limits on your wireless plan.\n15.Does WEA know where I am? Is it tracking me?\nNo. Just like emergency weather alerts you see on local TV, WEAs are broadcast from area cell towers to mobile devices in the area. Every WEA-capable phone within range receives the message, just like TV that shows the emergency weather alert. WEA, like the TV station, doesn’t know exactly who is tuned in.\n16.Will a WEA message interrupt my phone conversations?\nNo, the alert will be delayed until you finish your call.\n17.How often will I receive WEA messages?\nYou may get very few WEA messages, or you may receive frequent messages when conditions change during an emergency. The number of messages depends on the number of imminent threats to life or property in your area.\n18.If, during an emergency, I can’t make or receive calls or text messages due to network congestion, will I still be able to receive a WEA message?\nYes, WEA messages are not affected by network congestion.\n19.What if I don’t want to receive WEA messages?\nYou can opt-out of receiving WEA messages for imminent threats and AMBER alerts, but not for Presidential messages. To opt out, adjust settings on your mobile device.\n20.How will I receive alerts if I don’t have a WEA-capable device?\nWEA is only one of the ways you receive emergency alerts. Other sources include NOAA Weather Radio, news broadcasts, the Emergency Alert System on radio and TV programs, outdoor sirens, internet services, and other alerting methods offered by local and state public safety agencies.']"	['<urn:uuid:1f55ccd4-e3de-4eb6-89ad-040349b10678>', '<urn:uuid:7b37b68c-ae35-41c3-82a2-d2b93a4beb20>']	open-ended	direct	short-search-query	similar-to-document	three-doc	novice	2025-05-13T01:23:33.086345	6	84	1636
55	What's the survival impact of swimming lessons for toddlers?	According to research, having formal swimming lesson training can reduce the risk of drowning by 88% among children aged 1 to 4 years.	['Swimming swimming pools are turning into an more and more popular amenity in homes new and old, and symbolize an activity that is particularly satisfying in the summertime. Regrettably, pools can also be extremely harmful, especially for tiny and unsupervised young children. In truth, in accordance to the U.S. Centers for Condition Management and Avoidance, kids ages a single to four have the optimum drowning prices, most of which take place in residential swimming pools. Around three hundred young children under the age of five die from pool-relevant mishaps every yr, and 2,000 more youthful kids are hospitalized for submersion injuries. If you or a beloved 1 has been injured in a swimming pool incident or if a particular person has experienced damage or demise even though swimming in your pool, contact an seasoned private damage legal professional to go over your lawful choices.\nFactors That Influence Drowning Chance\nThere are a variety of crucial elements that influence drowning risk and ought to be taken into consideration when protecting against summertime swimming pool accidents and pool owner liability. It crucial for both mother and father and pool house owners to think about all aspects of pool basic safety ahead of allowing youngsters, adolescents or older people access to swimming swimming pools.\nDeficiency of Supervision and Obstacles\nIt is very critical for younger youngsters and even\ngrownups to be supervised when taking part in swimming pool activities, no matter whether by a professional lifeguard, parent or guardian. In addition, boundaries like pool fencing should be used to prevent younger young children from getting accessibility to a pool location without the awareness or supervision of a caregiver. According to CDC statistics, there is an eighty three% reduction in the danger of childhood drowning with a 4-sided isolation pool fence, in comparison to the 3-sided property line fencing.\nAbsence of Existence Jacket Use\nAlthough lifestyle jacket use is generally far more strictly enforced throughout boating pursuits or although swimming in lakes or rivers, little kids and even adolescents or grownups could also advantage from using a life jacket although swimming in a household pool.\nAlcoholic beverages Use\nUp to half of adolescent and grownup deaths related with water recreation entail alcohol use. Alcoholic beverages can seriously impact judgment, harmony and coordination, and its consequences are heightened by heat and solar exposure, producing it particularly dangerous during pool use.\nDrowning is the most widespread lead to of accidental injuries and death among men and women with seizure ailments.\nEffects Linked with Swimming Pool Accidents and Injuries\nIn 2007, there were three,443 lethal accidental drownings in the United States, which interprets to an typical of 10 fatalities for every working day. Even nonfatal drownings can have severe effects, potentially ensuing in mind injury and extended-phrase disabilities like permanent loss of basic functioning, memory issues and understanding disabilities. According to the CDC, a lot more than 55% of drowning victims dealt with in the emergency place call for even more hospitalization or transfer for larger stages of care. Aufstellpools related with swimming pool mishaps can be significant, and the healthcare fees related with pool accidents and injuries can also be really large. In the course of the first hospitalization by itself, healthcare fees can achieve $2,000 even for victims who recuperate fully. For swimming pool accident victims who endure serious accidents like brain harm, medical expenses can skyrocket to $80,000.\nHow to Avoid Swimming Pool Mishaps\nThe CDC provides guidelines to pool owners and end users which can assist stop swimming pool incidents, injuries and death.\nUnderstand to swim effectively. According to analysis, obtaining official swimming lesson instruction can decrease the threat of drowning by 88% between kids aged 1 to four several years.\nDesignate a liable grownup to view kids in and all around the water. Young children ought to be inside touch of this person at all times. Adults must not be engaged in other routines whilst viewing youngsters around a pool, like reading through, speaking on the telephone, and many others.\nAlways swim with a buddy and/or swim in sight of a lifeguard when feasible.\nDiscover CPR. The faster a bystander’s response time, the greater possibility the victim has of surviving.\nDo not use foam toys in place of existence jackets. Noodles and interior tubes are not designed to keep a kid risk-free, and may instill a fake feeling of safety when a youngster is playing in a swimming pool.\nKeep away from liquor use before and for the duration of pool-side supervision.\nHow to Stop Operator Liability in Swimming Pool Accidents\nIf you have a pool at home, you are responsible for taking the proper measures to make the area as protected as possible. The initial action you must take in pool possession is to install 4-sided fencing around the pool so that the residence and enjoy area of the lawn are fully divided from the pool location. The fence should be at the very least four feet high and ought to use self-latching gates that open outward with latches that are out of attain of kids. You may possibly also contemplate installing extra pool limitations like automated door locks or alarms to prevent unauthorized pool obtain or to notify you if somebody enters the pool location. An additional step to just take to avert swimming pool mishaps and injuries is to very clear the pool and pool deck of toys. Floats, balls and other toys should be removed from the pool and encompassing location right away after use so that young children usually are not tempted to re-enter the pool area with no supervision.\nMost young children who drown in swimming swimming pools have been very last seen in their home, had been out of sight for significantly less than five minutes, and had been in the care of a single or both parents at the time they have been previous noticed. Several communities have proven security regulations governing household swimming pools in addition to these regulations, mothers and fathers and pool house owners can get their own precautions to decrease the chances of unsupervised kids getting accessibility to swimming pool regions. If your child has experienced critical injury or death as a outcome of a swimming pool accident, make contact with a personal injury attorney, as you could be entitled to financial compensation for your kid’s injuries and the pain and suffering endured by you and your family. If you are a pool owner and an accident occurred at your home, there are measures you can just take to steer clear of facing proprietor liability for accidents incurred. With the aid of a experienced personal damage lawyer, you can be conscious of your lawful alternatives and ensure that your rights are sufficiently guarded.']	['<urn:uuid:108ab461-878a-491a-aeb5-c7b1beabe9ec>']	open-ended	with-premise	concise-and-natural	distant-from-document	single-doc	expert	2025-05-13T01:23:33.086345	9	23	1123
56	I found some plants that look like carrots growing near my house. Is it safe to pick and eat them?	No, you should not pick and eat plants that just look like carrots without absolute certainty of identification. Plants in the Apiaceae family (which includes carrots) all look very similar to beginners, and some members like hemlock (Conium maculatum) are lethal. You must be able to identify plants beyond any doubt and know their Latin names before consuming them, as many plants share common names but are different species.	['Ok, it’s not rocket science. Common sense is most of the times enough, but let’s take a look at some things we could have not considered.\nBe able to properly identify every plant that you harvest and know whether it’s safe for human consumption (edible). Learn their Latin name, not just their common ones. Many plants share the same common (laymen’s) name, yet are completely different plant species. The same plant could, all the same, have many common names, different from one area to the other. If you cannot identify them beyond a shadow of a doubt, don’t eat them. The game isn’t worth the candle.\nA good rule is to learn poisonous plants in your area, before going into wildcrafting. Know your enemy, there could be a lot of deadly look-alikes, out there! Also be sure to take a good look at the main parts of the plant in different seasons. Don’t rely on flowers, they could not be there to help you identifying the plant. Remember that Apiaceae (umbellifers like carrot) all look the same for a beginner, and they include hemlock-the lethal (Conium maculatum). Socrates knows something about that!\nPesticides & Co\nAre you sure that the place where you are collecting is “safe” from pollutants? Take a look around before you get started.\nPlease don’t over-harvest wild plants. Unless you know it to be a noxious weed that someone would like eradicated, the the best ratio to go by is one in ten (10%) of the population. This leaves most of the stand for reproduction and wildlife, and minimally impacts the ecosystem. In other words, if you only see one or two plants, please refrain from harvesting at all. Sustainability is a big focus here.\nLeave some behind\nWhen you reach a fertile patch, don’t denude the area. Flowers are meant to produce seeds for the next generation, if you take them all, they won’t be able to reproduce. Same for the leaves, they are meant to photosinthetize and the plant needs some to recover. It’s tempting to gather as much as you can carry, but consider to take only as much as you can process and eat. Washing, cutting and cooking takes longer than the harvesting itself.\nSome plants are not damaged easily, and should be the first choice of a wildcrafter. Dandelion (Taraxacum officinale), Plantain (Plantago spp.), Good King Henry (Chenopodium bonus–henricus), Fat Hen (Chenopodium album), Nettle (Urtica dioica) are nearly impossible to eliminate. Yarrow, (Achillea millefolium) can be cut with a lawnmower and still flourish regularly. You can pick them and not threaten their survival. I prefer, anyway, to avoid digging any roots, unless i grow them in my own garden from seeds.\nProtected & endangered species\nEndangered plants are species in danger of becoming extinct in the foreseeable future. Threatened plants are likely to become endangered in the foreseeable future. Many of these plants only grow in one special area (endemic). Unfortunately, they are not always easy for an amateur to identify. All folks who pick plants from the wild should try to familiarize themselves with the local protected plants. When in doubt, don’t pick it. Saprohytes, Orchids and most of the Lily family are to be considered “no-pick” plants.\nIf you’re inexperienced, wildcrafting isn’t something you want to do alone. Find an experienced forager or group in your area and sign up for a foraging expedition or outing with a professional. It will be more fun to share your experience and the best way to gain some hands-on experience! Field guides and web resources are not enough to properly identify every plant that you harvest, if you don’t have experience in the field.\nShare your know-how\nI learned a lot from others. Old people (in mountains) knows a lot about it, because they learned “on field” in their childhood. You could do the “hard-work” in return for their knowledge. Again, someone may be a good mushroom forager and willing to learn more about plants. Let’s go together and share knowledge, than!']	['<urn:uuid:cdd9975f-30f6-4a3f-b667-2bde8444feed>']	factoid	with-premise	verbose-and-natural	distant-from-document	single-doc	novice	2025-05-13T01:23:33.086345	20	69	670
57	which other countries legal documents influenced indian laws system	The Constitution of India drew inspiration from several countries' constitutions: From France came the ideals of Liberty, Equality and Fraternity; Great Britain influenced the Parliamentary System, law-making procedures, and single citizenship; Canada contributed the concept of strong central authority in a federal structure; the USA provided elements like 'We the people' in the Preamble, Fundamental Rights, and Judicial Review; Ireland contributed the Directive Principles of State Policy; Australia provided the Concurrent List; Germany's Weimer Constitution influenced the concept of Emergency; and the USSR contributed the concept of Five Year Plans.	['In 1946, the British began to seriously consider granting India independence, and sent a Cabinet Mission to India with a plan to meet representatives of the British government and various Indian states, to discuss the possibility of setting up a Constituent Assembly to draft the framework of the Indian Constitution. Here are some lesser known facts on how the Constitution of India was written.\n- 1946: The Constituent Assembly was formed under the Cabinet Mission Plan. Dr Rajendra Prasad was elected as its permanent Chairman and Dr B.R. Ambedkar was appointed Chairman of the Drafting Committee. 13 Committees were set up to prepare draft reports that went into the framing of the final constitution.\n- The Constituent Assembly comprised of 389 members and the initial break-up was – Provinces (292 representatives), States (93 representatives), Chief Commissioner Provinces (3), Baluchistan (1). Later, with Muslim League withdrawing the number was reduced to 299.\n- January 1948: The first draft of the Indian Constitution was presented for discussion.\n- November 4, 1948: Discussions commenced and carried on for 32 days. During this period, 7635 amendments were proposed and of these, 2473 were discussed in detail. The Constituent Assembly sat for 2 years, 11 months and 18 days.\n- January 24, 1950: The Constitution of India was signed by 284 members of the Constituent Assembly, which included 15 women.\n- January 26, 1950: The Constitution of India came into force on this day.\n- The original Constitution of India was handwritten in Hindi and English by Prem Behari Narain Raizada. Each page was written in intricate calligraphy and in flowing italic, and artists from Shantiniketan in West Bengal further worked upon the presentation of each page. The original copies in English and Hindi are preserved in specially designed helium-filled cases and kept in the Parliament of India Library.\n- The Constitution of India is the world’s longest and most comprehensive constitution and contains 25 parts, 448 articles and 12 schedules. The original Constitution had 395 articles and 9 schedules.\n- The Constitution of India is also viewed as one of the best drawn constitution in light of the diversity of culture, religion and geography; the fact that it has had only 101 amendments till date shows the comprehensive nature of thought that went into its writing by the Constituent Assembly.\nThe Constitution draws inspiration and features from Constitutions of other countries as below:\n- France: The Preamble of the Constitution of India includes the ideals of Liberty, Equality and Fraternity. The inspiration came from the French Revolution when these were first formed.\n- Great Britain: The Indian Parliamentary System is largely influenced by the structure and functioning of the British Parliamentary system. Inspiration on law-making procedures, rule of law and the principles of single citizenship came from the British Constitution.\n- Canada: Concept of strong central authority in a federal structure.\n- USA: The Indian Preamble begins with “We the people” and its inspiration came from the Preamble of the American Constitution. Features of Fundamental Rights, Independence of the Judiciary, the Judicial Review system, removal of Judges of Supreme Court and High Court also came from the US constitution.\n- Republic of Ireland: Directive Principles of State Policy.\n- Australia: Concurrent List.\n- Germany (Weimer Constitution): The concept of Emergency.\n- USSR: The concept of Five Year Plans.\nFederation or Union:\nIt was Dr B.R. Ambedkar who clarified that India was a Union and no state had the right to secede from the Union. The first article of the Constitution says, “India, that is Bharath, shall be a Union of states”. When the Constitution of India came into force of January 26, 1950, a debate began on whether India was a Union or a Federation. It was Dr Ambedkar who, at the drafting stage, spoke of India being a federation but with a strong center. The term ‘cooperative federalism’ soon became popular that spoke of dual-polity of states and the Center.\nThe Preamble of the Constitution of India states that India is a Sovereign Socialist Secular Democratic Republic. The term “Socialist” was added later in 1976 through the Constitution 42nd Amendment Act, 1976.']	['<urn:uuid:8dd31f2e-f529-40f7-9894-fda954ee3aaf>']	open-ended	direct	long-search-query	distant-from-document	single-doc	novice	2025-05-13T01:23:33.086345	9	90	689
58	tree benefits urban areas climate	Trees in urban areas provide multiple benefits, including reducing cooling costs through shade, lowering heating expenses by acting as windbreaks, and improving air quality. They also play a vital role in mitigating climate change by sequestering carbon dioxide through photosynthesis, while simultaneously helping to combat urban heat island effects and enhance overall quality of life for city residents.	['What Does An Arborist Do?\nTrees are vital to life. They’re the biggest and oldest species on earth—from backyard natives to the towering eucalyptuses found in our parks, trees provide a range of social, economic, health, and environmental benefits.\nBut to maximise the benefits that trees give us, they need regular professional care and maintenance. Most people have heard of tree surgeons or tree doctors, but fewer people know what an arborist does, and their importance in tree care.\nSo, what is an arborist? And what does an arborist do?\nWhat is an arborist?\nArborists or arboriculturists are professionals in the practice of arboriculture, which is the management and care of trees. Arborists typically carry out their job in places where people live, play and work.\nWhat does an arborist do?\nProper tree care is an investment, as trees that are well cared for can add considerable value to a property. Poorly maintained trees can be a liability, and pruning or removing large trees in particular can be dangerous work. Many people work with and around trees. However, arborists are trained, certified and qualified in all aspects of tree maintenance and care, and adhere to the recommended Australian standards.\nArborists offer professional advice on trees to ensure a safe and aesthetically pleasing result. They consider where trees are positioned, the surrounding area, and how to maximise their health. They are also knowledgeable about tree species and how they will react to different types of pruning. Arborists will evaluate the tree from all angles, and ensure that anything they cut won’t jeopardise the safety of the tree, homes, property, or people. They will also ensure regrowth will be strong and healthy and not require constant maintenance.\nDepending on their qualifications, arborists offer a variety of services, including:\n- Selecting and planting trees appropriate to the environment\n- Pruning young trees to ensure they grow into healthy, well-structured, mature trees\n- Undertaking the pruning, trimming, cutting, lopping, stump grinding and mulching of trees\n- Protecting and preserving trees during construction and development projects\n- Diagnosing and treating pests and diseases\n- Assessing and managing tree risk, and removing trees if necessary\n- Offering consulting services and preparing arborist reports if necessary\nIs an arborist the same as a tree lopper?\nNo – tree loppers “lop” trees by removing their branches and limbs. Due to a lack of training, they often don’t consider a tree’s position, its overall health or how it will regrow. This can result in undermining a tree’s stability and overall structure, or leave it exposed to disease and insect infestation.\nInefficient pruning can also leave tree limbs poorly attached with a higher risk of breaking. It increases the risk of trees “failing” in strong winds or storms, leaving homes and properties vulnerable to damage, and occupants at risk of injury. If an accident happens and the appropriate insurance isn’t in place, a home or property owner may be sued for allowing the work to be undertaken, and fined if the tree lopper is injured.\nWhat are the typical practices of an arborist?\nAn arborist’s work may involve working with large and complex trees to ensure they are safe, healthy and meet the needs of property owners or community standards.\nTrees in urban landscape settings are often subject to disturbances, whether human or natural, both above and below the ground. However, there is a vast difference between professional arborists who abide by the correct practices and techniques, and inadequately trained tree workers whose job is to trim trees, not realising that they may be disfiguring, damaging, weakening, or even killing the tree.\nArborists can provide appropriate solutions such as pruning trees for health, structure or aesthetic reasons, but there should be a specific purpose in mind. This is because every cut is a wound and every leaf lost is the removal of some of the tree’s photosynthetic potential.\nThey also perform “crown raising” to permit people to walk under trees, or “crown reduction” to keep trees away from fences, buildings and wires. The methods and timing of treatment depend on the purpose of the work and the species of the tree. Best practices, therefore, involve a knowledge of both botany and the local environment.\nArborists also assist in diagnosing, treating and preventing phytopathology or parasitism, preventing or interrupting predation, and removing vegetation deemed as hazardous.\nWhile some aspects of their work is done in an office, much of it is undertaken using specialised vehicles to access trees, or by tree climbers who use ropes, harnesses and other relevant equipment.\nWhat legal issues are involved?\nDepending on the location, there are several legal issues surrounding the practices of arborists, including public safety issues, boundary issues, and the community value of heritage trees.\nArborists frequently consult the facts for tree disputes between property owners. This includes the areas of ownership, the obstruction of views, nuisance problems, and the impacts of root systems crossing boundaries. They may also be asked to assess the value of a tree for insurance purposes if the tree has been destroyed, damaged, vandalised, or stolen.\nIn cities with tree preservation laws, an arborist’s evaluation may be required before a property owner can remove a tree, or to ensure the protection of trees in development plans and during construction operations.\nWho do arborists assist?\nArborists are engaged by a variety of individuals, business, and institutions, including:\n- Schools and educational institutions—services include enhancing aesthetics and dealing with tree-related safety risks like hanging branches, falling deadwood, raised roots and stumps and impacts to buildings and school surroundings.\n- Builders, landscapers and property developers—services include tree care and management services for landscaping and construction services, and assessment of the health and condition of trees.\n- Residents—services include trimming hedges, removing damaged or diseased trees, and advice on keeping trees healthy and well cared for.\n- Body corporates—services include ensuring all common areas like gardens are well maintained and pose no risks to occupants.\n- Governments and local councils—services include tree report services, and advice on local laws and vegetation protection orders. They may also offer diagnostic services like soil analysis, moisture testing, pH testing, microbiology, tree value appraisals, hazard assessments, disease and decay detection, and tree maintenance plans.\nDo arborists need to be qualified?\nArborists need to have qualifications to practice arboriculture, and experience working safely around trees. In Australia, arboricultural training is streamlined through a multi-disciplinary vocational training, education and qualification authority known as the Australian Qualifications Framework.\nTree climbers should possess a Certificate III in Arboriculture or higher qualifications. Companies offering these services should also adhere to the Australian Standard Pruning of Amenity Trees AS-4373-2007, and hold Quality Assurance Accredited with full insurances, including Public Liability cover.\nHow do arborists climb trees?\nArborists use a “work positioning system” to climb trees, which differs from the harness systems used in rope-access jobs or rock climbing. Every healthy tree can be climbed, however, some trees can be slippery, snappy or prone to releasing resin, making climbing a sticky job! Typical warning signs arborists need to look out for include cracks, termite or ant activity and unusual swelling on a tree’s trunk.\nIn terms of climbing trees, it is all about basic geometry and physics, particularly with rope angles. This means keeping a rope above their head (not out to the side), so their weight is on their harness rather than the tree. They also work off a high point, which is the highest part of the tree they can safely be attached to.\nArborists use a variety of ropes, pulleys, harnesses, throw-lines and even spurs, but the most common method of climbing is with a prussik. This is a knot or friction hitch used to attach a loop around a rope. Climbers then pull down on the working line and push up on the prussik using a “pull-down, push-up” technique.\nWhy we need trees\nIt’s hard to imagine a world without trees. But the benefits of trees extend way beyond their beauty, and trees planted today will offer benefits for years to come, including:\nTrees accentuate views, provide privacy, reduce glare and noise, and even enhance architecture. They also bring people together for activities like walking and bird watching, and enable children to discover their sense of adventure through climbing.\nTrees slow the rate of global warming, provide habitats for wildlife, and reduce flooding and soil erosion. They also serve as windbreaks, provide protection from rainfall, and moderate temperatures by shading areas and reflecting heat upwards from their leaves.\nTrees improve air quality, reduce noise pollution, and the calmness we feel when we’re near them can reduce fatigue, stress and even decrease the recovery time after an illness or surgery.\nTrees host complex microhabitats and offer food and shelter to a variety of insects, birds, fungi and lichen. The trunks of older trees also provide hollow cover for bats, possums, owls, parrots, snakes and frogs.\nTrees can provide shade which lowers the cooling cost of homes, and can also reduce heating costs by acting as windbreaks. Research also shows that organisations benefit from happier, more productive staff if there are trees and parks located close to their working environment.\n- 2020, Arborist, Wikipedia\n- 2020, Why We Need Trees, Arboriculture Australia\n- Penny Travers, 2016, Arborists to show off tree-climbing skills in inaugural ACT championships, ABC News', 'Environmental conservation is an urgent global imperative, with a growing focus on preserving and restoring natural ecosystems. Trees are pivotal in these efforts, as they are critical components of ecosystems, sequester carbon dioxide, and contribute to overall biodiversity. Tree expertise is paramount to conserving and managing trees effectively. We will explore how the team of certified tree experts, often called arborists, contribute to environmental conservation efforts, including the restoration of urban forests, preservation of native species, mitigation of climate change, and sustainable land management practices.\nHow do experts contribute to environmental conservation efforts?\n1. Urban Forest Restoration\nUrban forests, consisting of trees within cities and metropolitan areas, are vital for improving air and water quality, mitigating heat island effects, and enhancing urban residents’ overall quality of life. Nevertheless, urban trees face numerous challenges, including pollution, habitat fragmentation, invasive species, and disease. Tree experts play a significant role in restoring and maintaining urban forests. Arborists assess the health of urban trees, identify and manage pests and diseases, and implement tree planting and care programs. They also collaborate with city planners and local communities to select appropriate tree species and establish tree-friendly policies. By restoring and expanding urban forests, arborists contribute to conserving vital green spaces within urban areas, promoting sustainability, and enhancing urban biodiversity.\n2. Preservation of Native Species\nConserving native tree species is paramount for maintaining the ecological balance of natural ecosystems. Native trees furnish habitat and food for wildlife, contribute to soil health, and support native pollinators. In many regions, invasive tree species threaten native plant communities, disrupting local ecosystems. Arborists are instrumental in pinpointing and managing invasive species. They develop strategies for extracting and controlling invasive trees and promote the preservation of native tree species through habitat restoration efforts. By preserving native tree populations, tree experts contribute to the overall health and resilience of natural ecosystems and support the survival of native wildlife.\n3. Carbon Sequestration and Climate Change Mitigation\nTrees are among the most effective tools for mitigating climate change due to their capacity to absorb and store carbon dioxide (CO2) through photosynthesis. As CO2 levels in the atmosphere continue to rise, tree expertise becomes indispensable in maximizing the carbon sequestration potential of forests. Arborists work to ensure the health and longevity of trees in natural and urban settings. They assess tree health, implement appropriate care practices, and combat threats such as pests, diseases, and drought, which can weaken trees’ ability to sequester carbon effectively. Additionally, arborists promote sustainable tree-planting initiatives and advocate for incorporating trees into urban planning to enhance urban carbon sinks.\n4. Sustainable Land Management\nSustainable land management practices are essential for conserving trees and maintaining healthy ecosystems. Arborists collaborate with landowners, government agencies, and environmental organizations to develop and implement sustainable land management plans. These plans often involve tree preservation, reforestation, and afforestation efforts to combat deforestation and habitat loss. Arborists assess the suitability of tree species for reforestation projects, ensure proper planting and maintenance techniques, and monitor the progress of these initiatives. Guided by tree expertise, sustainable land management practices help protect and restore landscapes, promoting long-term ecological health and resilience.\n5. Community Education and Outreach\nArborists are instrumental in educating communities about the significance of trees and conservation. They engage in outreach activities, such as workshops, seminars, and public events, to raise awareness about tree care, native species preservation, and environmental sustainability. Community education efforts often include tree-planting initiatives that implicate residents in conservation projects. These initiatives foster a sense of ownership and stewardship among community members, ushering in continued support for conservation efforts. Arborists serve as valuable resources for disseminating information and promoting environmental consciousness within communities.\n6. Research and Innovation\nTree experts are implicated in research and innovation to advance tree care and conservation practices. They study tree biology, diseases, and pests and investigate innovative tree care and maintenance techniques. Research findings help announce best practices in tree care and conservation. Additionally, arborists collaborate with scientists and researchers to address emerging challenges, such as the impact of climate change on tree health. They contribute valuable data and insights to the broader scientific community, aiding in developing sustainable and effective conservation strategies.\nTree expertise is indispensable for environmental conservation efforts, implicating urban forest restoration, preservation of native species, climate change mitigation, sustainable land management, protection of endangered species, community education, and research and innovation. Arborists play a paramount role in safeguarding trees, which are paramount components of ecosystems and vital contributors to the well-being of our planet. As we face escalating environmental challenges, including habitat loss, deforestation, and climate change, the knowledge and skills of tree experts become increasingly important. Their dedication to conserving and caring for trees helps maintain ecological balance and contributes to our natural world’s overall health and resilience. Tree expertise is a cornerstone of environmental conservation, ensuring that future generations can continue benefiting from the invaluable services and beauty trees provide.']	['<urn:uuid:9d730256-412c-4ff4-951a-6531a4efbabe>', '<urn:uuid:c10bc732-39fa-4747-99d0-e6d3f323bd98>']	factoid	direct	short-search-query	distant-from-document	multi-aspect	novice	2025-05-13T01:23:33.086345	5	58	2371
59	how do ancient fossils influence dragon mythology and what characteristics define sea dragon class	Ancient fossils significantly influenced dragon mythology, as around 8000 BC, early miners and quarry workers discovered dinosaur and prehistoric animal remains. These people, lacking knowledge about extinction, used their understanding of animal anatomy to piece together these creatures, leading to legends about giant lizards. Regarding Sea Dragons, they are characterized as primarily huge and ferocious monsters that terrorize the seas. According to the source material, a real Sea Dragon could swallow ten large Viking ships in one gulp without noticing, and their temperament could shift rapidly from calm to raging. While most Sea Dragons are enormous, some exceptions exist, like the Golphin, which is described as smaller and friendly. This class also includes dragons associated with ice and snow.	"['In 2014, Cressida Cowell released The Incomplete Book of Dragons in the UK and The Complete Book of Dragons in the US. Both are identical except for the In-. These books detail many dragon species seen in the How to Train Your Dragon Books, as well as some that aren\'t seen in the series. Also introduced in these books are a classification system for the Book dragon species. Any classification is not really mentioned in the actual series. It is possible that the author introduced a classification system not only to organize the plethora of species, but also because classification is very popular in the How to Train Your Dragon Franchise.\nThe author appears, in large part, to base species classification on habitat, and a minor extent on ability and size. The Classes for Book dragons are: Cave Dragon, Tree Dragon, Bog Dragon, Sky Dragon, Mountain Dragon, Nanodragon, and Sea Dragon. There are also a large number of dragon species for whom a Class has not been specified and remain unknown.\nCave Dragons are Dragons that spend most or all of their lives underground, especially in tunnels and caves that riddle the Barbaric Archipelago. A few species, however, may be classified due to ability, such as the Driller-Dragon. Many species are described as primitive. Cave Dragons are not to be confused with dragons which hibernate for the winter in caves.\n|Burrowing Slitherfang (presumed)||Gronckle||Skullion|\n|Eight-Legged Nadder||Monstrous Strangulator||Stickyworm|\n|Electricsticky||Octodangle||Unknown Cave Dragon|\nTree Dragons are the species that live in and around the dense wild woods of the Barbaric Archipelago. For the most part they don\'t think much either way of humans, though a few are utilized by Vikings for riding such as the Red Tiger or other purposes like the Vampire Spydragon. A few species are opposed to humanity and join the Dragon Rebellion, like the Breathquenchers and Poison Darters. The species in this Class are:\n|Breathquencher||Giant Bee-Eater||Shortwing Squirrelserpent|\n|Cuckoo Dragon||Poison Darter||Vampire Spydragon|\nBog Dragons have adapted to the marshes, bogs and wetlands that can be found all over the Archipelago, the Isle of Berk included. Bog Dragons are abundant, with many species tamed and used by the Vikings.\nDragons of this Class are:\nNanodragons are extremely common and very small - they are the ""insects"" of the dragon world. They can be very stupid, but also wickedly intelligent. They are ruled by one dragon, which is always from the Emperor Beetleboog Bolderbug species. The Bolderbugs can live for several decades, but most nanodragon species are very short-lived.\nThe Nanodragon species are:\n|Centidile||Emperor Beetleboog Bolderbug||Plankenteenie||Unknown Nanodragon|\n|Electricsquirm||Long-Eared Flutterfire||Tiddly-Nip Tick-Botherer|\nMountain Dragons have members who are behaviorally at opposite extremes - some are the most aggressive and wildest of all species, while others are the \'Cuties\' of the dragon world, used as lap-dragons. Some of the more violent species despise Vikings, and were the first to join the Dragon Rebellion against humans. Many species are very prized by Vikings, though, because of various abilities such as tracking, hunting, spying, electricity, or camouflage.\nThe Mountain Dragon Species are:\n|Bullguard Slavedragon||Hellsteether (presumed)||Sniffer Dragon|\n|Bullpuff (presumed)||Hogfly||Snub-Nosed Hellsteether|\nSky Dragons are the most agile and strongest fliers of the dragon world. They are often the species used as riding dragons by Vikings (but not exclusively). Many species allayed themselves with the humans during the Dragon Rebellion, and are often very loyal to their riders. Some species, such as the Silver Phantom are referred to as ""Air Dragons"". Perhaps this was an alternate name for this classification.\nSpecies in this Class are:\n|Rocket Ripper||White Dragon (presumed)|\n|Devilish Dervish||Silver Phantom||Zebramount|\nMost Sea Dragons are huge and ferocious monsters, mysterious as the seas they terrorize. There are some exceptions, such as the Golphin, which is portrayed as a smaller, friendly dragon. Dragons who have a connection with ice and snow are also in this Class, so broadly this Class contains dragons associated with water.\n|“||A REAL Sea Dragon is fifty times as big as that little creature. A real Sea Dragon from the bottom of the ocean can swallow ten large Viking ships in one gulp and not even notice. A real Sea Dragon is a cruel, careless mystery like the mighty ocean itself, one moment as calm as a scallop, the next raging like an octopus.||”|\n|— Old Wrinkly in Book 1|\nDragons without a Class\nThere are a large number of dragons mentioned in the Books that the author did not assign a class in The Incomplete Book of Dragons. They are as follows:\n- Despite being Mountain Dragons, the Dragon List at the end of the book states that the Deadly Shadow, Triple-Header Rageblast and Horror are Sky Dragons and Sea Dragons, despite saying otherwise in earlier chapters.\n- Seeing how they almost look alike, and have the same stats and description, the Deadly Nadder and the Eight-Legged Nadder may be the same species.', '|‘Flight of the Paladin’, by William O’Connor, ©2012|\nThe most iconic image in all of fantasy art is the dragon. Powerful, ancient and beautiful it has for millennia captivated the minds of artists, writers and story tellers. But many of us may not know the origins of this terrific beast. I therefore present this brief history to illustrate the history of this mythological creature that has possessed out imaginations for so long.\nThe legend of dragons is as old as civilization itself, but where does this mythology come from? The birth of the idea of dragons (and all mythical beasts) around the world is fairly obvious. With the invention of mining for precious metals and quarrying of stone for building ( ca. 8000 bc.) it is likely that these ancients discovered the fossilized remains of dinosaurs and prehistoric animals just as we do. With an understanding of animal anatomy it would be easy for them to piece together the creatures, and imagine their appearance. The difference however these people would not understand that the animals were extinct. Legends would be built around these exotic, giant, unknown animals and the mythology of giant lizards was born.\nIn ancient Mesopotamia (ca. 5000 bc.) the creation myth is built around a dragon named Tiamat. This great beast of heaven is slain by Marduk, and cast into the sea making the world.\nDragons continue to populate the classical mythologies of ancient Greece and Rome, but the form of these creatures is varied. Hercules slays a dragon in one of his twelve labors, Jason subdues the dragon that is guarding the Golden Fleece, and Apollo slays a great serpent named Python. All of these monsters fall into the category of dragons, but there depiction in art is by no means standardized. The dragon is simply a generic giant lizard.\nNot until the fall of the Roman Empire and the Rise of Christendom does the dragon in western civilization begin to take on the familiar form we know today in bestiaries and myths. It is perhaps the story of Adam and Eve in the old testament and the appearance of Satan in the form of a serpent that first transfixes the medieval imagination. For nearly a thousand years, the dragon represents evil and becomes synonymous with demons. The dragon is also used by medieval knights emblazoned on their shields and standards to strike fear into the hearts of their enemies. Throughout the epics and romantic stories of this period dragons show up to be slain by the virtuous heroes of folklore, legend and religion.\nIn the epic of Beowulf (ca. 1000 ad.) the great Scandinavian king slays a monstrous fire-breathing dragon and dies in its arms.\nIcelandic tales in the Volsungasaga (ca. 1250 ad.) transforms the dwarf prince Fafnir into a dragon to be slain by Seigfried.\nThe Welsh Epic of The Mabinogion (ca. 1400ad.) depicts the great Red Dragon of Wales battling a White Dragon and causing earthquakes.\nIn the Christian faith Satan routinely appears a dragon to tempt the saints. The Golden Legend (ca. 1200 ad.) illustrates St. George and St. Margaret as well as several other saints, confronted by dragons.\n|‘St. George and the Dragon’, by Bernt Notke, 1489 (Stockholm Cathedral)|\nBy 1500 ad. the mystical apex of catholicism combined with the ever-increasing craft of the visual artists finds the archetypal fire-breathing dragon in its full splendor. This is well documented when Edmund Spencer describes his titanic monster in The Fairie Queene. (1590 ad.)\nExcerpt: Book One: Canto Eleven\n“By this the dreadfull Beast drew nigh to hand,\nHalfe flying, and halfe footing in his hast,\nThat with his largenesse measured much land,\nAnd made wide shadow vnder his huge wast;\nAs mountaine doth the valley ouercast.\nApproching nigh, he reared high afore\nHis body monstrous, horrible, and vast,\nWhich to increase his wondrous greatnesse more,\nWas swolne with wrath, & poyson, & with bloudy gore.\nAnd ouer, all with brasen scales was armd,\nLike plated coate of steele, so couched neare,\nThat nought mote perce, ne might his corse be harmd\nWith dint of sword, nor push of pointed speare;\nWhich as an Eagle, seeing pray appeare,\nHis aery plumes doth rouze, full rudely dight,\nSo shaked he, that horrour was to heare,\nFor as the clashing of an Armour bright,\nSuch noyse his rouzed scales did send vnto the knight.\nHis flaggy wings when forth he did display,\nWere like two sayles, in which the hollow wynd\nIs gathered full, and worketh speedy way:\nAnd eke the pennes, that did his pineons bynd,\nWere like mayne-yards, with flying canuas lynd,\nWith which whenas him list the ayre to beat,\nAnd there by force vnwonted passage find,\nThe cloudes before him fled for terrour great,\nAnd all the heauens stood still amazed with his threat.”\n|‘St. George and the Dragon’, by Peter Paul Ruebens, 1606|\nAfter the Protestant Reformation and the advent of the Age of Reason the dragon becomes more a creature of entertainment rather than of spiritual belief. Protestant artists are prohibited from depicting scenes from the bible and the stories of the saints are abandoned as idolatry. Dragons and other beasts take on a decorative nature, and as subjects of classical illustration. During this period it is the first evidence that the dragon is being treated as a fantasy creature.\n|‘The Great Red Dragon and the Woman Clothed in Sun’, by William Blake|\n|‘St. George and the Dragon’, by Arthur Rackham|\nThe 19th century sees a resurgence in the dragon as archeologists, historians and the stories of the “Pre Raphaelite” age are studied and adopted as acceptable subject matter, through Bullfinch, The Brother’s Grim and new translations of the classics and folklore. Throughout this period the dragon stories of the dark past are fodder for artists and writers alike. By the second half of the nineteenth century the neo-isms (Neoclassical, neo-gothic, neo-egyptian, neo-romanesque) of the beaux-arts academic style return to using the dragon as a decorative embellishment.\n|‘Woman and Dragon’, by Pablo Picasso (etching print)|\n|‘St. George and the Dragon’, by Wassily Kandinsky, 1911|\nWith the dawn of the Industrial Age twentieth century science and technology usurps romantic notions of the arts, pushing the stories of dragons into the genre of mythology and the realm of children’s stories. Painting and literature had embraced Realism and analytical minimalism throwing off all superstitions of the past to try to make a New kind of art. Any depiction’s of dragons during this period (and there are few) become an outward representation of the artist’s inner psyche. Psychology has replaced the dragon with the Id and the Ego.\nBy the 1970’s, however the art world was ready to once again re-embrace spirituality, mythology and the dragon. Post-Modern artists found the writings of J.R.R. Tolkein and Joseph Campbell. The mythologies they had drawn from had a renaissance.\nIn 1976 TSR introduced Dungeons and Dragons to a world hungry for fantasy and monsters, becoming a popular phenomenon. McCaffery’s Dragonriders of Pern 1970, Dragonslayer 1981 were all introduced to a mesmerized audience. Since then the dragon has entered into the popular consciousness in a way not seen since the Middle Ages. Everything from Harry Potter to World of Warcraft and Skyrim have adopted the dragon as their go-to monster to inspire awe and magic.\n|‘Dragonriders of Pern’, by Michael Whelan|\nWhat is it about the dragon that has captivated us for all of civilization? Is it the sheer power of nature that cannot be tamed. Is it a psychological metaphor for primal fear? Is it perhaps a innate memory of our long lost primitive prehistory? Whatever the reason, in all cultures around the world the dragon , in all its forms has haunted our minds and our imaginations and will continue to do so for millennia to come!']"	['<urn:uuid:69d2ec03-a64c-4c93-ab3d-128d2ce1fd0e>', '<urn:uuid:544021ae-ede8-48e1-95d9-998975768c14>']	open-ended	direct	long-search-query	similar-to-document	multi-aspect	expert	2025-05-13T01:23:33.086345	14	119	2109
60	meditative techniques in gentle yin and restorative traditions	Both Yin and restorative yoga incorporate meditative elements but approach them differently. Restorative yoga focuses on mindfulness, self-awareness, and observing thoughts and feelings without judgment while becoming more aware of the body and surroundings. It activates the parasympathetic nervous system for relaxation. Yin yoga, rooted in classical yoga tradition, uses stillness in poses as a means to still the mind for meditation. During long holds in Yin yoga, practitioners can observe the 'quiet inner dance of consciousness' and explore deep meditation through conscious and deepening pranayama (breathing practices).	['Restorative yoga originated in 1970s America from the teachings of B.K.S Iyengar. It was made popular by one of Iyengar’s instructors, Judith Lasater. Restorative yoga is also referred to as “Rest and Digest” or “an active relaxation.” It features a slow pace, with poses being held for longer periods of time than with traditional yoga positions.\nThe main focus is on deep breathing, long holds, stillness, and slow transitions. A restorative yoga session normally only involves a few yoga postures, and the poses are generally held for five minutes or more.\nTraditional yoga practice is well known for its multiple benefits for improved mental and physical well-being.\nRestorative yoga holds many of the same benefits but may better suit a variety of fitness levels due to its gentle nature and an improved sense of calm due to the slower pace.\nThis style of yoga tends to use a range of props to aid poses such as blocks, cushions, pillows, and blankets, which allow positions to be held for longer and provide increased comfort and protection of the bones, joints, and muscles during poses.\nThese support props allow participants to fully relax into the yoga poses and get the most out of every position.\nThe Benefits of Restorative Yoga Include:\n- Improved mental health – reductions in stress, anxiety, and depression due to enhanced mindfulness, self-awareness, and sense of serenity during sessions. Restorative yoga improves mood, helps people to observe their thoughts and feelings without judgment while becoming more aware of their bodies and surroundings.\n- Improved physical health – improved balance, strength, cardiovascular and respiratory health, and reduced musculoskeletal pain. This makes it useful for those that suffer from certain conditions or chronic pain.\n- Relaxation – restorative yoga activates the parasympathetic nervous system (the rest and digest system) which allows the body to recover and heal from stress. With modern lifestyles being very busy and stressful, this type of yoga can help people take a much-needed relaxation break.\n- Improved sleep – increased relaxation brings with it better sleep. Restorative yoga is known to boost melatonin production and reduce hyperarousal which can be highly beneficial for those who have trouble sleeping.\nIf you’re just starting out with restorative yoga and want to try a few simple poses to get you into the swing of things, it may be quite difficult to choose from the variety of positions out there.\nBelow we’ve collated three of the best restorative yoga positions for beginners, so you can enjoy a short restorative yoga session and start making your way towards true relaxation and improved well-being.\nChild’s Pose is excellent for targeting your lower body, spine, and shoulder muscles. It’s a highly relaxing position and great for relieving tension in the body.\n- Start by kneeling on the floor with your knees apart. Move your big toes together until they are touching, and sit back towards your heels.\n- Placing a couple of folded blankets or a cushion between your thighs will support your torso.\n- Exhale slowly and move forward. Keep your torso between your thighs. Gently lean your head down toward the floor.\n- Raise your arms out in front of you, above your head, with your hands on the floor.\n- You can put a cushion or blankets on the floor to support your head if required.\n- Hold this pose for around 5 minutes, breathing in and out deeply.\n- Lift your torso up into a seated position to finish the pose.\nThis pose targets your legs, feet, hamstrings, back, and neck. It’s great for relieving pain or stress in these areas.\n- Put your yoga mat on the floor against the wall. Putting a folded blanket or cushion in the middle of the yoga mat will help to support your head and neck.\n- Sit with your right side against the wall. Lie down on the yoga mat, on your back with your head on the folded blanket or cushion, and swing your legs up against the wall.\n- Try to get yourself as close up against the wall as possible, so your legs are facing straight up against the wall. You can use extra blankets or cushions to support your lower back if needed.\n- Move your arms to your sides or place them on your torso. Inhale and exhale. As you exhale, relax your hips, back, and neck into the floor. Hold this position for around 10 minutes, breathing deeply and keeping your body relaxed.\nThe Corpse Pose is known for its ability to instill a deep sense of calm and relaxation. This is a great position for whole-body relaxation and enhanced mindfulness.\n- Begin by putting one folded blanket at the top end of your mat, and a couple of folded blankets, a cushion, or a bolster at the bottom end of your mat.\n- Sit in the middle of the mat, between the top and bottom end. Keep your back straight and bend your knees.\n- Extend your legs out so that your knees are resting on the bolster, cushion, or folded blankets at the bottom end of the mat.\n- Lower yourself slowly until your head is resting on the blanket at the top end of the yoga mat and you are lying on your back.\n- Place your arms in a natural position at your sides with palms facing up. You will have a gap between your arms and body.\n- Hold this position for around 10 minutes, inhaling and exhaling gently throughout.\nOnce you’ve mastered these three relaxing restorative yoga poses, you can start to practice a wider variety of positions. As you can see, restorative yoga poses are quite simplistic and focus mainly on deeply relaxing the body, bringing calmness, and promoting an improved sense of balance and well-being.\nRestorative yoga will do wonders for your mental and physical health, and you should notice a marked reduction in your stress levels if you practice these exercises regularly.', 'Yin Yoga Poses\nThis article is an exploration of 10 Yin yoga poses. Yin is a style that is practiced by holding poses for a long time in a relaxed state. Yin stands in contrast to other contemporary styles of yoga, such as Vinyasa or Hatha, which generally move the practitioner from pose to pose quickly. Yin yoga ‘asana’, the Sanskrit word for poses, are practiced by following the three principles that Bernie Clark explains in Yin Yoga with Bernie Clark.\nThree Principles of Yin Yoga\n- Principal 1: Play with your edge\n- Principal 2: Stillness\n- Principal 3: Hold for Time\nPlay With Your Edge\nYin is a lunar practice, which tends to be healing and cooling. Unlike solar practices such as sun salutations, yin does not call for heating postures, breathing styles, or sequencing. Therefore the muscles are typically not warm throughout a Yin practice. Entering into poses with cool muscles requires special attention to the first edge.\nThe first edge is found by gently getting into the shape of a pose and noticing where the body naturally wants to stop. Yielding the natural limitations of the body prevents injury. There should be no pain at the first or any other edge, yet there may be some discomfort. Discomfort without radiating pain is a sign that the connective tissue around the joints is stretching. Reasonable discomfort is a gateway to more flexibility and greater range of motion. Props can add additional comfort and accessibility to yin yoga poses.\nYou may experience strong physical sensations during a Yin practice such as heat or discomfort. Finding the first edge is a method of exploring the strong sensations and sitting with them. When yoga asana is briefly mentioned in the Yoga Sutras of Patanjali, they are prescribed to be Sthira sukham asanam, or done with steadiness and ease. Steadiness points to the second principal, stillness, which is explored later in the article.\nAbout 30-60 seconds into a pose, or if and when the body shows signs of being able to go deeper, it is safe to move to the second edge (or third or forth). It is important that the body, not the ego, give the signal to deepen. This humble practice never asks the yogi to prove or force anything. There is no need to try to “achieve” or to look a certain way. Gentleness, acceptance and honesty are critical in a yin practice. If the body starts to tighten, it’s a sign to slowly come out of the pose.\nClassical yoga was practiced as a means to still the mind for meditation. Sitting still in yin yoga poses lets the contemporary yogi dip their toes into the waters of deep meditation. One exception to stillness is when the body opens to a new edge. With the awareness that the body is ready to deepen, Yin yogis consciously move deeper and return again to stillness and breath. Another exception to stillness is the awareness of pain. In response to pain, it’s time to come out of the pose slowly.\nHold for Time\nThe third principal is to hold the poses for time. Poses can last anywhere from one minute to longer than 15 and in general are done for 5-10 minutes. Using a timer tells you how long you are staying still, which can be a way to gauge stillness from practice to practice. A timer can also ensure that both sides of the body get the same amount of time, which results in feeling delightfully balanced. The breath can continue to deepen the longer a pose is held. The lungs can expand seemingly forever. Counting breath is an interesting way to observe the expansive nature of the breath, which becomes possible during long holds. Profound experiences become accessible only when conscious and deepening pranayama is practiced.\nBreathing expansively while remaining still presents the practitioner with what is rarely otherwise observed: the quiet inner dance of consciousness, an inner world so rich and mysterious, it is invisible most of the time. Holding (still) for time never seemed so tempting.\nWhat Do You Need to Practice Yin Yoga?\n- Yoga mat or as an alternative, a thick blanket or carpet.\n- Yoga blankets and bolsters or as an alternative, towels and pillows nearby\n- Blocks or (or books if you have none)\n- Sand timer\n- Soft music\n10 Yin Yoga Poses\n1. Chest and Shoulder Expansion\nSit cross-legged, your right side adjacent to the wall, reach your right hand back so the palm is flat against the wall at about shoulder height. Scoot your right hip in closer to the wall if you feel you can safely tolerate more stretch. Once the shoulder feels settled, bend the arm so you have the palm directly above the elbow at the height of your gaze. Sit and breathe for several moments. If you feel you can safely deepen, allow the left hand to come beside and slightly behind your left hip and press fingertips into the floor. Lift and open your chest. Slowly drop the left shoulder down and perhaps the chin and gaze point follow. Stay for 1-5 minutes breathing gently.\n2. Lower Back Pose\nSit on a cushion if you have tight hamstrings or flat on the mat for more open hamstrings. Let the legs extend and relax so that the feet flop out gently to their respective sides. Knees can be slightly bent. Pressing the fingertips downward into the ground beside the hips, lengthen the heart higher in contrast. Then tuck the chin into the chest. Let the upturned palms fall to the outside of the thighs near the knees. Allow the upper spine to round. For your second edge, the hands may move down the legs closer to the ankles. Remember not to strive or do too much. Stay and breathe 1-5 minutes.\n3. West-Facing Pose\nThis pose just like the lower back pose, sitting up legs relaxed and extended. Place one bolster over your thighs. Lift though the chest as you inhale and as you exhale fold forward and down, hinging from the hips so as to keep length in the low back. If you are not able to lay upon the cushion, add more props until you can easily lay your chest and face on them. Find your first edge. At this time you may determine to remove or reduce the props so that you can go deeper. Stay in the pose for 1-5 minutes. Note, this pose is called “west-facing” because it is cooling, like the setting sun, which sets in the west.\n4. Wide Leg Child Pose\nOn your mat, add cushion beneath the knees using a blanket or towel. Bring the knees as wide or wider than the mat and big toes to touch or towards one another. Press your hips back toward the heels and walk your hands forward as you fold from the hips. Imagine a gentle anchor keeps the hips downward and the low back spreading wide. From there, stretch the arms, chest and head forward and down. If the head cannot touch the floor, put a block or folded blanket beneath it so it can rest. You may prefer to rest upon the forehead or to turn the head to one side and then the other. Let your arms and hands relax into the floor. Notice your edge and deepen if and when it feels right. Come out if the knees start to bother you, moving slowly. Practice for 1-5 minutes.\n5. Wide Leg Child Pose, Thread the Needle Variation\nFrom wide leg child pose, slightly raise the head and walk the right hand back. Thread the right hand under the left until the right shoulder is on the ground and put the right temple on the floor. Repeat on the left side. Practice 1-5 minutes on each side.\nCome to your hands and knees with a folded blanket under the knees for padding. Bring your right foot forward so it is just to the right on the right hand, make sure your shoulders are over the wrists. The right knee stacks over the right ankle so your shin may feel as though it is moving forward in space. If the right knee pops out to the right, redirect it over the ankle, pressing down into the right foot and imagine pressing the shin forward.\nIf needed, bring your hands up on blocks, lift through the chest, curl the left toes under and squeeze the left leg so the thigh lifts off the ground. This will cause the thigh to rotate internally, which means the alignment is now safe for deepening. Now , bringing the left knee down onto the blanket and then flatten the top of left foot into the mat. You may stay like that or bring your elbows onto the blocks to deepen. You may end up with no blocks, elbows on the mat beneath the shoulders or you may end up staying high on blocks. Honor where the body needs to go and remain for 1-5 minutes before repeating on the other side.\nPlace a flat bolster horizontally across the mat or a folded blanket about halfway down the length of the mat. Place both knees on the bolster. Bring the right knee forward of the cushion in front of the right hip and the foot toward on left side of the mat. Place the palms on blocks beneath the shoulders and bring the left leg back so that the thigh and top of left foot are pressing into or towards the floor. Let the pelvis be supported by the cushion. You may stay upright, opening the chest, or choose to ease yourself down, perhaps deepening to the point where your head rests on blocks or on the floor. Remain in the pose 1-5 minutes and repeat on the other side.\n8. Supported Reclined Butterfly\nThe use of props to recline makes this supportive and relaxing. To do so, place a block the tall way and another the short way so they make an L. Lay a bolster or supportive cushion over the blocks so that it a slanted toward the floor. Place yourself with your back against the lower part of the cushion with the souls of your feet touching. To keep the feet together and to allow the hips to relax, use a strap or a long rolled blanket to wrap the feet.\n9. Supported Gentle Fish\nPlace a block at medium height near the top of the mat. Sit so the block is behind you with the legs extended and relaxed. Lay your back over the blocks so they land between your shoulder blades boosted your heart up. Let your head relax back onto a block, making sure the neck is supported. Allow the arms to pour open and drip into the floor on either side. Breathe into the heart and let the body seep down onto the props. As your edge moves, notice the chest may boost higher. Remain 1-5 minutes.\n10. Corpse Pose/ Savasana\nLay on the mat on your back. Let the arms and legs relax. Arms by your sides, feet slightly flop out. If your lower back is uncomfortable place blocks or a cushion under the knees. Stay as long as you wish. Let go of doing and drop into being.\nPhotos courtesy of author Lara Hocheiser and featuring Blair Smalls.\nYoga Poses to Ease Digestive Discomfort\nIf you’re one of the many American who suffers from occasional digestive discomfort, yoga offers a natural way to get relief. Just as you would adapt your diet to address your needs, try including some of these poses into your regular practice.\nCat-Cow – Marjaryasana-Bitilasana\n- Improves posture and balance\n- Strengthens and stretches the spine and neck\n- Stretches the hips, abdomen and bac\n- Increases coordination\n- Massages and stimulates organs in the belly, like the kidneys and adrenal glands.\n- Relieves stress and calms the mind\nDownward-Facing Dog – Adho Mukha Svanasana\n- Calms the brain and helps relieve stress\n- Stretches the shoulders, hamstrings, calves, arches, and hands\n- Strengthens the arms and legs\n- Relieves headache, insomnia, back pain, and fatigue\n- Therapeutic for high blood pressure\n- Helps prevent osteoporosis\nExtended Puppy Pose – Uttana Shishosana\n- Releases tension in you upper arms, shoulders, and neck\n- Expands the whole front of your chest\n- Stretches out your abdominal muscles\n- Gently stimulates your back muscles in preparation for further backbends\n- Opens up your hips and stretches your hamstrings\nBridge Pose – Setu Bandha Saravangasana\n- Streches your chest, neck, spine, and hips\n- Strengthens your back, buttocks, and hamstring muscles\n- Calms your brain and central nervous system\n- Alleviates stress and mild depression\n- Massages abdominal organs and improves digestion/li>\n- Relieves the symptoms of menopause\n- Reduces anxiety, backaches, headaches, and insomnia\nWind-Relieving Pose – Ardha Pawanmuktasana\n- Stretches the neck and back\n- Pressure on the abdomen releases any trapped gases in the large intestine\n- Blood circulation is increased to all the internal organs\n- Relieves constipation\n- Strengthens the back and abdominal muscles\n- Massages the intestines and other organs in the abdomen\n- Eases tension in the lower back']	['<urn:uuid:2c6db3f3-86ef-40eb-b3c4-c5ed2015d8b2>', '<urn:uuid:50b97e22-7991-46c9-abcd-5ba83c86205b>']	open-ended	with-premise	short-search-query	distant-from-document	comparison	expert	2025-05-13T01:23:33.086345	8	88	3209
61	sewage management green duwamish river vs ganges	The Green-Duwamish and Ganges rivers have different approaches to sewage management. For the Green-Duwamish, there is an active pollutant loading assessment (PLA) program working with the EPA to identify, control, and clean up pollution sources, with technical advisory committees and community engagement to develop pollution reduction strategies. In contrast, the Ganges faces severe sewage management challenges with nearly all sewage (over 1.3 billion liters daily) from 29 cities and 70 towns going directly into the river untreated. Only about one-third of the sewage and industrial effluent discharged into the Ganges is currently treated, while the Green-Duwamish has structured programs to assess and address pollution sources.	"[""The Green-Duwamish pollutant loading assessment (PLA) will identify sources of pollution in the Green-Duwamish watershed and strategies to reduce those sources. We're partnering with the U.S. Environmental Protection Agency (EPA) to conduct this multi-year project.\nThe PLA will provide information on:\n- What pollutants enter the river.\n- Where pollution comes from.\n- How much comes from different sources.\nThe information will help us develop strategies so the basin's water and sediments meet environmental quality standards.\nTechnical advisory committee and community engagement\nWe're working closely with a technical advisory committee made up of surface water experts from local and tribal governments. We also hold meetings for interested parties — people from businesses, non-profit groups, local and tribal governments, and the general public — to provide updates on the project and get feedback.\nWe're committed to these relationships and believe this public involvement will help ensure we protect the environment of the Green-Duwamish watershed.\nPollutants entering the Duwamish River\nWe work with the EPA to identify, clean up, and control sources of pollution to the lower 5.5 miles of the Green-Duwamish River, also known as the Lower Duwamish Waterway (LDW). We and EPA began a comprehensive assessment of the pollutants entering the watershed from the Howard Hansen Dam in the Cascade Mountains to Elliott Bay. The assessment will lay a foundation for future work to protect human health and the environment in the Green-Duwamish basin.\nThe pollutant loading assessment is needed to protect and improve the effectiveness of the Lower Duwamish cleanup of contaminated sediments.\nThe assessment will:\nExisting models completed for the cleanup indicate that sediments may still exceed target levels after cleanup because of pollutant concentrations in sediment coming from upstream.\n- Create tools to identify upstream pollution sources.\n- Identify strategies to reduce pollution sources.\n- Combine direct measurements with advanced custom models of how pollutants interact with the basin's surface water, sediments, and its fish and shellfish.\nBroad involvement supports the project\nA technical advisory committee and interested parties are helping us develop the modeling tools we'll use to conduct the PLA. We expect the PLA will require many years to complete, and that the project will be conducted in phases. We will continue to work with the technical advisory group throughout the assessment.\nHow can I stay informed?\nJoin our Duwamish River Updates email list for announcements about interested parties meetings and other work about our Lower Duwamish Waterway source control actions.\n- A Pollutant Loading Assessment (PLA) for the Green-Duwamish Watershed (Focus Sheet, 2014)\n- Green/Duwamish River Watershed Pollutant Loading Assessment Technical Approach, (TetraTech for EPA and Ecology, 2014)\n- Appendices to the Green/Duwamish River Watershed Pollutant Loading Assessment Technical Approach\n- Existing Data and Model Evaluation memo, TetraTech, February 2015\n- Data Gaps and Pollutant Groupings memo, TetraTech, June 2015\n- Modeling QAPP for Green/Duwamish River Watershed PLA, TetraTech for EPA, 2016\n- LSPC Model Development and Hydrology Calibration for the Green/Duwamish River PLA, February 2017\n- Appendices to the LSPC Model Development and Hydrology Calibration for the Green/Duwamish River PLA, February 2017\nTechnical Advisory Committee Meeting #11\nTukwila Community Center Meeting Room A\nJune. 6, 2018, 9:00 a.m. – 12:00 p.m\n2424 42nd Avenue South\nTukwila, WA 98168"", ""MAN-MADE IMPACT ON GANGA RIVER AND FISHERIES\nJitendra Kumar, Ashish K. Pandey1, A.S. Kumar Naik*, V. Mahesh, Antim Shukla1, Avantika Pathak2\nDepartment of Fisheries Resources and Management,\nCollege of Fisheries, Karnataka Veterinary, Animal and Fisheries Sciences University, Mangalore - 575002, India, 1Narendra Deva University of Agriculture & Technology Kumarganj Faizabad (U.P.). 2Chandra Shekhar Azad University of Agriculture &Technology Kanpur (U.P.)\nThe Ganga river system, which has a total length of about 8,047 Km, is the most important river system in India and one of the largest in the world. The Ganga is a major river of the Indian subcontinent rising in the Himalaya Mountains and flowing about 2,510 km (1,560 mi) generally eastward through a vast plain to the Bay of Bengal. On its 1,560-mi (2,510-km) course, it flows southeast through the Indian states of Uttar Pradesh, Bihar, and West Bengal. In central Bangladesh it is joined by the Brahmaputra and Meghna rivers.\nGanga River known as Ganga Maata or Mother Ganges is revered as a goddess whose purity cleanses the sins of the faithful and aids the dead on their path toward heaven. In most Hindu families, a vial of water from the Ganga is kept in every house. It is believed that drinking water from the Ganga with one's last breath will take the soul to heaven. Hindus also believe life is incomplete without bathing in the Ganga at least once in their lifetime. Some of the most important Hindu festivals and religious congregations are celebrated on the banks of the river Ganga such as the Kumbh Mela or the Kumbh Fair and the Chhat Puja.\nBasic Information about the Ganges River of India\nTotal Length of River Ganges\n2,510 Kms (1,560 miles)\nAverage depth of Ganga River\n52 Feet (maximum depth, 100 feet)\nPlace of Origin of Ganga River\nFoot of Gangotri Glacier, at Gaumukh, at an elevation of 3,892 m\nArea drained by Ganges River (Ganges Plains)\n1,000,000 Square Kilometres\nMajor Tributaries of Ganges\nYamuna, Son, Kosi, Gandak, Gomati, Ghaghara, Bhagirathi etc...\nCities on the bank of Ganges\nKanpur, Soron, Allahabad, Varanasi, Patna, Ghazipur, Bhagalpur, Mirzapur, Buxar, Saidpur etc...\nHowever, the river is not just a legend; it is also a life-support system for the people of India\nIt is important because:\n• The densely populated Ganga basin is inhabited by 37 per cent of India's population.\n• The entire Ganga basin system effectively drains eight states of India.\n• About 47 per cent of the total irrigated area in India is located in the Ganga basin alone.\n• It has been a major source of navigation and communication since ancient times.\n• The Indo-Gangetic plain has witnessed the blossoming of India's great creative talent.\nPollution in Ganga River\nToday, over 29 cities, 70 towns, and thousands of villages extend along the Ganga banks. Nearly all of their sewage - over 1.3 billion litres per day - goes directly into the river, along with thousands of animal carcasses, mainly cattle. Another 260 million litres of industrial waste are added to this by hundreds of factories along the rivers banks. Municipal sewage constitutes 80 per cent by volume of the total waste dumped into the Ganga, and industries contribute about 15 percent.\nThe majority of the Ganga pollution is organic waste, sewage, trash, food, and human and animal remains. Over the past century, city populations along the Ganga have grown at a tremendous rate, while waste-control infrastructure has remained relatively unchanged. Recent water samples collected in Varanasi revealed fecal coli form counts of about 50,000 bacteria per 100 millilitres of water, 10,000% higher than the government standard for safe river bathing. The result of this pollution is an array of water-borne diseases including cholera, hepatitis, typhoid and amoebic dysentery. An estimated 80% of all health problems and one-third of deaths in India are attributable to water-borne diseases.\nThere is an urgent need to make people aware and get started to stop its pollution and degradation\nThe principal sources of pollution of the Ganga River can be characterized as follows:\n• Domestic and industrial wastes. It has been estimated that about 1.4 × 106 m3 d-1 of domestic wastewater and 0.26 × 106 m3 d-1 of industrial sewage are going into the river.\n• Solid garbage thrown directly into the river.\n• Non-point sources of pollution from agricultural run-off containing residues of harmful pesticides and fertilizers.\n• Animal carcasses and half-burned and unburned human corpses thrown into the river.\n• Defecation on the banks by low-income people.\n• Mass bathing and ritualistic practices.\nImpact of River water pollution\nThe pollutants include oils, greases, plastics, plasticizers, metallic wastes,suspended solids, phenols, toxins, acids, salts, dyes,cyanides, pesticidesetc. Many of these pollutants are not easily susceptible to degradation and thus cause serious pollution problems. Contamination of ground water and fish-kill episodes are the major effects of the toxic discharges from industries. Discharge of untreated sewage and industrial effluents leads to number of conspicuous effects on the river environment. The impact involves gross changes in water quality viz reduction in dissolved oxygen and reduction in light penetration that tends to loss in self purification capability of river water.\nThe Facts about the Ganges River Pollution\nApproximately 1 billion litres of raw, untreated sewage are dumped in the river on a daily basis. The amount has more than doubled in the last 20 years and experts predict another 100% increase in the following 20 years.\nThe rapid explosion of India's population in the last 25 years coupled with lax regulations on industry has put a huge strain on the river.\nThousands of bodies are cremated on the banks of the river yearly with many being released into the river with hopes that their souls may have a direct path to heaven.\nHundreds Unwanted or 'illegitimate' babies, cattle and other animal carcases are also dumped in the Ganges again with religious significance\nThe levels of Colliform bacteria are over 2800 times the level considered safe by the W.H.O (World Health Organization).\nImpact on Fisheries\nFarakka Barrage has also resulted in occupational displacement of the fisher people in both upstream and downstream. For a long time fisher people in Bihar have been protesting against the barrage as this has hindered the natural migration of valuable fishes from the sea, especially Hilsa, a delicacy.\nHuman impact on rivers\nMan impacts rivers in many ways. The often harmful flow of substances produced by humans is referred to as emission or load. Solid matter, humus, certain metals, nutrients and acidifying substances leach into rivers as a consequence of land use and point loading within a drainage basin. These substances cause many kinds of changes in the aquatic environment and also in the species distribution and abundance of aquatic organisms. Moreover, often the recreational value of the river becomes diminished. Often human actions also change the river flow or the shape of the riverbed. This happens, for example, when riverbeds are cleaned or the water flow is regulated.\n1. Illegal Mining\nExample: Haridwar In and around the town, boulders abutting the river are being removed for construction, causing damage to the river's banks and bed. Tractors and trucks often just drive through the bed in the dry season.\n2. Climate Change\nExample: Devprayag The melting of the Gangotri glacier, the source of the Ganga, has accelerated. This could impact water flow in the river in dry seasons, specially in Devprayag where around 30 per cent of the water flow is from melted snow and glacier. One controversial estimate says the glacier may disappear by 2035.\n3. Ill-Planned Dams\nExample: Downstream Uttarkashi Over 600 dams are either operational, under construction or being planned along the rivers Alaknanda and Bhagirathi, that combine to form the Ganga at Devprayag. Besides causing great damage to the geography of the region, the dams obstruct the natural flow of the river, thus reducing the oxygen content in the river. A lack of it is killing many of the living organisms that are key to ensuring the health of the river.\n4. Untreated Sewage\nExample: Kanpur Untreated sewage is a big problem all along the river from source to sea. Tanneries in Kanpur have been dumping waste into the Ganga. Only about a third of the sewage and industrial effluent being discharged into the river is currently treated.\n5. Sewers & Waste Water Canals\nExample: Calcutta Twenty-six major and numerous minor nallahs flow into the Ganga in Bengal. The sewage treatment plants do not function effectively. According to experts, the water of the Ganga is not even fit for bathing in Bengal, Varanasi and most major cities along the river (see map). Along with industrial pollution, untreated sewer water is a major problem.\n6. Body Dumping\nExample: Varanasi Half-burnt bodies are washed away in the river. Some bodies, like those of sadhus, are traditionally thrown into the river and not cremated. Around 35,000 bodies are cremated on the ghats in Varanasi every year. An electric crematorium set up in 1989 is in a state of disrepair.\n7. Industrial waste\nAbout 35.3% of the over 3 million small-scale industrial units (SSIUs) are of polluting nature. In case of large water polluting industrial units discharging effluents into the rivers and lakes, only 29% have adequate effluent treatment plant (Ministry of Environment & Forests. 1997). CPCB has identified 17 categories as highly polluting industries for priority action like sugar sector followed by pharmaceutical, distillery, cement and fertilizer.""]"	['<urn:uuid:de6fd6a9-7520-4bd1-a107-4a0f31fc0b4d>', '<urn:uuid:5247a7a0-e02f-4615-953b-1dc0f74bad79>']	open-ended	with-premise	short-search-query	similar-to-document	comparison	novice	2025-05-13T01:23:33.086345	7	105	2102
62	As someone researching Pacific War military leadership, I've been comparing the situations of MacArthur and Yamamoto in 1942 - how did their respective positions and early war experiences differ after Pearl Harbor?	After Pearl Harbor, MacArthur and Yamamoto had quite different trajectories. Yamamoto, as Commander-in-Chief of the Combined Fleet, led Japan to initial successes including the devastating attack on Pearl Harbor and subsequent victories in the Western Pacific. His forces sank eight US battleships at Pearl Harbor and achieved air and naval superiority across the region. Meanwhile, MacArthur was forced to evacuate from the Philippines to Australia in March 1942, but was then appointed supreme commander of all Allied forces in the Southwest Pacific Area. In Australia, MacArthur established his General Headquarters in Melbourne and developed a complex intelligence network under Col. Willoughby, including the highly effective ATIS program.	['Allied Translator and Interpreter Section activated\nSept. 19, 1942\nAfter the bombing of Pearl Harbor on Dec. 7 1941, President Roosevelt declared war on Japan the next day. Three days later, Hitler declared war on the United States. Thus the American military was thrust suddenly into World War II, fighting in two different theaters, against vastly different enemies.\nGen. Douglas MacArthur, who had been commanding forces in the Philippines, arrived in Australia in March 1942 and was soon thereafter appointed as the supreme commander of all Allied forces in the Southwest Pacific Area, or SWPA.\nThe SWPA was constituted on April 18, 1942, by agreement among the Governments of Australia, the United Kingdom, the Netherlands and the United States. On that date, MacArthur assumed command and proceeded to establish his General Headquarters at Melbourne.\nMacArthur’s assistant chief of staff for Intelligence was Col. Charles Willoughby, who oversaw the complex intelligence network of the SWPA Theater.\nWilloughby set up a number of Allied intelligence collection organizations under his direct control. The Central Bureau handled cryptologic functions. Human Intelligence came from the Allied Intelligence Bureau, whose mission was to collect intelligence through clandestine operations behind enemy lines, conduct sabotage operations and recruit aid from the natives. The Allied Geographical Section was formed to collect and assemble topographic information, and to prepare and publish reports and locality studies on areas of immediate tactical interest. The most productive single intelligence agency established under the G2 of the SWPA was the Allied Translator and Interpreter Service, or ATIS, which was organized on Sept. 19, 1942.\nATIS effectively neutralized the Japanese language barrier — one of the greatest advantages possessed by the Japanese, as it was almost as effective as a secret code. The G2 employed hundreds of second-generation Japanese-Americans, called Nisei, in linguist detachments under ATIS. They accompanied assault landing forces across the beachhead and on inland, conducted spot interrogations, translated captured maps and plans, and gave the psychological warfare planners excellent insight into the morale problems of enemy soldiers through the exploitation of letters and diaries. Other captured and translated documents revealed the enemy’s food and supply problems, his order of battle, the effects of Allied air attacks and the effectiveness of both Allied and Japanese weapons.\nThe Commander of the ATIS, Col. Sidney Mashbir, in his book “I Was an American Spy,” called the ATIS document translation process a “brain-power quantity production line.” He stated that by the end of the war, ATIS had interrogated 14,000 prisoners, translated almost two million documents, and published more than 20 million pages of Japanese intelligence.\nThe irony was that the Japanese kept scrupulous records, which they rarely encrypted, having absolute confidence in the security the language barrier afforded. ATIS used this to their complete advantage, compiling nearly complete sets of unit papers, including war diaries, organizational rosters, intelligence reports, pay books, postal-savings books, correspondence and personal possessions. Using these captured items, they slowly and patiently built up a mosaic picture of the enemy force.\nBy the time the American forces reached Manila in January 1945, Mashbir boasted that [ATIS] “literally knew more about the Japanese Army than most of its own officers, because, as a matter of fact, we had their records. This was equally true of the Navy and Air Force.” Over the course of the war, ATIS issued thousands of printed documents, providing intelligence of immediate operational importance as well as overall strategic value.', 'Japan entered the Pacific War in December 1941 (the 16th year of the Showa Era), plunging into a “sea of battles” for three years and nine months. What on earth was the purpose of this war? How did Japan end up headed for catastrophe after an initial series of victories? This section will mainly examine the Battle of Midway, a decisive battle which set the course for Japan’s defeat.\nAmbiguity in war goals\nAdmiral Isoroku Yamamoto was the Commander-in-Chief of the Combined Fleet. (See Footnote.)\nOn December 8, 1941, Air Strike Leader of the aircraft carrier Akagi of the Combined Fleet, Mitsuo Fuchida, radioed the following coded message when his plane started to fly over the U.S. Pacific fleet anchored at Pearl Harbor in Hawaii: “To to to to.... (All units, charge).” “To” stood for totsugeki, meaning “charge.”Attack bomber regiments that took off from six aircraft carriers dealt devastating blows to all eight battleships belonging to the U.S. Pacific Fleet.\nMeanwhile, in operations southwest of Japan, the Twenty-fifth Army commanded by Tomoyuki Yamashita landed on the Malay Peninsula while regiments of Zero fighters based in Taiwan took control of the air over the Philippines within a few days.\nOn December 10, two days after the opening of the Pacific War, more than 80 Japanese attack planes succeeded in sinking the British battleship HMS Prince of Wales and the battle cruiser HMS Repulse just prior to the Battle of Malaya. In February 1942, Japan conquered and occupied the British colony of Singapore. This turn in events was soon followed in March by the evacuation of U.S. General Douglas MacArthur, Commander of U.S. Forces from the Philippines to Australia.\nIndeed, Japan was able to take control of the sea and air in the Western Pacific all at once.\nAfter a series of victories, however, Japan started on the a path to its defeat.\nHere, we would like to examine Japan’s purpose in this war—whether it went to war for “self-preservation and self-defense” or whether it aimed to create the Greater East Asia Co-prosperity Sphere.\nIndeed, the very ambiguity of Japan’s war goals led to the failure of its war management.\nOn November 2, 1941, Prime Minister Hideki Tojo had an audience with Emperor Showa to inform him of the government’s decision to go to war. The Emperor asked Tojo, “What do you think is the just cause of starting this war?” Tojo replied, “We are in the process of studying that, and I will report to you on that soon, Your Majesty.”\nAt the Imperial Headquarters-Government Liaison Conference, where the Imperial policy decision was made to wage war earlier, no substantial discussion was conducted on the “just cause” of going to war. Within the Navy, the dominant opinion was that the purpose of the war should be “self-preservation and self-defense.” The Navy insisted that Japan had no choice but to go to war because it had no oil available after the United States imposed an oil export embargo.\nAlso within the Army, the majority supported the notion that Japan would go to war for “self-preservation and self-defense.”\nBut Tojo, who also held the War Minister portfolio, made it clear that the creation of the Greater East Asia Co-prosperity Sphere was the “basis” of Japan’s policies every time he was briefed on the process of planning national policies. Kenryo Sato, chief of the Military Affairs Section of the War Ministry, constantly insisted that Japan’s goal was to create the Greater East Asia Co-prosperity Sphere while ignoring the arguments for “self-preservation and self-defense.”\n“[The Army is not so enthusiastic about] a war with the United States as the Navy and the government. Nevertheless, the Army is willing to fight a 100-year war if that’s what it takes to complete the Second Sino-Japanese War.” The Army obviously was preoccupied with the war with China.\nFor Tojo and others, the establishment of the Greater East Asia Co-prosperity Sphere through war with the United States and Britain was Japan’s last resort to make China surrender.\nThe Principles for Implementation of the Imperial Policy, which was officially approved at a meeting in the presence of the Emperor on November 5, 1941, stated that “the Empire of Japan decided to wage war against the United States, Britain and the Netherlands to make a breakthrough in the current crisis, realize self-preservation and self-defense and build a new order in the Greater East Asia.”\nHowever, the outline merely put in writing the arguments of both the Army and Navy. Later, the phrase “Greater East Asia Co-prosperity Sphere” developed a life of its own, and the Army and Navy made separate plans for operations without trying to coordinate their moves.\nThe Daikairei 1-go, Navy Operation Order Number 1, issued on November 5 in preparation for attacking the southern areas (Southeast Asia), stated the purpose of the war was for “self-preservation and self-defense.” It was written on the assumption that the war would be short. On the other hand, the Dairikumei 564-go, Army Operation Order Number 564, which cited both “self-preservation and self-defense” and “building a new order in the Greater East Asia” as pretexts for going to war, envisioned a long, drawn-out war toward the goal of the reorganization of the Asian colonies held by Britain, the Netherlands and the United States.\nThe Imperial Rescript on the War issued on December 8, underlined that the war was started for self-preservation and self-defense. On December 10, however, the Imperial Headquarters-Government Liaison Conference decided to call the war the “Greater East Asia War.” According to the Cabinet’s Information Department, the war was so named to indicate its purpose of building a new order in the Greater East Asia.\nThe central concept of “Greater East Asia Co-prosperity Sphere” was to geopolitically divide the world into four major blocs: Greater East Asia, the Americas, the Soviet Union and Europe, with Japan becoming the leader of the Greater East Asia Co-prosperity Sphere. Under this scenario, the Kwantung Army was beefed up in strength to prepare for a possible war with the Soviet Union even after Japan started the war against the United States; the Army also maintained the strength of Japan’s China Expeditionary Army. The Army intended to let the Navy engage in battle with the United States in the Pacific.\nAfter the war ended, Tojo said, “The motivation behind resorting to the use of force was the desire for self-preservation and self-defense.” He also said, “Once the war started, however, we saw to it that the policies aimed at realizing the Greater East Asia Co-prosperity Sphere could be implemented.” Thus the creation of the Greater East Asia Co-prosperity Sphere became Japan’s national policy without due political process.\nMeanwhile, the final goal for Japan’s southward advancement—that was launched to attain self-sufficiency—was the Dutch East Indies (presently Indonesia). The Netherlands, however, was excluded from the enemy countries listed by the Imperial Rescript on the War. The Netherlands was regarded by Japan as a “quasi-enemy,” and the idea was floated to negotiate with the Dutch East Indies once the war started so that Japan would be able to bloodlessly invade and occupy the archipelago under control of the Dutch East India Company. It was not until January 1942 that Japan launched attacks on the Dutch East Indies.\nAt the time the war started, the United States and Britain were alarmed by the possibility of Japan’s gaining access to oil and other resources in the Dutch East Indies by controlling Southeast Asia. They feared Japan was prioritizing gaining a foothold in Asia to fight a long drawn-out war, and avoiding direct confrontation with the United States and Britain and carefully watching their war against Germany in Europe. But Japan defied their assumptions and chose to directly confront the United States and Britain with an attack on Pearl Harbor.\nJapan’s Combined Fleet\nThe core of the Imperial Japanese Navy consisted of two or more fleets. The Combined Fleet was formed before the opening of the first Sino-Japanese War in 1894. Based on the Navy’s tradition of commanders taking the lead and setting the example for subordinates, the Fleet Commander was based aboard his flagship. The Commander-in-Chief was assisted by his Chief of Staff and a large number of staff officers. The Combined Fleet Commander-in-Chief received orders for administration and for operations including battle campaigns from the Chief of the Naval General Staff. But, on many occasions, once the Combined Fleet commander was entrusted with his assignment as the supreme commander of naval operations, he would increasingly clash with the Naval General Staff.']	['<urn:uuid:e25cb3d7-6f38-493d-8cc7-305414eb0591>', '<urn:uuid:68f157d8-b12f-4126-8331-3c4f85740fa3>']	open-ended	with-premise	verbose-and-natural	similar-to-document	comparison	expert	2025-05-13T01:23:33.086345	32	107	1991
63	beneficial role gut bacteria health effects antibiotics damage intestine	Beneficial gut bacteria like Bifidobacterium play crucial roles in maintaining intestinal health by producing lactic acid and acetic acid, which help inhibit harmful microorganisms and maintain a healthy gut environment. However, antibiotic use can significantly disrupt these beneficial bacterial communities, leading to various negative effects including reduced barrier integrity, increased inflammation, and compromised immune responses. Even short antibiotic courses can cause dramatic alterations to intestinal bacterial communities, particularly reducing beneficial Lactobacillus species. This disruption can affect tight junction expression, mucin production, and antimicrobial peptides, potentially leading to both local gut problems and systemic health issues.	"['Characteristics of Bifidobacterium\nMembers of the bacterial genus Bifidobacterium are gram positive, strictly anaerobic, non-spore-forming, and usually branch-shaped, rod bacteria. As this species of bacteria is primarily anaerobic and typically thrives in the absence of oxygen, the oxygen-poor environment in the large intestine makes the perfect home for them.\nWith more than 30 species already identified, Bifidobacterium represents one of the largest genera within the Actinobacteria phylum. This genus of bacteria was isolated in 1899 from the faeces of breast-fed babies; this fact is of particular interest as Bifidobacterium are the most common genus of bacteria found in the intestines of healthy infants. Bifidobacterium are found mainly in animals and humans.\nBifidobacterium and Breast-fed Babies\nThis prevalence of Bifidobacterium in the gut of breast-fed infants may be due to the ability of this species to metabolise lactose (milk sugars). In infants who are breast-fed, Bifidobacteria constitute about 90% of their intestinal bacteria; however, it is interesting to note that this number is typically lower in bottle-fed infants. The lower number of Bifidobacteria in formula-fed babies might account for the higher incidence of diarrhoea and allergies that has been noted in these babies.\nWhile Bifidobacterium infantis, Bifidobacterium breve, and Bifidobacterium longum are the largest group of bacteria found in the intestine of infants, Bifidobacteria are said to be only the 3rd or 4th largest group in adults and comprise only 3-6% of adult faecal flora. It is noted that populations of Bifidobacteria decline quite rapidly as humans reach old age, which could account for the gastro-intestinal difficulties that are so common in older people.\nBifidobacterium and Fermentation\nBifidobacterium are known as LAB (Lactic Acid Bacteria) as they ferment carbohydrates, and produce lactic acid as a primary metabolic end product of the fermentation process. More specifically, Bifidobacteria are known to have the ability to break down oligosaccharides, complex carbohydrates made up of multiples of simple sugars such as glucose, which are often referred to as ‘prebiotics’.\nThe production of lactic acid by Bifidobacterium, and the resulting acidification of their environment, helps to inhibit the growth of other micro-organisms that are not conducive to their own existence or the health of their host, such as pathogenic bacteria and yeasts. The pathogens do not thrive in a low pH (acidic) environment, so this action helps to maintain a healthy balance of intestinal flora; provides protection from pathogenic intruders; helps to aid mineral absorption for the host, and may help to protect against intestinal permeability or ‘leaky gut’, a condition associated with higher levels of pathogenic micro-organisms in the intestines.\nBut whilst they are more commonly known for being lactic acid producers, Bifidobacteria actually produce a large amount of acetic acid, a short chain fatty acid (SCFA). Acetic acid is even more effective than lactic acid at reducing problematic yeasts and moulds in the gut environment, another factor that renders this genus of bacteria a highly desirable resident of the colon.\nAs some properties & benefits of probiotics may be strain-specific, this database provides even more detailed information at strain level. Read more about the strains that we have included from this genus below.\nBifidobacterium lactis strains: Bifidobacterium lactis Bi-07, Bifidobacterium lactis BB-12®, Bifidobacterium lactis HN019 and Bifidobacterium lactis Bl-04.\nBifidobacterium infantis strains: Bifidobacterium infantis 35624.\nBifidobacterium breve strains: Bifidobacterium breve M-16V®.\nFor more insights and professional updates on probiotics, please visit the Probiotic Professionals pages.\nBaffoni L.B. et al., (2013), ‘Identification of species belonging to the Bifidobacterium genus by PCR-RFLP analysis of a hsp60 gene fragment’, BMC Microbiology, 13:149.\nConlon, M. A., & Bird, A. R. (2015), ‘The Impact of Diet and Lifestyle on Gut Microbiota and Human Health’. Nutrients, 7(1), 17–44.\nLee Y., & Salminen S., (2009), ‘Handbook of Probiotics and Prebiotics’, Hoboken, New Jersey: John Wiley & Sons, pp 7-14.\nMayo, B. & van Sinderen, D., (2010), ‘Bifidobacteria: Genomics and Molecular Aspects’.Poole: Caister Academic Press.\nMitsuoka, T., (1996), ‘Intestinal flora and human health’. Asia Pacific J. Clin. Nutr., 5:2-9.\nGarrity G., (1986), Bergey’s Manual of Systematic Bacteriology (Vol. 2, The Proteobacteria). Baltimore, MD: Williams & Wilkins, pp. 1418-1434.\nInformation on this genus was gathered by Joanna Scott-Lutyens BA (hons), DipION, Nutritional Therapist; and Kerry Beeson, BSc (Nut.Med) Nutritional Therapist.\nLast updated - 18th September, 2018', ""Michael Ash BSc (Hons), DO, ND FDipION reviews the current understanding of the role of antibiotics in the initiation of gut associated inflammation and local and systemic health problems, and briefly explores some strategies to prevent and manage this.\nWhat is perhaps the greatest medicinal discovery in the last 100 years has a sting in its tail, the tremendous success in managing bacterial infection has encouraged over and inappropriate use of antibiotics, the problems of which have been well documented. This review explores the developing comprehension that even a single day of antibiotic use has consequences that may produce transient and long term effects that compromise the health and well being of the patient and their bacterial co-habitants.\nSir Alexander Fleming discovered the antibiotic substance penicillin in 1928 and was awarded a co share in the Nobel Prize in Medicine in 1945.\nIt was a discovery that would change the course of history. The active ingredient in that mould, which Fleming named penicillin, turned out to be an infection-fighting agent of enormous potency. When it was finally recognised for what it was—the most efficacious life-saving drug in the world—penicillin would alter forever the treatment of bacterial infections. By the middle of the century, Fleming’s discovery had spawned a huge pharmaceutical industry, churning out synthetic penicillin’s that would conquer some of mankind’s most ancient scourges, including syphilis, gangrene and tuberculosis. (Time Magazine April 1999)\nHowever, as the combined benefits of decent engineering for sanitation, prevention via vaccination and bacterial infection control through antibiotics have contributed to life extension, they have also produced microbe and human disturbances. The incidence of immune mediated disorders is continuing to increase and the gastrointestinal tract is continuing to gain traction as a site of significant origination.,\nMost healthcare professionals are now aware of the complication of prolonged antibiotic therapy and the risk of developing antibiotic resistance or trans/bacterial development. Patients are often less comfortable with this notion and will still seek antibacterial treatments for primary viral illness, follow the treatment protocol badly and continue to apply dubious standards of self care.\nThe Journal of Mucosal Immunology in March 2010, ran an editorial and paper on the adverse effects of antibiotics beyond those previously understood. Most Nutritional Therapists and clinicians will agree that there may be some level of bacterial disruption after a course of treatment. Most conservative researchers had suggested that the ‘fingerprint’ of species in the gut, maintained through bacterial communication techniques would ensure the bacterial architecture resolves a short time after cessation. Bonnie Basler PhD describes this quorum communication in an informative film from one the famous TED presentations.\nSome intestinal bacteria are mutualists that promote normal human physiology including proper digestion, metabolism, epithelial cell function, angiogenesis, enteric nerve function, and immune system development. Although bacterial communities in the intestine promote normal immune homeostasis, patients with inflammatory bowel disease or allergies have altered intestinal bacteria, indicating that microbial populations might influence disease pathogenesis and in particular those linked with adverse immune driven inflammation.\nIt is now understood that the composition of the microbiota is significantly altered by the use of antibiotics including the increase in urinary tract infections, diminished carbohydrate fermentation, loss of bile acid metabolism, pathogenic bacterial colonisation, immune disturbances, barrier defects and mucin degradation.\nMicrobe Associated Molecular Patterns (MAMP’S) are the molecular signatures used by bacteria to impart friendly messages to the mucosal immune receptors. They are also used to promote defence when a pathogen’s sticky patterns are identified. It is proposed that a change in the concentrations of these messengers by antibiotic therapy could disrupt the homeostatic health of the gut contributing to various significant changes in the competence of the gut related immune responses. It is becoming clear that a large part of our bacterial bedfellows have similar MAMP’S allowing them to operate in a symbiotic manner for immune tolerance and gastrointestinal health. It is this strategy used by probiotics to impart different messages to maintain a healthy digestive tract, and may also be used in the resolution or management of more complex immune mediated diseases.\nOur mucins are a vital part of the overall management, composed of an inner layer, a dense composition and free of bacteria, and the outer layer, made up of a looser matrix and offering a home to bacteria. The inner layer acts like a muffler, diminishing contact with the intraepithelial lining and yet still able to allow MAMP’s via dendritic cells to communicate with the immune system. The use of probiotics; L.plantarum and L.Rhamnosus (LGG) have shown a beneficial increase in this mucin layer, adding volume to the material and improving resistance to pathogen adhesion.\nA loss of this layer as shown in the diagram above may increase the total information load on the lining cells by loss of the muffling mucins in effect increasing the frequency and noise of the bacterial chatter, and may also facilitate translocation of bacteria across the lining, inducing an inflammatory response and additional loss of barrier quality. This increase in inflammation may induce local damage such as IBD but may also induce collateral damage to tissues far distant from the gut, such as the brain, joints and skin as described in a previous post.\nBacteriotherapy (the treatment of disease by the use of bacteria or their products)\nThe use of certain strains of probiotics have demonstrated barrier protection and cell survival. The opposite effect of loss of barrier integrity has many potential health complications related to inflammation as explained in a previous article. Antibiotic therapy offers the reverse scenario and contributes to pro-inflammatory cytokine production and loss of antimicrobial proteins, used by the gut tissues to maintain microbiota colonies and prevent pathogens binding. The use of selected probiotics (MAMP’s) during and post antibiotic therapy to selectively agitate the mucosal immune system represents a safe strategy.\nOur bacterial bedfellows and the single cell lining of the gut communicate through a variety of mechanisms, a loss of the bacterial composition can alter, tight junction expression, mucins, antimicrobial peptides and cytokine ratio’s. The resulting disturbance may then as a result affect the TH17/Treg ratio in the gut leading to increased production of the cytokine (IL-17), associated with inflammatory and autoimmune disease. Interestingly we as humans use our food source to provide control over the adverse production of this protein, Vitamin A derived from our foods, helps to programme the naive T cells in the gut to develop Treg or peacekeeping cells, rather than the inappropriate conversion into TH17 cells. The role of Vitamin A in mucosal health is well understood but it’s unique role in mucosal tolerance is only just being fully elucidated.\nAntibiotics used in clinic might be used to remove or suppress undesirable components of the human microbiome. Antibiotic treatment causes significant temporal and spatial alterations in various colonies of bacteria that have been proposed to have causative or therapeutic roles in human diseases. Antibiotic treatment also has significant negative effects on the output of pro-inflammatory cytokines in gut associated lymphoid tissues.\nFindings from this paper indicated that even short antibiotic courses can result in dramatic alterations to intestinal bacterial communities. In particular, significant reductions in the frequency of mucosal-associated Lactobacillus species were observed in antibiotic-treated as compared with control-treated animals.\nProbiotics can introduce missing microbial components with known beneficial functions for human health. Prebiotics such as those derived from apple soluble fibre can enhance the proliferation of beneficial microbes or probiotics as well as diminishing total inflammation, to maximise sustainable changes in the human microbiome.\nCombinations of these approaches might provide synergistic and effective therapies for specific disorders. The human microbiome could be manipulated by such “smart” strategies to prevent and treat acute gastroenteritis, antibiotic-associated diarrhoea and colitis, inflammatory bowel disease, irritable bowel syndrome, necrotising enterocolitis, and a variety of other disorders in which altered inflammation is a key driver.\nNutritional Therapists and other healthcare practitioners are going to have to consider the interactions of the medicines, foods, Saccharomyces Boulardii, and strain specific probiotics to optimise rather than compromise the restoration of gastrointestinal function – this is an exciting area of opportunity for specialism and research.\nThe incidence of lactic acid bacterial depletion and its potential role in the health of humans may make the use of a special strain such as LGG or combinations of strains a practical approach to post antibiotic minimisation of adverse immune disruption in the mucosal tissues. The role of SIgA in mucins and bacterial balance is also an area of significant opportunity for treatment, and Saccharomyces Boulardii has been shown to reduce IL-8 and increase SIgA as well as limit adverse effects associated with antibiotic therapy. In the gastrointestinal tract, SIgA exhibits properties of a neutralising agent (immune exclusion) and of an immunopotentiator inducing effector immune responses in a ‘non-inflammatory context’ favourable to preserve local homeostasis and control disease risk.\nS. boulardii effectively reduces the risk of antibiotic-associated diarrhoea in children. \nAntibiotics, and most likely other agents of bacterial disruption, can alter the composition of the human gastrointestinal bacteria. In some people this may be enough to cause a loss of barrier quality and other adverse changes. The increase in immunological chatter can then cause damage locally and systemically via numerous routes.\nNo longer can antibiotics be viewed as a benign self recovering challenge to commensal bacteria. Nutritional Therapists can lead the way in the judicious use of strain specific bacteria and yeast to prevent and restore these damaged tissues.\n Johansson ME, Phillipson M, Petersson J, Velcich A, Holm L, & Hansson GC (2008). The inner of the two Muc2 mucin-dependent mucus layers in colon is devoid of bacteria. Proceedings of the National Academy of Sciences of the United States of America, 105 (39), 15064-9 PMID: 18806221\n Mack DR, Michail S, Wei S, McDougall L, & Hollingsworth MA (1999). Probiotics inhibit enteropathogenic E. coli adherence in vitro by inducing intestinal mucin gene expression. The American journal of physiology, 276 (4 Pt 1) PMID: 10198338\n Brandl K, Plitas G, Mihu CN, Ubeda C, Jia T, Fleisher M, Schnabl B, DeMatteo RP, & Pamer EG (2008). Vancomycin-resistant enterococci exploit antibiotic-induced innate immune deficits. Nature, 455 (7214), 804-7 PMID: 18724361\n Hill DA, Hoffmann C, Abt MC, Du Y, Kobuley D, Kirn TJ, Bushman FD, & Artis D (2010). Metagenomic analyses reveal antibiotic-induced temporal and spatial changes in intestinal microbiota with associated alterations in immune cell homeostasis. Mucosal immunology, 3 (2), 148-58 PMID: 19940845\n Kotowska M, Albrecht P, & Szajewska H (2005). Saccharomyces boulardii in the prevention of antibiotic-associated diarrhoea in children: a randomized double-blind placebo-controlled trial. Alimentary pharmacology & therapeutics, 21 (5), 583-90 PMID: 15740542\n- All Immunity is Mucosal – The GUT is No 1\n- A Novel Approach to Treating Depression – How Probiotics Can Shift Mood by Modulating Cytokines\n- Coeliac Disease – Local & Systemic Consequences\n- Lactobacillus GG: A Potent Immune Regulator Effective in Many Disorders\n- Whats New in The Understanding Of The Immunology Of Ulcerative Colitis?\nKeywords:antibiotics, autoimmune, bacteria, cytokines, diet, dysbiosis, evidence, gut, gut health, immunity, inflammation, microbiome, mucosal, nutrition, probiotics, regulatory T cells, Saccharomyces Boulardii\n3 Responses to “Antibiotics Can Cause Gut Related Diseases”\nSee what others are saying about this post...\nYou can ask technical questions, be as supportive, critical or controversial as you like, but please don't get personal or offensive, and do keep it brief. Your comments will be published only after verification.""]"	['<urn:uuid:81443edb-f4d2-4218-ad93-d61b5077e90f>', '<urn:uuid:c93be313-9e75-42db-9fe1-db72fc438485>']	open-ended	direct	long-search-query	distant-from-document	multi-aspect	novice	2025-05-13T01:23:33.086345	9	95	2593
64	what connects climate change indicators sustainability reporting	Climate change indicators are integrated into sustainability reporting frameworks, particularly through GRI Standards. In measuring progress towards sustainable development, climate change indicators focus on both absolute emissions levels and efficiency measures, while GRI Standards include specific emissions standards (GRI 305) and comprehensive environmental reporting requirements that help organizations track and report their climate impact.	['12.1.3 Measurement of progress towards sustainable development\nAs what is managed needs to be measured, managing the sustainable development process requires a much strengthened evidence base and the development and systematic use of robust sets of indicators and new ways of measuring progress. Measurement not only gauges but also spurs the implementation of sustainable development and can have a pervasive effect on decision-making (Meadows, 1998; Bossel, 1999). In the climate change context, measurement plays an essential role in setting and monitoring progress towards specific climate change related commitments both in the mitigation and adaptation context (CIESIN, 1996-2001).\nAgenda 21 (Chapter 40) explicitly recognizes the need for quantitative indicators at various levels (local, provincial, national and international) of the status and trends of the planet’s ecosystems, economic activities and social wellbeing (United Nations, 1993). The need for further work on indicators at national and other levels was confirmed by the Johannesburg Plan of Implementation (UNEP, 2002).\nAs pointed out by Meadows (1998), indicators are ubiquitous, but when poorly chosen create serious malfunctions in socio-economic and ecological systems. Recognizing the shortcomings of mainstream measures, such as GDP, in managing the sustainable development process, alternative indicator systems have been developed and used by an increasing number of entities in various spatial, thematic and organizational contexts (Moldan et al., 1997; IISD, 2006).\nIndicator development is also driven by the increasing emphasis on accountability in the context of sustainable development governance and strategy initiatives. In their compilation and analysis of national sustainable development strategies, Swanson et al. (2004) emphasize that indicators need to be tied to expected outcomes, policy priorities and implementation mechanisms. As such, the development of indicators may best be integrated with a process for setting sustainable development objectives and targets, but have an important role in all stages of the strategic policy cycle. Once priority issues are identified, SMART indicators need to be developed - indicators that are Specific, Measurable, Achievable, Relevant/Realistic and Time-bound.\nBoulanger (2004) observes that indicators can be classified according to four main approaches: (1) the socio-natural sectors (or systems) approach, which focuses on sustainability as an equilibrium between the three pillars of sustainable development but which overlooks development aspects: (2) the resources approach, which concentrates on sustainable use of natural resources and ignores development issues: (3) a human approach based on human wellbeing, basic needs; and (4) the norms approach, which foresees sustainable development in normative terms. Each approach has its own merits and weaknesses. Despite these efforts at measuring sustainability, few offer an integrated approach to measuring environmental, economic and social parameters (Corson, 1996; Farsari and Prastacos, 2002; Swanson et al., 2004). This review of indicators illustrates a significant gap in macro-indicators in that few include measures of progress with respect to climate change.\nIndicator system development typically builds on a conceptual framework serving as a link between relevant world views, sustainability issues and specific indicators. Some of the more common ones include the pressure-state-impact framework and capital-based frameworks covering social, environmental and economic domains. Given the ambiguity of the concept of sustainable development and differences in socio-economic and ecological context, even the use of comparable indicator frameworks usually results in non-identical indicator sets (Parris and Kates, 2003; Pintér et al., 2005).\nVarious alternative approaches to estimate macro progress towards sustainable development have been developed. Many of these approaches integrate, though not necessarily focus on, aspects of climate change. One approach to indicator development focused on monetary measures and involves adjustment to the GDP. These include, for example, calculation of genuine savings (Hamilton and et al., 1997; Pearce, 2000), Sustainable National Income (Hueting, 1993), and efforts to develop a measure of sustainability (Yohe and Moss, 2000). In an attempt to aggregate and express resource consumption and human impact in the context of a finite earth, a number of indices based on non-monetary, physical measures were created. These indices may be based on the concepts of environmental space or ecospace, and ecological footprint (Wackernagel and Rees, 1996; Venetoulis et al., 2004; Buitenkamp et al., 1993; Opschoor, 1995; Rees, 1996). Vitousek et al. (1986) proposed the index of Human Appropriation of Net Primary Production (HANPP). This approach specifies the amount of energy that humans divert for their own use in competition with other species.\nIn trying to avoid shortcomings from the concept of carrying capacity applied to human societies the formula I = PAT, where I is the human impact on the environment, P the human population, A the affluence (presumably per capita income), and T the effect of technology on the environment, has been commonly used in decomposing the impact of population, economic activity, and fuel use on the environment in general and on historical and future carbon emissions in particular (IEA, 2004c; Kaya, 1990; Schipper et al., 1997; Schumacher and Sathaye, 2000). Other approaches include the development of a ‘global entropy model’ that inspects the conditions for sustainability (Ruebbelke, 1998). This is done by employing available entropy data to demonstrate the extent to which improvements in entropy efficiency should be accomplished to compensate the effects of increasing economic activity and population growth. Other sets of metrics have less precise ambitions but aim to explain to the larger public the risks of environmental change, such as the notion ‘ecological footprint’ [see above] used by some NGOs. In this, the aggregate indicators are noted as the number of planets Earth needed to sustain the present way of living of some regions of the World.\nAs Bartelmus (2001) observes, many of the aggregate indices are yet to be accepted in decision-making due, among others, to measurement, weighting and indicator selection challenges. However, besides efforts to develop aggregate indices either on a monetary or physical basis, many efforts are aimed at developing heterogeneous indicator sets. One of the commonly accepted frameworks uses a classification scheme that groups sustainability issues and indicators according to social, ecological, economic, and in some cases, also institutional categories. Several indicator systems developed at international and national level have adopted a capital-based framework following the above categories. They link indicators more closely to the System of Integrated Environmental and Economic Accounts System of National Accounts (SNA), including its environmental component, (Pintér et al., 2005). At the United Nations, the Division for Sustainable Development led the work on developing a menu and methodology sheets for sustainability indicators that integrate several relevant for climate change from the mitigation and adaptation point of view (UNDSD, 2006). Also, the UNECE/Eurostat/OECD Working Group on Statistics for Sustainable Development is developing a conceptual framework for measuring sustainable development and recommendations for indicator sets. A set of climate change mitigation input and outcome indicators should be included.\nWhile not necessarily focused on climate change per se, many of these indicator efforts include climate change as one of the key issues, on the mitigation or adaptation side. Keeping a broader perspective is essential, as climate change, including its drivers, impacts and related responses, transcend many sectors and issue categories. Indicators are needed in all in order to identify and analyze systemic risks and opportunities. In the mitigation context, quantifying emissions and their underlying driving forces is an essential component of management and accountability mechanisms. GHG emissions accounting is a major new field and is guided by increasingly detailed methodology standards and protocols in both the public and private sector (WBCSD, 2004).\nWhether part of integrated indicator systems or developed separately, climate change indicators on the mitigation side may focus on absolute or efficiency measures (Herzog and Baumert, 2006). Absolute measures help track aggregate emissions, thus quantify the direct pressure of human activities on the climate system. Efficiency measures indicate the amount of energy or materials used or GHG emitted in order to produce a unit of economic output, or more generally, to achieve a degree of change in human wellbeing. Depending on the policy context, both absolute measures and efficiency measures may be useful. But from the climate system perspective, it is ultimately indicators of absolute emission levels that matter.\nAt the sectoral level, several initiatives are being implemented to measure and monitor progress towards sustainable development, including the reduction of greenhouse gas emissions. In the buildings sector, for instance, the US Green Buildings Council, has established Leadership in Energy and Environmental Design (LEED) that sets a voluntary, consensus-based national standard for developing high-performance, sustainable buildings. About 2000 large buildings have received certificates. The Global Reporting Initiative (GRI) is a multi-stakeholder process whose mission is to develop and disseminate globally applicable Sustainability Reporting Guidelines. These Guidelines are for voluntary use by organizations for reporting on the economic, environmental, and social dimensions of their activities, products, and services. Over 700 large industrial corporations are annually reporting their sustainable development progress using these guidelines. Industry sectors, such as cement and aluminium, which are among the most intensive energy users, have their own initiatives to track progress (For more information on sectoral indicators, see Section 12.3.1).\nIn essence, while tools for measuring progress towards sustainable development are still far from perfect, considerable progress in the development of such tools and considerable uptake in their use has occurred. The trend is clearly towards more refinement in the tools and an increase in their use by governments, business and civil society.', 'How the GRI standards can bring consistency to your sustainability reporting\nLaunched in 2016, the GRI Standards were the first global standards for organizations’ sustainability reporting. The Standards aim to bring consistency and rigor to the sustainability reporting process, in response to trends towards greater sustainability and impact reporting.\nThey are the latest in a series of reporting frameworks built by the Global Reporting Initiative (GRI). Why is GRI important? — because it is an international independent standards body that aims to bring standardization to sustainability reporting.\nAs transparency in business grows more highly prized, and climate disclosures become more commonplace, businesses look for benchmarks and frameworks to help them. Organizations needing to publish a sustainability report (sometimes termed a Corporate Social Responsibility (CSR) report or Environmental, Social and Governance (ESG) report) are looking for standardized ways to fulfill their obligations. The GRI Standards are one of the ways they’re attempting to do this.\nThe Growing Role of Sustainability Reporting\nESG considerations are growing ever more central to business. The importance of ESG spans corporate size, geography and sector, as customers, investors, business partners and employees vote with their conscience and their wallets.\nOrganizations recognize that strong ESG performance can be good for business — but the potential for accusations of greenwashing means that ESG claims need to be evidenced. The challenges in capturing and reporting on ESG metrics are leading increasing numbers of businesses to seek defined standards and guidance on sustainability reporting. Universal standards can be hard to come by — making the GRI Standards a welcome aspect of the ESG reporting landscape.\nHow Many Organizations Report Using the GRI Standards?\nIn their 2020 Survey of Sustainability Reporting, KPMG found that 96% of the world’s 250 largest companies report on their sustainability performance, with 73% of them using the GRI Standards to do so.\nIn total, more than 500 organizations from over 70 countries are part of the GRI Community, representing organizations from across a range of sizes, geographies and sectors.\nThis makes the GRI Standards the most widely-used sustainability reporting standards globally.\nWhat Are GRI Standards?\nThe GRI Standards are issued by the Global Sustainability Standards Board (GSSB), an independent operating entity of GRI.\nThey are modular, comprising several sets of standards that can be adopted by companies of different sizes and sectors:\n- Universal Standards — apply to all organizations\n- Sector Standards — designed to allow consistent reporting on sector-specific issues\n- Topic Standards — relevant to a particular topic\nThe GRI reporting standards are reviewed to ensure they continue to reflect global best practices on sustainability reporting, in line with evolving requirements and regulations.\nHow Many GRI Standards Are There?\nThe GRI framework and standards are comprehensive: you can see a full list on the GRI’s website.\nThe GRI Universal Standards help organizations to identify their material topics and introduce principles they should use when reporting.\nThe Sector Standards are organized into four Priority Groups:\n- Priority Group 1: Basic materials and needs\n- Priority Group 2: Industrial\n- Priority Group 3: Transport, infrastructure and tourism\n- Priority Group 4: Other services and light manufacturing\nThe Topic Standards are split into three series: the 200 Series, which focuses on economic topics, the 300 Series, which explores environmental topics and the 400 Series, which focuses on social topics.\nGRI Standards List\nThe full list of GRI Standards is as follows:\n- GRI 1: Foundation 2021\n- GRI 2: General Disclosures 2021\n- GRI 3: Material Topics 2021\n- GRI 11: Oil and Gas Sector 2021\n- GRI 12: Coal Sector 2022\n- GRI 201: Economic Performance 2016\n- GRI 202: Market Presence 2016\n- GRI 203: Indirect Economic Impacts 2016\n- GRI 204: Procurement Practices 2016\n- GRI 205: Anti-corruption 2016\n- GRI 206: Anti-competitive Behavior 2016\n- GRI 207: Tax 2019\n- GRI 301: Materials 2016\n- GRI 302: Energy 2016\n- GRI 303: Water and Effluents 2018\n- GRI 304: Biodiversity 2016\n- GRI 305: Emissions 2016\n- GRI 306: Effluents and Waste 2016\n- GRI 306: Waste 2020\n- GRI 308: Supplier Environmental Assessment 2016\n- GRI 401: Employment 2016\n- GRI 402: Labor/Management Relations 2016\n- GRI 403: Occupational Health and Safety 2018\n- GRI 404: Training and Education 2016\n- GRI 405: Diversity and Equal Opportunity\n- GRI 406: Non-discrimination 2016\n- GRI 407: Freedom of Association and Collective Bargaining 2016\n- GRI 408: Child Labor 2016\n- GRI 409: Forced or Compulsory Labor 2016\n- GRI 410: Security Practices 2016\n- GRI 411: Rights of Indigenous Peoples 2016\n- GRI 413: Local Communities 2016\n- GRI 414: Supplier Social Assessment 2016\n- GRI 415: Public Policy 2016\n- GRI 416: Customer Health and Safety 2016\n- GRI 417: Marketing and Labeling 2016\n- GRI 418: Customer Privacy 2016\nWhy Use GRI Standards?\nYou might wonder: why is sustainability reporting using the GRI Standards important for you or your organization? And how do you use GRI Standards in your reporting?\nThere are many reasons why a company might use the GRI Standards as a framework for the preparation of its sustainability reports.\n- They provide a structure to your reporting: the GRI Standards create a set of principles via which organizations can define the content of their reports.\n- The sector-specific focus and modular nature of the Standards mean that companies can focus on the areas most material to their business.\n- The Standards are flexible, enabling organizations to adapt their reporting according to their priority areas.\n- They are wide-ranging, covering sustainability topics ranging from anti-corruption to water pollution.\n- They are regularly updated, to ensure they keep pace with the latest sustainability thinking and regulation.\n- The GRI Standards are aligned with other, recognized international frameworks and instruments for responsible corporate behavior, including the UN Guiding Principles on Business and Human Rights and the International Labor Organization Conventions.\n- You can also use the GRI Standards to report on your organizational impact and progress on the UN’s Sustainable Development Goals (SDGs), with the GRI providing guidance on integrating the SDGs into sustainability reporting.\nMake Use of the GRI Standards To Accelerate Your ESG Journey\nIf you’re already using the GRI Standards as a framework for your sustainability reporting, you will know that they can bring consistency and confidence to your ESG metrics, positioning you well for any ESG due diligence your investors or customers may do.\nIf you’re yet to fully explore the potential of the Standards, hopefully, this GRI Standards summary has helped you to understand their structure and benefits.\nMaximize the Consistency and Reliability of Your ESG Reporting\nOrganizations reporting via the GRI Standards are required to comply with Reporting Principles that mandate using reliable and quality information. This is central, as the quality and availability of ESG data can be one of the obstacles for businesses trying to improve their ESG reporting.\nBusinesses can (but do not have to) use external assurance to enhance the credibility of their reporting. If an organization does seek external assurance, it must also include the assurance report with its GRI reporting.\nWithout external assurance, organizations need other checks and measures to ensure their reporting is valid and robust.\nThis can be a challenge; ESG is a vast and multi-faceted topic for businesses to get to grips with, and data can be a barrier. Diligent ESG can help you to access this data and give assurance that it is accurate. As a result, you can accelerate your progress, no matter where you are on your journey.\nDiligent Modern ESG allows you to map ESG data against a range of standards, including GRI, helping you with reporting according to GRI standards and allowing you to track ESG progress against your peers and competitors.']	['<urn:uuid:4f0e9a7c-41f9-4fc2-81a4-d9515abd2453>', '<urn:uuid:5073dece-f69e-4996-b24b-5a1604b7af73>']	factoid	with-premise	short-search-query	similar-to-document	comparison	novice	2025-05-13T01:23:33.086345	7	54	2823
65	I'm curious about plant diseases - how long can the germs that cause clubroot disease survive in soil before dying?	The clubroot pathogen can survive in soil for up to about 20 years in the form of resting spores. These spores are protected by chitin, a fibrous substance that helps them survive for such a long time.	['By Carolyn King\nThe impacts of clubroot on susceptible canola cultivars are usually pretty obvious – the plants look drought-stricken and have large, irregular swellings (galls) on their roots. But the pathogen itself has remained somewhat enigmatic. Now a team of researchers mostly from Western Canada, led by Hossein Borhan and in collaboration with scientists from England and Poland, has sequenced the clubroot genome. This work is generating insights into the pathogen and how it functions, and is providing a springboard for future advances in clubroot management.\nBy Carolyn King\nThe clubroot pathogen, Plasmodiophora brassicae, attacks plants in the Brassicaceae family, including canola, mustard, broccoli and cabbage. This unusual pathogen spends its complex, multi-stage life cycle hidden in the soil and inside plant cells.\n“Plasmodiophora brassicae was first described by the Russian biologist Mikhail Woronin in 1878 as the pathogen of clubroot disease. Since its discovery, there has been progress in understanding some aspects of the pathogen’s biology, such as its life cycle, infection and disease development, and the genetics of host resistance to clubroot. However, until recently we had very little knowledge about other aspects of the pathogen’s biology, such as its genome and genes that contribute to its virulence and growth in the host,” says Borhan, a research scientist with Agriculture and Agri-Food Canada (AAFC) in Saskatoon.\nProgress in understanding the pathogen has been slow mainly because Plasmodiophora brassicae is difficult to study. “The pathogen is an obligate parasite. That means it can only grow and reproduce when living inside the host plant. So we can’t culture it, we can’t do experiments in Petri dishes and so on that we can do with other pathogens,” explains Stephen Strelkov, a professor at the University of Alberta.\nBorhan adds, “It is one of the rare pathogens that is a multicellular pathogen that lives completely inside a plant cell. The majority of plant pathogens live either entirely between the cells or they produce feeding organs inside the cells, but they don’t reside completely inside the cell. Interestingly, clubroot is the only example I know of a eukaryotic pathogen that lives completely inside the cell.” Eukaryotes are organisms whose cells have a membrane-bound nucleus, unlike prokaryotes, such as bacteria, which don’t. Plasmodiophora brassicae belongs to a diverse group of eukaryotes called protists.\nTo better understand this major pathogen, Borhan, Strelkov and the rest of their team, who were from various research agencies, started their clubroot genome study in about 2008.\nBorhan explains the clubroot pathogen offers some challenges for genome sequencing. “Since the pathogen cannot be cultured, DNA for genome sequencing needs to be extracted from root galls. This causes contamination of the pathogen’s DNA with other soil microbes and plant DNA. In addition there could be more than one clubroot pathotype present within the galls. You don’t want a mixture of isolates because that would make sequencing and genome assembly very complicated. So the unwanted DNA needs to be removed.”\nFortunately Strelkov’s lab had prepared single-spore, purified isolates for all the clubroot pathotypes, or strains, known in Canada at that time – identified as pathotypes 2, 3, 5, 6 and 8.\nFor the genome sequencing work, the samples for these five pathotypes were obtained from various parts of Canada. “Pathotypes 3 and 2 were single-spore isolates derived from Alberta samples, pathotypes 5 and 8 were from Ontario samples, and pathotype 6 came from British Columbia,” Strelkov notes.\nThe researchers generated draft genomes for pathotype 3 and pathotype 6 because these two pathotypes have some significant differences from each other. “Pathotype 3 is one of the most important and virulent pathotypes on canola. It was the predominant pathotype on Canadian canola and the one that could cause the most disease on canola before resistant cultivars became available,” Strelkov explains. “Pathotype 6 is mainly a pathotype of cruciferous vegetables; it doesn’t do very much in terms of attacking canola.” The two pathotypes also differ in their geographical distribution: pathotype 3 is very prevalent on the Prairies, and pathotype 6 is mainly in Eastern Canada and British Columbia.\nThe researchers also “re-sequenced” pathotypes 2, 5 and 8 to get a more complete picture of all the Canadian pathotypes. Resequencing refers to genome sequencing and assembly that uses a standard or reference genome as a template, so re-sequencing these other pathotypes was easier than creating the first draft genomes of pathotypes 3 and 6. The researchers used pathotype 3 as their reference genome for re-sequencing.\nThey then studied the characteristics of the clubroot genome and determined how the pathotype genomes differed from each other.\nThe researchers also did some RNA sequencing experiments to explore how the pathogen causes the disease in the host plant and how it takes nutrients from the host. For this work, they used two host species, Brassica napus (canola) and Arabidopsis thaliana(a small plant in the Brassicaceae family that has a small genome).\nAlthough a European group published a genome of a single clubroot isolate a few months before Borhan’s group published its genome work, the two groups were working in parallel. The sequencing work by Borhan’s group, which involved many more genomes, had already been completed when the European paper was published.\nHighlights of key findings\nOne of the most important findings from this work was that clubroot has a small, compact genome. “The genome is about 24 megabase pairs, which is small compared to most eukaryote pathogens, like fungal pathogens of plants. For instance, the clubroot genome is roughly half the size of the blackleg genome, which is about 45 megabase pairs, and it is around one-tenth the size of some of the oomycete genomes like the pathogen that causes late blight of potato,” Borhan says.\nTheir work also showed clubroot has about 11,000 genes – about the same number of genes as in the bigger genomes of fungal pathogens. Clubroot’s smaller genome is owed to the fact that it has relatively small spaces between the genes, called intergenic spaces, and a relatively low number of repeated DNA sequences, compared to many other organisms. Borhan explains intergenic spaces are important in gene regulation. Having such small intergenic spaces affects how the clubroot genes are regulated and how they are co-ordinated in terms of expression; for instance, it may mean there are overlapping regulatory sequences for multiple adjacent genes.\nAnother discovery related to how the pathotype genomes differ from each other. “We looked at the profile of the mutations in the DNA across the genome – the profile of single nucleotide polymorphisms [SNP] – and we found that pathotypes 6 and 3 are strikingly different from each other,” Borhan explains.\n“Pathotypes 2, 5 and 8 were really similar to pathotype 3, so together they form one group, and pathotype 6 stands alone. That correlates with their distribution and host specificity, since pathotypes 2, 3, 5 and 8 are all found mainly in the Prairie provinces and they all infect canola, while pathotype 6 is mainly in the east and infects vegetable brassicas.”\nThe researchers learned about the prevalence of chitin synthesis genes in clubroot; chitin is a fibrous substance that occurs in the exoskeletons of arthropods, like crustaceans, and in the cell walls of fungi. Borhan notes, “Chitin has been reported in clubroot resting spores before, but we noticed that the expression of chitin-related genes goes up when the spores are forming and when they are germinating. We believe chitin has a very important role in protecting the spores so they can survive in the soil for a long time [up to about 20 years].”\nAs well, they identified genes in clubroot that could affect the regulation of cytokine and auxin in the host plant. Strelkov explains, “These two plant hormones play a role in the development of the typical galls of clubroot on the roots, so this finding has potential implications for how the pathogen is causing the disease and how it is causing symptom development.”\nThe researchers found the clubroot genome lacks genes for making thiamine (vitamin B1) and some amino acids. “It looks like the pathogen is completely dependent on its host to get these nutrients; it can’t synthesize them by itself. It helps to explain why the pathogen won’t grow on its own,” Strelkov says.\nThe researchers also figured out how the pathogen might be taking up nutrients from its host. Borhan explains, “We found a novel group of transporters that we believe are involved in the uptake of nutrients from the plant. They have some similarities to bacterial transporters and some similarities to eukaryotic transporters, so they are a bridge between prokaryote and eukaryote transporters. This has not been reported for any other organism that we know of. Our collaborators in the U.K. are now investigating the role of these transporters.”\nOne of the areas the Canadian collaborators are now delving into relates to how the pathogen infects its host and how that differs between the pathotypes. “In any pathogen, there are many, many pathogenicity genes; we call them effectors or virulence genes. For example, there are several hundred of these effectors in blackleg. Our prediction is that there are close to 600 of them in clubroot. So to get to the bottom of how they work together to overcome the plant defences is a daunting task,” Borhan says. “We have cloned some of these effectors and are testing them in plants. We have indications that some of the effectors from pathotype 3, for example, make the plant more susceptible. Hopefully our work and other people’s work will provide some clues about clubroot virulence, for example, why pathotype 3 is more aggressive on canola than 6.”\nMost of the funding for their clubroot genome sequencing work was from Agriculture and Agri-Food Canada through a program called the Canadian Crop Genomics initiative. The researchers also obtained some funding from Growing Forward 2 through collaboration with Alberta Agriculture and Forestry and the University of Alberta to study the virulence effectors.\nApplications and implications\nBorhan and Strelkov see various ways the information from this research could be used.\nIn the short term, information about how one pathotype differs from another, such as the SNP profiles, could be used in developing molecular markers for pathotyping clubroot isolates. Markers would make it much easier for researchers to track changes in the pathogen strains, which would be very helpful for canola breeders and growers.\nStrelkov explains that, currently, the pathotypes are identified based on testing the isolates on a standardized set of host plants, referred to as differential hosts, and then seeing which particular hosts get the disease. “We inoculate the isolates on many differential hosts and then let the plants grow for six to eight weeks [to assess the disease symptoms]. That takes a lot of labour to do the inoculations and space to grow the plants because the testing must be in a bio-secure environment,” he says. “With molecular markers, you could potentially do the extraction of DNA and then run the tests in a day or two. So you would get the same information in a much, much shorter time for a much lower cost. And you could potentially run many more samples than we do now. At present we have only so much room in our greenhouses, so we always have to carefully consider which samples to include.”\nStrelkov adds, “This research provides some very important information as a starting point. Since 2013-14, we have found 11 new clubroot strains that are capable of overcoming the genetic resistance we had against the older pathotypes, including resistance to pathotype 3. Potentially, as we move forward, we could get genome information about some of these new pathotypes that could help us to understand where they came from and how they differ from the older pathotypes and maybe provide clues as to how they can overcome resistance and so on.”\nBorhan and Strelkov believe the information from the clubroot genome opens up many possibilities for research that could lead to new ways to control the disease. For example, perhaps scientists could find a chemical or biocontrol option that would affect chitin or some other factor important to spore survival. Or perhaps, as they learn more about the pathogen’s infection process, they might find ways to make changes in the host plant to disrupt that infection process, resulting in stronger and more sustainable disease resistance.\n“We hope that what we have generated will be a tool and resource for the research community in Canada and worldwide to tap into and learn more about the biology of clubroot and to come up with new ways of tackling the pathogen,” Borhan says.']	['<urn:uuid:c43dee04-2760-47fb-8822-3bf89d9fed2f>']	factoid	with-premise	verbose-and-natural	distant-from-document	single-doc	novice	2025-05-13T01:23:33.086345	20	37	2099
66	Do both waterways attract tourists and boats throughout the whole year?	No, the waterways have different peak usage times. The ICW's best fishing opportunities are specifically during May through August, especially in the early morning or during full moon nights. The Welland Canal, being a crucial commercial shipping route connecting Lake Erie to Lake Ontario with a 325-foot elevation difference, operates as a vital transportation corridor year-round for commercial vessels navigating between the Great Lakes.	"['Fly Fishing on the Edge - by Capt. Scott Sparrow\n[img2=""left""]http://www.theflyfishingforum.com/forums/attachment.php?attachmentid=165&stc=1[/img2]Fly Fishing on the Edge:\nThe Remarkable Fishing Opportunities Along the ICW\nby Capt. Scott Sparrow\nA dominant feature of the Texas bay system is the Intracoastal Waterway (ICW), a 3000-mile-long passage for commercial barge traffic than runs from Boston, Massachusetts to Brownsville, Texas. Construction of the ICW commenced in the 1920s, but the last section -- connecting Corpus Christi to Brownsville through the Upper and Lower Laguna Madre -- was only completed in the late 1940s. Today, the ICW effectively links otherwise isolated ports on the Atlantic and Gulf coastlines, creating a seamless exchange between supply and demand.\nFew coastal anglers can remember what it was like before “the ditch” existed. For most of us who grew up on the Texas coast, the ICW has always been there, and it was a place that excited our childhood dreams. In the age before shallow water sight casting, we would stand side by side with friends and brothers aboard old boats distinguished only by service, and we would cast live shrimp on treble hooks into the deeper water and wait for the bobber -- which we properly called a “cork” regardless of its composition -- to disappear. We caught a lot of fish that way, but in time many of us began to wonder what mysteries lay beyond ""the edge.""\nSome of us eventually left baitfishing behind, and turned our attention to sight casting to the larger reds and trout that fed in shallower water. Assisted by shallow-running “scooters” and tunneled skiffs, we were suddenly able to stalk visible fish and to rely on our casting and sighting skills instead of blind luck. Fishing became more like hunting, and the thought of going back to watching a “cork” until it disappeared offered little appeal compared with the multisensory experience of stalking visible fish. It’s been over 40 years since I cast my last shrimp into the ICW, and today I spend much of my free time stalking reds and trout far from a channel’s edge. But there are days and nights in the late spring and summer when fly fishing the edge of the ICW on the Lower Laguna Madre rises to the top of my list of things to do. In my opinion, fly fishing “the edge” remains a best-kept secret for fly fishers on the Texas coast.\nThe months of May through August offer fly fishers especially good action. During the outgoing tide, in particular -- and when the absolute water level is especially low -- the mullet will congregate on the deeper side of the channel’s edge, and mill about on the surface, giving the appearance of a light breeze upon the water. Beneath this constant surface commotion, speckled trout and ladyfish will feed aggressively on shrimp http://www.lagunamadre.net/BillDavenportICWcasting.jpgand small finfish, and will attack just about anything small that you throw their way. It’s not particularly challenging action, but the sheer numbers of strikes and hookups will bring a smile to anyone’s face.\nBy wading parallel to the dropoff, and stripping small topwaters along the edge, a fly fisher may reap as many as 100 strikes from trout, ladyfish, and an occasional redfish feeding just beneath the mullet stream. While the fish will often miss the popper, it’s not unusual to land 20 or more fish before the action subsides by midmorning. And some of the trout will surprise you by their size.\nKathy and I love this action, not only because it offers us the best blindcasting opportunities on our home waters, but because it allows us to sight cast to trophy trout at the same time. The square tail of a big trout will often appear starkly defined against the flickering disarray of mullet tails. The noise of the mullet stream, while drowning out the sound of the popper to some extent, will also mask the angler’s presence, giving him time to make multiple casts to a gamefish famous for its intolerance of imperfection.\nWhile casting poppers across the channel’s edge, a fly fisher can also scan the shallow flat adjacent to the ICW’s edge for signs of trophy trout. Big trout will saunter on and off the flat, spending only a few minutes prowling the skinny water before slipping over the edge and disappearing into the mullet stream. Resembling a snake swimming acoss the surface, these tailing and cruising predators often lure us away from the sure success of the mullet stream in favor of a more exacting, and singular challenge. If catching a “schoolie” trout in the deep water is easy, catching a big trout on the adjacent flat is as difficult a challenge as you will find anywhere,\nFly fishing experts who specialize in catching big trout will tell you that the best venues for catching the trout of a lifetime all lie within 100 yards of the ICW. While these anglers may speak in vague generalities when referring to their favorite big trout locales, I think they would all agree that the flats adjacent to the ICW offer incomparable action for trout over four pounds.\nTrout spawn on the shallow, grassy flats adjacent to the ICW, and can often be seen -- with back and tails out of the water -- in less than eight inches of water on a calm summer morning. Most people don’t look for them, so they don’t see them. But once you know what to look for, big trout opportunities along the ICW can become a regular part of your day. While the grassy conditions make it nearly impossible to capitalize on these opportunities with conventional tackle, a fly rod can work magic in these conditions. Indeed, Capt.http://www.lagunamadre.net/ICW1.jpg Skipper Ray of Laguna Vista made history by becoming the first fly fisher to win the entire Bay Division of the Texas International Fishing Tournament by catching two trout over seven pounds within a few yards of the ICW.\nThe ICW’s edge is a haven for redfish, as well, especially after the tides subside in early June. As the redfish follow the migration of the brown shrimp from the back lagoons toward deeper and cooler waters, they will congregate along the edges of the ICW and tail singly and in pods within a stone’s throw of the deeper water. In the spring, birds will often work over the pods, giving boating anglers a “heads up” on the action, but summer podding along the ICW is usually devoid of gulls. This is when the pods will often go unnoticed by boaters. But if one is willing to stop and wait quietly, tails will usually pop up within a minute or two of shutting down.\nThere are two times that fishing the ICW’s edge is predictably good -- from daybreak until midmorning on a calm summer day; and on a cloudless summer night under a full moon. As for the early morning action, the trout will readily take small poppers until the sun becomes too bright for them to feed on top. By switching to lightly weighted flies, however, a fly fisher can extend the action by another hour or more. While the deep water trout action usually falls off as the sun rises, the shallow water sight casting along the edge of the ICW can actually improve if sky is cloudless, and the water is clear.\nIn early August of last year, Kathy and I hosted fly fishing author Danny Hicks and his wife Diane, who wanted us to fish with them so they could get some photographs for Danny’s new book, Fly Fishing Texas (Amato Publications, 2005). Knowing that the ICW action was “on” from previous days on the water, we headed north from the mouth of the Arroyo, and shut down near one of the passes into Payton’s Bay. Danny and Diane, along with Kathy and me, took up positions within a few feet of the ICW dropoff, and began casting VIP poppers over the edge. Within an hour, we’d landed three trout around three pounds apiece, over a dozen smaller trout, three reds and a bunch of ladyfish. Around 9:00, we headed toward one of our favorite remote venues, where we shared a more challenging chapter in the Lower Laguna’s play book by casting poppers to tailing reds. But the confidence established by our sunrise vigil on the edge of the ICW carried us through the rest of the day.\nOne might think that weekend boat traffic would interfere with this high-paced action. Certainly, it is better to fish the edge on a weekday, but on the Lower Laguna Madre, the visible gamefish cruising the edge will usually reappear within moments of a boat’s passage. The fish seem to grow accustomed to the regular “highway” sounds and do not let these intrusions keep them from feeding for long.\nFishing along the ICW is a strategy that most fly fishers probably overlook. It’s just too accessible, and most of us assume that if a place is accessible to everyone, then it can’t possibly be good. But if almost everyone thinks that way, then the most accessible venues will host some of the least pressured gamefish. Most likely, there are places along the ICW in your own area that virtually no one fishes -- areas out the way of crossing boat traffic with all of the ingredients that big trout and reds love.\nSo if you’re tired of grinding your prop to a nub in those far-off waters, then try the edge of the ICW -- a place that’s close at hand and likely to surprise you with its fly fishing opportunities.\nAnd by the way...\nFly fishing the edge of the ICW under the full moon can be an unforgettable experience. The last time that Kathy and I went out together was during the August full moon. We left the dock on impulse at 11:00 p.m. on a windless night, and headed for one of our favorite big trout areas. Upon anchoring, we could hear dolphins surfacing breathlessly amid the popping sounds of trout and ladyfish chasing shrimp near the surface. I waded away from the boat, and took up a position about 20 feet from the channel’s edge. I could make out the black swirls of feeding fish on the silvery surface. My first cast was greeted by an explosive strike, a swift tug, and the sound of a trout shaking its head above the water. Seventeen trout up to 22 inches, and countless ladyfish later, the moon descended behind a bank of clouds, and the fish stopped feeding as if someone had flipped a switch. As for Kathy, her silence had long since told me that the woman who often outfished me had traded her fly rod for a pillow.\nSince that time, I have returned many times to fish the ICW’s edge during a full moon --even to the extent of sleeping on my boat under the full moon for 13 consecutive lunar cycles, from January, 2004 through December. In July, I decided to sleep near one of the entrances to Payton’s, and to fish for trout early the next morning. At 1:00, however, I was awakened by the sounds of fish feeding. Trout and ladyfish seemed to be competing for who could make the most noise. Unable to resist the urge to fish, I climbed out of my sleeping bag, put on my reinforced Chotas, and slipped into the water. Seeing was easy; indeed, the light was so bright that I could tie on my fly by holding it up against the moon.\nIn between landing five ladyfish and two small trout, two big trout missed my fly after hitting it with a unmistakable-sounding “pop.” Then, finally, a big fish slammed the fly and took me way into my backing as it headed for deeper water. I could feel it shaking its head, and I felt certain that it was trophy trout. I was already trying to figure out how I was going to photograph the fish in the moonlight before releasing it.\nI patiently inched the big fish closer until I could see its body shimmering in the moonlight. Instead of an eight-pound trout, I had snagged a 25-inch red. Visions of a world record evaporated in the moonlight, but I wasn’t disappointed. The night had been enough.\nCapt. Scott Sparrow is a college professor, psychotherapist, and fly fishing guide who -- with his wife Kathy own and operate Kingfisher Inn, a fly fishing lodge on the Lower Laguna Madre. He is the author of Healing the Fisher King: A Fly Fisher’s Quest, recently released by WePublishBooks.\nArticle courtesy of Capt. Scott Sparrow at the Kingfisher Inn in South Texas fly fishing Laguna Madre, fly fishing redfish, fly fishing South Texas, Kingfisher Inn, redfish, speckled trout, fly fishing lodge, fly fishing report, fly fishing guide, Arroyo City, fly fishing specked trout, Kathy Sparrow,Scott Sparrow\nRe: Fly Fishing on the Edge - by Capt. Scott Sparrow\nA great article! Lot of good information.\n|All times are GMT -5. The time now is 10:13 AM.|\nPowered by vBulletin® Version 3.8.4\nCopyright ©2000 - 2016, Jelsoft Enterprises Ltd.\nSearch Engine Optimization by vBSEO 3.6.0\n2005-2015 The North American Fly Fishing Forum. All rights reserved.', 'I bet you have never thought how ships get up and over Niagara Falls. I never had either. On the Niagara River, from Lake Erie to Lake Ontario (or visa versa), there is about 170 feet of elevation to negotiate over the thundering water. And, to make matters worse, there is an elevation difference of about 325 between the two lakes. Not to worry, the problem was solved back in 1829 with the opening of the WELLAND CANAL in Canada, construction having begun in 1824, just before the opening of the Erie Canal. Not only do you now know how ships can get from the Atlantic Ocean through to Lake Superior, but here probably for the first time ever you will get to see a sailing – MINE – from Lake Erie to Lake Ontario. And it happened in about seven hours Tuesday, September 10, 2019.\nYou will be able to sail (pun intended) through my next posts. Mainly locking and sailing, and few words. If something piques your interest, I trust you will research more, as I often do. Even when reading a well written mystery, set in a historical period or specific area, I will find myself googling as I pour over things I need to learn more about. So, feel free to stop, research, learn, and share with me, to even share here with others.\nHere is a map of the Welland Canal. We left Buffalo at 5AM to head back west to the entrance of the canal on the Canadian side of Lake Erie.\nthe elevation view below shows you what we did, sailing from right to left, going “down hill.” Note the flight of locks (6, 5, 4) for a quick high descent. Lock 8 is considered a “control lock” for water flow. Lock 7 is the southern most lift on the Niagara Escarpment. Locks 5, 6 and 4 are twin “flight locks.” There is an information center, and observation platform at Lock 3.\nwe arrived at the entrance to the canal at about 7AM.\nand, on the way\njust one of the many bridges that open up along the canal\na close-up of the old lock you may be able to see above\nabout to enter Lock 8\nfinally this “puppy” exited, and it was our turn to enter\nour turn for Lock 8 – the first lock on the Welland Canal when heading north to Lake Ontario.\nI am going to “lock you through” Lock 8 more than other locks so you can an idea of the process and progress. Image below is looking back north.\nand then exiting\none of the many “lift bridges” (remember you can open my galleries for larger images)\narriving at Lock 7\nand more “heavier” traffic as we approach the lock — NOTE the horizon. It appears the earth is flat, we are at the edge, and will fall off.\nThere are wires across the locks – “guard gates” – that these yellow booms pick up. The idea is to protect the lock doors from being rammed by a run away vessel. I really wonder if they could stop a ship with its inertia. Well, they have to be raised.\nexiting Lock 7, looking to Lock 6\nLeaving Lock 7\nand, now for Lock 6, a double lock\nour “friend” is leaving the other side\nLoosing track? I am, and it was really hard selecting images from the two cameras I was using to document these seven hours. but, now opening to Lock 5.\nnow we can pass through Lock 4 to get to Lock 3\nand, then exit\nnow approaching Lock 2 – a single, so we had to wait for this big guy to get out of our way\nand, leaving Lock 2 on the Welland Canal\nApproaching Lock 1\nwhere there is a car bridge\nand to be safe, a guard gate\nand looking back north while in Lock 1 with the car bridge lowered back down\nand the Pilot House of the GRANDE MARINER\nexiting Lock 1\nwhere a pilot boat met us to take off the pilot. We had one pilot from Lake Erie to Lock 8. Then he left, and another boarded to take us through the locks.\nbye Welland Canal\nand, onto Lake Ontario\nand, on the Lake\nOn Lake Ontario around 2PM it will not be until after midnight that we will dock in Rochester, New York. So, until then, enjoy, and do come back. Yours, RAY\nWATER WANDERINGS 3-18 SEPTEMBER 2019\nGREAT LAKES, WELLAND CANAL, OSWEGO CANAL,\nERIE CANAL, and HUDSON RIVER\nPPart 1 – Genesis\nPart 2 – Chicago – arriving aboard the Grande Mariner\nPart 3 – Underway to Wisconsin and Mackinac Island\nPart 4 – Greenfield Village, Dearborn, Michigan\nPart 5 – Port visits – Cleveland and Buffalo\nPart 6 – Welland Canal – Canada\nPart 7 – Rochester, NY and The Oswego Canal\nPart 8 – The Erie Canal – LOCKS 22 to 11 – Heading East\nPart 9 – The Erie Canal – LOCKS 11 to 1 – Heading East\nPart 10 – The Hudson River – Troy South to NYC\nPart 11 – NYC – and, Amtrak along the Hudson to Albany\nGood job with the photo’s and explanations.\nWhat sort of ship were you on?\nWhat a great way to spend 7 hours navigating the Welland Canal! The canal system was an ingenious way to navigate falls-something I’m sure Lewis and Clark would have appreciated in the early 1800’s when they got to Great Falls, MT…\nI think I would have been one of those people at the front of the Grand Mariner observing and taking pictures-oh, and of course, asking questions!\nGoing to read your next post now']"	['<urn:uuid:dc676d83-ad8b-4894-817e-a15ef7151079>', '<urn:uuid:15ec792a-1169-4fe3-b399-04e4cc4d4a0a>']	open-ended	with-premise	concise-and-natural	distant-from-document	comparison	novice	2025-05-13T01:23:33.086345	11	64	3165
67	Hey, I'm interested in phone lines. What's the highest speed they can transmit data?	A regular telephone line with a bandwidth of 3000 Hz (ranging from 300 Hz to 3300 Hz) and a typical signal-to-noise ratio of 35dB (which equals 3162) has a theoretical highest bit rate of 34,860 bps. This is calculated using the formula C = B log2(1 + S/N) = 3000 log2(1 + 3162) = 3000 log2(3163) = 3000 × 11.62 = 34,860 bps.	"['Presentation on theme: ""Chi-Cheng Lin, Winona State University CS412 Introduction to Computer Networking & Telecommunication Theoretical Basis of Data Communication.""— Presentation transcript:\nChi-Cheng Lin, Winona State University CS412 Introduction to Computer Networking & Telecommunication Theoretical Basis of Data Communication\n2 Topics l Analog/Digital Signals l Time and Frequency Domains l Bandwidth and Channel Capacity l Data Communication Measurements\n3 Signals l Information must be transformed into electromagnetic signals to be transmitted l Signal forms Analog or digital Periodic or aperiodic\n4 Analog/Digital Signals l Analog signal Continuous waveform Can have a infinite number of values in a range l Digital signal Discrete Can have only a limited number of values E.g., 0 or 1\n5 Figure 3.1 Comparison of analog and digital signals\n6 Periodic/Aperiodic Signals l Periodical signal Contains continuously repeated pattern Period (T): amount of time needed for the pattern to complete l Aperiodical signal Contains no repetitive signals\n7 Analog Signals l Simple analog signal Sine wave 3 characteristics 1. Peak amplitude (A) 2. Frequency (f) 3. Phase ( ) l Composite analog signal Composed of multiple sine waves\n9 Figure 3.3 Amplitude t s(t): instantaneous amplitude\n10 Characteristics of Analog Signal l Peak amplitude: highest intensity l Frequency (f) Number of cycles/rate of change per second Measured in Hertz (Hz), KHz, MHz, GHz, … Period (T): amount of time it takes to complete one cycle f = 1/T l Phase: position of the waveform relative to time 0\n26 Example 3 If a periodic signal is decomposed into five sine waves with frequencies of 100, 300, 500, 700, and 900 Hz, what is the bandwidth? Draw the spectrum, assuming all components have a maximum amplitude of 10 V. Solution B = f h f l = 900 100 = 800 Hz The spectrum has only five spikes, at 100, 300, 500, 700, and 900 (see Figure 13.4 )\n28 Example 4 A signal has a bandwidth of 20 Hz. The highest frequency is 60 Hz. What is the lowest frequency? Draw the spectrum if the signal contains all integral frequencies of the same amplitude. Solution B = f h f l 20 = 60 f l f l = 60 20 = 40 Hz\n30 Example 5 A signal has a spectrum with frequencies between 1000 and 2000 Hz (bandwidth of 1000 Hz). A medium can pass frequencies from 3000 to 4000 Hz (a bandwidth of 1000 Hz). Can this signal faithfully pass through this medium? Solution The answer is definitely no. Although the signal can have the same bandwidth (1000 Hz), the range does not overlap. The medium can only pass the frequencies between 3000 and 4000 Hz; the signal is totally lost.\n31 Digital Signals l 0s and 1s l Bit interval and bit rate Bit interval: time required to send 1 bit Bit rate: #bit intervals in one second\n32 Example 6 A digital signal has a bit rate of 2000 bps. What is the duration of each bit (bit interval) Solution The bit interval is the inverse of the bit rate. Bit interval = 1/ 2000 s = 0.000500 s = 0.000500 x 10 6 s = 500 s\n33 Digital Signal - Decomposition l A digital signal can be decomposed into an infinite number of simple sine waves (harmonics), each with a different amplitude, frequency, and phase A digital signal is a composite signal with an infinite bandwidth. A digital signal is a composite signal with an infinite bandwidth. l Significant spectrum Components required to reconstruct the digital signal\nFigure 4-20 WCB/McGraw-Hill The McGraw-Hill Companies, Inc., 1998 Harmonics of a Digital Signal\n35 Bandwidth-Limited Signals l (a) A binary signal and its root-mean- square Fourier amplitudes.\n36 Bandwidth-Limited Signals (2) l (b) – (e) Successive approximations to the original signal.\nFigure 4-21 WCB/McGraw-Hill The McGraw-Hill Companies, Inc., 1998 Exact and Significant Spectrums\n38 Channel Capacity l Channel capacity Max. bit rate a transmission medium can transfer l Nyquist theorem C = 2H log 2 V where C: channel capacity (bit per second) H: bandwidth (Hz) V: signal levels (2 for binary) C is proportional to H Significant bandwidth puts a limit on channel capacity\n39 Figure 3.18 Digital versus analog To transmit 6bps, we need a bandwidth = 3 - 0 = 3Hz\n40 Channel Capacity l Nyquist theorem is for noiseless (error- free) channels. l Shannon Capacity C = H log 2 (1 + S/N) where C: (noisy) channel capacity (bps) H: bandwidth (Hz) S/N: signal-to-noise ratio dB = 10 log 10 S/N l In practice, we have to apply both for determining the channel capacity.\n41 Example 7 Consider a noiseless channel with a bandwidth of 3000 Hz transmitting a signal with two signal levels. The maximum bit rate can be calculated as Bit Rate = 2 3000 log 2 2 = 6000 bps Example 8 Consider the same noiseless channel, transmitting a signal with four signal levels (for each level, we send two bits). The maximum bit rate can be calculated as: Bit Rate = 2 x 3000 x log 2 4 = 12,000 bps Bit Rate = 2 x 3000 x log 2 4 = 12,000 bps\n42 Example 9 Consider an extremely noisy channel in which the value of the signal-to-noise ratio is almost zero. In other words, the noise is so strong that the signal is faint. For this channel the capacity is calculated as C = B log 2 (1 + S/N) = B log 2 (1 + 0) = B log 2 (1) = B 0 = 0\n43 Example 10 We can calculate the theoretical highest bit rate of a regular telephone line. A telephone line normally has a bandwidth of 3000 Hz (300 Hz to 3300 Hz). The signal- to-noise ratio is usually 35dB, i.e., 3162. For this channel the capacity is calculated as C = B log 2 (1 + S/N) = 3000 log 2 (1 + 3162) = 3000 log 2 (3163) C = 3000 11.62 = 34,860 bps\n44 Example 11 We have a channel with a 1 MHz bandwidth. The S/N for this channel is 63; what is the appropriate bit rate and signal level? Solution C = B log 2 (1 + S/N) = 10 6 log 2 (1 + 63) = 10 6 log 2 (64) = 6 Mbps Then we use the Nyquist formula to find the number of signal levels. 4 Mbps = 2 1 MHz log 2 L L = 4 First, we use the Shannon formula to find our upper limit.\n45 Data Communication Measurements l Throughput How fast data can pass through an entity l Propagation speed Depends on medium and signal frequency l Propagation time (propagation delay) Time required for one bit to travel from one point to another l Wavelength Propagation speed = wavelength X frequency']"	['<urn:uuid:f572bfef-fd75-4695-b58f-d2278fa195be>']	open-ended	with-premise	concise-and-natural	distant-from-document	single-doc	novice	2025-05-13T01:23:33.086345	14	63	1138
68	How has the Department of Agronomy evolved in terms of rice variety development, and what educational programs are currently available for sustainable food systems?	The Department of Agronomy has developed several significant rice varieties, including TN1 (the world's first semi-dwarf rice variety), TNG67 (popular for over 25 years), and TNG71 (a popular aroma rice variety). Currently, educational programs include the Associate in Applied Science in Sustainable Food Systems, which offers courses in sustainable food careers, food production systems, and kitchen practices, preparing students for careers in food service, community gardens, and farm-to-table operations.	"['View count: 997\nIntroductionNational Chung Hsing University\nCollege of Agriculture and Natural Resources\nDepartment of Agronomy\nHistory of Agronomy Department by year\n1919 – this department was first founded as the College of Agronomy and Forestry in Taipei.\n1922 – renamed as the Advanced College of Agronomy and Forestry.\n1928 - reformed to the Special Division of Agronomy and Forestry of Taihoku Imperial University (now National Taiwan University).\n1943 - moved to Taichung as an independent campus, renamed as Taichung Advanced College of Agronomy and Forestry.\n1945 - the school was designated as the Taiwan Provincial Junior College of Agriculture.\n1946 - the school was reorganized as the Taiwan Provincial College of Agriculture included Agronomy, Forestry, and Agricultural Chemistry departments.\n1949 - was renamed as the Agriculture Department included Agronomy and Horticulture divisions.\n1954 - the above two divisions became independent as Department of Agronomy and Department of Horticulture.\n1968 - the graduate school was first established as The Graduate Institute of Food Crops under the Agronomy Department to offer the M.S. program.\n1985 - the graduate institute offered the Ph. D. program.\n1987 - the graduate Institute of Food Crops was renamed as Graduate Institute of Agronomy.\n1998 - the undergraduate and graduate programs were merged under the administration of the Department of Agronomy\nTo solve the world agriculture and food security demands, and to provide the agricultural system with living, healthy, ecological, sustainable, and productive considerations through educational and research programs.\n1. To meet the demands of social and world agricultural trends, to improve agronomic technology, to develop modern agriculture, to well-utilize the natural resources, and to develop new crop varieties.\n2. Application of modern technology to improve the quality and quantity of plants and their products for food, feed, fuel, and medicinal utilization.\n3. Expand the agricultural functions to benefit health, environment, entertainment, and culture to become an integrated agricultural industry.\n4. Educate undergraduate with basic agronomic knowledge and general agricultural skills for agricultural industry.\n5. Educate graduate students to become well-knowledged, highly skilled, creative and with the potential to be advanced researchers.\nOur education and research program demonstrate and provide opportunity to whom are interested in extending the application of agronomy in agricultural science and ecology to contribute the human life.\nThe Agronomy department was established in 1919. Since then, many of our graduate students have played important roles in agricultural research and agricultural policy in government and have done great contributions to the successes of Taiwan agriculture.\nThe following are several achievements of our alumni:\n1. The TN1 (Taichung Native 1) rice variety was bred by Mr. Ke-Ming Lin (1957) - the world first semi-dwarf rice variety that lead to the Miracle rice variety - IR8 in 1966 and the “Green revolution”.\n2. The TNG67 (Tninung 67) rice variety was bred by Dr. Tseng-Sheng Huang (1948) – the most popular rice variety in Taiwan for more than 25 years and also one of the 12 varieties used in the rice whole genome SNP project.\n3. The TCS10 (Taichung-Sen 10) rice variety was bred by Mr. Tsai-Fa Lin – a long grain indica rice variety.\n4. The TC5 (Taichung 5) grain sorghum variety was bred by Dr. Shun Tu (BS. 1963) – a high yield and early maturing grain sorghum variety.\n5. The TK9 (Tai-Keng9) rice variety was bred by Dr. Shih-Shen Hsue (BS. 1980, Ph. D. 2005) – a high grain quality rice variety.\n6. The TNG71 (Tainung 71) rice variety was bred by Dr. Yih-Chuan Kuo (BS. 1968) – a popular aroma rice variety.\n1. Many scholarships are offered by Department every year.\n2. Grandma scholarship offers every semester by Agronomy alumni Mrs. Tze-Hsing Hsieh.\n3. Know-you agricultural scholarship offers by Know-You Seed Company.\n4. Many part-time lab-working opportunities are available at the Agronomy department.', 'The Associate in Applied Science (AAS) in Sustainable Food Systems curriculum is designed for students with diverse interests in sustainable food careers. Graduates will gain a depth of understanding of the impact of agricultural practices and policies on ecosystems, economies and human cultures. Courses include concepts supporting contemporary food movements, which focus on local and global food systems. In addition, courses will cover careers in sustainable food systems, fundamentals of sustainability, basic nutrition, organic foods production, community garden concepts, food preparation laws and sanitation guidelines, basic principles and techniques for sustainable cooking, and food practices, attitudes and beliefs of different cultures. The program also includes a Certificate of Completion (CCL) in Sustainable Food Systems. Foundational courses will equip students with the necessary hands-on skills for employment or self-employment in food service, community gardens and farm-to-table operations.\nThis pathway map will help you gain the expertise needed to:\n- Describe basic nutrition principles.\n- Explain how food culture varies by factors such as region, country, ethnicity, religion, and climate.\n- Examine food entrepreneurship principles and processes.\n- Apply the knowledge, skills, and habits of mind required in the range of careers available in sustainable food systems.\n- Relate high-input and low-input sustainable agricultural production to local, regional, and global impact.\n- Communicate effectively, orally and in writing, utilizing the vocabulary of food systems.\n- Operationalize sustainable solutions to the problem of food waste.\n- Describe how agricultural practices have altered communities, cultures, health, and food safety.\n- Recognize the cultural, economic, environmental and sociological influences on food systems throughout history.\n- Explain food and agricultural policy effects on human, food, and economic systems.\n- Implement sustainable kitchen practices and cooking techniques, including strategies for maximizing use of whole, local, seasonal, and nutrient-dense food.\n- Implement basic garden-to-table food production and preparation skills.\n- Employ food safety and sanitation guidelines.\nSuccessful completion of this degree may lead to employment in a variety of different occupations and industries. Below are examples of related occupations with associated Arizona-based annual median wages* for this program. Education requirements vary for the occupations listed below, so you may need further education or degrees in order to qualify for some of these jobs and earn the related salaries. Please visit with an academic advisor and/or program director for additional information.\nDietitians and Nutritionists\nFood Service Managers\nHealth Education Specialists\nProject Management Specialists and Business Operations Specialists, All Other\nSocial and Community Service Managers\nThe following is the suggested course sequence by term. Please keep in mind:\n- Students should meet with an academic advisor to develop an individual education plan that meets their academic and career goals. Use the Pathway Planner tool in your Student Center to manage your plan.\n- The course sequence is laid out by suggested term and may be affected when students enter the program at different times of the year.\n- Initial course placement is determined by current district placement measures and/or completion of 100-200 level course and/or program requirements.\n- Degree and transfer seeking students may be required to successfully complete a MCCCD First Year Experience Course (FYE) within the first two semesters at a MCCCD College. Courses include AAA/CPD150, AAA/CPD150AC, CPD104, and AAA115/CPD115. Course offerings will vary by college. See an academic, program, or faculty advisor for details.\n- Consult with your faculty mentor or academic advisor to determine educational requirements, including possible university transfer options, for your chosen career field.\nFull-time status is 12 credits to 18 credits per semester.\n|SUS231||Careers in Sustainability||Gateway course||RC||1|\n|ENG101 or |\n|First-Year Composition or First-Year Composition for ESL||Prerequisites: Appropriate writing placement test score, or a grade of C or better in ENG091 or ESL097 or WAC101, or a grade of B or better in ALT100, or (a grade of C in ALT100 AND Corequisites: ENG101LL or ENG107LL OR WAC101 OR ENG100A+). or Prerequisites: Appropriate writing placement test score, or a grade of C or better in ENG091 or ESL097 or WAC101, or a grade of B or better in ALT100, or (a grade of C in ALT100 AND Corequisites: ENG101LL or ENG107LL OR WAC101 OR ENG100A+).||Critical course||RC||3|\n|MAT112||Mathematical Concepts and Applications||Prerequisites: An appropriate mathematics placement score, OR a grade of ""C"" or better for MAT090, or MAT091, or MAT092, OR (an appropriate diagnostic score, or a grade of ""C"" or better in each of the following courses: MAT055, MAT056, and MAT057).||Critical course OR satisfactory completion of a higher level mathematics course||MA||3|\n|SSH111||Sustainable Cities||Gateway course SSH111 carries the G, HU, SB Values||RC||3|\n|AAA/CPD115 or |\n|Creating College Success or Strategies for College Success or Educational and Career Planning or Career and Personal Development||0–3|\n|FON161||Sustainable Food Production Systems||Critical course Gateway course||RC||3|\n|ENG102 or |\n|First-Year Composition or First-Year Composition for ESL||Prerequisites: Grade of C or better in ENG101. or Prerequisites: A grade of C or better in ENG107.||FYC||3|\n|CRE101||College Critical Reading and Critical Thinking||Prerequisites: (A grade of C or better in ENG101 or ENG107) and (appropriate reading placement test score or a grade of C or better in RDG095 or RDG100 or RDG111 or RDG112 or RDG113 or permission of Instructor).||Critical course OR equivalent as indicated by assessment||CR||0–3|\n|FON100 or |\n|Introductory Nutrition or (Principles of Human Nutrition and Principles of Human Nutrition Laboratory)||No requisites or No requisites and Prerequisites: A grade of C or better in FON241 or Corequisites: FON241.||Critical course FON241 AND FON241LL recommended for students intending to transfer||RC||3–4|\n|FON104||Certification in Food Service Safety and Sanitation||RC||1|\n|FON163||Sustainable Kitchen Practices||RC||3|\n|FON143||Food and Culture||RC||3|\n|BIO105 or |\n|Environmental Biology or Plant Growth and Development||RC||4|\n|COM100 or |\n|Introduction to Human Communication or Interpersonal Communication or Small Group Communication||COM||3|\n|Restricted Electives||Restricted Electives||101-level language course recommended for students intending to transfer to ASU to earn a BS in Sustainability||0–5|\n|SUS232||Professional Skills in Sustainability Practice||Prerequisites: A grade of C or better in SUS110, SSH111, and (ENG101 or ENG107).||RC||3|\n|Restricted Electives||Restricted Electives||(HU AND C) OR (SB AND C) OR (L AND C) OR C recommended for students intending to transfer||3|\n|Restricted Electives||Restricted Electives||Any CS recommended for students intending to transfer, BPC110 OR CIS105 recommended for students intending to transfer to ASU to earn a BS in Sustainability||3|\n|Restricted Electives||Any course that carries both the C and L Value||102-level language course recommended for students intending to transfer to ASU to earn a BS in Sustainability||0–5|\nGateway Course = Generally the first major-specific course in a pathway.\nCritical Course = A course that is highly predictive of future success in a pathway.\nStudents must earn a grade of C or better for all courses required within the program.\nCourse Sequence total credits may differ from the program information located on the MCCCD curriculum website due to program and system design.\nView MCCCD’s curriculum website for the Associate in Applied Science in Sustainable Food Systems (http://aztransmac2.asu.edu/cgi-bin/WebObjects/MCCCD.woa/wa/freeForm3?id=157468).\nAt Maricopa, we strive to provide you with accurate and current information about our degree and certificate offerings. Due to the dynamic nature of the curriculum process, course and program information is subject to change. As a result, the course list associated with this degree or certificate on this site does not represent a contract, nor does it guarantee course availability. If you are interested in pursuing this degree or certificate, we encourage you to meet with an advisor to discuss the requirements at your college for the appropriate catalog year.\nThe pathway map presented above is for the current catalog year and is the intended pathway map for new students. Other versions of this pathway map from different catalog years are available below:\n|Catalog Year||Effective Dates|\n|2019-2020||5/28/2019 – 5/25/2020|']"	['<urn:uuid:6eb1cd7c-110a-466d-bfea-5a9549f8cf34>', '<urn:uuid:28a59454-3880-4cab-9625-411761a44c3f>']	factoid	direct	verbose-and-natural	similar-to-document	multi-aspect	expert	2025-05-13T01:23:33.086345	24	69	1884
69	when was deutsche oper berlin destroyed	The Deutsche Oper Berlin (then called Deutsches Opernhaus) was destroyed by a RAF air raid on November 23, 1943. After the destruction, performances continued at the Admiralspalast in Mitte until 1945.	"['Deutsche Oper Berlin\nThe Deutsche Oper Berlin is an opera company located in the Charlottenburg district of Berlin, Germany. The resident building is the country\'s second largest opera house and also home to the Berlin State Ballet.\nSince 2004 the Deutsche Oper Berlin, like the Berlin State Opera, the Komische Oper Berlin, the Berlin State Ballet, and the Bühnenservice Berlin (Stage and Costume Design), has been a member of the Berlin Opera Foundation.\n|This section needs additional citations for verification. (November 2012)|\nThe company\'s history goes back to the Deutsches Opernhaus built by the then independent city of Charlottenburg—the ""richest town of Prussia""—according to plans designed by Heinrich Seeling from 1911. It opened on November 7, 1912 with a performance of Beethoven\'s Fidelio, conducted by Ignatz Waghalter. In 1925, after the incorporation of Charlottenburg by the 1920 Greater Berlin Act, the name of the resident building was changed to Städtische Oper (Municipal Opera).\nWith the Nazi Machtergreifung in 1933, the opera was under control of the Reich Ministry of Public Enlightenment and Propaganda. Minister Joseph Goebbels had the name changed back to Deutsches Opernhaus, competing with the Berlin State Opera in Mitte controlled by his rival, the Prussian minister-president Hermann Göring. In 1935, the building was remodeled by Paul Baumgarten and the seating reduced from 2300 to 2098. Carl Ebert, the pre-World War II general manager, chose to emigrate from Germany rather than endorse the Nazi view of music, and went on to co-found the Glyndebourne opera festival in England. He was replaced by Max von Schillings, who acceded to enact works of ""unalloyed German character"". Several artists, like the conductor Fritz Stiedry or the singer Alexander Kipnis followed Ebert into emigration. The opera house was destroyed by a RAF air raid on 23 November 1943. Performances continued at the Admiralspalast in Mitte until 1945. Ebert returned as general manager after the war.\nAfter the war, in what was now West Berlin, the company, again called Städtische Oper, used the nearby Theater des Westens; its opening production was Fidelio, on 4 September 1945. Its home was finally rebuilt in 1961 but to a much-changed, sober design by Fritz Bornemann. The opening production of the newly named Deutsche Oper, on 24 September, was Mozart\'s Don Giovanni.\nPast Generalmusikdirectoren (general music directors) have included Bruno Walter, Kurt Adler, Ferenc Fricsay, Lorin Maazel, Gerd Albrecht, Jesús López-Cobos, Christian Thielemann. In October 2005, the Italian conductor Renato Palumbo was appointed Generalmusikdirector as of the 2006/2007 season. In October 2007, the Deutsche Oper announced the appointment of Donald Runnicles as their next Generalmusikdirector, effective August 2009, for an initial contract of five years. Simultaneously, Palumbo and the Deutsche Oper mutually agreed to terminate his contract, effective November 2007.\nOn the evening of 2 June 1967, Benno Ohnesorg, a student taking part in the German student movement, was shot in the streets around the opera house. He had been protesting against the visit to Germany by the Shah of Iran, who was attending a performance of Mozart\'s The Magic Flute.\nIn September 2006, the Deutsche Oper\'s Intendantin (general manager) Kirsten Harms drew criticism after she cancelled the production of Mozart\'s opera Idomeneo by Hans Neuenfels, because of fears that a scene in it featuring the severed heads of Jesus, Buddha and Muhammad would offend Muslims, and that the opera house\'s security might come under threat if violent protests took place. Critics of the decision include German Ministers and the German Chancellor Angela Merkel. The reaction from Muslims has been mixed — the leader of Germany\'s Islamic Council welcomed the decision, whilst a leader of Germany\'s Turkish community, criticising the decision, said:\nThis is about art, not about politics ... We should not make art dependent on religion — then we are back in the Middle Ages.\nAt the end of October 2006, the opera house announced that performances of Mozart\'s opera Idomeneo would then proceed. Kirsten Harms, after announcing in 2009 that she would not renew her contract beyond 2011, was farewelled in July of that year.\nIntendanten (General Managers)\n- Georg Hartmann (1912–1923)\n- Wilhelm Holthoff von Faßmann (1923–1925)\n- Heinz Tietjen (1925–1931)\n- Carl Ebert (1931–1933)\n- Max von Schillings (1933)\n- Wilhelm Rode (1934–1943)\n- Hans Schmidt-Isserstedt (1943–1944)\n- Michael Bohnen (1945–1947)\n- Heinz Tietjen (1948–1954)\n- Carl Ebert, again (1954–1961)\n- Gustav Rudolf Sellner (1961–1972)\n- Egon Seefehlner (1972–1976)\n- Siegfried Palm (1976–1981)\n- Götz Friedrich (1981–2000)\n- André Schmitz (interim, 2000–2001)\n- Udo Zimmermann (2001–2003)\n- Heinz Dieter Sense / Peter Sauerbaum (interim, 2003–2004)\n- Kirsten Harms (2004–2011)\n- Christoph Seuferle (interim, 2011–2012)\n- Dietmar Schwarz (since August 2012)\nGeneralmusikdirektoren (Music Directors)\n- Ignatz Waghalter (1912–1923)\n- Bruno Walter (1925–1929)\n- Kurt Adler, resident conductor (1932–1933)\n- Artur Rother (1935–1943, 1953–1958)\n- Karl Dammer (1937–1943)\n- Ferenc Fricsay (1949–1952)\n- Richard Kraus (1954–1961)\n- Heinrich Hollreiser, chief conductor (1961–1964)\n- Lorin Maazel (1965–1971)\n- Gerd Albrecht, resident conductor (1972–74)\n- Jesús López-Cobos (1981–1990)\n- Giuseppe Sinopoli (1990)\n- Rafael Frühbeck de Burgos (1992–1997)\n- Christian Thielemann (1997–2004)\n- Renato Palumbo (2006–2008)\n- Donald Runnicles (2009–present)\n- Oper in Berlin on oper-in-berlin.de\n- Ben Mattison (7 October 2005). ""Deutsche Oper Berlin Names Music Director"". Playbill Arts. Retrieved 2 September 2007.\n- Matthew Westphal (31 October 2007). ""In Sudden Appointment, Donald Runnicles Named Next Music Director of Deutsche Oper Berlin"". Playbill Arts. Retrieved 1 November 2007.\n- David Fickling (27 September 2006). ""Merkel voices concern over opera cancellation"". The Guardian. Retrieved 27 September 2006.\n- ""Fear Of Muslim Ire Stops German Opera"". CBS News Online. 27 September 2006. Retrieved 27 September 2006.\n- ""Shelved Muhammad opera to return"". BBC News Online. 27 October 2006. Retrieved 10 November 2006.\n- Goldmann, A.J. (December 2009). ""Die Frau ohne Schatten (Berlin, Deutsche Oper Berlin)"". Opera News 74 (6). Retrieved 3 April 2011.\n- Rosemarie Frühauf, ""Abschied in Berlin von Deutsche Oper-Intendantin Kirsten Harms"", The Epoch Times, 11 July 2011. Retrieved 24 December 2011.\n- Media related to Deutsche Oper Berlin at Wikimedia Commons\n- Deutsche Oper Berlin\n- Interview with Deutsche Oper musical director Donald Runnicles in Exberliner Magazine']"	['<urn:uuid:2908e191-1760-4e4c-8d26-f06ab3c4b870>']	open-ended	with-premise	short-search-query	distant-from-document	single-doc	novice	2025-05-13T01:23:33.086345	6	31	1013
70	compare immediate survival chances untreated cardiac arrest vs broken bones from falling senior	For cardiac arrests, according to the CDC, if no CPR is performed, the victim's survival chances decrease by 7% every minute of delay, with brain death starting to occur within 4-6 minutes. In contrast, for falls in seniors (aged 65 and older), while they occur as frequently as once per second, only one in five falls results in hospitalization. Falls primarily cause broken bones and head injuries, but also lead to psychological issues like fear of independent living and hesitation to engage in daily activities.	['The 7 Best Balance Exercises for Everyone Over 60\nExercising for better balance may not sound very sexy, and the best balance exercises may not be on the top of your priority list, but they should be, especially as those birthdays add up. Maintaining good posture, working on your flexibility, and exercising for better balance will all go a long way when it comes to optimal overall health and wellness long into the future.\nWhat Is Balance?\nBalance is comprised of many aspects including postural and equilibrium control. That is, balance is the ability to control both the movement of your body and the state of your body at rest. Your ability to move and not fall, as well as your ability to move into and maintain certain positions, can be compromised if you don’t work on your balance.\nThe Importance of Posture in Relation to Balance\nAlong with your ability to balance comes posture control. If you have bad posture, your body will automatically be misaligned, and that will make it so much more difficult to move without placing undue stress on your body. Being aligned and alleviating pressure, tension, and stress on your body is necessary to balance properly.\nThe Importance of Flexibility in Relation to Balance\nSometimes folks will have bad posture due to a lack of flexibility. You can imagine if your muscles are tight, it could potentially pull other muscles (and even bones) out of place over time. A misaligned frame with bad posture and inflexible, tight muscles will make it difficult to maintain your balance and will hinder your ability to move.\nAccording to the Centers for Disease Control and Prevention, falls are a leading cause of injury, and even death, in Americans aged 65 and older. The Center says that seniors experience falls as often as once a second, with one out of five falls causing a need for hospitalization. Falls are dangerous for many reasons, among those reasons being broken bones and head injuries.\nFalling can also cause a host of psychological issues as the person then fears living on their own, is afraid to move about, and is hesitant to engage in normal, everyday activities.\nTry these 7 Best Balance Exercises\nMaintaining good balance becomes all the more important as you continue to age. Regular exercise is important for maintaining not only muscular strength but bone strength as well. It’s a known fact that you lose muscle mass, bone mass, and strength over the years, and doing whatever you can to maintain and increase these should be a priority. In addition to a regular resistance training routine, it’s just plain smart to also incorporate some balance exercises into your program. Here are a few for you to try:\n1. Foot Lifts\nThis exercise is a great starting point if you’re new to balance exercises or you’re recovering and rehabbing from injury. Begin by standing with your feet about hip-width apart. You can place your hands on your hips or your arms out to your sides, whatever feels good for you.\nLift your right knee so your foot is off the ground. Hold this position for a count of five and then return your leg to the starting position. Repeat on the opposite side. As you get better at balancing, you can hold this position longer.\n2. Flamingo Stand\nTrue to its moniker, the flamingo stand strives to emulate the effortlessness with which flamingos stand. Commonly seen on one leg, flamingos have the unique ability to balance for long periods of time on one leg while resting the opposite leg on the weight-bearing appendage.\nBegin by standing with your hands on your hips. Slowly slide your right foot up the inner side of your left leg until it comes to about knee level. Hold this position for a count of five and then return your foot to the starting position by sliding it back down your leg. Repeat on the opposite side.\n3. Tightrope Walk\nYou may want to start this balance exercise relatively close to a wall so you can gradually work on your balance over time. Begin by standing up straight with your left side near a wall and your feet next to each other, pointing straight ahead.\nWith your hand lightly supporting you on the wall, place your right foot directly in front of your left foot (you’ll be walking toe to heel). Next, place your left foot directly in front of your right foot, with your left heel directly in front of the toes of your right foot.\nContinue this “tightrope” walk for the length of the wall. Then go back in the opposite direction using your right hand to support you. Over time, you should be able to do this walk without the support of the wall.\n4. Grapevine Walk\nGraduate to this walk once you have fully mastered the tightrope walk. You’ll start in a similar position standing straight with your hands on your hips and your feet together.\nLift your right leg and place your foot in front of and to the left of your left foot so your legs cross over each other. Bend your knees slightly as you take this step forward. Next, while maintaining balance on your right foot and leg, bend your knees slightly, lift your left foot and place it over, in front of, and slightly to the right of your right foot.\nYou’ll almost be forming figure 8s as you walk. Take your time to regain your balance in between steps.\n5. Chair Squats\nStand in front of a stable chair, facing away from it. With your arms outstretched directly in front of you and parallel to the ground, and your feet about hip-width apart, push your hips backwards as you bend at the knees.\nKeep bending at the knees while kicking your hips back until your butt hits the seat. Maintain an upright chest during this exercise and keep your head up and looking straight ahead.\nOnce you touch the seat, push upwards using your leg and glute muscles to return to a standing position. Repeat this balance exercise 5 – 10 times.\n6. Spine Mobility\nStart on a mat on your hands and knees with your palms face down on the floor and about shoulder-width apart. Put your right hand on the back of your head (just touching—no pressure). Your elbow will be pointed toward the floor in this beginning position.\nNext, slowly rotate your head and torso so your elbow points skyward and you are looking up to the right side. Hold this stretch as you feel all the tightness in your upper body, neck, and shoulders dissipate.\nTwist your body and arm back to the starting position and repeat the entire sequence on the opposite side.\n7. Table Plank\nYour core is an integral part of the equation when it comes to good balance and posture. A strong core will help support your limbs as you move and lift them and will give you the physical strength needed to balance and move.\nBegin by standing facing a table. Place your forearms on the table and take a giant step backwards (placing your feet side by side) so you are leaning forward onto your forearms.\nStraighten your body so your legs, glutes, back, and abdominals are tight and form a line, diagonal to the floor. Keep your head in a normal and comfortable upright forward position.\nHold this plank position for 30 seconds (or longer once you get stronger). Return to the start by taking a giant step forward and raising your torso to a standing position again. As your core gets stronger, you can move to a full plank on the floor.\nBest Balance Exercises for People Over 60: A Recap\nImproving your balance may not sound all that sexy, yet it’s vitally important to your long-term health, longevity, and independence. Start incorporating these balance exercises into your regular routine to build balance. Even if you already have a regular workout practice, you can easily fit these exercises in for a more balanced (forgive the pun) routine.', 'Updated Feb 2019\nCpr facts and stats published and released annually by various organizations and publications in the health and safety space. According to recent stats, more than 70% SCA or Sudden Cardiac Arrests occur at home or similar private settings.95% of Sudden Cardiac Arrest victims die prior to even reaching the hospital. Out of all these numbers, only 6% survive cardiac arrest. Therefore, getting trained and acquainted with the basics of CPR and learning how to perform CPR will help you save life of a loved one.\nAn unresponsive victim with no signs of breathing and pulse is considered to be in a cardiac arrest. Providing CPR in the initial minutes can help circulate blood that contains oxygen to the victim’s brain and other vital organs.\nKEY CPR FACTS AND STATISTICS\n- More than 350,000 out-of-hospital cardiac arrests occur in the United States per year, out of which 70% happen inside homes\n- 90 percent of people who suffer cardiac arrest die prior to reaching a hospital or medical care facility.\n- Effective CPR provided by a bystander in the first few minutes of cardiac arrest can increase the chances of survival by 2x or 3x.\n- According to AHA, the bystander should push the chest at a rate of 100 to 120 compressions per minute.\n- Less than 20 percent Americans are equipped to perform CPR during a medical emergency situation.( AHA Study)\n- CPR aids in maintaining vital flow of blood that to the brain and heart. It also aids in increasing the duration of electric shock provided via a defibrillator, thereby, making the process more effective.\n- If a bystander does not perform CPR, the survival chances of a victim will decrease 7% in every single minute of delay.\n- Within 5 – 6 minutes after a victim has experienced cardiac arrest and within that time span, no CPR is performed, followed by defibrillation, the victim might further suffer from brain death crisis.Within 4 to 6 minutes after a victim has experienced cardiac arrest brain death and permanent death starts to occur\n- In addition to cardiac arrests, 7 million people, including children and adults suffer injuries every year in their homes or within similar environments. Use of CPR is commonly required in those medical emergencies.\n- As per studies, 45% heart attacks occur amongst people under 65 years of age.\n- As per AHA, 1 in 6 men and 1 in 8 women, above 45 years of age have had stroke or heart attack.\n- When providing mouth-to-mouth resuscitation , the victim receives oxygen concentration of 16 vs the 20 to 21 percent received by ambient air.\nMedical Emergencies where CPR can be used\n- Heart attack\n- Electric shock\n- Allergic reactions of severe nature\n- Drug overdose\nTimeline of CPR\n- 0 to 4 minutes, unlikely development of brain damage\n- 4 to 6 minutes, possibility of brain damage\n- 6 to 10 minutes, high probability of brain damage\n- 10 minutes and over, probable brain damage\nAHA or American Heart Association has been involved in providing education related to CPR only within the contours of public chest compression, in order to encourage more and more people to combat and respond to medical emergencies that require CPR and first aid performance.\nMost individuals or bystanders get apprehensive about medical emergencies and similar situations. They either fear handling a victim or lack the knowledge of giving CPR and responding effectively in such situations. For the first few minutes, a bystander can relax and gear up before responding to the victim because the victim has sustainable amount of oxygen in his/her blood within those few minutes. The most important thing to do is get the oxygenated blood circulated via the heart, brain and lungs of the victim, which is ensured when the bystander or rescuer starts giving chest compressions.\nWhy should you learn CPR skills ?\nCardiac arrests are not uncommon medical emergencies. They can occur at any time and any place. Even an individual who might appear to be healthy can suffer from a cardiac arrest or similar condition. Furthermore, cardiac arrest and heart attack are not same conditions. Sudden cardiac arrest does not occur out of any severe heart condition. It can occur out of electrical impulses, severe injuries etc. Therefore, it will be helpful for not only you but others around you, to learn at least the basics of CPR and techniques of performing the same on victims, irrespective of gender or age.\nVital facts of choking\n- According to AHA, over 90% deaths due to foreign objects occur amongst children below the age 5 years, while 65% is amongst infants.\n- Liquids are amongst the most common causes of choking, especially in infants.\n- 19% causes of choking are due to candies, while 65% are due to hard candies.\n- 160 children within the age 14 years, in 2000, died because of respiratory tract obstruction due to ingestion or inhalation of foreign bodies.\n- In 2001, about 11% children were treated for choking emergencies.\nWhat percentage of people are saved by CPR ?\n- According to a study by AHA (PDF)\n4% to 16% of patients who received immediate CPR were eventually discharged from the hospital. This number includes people who were already at the hospital during the cardiac arrest\n- According to The new England journal of medicine, About 18% of people aged above 65+ who received CPR at the hospital survived and were discharged']	['<urn:uuid:6f720d12-ac50-41bc-99b9-bdb80607dcf4>', '<urn:uuid:f629bf04-58c9-4ee2-93e0-76f5779cfd5f>']	open-ended	direct	long-search-query	distant-from-document	comparison	novice	2025-05-13T01:23:33.086345	13	85	2267
71	thickness paper material used restore vintage clothing patterns	The Resto-Vival patterns are printed on sturdy bond paper, unlike original historic patterns which were made from tissue paper.	"['#3017 ""Lounging at the Lido"" 1930s Beach or Lounging Pyjamas and Eton Jacket\nThis pattern is a ""Resto-Vival"" pattern and is suitable for looks from the early to mid 1930s. These pyjamas (or pajamas) are as chic to wear for the seaside, on the boardwalk, or by the poolside as they are for the boudoir. In cottons and linens they are cool summer wear. In satins and silks these pajamas are glamourous for the hostess or for lounging.\nThe sleeveless pajamas have an open neckline accented by a two piece collar. The loose fit of the pajamas can be cinched in at the waist by using ties that can be applied by stitching at the side fronts or by creating a slit opening and passing the belt through. The ties create a bow at the center back. The back of the pajamas button up. These pajamas have very wide legs, the opening of the size 16 (bust 34) at the lower edge of the leg is 1 3/4 yards.\nThis pattern also includes a short sleeved cropped Eton Jacket, or bolero.\nThis pattern is available in Misses Sizes only. Please note that Sizes 42-46 are not for plus sizes and are based on a Misses Size block.\nIf you are between sizes cut the next size larger, as my patterns have less ease than modern standards. Please order pattern by measurements (given in inches), as the size numbers do not equate to modern pattern sizing and pattern sizing can vary from pattern to pattern on my website.\nNote that these instructions are not illustrated and are text only and sparce compared to modern patterns. Please read below for more information on ""Resto-Vival"" Patterns.\nThis pattern was based on a vintage size 16 (bust 34) original historic pattern. This pattern was originally only available in sizes 30 inch to 38 inch bust and yardage requirements are given only for those sizes. A cutting guide is included only for size 16 (bust 34). For all other sizes please use your discretion when buying fabric lengths, and all other sizes will need to create their own cutting layout. The trouser pieces are very large and it is suggested to use at least 45"" wide fabric for size packs B and C.\nThe sample of the pattern was made up in a linen for the blouse section and printed cotton for the trouser section with vintage buttons up the back. Please note that additional buttons were used on the sample garment, more than are indicated on the original pattern.\nThis listing is for the pattern to sew this yourself, not a finished garment.\nAbout Wearing History Resto-Vival™ Patterns\nResto-Vival™ Patterns are original historic patterns that have been restored and revived. Original patterns are usually available only in single sizes, precut from tissue paper and totally unprinted, with details like grainlines and darts indicated only by small perforations. Resto-Vival™ patterns are printed on sturdy bond paper instead of tissue and clearly marked with drawn and labeled markings. These markings aid the modern sewer in understanding the markings of the original pattern and the construction of the garment. Resto-Vival™ patterns follow the period shapes of the original patterns, maintaining the historical accuracy of the completed garment. Original period instructions are included. These instructions are text only (unless otherwise noted) and fairly minimal, especially compared to instructions for modern patterns. At least an intermediate knowledge of dressmaking and a good familiarity with pattern construction is suggested. You may choose to have a modern or period sewing book handy to help with basic construction methods that the pattern instructions do not cover in detail. Also, fitting a muslin mockup is strongly recommended, as all garments were meant to be worn over period foundation garments or corsetry.\nPhotos by Allison Barcenas Photography.\nReviews Hide Reviews\nGORGEOUS EASY TO READ PATTERN! BEAUTIFUL PRESENTATION!\nI quicly received this gorgeously wrapped pattern in the mail and can’t wait to get started on the project! The pattern is extremely easy to read and is printed on lovely thick paper.\nA VERY FUN PATTERN!\nI was initially a bit on the fence about how this pattern would turn out and work for my figure but I’m so glad that I made it! It’s very fun and easy to wear. Even if you don’t live near a beach you’ll enjoy having a pair of these beach pyjamas! I ended up using seven buttons for the back closure. The arm holes are a bit small when compared to modern ones. Tall gals might want to lengthen the legs. I’m only 5’4″ and size 18 ended up being just the right length for me. For a more in depth review and some pictures check out my blog review: http://star-spangledheart.blogspot.com/2011/10/lounging-around.html\nOH SO COMFORTABLE BEACH PAJAMAS\nThis pattern was super fast to put together not to mention fast. I wouldn’t say I am an advanced sewer and I had absolutely NO problems. I was able to scale it up, make a mock up, cut it out and sew it together in a days worth of work. The only suggestion I would make is to add more buttons in the back. I was so happy the pattern maker already suggested that herself otherwise I might have thought I did something wrong, but she had it covered.']"	['<urn:uuid:42921ae1-e2af-4210-b52b-a49cbc0b0d14>']	factoid	direct	long-search-query	distant-from-document	single-doc	novice	2025-05-13T01:23:33.086345	8	19	886
72	What are the structural design features of a high-quality garden fork, and what are the various specialized types available for different gardening tasks?	A high-quality garden fork should feature sturdy construction with forged steel tines and head, which offers the greatest strength. The handle should ideally be made of hardwood (particularly ash) or steel, as fiberglass is only suitable for light duties. The best construction methods include handles pressed into the head and riveted or bolted in place, or single-piece forged steel for both head and handle. As for specialized types, there are several variations: the classic digging fork with four sharp, long tines for breaking hard soil; potato forks with blunt edges for lifting root vegetables; spading forks that are lighter and better for loose soil; border forks for small spaces and raised beds; and manure forks with bent tines for turning compost piles.	['Saving you money, not hurting your back, that’s also what you’re looking for when you work the soil in your garden, right? In any case, ergonomics and ease of use are subjects to discuss when talking about a garden fork. The garden fork is a useful tool. It is an instrument that you can use to flip and move dirt. The garden fork can also be used to move plants. Is it really worth investing in such a tool? What is the function of a garden fork? What should you pay attention to when buying? This is the subject of this article.\nWhy is tillage so important?\nCultivating, aerating and loosening the soil is very important in providing plants with nutrients and water, as this is how you optimally prepare for their absorption. If you also want to improve the soil and mix potting soil, fertilizer, or compost into the soil, a garden fork can come in handy. Because for this task it is enough to work a maximum of twenty to thirty centimeters below the surface of the ground. Due to the long teeth of the fork, you will save yourself a much longer and more laborious job than with a shovel.\nIt is especially in the spring when the frost period ceases that it may be relevant to refresh the dry and nutrient-depleted soil and to enrich it with the ingredients it needs. Working with a beaker fork allows moisture and air to penetrate under the compacted soil, resulting in well nourished roots and healthier plants. This is why many garden owners use the garden fork as a suitable gardening tool this time of year. It is at this time of the year that the best possible conditions must be created for seedlings, plants and new flowers. But the garden fork can also be used at all other times of the year when it comes to creating excellent growing conditions for plants or removing unwanted weeds.\nWhat is a garden fork?\nA garden fork is a multi-pronged tool that is used to loosen and turn the soil. The excavation fork has 2, 3 or 4 wide, lance-shaped tines, about 30 cm long, made of hardened steel, which are arranged in one plane, at the lower end, depending on the use planned. The teeth and handle support are made from one piece. The digging fork usually has a handle with a T or D handle. Like the spade, the digging fork is used for digging and loosening the soil, especially when the ground is stony or sticky and working with the spade is difficult, or even impossible.\nA garden fork – also known as a spade fork or a digging fork – can be used both for digging and loosening the soil and for gently digging up plants to be moved around the garden. A garden fork is essential, especially when harvesting potatoes. The spade fork basically looks like a spade and can be used in exactly the same way, but instead of a smooth, continuous cutting blade, it has several mostly flattened teeth, giving it the appearance of an oversized fork.\nWhy invest in a garden fork?\nWhen you dig the earth with a garden fork, the teeth of the garden fork protect both the roots, which should not be damaged, as well as useful animals such as earthworms, which are often found cut in. two when using a conventional spade. Even ripe potatoes can be lifted off the ground relatively gently using a garden fork. A spade fork is also particularly useful for digging in very stony ground, where a spade would constantly encounter resistance, or for digging particularly hard soil.\nIf you are buying a garden fork, prefer a sturdy wooden handle, why not lacquered and polished. Make sure the teeth of the tool are made of high quality and stable stainless steel. Regarding its length, you can adapt it: the forks are available in different sizes. There are mini-forks for small areas which serve the same purpose but which, due to their size, are much more manoeuvrable. Practical for small flower beds, they are small weights. Also ideal for children who want to help in the garden.\nThe garden fork rather than the shovel\nWith a garden fork, you work to loosen the soil surface, you work in fresh soil or pull out a few weeds. If you only want to turn over the top layers of the soil, you will be able to do this job with relatively little effort. The deeper you want to work the soil, the more effort is required. The use in this case of a garden fork makes perfect sense.', 'Gardening is a delightful hobby for many, and it’s even more enjoyable and rewarding if you have all the right tools. A garden fork is one of the fundamental tools you’ll need, along with a shovel and rake to make even the toughest gardening chores effortless and fast. You only want the best heavy duty garden fork to make your work easier, especially if you’re a perpetual gardener.\nFinding the best heavy duty garden fork is far from easy, especially since there are so many different varieties out there and so many things to consider. Other than a well-constructed fork from a decent company, you also need to consider the type. Let’s consider every aspect of the garden fork and focus on the kind that will serve you best.\nTypes of Garden Forks\nThere are slight differences among different garden forks based on their uses. Think of why you’re getting the garden fork in the first place, and you’ll know which type to select.\nThis is the classic design and the most popular one since it comes in handy with various tasks. It’s best for breaking and turning clay or hard soil. Four, sharp, long tines make it easier to dig challenging grounds.\nThis is the fork you need if you plan on digging up root vegetables. Though you can also use them for digging soil before planting, they’ll have tines with blunt edges to prevent damage when lifting vegetables.\nLighter and more comfortable to handle than traditional garden forks, spading forks are best for digging and amending loose, sandy soil. You can also use them to dig root vegetables.\nSmaller than the typical garden fork, border forks work best in small spaces, ideal for preparing a small garden bed or raised bed.\nAlso known as a pitchfork, it is similar to the traditional garden fork, except for the slight bend in the tines, designed for scooping light material. They’re ideal for turning compost piles to speed up composting.\nHow To Use A Garden Fork?\nHaving a good quality garden fork is essential for any gardener. Garden forks have wide but not limited uses. It’s easy to get confused about the differences between a garden fork and a spade. Here are just a few common tasks.\n- Dig Hard / Stony or Clay Soil – Digging Clay soil can be impossible with a garden spade but a well-sharpened fork makes the job easy.\n- Dig up large roots – Digging Forks make this job easier to work around the roots\n- Break Up Soil – use in combination with a spade to dig up the soil and then use the spade to remove the soil\n- Digging in mulch to areas before adding plants\nHow To Use A Garden Fork Safely?\nAs with any tool its important you put safety first. Here are some following tips for garden fork tool safety.\n- Point the tines (spikes) away from you at all times\n- Don’t use a garden for in close proximity to others\n- Allow plenty of space to work\n- Clear any removable obstacles when working\n- Use the correct fork for the job\n- When pushing your fork in the ground keep it upright\n- Digging at an angle will increase the risk of injury or the tines breaking\n- Use a single foot to apply pressure to the tines to drive them into the ground\n- Pull the handle towards you to loosen the soil\n- lift with the knees and not the back, use the handle for stability.\n- take regular breaks to avoid fatigue\nWhat Makes The Best Heavy Duty Garden Fork\nNow that you know how to pick between the different types, there are some useful points regarding the build. Cheaper forks may attract you at the first look, but you’ll regret your decision when it bends or snaps at the first encounter of hard soil. Maintaining a garden is heavy work; it’s not a job for a flimsy garden fork since they’re just not built to survive the abuse.\nHere are some things you need to look for in the best heavy duty garden fork:\nThe fork, or the tines and head, are probably the most critical parts of a garden fork. Forged steel forks are the strongest and the most popular ones in the market. You can also choose stainless steel since it offers greater versatility and works well on most types of terrains.\nCarbon fiber and aluminum forks are also available but will only work for light-duty chores. Though they’re lightweight and easy to handle, we wouldn’t recommend them because of their low durability.\nIt’s not just the material of tines and heads that’s important. If you’re getting the garden fork for heavy-duty use, its handle’s material is also crucial to consider. Garden forks are usually available with hardwood, fiberglass, or steel handles.\n- Hardwood, preferably ash handle, is a popular choice since it’s not too heavy and will not break easily, even when digging up hard soil.\n- Forks with steel handles can be expensive and heavy, but on the brighter side, it’s practically impossible to break them.\n- Fiberglass handles are lightweight but are only good enough for light-duty work. Heavier tasks will easily shatter these handles.\nChoose a garden fork with a comfortable length of the handle. You should be able to use the tool without bending a lot. Longer handles are especially useful for elderly gardeners or those with back pain since it helps maintain a good posture while working.\nThe assembly and construction of your heavy duty garden fork are also necessary for judging its longevity. There are different types of constructions available. Among these, the strongest ones include:\n- Those that have the handle pressed into the head and riveted or bolted in place won’t break easily.\n- Steel handled forks that come as a single piece of forged steel for the head and the handle are also exceptionally strong.\n- Forks that have the head welded into the metal handle are also great options.\nBest Heavy Duty Garden Forks\nNow you know the differences between all the types of forks I am going to recommend a heavy-duty fork for each type. As a vegetable gardener, I recommend you start out with a standard heavy-duty garden fork, a border fork, and if you are growing potatoes a potato fork. Trust me it’s frustrating when digging up potatoes to constantly stick a standard fork through them.\nHeavy Duty Garden Fork Recommendation – True Temper Digging Fork\nThe True Temper digging fork is a popular favorite heavy duty garden fork. The four tines are diamond pointed for easy soil penetration. It has a 30″ hardwood handle which not only reduces weight but also makes it strong and durable.\nHeavy Duty Spading Fork Recommendation – TABOR Tools Digging Fork\nThis super heavy duty 4 tine spading fork from tabor tools is listed as virtually unbreakable. It’s not hard to see why when you look at the construction.\n- Forged from hardened and tempered high carbon steel\n- Non-Flexing Welded Steel Shaft\n- Metal D Grip Handle\n- Weighs 5.5lbs\nHoss Tools Garden Digging Fork – Heavy-Duty\nThis Heavy Duty Garden Fork has also made the list because it’s great for digging up potatoes. Built to last a lifetime and has a 5-year warranty. The weight of this is just 5.5lbs the overall length is 42″\nIn the end, it all comes down to this – you get what you pay for. Spending a little extra on a decent fork that’s going to live up to all the physical torture you put it through is definitely the wise choice. Consider all the things highlighted above and pick the best heavy duty garden fork that will serve you well through countless growing seasons. A good quality garden fork should easily last you a lifetime.']	['<urn:uuid:8b18c4e0-ff80-4a8d-a160-52a2a80a0de9>', '<urn:uuid:fabfe563-cd34-4089-9d54-6558f53d7255>']	open-ended	direct	verbose-and-natural	similar-to-document	multi-aspect	expert	2025-05-13T01:23:33.086345	23	122	2095
73	need info about louis pasteur contribution to medicine and proof against spontaneous generation what experiments he did	Louis Pasteur made significant contributions to medicine as a world-renowned French chemist and biologist. He founded microbiology, proved the germ theory of disease, invented pasteurization, and developed vaccines for several diseases including rabies and anthrax. Regarding spontaneous generation, Pasteur definitively disproved it through his famous swan-necked flask experiment in 1859. He used a specially designed flask that allowed air in but trapped dust and microbes. When he boiled the nutrient broth in the flask, no growth occurred, even after many days. However, when he tipped the flask to allow the broth to contact the trapped dust containing microbes, growth occurred. This experiment conclusively demonstrated that microorganisms come from other microorganisms, not from non-living matter.	"[""What were louis pasteur's accomplishments louis pasteur's full name is dr louis jean pasteur louis pasteur it quotes the last 4 paragraphs from 'the life of pasteur' by renã . Louis pasteur accomplishments introduction: pasteur, louis (1822-1895), world-renowned french chemist and biologist, who founded the science of microbiology, proved the germ theory of disease, invented the process of pasteurization, and developed vaccines for several diseases, including rabies. During the 19th century life the life and accomplishments of louis pasteur in britain was the life and accomplishments of louis pasteur accomplishments of louis pasteur.\nLouis pasteur’s research and work gave birth to several branches of science and he was responsible for many of the theoretical concepts and applications that are still used today childhood, career and personal life. Learn all about louis pasteur, the scientist who first began pasteurization readers will discover the incredible accomplishments of pasteur and how he dedicated his life to studying various molds, bacteria, . The story of louis pasteur is a 1936 american black-and-white biographical film from warner bros, produced by henry blanke, directed by william dieterle, .\nPasteur vallery-radot wrote a brief biography of his famous grandfather in 1958, and claimed that pasteur did not consider spontaneous generation altogether impossible he even claimed pasteur “had dreams about creating or modifying life”. According to the encyclopedia britannica (2013) louis pasteur was both a chemist and a microbiologist this human was a man of many great accomplishments who made many contributions to the field of science, technology and medicine louis pasteur was a pioneer of his field and should be greatly . Louis pasteur, (born december 27, 1822, dole, france—died september 28, 1895, saint-cloud), french chemist and microbiologist who was one of the most important founders of medical microbiology pasteur’s contributions to science , technology , and medicine are nearly without precedent. Louis pasteur louis pasteur was born in dole on december 27,1822, the only son of a poorly educated tanner louis pasteur grew up in the small town of arbois louis was not an outstanding student during his years of elementary education, preferring fishing and drawing to other subjects.\nLouis pasteur biography, life, interesting facts louis pasteur was a french microbiologist as well as a french chemist that developed the first vaccines for rabies as well as anthrax he was creditedwith the invention of a technique for treating milk and wine to stop bacterial contamination. Louis pasteur (1822–1895) is the life and accomplishments of louis pasteur revered by his successors in the life sciences as well as by the general public 11-5-2017 his breakthroughs have saved countless lives and improved the the life and accomplishments of louis pasteur medical research quantitative paper crequite quality of life for . #5 louis pasteur decisively disproved spontaneous generation spontaneous generation was a prevailing notion during pasteur’s time by which simple life-forms were spontaneously generated from non-living matter .\nLouis pasteur (1822–1895) is revered by his successors in the life sciences as well as by the general public in fact, his name provided the basis for a household word—pasteurized. Louis pasteur was a famous chemist and biologist whose works led to the advancement of the life we know today he created vaccines and medicines, as well as invented the pasteurization process for milk products. Louis pasteur’s pupil paul vuillemin coins the term “antibiotic” from the word antibiosis: process by which life could be used to destroy life 1890 albert calmette becomes roux’s assistant at the institut pasteur.\nLouis pasteur was born on 27 december 1822 in dole, jura, france he was to become known as the founder of microbiology pasteur was the one who discovered the role of bacteria in fermentation. Louis pasteur (1822–1895) was a french biologist and chemist whose breakthrough discoveries into the causes and prevention of disease ushered in the modern era of medicine louis pasteur was born december 27, 1822 in dole, france, into a catholic family he was the third child of jean-joseph . Personal life pasteur was born in 1822 in dole, the only son of a poorly educated tanner, jean pasteur, who had served in napoleon's grande armee.\nLouis pasteur was a french chemist who proved that germs cause disease, developed vaccines for anthrax and rabies and created the process of pasteurization louis pasteur: biography & quotes. With these facts about louis pasteur, let us learn more about his early life, family, education, work, inventions, patents, and accomplishments about his childhood and early life 1. Heroes of wine: louis pasteur (1822-1895) and to help me with this article on the life of louis pasteur, louis pasteur - biographycom."", 'Presentation on theme: ""spontaneous generation Redi, Needham, Spallanzani, and Pasteur.""— Presentation transcript:\nspontaneous generation Redi, Needham, Spallanzani, and Pasteur\nSpontaneous Generation For much of history, people believed that animals could come from non-living sources. They thought: –Frogs developed from falling drops of rain – mice arose from sweaty underwear – and flies arose from decaying meat. This is called abiogenesis Also known as spontaneous generation\nThese ideas were followed because people simply accepted what they were told\nThe Power of Authority In the past, people believed what they were told by “authorities” such as the Church, or the ancient Greek philosopher Aristotle Questioning Aristotle was like questioning the Church....\nOne “scientist” put forward the belief that mice could be generated spontaneously from wheat and a sweaty shirt. The wheat provided the “nutritive power” and the shirt provided the “active principle.” “active principle” = a mysterious “life-force” that allowed spontaneous generation to occur.\nFrancisco Redi (Italian physician & poet )-- attempted to disprove the theory of Spontaneous Generation.\n“The flesh of dead animals cannot engender worms unless the eggs of the living being deposited therein” Put dead snakes, eels, and veal in large wide mouthed vessels. Sealed one set with wax and left the other set open to air. Decaying meat was teeming with maggots, sealed meat had no maggots Wax sealed vessels failed to produce maggots because flies were unable to reach the meat\nRedi’s critics said: You have too many variables There is a lack of access and a lack of air. We ALL know that everything needs air Of course no flies grew! You haven’t proven anything.\nRedi part 2 – answer to critics flies laid eggs on top of mesh no maggots in meat fine mesh allows in air, but not flies\nRedi’s Conclusions: “All living beings come from seeds of the plants or animals themselves” However, if someone were to demonstrate even one exception to this hypothesis, then Redi’s hypothesis would be rejected.\nJohn Needham (English Clergyman) wondered if this would work with micro organisms in1745 Everyone knew that boiling killed organisms. Needham prepared various broths and showed that they contained microbes. Then he boiled them, and showed that there were no longer any microbes. He ensured the stoppers were loose, so that air would not be excluded Then, after a few days, microbes had reappeared! This was “proof” that the microbes had spontaneously generated from the non-living broth.\nNeedham’s error BUT: how was this evidence of a faulty experiment? –what ERROR in experimental method is shown here? Hypothesis: microbes MUST HAVE arisen spontaneously from the broth. Assumption: there is no other place the microbes could come from (other than the broth). error: microbes could have come from the air!\nSpallanzani’s (Italian Naturalist) Disagreed with Needham Claimed he didn’t seal jars well enough He said microbes could have come from the air He repeated Needham’s experiment, but changed two things: –boiled flasks longer, and –SEALED THEM after boiling by fusing the glass tops shut –(hermetically sealed – absolutely airtight) Result: NO growth in ANY flask\nNeedham criticizes Spallanzani’s first experiment BUT Needham said: you boiled it TOO LONG, and: You spoiled the vegetative power by boiling. You killed the ability of the broth to give life. Life can still come from broth -- but the broth must not be “damaged” by boiling.\nSpallanzani’s second experiment he did TIMED BOILINGS then left them partially sealed some partially sealed, some hermetically sealed as in his previous experiment hypothesized that more boiling should lead to less life he left some jars as Needham had (leaky seals), to ensure “active principle” was not damaged 30 mins 60 mins 90 mins 120 mins tight seal loose seal\nSpallanzani’s second experiment -- results this showed TWO main things: boiling did NOT damage broth’s ability to support life growth depended on the SEAL only 30 mins 60 mins 90 mins 120 mins tight seal loose seal\nLouis Pasteur 1859– used swan-necked flask flask allowed in air, but trapped dust (and microbes) boiled infusion showed that NO growth occurred, even after many days BUT -- what about damaging the “active principle”? (French chemist) entered a contest sponsored by French Academy of Sciences to prove or disprove Spontaneous generation.\nPasteur showed that the active principle was NOT damaged at any later time, he could tip the flask this allowed nutrient broth to contact the dust this carried microbes into the broth result: growth! area where dust had been trapped\nPasteur squashes the idea of abiogenesis completely! Since then, no one has been able to refute Pasteur’s experiment scientists everywhere soon came to accept that abiogenesis did NOT EXIST. but: then how did life on this planet start in the first place?']"	['<urn:uuid:d22ded87-2e51-4334-b4f7-2627abe2d009>', '<urn:uuid:0133d648-8a82-47c3-b20b-3f7fd5b3ffd5>']	open-ended	with-premise	long-search-query	similar-to-document	multi-aspect	novice	2025-05-13T01:23:33.086345	17	114	1551
74	Do performance power supplies and CPU performance governors prioritize efficiency similarly?	No, they prioritize efficiency differently. Power supplies use the 80 PLUS rating system to maximize energy efficiency during AC to DC conversion, with ratings from 80% to 90% efficiency at full load. In contrast, Clear Linux OS's CPU performance governor prioritizes maximum performance by maintaining highest clock frequency, operating under the principle that finishing tasks quickly allows faster return to low-energy idle states.	['What You Need to Know About Picking a Power Supply for a Gaming PC\nFrom next generation CPUs, to the fastest RAM, to the beefiest graphics cards—if you’re building a gaming PC, chances are you’ve already got your heart set on the latest and greatest PC components.\nBut there’s one more component that often gets overlooked: the power supply. It won’t improve your framerate and it isn’t the most aesthetically appealing part of a build, but without a power supply, your gaming rig is literally dead weight.\nBeginners often make the mistake of settling for a cheap off-brand unit that boasts a large power supply rating. Others might overspend on a larger power supply rating to ensure they’ll have enough juice for their high-end components.\nBut if you’re looking to build a gaming PC that will last, a good power supply is a must. Here’s what you need to know about picking a power supply for a gaming PC.\nHow much wattage do you need?\nIt’s no secret that high-speed circuits such as GPUs and CPUs are energy hogs. Add fans, lighting, and liquid cooling to the mix, and you begin to understand how much energy you’ll need to power your rig.\nYou can find power consumption requirements for different components online from manufacturers or on sites such as Tom’s Hardware. Or you could just use a handy power supply calculator like this one from OuterVision. It lets you build a hypothetical rig by selecting parts from drop-down menus and calculate the recommended wattage for your system.\nHere are some typical benchmarks for power draw requirements for high performance PC components:\nCPU: Intel Core i7-9700K CPU, 95W\nVideo Card: Nvidia GTX 1080 Ti, 250W\nRAM: DDR4 2133 8GB, 3W\nSSD: Samsung SSD 850 EVO 500GB, 4W\nYou’ll notice that the video card takes up a lot of power. Maybe you don’t really need the latest flagship Nvidia GTX Titan series card, and can settle for last year’s model. The Nvidia GTX 1070 Ti only draws 180W. It won’t only save you on the cost of the card, but also the cost of your energy bill and the power supply unit for your rig.\nIt’s not all about size: 80 plus rating system\nBesides the raw power rating listed on a power supply, energy efficiency is another important metric that you’ll want to understand.\nYour GPU, CPU, and other computer components all run off of DC (direct current) power. Your wall outlet only supplies AC (alternating current) power. When your power supply converts AC power from the wall into the DC power used by the rest of the components in your gaming rig, at least some of that energy is lost as heat.\nThat means you’re never getting 100% of the power drawn from your wall. So how do you compare two power supplies that have close to or the same wattage rating?\nEnter the 80 PLUS rating system. It was created to encourage manufacturers to create more energy efficient power supplies. The 80 PLUS rating system certifies power supplies are able to convert more than 80% of the AC power received from your wall into the DC power needed by your components. There are six levels for non-redundant (power) power supplies:\n80 PLUS: 80%\n80 PLUS Bronze: 82%\n80 PLUS Silver: 85%\n80 PLUS Gold: 87%\n80 PLUS Platinum: 89%\n80 PLUS Titanium: 90%\nAll efficiencies were based on 100% of rated load. Efficiencies can be higher at lower loads, as with 80 PLUS Titanium hitting 94% efficiency at 50%.\nBudget for a quality power supply in your wattage range\nWhile it can be tempting to snag deals on power supplies with high wattage ratings, it’s important to do your due diligence and check user reviews, manufacturer reputations, power draw requirements, and energy efficiency ratings. Often times if a deal sounds too good to be true, it probably is.\nThe right way to save money on power supplies is to pick a quality unit from a reputable manufacturer that’s within your required wattage range. Be sure to add enough margin to account for any upgrades you plan to make in the future. Choose the best power supply for your needs.\nAbout the AuthorFollow on Linkedin Visit Website More Content by Cadence PCB Solutions', 'CPU Power and Performance¶\nThis guide explains the CPU power and performance mechanisms in Clear Linux* OS.\nModern x86 CPUs employ a number of features to balance performance, energy, and thermal efficiency.\nBy default, Clear Linux OS prioritizes maximum CPU performance, assuming that the faster the program finishes execution, the faster the CPU can return to a low energy idle state. It is important to understand and evaluate the impact of each feature when troubleshooting or considering changing the defaults.\nC-states and P-states are both CPU power saving mechanisms that are entered under different operating conditions. The tradeoff is a slightly longer time to exit these states when the CPU is needed.\nC-states (idle states)¶\nHardware enters a C-state when the CPU is idle and not executing instructions. C-states decrease power utilization by reducing clock frequency, voltages, and features in each state. Although C-states can typically be limited or disabled in a system’s UEFI or BIOS configuration, these settings are overridden when the intel_idle driver is in use.\nTo view the current\ncpuidle driver run this command in a terminal:\nFor troubleshooting, C-states can be limited with a kernel command line boot parameter by adding processor.max_cstate=N intel_idle.max_cstate=N or completely disabled with idle=poll.\nprocessor.max_cstate=0 is changed to a valid value by the kernel: processor.max_cstate=1.\nintel_idle.max_cstate=0 disables the Intel Idle driver rather than set it to C-state 0.\nP-states (performance states)¶\nThe CPU can enter a P-state, also known as Intel SpeedStep® technology on Intel processors or AMD* Cool’n’Quiet* technology, while it is active and executing instructions. P-states reduce power utilization by adjusting CPU clock frequency and voltages based on CPU demand. P-states can typically be limited or disabled in a system’s firmware (UEFI/BIOS).\nIntel® Turbo Boost Technology, found on some modern Intel CPUs, allows cores on a processor to temporarily operate at a higher than rated CPU clock frequency to accommodate demanding workloads if the CPU is under defined power and thermal thresholds. Intel Turbo Boost Technology is an extension of P-states, so it can be impacted by limiting C-states or P-states.\nIntel Turbo Boost Technology can be disabled in a system’s UEFI/BIOS or in Clear Linux OS:\necho 1 | sudo tee /sys/devices/system/cpu/intel_pstate/no_turbo\nLinux uses the Intel P-state driver, intel_pstate, for modern Intel processors from the Sandy Bridge generation or newer. Other processors may default to the acpi-cpufreq driver which reads values from the systems UEFI or BIOS.\nTo view the current CPU frequency scaling driver, run this command in a terminal:\nClear Linux OS sets the CPU governor to\nperformance which calls for the CPU to\noperate at maximum clock frequency. In other words, P-state P0. While this may\nsound wasteful at first, it is important to remember that power utilization\ndoes not increase significantly simply because of a locked clock frequency\nwithout a workload.\nTo view the current CPU frequency scaling governor, run this command in a terminal:\nEach core will report its own status. Your output should look similar to this example with four cores:\nperformance performance performance performance\nThe list of all governors can be found in the Linux kernel documentation on CPUFreq Governors.\nThe intel_pstate driver only supports performance and powersave governors.\nThere are 2 ways to change the CPU frequency scaling governor:\nDisable Clear Linux OS enforcement of certain power and performance settings:\nsudo systemctl mask clr-power.timer\nChange the governor value in\n/sys/devices. In the example below, the governor is set to performance:\necho performance | sudo tee /sys/devices/system/cpu/cpu*/cpufreq/scaling_governor\nthermald is a Linux thermal management daemon used to prevent platforms from overheating. thermald forces a C-state by inserting CPU sleep cycles and adjusting any available cooling methods. This can be especially desirable for laptops.\nthermald is disabled by default in Clear Linux OS and starts automatically if it detects battery power. Enable thermald manually by using the systemd service by running the command:\nsudo systemctl enable --now thermald\nFor more information, see the thermald man page:\nThermalMonitor is a GUI application that can visually graph and log temperatures from thermald. To use ThermalMonitor, add the desktop-apps-extras bundle and add your user account to the power group:\nsudo swupd bundle-add desktop-apps-extras sudo usermod -a -G power <USER> ThermalMonitor\nAfter adding a new group, you must log out and log back in for the new group to take effect.\nEnhanced thermal configuration¶\nBetter thermal control and performance can be achieved by providing platform specific configuration to thermald.\nLinux DPTF Extract Utility is a companion tool to thermald,\nThis tool uses Intel® Dynamic Platform and Thermal Framework (Intel® DPTF)\ntechnology and can convert to the\nthermal_conf.xml configuration format\nused by thermald. Closed-source projects, like this one, cannot be\npackaged as a bundle in Clear Linux OS, so you must install it manually:\nMake sure your machine’s BIOS has DPTF feature and is enabled. It will usually be in the Advanced or Advanced>Power section of the BIOS.\nIntel DPTF requires BIOS support and is typically only available on laptops.\nGenerate thermal configuration. thermald configuration files will be generated and saved to\nsudo swupd bundle-add acpica-unix2 # install acpi tools git clone https://github.com/intel/dptfxtract.git cd dptfxtract sudo acpidump > acpi.out acpixtract -a acpi.out sudo ./dptfxtract *.dat\nRestart thermald service to take effect.\nsudo systemctl restart thermald.service\nCheck whether the configuration is in use.\nsudo systemctl status thermald.service\nThe following output means the configuration has already been applied:\nthermald[*]: [WARN]Using generated /etc/thermald/thermal-conf.xml.auto\nIntel® Turbo Boost Technology requires a PC with a processor with Intel Turbo Boost Technology capability. Intel Turbo Boost Technology performance varies depending on hardware, software and overall system configuration. Check with your PC manufacturer on whether your system delivers Intel Turbo Boost Technology. For more information, see http://www.intel.com/technology/turboboost\nIntel, Intel SpeedStep, and the Intel logo are trademarks of Intel Corporation or its subsidiaries.']	['<urn:uuid:b7d0371e-4029-4226-9649-59e5142a08f8>', '<urn:uuid:e5a8c556-28d0-4660-a403-12ed63740752>']	open-ended	direct	concise-and-natural	similar-to-document	comparison	expert	2025-05-13T01:23:33.086345	11	63	1668
75	what is ko rule in go	The ko rule prevents players from capturing back and forth indefinitely in a ko shape. When one player takes a ko, the other must play in another place before they can play at the ko again.	"['One very important concept in the game of Go is the concept of liberties, or free intersections surrounding a stone. To count how many liberties a stone has you have to simply count how many free intersections, surrounding that stone, aren\'t occupied by other stones. So a lone stone in the middle of the board has four liberties (intersections placed diagonally to each other aren\'t connected), while a lone stone on the side of the goban has only three liberties and one placed in a corner has two.\nWhenever an enemy stone occupies one of these intersections, the original stone loses one liberty (since the intersection isn\'t free anymore). It\'s also true that placing a stone of the same colour on an adjacent intersection reduces the liberties of the first stone by one, but they form a group, so their individual liberties aren\'t as valuable as before.\nWhen two or more stones, of the same colour, are placed on adjacent intersections (remember, not diagonally) they form a group. When a group is formed, its liberties are now the sum of free intersections adjacent to any single stone composing the group itself. So two adjacent stones, in the middle of the board can have at most 6 liberties (three for each stone, since the fourth liberty is occupied by the other stone in the group). We can see that grouping stones is a good way to increase liberties.\nTo capture a stone, or a group of stones, one must completely cut off all of its liberties (in other words, the stone is surrounded on all four sides). Once a stone is captured, it becomes a prisoner. This is important, because for every stone captured a point is subtracted from that player\'s total amount of territory. Capturing works the same way with groups of stones — all captured stones are removed from the board and kept by the player who captured them. The area where the stones were removed from becomes the territory of the capturing player.\nNeeds to mention suicide, both legal (when taking a group) and illegal (otherwise) A diagram would also be nice for that purpose\nA stone with only one liberty remaining is in ""atari."" This means the stone is vulnerable to capture in a single move by the opposing player. This is somewhat comparable to ""check"" in the game of Chess, but occurs more often and is ""less important"" to the overall strategy of the game. In Chess, when the King is in check, direct action must be taken to resolve this (e.g., the king must be moved out of check, another piece must move to block the attacking piece, or the attacking piece must be captured). In Go, there is no such requirement; when a stone is placed in atari, it is not necessary to respond at all. The stone can remain in atari for many moves, or be captured by the opposing player immediately, or protected by the attacked player immediately. When a stone is placed in atari, a player may see fit to attack another area of the board or continue working to capture other territory. A player placing an opponent\'s stone in atari has no obligation to proclaim it, as is the rule in Checkers.\nLearn to recognize capturable stones and groups: Go/Capturing exercise\nA ko is a shape (an arrangement of stones) where one player is able to capture exactly one of his opponent\'s stones with one move, but after capturing the stone his opponent is able to capture (in one move) the stone that was just played again, creating an opportunity for his opponent to capture that stone in the same way again. One could capture back and forth in a ko indefinitely. This could be a problem and stop all progress in the game, so a rule was made to stop this from continuing. When one player takes a ko the other must play in another place before they can play at the ko again. This gives the first player a chance to fill in the ko and stop the fight.\nTo prevent the first player from settling the ko, the second player must compel the first to also play elsewhere. This is done by identifying locations where, if the second player could play two stones in a row, the result would cost the first player more than winning the ko gains. These are called ko threats and can include threatening to rescue a large dead group, kill a large living one of the opponent, or mount a costly invasion into the opponent\'s territory. If the first player responds, the second player is now allowed to retake the ko and it is the first player who must now find a ko threat. If the first player ignores the threat and settles the ko, the second player follows through on their threat and takes some compensation for losing the ko.\nTo balance a game being played between players of significantly different skill, the weaker player is sometimes granted a handicap of one or more ""free"" stones. Some rule sets (Chinese, New Zealand, etc.) allow these to be placed anywhere the player desires. Other rules (Japanese, e.g.) have predefined handicap placement. Placing handicap stones is treated as black\'s first move, so white moves first after the handicap stones have been placed. One handicap stone is often given for every on rank difference between players. Thus if a player (white) with the rank of 7 kyu played a player (Black) with a rank of 12 kyu black would receive a handicap of 5 stones. though if the two players are only a rank or two different it is often unnecessary to use a handicap. In addition, one handicap stone is worth roughly 10 points, so if either you or your opponent does not know their rank play an even game and if one player loses by a large margin give them one stone for every ten points they lost by (as an estimate) and play another game. Repeat this until the proper handicap has been reached.\nIn even games (games without any handicap stones), white is given a compensation for the fact that black moves first. In Japanese rules, this compensation is worth 6.5 points (the half point ensuring that no game will ever be drawn). In handicap games, white is occasionally given a half (0.5) point of compensation, simply to prevent a draw from occurring.\nSometimes, instead of giving white the compensation (komi), black is given that compensation instead, as a sort of handicap. This is occasionally referred to as a ""one-stone handicap"", but is more popularly and simply called ""reverse komi"".\nStones are placed at the points created by the intersecting lines of the grid. A stone may be placed on any empty point, unless that point has no adjacent empty points and the placement does not cause the capture of adjacent enemy stone(s). The traditional way of placing stones on the board is by grasping the stone with the index and middle fingers, with the stone held horizontally between the index (below) and the middle (on top) fingers. This may feel awkward the first few times it is done.\nTerritory is surrounded when all of the points/intersections in an area are controlled by a single colour. Only an outline is needed to claim territory — placing stones at every point within an area will not claim territory (and in fact reduces the actual territory claimed — in Japanese rules the game is scored by counting open territory claimed, not spaces occupied by stones). An ""eye"" is one example of a group (joined stones) that claims territory. An opponent may play within territory if it is legal to do so.\nCutting occurs when a stone is placed between two groups of enemy stones in order to prevent the groups from connecting.\nPerhaps this should be moved more towards an intermediate level\nInitiative (Sente - Gote)Edit\nSente is when a player is ""ahead"" of his opponent if you will. It is when he forces the other player to respond to his moves. It is said that this player is in sente and his opponent in gote.\nThis section needs to be expanded. It may also need to be moved to a more intermediate level\nLife and DeathEdit\nA dead group is one that is unable to make more than one eye. Such a group is at risk of capture because it cannot be completely defended. A live group is one that is able to make two or more eyes. This is a very powerful group because the enemy cannot attack it successfully; any attack will result in immediate suicide or eventual capture of more territory or stones by the owner of the live group.\nGroups can sometimes be neither completely dead nor completely alive.\nA single group of interconnected stones with one point of territory forms an eye. A group with two or more eyes cannot be captured, as at both points white could not play without the stone played having no liberties (called suicide, and is not a legal move in the Japanese rule set), and is a powerful structure within the game.\nTsumego is Japanese for life and death problems. You are given a situation and you have to figure out how to make a group live (or how to kill a group). There are several sites that offer life and death problems, for example goproblems.com. There are also books containing these problems.\nTsumego is one part of Go/Igo/Baduk/Wei Qi, though studying tsumego alone isn\'t enough to progress it may help a lot in battles, as well as your calculative abilities and your ability to perceive weak groups.\nA Seki situation happens when opposing groups are placed so that both depend on the same liberties to survive. Neither player can place the other into atari because this would place themselves into atari as well. Seki situations are always left alone until the end of a Go game. The free points in them are not counted when it\'s time to count for scoring.\nOrder of playEdit\nFirst you should play the most urgent or necessary moves. These are the moves that could cause you to lose considerable amount of territory or weaken your position. Ask yourself: do I have any groups that could be killed in less than three turns, and should I protect them?\nIf you can find no moves of the first type, you should look for ones that take advantage of an opponent\'s weakness. Also find what would be your opponent\'s urgent moves and block those, or otherwise take advantage of them.\nIf there are none of the first two types of moves then find the move that would take the most territory or the biggest move.']"	['<urn:uuid:0cb72446-182e-4b20-8d4b-3cff5a935362>']	factoid	direct	short-search-query	similar-to-document	single-doc	novice	2025-05-13T01:23:33.086345	6	36	1799
76	physiotherapy initial treatment steps soft tissue injuries imaging requirements pros cons	Initial physiotherapy treatment involves RICE (rest, ice, compression, elevation), bandaging, taping, and gentle mobilization. Regarding imaging, there's often no need to rush getting scans for mild injuries - a watch and wait approach is appropriate. However, if concerned about further damage or poor response to treatment, imaging may be needed. Various options exist: X-rays are common but basic, MRI provides detailed soft tissue images without radiation, CT scans offer 3D views but expose patients to radiation, and ultrasound can effectively locate soft tissue damage.	['Joint and Muscle Injury\nMost people will experience a joint or muscle injury at some time in their life. They happen when we least expect it. It could be misjudging a step, playing sport, twisting suddenly. How the injury happened as well as signs and symptoms all help your physiotherapist determine the type and extent of the injury.\nExamples include a calf or hamstring strain, or a sprained ankle, meniscus tear in the knee, or tendinitis of the elbow.\nMuscle injuries (strains) and joint injuries (sprains) are broadly graded into three categories. Grade 1 (mild - disruption of a few fibres but no significant loss of stability or strength), Grade 2 (moderate – more disruption of fibres and more pain and swelling) and Grade 3 (severe/complete tear of muscle or ligament).\nThe approach taken by your physiotherapist will be determined by the severity of the injury. A more severe injury may require immobilisation, whereas with a mild injury “relative rest” may be encouraged rather than to stop moving.\nYou do not require a referral to see a physiotherapist as a private patient for any injury.\nIf there is any need for prescription medication or imaging, it is best that you see your doctor following a joint or muscle injury.\nWhether or not you need an x-ray, ultrasound, CT scan or MRI scan, is determined by the severity of the injury and your symptoms. With mild or even with some significant injuries there is often no need to rush imaging. It is often appropriate to watch and wait. If there are any concerns about further damage occurring or poor response to time or treatment, then your physiotherapist will liaise with your doctor about appropriate imaging and medication.\nThere are lots of things your physio can do to facilitate the healing process (keeping in mind that the actual healing is done by your body with the passage of time, not by any therapist).\nThis includes early advice about crutches, using bandaging or taping, ice and compression, rest and medication. Massage, ultrasound and gentle mobilisation may also be used early on. As you improve, exercises may be prescribed for regaining strength and flexibility. Your physiotherapist will advise you if further tests or imaging are required.\nIf the sprain or strain occurred for no apparent reason (it just “went”), it is important to examine the possible cause – this could be an imbalance in muscle length or strength, or sports technique which is putting strain on joints and needs correcting. For example, you may tend to over-utilize calf muscles instead of gluteal muscles when running, leading to calf strain or cramping.\nA common mistake is to accept an improvement in strength and condition of about 75%, and then expect to return to previous level of activity. Unfortunately, the joint or muscle is at risk of re-spraining. A return to activity which matches your level of rehabilitation is much safer. A sensible example is a tennis player who starts with exercises, then drills, then a return to social competition before pennant competition.\nAt Macquarie Street Physiotherapy, your physiotherapist will assist not only recovery from injury, but prevention of recurrence.', 'There are many different technologies available for locating soft tissue injuries, but which one is best? It depends on the individual situation. Which Technology Would Be Best in Locating Soft Tissue Injuries? For example, if you are looking for a specific injury such as a torn ligament, MRI would be the best technology.\nHowever, if you are simply trying to locate any areas of soft tissue damage, ultrasound or x-ray might be better options. Ultimately, it is up to the medical professional to decide which technology would be best in each individual case.\nTable of Contents\nWhich Technology Would Be Best in Locating Soft Tissue Injuries?\nSeveral different types of technology can be used to locate soft tissue injuries. Some of these include 1. X-rays – These are commonly used to diagnose broken bones, but can also help locate soft tissue damage.\n2. MRI – This type of imaging can provide detailed images of the inside of the body and is often used to diagnose problems with the spine or brain. It can also help identify soft tissue damage. 3. CT scan – A CT scan uses x-rays to create a three-dimensional image of the inside of the body.\nThis can help diagnose problems with organs or blood vessels, as well as identifying soft tissue damage. 4. Ultrasound – This type of imaging uses sound waves to create an image of the inside of the body. It is often used to examine pregnant women and their unborn babies, but can also help locate soft tissue damage.\nWhich of These is an Advantage of an Mri Over an X-Ray\nIf you’re looking for a more detailed look at your insides, an MRI is the way to go. MRI machines use radio waves and magnetic fields to produce high-quality images of your organs, bones, and other tissues. That means that they can show things that X-rays can’t, like soft tissue damage and certain types of tumors.\nWhich of These is an Advantage of an MRI Over a Ct Scan\nThere are a few advantages that MRI has over CT scan. First, MRI does not use ionizing radiation, while CT scan does. This means that MRI is much safer for the patient, especially if the patient needs to have multiple scans.\nAdditionally, MRI can provide more detailed images of soft tissues than CT scan can. Finally, MRI is generally better at detecting lesions that are smaller and located deep within the body than CT scan is.\nMatch Each Technology With the Disease It Helps Detect Or Treat\nThere are a variety of different technologies that can be used to detect or treat diseases. Here is a look at some of the most common:\nX-rays: X-rays are commonly used to detect broken bones, but they can also be used to diagnose other conditions such as pneumonia and heart problems.\nCT scans: CT scans use x-rays to create detailed images of the inside of the body. They can be used to diagnose cancer, heart problems, and other conditions. MRI: MRI uses magnetic fields and radio waves to create detailed images of the inside of the body.\nIt can be used to diagnose brain tumors, multiple sclerosis, and other conditions. Ultrasound: Ultrasound uses sound waves to create images of the inside of the body. It can be used to examine pregnant women and their unborn babies, as well as diagnosing other conditions such as gallstones.\nWhich of These is a Disadvantage of Ct Scan Technology\nThere are a few disadvantages of CT scan technology to be aware of. First, the radiation exposure from a CT scan can be significant, especially for young children or pregnant women. Second, CT scans are very expensive, so they are not always an option for patients with limited resources.\nFinally, CT scans can sometimes produce false positive results, leading to unnecessary anxiety or even invasive procedures.\nMatch Each Device With Its Advantage\nWhen choosing what devices to use for your business, it’s important to first consider what advantages each one offers. Only then can you make an informed decision about which will be the best fit for your needs. One of the most popular devices used in business today is the smartphone.\nIts main advantage is its portability, as it can be easily carried around and used anywhere. Additionally, smartphones typically have a wide range of features and applications that can be very useful in a business setting, such as email, calendars, and GPS. Tablets are another popular choice for businesses, and their main advantage is their larger screen size compared to smartphones.\nThis makes them ideal for tasks that require more screen real estate, such as viewing documents or browsing the web. Additionally, tablets usually offer longer battery life than smartphones. Laptops are often seen as the go-to choice for businesses due to their versatility.\nThey offer a large screen size like tablets but are also powerful enough to handle more demanding tasks like video editing or graphic design. Plus, laptops come with a variety of ports and connections that allow you to connect external devices like printers or scanners. Finally, desktop computers are still widely used in businesses despite the rise of portable devices.\nThe main advantage of desktops is their raw power and performance; they’re simply unmatched when it comes to handling resource-intensive tasks. Additionally, they tend to be more affordable than laptop computers with comparable specs.\nWhich Technology Would Be Best in Locating?\nThere are many different types of technology that can be used for locating purposes. GPS is perhaps the most well-known and widely used type of technology for this purpose, but there are also other options available. Radio frequency identification (RFID) tags are one example of a type of technology that can be used for locating people or objects.\nNFC tags are another option that can be used for similar purposes.\nWhat is the Protocol to Treat a Soft Tissue Injury?\nThere is no one-size-fits-all answer to this question, as the protocol for treating a soft tissue injury will vary depending on the specific type and severity of the injury. However, there are some general principles that can be followed in most cases. The first step is always to seek medical attention, as even seemingly minor soft tissue injuries can become complicated if not properly treated.\nOnce you have been seen by a doctor or other medical professional, they will likely recommend some combination of rest, ice, compression, and elevation (RICE) for the first 24-48 hours after the injury. This will help to reduce swelling and pain. After the initial RICE period, you may be advised to start gently stretching and exercising the injured area.\nThis helps to prevent stiffness and aids in healing. As your injury heals, you can gradually increase the intensity of your activity level until you are back to your normal routine. If at any point during your recovery you experience increased pain or swelling, be sure to consult with your medical provider as this may indicate that you are doing too much too soon.\nFollowing these general guidelines should help you recover from a soft tissue injury without further complications.\nHow Do You Prevent Soft Tissue Damage?\nThere are many ways that you can prevent soft tissue damage. One of the most important things that you can do is to warm up before you exercise. Warming up helps to increase blood flow to the muscles and tendons, which can help to reduce the risk of injury.\nAnother way to prevent soft tissue damage is to avoid repetitive motions. If you do the same motion over and over again, it can cause strain on the tissues and lead to injuries. Instead, mix up your routine and give your body a variety of different movements to keep things fresh.\nFinally, be sure to stretch after your workout. This will help your muscles recover from any exertion and prevent them from becoming tight or injured.\nThe use of diagnostic ultrasound is an effective means of detecting soft tissue injuries. However, there are several different technologies that can be used to locate these injuries, and it can be difficult to determine which one would be best for a particular patient. A new study has evaluated the accuracy of three different techniques—magnetic resonance imaging (MRI), computed tomography (CT), and ultrasonography—in identifying soft tissue injuries.\nThe results showed that MRI was the most accurate, followed by CT and then ultrasonography. This study provides important information that can help clinicians choose the best technology for their patients.']	['<urn:uuid:e99b29b6-020c-4e14-9af6-d70c04e98dfb>', '<urn:uuid:7e6991b9-2280-4161-afdd-0ead9c03d2d1>']	factoid	direct	long-search-query	similar-to-document	multi-aspect	novice	2025-05-13T01:23:33.086345	11	84	1944
77	studying ancient artifacts analysis which method better reveals hidden information neutron activation or probabilistic time theories	Neutron activation analysis provides more concrete information for studying ancient artifacts. While Prigogine's theories about time are important philosophically, neutron activation analysis has proven practical applications - it was first used at Brookhaven to study archaeological materials like Greek figurines, Roman pottery, and paintings, revealing specific information about their origins and manufacturing techniques through trace impurity analysis and activation autoradiography.	"['In Rational Defense of the Epiphany\nIn his 1997 book, The End of Certainty, physicist Ilya Prigogine (Nobel Prize in Chemistry, 1977) thrusts a well-honed sword deep into the heart of ""Laplace\'s Demon"" and leaves it lying motionless at the mouth of its cave. In layman\'s terms, Laplace\'s Demon is the notion that if you knew the position and momentum of every particle in the universe at a given point in time and also knew all of the equations governing movement and interaction, you could predict the exact state of the entire universe at any point in the future—as well as reconstruct its state at any point in the past. Or, as the marquis Pierre Simon de Laplace himself put it:\n""We may regard the present state of the universe as the effect of its past and the cause of its future. An intellect which at any given moment knew all of the forces that animate nature and the mutual positions of the beings that compose it, if this intellect were vast enough to submit the data to analysis, could condense into a single formula the movement of the greatest bodies of the universe and that of the lightest atom; for such an intellect nothing could be uncertain and the future just like the past would be present before its eyes.""\n""Laplace\'s Demon"" is the idea that the exact state of the universe (and, by inference, all human affairs) is set in existential stone. In other words, the future is knowable, we just don\'t know enough to know it.\nAlthough Laplace never actually referred to his hypothetical ""intellect"" as a ""demon"" (that tag was applied later by his biographers), the image makes sense given the era in which he lived (1749–1827). For those of us who dwell in the 21st century, the analogous ""intellect"" is more likely to be thought of as a computer. Regardless of image, however, the notion is important, because it implies that the future state of the universe (and, by inference, each event in every human life) is already set in metaphorical stone. In other words, the future is completely knowable—we just don\'t know enough to know it. Our inability to predict what will happen in the future is merely the consequence of our limited capacity for knowledge. (No offense.)\nMost people find the notion of a deterministic universe somewhat grating, because they like to think of themselves as having free will. And the idea tends to be especially noxious to storytellers and other creative artists, who are prone to favor the idea that their works arise, at least in part, from shining sparks of chaos rather than the lock-step, if/then march of Nature.\nIf Laplace was right, then free will is merely an illusion.\nBut there it is—an idea put forth by a scientist and mathematician on the order of Isaac Newton. And if it is true, then free will is merely an illusion, because every action we take is already determined. And the idea of artistic creativity is a lie. Those of us who create new works are merely carrying out the predetermined orders of the universe.\nEnter Ilya Prigogine.\nAs a physicist specializing in non-equilibrium thermodynamics, Prigogine was an expert in entropy—the tendency of the universe to move toward increasing disorder (think of your office desk or kitchen table). And that expertise led him to argue that the world of science treats time incorrectly. Specifically, he proposed that time is not a ""fourth dimension"" through which one can move back and forth and that it is, in fact, something special, with a directional arrow that points only one way... into the future. You can\'t un-brew a pot of coffee or un-mix a milkshake (or un-send that email you fired off in a moment of anger—a word to the wise).\nEnter Ilya Prigogine, armed with a Nobel prize in chemistry, to argue that the world of science treats time incorrectly—and that the world is not deterministic, it is probabilistic.\nThe implications of Prigogine\'s premise are profound, because if it is correct, then the universe is not deterministic at all. It is probabilistic. The Demon is free to predict what might happen, but he can never predict what will happen. In other words, it\'s not that the future is knowable and we just don\'t know enough to know it. It\'s that we cannot know the future because it is, by its very nature, unknowable.\nTime to warm up that coffee now. (And keep in mind, you can\'t un-warm it. You can let it grow cold again, but that\'s not the same thing.)\nIt\'s not that the future is knowable but we don\'t know enough to know it. It\'s that, by its very nature, it is unknowable.\nIt is doubtful that Prigogine had the world of storytelling in mind when he formulated his arguments, but his premise has deep implications for the inhabitants of that world, especially when it comes to inspiration—that exhilarating rush of creative power that surprises the writer with insights that cannot be reached by methodical plotting. Every writer who has ever had a true moment of epiphany when writing a story knows that breathtaking rush. It comes out of nowhere and assembles an array of seemingly disparate elements into a unified truth that is greater than the sum of its parts—a truth that cannot be reached by a staircase but must be flown to on the wings of inspiration.\nThe problem is, of course, that inspiration can be fickle and elusive. It can be hunted for but never trapped and tamed. And those who seek to lure it into the open must use as bait the one great delicacy it cannot resist—an open mind. But the very idea of a deterministic universe, where the fate of every creature is unchangeable and creativity is an illusion, is a rationalistic mind-closer of the highest order that shutters the great open windows through which inspiration flies.\nThe more a writer clings to rationalism, the more susceptible he becomes to the idea that inspiration is a value-less illusion.\nAnd the longer the writer remains shuttered in rationalistic darkness, the more susceptible he becomes to the idea that inspiration is a value-less illusion—and that what matters in life is the solid and measurable face that the universe presents to those who choose to observe it strictly in terms of its particulars. And when the idea takes hold completely, it commences a virus-like replication that can sweep through the writer\'s entire creative mind, choking all non-rational notions into oblivion.\nIs there any hope of salvation for a writer who falls prey to the clutches of deterministic thought? The answer is yes, absolutely. Sometimes, it comes in the form of a life-wrenching personal event—something unexpected that shakes the foundations of the writer\'s life and causes him to rethink its every aspect, including the dark, smothering funk of determinism. But since one cannot always count on such an event to occur, it is handy to have a ladder nearby—a step-by-step, if/then path out of the darkness.\nAnd that is what Prigone\'s premise provides. In a nutshell, it looks like this:\n- The universe is not deterministic and rigid, it is probabilistic and fluid—a place that can never be fully known. And it is filled with a mix of speculation and certainty in varying proportions.\n- Because the universe is unknowable, its non-rational elements are no less elemental than those deemed ordinary and rigid. And creative works are not merely the predictable results of mechanistic generation. They arise, in part, from the surprising spontaneity inherent in the structure of all that is.\n- Inspiration is not a product of delusion. It is the tangible manifestation of universal fluidity—and is worth nurturing.\n- The creative path is unknowable by definition. You are free to allow yourself to not require that you know it... and to move forward when it seems like everything is already known, confident that you are incorrect.\n- It\'s time to emerge from your funk and get back to work.\nAnd there you have it—a rational way out of the inhibiting restrictions of rationalism. A step-by-step path leading away from the undeserved veneration of step-by-step paths.\nFeel free to pass it on.', 'Chemistry\'s Greatest Hits\nWhat are some of the most significant accomplishments in the fifty-seven year history of the Chemistry Department? This is obviously a very subjective list, but at the clear risk of offending everyone, here we go:\nRay Davis, Solar Neutrinos, and the 2002 Nobel Prize in Physics\nWe are justifiably proud of our colleague, Ray Davis and his winning of the 2002 Nobel Prize in Physics. While work at Brookhaven has been associated with more than seven Nobel Prizes, this prize was the first won by a member of the BNL permanent staff. In addition, as chemists, we are pleased by the affirmation of the unity of science reflected by the awarding of the prize in physics to Ray, a chemist.\nFluorodeoxyglucose and PET\nPositron Emission Tomography (PET) is a widely used technique to probe brain function, whereas most other imaging methods reveal structure. PET images are developed by geometric reconstruction of the locations at which positrons are annihilated after being emitted from radioactive compounds metabolized in the brain. Therefore, radioactive compounds must be used that pass the blood-brain barrier, are used in the brain, and are nontoxic. The method used for the discovery and synthesis of these compounds grew out of ""hot atom chemistry"", a field nourished by the Atomic Energy Commission, the original federal funding agency supporting Brookhaven, and a discipline in which the Chemistry Department played an important role. The synthesis and use of 18F-labeled fluorodeoxyglucose (FDG) was pioneered by researchers in the Brookhaven Chemistry Department. FDG is now the most popular compound used in PET studies worldwide.\nElectron Transfer in Solution\n""Oxidation-Reduction is the simplest reaction."" Well, maybe. It certainly represents one of the most important classes of reactions, governing electrochemistry, which in turn provides understanding of batteries, fuel cells, corrosion, and a host of other phenomena. Experiments in electron transfer were an early fixture in the Chemistry Department, (when it needed to be shown that the rates of such rapid reactions could be measured at all), and continue today with connections to solar energy conversion and the ""hydrogen economy"". In addition, studies of electron transfer helped to stimulate theoretical efforts to describe these processes. The most successful theory of electron transfer is ""Marcus Theory"", authored over several decades by Professor Rudolph A. Marcus, now of the California Institute of Technology, and the recipient of the 1992 Nobel Prize for Chemistry, awarded for his theory of electron transfer. Much of the experimental work confirming the validity of Marcus theory was performed by researchers in the Brookhaven Chemistry Department.\nThe Radiation Chemistry of Water\nUnderstanding radiation effects in water is basic to fundamental radiobiology and applications of nuclear technology depending on neutron moderation by water. The nature of the reducing and oxidizing radicals formed in water irradiation was first demonstrated in the Brookhaven chemistry Department. It was shown that the predominant reducing species has a negative charge and is a hydrated electron. This not only introduced an important new species to chemistry, but also made possible the first correct treatment of water decomposition and re-formation in nuclear applications.\nAccurate measurement of absolute reaction rates of the hydroxyl and perhydroxyl radicals in irradiated water, difficult to make because of their rapidity, were also first measured in the Brookhaven Chemistry Department. A new superoxide of hydrogen, HO2, was also first discovered there.\n""Hydrogen Atoms in the Radiolysis of Water"" N.F. Barr and A.O. Allen, J. Phys. Chem. 63 928 (1959).\n""Determination of Some Rate Constants for the Radical Processes in the Radiation Chemistry of Water"" H.A. Schwarz, J. Phys. Chem. 66 255 (1962).\n""The Nature of the Reducing Radical in Water Radiolysis"" G. Czapski and H.A. Schwarz, J. Phys. Chem. 471 (1962).\nNeutron Activation Applied to Art and Archaeology\nThe first application of neutron activation analysis to the study of archaeological materials was made at Brookhaven. In some Greek terra cotta figurines. It was found that the trace impurity compositions fall into patterns which are correlated to locations of origin and manufacture. Ancient commerce in Roman Aretine pottery, in South Arabian pottery, and in Mayan pottery was traced. An extensive study of ancient glass revealed many facts concerning the technology of its manufacture. This method was subsequently adopted elsewhere. At Brookhaven, activation autoradiography was applied for the first time to investigations of works of art such as paintings and to recover images from badly faded historic photographs. In paintings, the different pigments give rise to radiations decaying with different half-lives, and a sequence of radioautographs can provide information about layers of pigments used by the artist and techniques of application.\n""Neutron Activation Autoradiography of Oil Paintings"" E.V. Sayre and H.N. Lechtman, Studies in Conservation 13 161 (1968).\n""Activation Analysis Applications in Art and Archaeology"" E.V. Sayre, Adv. in Activation Analysis Vol. II, pp.156-94 (1970).\nLast Modified: June 28, 2012']"	['<urn:uuid:2e6e1362-4bae-4d2c-a027-263062868864>', '<urn:uuid:ce6606a9-1c3a-4b2a-8395-ce97f502a161>']	factoid	with-premise	long-search-query	similar-to-document	comparison	expert	2025-05-13T01:23:33.086345	16	60	2175
78	i am new to investing what is responsible investment and how does it help create long term value	Responsible Investment (RI) focuses on integrating environmental, social & corporate governance (ESG) risks and opportunities into investing, along with active ownership through voting and engagement. This approach is more likely to create and preserve long-term value by considering ESG risks and opportunities that have material impact on long-term risk and returns. It involves taking a long-term view, challenging the status quo, questioning capital allocation, and considering future risks, ensuring value sustainability not just for 5-10 years, but for 20, 30, or 50 years ahead.	['Responsible Investment (RI) is focused on the integration of environmental, social & corporate governance (ESG) risks and opportunities into the investment process, along with the exercise of active ownership (voting and engagement). RI champions sustainable investment approaches that consider these ESG risks and opportunities, recognising their material impact on long-term risk and return outcomes.\nLooking at investment through an RI lens offers an investment view that is more likely to create and preserve long-term value. This is an important touchpoint within Mercer’s investment beliefs. It ensures the viability and sustainability of value, not just over the next five or ten years, but over the next 20, 30 or 50. Taking a long-term view is never easy, especially when short-term demands require so much consideration and compete for the attention of already stretched resources. It involves challenging the status quo, questioning how capital could be better allocated and utilised, and considering what risks might emerge in the future.\nMercer has been at the forefront of this important aspect of investing since forming a dedicated global team of RI specialists in 2004.\nOur RI team’s thought leadership, innovation, advice and research helps Mercer’s clients and our own portfolio managers mitigate risk, demonstrate active ownership, and construct investment portfolios targeting long-term returns and sustainable growth. The team also helps those investors seeking greater alignment between their organisation’s values, policies, and investment allocations. It seeks to challenge current thinking and always think long-term to anticipate and drive beneficial change. In short, our RI team helps shape and set the ESG agenda for investors around the world.\nOur research and innovative thinking in this area dovetails with our broader Mercer Governance Practice Solutions and Mercer’s industry engagement with highly-regarded groups like the World Bank Group’s International Finance Corporation (IFC) and the Investor Groups on Climate Change (IGCC).\nMercer’s Investment Framework for Sustainable Growth\nMercer’s Investment Framework for Sustainable Growth outlines how we work with our clients in each stage of the investment process, from beliefs and policy, to processes and portfolio. Whatever your beliefs, can you express and stand by them? Mercer helps Boards and investment teams review, agree on, understand and articulate their beliefs, investment policies and processes to guide future decision making and remain alert to ESG factors. Mercer can also help construct a sustainable investment portfolio that will be rolling along years from now.\n|Beliefs||Policy & Process||Portfolio|\nThe Pursuit of Sustainable Returns\nAs per Mercer’s investment framework for sustainable growth, sustainable investment returns for a portfolio need to consider how ESG and sustainability themes are integrated by asset class. Incorporating investments that reflect a greater level of ESG integration or sustainability themes into the asset allocation framework is an area in which we’ve seen growing interest. We believe regulatory pressures to meet global standards of ESG integration and responsible stewardship will only increase in coming years and we therefore see this as an area of increasing importance over the long term for all investors.\nMercer has developed a complete portfolio reference guide, which outlines all the drivers for addressing sustainable growth trends at a portfolio level for each major asset class. The Pursuit of Sustainable Returns summary of this complete guide is accessible at right.\nOver a Decade of Innovation and Research\nMercer has been leading the field in sustainable investing since 2004. Our RI team has a long history of innovative research, developing new tools and processes, and partnering with industry leaders. We examined sustainability trends with our seminal 2011 study examining the strategic asset allocation implications of climate change and returned to build upon that knowledge with the groundbreaking Investing in a Time of Climate Change study that confirmed climate change as an investment risk that could no longer be ignored, and contributed to the discussions leading to the December 2015 Paris Agreement. We have championed governance, as with our 2013 report on building a long-term shareholder base through loyalty-driven securities, and we’ve led in the area of ESG integration, assigning explicit ESG ratings across all asset class strategies as part of our manager research process.\nJane Ambachtsheer ‘in person’\nThe PRI in Person team caught up with Jane Ambachtsheer to reflect on a decade of Mercer RI.\nInvesting in a Time of Climate Change\nHow could economic, environmental, technological, societal, and geopolitical issues impact the ability to meet return objectives? What steps can be taken to mitigate risks or capture opportunities accordingly?\nTHE FUTURE MAKERS\nLong Term Investors as Climate Change ‘Cops’\nInvestors can take specific actions right now to make their investment portfolios more resilient. These investors can be described as “future makers.”\nTHE CLIMATE CHANGE VARIABLE\nWhat’s the impact on Investor portfolios?\nThe investment portfolios of institutional investors of all types will be affected, including insurers, corporations, pension funds, endowments and foundations, as well as individuals. Learn how they will be affected.\nModeling Asset Sensitivity to Climate Change\nExisting modeling does not capture very long-term structural changes, precisely the type expected as the world manages the risks posed by climate change.\nCLIMATE CHANGE SCENARIOS\nScenarios and the Goal of Transformation\nThe year is 2050. Investors, business, governments, and civil society have worked collaboratively and with success to mitigate the long-term effects of climate change. This is what a Transformation scenario looks like.\nCLIMATE CHANGE RISK FACTORS\nBringing the Long-Term into Closer, Sharper Focus\nImagine the investable universe could be examined through a telescope. The simplest scope might give you a better view of the more obvious risks and opportunities but professional investors need more sophisticated systems of lenses allowing for greater magnification.\nInvestor Action On Climate Risk\nWhat are the key motivations for investor action and what should that action look like? These are among the questions investors need to be asking.']	['<urn:uuid:ada8214e-0c15-4b30-853b-34c731772249>']	open-ended	with-premise	long-search-query	similar-to-document	single-doc	novice	2025-05-13T01:23:33.086345	18	84	955
79	What unique historical sites can tourists explore when visiting Karystia?	In the Karystia region, visitors can explore several significant historical sites. Notable among them is Filagra, an ancient fortress that effectively controlled the southeastern coast of Evia and communicated with signals to the castles of Karystos and Styra. The site features castle walls, an underground gallery to the sea, and two small churches. The Byzantine church of Agios Ioannis the Theologian, built on ruins from the Frankish period, showcases interesting folk-style paintings and unique dome architecture. In Karystos itself, tourists can visit the ancient marble quarries in Kylindroi, the restored 13th-century Venetian tower called Bourtzi on the beach, and the Castello Rosso ruins from the 13th-15th centuries. The area also features the famous 'drakospita' on Mount Ochi, megalithic constructions dating from the 6th to 2nd century B.C.	"[""On Sunday 1/9/2013, SPPENK (t/n: Society for the Protection of the Environment of South Karystia) organized a hike from Aktaio to Filagra, with the participation of 20 members and friends of the association. With the hikes, the association wants to give the opportunity to those who are interested, to know the unique nature of Karystia, as well as the works of traditional culture and the ruins of the past.\nThe route by car from Steno to Aktaio has an impressively dense forest vegetation of evergreen oaks, holly and heaths. The scattered houses of the village seem to hang over the steep shore with the sea always buzzing restless.\nThe hike starts from the church of the Assumption of the Virgin Mary on a clear path that descends to the coast in the direction of Giannitsi. The rich vegetation and the blue of the sea in the background flood the eyes. Soon we meet an old half-ruined farmhouse in Afrati and a little further, a threshing floor that looks like a balcony over the sea. The path continues downhill and just below, is a spring with a cistern. The old farmers had built terraces that were once cultivated creating a paradise in the arid landscape. Farther from the path, we can see a few shepherds' huts that still retain their earthen roof and are still in use.\nAfter the spring, a short distance away, we meet the Byzantine church of Agios Ioannis the Theologian, which stands like an elegant work of art in the silent landscape. The church has been built on the ruins of an older church of the same name, that dates back to the years of Frankish rule. The original church was larger than the newer one and had a dome, at some point it seems that it was ruined and built from the beginning probably during the Turkish occupation and more recently in 1944. The existence of a Byzantine church in the desert is probably explained by the fact that the area around Giannitsi (Ioannikion) presented religious activity in the early Byzantine years and had become a pole of attraction for monks. From the original temple, only the foundations of the sanctuary arch and some stones on the side walls survive. The last reconstruction of the temple, which we see today almost intact except for the doors, was done with poorer means, partial use of bricks of the older temple and slates of the area. The popular masons of 1944, led by Andreas Regkoukos from Melissona, followed the plan of a cruciform church on the ground floor, while they placed a dome on the roof. The choice was bold, since the domed temples in Evia were not usual and connect the temples with a dome and the cruciforms. Interesting folk-style paintings are preserved inside the walls. The coincidence of the 1944 repair with the liberation from the Germans, leads to the hypothesis that the repair was probably done as a fulfillment of a promise or vow.\nBehind the sanctuary of the temple, begins a dirt road that leads to the rocky hill of Filagra that dominates the landscape. The ascent to Filagra is made by an uphill path that starts from the access road that surrounds the steep hill. The view from above (300m.) is panoramic to the sea, the east coast of Evia and the mountains around Giannitsi. The amazing position of the hill and the natural fortification offered by the cliffs, were used from antiquity until the time of the Ottomans by the rulers of the island. On the south side, the walls of the castle can still be seen. The name Filagra comes from the name Fylaktra, because the fort effectively controlled the sea on the southeast coast of Evia and then communicated with signals to the castles of Karystos and Styra through intermediate forts. The southeastern coast of Evia was known in antiquity as Koila of Evia, which owes their name to the convex shape of the southeastern coast of Evia rather than the existence of a port. Koila of Evia are mentioned by many writers of antiquity with awe for the strong dangerous winds that caused shipwrecks. Filagra, at times in the Middle Ages, was a stronghold of revolutionaries or pirates for raids against the castles of Karystos and Armenos. From the castle, starts an underground gallery that ends at the sea. According to the legend, the Turks, in order to occupy the castle, threatened a young woman who was washing her clothes in the river to show them the secret passage and so they occupied the castle. Since it was not handed over to them by the Franks like the rest of Evia after the fall of Chalkida (1470), it is possible that it was owned by pirates. But apart from being a fortress, Filagra was also a sacred place where pagan ceremonies were held to clear the strong winds. Today there are two small churches.\nΓια την επιστροφή ακολουθήσαμε την ίδια διαδρομή. Η πεζοπορία ολοκληρώθηκε με άνεση σε 3 ώρες.\nFor the return, we followed the same route. The hike was completed comfortably in 3 hours.\nΧ. Φαράντος, Ο ναός του Αγίου Ιωάννη του Θεολόγου, Ευβοϊκά Χρονικά\nΣτ. Παπαμιχαήλ, Τοπωνύμια και ακτωνύμια του Καβοντόρου, Ευβοϊκά Χρονικά\nW.P. Chapman, Karystos: City-state and Country Town"", 'What to See in Evia (Euboea)\nTour in the city of Vasiliko\nVasiliko is located 8 km SE of Chalkida. The settlement is in the historic Lilantio Pedio (Plain of Lilantio), which according to Greek writer K. Palamas is the most ""fruitful plain” of Evia at present. The old square, the manor houses, the Frankish tower and the church of Panaghia (Holy Mother) are worth visiting. The coastal settlement of Lefkantio is located 2 km away from Vasiliko. In the Plain of Lilantio, to the NE of Vasiliko, you will find the settlement of Fylla with the Venetian Castle of Likari (Kastelli, 13th -15th centuries) as well as Andreas Miaoulis\' house behind the sanctuary of the church of Panaghia. The two identical Frankish towers in the village of Mytikas (3 km NW) are also worth a visit.\nTour in the city of Limni Evias\nLimni Evias is a small coastal town, beautiful all year long. Its architecture and town planning remind us of an island settlement. Limni Evias is located 80 km NW of Chalkida.\nThe church of Genisis tis Theotokou (Virgin Mary’s Heavenly Birth) in the entrance of the town. It is a three-aisled basilica built in 1837 with a remarkable marble belfry built in 1879 by Mastroloukas, a sculptor from Tinos island.\nThe church of Zoodoxos Pigi (Life giving font of Virgin Mary) and the ruins of a basilica dating back to the Early Christian period and built on Roman spas.\nThe Historical Archive (including documents from 1732) housed in the Town Hall.\nThe buildings of the Police Station and Public Financial Service, designed by Chiler.\nThe manor house of Angelis Flokos housing the Folklore and History Museum, where among other things local archaeological findings are also exposed.\nThe hermitage of Aghios Christodoulos (12th century) at the west end of the settlement near the beach.\nThe historic female monastery of Aghios Nikolaos (7th - 8th century), commonly known as Moni Galataki, located 10 km SE. It is known as the Galataki Monastery. On the SE side is the ascetic shelter of Aghios Andreas. It is a small single-room arched building, built in a natural cave.\nThe Museum of Hydrobiology, on the road to the Monastery, including fossilized exhibits coming from the seabed, shells and fish from Greek and foreign seas.\n“Elymnia”: cultural events organized in Limni and the surrounding villages from mid-July until August 15th.\nFestivities and treasure hunting during the Carnival\nFair of Panaghia Limnia on September 7th - 8th.\nTour in the city of Psahna\nPsahna is located 16 km to the north of Chalkida. The town is built on a lush green landscape near the sea and has many listed buildings. Your sightseeing tour in Psahna should include the stone-built church of Metaforfosi (Transfiguration) as well as the Monasteries of Panaghia (Holy Mother) of Makrymalli (13th - 15th centuries) and Osios Ioannis Kalybitis (9th-12th centuries). The villages of Makrykapa, Palioura, Attali, Aghios Athanasios and Triada (where you can see the tower of Nikolaos Kreziotis, Greek military commander) are also worth visiting. The Messapios River flows through the fertile plain.\nTour in the city of Aliveri\nBuilt on a hill, in the shape of an amphitheater and near the sea, Aliveri is an industrial town of Evia with rich lignite deposits. It was the center of rebellion in South Evia during the first part of the 1821 Revolution. Karavos is the seaport of Aliveri as well as the tourist area of the region. The industrial zone and the thermoelectric power plant of the Public Power Corporation are located NE of the village. Finally, Alivery is located 48.5 km SE of Chalkida.\nThe small arched-roof church of Koimisi tis Theotokou (Panaghitsa), which dates back to 14th century, built on a rock in the center of Aliveri.\nThe frescos of Aghios Nikolaos, in the precinct of the Cathedral of Aghios Georgios on top of the hill.\nThe Mycenaean tomb discovered in the area of Katakalou.\nThe red church of Aghios Dimitrios (in the same region), built with red stones and columns, where you can see early Byzantine frescos.\nThe Neolithic settlement in Dystos.\nThe ruins of Roman spas in Karavos.\nThe scenic lush green village of Aghios Ioannis which is located 6km NW of Aliveri. On its west side flows the tributary of Pertheniatiko.\nRizokastro in Milaki, located 5 km SE of Aliveri, one of the oldest fortresses dating back to the Frankish rule, with its water tanks, warehouses, barracks and a tower.\nThe hydro-habitat of the lake of Dystos, after Milaki and Prasino. The habitat offers shelter to a great variety of aquatic birds. On a small hill next to the lake, you will have the opportunity to observe the seraglios, buildings dating back to the Ottoman rule.\nThe islet of Kamila, in the southeastern part of the Aghioi Apostoloi bay (on the side of the Aegean Sea).\nSwimming at the beaches of Nireas, Karavos, Kalami, Pontikou and Kambos, all the beaches of Southern Evia bay as well as the magnificent beaches of coastal villages Aghioi Apostoloi and Petries (Limionas, Stomio, Cheromylos, Liani Ammos) on Aegean Sea.\nThree-day feast of Aghios Ioannis, from August 27th to 29th.\nTour in the city of Prokopi\nThe Kireas river flows through the fertile valley with plane tree forest (the region reminds us of Tembi in Thessaly). The village of Prokopi, mainly known for the church of Aghios Ioannis Rossos that is very popular with pilgrims, is located in the valley. Prokopi is located 49.5 km to the north of Chalkida.\nThe church of Aghios Ioannis Rossos. The shrine was brought here by refugees arriving from Cappadocia and nowadays it is kept in this monastery built in a lush green area. Lots of pilgrims visit the monastery every day, while on May 27th , there are celebrations for the Saint\'s Feast.\nThe Baker Manor House\nThe village of Dafnoussa (5 km W) and the nearby medieval tower.\nThe magnificent route to Pili and Vlachia located 13 and 27 km to the east respectively.\nThe gigantic old plane tree in the road to Mantoudi next to Kireas’s riverbank (""Paraskevorama”).\nThe Frankish castle of Kleisoura (13th -15th century)\nSwimming at the beaches of Mantoudi\nTour in the city of Avlonari\nA beautiful village built in the shape of an amphitheater with traditional stone houses, where the visitor can see the skills and artistry of the craftsmen of Epirus. Avlonary is located 68.5 km east of the town of Chalkida.\nThe Cultural Center of ""Ioannis Ar. Tzemis”. Tzemis”.\nThe Venetian tower in the center of the village.\nThe Chania of Avlonari. - The settlement Chania Avlonariou, where you will have the opportunity to observe the hagiographies of Aghios Dimitrios Monastery. You will also see the imposing century old plane tree out of the monastery.\nThe stone arched bridges, near Aghios Georgios.\nThe female monastery of Aghios Charalambos of Leukon (Middle Byzantine period).\nThe Folklore Collection in the village of Orio.\nThe church of Aghia Thekla around 2 km W, in the homonym settlement. The church has some impressive wall paintings (13th-15th century).\nOktonia or Ochthonia, 5km NE, with running waters and rich vegetation. Here, you can visit the Byzantine monastery of Aghios Dimitrios Kataraktou.\nThe village Vrisi, 10km NW, in a location full of plane and chestnut trees. The old watermill outside the village which still works and Kolethra spring which comes out from underneath a rock are worth a visit.\nSwimming at Mourteri, Korasida and Kalamos.\nThree-day feast of Aghios Ioannis Prodromos (St. John the Baptist) on August 29th.\nTour in the city of Mantoudi\nMantoudi is a beautiful region with plane trees and lots of running waters. Walk around the main square and the alleys with old and modern houses. Once, there were metal factories here and magnesite was extracted. Mantoudi is situated 58.5 km NW of Chalkida.\nThe church of Aghios Ioannis Theologos.\nΤhe local library\nΤhe villages Kirinthos and Strofylia situated at 4 and 8 km NW respectively. In Kirinthos you will see the residencies of Zaimis and Averof families, whereas in the latter there is a remarkable stone-built church of Aghia Τriada (Holy Trinity).\nThe remains of ancient Kirinthos on the beach of Peleki.\nKalyvia (14 km SW) with a spring and a plane tree which is over a hundred years old.\nSwimming at the beaches of Kymasi, Amitsa, Palaios Dafnopotamos and Dafnopotamos on the Aegean Sea.\nFair of Aghios Ioannis Theologou on September 26th , lasting 8 days.\nFestivities during the Carnival.\nCultural events organized by the Municipal authorities during the summer.\nTour in the city of Karystos\nKaristhos is a picturesque town and port, with many squares and a functional street planning which was designed by city planners of king Othonas. It is known from ancient times for its famous Karistos slabs, slate rocks of the area. The village is located 124 km SE of the town of Chalkida.\nThe ancient marble quarries in Kylindroi after Myloi.\nThe restored Venetian tower (Bourtzi) on the beach to the east side of the town. The tower was built in 13th century.\nThe ruins of a Venetian castle (Castello Rosso, 13th - 15th century) which is situated further from the small forest.\nThe imposing Town Hall.\nThe Giokaleio Foundation housing the Archaeological Museum with exposition of important findings from the region.\nThe Cathedral of Aghios Nikolaos.\nThe mausoleum of a Roman official surrounded by a peristyle.\nThe Monastery of Aghia Mavra, which follows the old calendar.\nThe Monastery of Taxiarches (Archangels) in Kalyvia dating back to 9th -12th century and the one of Aghios Georgios in Methochi.\nThe mountain Ochi, on top of which stands one of the renowned “drakospita”. It is a megalithic construction dating back to the 6th to 2nd century B.C.\nThe mountain settlements of Ochi (Kalyvia, Aghios Dimitrios, Pothi, Giannitsi).\nThe village of Platanistos (18 km SE), where you can see a defensive wall of the Hellenistic period.\nThe lush green landscape of Aghia Triada.\nThe village of Palaiochora, where there was a cemetery dating back to the Hellenistic period.\nThe picturesque villages of Kavo Doro, Amygdalia, Drymonari, Prinia, etc.\nThe gorge of Dimosari, which begins from the source of the Dimosaris River near the Myli and ends on the beach of Kolianou 10 km further.\nSwimming at the beach of Karystos, as well as Livadaki, Plaiopithara, Mnimata, Aghia Paraskevi, Psili Ammos, Kavos, Bouros, Bathy Avlaki, Aghioi, Kalami, Kioni, Platys Gialos and Livadi.\nCultural events are organized during the summer.\nThe Wine Festival from late August until early September.\nTour in the city of Kymi\nKymi is called “balcony to the Aegean sea” because of the magnificent view it offers. At the same time it has one of the biggest artificial ports of the country providing ferry connection with Skyros. Kymi is the homeland of prominent physician Georgios Papanikolaou (1883-1962), who invented a method for diagnosing the cancer of the uterus, commonly known as the ""Pap test”. You will have the opportunity to observe the typical residences, examples of the local architectural tradition (mainly neoclassic residences of Captains) with details such as elaborate railing patterns on balconies, brackets and owner inscriptions, while walking around the traditional settlement. Kymi is located 87 km NE of Chalkida.\nThe ancient city ruins near the modern settlement.\nThe Aghios Athanasios square with stone houses and paved alleys.\nThe church of Panaghia Liaoutsanissa, which dates back to 1849. The chancel and the pulpit of this church are created by the father of well-known sculptor from Tinos, G. Chapela.\nThe Monastery of Aghios Panteleimon at the eastern end of the town.\nThe Folklore Museum housing a collection of local traditional objects.\nThe location of Choneutiko, a lush region situated 2 km NW and named after the spring drinkable water with therapeutic and digestive qualities.\nThe female monastery of the Metamorfosi (Transfiguration) (17th century) located 5 km to the north and offering a magnificent view.\nThe particular road connecting Paralia to Kymi, marked out in the middle of 19th century, about 4 km long with 52 bends in total, which was recently integrated in the official Greek Rally Championship as an ascent.\nSwimming at Soutsini, Chili, Platana and the small harbours until the beach of Stomio.\nSquid fishing in August\nChalkida, the capital of Evia, has 53,000 inhabitants and is built at the narrowest point of the strait of Evripos, and is a transport interchange as well as the administrative, commercial, industrial and cultural center for the wider region. As it has been proved, Chalkida\'s history begins during the 3rd millennium B.C. in the region of Manika. The excavations revealed a town with a fairly large population that was very prosperous during the Classical, Hellenistic and Roman period. Chalkida was conquered successively by Athenians, Macedonians, Romans, Byzantines, Franks and Venetians. It came under Turkish rule in 1470 and was liberated along with the rest of Evia in 1830. Chalkida was the homeland of rhetor Isaios and poet Lykofronas, and Aristotle died in this town. In addition to this, prominent personalities of art and literature of the modern history, such as musician Nikos Skalkotas, writer Yannis Skaribas, painter Dimitris Mytaras as well as actors Orestis Makris and Elli Lambeti come from Chalkida. Chalkida is located 80 km north of Athens and 470 km to the SE of Thessaloniki.\nArchaeological Museum: 22210-25.131\nFolklore Museum: 22210-21.817\nMunicipal Art Gallery: 22210-22031Website for the town of Chalkida: www.chalkida.gr\nDrakospita (dragon houses) of Stira\nDrakospita are “Cyclopean"" constructions and are located in south Evia and particularly in the Municipality of Stira. They are built in steep areas and they are massive and imposing constructions that intrigue visitors’ curiosity. Nobody knows when they were built and for what reason. The most known and well preserved Drakospito is the one of Ochi and has been characterized by the scientists as an ancient temple where god Zeus and goddess Hera were worshiped.\nAncient Theater of Eretria\nThe theater is located close to Eretria settlement. From a structural and morphological point of view it has many similarities with the theater of Dionysus in Athens. According to descriptions of Ministry of Culture “It is one of the most known ancient monuments of its kind. According to evidence from architectural ruins of the stage it must have been built during the 5th century when the city was reconstructed after the Persian destruction. Its flourishing period was in the 4th century B.C.”.\nSteni Forest in Evia\nThe Aesthetic Forest of Stenis lies on the southern slope of Mount Dirfys. This mountainous area is characterized by varied slopes, ranging from small to steep ones. The soil’s native rocks are comprised of crystalline schist, shale, and individual outcrops of limestone of the Permian, Upper Triadic and Jurassic Period. The existence of numerous springs, small streams and gorges crossing the forest area is also noteworthy. The main tree species are the Pinus halepensis, Castanea sativa, Abies cephalonica (at higher altitudes), Quercus sp., and Platanus orientalis.\nThe area attracts particular aesthetic and ecological interest due to its combination of floral diversity, geomorpholoy and hydrological features. In this area there are several representative types of natural habitat which host numerous endemic species either at a local or national level. Emphasis should also be placed on the presence of endemic species of vertebrates and invertebrates which are endangered although their populations have not been yet accurately established. Several of these species have limited proliferation.\nEvia island has several interesting caves. Some of the most famous are:\nKaristos: AGHIA TRIADA CAVE where scientists found remains of Neolithic settlement.\nAedipsos-Loutra Aedipsou: SILLAS CAVE located close to Aghii Anargiri springs where Sillas tried to heal his arthritic ""gout"".\nLichada-Aghios Georgios THE CAVE located in “Pasogourni” area on Profitis Ilias hill and which has not been explored yet.']"	['<urn:uuid:e7048198-f544-4787-a6b5-898d2defbc5f>', '<urn:uuid:7e28d8d4-2d8d-4ab0-8fa4-176fb302233e>']	open-ended	direct	concise-and-natural	distant-from-document	three-doc	expert	2025-05-13T01:23:33.086345	10	127	3505
80	customer grouping methods supply chain benefits	Several advanced methods exist for customer grouping: Cluster Analysis (K-Means) identifies groups based on similar characteristics, CHAID creates decision trees based on significant variables, and neural networks recognize complex patterns in customer data. These segmentation approaches help businesses navigate supply chain disruptions by enabling them to identify geographical service complexities, evaluate freight policies, assess cost-to-serve variations by channel, and optimize pricing for different customer groups. The segmentation should be validated before implementation and can be used to develop targeted promotional offers based on segment characteristics.	['The Importance of Retail Customer Segmentation\nEven in a digital age when consumers can be targeted at an individual level, customer segmentation strategies have weathered the test of time. Need proof? Think millennials. This generation-based segment is just about as modern as it gets, and it’s the most talked about segment in marketing since baby boomers entered the scene. The point is that segmentation is still a valid technique to use for improving marketing ROI.\nOf course, there are more sophisticated ways to generate market segments than simply grouping consumers by generation. Need more proof? Consider this excerpt from an article entitled “New Criteria for Market Segmentation” by Daniel Yankelovich that appeared in the Harvard Business Review circa 1964:\n“The director of marketing in a large company is confronted by some of the most difficult problems in the history of U.S. industry. To assist him, the information revolution of the past decade puts at his disposal a vast array of techniques, facts, and figures. But without a way to master this information, he can easily be overwhelmed by the reports that flow in to him incessantly from marketing research, economic forecasts, cost analyses, and sales breakdowns. He must have more than mere access to mountains of data. He must himself bring to bear a method of analysis that cuts through the detail to focus sharply on new opportunities.”\nSound familiar even today? That’s because Yankelovich was describing a method that today we call segmentation analysis.\nThe Segmentation Schema\nWhen discussing advanced customer segmentation techniques, what comes to mind? Certainly there are “mountains of data” to consider and aggregate (we’ll get to that later). But first and foremost, it’s the overall marketing objective that guides the segmentation schema and technique to implement. The word schema is defined as “a structured framework or plan.” Think of the segmentation schema as the approach to be utilized. A business objective could be as follows:\nOBJECTIVE : Partition existing customers into finer segments for development of more relevant offers and promotions.\nAs a side note, it’s equally important to be sure there is buy-in from project stakeholders. The purpose and success factors driving the segmentation schema must be clearly articulated to those within the business who have a vested interest in the project. These stakeholders need to reach agreement regarding the approach and business outcomes for the segments. Incorporating their feedback will be crucial to gathering final buy-in.\nScientific and Human Elements of Retail Customer Segmentation\nThe technical building blocks that go into the schema represent the scientific element. This is the period when the analyst encounters mounting issues regarding data to access, filters to use (scrubbing and screening the data), measurements to produce and other specifications required for preparing the data for analysis.\nThe implementation of a final solution and strategy represents the human element. This is where the right brain kicks into action. It’s the period when the marketer engages in ways to humanize the segments from a more creative perspective. This includes asking:\n- What do these segments look like?\n- How should we communicate with these segments?\n- What might make these segments unique from a demographic, behavioral, and intellectual standpoint?\nThere will also be some crossover between the scientific and human elements that takes place as the schema is fleshed out. This is the period when the analyst and marketer are linked together, each considering the interests of the other as the project progresses. When the left hand (analyst) doesn’t know what the right hand (marketer) is doing, then there is potential for a dysfunctional outcome.\nData for Retail Customer Segmentation\nEarlier we referred to the “mountains of data” available for segmentation. In reality, it’s more like an avalanche of data with slabs of characteristics that describe customers/prospects with respect to:\n- Who they are (demographics)\n- What they do (lifestyle/activities/interests)\n- How they act (behavioral)\n- What they buy (purchase preferences)\n- What they value (attitudinal)\n- … and so on\nAt the onset of a segmentation plan, it’s a good idea to conduct an inventory of available data. Case in point, many years ago I would introduce segmentation concepts with an example designed to classify people into different types of fish. Using a simple questionnaire consisting of five questions with two possible outcomes for each question (for example, are you a day person or a night person?), there were 32 segments that could be created. Compare this to all the data available today and the chaos it can inflict when trying to translate the information into a manageable number of segments.\nAdvanced Customer Segmentation Solutions\nNow, let’s consider some of the more advanced schemas and solutions that retailers might leverage.\nCluster Analysis (K-Means)\nThe main idea of cluster analysis is to identify groupings of objects (customers) that are similar with respect to a collection of characteristics and are as dissimilar as possible from an adjacent grouping of objects. The number of clusters is dependent upon the type of algorithm used to identify the clusters. The characteristics can be a set of data elements that describe each customer.\nWithin a cluster analysis, one widely used methodology for datamining is K-Means Clustering, where K represents the number of clusters to be created. Each cluster is centered around a particular point called a centroid. Think of the clusters as planets in multi-dimensional space, with the number of dimensions equivalent to the number of data elements and where each object has a center point. The center point is made up from the average values of the data elements making up the clusters.\nIn the end, the K-Means solution might yield five manageable segments. A customer is assigned to a segment by calculating a distance measurement to each of the centroids. The score is commonly calculated from the Euclidean distance. The centroid with the closest distance becomes the customer’s home segment.\nK-Means Clustering Analysis Diagram\nCHAID (chi-square automatic interaction detection) or Tree Analysis starts with partitioning each object (customer) into one of two outcomes (this is the dependent variable). Examples include response to a marketing promotion (0=No, 1=Yes), purchasing a specific product or making a repeat purchase. This is the starting point for the tree.\nOther characteristics (the independent variables) are used to further partition customers. Each partition creates a new branch of the tree. For example, customers who responded to a marketing promotion might be significantly more likely to fall within a specific age range (say, age 24 to 30) compared to customers who did not respond.\nFurther analysis might show that customers age 24 to 30, living in urban markets, who do not have any children living at home, are even more likely to respond. This creates a new branch of the tree (perhaps called “Childless Metro Millennials”). The branching process continues until there are no more statistically significant partitions that can be made. Each ending branch (called a node) represents a customer segment. The number of final nodes depends on how well the independent variables are able to differentiate the behavior under study.\nAs an example, below is a tree showing survival of passengers on the Titanic (“sibsp” is the number of spouses or siblings aboard). The figures under the nodes show the probability of survival and the percentage of observations in the branch. The tree analysis shows that your chances of survival were good if you were part of segment (1) a female or segment (2) a male older than 9.5 years with three or more siblings.\nTree Analysis: Titanic Survivors Example\nThe concept of neural networks for segmentation is derived from an imitation of neural networks in the human brain. In short, it’s a sophisticated version of connect-the-dots, where the algorithm deduces certain outcomes based on multiple layers of input data.\nNeural Network Example\nA neural network must be trained by being presented with numerous examples of the behavior to be understood. Much like the human brain considers a multitude of inputs to come to a conclusion or make a decision, the neural network algorithm connects all the available data in order to recognize patterns in the information that can be most associated with an expected outcome.\nSome applications of neural networks include:\n- Creating buyer segments from the total population of consumers\n- Developing customized content for specific segments\n- Making predictions regarding certain customer preferences/behaviors\nNo matter what schema is used, the resulting segments should be validated before being implemented into any type of marketing campaign. Validation is a reality check to make sure the segments hold up in a real-world setting. Some schemas allow for the use of a “training” dataset, used for developing the segmentation model, and a “validation” or “test” dataset, used for testing the overall accuracy of the solution. The training and validation datasets are mirror images of each other with respect to the input data and the output behaviors. This ensures the statistical relevancy of the end solution.\nImplementing Retail Customer Segmentation\nNow that the solution has been validated and is ready for action, it’s time to set parameters for implementing the segments into your marketing strategy. Let’s go back to the sample objective we stated earlier: to partition existing customers into finer segments for development of more relevant offers and promotions.\nSpecific promotional offers can now be targeted based on the composition of each segment. For example, offers developed for “Childless Metro Millennials” should emphasize particular product features that can be differentiated compared to offers for other segments. The impact of a marketing plan that incorporates the segments should be tested against a more generic strategy. This allows the ROI to be quantified against the cost of building the segments.\nAway We Go\nNo segmentation solution is perfect and, of course, there is a limit to the amount of resources, time and money to be allocated to a segmentation strategy. But hidden inside and beneath our customer data is the probable answer to some of the most complex problems that a marketer needs to solve. We have at our disposal many alternatives to “master the information” and move in a direction that “cuts through the detail,” as Daniel Yankelovich first suggested. Segmentation analysis, in its many forms, is a track to continually keep in mind for more focused and relevant target marketing.\nAs Principal Analyst for IntelliStats Analytics Solutions, Bill Schneider leads internal teams and works with external business partners, including CCG, in the development of advanced analytical solutions that utilize transaction level data analysis, predictive modeling, consumer segmentation and other emerging marketing science techniques. He has worked with clients across a wide spectrum of industries including retail, financial services, sports and recreation, telecommunications, hospitality and non-profit organizations.\nLet our experienced retail marketing group help you leverage your data to provide quantifiable, actionable results. Our data and analytics services include customer segmentation and cluster analysis, qualitative and quantitative research, modeling and marketing performance analysis. Click below to schedule a free consultation with one of our retail marketing experts or call 800.525.0313 today.', 'Whether it’s freight constraints, labor availability, supply shortages, inventory challenges, trade laws, or any other supply chain disruptions, navigating an ever-changing market can put businesses at risk of both losing customers and eroding profitability.\nTo stay profitable during supply chain disruptions, it’s more important than ever to ensure you understand your customer needs – and the costs to serve that customer, product, or service. One pricing strategy that can help improve profitability is customer segmentation – that is, the evaluation of unique aspects of your customers that place them in peer groups. By doing this, businesses can manage and optimize pricing actions, policies, programs, and other operational standards across similar-behaving customers.\nDetermine Optimal Customer Segmentation Relevant to Supply Chain Disruptions:\n- Are there certain geographical areas that are more complex, expensive, or slower to service?\n- Where are your highest-value customers located?\n- Do your freight policies put you in an advantage or disadvantage?\n- Does the cost to serve your customers vary by channel, market, or customer type?\n- Do certain customer rebate or incentive programs have thresholds that are no longer profitable?\n- Are smaller customers receiving better programs than larger customers because of the channel they are buying through?\n- Are certain end-markets seeing higher demand, greatly impacting your ability to stock and service at the same levels?\n- Have you seen a change in profit margin for historically stable customers?\n- Have you seen an uptick or decline in new customers due to changing market demand?\n- How has this impacted your wallet share? Your ability to quote new business?\n- Have you seen changes in lead time affect customer relationships or contracts?\n- Are certain customers willing to pay any price just to purchase your product or service?\nEach of these items can be tied back to a cost-to-serve element that affects profitability. For example, you may consider adjusting your freight policy for customers with lower margin that need product shipped to harder-to-get-to regions. While that may sound simple, and perhaps obvious, our real-world customers are usually in a more complex scenario. Perhaps existing contracts restrict certain price or program adjustments, or your large, multi-regional customer won’t accept regional-based price policies. It might be that your lowest margin customers are also the largest % of your revenue, or that your warehouse is full of product that those customers don’t care about because unpredictable shipments left you overstocked – leaving no room for the product they do care about. These scenarios illustrate why customer segmentation can help optimize pricing, especially when supply chain disruptions occur.\nIdentify Your Price Drivers (e.g. Customer Traits Key to Your Business Success):\n- High revenue\n- Strategic partners\n- Long tenure\n- Growth potential\n- Frequent buyers / high volume\n- High margin\n- Low price sensitivity or high difficulty to switch suppliers\n- Within a stable or growing industry\nBy identifying and prioritizing the ideal customer profile and behavior, you can begin to segment customers based on these price drivers. Ideally, these customer drivers are aligned to business objectives and also work in tandem with your product and order price drivers (or segmentations). Illustrated below are examples of key price drivers for optimal customer segmentation:\nThen, as shown in the table below, this customer segmentation provides clearer pricing guidance when determining how to optimize price changes when supply chain disruptions occur.\nIf you’re getting started with customer segmentation, make sure you have the right data that can be used to generate actionable knowledge. And if you already have your customer segmentation in place, improve your ROI with micro-segmentation of customer, product, and order factors that work together to generate the ideal target price.\nWhen supply chain disruptions hit, there is no one-size-fits-all answer. Knowing who your best customers are and how to take care of them is key to maintaining – and growing – relationships and ultimately improving profitability and driving impact to your bottom line.']	['<urn:uuid:f52acab4-8062-43e3-b60d-7390ae158854>', '<urn:uuid:d88c3c52-23e7-44a5-bdac-21847a0d8aab>']	open-ended	direct	short-search-query	distant-from-document	multi-aspect	expert	2025-05-13T01:23:33.086345	6	85	2488
81	Did any female pilot manage to win a major competition that was open to both men and women back in the early days of aviation?	Yes, Louise Thaden achieved a breakthrough by winning a major open-to-all endurance race	['“Fly Girls” by Keith O’Brien (Eamon Dolan/HMH; 352 pages, $28)\nYou don’t have to be a devoted reader of the history of women’s progress in the 20th century, or of the early days of aviation, to be frequently swept up in the tales laid out here by veteran journalist Keith O’Brien. He has five main subjects here — women who took to the air in the ‘20s and ‘30s. They faced all kinds of challenges — for example, finding sponsors and crews to help them get the latest (which often meant safest) planes. Also, they faced sexism and tokenism that could come out from family or society, while they worked to maintain their integrity as explorers. That is, professionals at putting nuts-and-bolts in place to serve an insatiable wanderlust.\nAmelia Earhart is already an historical figure, with complete books on her triumphs and enigmatic disappearance. Here, she’s just one among several at the forefront. Like the others, she wins and loses races, and she sets records that are then broken, driving her onward and toward more risk. She finds fame and fortune that rise and fall in response to the capricious demands of the curious crowd, plus those of the early aircraft manufacturers who want to get their brands recognized.\nNot long after introductions to Earhart and the other fliers who get primary attention, O’Brien curiously zooms in on another character: an air-race promoter. When the women get their own sideshow version of a race (disparagingly called “The Powder Puff Derby”), and when there are breakthroughs (notably Louise Thaden’s win in a major open-to-all endurance race), the promoter is a stand-in for all the gatekeepers. There are many — most self-appointed; many ready to hinder progress; and some well-meaning but not aligned with the spirit that the Fly Girls nurtured among themselves. But the author really digs into the diverse backgrounds of Earhart, Thaden, Ruth Elder and Ruth Rowland Nichols and the particularly-rambunctious Florence Klingensmith. O’Brien leaves no doubt that each is a full-blooded person up in the air, risking her life while trusting that the wing’s fabric stays in place and the spruce structures don’t snap when facing headwinds at a 180 mph.\nThere’s a bit of potential to compare this book to Tom Wolfe’s classic “The Right Stuff.” Both look at a group that went skyward as pioneers, while peers succumbed to the dangers — all with America’s cultural and media circus playing an important role. But Wolfe wanted the facts to serve as a launchpad for his stylistic artistry, and O’Brien has no such agenda. He may sometimes let a surfeit of facts and subsequent interpretation fill up spaces that might’ve made for an even more compelling read. But it’s thorough research on display, and the author’s enthusiasm for reporting rivals the enthusiasm shown by his daring subjects.\nBesides, O’Brien finds moments where he summons poetic power. (“The two women did not speak of winning the Bendix. They just flew: over the flatlands where Thaden had gotten her start selling coal a decade earlier and the jagged mountains that the men had once warned were too dangerous for female pilots to cross…”) You can see the seams in some places, where he’s stretching in the service of wonderment, but he doesn’t muddy up correlations to suggest dramatic cause-and-effect. By not crossing that line and bringing to light a grand and underreported story, O’Brien’s delivered on the promise of popular history.']	['<urn:uuid:13440886-01e9-492f-b9b3-20b8173a1d22>']	factoid	direct	verbose-and-natural	distant-from-document	single-doc	novice	2025-05-13T01:23:33.086345	25	13	573
82	detector types hplc compound separation	Different types of detectors are used depending on the compound properties. For UV light absorbing compounds, a UV-absorbance detector is used. For compounds that fluoresce, a fluorescence detector is used.	"['The detector is wired to the computer info station, the HPLC process element that data the electrical sign necessary to generate the chromatogram on its Show and also to recognize and quantitate the focus on the sample constituents (see Determine File). Considering that sample compound traits can be very different, many types of detectors are already made. By way of example, if a compound can take up ultraviolet gentle, a UV-absorbance detector is used. If your compound fluoresces, a fluorescence detector is used.\nAlongside one another the variables are variables in a resolution equation, which describes how properly two elements\' peaks separated or overlapped each other. These parameters are mostly only used for describing HPLC reversed period and HPLC typical stage separations, considering that Those people separations tend to be a lot more delicate than other HPLC modes (e.g. ion exchange and measurement exclusion).\nOur selection of large-quality Examine valves and relief valves are available in brass or stainless-steel and a variety of relationship dimensions for the link of cylinders to devices.\nA detector is required to see the divided compound bands because they elute in the HPLC column [most compounds have no shade, so we can\'t see them with our eyes]. The cell stage exits the detector and will be sent to waste, or collected, as wished-for. When the cell stage contains a separated compound band, HPLC presents a chance to acquire this portion of your eluate containing that purified compound for additional analyze. This is named preparative chromatography [discussed while in the section on HPLC Scale].\nSize-exclusion chromatography (SEC), also referred to as gel permeation chromatography or gel filtration chromatography, separates particles on the basis of molecular dimensions (basically by a particle\'s Stokes radius). It is normally a very low resolution chromatography and therefore it is usually reserved for the ultimate, ""sprucing"" phase on the purification. It is additionally helpful for identifying the tertiary construction and quaternary structure of purified proteins.\nIn UPLC, or ultra-superior overall performance liquid chromatography, column particle dimension of under 2um may be used. This enables for better separation than the typical particle size of 5um which can be used in HPLC.\nIn Determine H, the yellow band has entirely handed through the detector flow cell; the electrical sign created has become sent to the computer information station. The resulting chromatogram has begun to seem on screen. Be aware that the chromatogram begins when the sample was 1st injected and starts off as a straight line set in close proximity to the bottom on the display. This can be called the baseline; it signifies pure cellular phase passing in the stream mobile as time passes.\nSCIEX forensic analysis answers provide fast, really precise data across a large number of compounds and biomarkers, through the regarded to the new and novel.\nAdvance your investigation with entrance-conclude devices created that will help you notice the complete power of your respective mass spectrometer. SCIEX has the broadest portfolio of ESI-MS entrance-ends which will aid many flow charges, sample necessities and sensitivities.\nBy reducing the pH with the solvent inside of a cation Trade column, For example, far more hydrogen ions are offered to contend for positions on the anionic stationary phase, thereby eluting weakly certain cations.\nMade with expandability and compatibility in mind, the Nexera XR ultra large effectiveness liquid chromatograph enables a lot more buyers to utilize high-speed, higher-resolution systems.\nEvery single vMethod supplies strategy problems, advisable sample prep, LC and MS problems, and information for applicable MS/MS library databases for important purposes.\nThe essential basic principle of displacement chromatography is: A molecule with a higher affinity with the chromatography matrix (the displacer) will compete proficiently for binding web pages, and so displace all molecules with lesser affinities.[eleven] You will discover distinctive discrepancies involving displacement and elution chromatography. In elution method, substances generally emerge from a column in narrow, Gaussian peaks. Broad separation of peaks, preferably to baseline, is desired in order to reach utmost purification. The pace at which any component of a combination travels down the column in elution manner is dependent upon several elements. But for 2 substances to journey at distinct speeds, and thereby be here fixed, there needs to be considerable distinctions in certain conversation in between the biomolecules and the chromatography matrix.\nis usually a xanthine alkaloid (psychoactive stimulant). Caffeine has some legitimatemedical makes use of in athletic schooling and inside the reduction of rigidity-sort complications. It is a drug that isnaturally made during the leaves and seeds of many plants. It’s also developed artificially andadded to sure foods. Caffeine is defined being a drug mainly because it stimulates the central nervoussystem, producing improved alertness.']"	['<urn:uuid:a1d2910e-63d5-466c-b5d0-9247238cf6b6>']	factoid	with-premise	short-search-query	similar-to-document	single-doc	expert	2025-05-13T01:23:33.086345	5	30	775
83	history dribble basketball invention	The dribble was invented in New Britain in 1895 to slow down the game and help prevent injuries from tackling.	"[""YMCA New Britain - Berlin100% Local\n50 High Street\nNew Britain , CT 06051 (view map)\n50 High Street, New Britain, CT 06051\n365 Main Street, Kensington, CT 06037\nThe YMCA Mission is to put Judeo-Christian principles into practice through programs that build healthy spirit, mind and body for all.\nThe Young Men’s Christian Association of New Britain-Berlin is an integral part of a Worldwide movement committed to love and serve all people. Our purpose is to enable members and constituents to develop values and behavior consistent with Judeo-Christian ideals by:\n- Providing cultural, intellectual, physical, spiritual and social programs and services;\n- Serving as advocates for human rights, social justice and world peace;\n- Enriching family and other human relationships; and\n- Cooperating with other organizations and institutions having similar ideals.\n- Our YMCA encourages the participation of all people regardless of religion, age, sex, race, nationality and handicap.\nThe YMCA is a place where you belong!\nWe have programs and activities for everyone - all ages and all abilities!\nAs America’s largest human service organization the YMCA offers an immense variety of programs, everything from parent/tot swimming to senior fitness classes.\nPlease take a few minutes to see what we have to offer and if you don’t see something that you are interested in contact us. The YMCA is constantly changing and offering new programs and opportunities.\nAquatics: Swim Lessons\nParents select the YMCA as their child's swim teacher because the Y has a reputation for offering programs that are safe, educational and fun. All classes, contain components related to safety, personal growth, stroke development, water games, sports and rescue. A student centered approach is used towards learning and having fun.\nHealth and Wellness\nAlmost all health and wellness programs are included in the price of your YMCA membership. Please refer to our latest program brochure or call the YMCA for schedules.\nSenior Fit: our best class for strengthening your heart and burning calories. Heart rates and blood pressure are monitored as participants work out on bikes, treadmills and ellipticals.\nSenior Strength Training: Designed for anyone 55 years and over. For those that have never used strength training equipment or just need a little extra attention.\nKarate: come and try a different way to condition your spirit, mind and body. Beginners to advanced levels welcome. Basic self-defense techniques and karate skills along with confidence building.\nAdult Basketball League: The tradition of high quality men’s basketball at the Y lives on. Get together a roster of players and join the competition. All team members must be 18 years or older and have shirts with numbers! Leagues are generally held Tuesdays and Thursdays, each fall and spring.\nCo-Ed Volleyball League: Bump! Set! Spike! The State’s premier co-ed volleyball league. The league is held on Monday evenings.\nFree YMCA Personal Training: Members are encouraged to take advantage of free Personal Training with one of our YMCA certified Personal Trainers. Our Personal Training program is designed to encourage, motivate and instruct individuals in the proper use of all fitness equipment. Our Certified Personal Trainers design specific programs depending on the person’s fitness goals and objectives. We provide personalized attention, guidance and coaching from caring, knowledgeable and well trained staff. This personal approach is designed to decrease barriers and intimidation, and increase motivation for healthy lifestyle change. The 1 hour sessions are free for members and monthly follow-up sessions are encouraged.\nRock Climbing: The YMCA’s American Savings Foundation Rock Climbing Center provides members and others with a state of the art, three wall, 20 foot high technical rock climbing experience. All participants go through an orientation class or show proficiency with work climbing equipment and procedures including: harnessing, belay techniques, knot tying and basic climbing skills. (See Climbing Center schedule underSchedules.)\nThe Y offers a wide variety of classes and individual climbing times, including: Orientation, Adventure Climbers ( grades 1-4), Cliff Climbers (grades 5-8), teen, adult and family climbing.\nBirthday Party Climbing: The Y offers climbing wall birthday parties under our expert staff supervision. (Minimum age 6, maximum number of climbers 10.) Call for additional information.\nMassage Therapy: Enjoy this invigorating 25 minute massage. It’s a great way to relax your spirit, mind and body after a hard day at work! Tuesday and Thursday appointments are available.\n10 Minute Sport Massage: Have you reached a plateu in your exercise routine and don’t know how to progress to the next level? Try a 10 minute massage to give your muscles the boost they need! $10.00 for 10 minutes.\nRockin’ Abs: This class will get you going! Highly energetic instructor will lead participants in an abundance of abdominal exercises. Target your body’s core stabilizing muscles: upper abs, lower abs, and the obliques. A mix of crunches and core exercises using body bars and fitness balls will be safely shown. All fitness levels.\nMiddle School Strength Training: Designed for youths ages 11-13. An introduction to the proper use of cardiovascular and strength training equipments as well as proper etiquette. Supervised by a trained fitness instructor, individuals progress at their own pace. Upon successful graduation from the program, youth will receive a special card that allows them use of the fitness center when accompanied by a parent.\nActivate America: The New Britain - Berlin YMCA has joined a national initiative that is rallying YMCA’s across the country to further enhance their service and support to kids, adults, and families who want to lead a healthy lifestyle, but struggle to do so. As part of our commitment to YMCA Activate America, the New Britain - Berlin YMCA will offer several programs that will encourage children and families to adopt behaviors that support healthy lifestyles.\nAquacize: this popular class will improve your energy and fitness level with no impact on your joints. A fun environment where no swimming skills are required.\nAdult Swim Lessons: whether you are just beginning or looking to improve your lap swimming, we can help!\nWaterwalking: this exercise program is gentle on your joints while helping you get in shape. Flotation belts are provided and no swimming skills are required.\nArthritis: a series of gentle exercises in warm water. Participants gain increased motion in stiff, sore joints while finding relief from pain. Held in shallow water.\nSchool Age Child Care: Our licensed school age child care program, called On Track, is available before and after school for children attending schools in New Britain. The children are picked up and dropped off by the school system bus. This program follows the New Britain School District calendar. Enrollment is limited to this fun-filled program which provides homework assistance, tutoring, recreation and arts activities. Participants are eligible for snow day care. Contact the Child Care Director\nVacation Fun Days: This program includes a wide variety of social and physical development activities during the school year when school is not in session. Contact the Child Care Director for specific dates.\nCamp Hurricane: A state-licensed day camp that runs for 9 weeks of the summer for children ages 4-15. Participants receive a free breakfast and lunch. Contact the Program Director for more information.\nYMCA/BRACC Youth Basketball League: This co-ed basketball league is operated in cooperation with the New Britain Area Churches. Each Saturday from December - March, a fun basketball league for boys and girls ages 10 years and up is held. Any child interested should contact the Program Director.\nY.S.U.P. Program: Youth Succeeding Under Pressure - For teens in grades 6 - 8 this program emphasizes community pride and leadership skills. Teens actively participate in field trips, dances, group activities, mentoring, tutoring and college tours.\nMeeting times at local school: Monday - Friday school dismissal to 5:00 p.m.\nMeeting times at the YMCA: Monday - Thursday from 6-8:30 p.m.\nContact the Teen Director for more information.\nYouth and Government: This program, which is new to New Britain, is for youth in grades 9 - 12. This program starts in the fall and finishes in the spring with a weekend at the Hartford Capitol. Leading up to this weekend the students learn about the legislative process, government, social issues and how bills become laws. Contact the Program Director to register.\nTeen Dances: Friday nights from 6:30 - 9:30 p.m. Call the YMCA for specific dates.\nContact the Teen Director for more information.\nPrime Y'ers (55+): A YMCA group for active older adults. This program provides personal growth and enrichment through cultural, educational and social activities. Meetings are held the second Friday of each month beginning at 1:30 pm. The group also plans trips and other activities which are held away from the Y.\nGroup Exercise Classes: The YMCA offers a wide variety of health and fitness programs which are appropriate for active older adults or those wishing to become more active. Click below for more information.\nResidence Rooms: The New Britain YMCA is proud of its 125-year tradition of providing affordable housing for men. Eighty-seven clean, comfortable rooms are available for rent on a weekly or monthly basis. Members in Residence have full privilege use of the facilities and housekeeping service is provided. Applications are available at the Member Service Desk.\nHistory: The New Britain YMCA was founded in 1883 to protect young men and boys from the sins and temptations of a rapidly industrializing urban America. Today, it cuts across social, racial, economic, religious and gender lines because it was founded to serve basic human needs.\nSome interesting facts about the YMCA of New Britain and Berlin:\n- In 1895, 3 years after the invention of basketball at the Springfield YMCA Training School, the New Britain YMCA Basketball Team was the World Champions.\n- Also, in 1895 the “dribble” was invented in New Britain, in order to slow the game down and to help prevent injuries from tackling.\n- The old YMCA building was located on the corner of Court and Main Streets. Dedicated in 1899, the building was nearly destroyed by fire in 1906. It was rebuilt in 1907.\n- The ‘new’ YMCA building was dedicated on May 11, 1954, and people threw coins into the new pool.\n- In 1954 the game of racquetball was born at the New Britain YMCA, after having been conceived by Joe Sobek of the Greenwich, CT YMCA. The New Britain YMCA can boast four past world champions of racquetball.\n- The Berlin-Kensington Branch was created in the late sixties and continues to offer high quality child care, youth and family programs.\n- The New Britain-Berlin YMCA has had four successful capital campaigns in the last 20 years. The Y has raised more than $5 million during that period for its charitable works.\n- The New Britain Y has had only ten CEO’s in 125 years.\n- The Y is guided by four core values: caring, honesty, respect, responsibility.\n- We believe that to bring about meaningful change in individuals and communities, we must be focused and accountable. At the Y, we measure the success of our cause by how well we engage communities in our three areas of focus.""]"	['<urn:uuid:c0d650c4-c841-4298-921a-babb2d464fa5>']	factoid	direct	short-search-query	distant-from-document	single-doc	expert	2025-05-13T01:23:33.086345	4	20	1838
84	best time visit tanzania serengeti wildebeest migration and dust problems while wildlife viewing	The best time to visit is from June to September and December through February, with optimal wildlife viewing and minimal rain. During the wildebeest migration, from mid-December through May, the herds are in the southern Serengeti. However, be aware that the Serengeti Plains are prone to dusty conditions, especially during the dry season, due to shallow soil base and lack of long grass roots. It's important to protect photography equipment from dust and bring photo soft cloths for lens cleaning. Contact lens wearers should bring ample lens lubricant.	['Where is the best place to go on safari?\nSafari in Kiswahili, the language of East Africa, simply means a journey. Today it is synonymous in English with a wildlife viewing adventure in the African Bush. If your primary reason for traveling to Africa is to experience an abundance of African wildlife in unspoiled wilderness, then Tanzania should be your destination of choice. Tanzania protects over 25% of its land through national parks and reserves, more than any other country on the continent. You simply cannot beat the wildlife concentrations found in Tanzania. The parks and wildlife reserves of Tanzania are inhabited by vast herds of wildebeest spread out across the Serengeti savanna, huge populations of elephant and buffalo, as well as plains game and their predators. All these animals interact and roam freely, the same as they have for thousands of years. Here you’ll witness an incredible diversity of ecology and will find the vegetation and bird life as fascinating as the big game. This is the home to 90% of the film series produced on African animals. Tanzania also boasts a remarkable number of World Heritage Sites including, Serengeti National Park, Kilimanjaro National Park, Ngorongoro Conservation Area, Selous Game Reserve, (this reserve alone is the size of Denmark), Kilwa Kisiwani and the Songo Mnara Ruins and Mafia Marine park.\nWhere is the best place to go for wildlife viewing in Tanzania?\nYou simply can’t beat northern Tanzania for wildlife concentrations, although this is were most people tend to travel. Most people have heard of the spectacular Ngorongoro Crater and the wide array of wildlife dwelling on the crater floor. Most have heard of the vast Serengeti savanna, which hosts the annual Migration of Wildebeest and the predators that follow in its wake. This is only the beginning of the natural phenomena awaiting your discovery on a safari to Tanzania.\nWhat kinds of animals will I see on safari in Tanzania?\nTanzania is home to over 35 species of large four-legged mammals and has over 1000 species of birds listed. On a typical safari in northern Tanzania you can expect to see elephant, buffalo, giraffe, hippo, baboon, monkeys and a variety of plains game such as wildebeest, hartebeest, zebra, impala and gazelle. Most people see lion and hyena, and possibly cheetah or leopard. In Ngorongoro Crater you may see one of the few remaining black rhino to be found in Tanzania. You’ll undoubtedly see several different species of mongoose and some hyrax and other small mammals. If you’re lucky you’ll see one or more of the smaller cats, foxes, wild dog or the more reticent antelope like lesser kudu, bushbuck, oryx or eland.\nWhat is the migration?\nEvery year, over one million wildebeest move through the Serengeti plains in search of food and water. The phenomenon of these animals moving en mass through the African savanna is known as the Migration. The seasonal rains that water the grazing pastures drive their movement. It is impossible to predict in advance exactly how or when this progression will take place, but there is a pattern. Generally from mid-December through May the herds are feeding in the southern Serengeti. During February thousands of calves appear on the plains. Between June and July, the wildebeest begin their annual migration north reaching the Mara River that marks the Kenyan border sometime between the end of July and beginning of August. After the first short rains, usually in the beginning of November, the herds move back into Tanzania’s Serengeti and make their way to the southern pastures where they rest and feed through the rains until their search for better grazing leads them to begin their annual migration once again. Even when the “migration” moves into Kenya for the summer months, there are many resident herds in the Serengeti and there is always an incredible array of wildlife to experience there. Also, in the summer months, which are the height of the dry season, thousands of elephant congregate around the Tarangire River. This park is at it’s prime during these months.\nWhat’s the weather like in Tanzania?\nLocated at an altitude of 5,000 to 7,600 feet, northern Tanzania’s dry sunny climate is nothing like the steamy African jungle of Tarzan movies. The weather is spring-like year round, with daytime temperatures in the 70s and 80s, evenings in the 60s. From June until August, temperatures are slightly cooler, ranging from the 50s to the mid-70s. The coastal and lowland areas tend to be more tropical in temperature.\nWhat clothes should I bring on safari?\nSafari dress is comfortable and casual – layers are recommended. Keep it simple and bring things you don’t mind getting dusty. After you have booked your safari, we send you a pre-departure information through emails to help you prepare for your safari.\nAre safaris in Tanzania safe?\nToday’s modern safari is a far cry from the rugged safaris of the past. After a stimulating day of game viewing, you can relax at comfortable, attractive lodges, with amenities like swimming pools, full service restaurants and en-suite bathrooms. Located right in the scenic settings of the wildlife reserves, the lodges are close to natureâ€¦ but not too close for comfort. Tanzania is one of Africa’s most stable countries, where you’ll receive a warm welcome from its gracious people. Every care is taken to ensure your health and safety on safari.\nHow far in advance should I book my safari?\nIt is better to book as far in advance as possible to ensure availability at the time you wish to travel, especially during the peak seasons (July & August and Christmas time). This is especially important for those wishing to travel on private custom safaris and those adding extensions to the scheduled trips.\nWhat is the best time of the year to go on safari?\nIn general the month from June to September and from December through February tend to be the ideal months, with the least amount of rain and most amount of animals.', 'The fear of bugs and insects is generally much greater than the reality of what you will encounter. However, tolerances differ widely from individual to individual. The temperate climate and high elevation of Tanzania’s Northern Parks mean that insect concentrations are significantly less then other areas of Africa. Please be aware though that insects can be present in significant numbers depending upon your location and current weather patterns. This could pose to be an annoyance for some individuals.\nMosquitoes are present but they are generally not active during the day. The African Mosquito is most active from dusk to dawn. Flies can be more of a nuisance, especially when you’re near the wildebeest migration. Flies are attracted to animals and the droppings of herd animals, so you don’t get one without the other. You will undoubtedly know when you have found the larger migratory wildebeest herds (100,000 plus!)\nTsetse flies are worse than the average fly and they are mainly found in the woodlands, and their bite does hurt. There is no insect repellent that is effective against the tsetse fly. The best protection is to wear long sleeves, pants and socks and to roll the windows up when you are driving through a tsetse fly infested area. Additionally, dark blue and black colors attract tsetse flies and it is recommended not to wear these colors when game driving in tsetse areas. Tsetse flies require the thick bush and woodlands to breed and survive. The open plains of the Southern and Eastern Serengeti as well as the Ngorongoro Crater and the southern parts of the Central Serengeti are tsetse free. The highest concentrations of tsetse flies are found in Tarangire National Park and the Western Serengeti.\nPlease keep in mind that if it weren’t for the tsetse fly, many of the parks and reserves in Tanzania would simply not exist in their current capacity. The tsetse fly is commonly referred to as the ‘greatest conservationist in Africa’! The tsetse fly transmits a blood parasite that causes the ‘sleeping sickness’ in cattle but is very rarely transmitted to humans in East Africa. Wild animals are immune to this disease. The tsetse fly has inadvertently forced ranchers and their cattle out of areas like the Serengeti and Tarangire leaving these important refuges ecologically intact for use by their native and wild inhabitants.\nMost people have no adverse reactions to tsetse flies but some individuals have an allergic reaction and the area around the bite mark swells and becomes itchy and irritated. Benadryl makes a product called the ‘Benadryl Itch Relief Stick’. This handy little stick can relieve the itch of bites and is highly recommended.\nSnakes are common throughout Africa but they are seldom encountered on safaris. There are a few python sightings reported in the trees that line Silale Swamp in Tarangire but that is the normal extent of snake sightings. The vast majority of tourists never see a snake while on safari.\nPlease be aware that game driving can be very bumpy and may pose a problem for some individuals including those with back problems. Please inform us well in advance if you have any conditions that may be adversely affected by bumpy roads and we will plan accordingly. The most comfortable seat is the passenger seat at the front of the vehicle next to the driver-guide. This seat offers the smoothest ride and is highly recommended (especially on longer game drives) for those individuals experiencing discomfort due to poor road conditions.\nThe roads to and from Arusha/Kilimajaro leading up to Tarangire, Lake Manyara and the Ngorongoro Gate were completed in December 2004 and are completely paved. However, the roads and tracks in the national parks and conservations areas are not paved. Many game drives (especially in the Serengeti and Ngorongoro Conservation Area) will be entirely off road/cross country. Game driving off road and on poor tracks, which are found in most areas, can be aggravating and exhausting for some individuals. This is compounded on longer game drives where you may be on rough roads for several hours at a time.\nGame Drives and Transit Time\nPlease be aware that the majority of time on safari is spent in a vehicle game driving and wildlife viewing. Please advise us well in advance of any potential problems with long duration game drives and we will plan your itinerary accordingly and alert your driver of the situation in order to minimize any negative impact. Our private safaris are completely flexible and if need be we can shorten game drives and transit times, add additional flights and eliminate areas of rough terrain from your itinerary.\nEven in the green season, dusty conditions can be aggravating while out on game drives. The Serengeti Plains are especially prone to dusty conditions due to the shallow soil base and lack of long grass roots. During the dry season, dusty conditions are significantly worse. Please be prepared for dusty conditions and let us know in advance if you’re especially sensitive to dust and we will adjust your itinerary accordingly to help minimize any negative impacts.\nPhotography and video equipment may be especially prone to dusty conditions. It is a good idea to bring a bag that can be easily opened and completely sealed so you may store your equipment when not immediately needed. Bring a couple photo soft cloths to wipe dust from the lenses. Contact lens wearers may be especially sensitive. Please plan accordingly and bring an ample supply of lens lubricant.\nTanzania resides just south of the equator. The strong equatorial sun combined with the fact that you are at elevation can lead to sunburn and sun poisoning quickly. The vehicles have a convertible canvas top, which means that you will be completely exposed when game viewing. You may of course quickly close the canvas top as needed. The canvas top may also be rolled back half way in order to provide shade for the first set of seats and sun for the back seats. It is important that you wear a hat and apply sun block frequently to all exposed areas.\nThe secret is out with Northern Tanzania and most of the general public is now aware that this area offers the very finest wildlife viewing in all of Africa. Tourists from all over the world are flocking to the famous northern safari circuit to enjoy its beautiful scenery and abundant wildlife. Please note that high vehicle concentrations in many high use areas including the Central Serengeti and Ngorongoro Crater may pose an inconvenience. To combat this potential problem, we highly recommend early morning game drives and adventure game drives to some of the more remote areas of the Serengeti, which can be even more rewarding. Please keep in mind that by departing at 6.00am each morning, you will have most areas all to yourself until about 10.00am when the package tours begin. Our expert guides know many secret spots and strategies to get you off the beaten path and avoid vehicle concentrations. The Serengeti, in particular, is a massive park and there are plenty of areas where you will likely encounter not one other vehicle but rather an abundance of wildlife and beautiful pristine topography. Make sure to communicate to your driver-guide your interest in adventure and exploring the more remote areas.']	['<urn:uuid:14851a1a-21b8-471d-b6e7-b4576f9d001d>', '<urn:uuid:2c9b7a75-9821-434e-9132-8dbd11083f35>']	factoid	with-premise	long-search-query	similar-to-document	multi-aspect	novice	2025-05-13T01:23:33.086345	13	88	2219
85	compare fortifications pagan city defenses time anawrahta mongol invasion 1287	Pagan's fortifications underwent significant changes over this period. The city's original defensive wall was built around 850, about 200 years before Anawrahta's time, forming a backwards L-shape that abutted the Irrawaddy River, which formed the other sides of the city's perimeter. The enclosed area was approximately one square mile, though river course changes have reduced this size. These defenses proved insufficient by 1287 when the Mongols under Kublai Khan successfully invaded, leading to the collapse of the first Myanmar Empire and the division of the country into smaller kingdoms.	"['Notes on the Geography of Burma / Myanmar: Pagan 2: More Monuments\nThe builders of Pagan were very conservative, but the biggest change in Pagan\'s architectural style occurred relatively quickly during the reign of Anawrahta\'s grandson, Sithu I. Unlike the horizontal Kyauk-ku and Nan-hpaya, Sithu\'s temples rise toward the heavens, elevating the ""cave"" to the spiritual realm. So ever after did all sizeable temples at Pagan. Later innovations were minimal, surviving even the trauma of the Mongol invasion of 1287.\nShwe-gu-gyi, the ""Great Golden Cave,"" built about 1131 by Sithu I. Unlike Anawrahta\'s temples, this one, like a Gothic cathedral, emphasizes the vertical.\nThat-byin-nyu (the ""Omniscient,"" referring to the Buddha) was also built by Sithu I but follows Shwe-gu-gyi by 20 years. It not only emphasizes the vertical but creates an upper-level shrine reached by stairs.\nIn the foreground is a bit of Pagan\'s remnantal city wall, built about 850, roughly 200 years before Pagan became an imperial power. It forms a backwards L, abutting the Irrawaddy, whose curvature forms the other sides of the city\'s perimeter. The enclosed area is about one square mile, though changes in the river\'s course have made it smaller than it once was.\nDhamma-yan-gyi, a veritable mountain-cave begun in 1165 by Sithu I but never finished. (Why? One explanation is that Sithu was about to rise from his deathbed, which prompted an ambitious son to smother him. The son had no wish to increase his father\'s merit and so ordered ""down tools."") Despite its immense bulk, the temple has only one level, and the inner ambulatory, through which pilgrims might have passed, was immediately blocked with rubble. It remains blocked today, not only by rubble but by modern gates that keep visitors completely outside.\nThe zedi evolved, too. This is Sapada, named for a monk of that name who visited Sri Lanka and returned to Pagan in 1181. He built this Sri Lankan-style pagoda, whose treasure is housed in the box-like tabena above the anda. It\'s one-of-a-kind at Pagan.\nHere, in the Dhamma-yazika, is a uniquely Burman innovation: pentagonal terraces. Completed in 1196 by Sithu II, the pagoda indicates the strength at Pagan of the cult of Mettaya, the Buddha-to-come. (A century earlier, Anawrahta had hoped to see him in a future life.) The pentagonal design is simply explained: Mettaya is the fifth Buddha of this cosmic cycle.\nThe gilding is recent and occurs not only on the main pagada but on the small temples that mark each of the five sides.\nSula-mani, the ""crowning jewel,"" built before 1225 by Sithu II. This is the temple that defines the mature Pagan style, with the elevated shrine of Than-byin-nu and the mass of Dhamma-yan-gyi. Shrine and hall are duplicated on each of the two levels, which are connected by hidden stairs. Want to see the upper level? Sorry: it\'s padlocked.\nThe Sula-mani facade, from pediment to sikhara.\nThe plaster decoration is in good shape, as in this flaming pediment.\nA smaller version of the two-tier gu. This is the South Gu Ni, one of two very similar temples in the Sula-mani style. In these cases, though, the stairs are open, permitting this picture from the top of the adjacent North Gu Ni. In the background, a hot-air balloon carrying tourists and periodically huffing as it drifts past.\nThe north Gu Ni, showing the structure of double-stacked shrines and halls under a convex sikhara.\nThe established form was repeated many times. Here: Tayok-pye, or ""Fleeing from the Chinese,"" in reference to the king\'s behavior when the Mongols arrived in 1287.\nThe decorative motifs are like those at Nan-hpaya, built centuries earlier.\nA close up of Min-o-thi-la, a twin-bodied lion.\nClose by, Paya-thon-zu, ""three temples,"" also from late days. The rooms are dark but remarkable for their paintings.\nBuddha images are arrayed in a honeycomb.\nThe ceiling vault is decorated with hundreds of Buddha images around the lotus pool of creation.\nOn the wall, a bodhisattva.\nA rare post-Mongol innovation: Thisa-wadi was built in 1334, 50 years after the Mongol invasion, and it pushes the Pagan style into a one-of-a-kind three-tiered form.\nThe stairs, traditionally, are hidden to the side.\nThe Buddhas within are modern and rote but faithful to classical iconography: here, in the position of bhumisparsamudra, the Buddha touches the earth with his right hand, a sign of the moment that he attained enlightenment.\nHence the sly expression: ""I know something you don\'t know."" Even a modern production like this follows the canon, which dictates three folds in the skin of the neck, ears of the breadth of eight fingers, and a long, thin upper lip over a short, full lower one.\n* Argentina * Australia * Austria * Bangladesh * Belgium * Botswana * Brazil * Burma / Myanmar * Cambodia (Angkor) * Canada (B.C.) * China * The Czech Republic * Egypt * Fiji * France * Germany * Ghana * Greece * Guyana * Hungary * India: Themes * Northern India * Peninsular India * Indonesia * Israel * Italy * Japan * Jerusalem * Jordan * Kenya * Laos * Kosovo * Malawi * Malaysia * Mauritius * Mexico * Micronesia (Pohnpei) * Morocco * Mozambique * Namibia * The Netherlands * New Zealand * Nigeria * Norway * Oman * Pakistan * Peru * The Philippines * Poland * Portugal * Romania (Transylvania) * Senegal * Singapore * South Africa * South Korea * Spain * Sri Lanka * Sudan * Syria (Aleppo) * Tanzania * Thailand * Trinidad * Turkey (Istanbul) * Uganda * The U.A.E. (Dubai) * The United Kingdom * The Eastern United States * The Western United States * Oklahoma * Uruguay * Uzbekistan * Vietnam * The West Bank * Yemen * Zambia * Zimbabwe *', 'BRIEF HISTORY OF MYANMAR\nA.D.832 Pyu kingdoms were conquered by the State of Nanchao, and the Pyus, merged with the Mons and( the second group of Tibeto‑Burman, the Bamars. (Burmese)849 Founding of Bagan as a walled city with twelve gates and a moat by King Pyinbya.1044 King Anawratha (1044 ‑ 1077),(အေနာ္ရထာမင္း) the 42nd king of Bagan Dynasty(႔ပုဂံေခတ္) according to the list in ancient Myanmar chronicles, acceded to the throne of Bagan and unified Myanmar into a first distinct political entity in the history of Myanmar, which is usually referred to as ""the first Myanm႔႔ar Empire"". Bagan was its capital. Anawratha introduced Theravada Buddhism (ေထရ၀ါဒဗုဒ)into the whole of Myanmar, and the Golden Era of Pagoda building began. The empire flourished for 250 years.\n1084 KingKyansittha(AD 1084 -1112),(က်န္စစ္သားမင္းႀကီး) the 2nd king of fame in the Bagan Dynasty, consolidated the Bagan Kingdom founded by his father King Anawrahta. He also put Theravada Buddhism on a strong, firm foothold.\n1113 King Alaung Sithu (AD 1113 - 1160)(အေလာင္းစည္သူမင္းၾကီး)The grandson of King Kyansittha, became the King of Bagan at the age of 23 and his reign lasted for 47 years. During his reign as the king of Bagan, he built many elegant and prominent temples, including Thabyinnyu and Shwegu-gyi.\n1287 The first Myanmar Empire collapsed with the invasion of the Mongols led by Kublai Khan. The country was separated into smaller kingdoms.1486-1530King Minkyino(မင္းက်ီးညိဳ) ascends the throne of the Burmese town of Taungoo in 1486.\nAfter King Minkyino\'s death in 1530 his 16-year-old son Tabin-shweti becomes Taungoo\'s new King. Tabin-shweti follows an aggressive policy aiming to resurrect the Burmese realm within the borders of the former Bagan empire.\n1531-1539Tabin-shweti \'s(တပင္ေရြထီး) troops conquer the Mon port town Bassein and in 1539 the most important Mon town of that time, Bago. Further conquest campaigns into the northern Ayeyarwaddy valley ensure Tabin-shwet \'s reign over an area, which roughly represents today\'s Myanmar. 1541 Myanmar was reunited by King Tabin-shweti (1531 ‑ 1551) and King Bayinnaung (1551 ‑ 1581) of Taungoo Dynasty and extended control over the whole Chao Phraya valley of present-day Thailand.1564 Bayint Naung (according to Siamese sources: Bhueng Noreng) lays siege to the Siamese capital of Ayutthaya, until it surrenders to the conditions of the Burmese.\nThe Siamese Princess Suphankalaya and Prince Naresuan, were abducted to Myanmar, as well as thousands of people and a number of highly valued white elephants. 1569 Bayint Naung(ဘုရင့္ေနာင္) was forced to invade Siam again, leading an army of 200,000 men. After a siege of seven months Ayuthaya was taken by force. Other states on Myanmar‑Chinese border and Manipur, now part of India, paid tribute to Myanmar.\nBago was its capital. However, after King Bayint Naung\'s death, the Empire fell apart eventually.1607 Re‑unification of Myanmar by King Anauk‑hpet‑Lun (1606 ‑ 1628).(အေနာက္ဘက္လြန္မင္းတရားၾကီး)1635King Thalun (1628 ‑ 1648)(သာလြန္မင္းတရားၾကီး) moved the capital from Bago to( Innwa (Ava), (အင္း၀)near present‑day Mandalay.\nThe realm of the Burmans continues to lose in influence. At the same time the realm of the Mon, whose capital is still at Bago, grows in strength.\n1650 Central power of the Empire weakened during the reign of King Pindale (1648 ‑1661) and deteriorated eventually.\nThe Mon conquer Innwa (Ava ) in 1752 and make it temporarily their own capital.\n1752King Alaungpaya (1752 ‑ 1760)(အေလာင္းမင္းတရား) of Konbaung Dynasty once again reunited the whole country and established the third Myanmar Empire. Shewbo, north of present‑day Mandalay, was its capital. 1765 Re‑establishment of Innwa (Ava) as Myanmar\'s capital by King Sinbyushin (1763‑1776). Burmese forces of King Sinbyushin invaded Siam in 1765 and fought a fierce battle with the Thais for two years before gaining control of the capital, Ayuthaya. During the process Ayuthaya was largely destroyed.1783 Capital moved to Amarapura,(အမရပူရ) near present‑day Mandalay, by King Bodawpaya (1782 ‑ 1819).1823 Capital moved back to Innwa (Ava) by King Bagyidaw (1819 ‑ 1838)1824 The First Anglo‑Myanmar war (1824 ‑ 1826) broke out and the British annexed Rakhine and Tanintharyi regions in Lower Myanmar.1838 Capital moved to Amarapura by King Tharyarwaddy (1838 ‑ 1846)1852 The second Anglo‑Myanmar war broke out and the British took control of Lower Myanmar.\n1857 King Mindon (1853 ‑ 1878)(မင္းတုန္းမင္းၾကီး) founded city of Mandalay and two years later it was officially inaugurated as capital of Myanmar. King Mindon and his brother Prince Kanaung sought to modernize the country. Young men were sent to European countries to study. Several factories were build. Emissaries were sent abroad to establish good relations.1885 Despite King Mindon\'s efforts to save the country from the colonialists., Myanmar was once again waged a war of aggression by the British during the reign of his son King Thibaw (1878 ‑ 1885).\nKing Thibaw (သီေပါမင္း)was exiled to India after the war, so marking the end of the Myanmar monarchy and Myanmar\'s independence as a sovereign state. Local resistance, however, was fierce and it was not until 1890 that all of Myanmar was brought under British control.1948 On 4 January 1948, Myanmar regained its independence after sacrificing thousands of lives during the struggle against foreign rule. Click here for the time-line beyond 1948 to the present day']"	['<urn:uuid:f2f03c1d-2d3c-4062-bdfa-b7295536901c>', '<urn:uuid:39195146-90ab-4528-b628-40fa3745a889>']	open-ended	direct	long-search-query	similar-to-document	comparison	novice	2025-05-13T01:23:33.086345	10	89	1801
86	Why is JPEG format ideal for photographs?	JPEG is ideal for photographs because it reduces image size while maintaining quality, provides better color resolution, and is recognized by all browsers.	['What are the different types of computer images? The importance of images in any form of communication may surpass verbal communication. In order to achieve various types of computer images, the computer must have very good image graphics. The quality of any image will always depend on the format in which the final image is stored. This therefore requires one to clearly define the purpose of the image- where or how it’s to be used. Without good graphics, representation and manipulation of various types of computer images becomes very tedious. In fact, synthesis and manipulation of visual content may be nearly impossible.\nChoosing the right format for different types of images is very important when dealing with different types of computer images. In most cases, different types of computer images can always be rasterized, that is, images are converted into the form of various grid pixels. Each image has a number of bits which are responsible for designating the image color.\nComputer images can always be compressed to suit the desired requirements.\nThere two ways in which all types of computer images can be compressed.\n– The lossless compression – In this type of compression, the reduction in the file size of the image is virtually negligible. It’s normally recommended when editing as there is always no accumulation of recompression.\n– The lossy compression – In this type of compression, smaller sized files are normally achievable.\nUnderstanding various types of computer images file types is very useful when it comes to editing, saving or sending the images from one media to the other.\n– Joint Photographic Expert Group (JPEG) – It’s the ideal format for photographs. It reduces the size of computer images while still maintaining the image’s quality. It is a commonly used format since it’s recognized by all browsers. It guarantees a better color resolution and compression of all type of computer images. It’s very convenient to store images in this type of format\n– Tagged Image File Format (TIFF) – This format is used during the reproduction of images in printers. It is mostly preferred for pixel based images as there is no quality loss. It is applied in most photography processes. Extra information may also be stored, but this will depend on the type of picture editor used.\n– Since these types of computer images also include animations, Graphic Interchange Format (GIF) is preferred. Animations are images with low colors and therefore GIF is very suitable for the production of various cartoon based images. This format is limited to a maximum use of 256 colors.\n– Portable Network Graphics (PNG) – This is basically an improved version of GIF. Unlike GIF, it has the capability to support over 16 million colors.\nDifferent computers come with different graphics which enhances clarity of a wide range of various types of computer images. However it will always come with some additional costs. On the other hand, the file formats also keep on improving with more advanced formats being developed to replace the parent image formats. The efficiency with which saving, editing or sending any type of computer images will exclusively depend on the format chosen.']	['<urn:uuid:d1b556f1-6361-4754-8489-91af5e2cbd79>']	factoid	direct	concise-and-natural	similar-to-document	single-doc	expert	2025-05-13T01:23:33.086345	7	23	526
87	curious basics geometry space made up tiny pieces how they look	At the fundamental level, space is made up of tiny grains that can be visualized as polyhedra (three-dimensional geometrical shapes) with faces of fixed area. However, in the full quantum theory, these polyhedra are not precise shapes like Platonic solids - they are 'fuzzed out' due to quantum effects. This is similar to how we cannot think of a quantum particle as a literal spinning ball. The tetrahedron is the simplest possible polyhedron among these space grains.	"['Title: Dynamical chaos and the volume gap\nPDF of the talk (8Mb) Audio [.wav 37MB] Audio [.aif 4MB]\nby Chris Coleman-Smith, Duke University\nAt the Planck scale, a quantum behavior of the geometry of space is expected. Loop quantum gravity provides a specific realization of this expectation. It predicts a granularity of space with each grain having a quantum behavior. In particular the volume of the grain is quantized and its allowed values (what is technically known as ""the spectrum"")have a rich structure. Areas are also naturally quantized and there is a robust gap in their spectrum. Just as Planck showed that there must be a smallest possible photon energy, there is a smallest possible spatial area. Is the same true for volumes?\nThese grains of space can be visualized as polyhedra with faces of fixed area. In the full quantum theory these polyhedra are fuzzed out and so just as we cannot think of a quantum particle as a little spinning ball we cannot think of these polyhedra as the definite Platonic solids that come to mind.\nIt is interesting to examine these polyhedra at the classical level, where we can set aside this fuzziness, and see what features we can deduce about the quantum theory.\nThe tetrahedron is the simplest possible polyhedron. Bianchi and Haggard  explored the dynamics arising from fixing the volume of a tetrahedron and letting the edges evolve in time. This evolution is a very natural way of exploring the set of constant volume polyhedra that can be reached by smooth deformations of the orientation of the polyhedral faces. The resulting trajectories in the space of polyhedra can be quantized by Bohr and Einstein\'s original geometrical methods for quantization. The basic idea here is to map some parts of the smooth continuous properties of the classical dynamics into the quantum by selecting only those orbits whose total area is an integer multiple of Planck\'s constant. The resulting discrete volume spectrum gives excellent agreement to the fully quantum calculation. Further work by Bianchi, Donna and Speziale  extended this treatment to more complex polyhedra.\nMuch as a bead threaded on a wire can only move forward or backward along the wire, a tetrahedron of fixed volume and face areas only has one freedom: to change its shape. Classical systems like this are typically integrable which means that their dynamics is fairy regular and can be exactly solved. Two degree of freedom systems like the pentahedron are typically non integrable. Their dynamics can be simulated numerically but there is no closed form solution for their motion. This implies that the pentahedron has a much richer dynamics than the tetrahedron. Is this pentahedral dynamics so complex that it is actually chaotic? If so, what are the implications for the quantized volume spectrum in this case. This system has recently been partially explored by Coleman-Smith  and Haggard  and was indeed found to be chaotic.\nChaotic systems are very sensitive to their initial conditions, tiny deviations from some reference trajectory rapidly diverge apart. This makes the dynamics of chaotic systems very complex and endows them with some interesting properties. This rapid spreading of any bundle of initial trajectories means that chaotic systems are unlikely to spend much time \'stuck\' in some particular motion but rather they will quickly explore all possible motions. Such systems \'forget\' their initial conditions very quickly and soon become thermal. This rapid thermalization of grains of space is an intriguing result. Black holes are known to be thermal objects and their thermal properties are believed to be fundamentally quantum in origin. The complex classical dynamics we observe may provide clues into the microscopic origins of these thermal properties.\nThe fuzzy world of quantum mechanics is unable to support the delicate fractal structures arising from classical chaos. However its echoes can indeed be found in the quantum analogues of classically chaotic systems. A fundamental property of quantum systems is that they can only take on certain discrete energies. The set of these energy levels is usually referred to as the energy spectrum of the system. An important result from the study of how classical chaos passes into quantum systems is that we can generically expect certain statistical properties of the spectrum of such systems. In fact the spacing between adjacent energy levels of such systems can be predicted on very general grounds. For a non chaotic quantum system one would expect these spacings to be entirely uncorrelated and so be Poisson distributed (e.g the number of cars passing through a toll gate in an hour) resulting in most energy levels being very bunched up. In chaotic systems the spacings become correlated and actually repel each other so that on average one would expect these spacings to be quite large.\nThis is suggestive that there may indeed be a robust volume gap since we generically expect the discrete quantized volume levels to repel each other. However the density of the volume spectrum around the ground state needs to be better understood to make this argument more concrete. Is there really a smallest non zero volume?\nThe classical dynamics of the fundamental grains of space provide a fascinating window into the behavior of the very complicated full quantum dynamics of space described by loop quantum gravity. Extending this work to look at more complex polyhedra and at coupled netwworks of polyhedra will be very exciting and will certainly provide many useful new insights into the microscopic structure of space itself.\n: ""Discreteness of the volume of space from Bohr-Sommerfeld quantization"", E.Bianchi & H.Haggard. PRL 107, 011301 (2011), ""Bohr-Sommerfeld Quantization of Space"", E.Bianchi & H.Haggard. PRD 86, 123010 (2012)\n: ""Polyhedra in loop quantum gravity"", E.Bianchi, P.Dona & S.Speziale. PRD 83, 0440305 (2011)\n: ""A “Helium Atom” of Space: Dynamical Instability of the Isochoric Pentahedron"", C.Coleman-Smith & B.Muller, PRD 87 044047 (2013)\n: ""Pentahedral volume, chaos, and quantum gravity"", H.Haggard, PRD 87 044020 (2013)']"	['<urn:uuid:0adf654d-89ac-4846-b985-2c45adee8557>']	open-ended	with-premise	long-search-query	distant-from-document	single-doc	novice	2025-05-13T01:23:33.086345	11	77	983
88	What topics do the indoor wildlife sessions cover?	The indoor wildlife workshops cover twelve program topics: Bats, Bears, Wolves, Owls, Wildcats of BC, Marine Wildlife, Temperate Rainforest, Wildlife of BC, Endangered Species, Climate Change, Reptiles & Amphibians, and Salmon of the Pacific.	['Through engaging education and stewardship programs, Northwest Wildlife Preservation Society introduces British Columbians to the wonders of the natural world to encourage understanding of and an enduring respect for wildlife in the Pacific Northwest. Our three main programs are Indoor Wildlife Workshops, Nature Walks & Workshops, and Youth Environmental Leadership programs. All programs are offered year-round. Thanks to our generous corporate and foundation grantors and individual donors, programs are free of charge to inner-city school children and youth, as well as other at-risk/disadvantaged/marginalized groups to ensure everyone is given an equal opportunity to understand their interconnected relationship with wildlife and the importance of protecting it. For groups more fortunate, we request a modest honorarium to help cover our costs. We work with all kinds of groups including but not limited to public and private school classes, First Nations groups, summer camps, youth groups, boy/girl scouts, retirement homes, and new immigrants.\nIndoor Wildlife Workshops\nLearn about wildlife in these engaging workshops tailored to groups of any age, from preschoolers to seniors.\nWith twelve program topics to choose from, these highly interactive hour-long sessions are a wonderful way to learn about the incredible biodiversity we are blessed with in British Columbia. Program topics include Bats, Bears, Wolves, Owls, Wildcats of BC, Marine Wildlife, Temperate Rainforest, Wildlife of BC, Endangered Species, Climate Change, Reptiles & Amphibians, and Salmon of the Pacific.\nFor school groups, the content is created in line with the curriculum to enhance the topics currently being studied in class. All the programs are designed around principles of interactive education and include ample opportunity for group discussion and engaging hands-on learning. This teaching style is facilitated by the use of biofacts like animal skulls, antlers, and animal mounts (ie bats and owls), helping students connect better with the topic and seeing for themselves physical characteristics and clues to animal diets and behaviour.\nUnlike exposure to similar artifacts in a museum or with live animals, by using biofacts we can encourage participants to learn by touching and examining the biofacts closely without stressing out a live animal. We also use various forms of technology including awe-inspiring photography and video as well as audio recordings of animal vocalizations. Using diverse learning tools is complimented by interactive demonstrations, educational games and activities that make learning both exciting and memorable.\nThanks to our generous donors, these workshops are offered free of charge to inner city schools and disadvantaged groups. For those more fortunate we ask for a modest honorarium to help cover our costs. The honorarium is $100/group throughout the Greater Vancouver Regional District. For program requests outside of GVRD, please contact us for pricing.\nNature Walks & Workshops\nOur Nature Walks & Workshops provide first-hand experience learning about wildlife during a full-day outdoor class and field workshop in a regional park. Participants are engaged in discussions about natural history, wildlife behaviour, animal adaptations, biodiversity, threats to the environment and solutions to these issues. Learning about wildlife in this setting allows people to use all of their senses to understand the environment and to become connected with the natural world that surrounds them.\nIncreased stress on our natural world has resulted in a growing need for environmental education and activities that work with nature instead of against it. Children especially are becoming more disconnected from their environment, particularly in large cities where many natural spaces are fragmented, bordered by busy roadways and not easily accessed by public transportation. Due to this separation, many community residents are unaware of the impacts their actions have on wildlife. Our programs offer students the opportunity to get acquainted with their environment. Connecting them with the incredible biodiversity we are so fortunate to have here in British Columbia is pivotal for inciting a passion for preservation in our future generation of leaders.\nThis program is predominantly offered to inner-city school classes thanks to the generous funding support from our corporate and foundation partners, as well as our amazing group of individual donors. All costs including the school bus to transport students are taken care of. Please inquire for more information if you are interested in booking a program. Note this program is in very high demand and often has a wait-list.\nYouth Environmental Leadership Programs\nThe Youth Environmental Leadership program is innovative, extensive and hands-on. It provides youth aged 9 to 14 the opportunity to participate in real conservation work, which they design and lead under the guidance of our program instructor. Students learn how important healthy habitats are to the sustainability of many threatened plant and animal species as well as to the entire ecosystem of which we’re all a part. For example, they learn how conserving and restoring wild habitats can help mitigate the effects of climate change, as well as provide sanctuary to various wildlife species that have previously been extirpated from an area.\nOur programs are inclusive and provide opportunities to kids with all types of learning abilities. Students acquire and enhance many skills related to biology, environmental science, ecology and chemistry. It also promotes healthy, active living as students are walking outdoors and completing physical activities for a good cause.\nThanks to our generous donors, these programs are offered free of charge to inner city schools and disadvantaged groups. Please inquire for more information if you are interested in booking a program. Note this program is in very high demand and often has a wait-list.']	['<urn:uuid:e9d487f5-2479-46b3-80d3-234c92a14272>']	factoid	direct	concise-and-natural	distant-from-document	single-doc	expert	2025-05-13T01:23:33.086345	8	34	900
89	I'm planning to start cattle farming and want to know about temperature control. What's the ideal temperature range for keeping beef cattle healthy and getting good fattening results?	The optimum environmental temperature for beef cattle breeding is between 7°C and 27°C, with an ideal relative humidity of 65%-70%. Temperatures above 30°C should be avoided as they can cause heat stress in beef cattle, which is harmful to both fattening and cattle health. Spring and autumn are the most suitable seasons for fattening, while the extreme temperatures of winter and summer are not conducive to weight gain. If you must fatten cattle during winter or summer, it's important to implement proper heat prevention and cold resistance measures to maintain optimal temperatures.	"[""1. Selection of suitable fattening population\nThe breed, appearance and body shape of beef cattle play an important role in the fattening effect, but in addition to selecting beef cattle with good characteristics of the breed, other conditions should be considered. When choosing feeder cattle to fatten, the appropriate age, the price for feeder cattle and cattle feeder futures should be chosen. Generally, shelf cattle aged 1.5～2 can gain higher economic benefits after 90～120 days of fattening.\nThe increasing of bulls is more important than that of steers in sex selection because testosterone secreted by bulls testis is involved in blood production, fat and feed metabolism, promoting calcium balance in the body, which can significantly improve the growth rate and feed utilization of beef Charley cattle. Bulls have a higher lean meat percentage, less fat and better meat quality, so we should choose bulls as far as possible to fatten them. However, bulls over 2 years old should be castrated before fattening, otherwise, it is difficult to manage and will affect meat quality.\n2. Selection of suitable fattening season\nThe optimum environmental temperature for beef cattle breeding is 7～27 ℃, and the relative humidity is 65%～70%. Excessive temperature will cause heat stress in beef cattle, which is not only harmful to fattening but also harmful to the health of beef cattle. Therefore, the best choice for the fattening season is when the temperature is below 30 ℃. Suitable temperature can promote beef cattle feed intake and feed utilization, and can also reduce cattle feed prices and the invasion of ectoparasites such as mosquitoes and flies on cattle and the loss of beef cattle's physical energy, which is beneficial to fattening.\nThe suitable season for beef cattle fattening is spring and autumn. The best season for beef cattle slaughtered in late autumn. The low temperature in winter and high temperatures in summer are not conducive to beef cattle gaining weight. If we choose to fatten beef cattle in winter and summer, we should do a good job of heat prevention and cold resistance to adjust the temperature to the most suitable for beef cattle fattening.\n3. Choosing appropriate fattening and feeding methods\nBefore fattening, beef Brahman cattle should be grouped reasonably according to their breed, weight, age, body condition, and sex, to avoid fighting between cattle and facilitate management. Beef cattle fattening should be carried out in stages, which can be divided into three stages, they are the pre-fattening stage, mid-fattening stage, and post-fattening stage. These three stages should be raised and managed according to the characteristics of the growth and weight gain of beef cattle to avoid wasting feed.\nIn general, the pre-fattening stage is 15～20 days, which mainly restricts the activity of beef Limousine cattle, so that beef cattle can adapt to the environment and feed. The concentrate feeding amount is 0.5 kg per day, and the protein content in the diet should reach 12%. The mid-fattening stage is generally 45 days, during which concentrate feeding should be increased to achieve the purpose of rapid fattening, and the protein content in the diet should be 10%. The post-fattening stage is a period of sudden fattening.\nThe main task is to increase the fat content between muscles to improve the flavor and tenderness of beef. This should be mainly fed with energy feed, and the corresponding protein content should be lowed. In this stage, the feeding times should be increased appropriately, and the roughage can be fed once at night while ensuring that adequate drinking water is provided.\nTo select suitable feeding methods for fattening beef cattle, at present, the feeding methods of tethered feeding, free feeding, and free drinking water are mostly adopted to reduce the exercise and energy consumption of cattle. The reins tied to beef cattle should not be too long, usually 60 cm. Feeding forage grass should obey the principle of feeding less frequently, and the feeding time of beef cattle can be prolonged properly by feeding roughage first and then concentrate so that beef cattle can get the maximum feed intake. Feeding beef cattle requires a comprehensive and reasonable nutritional composition, diversified feed, and good palatability, to improve feed utilization and speed up weight gain.\n4. Strengthen daily management\nIn the process of fattening beef longhorns cattle, we should do a good job of daily observation. We should observe the mental state, diet, and excretion of beef cattle every day. If we find abnormalities, we should find out the reasons in time and solve them pertinently. To ensure the safety of feed, do a good job in the storage of feed, and forbid feeding beef cattle moldy and deteriorated feed. Moreover, attention should be paid to the hygiene of drinking water to ensure that beef cattle drink enough, especially in hot summer should provide adequate and cool drinking water.\nIn spring and winter, the temperature of drinking water should be kept at no less than 10 ℃. To maintain the health of beef cattle, brushing the cattle body 1 to 2 times a day can not only promote blood circulation but also maintain surface hygiene, eliminate ectoparasites and enhance physical fitness. When beef cattle suffer from parasites in vivo and vitro, parasites will directly take the nutrition of beef cattle. Besides, toxic substances produced by parasite metabolism will also lead to beef cattle disease, seriously affecting the fattening effect of beef cattle, and will also pose a threat to their health.\nTherefore, comprehensive screening of insect repellent and disease should be carried out in beef cattle fattening to ensure that each head of cattle is in good health. Besides, cattle should be treated once every spring and autumn, and insecticides with high efficiency and low toxicity can be selected. For beef cattle in poor condition, 100～150 g of salt can be fed to invigorate their stomach.\n5. Provide a suitable feeding environment\nEnvironmental factors play an important role in the fattening effect of beef cattle. During the fattening period of beef cattle, it is necessary for the cattle rancher to provide a comfortable living environment for beef cattle to facilitate their growth and weight gain, to achieve the best fattening effect. Keep the shed clean, dry and fresh. The feces and dirt in the shed should be cleaned up in time. Do a good job of ventilation and bedding should be changed frequently. Keep quiet near the cattle shed so as not to disturb the herd. Attention should be paid to keeping proper temperature, relative humidity and concentration of harmful gases in the shed. The cattle shed and combed are regularly disinfected thoroughly and comprehensively to reduce the damage to pathogenic microorganisms to cattle.""]"	['<urn:uuid:e8c05408-0cf8-480c-ae9b-039bb382caa1>']	open-ended	with-premise	verbose-and-natural	similar-to-document	single-doc	novice	2025-05-13T01:23:33.086345	28	92	1109
90	How are traditional practices being disrupted in both the art authentication field and the transportation sector, and what solutions are emerging to address these changes?	In art authentication, traditional scholarly practices are being severely disrupted as experts become hesitant to provide definitive opinions due to lawsuit threats. This has led to a situation where even obvious fakes may not be denounced, affecting the integrity of catalogue raisonnés and potentially deceiving innocent buyers. Meanwhile, in the transportation sector, traditional delivery practices are being transformed through innovative solutions, with companies like Amazon and UPS adopting electric vehicles to address both environmental concerns and anti-idling laws in cities like New York and Philadelphia. Legislative support is also emerging, with Democratic lawmakers proposing a bill to create a nationwide EV charging network within five years.	['Imagine you’re an expert on some famous artist. Now imagine someone comes to you with a painting they believe is by that famous artist. After looking at the painting, you give your expert opinion that the work isn’t by that famous artist. Imagine that the unhappy owner is unhappy enough to sue you over your answer. In the owner’s mind, you’ve just cost him or her thousands or even millions depending on the magnitude of the painter’s fame. That scenario is happening for real more and more often. The consequences to art scholarship are chilling. Fear to use expertise is as good as no expertise at all. A world without art historians helping us understand art is one I can’t even imagine.\nIn a recent issue of The Art Newspaper, Georgina Adam and Riah Pryor discuss this troubling new trend of “law versus scholarship.” They open with a tale of disputed drawings allegedly by Francis Bacon that the expert will neither confirm nor deny, giving only a noncommittal opinion that they are “unlike any authenticated works.” Such silence is deafening and disturbing. Robbed of academic freedom, the expert can’t practice his craft. The problem is even worse in America, land of the litigious. Adam and Pryor quote New York art lawyer Ronald Spencer saying that “[t]his is a very serious problem. Specialists are often academics earning $100,000 [or less] a year and they can’t afford litigation they are fearful of being a defendant in a lawsuit, even if they should win.” Although Spencer concedes that America’s litigiousness is a “cliché,” the U.S.’s system in which the plaintiff is not required to pay the legal fees of a successful defendant increases the likelihood of such lawsuits versus in other countries such as the U.K.\nEven something as seemingly innocent as a catalogue raisonnés can become a legal battlefield. Including or not including a work in the official record of an artist can land you in court. The President of the Catalogue Raisonné Scholars Association, Nancy Mathews, told Adam and Pryor of threats from angered owners and “legal action whether it’s justified or not—it’s emotional—[that] causes headaches for the scholar even if it has no real legal basis.” “As a result even flagrant fakes are not denounced,” Adam and Pryor conclude, “meaning that innocent people could be deceived.” In a way, the angered owners are hurting themselves as well as the market.\nThe environment has reached the point that organizations created specifically to authenticate works, such as the Andy Warhol Art Authentication Board, close because of the cost of lawsuits over their pronouncements. The Pollock-Krasner Foundation, which folded in 1995, still fields lawsuits related to its catalogue raisonné of Jackson Pollock’s art. I doubt authentication boards will proliferate in the future in the face of owner fury and shaky finances.\nWhat saddens me the most about the litigious end of the art scholarship and expertise that translates into catalogues raisonné is the lost multidimensionality you get of the great artists through such scholarship. Collectors want consistency in an artist—a “name brand” as recognizable as Modigliani’s long necks. But I love to see the little known exceptions to the rule: the landscapes by portrait painters, the youthful abstractions of a committed realist, the late works that seem done by another hand due to philosophical or physical change (and sometimes both). “Do I contradict myself?” Walt Whitman wrote, “Well, then, I contradict myself. I contain multitudes.” Great artists contradict themselves because the “multitudes” within them yearn to be free. One genre, one style, one medium are rarely enough. Those “multitudes” raise questions for us today. We can either enjoy the doubts they may raise or ignore that uncertainty in the name of name branding and the certainty of a solid investment.', 'Electric vehicles (EVs) hold a lot of promise for the private sector — especially as consumers, who are increasingly aware of the relationship between emissions and climate change, are starting to demand eco-friendly delivery options. EV adoption, however, has been slowed down by a few different challenges — the US’s poor EV charging infrastructure in particular.\nNow, however, we’re beginning to see signs that major businesses are willing to buy into EVs, despite potential road bumps.\nHere are the businesses that are leading the way when it comes to EV adoption.\nAmazon and UPS Lead Way on EV Adoption\nTwo delivery giants — Amazon and UPS — have begun to aggressively add EVs to their delivery fleets.\nEarlier this year in January, Amazon ordered 100,000 electric delivery trucks from EV manufacturer Rivian, as well as 10,000 electric delivery rickshaws for their operations in India. Then, around the end of the month, UPS announced that it had ordered 10,000 electric trucks from the UK-based manufacturer Arrival Ltd., and would soon be teaming up with self-driving car manufacturer Waymo for a pilot test of self-driving delivery vehicles.\nThe moves are part of broader pushes towards carbon neutrality and self-driving delivery by the two companies. Last year, Amazon announced the company’s plan to be 100 percent carbon-neutral by the year 2040. UPS already offers carbon-neutral and carbon-offset delivery options.\nThe moves also come as more cities around the U.S., including New York and Philadelphia., have begun to adopt anti-idling laws that allow the city to fine companies over idling delivery vehicles.\nSome cities have even developed apps that allow citizens to report idling vehicles based on that vehicle’s DOT number — making these policies even more costly for delivery companies. Because electric vehicles produce no emissions, they’re typically free from being fined — meaning savings for businesses that adopt EVs for city deliveries.\nThe announcements are both historic. While other companies have announced EV purchases — like Lyft, which plans to deploy 200 EVs in Denver as part of its rental vehicle program there — there’s been nothing near scale of these announced by Amazon and UPS.\nWhile neither UPS nor Amazon has plans to go fully electric any time soon, the purchases are a welcome sign for the EV industry. Coupled with similar positive signals from the individual consumer side of the industry, they likely demonstrate that despite early growing pains, EVs may be on track for widespread adoption in the near future.\nChallenges Facing Further EV Adoption\nHowever, there still remain significant barriers that may slow or prevent full EV adoption, primarily the weak EV charging infrastructure in the US and limited number of charging stations — although this, too, seems like it’s starting to change.\nChargePoint, in coalition with the National Association of Truck Stop Operators (NATSO) has formed the National Highway Charging Collaborative, which plans to install new charging stations at more than 4,000 highway-side locations in the U.S., in order to increase the availability of EV charging stations in rural areas.\nAt the same time, legislative support for stronger EV infrastructure is beginning to build. In February, Democratic lawmakers in the House of Representatives announced a new bill that would create a nationwide EV charging network within the next five years.\nUpgrades to existing infrastructure would likely encourage further adoption. They may also be especially beneficial for businesses like Amazon and UPS, as both companies regularly make deliveries to rural parts of the country — areas that don’t always have the charging infrastructure needed to support EVs.\nThe Future for EVs in Business\nEV adoption in the private sector, which has lagged in the past, seems to be accelerating. Two major delivery companies have now announced that they will be adding significant numbers of EVs to their delivery fleets, with more likely to come in the near future as both pursue low-carbon delivery options.\nWhile challenges remain that may slow down EV adoption — primarily the nation’s weak EV charging infrastructure — the purchases are likely a good sign for the industry and the future of EVs in the private sector.']	['<urn:uuid:35b04ac4-86cb-498e-b424-08a4f3bdf5f4>', '<urn:uuid:a3b2914f-bfcd-4107-a76d-db9ca5d880cd>']	open-ended	direct	verbose-and-natural	similar-to-document	multi-aspect	expert	2025-05-13T01:23:33.086345	25	106	1310
91	What are the important steps that need to be considered when testing the air quality in small enclosed work areas to make sure it's safe?	When conducting representative sampling in confined spaces, several factors must be considered: the presence of stratified atmospheres and pockets of contaminated air, the selection of appropriate testing equipment based on circumstances and potential hazards, the use of continuous monitoring when practical, and equipment performance characteristics including detection principles, specificity, interferences, detection range, response time, calibration requirements, and intrinsically safe equipment for spaces with potential flammable hazards.	['Working within safe limits in a confined space\nBy Jean Lian\nBy Jean Lian\nAccording to Statistics Canada, there were 4,990 deaths associated with carbon monoxide (CO) poisoning between 2000 and 2013. Although this number includes exposure incidents in residences, many CO-exposure cases occur in workplaces.\nCarbon monoxide is one of the most widespread and dangerous industrial hazards, WorkSafeBC notes. Potentially lethal at concentrations as low as 1000 parts per million (ppm), it is also the most common cause of occupational gas poisoning leading to death. As there is no way to detect CO by odour, colour or irritation, special detection methods like the following must be used:\n- Gas detector tubes: These clear tubes are about the size and shape of a ballpoint pen and can be read much like a thermometer. The tube contains a material that may change colour when it reacts with air drawn through it by a small hand pump. The amount of colour change depends on the CO level.\n- Electronic detectors: These range from small personal samplers to large, stationary monitors with a display screen that shows the gas level. When CO levels exceed a set limit, these detectors sound an alarm, flash a light or vibrate.\n- Blood samples: If there is particular concern about the effects of CO in a workplace, blood samples can be taken from workers to determine the amount of Carboxyhemoglobin, a stable complex of carbon monoxide that forms in red blood cells when carbon monoxide is inhaled.\nFor those who work in confined spaces, using instrumentation devices to conduct atmospheric testing is a must. The employer must appoint a qualified person with adequate knowledge, training and experience to perform adequate tests. Testing is required as often as necessary before and while a worker is in a confined space to ensure that acceptable atmospheric levels are maintained, according to Ontario’s Ministry of Labour.\nWhen conducting representative sampling, the following needs to be taken into consideration:\n- The presence of stratified atmospheres and pockets of contaminated air within the confined space;\n- The selection of testing equipment will depend on the circumstances and the nature of work in the confined space and knowledge of possible atmospheric hazards;\n- Whenever practical, continuous monitoring should be considered; and\n- Equipment performance characteristics to be considered include the principle of detection of the hazards of concern, specificity, interferences, detection concentration range, response time, calibration requirements, and intrinsically safe equipment for spaces with potential accumulation of flammable hazards.\nTo ensure that a gas-detection device is functioning as it should, it is important to maintain and calibrate an instrument so that it can detect the presence of gases accurately. Full calibration involves adjusting the instrument’s reading to match the actual concentration of a certified standard test gas, while a “bump test” verifies that the instrument is working properly by exposing it to a known concentration of gas, WorkSafeBC notes.\nWith regards to alarm settings, a qualified person should set alarm limits based on the hazard assessment of the confined space, the length of time that workers will be in the confined space, work activities taking place within the confined space and the exposure limits for the contaminants of concern.\nJean Lian is editor of OHS Canada.']	['<urn:uuid:a8a47403-a4ee-4d25-bb4b-333fb7674d82>']	open-ended	direct	verbose-and-natural	distant-from-document	single-doc	novice	2025-05-13T01:23:33.086345	25	66	542
92	What was the first known true work of art?	The first known true work of art was Der Löwenmensch, a 30-centimeter-high carved figure from south-west Germany made 40,000 years ago. It has human legs and an arm but the head of a lion.	['To mark the publication of “Go Figure”, a collection of The Economist’s explainers and daily charts, the editors of this blog solicited ideas on Facebook and Twitter. This week we publish five explainers suggested by our readers, who will each receive a copy of the book.\nTHE first artists emerged in the Ice Age, when humans moved out of the eastern Mediterranean and into the rich central European plain with its plentiful supply of food. Initially they painted what they saw around them—people, animals, hunting—as scenes depicted in ochre and mud on the walls of caves. Then 40,000 years ago a skilled artisan in south-west Germany carved a standing figure from his imagination. Thirty centimetres high and known as Der Löwenmensch, it has human legs and an arm but the head of a lion. This was the first known true work of art. In due course, artworks became precious, desired and traded. The earliest illustration of the art market that we know of is on a Greek cup, painted by Phintias and dating to 500BC, that shows a young man buying a vase. But what makes a work of art valuable?\nThe art market as we know it today emerged in the 18th century. Before that works of art were commissioned directly from artists, chiefly by wealthy and powerful patrons such as the Medicis and the Catholic Church, rather than traded. Other collectors began to emerge with the industrial revolution and the rise of the middle class in Europe. Christie’s, an auction house, was founded in 1766. Wealthy dealers, who brought buyer and seller together, emerged in the 19th century. In the 21st century the market is enormous. Art sales in 2015 totalled $64bn, according to a report by Clare McAndrew, an art economist, making it bigger than the economy of Kenya or Costa Rica. Fashion drives the contemporary art market, as does scarcity (but not too much). “Validation” is important too: association with a great collector (such as David Bowie), participation in a much-lauded exhibition or being recognised as part of the story of art history.\nAbout half of the market is accounted for by sales through dealers, either directly from the artist’s studio (the “primary market”) or by reselling a work that has already been bought once (the “secondary market”) and which is now being sold on, often with several commissions being paid to intermediaries along the way. The other half of the art market is made up of sales by auction, a system that claims to be open and competitive, with both artwork and potential bidders being visible to all. In reality, it can be as secretive as the dealing world, with complex financial arrangements and discounted commissions being not uncommon. The most expensive work of art sold at auction was Pablo Picasso’s “The Women of Algiers (Version ‘O’)”, which was consigned, in 2015, by a Saudi collector, who had kept the picture hidden away in his London home for nearly 20 years. It was bought, after frenzied bidding, by Hamad bin Jassim bin Jaber al-Thani, a former Qatari prime minister, for $179.4m. The imposing naked women it depicts (see picture, above) means it will probably never be exhibited in the Middle East, but it remains a trophy nonetheless, if only because of the record price.\nWhat of the non-financial value of art? This is perhaps the hardest question of all. Yet it is essential. To stand in front Matisse’s circle of dancing maidens or Rembrandt’s portrait of his mother is to recognise that they are masterpieces. As a viewer you are transfixed, lifted out of yourself, you feel your consciousness being stretched by a story that is both timeless and unending. That is the truest value of art.\nToday’s explainer was suggested by Joshua Chiang. This is the last in a series of five. Other explainers in this series include:']	['<urn:uuid:c6e167f1-5ca7-4631-8996-55a4894291ee>']	factoid	with-premise	concise-and-natural	similar-to-document	single-doc	expert	2025-05-13T01:23:33.086345	9	34	644
93	What is the identity element in addition?	The identity element in addition is zero, because adding zero to a value doesn't change the value (0 + 42 = 42 + 0 = 42).	"[""Monoids by Mark Seemann\nIntroduction to monoids for object-oriented programmers.\nThis article is part of a larger series about monoids, semigroups, and related concepts. In this article, you'll learn what a monoid is, and what distinguishes it from a semigroup.\nMonoids form a subset of semigroups. The rules that govern monoids are stricter than those for semigroups, so you'd be forgiven for thinking that it would make sense to start with semigroups, and then build upon that definition to learn about monoids. From a strictly hierarchical perspective, that would make sense, but I think that monoids are more intuitive. When you see the most obvious monoid example, you'll see that they cover operations from everyday life. It's easy to think of examples of monoids, while you have to think harder to find some good semigroup examples. That's the reason I think that you should start with monoids.\nMonoid laws #\nWhat do addition (\n40 + 2) and multiplication (\n6 * 7) have in common?\n- binary operations\n- with a neutral element.\nBinary operation #\nLet's start with the most basic property. That an operation is binary means that it works on two values. Perhaps you mostly associate the word binary with binary numbers, such as 101010, but the word originates from Latin and means something like of two. Astronomers talk about binary stars, but the word is dominantly used in computing context: apart from binary numbers, you may also have heard about binary trees. When talking about binary operations, it's implied that both input values are of the same type, and that the return type is the same as the input type. In other words, a C# method like this is a proper binary operation:\npublic static Foo Op(Foo x, Foo y)\nOp is an instance method on the\nFoo class, it can also look like this:\npublic Foo Op (Foo foo)\nOn the other hand, this isn't a binary operation:\npublic static Baz Op(Foo f, Bar b)\nAlthough it takes two input arguments, they're of different types, and the return type is a third type.\nSince all involved arguments and return values are of the same type, a binary operation exhibits what Eric Evans in Domain-Driven Design calls Closure of Operations.\nIn order to form a monoid, the binary operation must be associative. This simply means that the order of evaluation doesn't matter. For example, for addition, it means that\n(2 + 3) + 4 = 2 + (3 + 4) = 2 + 3 + 4 = 9\nLikewise, for multiplication\n(2 * 3) * 4 = 2 * (3 * 4) = 2 * 3 * 4 = 24\nExpressed as the above\nOp instance method, associativity would require that\ntrue in the following code:\nvar areEqual = foo1.Op(foo2).Op(foo3) == foo1.Op(foo2.Op(foo3));\nOn the left-hand side,\nfoo1.Op(foo2) is evaluated first, and the result then evaluated with\nfoo3. On the right-hand side,\nfoo2.Op(foo3) is evaluated first, and then used as an input argument to\nfoo1.Op. Since the left-hand side and the right-hand side are compared with the\n== operator, associativity requires that\nIn C#, if you have a custom monoid like\nFoo, you'll have to override\nEquals and implement the\n== operator in order to make all of this work.\nNeutral element #\nThe third rule for monoids is that there must exist a neutral value. In the normal jargon, this is called the identity element, and this is what I'm going to be calling it from now on. I only wanted to introduce the concept using a friendlier name.\nThe identity element is a value that doesn't 'do' anything. For addition, for example, it's zero, because adding zero to a value doesn't change the value:\n0 + 42 = 42 + 0 = 42\nAs an easy exercise, see if you can figure out the identity value for multiplication.\nAs implied by the above sum, the identity element must act neutrally both when applied to the left-hand side and the right-hand side of another value. For our\nFoo objects, it could look like this:\nvar hasIdentity = Foo.Identity.Op(foo) == foo.Op(Foo.Identity) && foo.Op(Foo.Identity) == foo;\nFoo.Identity is a static read-only field of the type\nThere are plenty of examples of monoids. The most obvious examples are addition and multiplication, but there are more. Depending on your perspective, you could even say that there's more than one addition monoid, because there's one for integers, one for real numbers, and so on. The same can be said for multiplication.\nThere are also two monoids over boolean values called all and any. If you have a binary operation over boolean values called all, how do you think it works? What would be the identity value? What about any?\nI'll leave you to ponder (or look up) all and any, and instead, in the next articles, show you some slightly more interesting monoids.\n- Angular addition monoid\n- Strings, lists, and sequences as a monoid\n- Money monoid\n- Convex hull monoid\n- Tuple monoids\n- Function monoids\n- Endomorphism monoid\n- Maybe monoids\n- Monoids accumulate\n==operator. On the other hand, there's no\nTimeSpan, because what does it mean to multiply two durations? What would the dimension be? Time squared?\nA monoid (not to be confused with a monad) is a binary operation that satisfies the two monoid laws: that the operation is associative, and that an identity element exists. Addition and multiplication are prime examples, but several others exist.\n(By the way, the identity element for multiplication is one (1), the all monoid is boolean and, and the any monoid is boolean or.)\nNext: Angular addition monoid""]"	['<urn:uuid:7d03a359-5b99-48f4-87af-5490ca964c36>']	factoid	direct	concise-and-natural	similar-to-document	single-doc	novice	2025-05-13T01:23:33.086345	7	26	939
94	commercial music streaming platforms legal requirements licenses cost alternatives	To legally stream commercial music, various licenses are required: Public Performance Rights from organizations like ASCAP, BMI, and SESAC for musical compositions, Master Use Licenses for sound recordings, and Mechanical Licenses for reproduction. The costs vary, with PRO licenses starting from $200 per year per organization. Alternative solutions include licensed services like SiriusXM Business or Pandora Business ($25-30 monthly), MoodMedia (around $200 yearly), or royalty-free music collections from platforms like stockmusic.com and streamlicensing.com ($130-200 yearly). These alternatives may have content limitations but can help avoid copyright infringement penalties, which can range from $700 to $30,000 per unlicensed song, or up to $150,000 for willful infringement.	"['What was Turntable.fm?\nTurntable.fm was a social media-driven music streaming service that allowed users to interact with each other in virtual rooms, where they could listen to music and chat. It was founded by Billy Chasen and Seth Goldstein and launched in 2011. In Turntable.fm, users could create their own rooms or join existing ones, where a rotating set of DJs would play songs for the audience. Each room had a specific theme or genre, and users could vote on whether they liked the song being played. DJs would gain points based on the audience\'s reactions, which could be used to customize their avatar or unlock other features.\nTurntable.fm gained significant popularity in the early 2010s, but it faced challenges with licensing issues, competition from other music streaming services, and maintaining user engagement. Due to these struggles, the service was shut down in December 2013. Despite its relatively short lifespan, Turntable.fm was influential in the development of social music experiences online, and its concept inspired similar platforms that followed.\nWhat are the necessary licenses for performing recorded music in public?\nTo legally perform recorded music in public, various licenses are required, which usually cover the rights to the musical composition and the sound recording. Here\'s an overview of the key licenses involved:\nPublic Performance Rights: These licenses are needed to perform a musical composition (the underlying musical work, including melody, lyrics, and harmony) in public. In the United States, public performance rights are managed by Performing Rights Organizations (PROs) such as ASCAP, BMI, and SESAC. Businesses and venues typically obtain blanket licenses from these organizations to cover the public performance of the music in their catalogs.\nMaster Use License: This license grants permission to use a specific sound recording (the actual recorded performance of the musical composition). Master use licenses are typically obtained from the record label that owns the rights to the recording or the artist directly if they own their own master rights. This license is particularly relevant when using a recording in movies, TV shows, commercials, or any other audiovisual production.\nMechanical License: This license is needed when reproducing and distributing a musical composition (e.g., making copies of a song for sale). In the United States, mechanical licenses are typically managed by the Harry Fox Agency or can be obtained directly from the copyright holder, often the music publisher or the songwriter.\nSynchronization License: Also known as a ""sync"" license, this permits the use of a musical composition in timed synchronization with visual elements, such as in movies, TV shows, commercials, or video games. Sync licenses are typically negotiated directly with the music publisher or the songwriter.\nWhat methods can DJs or music enthusiasts employ to curate music without encountering public performance-related challenges?\nDJs and music enthusiasts can adopt several approaches to curate music without facing public performance-related issues. Here are a few suggestions:\nPrivate Listening Sessions: Organize private listening sessions or gatherings with friends and family, where the music is played for a small, closed group rather than a public audience. This typically doesn\'t require public performance licensing.\nUse Royalty-Free Music: Curate playlists using royalty-free music, which can be freely used without obtaining licenses or paying royalties. There are numerous online platforms and libraries that offer royalty-free music in various genres.\nCreative Commons Licensed Music: Utilize music released under Creative Commons licenses, which allow for more flexibility in terms of usage, depending on the specific license chosen by the creator. Some licenses may permit public performance without fees, but make sure to carefully review the terms of the chosen license.\nCollaborate with Independent Artists: Connect with independent musicians and obtain their permission to use their music in your DJ sets or playlists. This can help build relationships with emerging artists and bypass traditional licensing requirements, as long as you have their consent.\nUse Licensed Music Services: For businesses or venues, consider using licensed background music services . These services typically cover the necessary public performance licenses and offer curated playlists and customizable music options.\nFocus on Educational or Non-Commercial Settings: If you are involved in music curation for educational purposes or non-commercial events, you may fall under different licensing requirements or exemptions. Check local regulations to understand the specific rules and potential exemptions for educational or non-commercial uses.', 'WHO ARE THESE GUYS AND WHY DO WE HAVE TO PAY THEM?\nHome / WHO ARE THESE GUYS AND WHY DO WE HAVE TO PAY THEM?\nAlmost all communities are playing music for resident listening, and/or TVs for resident viewing. However, most communities aren’t aware that if music or TVs are being played for residents, a license may be required, and if a license is required and the community doesn’t have one it is in violation of copyright law. Many communities are discovering that copyright licenses are required because more communities are being notified by performance rights organizations (PROs) that the community’s current music and TV practices are in violation of copyright law. Specifically, performance rights organizations are notifying apartment communities that they are infringing on copyrights and demanding that the communities start paying licensing fees.\nDoes your community need a license to play music or TVs? The short answer is probably yes. However, evaluation of licensing requirements can be complicated. Copyright licensing issues are further complicated by the fact that licensing rules are different for music and TV. Licensing requirements depend on the facts and circumstance of each case, what is being played, where it is being played, and how it is being played.\nA multifamily community can be involved with copyrighted material requiring a license in a variety of ways. Music from a host of sources may be being played at the community. Locations, where music is played, may include the leasing office, models, lounges, pools, fitness centers, business centers, game rooms, theatre rooms, and clubhouses. The community may be playing TVs in the fitness center and elsewhere. The community may be using social media and posting music videos or music to the Internet.\nIf you are playing music anywhere in public at your community, or have any TVs in public at your community, you may be required to pay copyright licensing fees to avoid infringing the artists’ copyright. Copyright infringement is the unauthorized use of another’s creative work. If you play music without a license, when one is required, it is copyright infringement. Lack of knowledge of license requirements or intent is irrelevant. You can’t defend copyright infringement based on ignorance or lack of intent. You should not ignore this issue and hope that it goes away because the penalties for copyright infringement are severe. Statutory damages range from $700 up to $30,000 per song performed without a license. Willful infringement could subject you to damages of up to $150,000 for each song performed without a license.\nIf music or television broadcasts are copyright protected, licenses are almost always required if publicly performed. Public performance is not limited to a live performance by the artist. If a song is played in public, then it is publicly performed. Under copyright law, publicly performed is much narrower than you would think. A public performance occurs where people gather. It doesn’t have to be a large gathering. If a gathering is more than a small circle of family or social acquaintances, then it is a public gathering. Similarly, any space that is open to more than a small circle of family or social acquaintances is a public space. For example, even though your fitness center is not open to the general public, your fitness room is a public space under copyright law, and any music played there would be considered to be publicly performed. Public performances also include any transmissions to the public, for example, radio or TV broadcasts, as well as content delivered over the Internet. Copyright owners have the exclusive right to publicly perform copyrighted music, and the legal right to compensation when anyone else publicly performs copyrighted material.\nPROs grant and administer licenses on behalf of copyright owners. Artists and copyright holders join PROs so that the PRO can enforce their copyrights. The three major PROs, that enforce copyrights in the United States, are ASCAP, BMI, and SESAC. ASCAP is the American Society of Composers, Author & Publishers; BMI is Broadcast Music, Inc.; and SESAC is the Society of European State Authors & Composers.\nUnder the law, PROs must offer licenses to anyone, and the PROs must charge all license seekers the same rates for the same or similar uses. Information, regarding PRO licensing rates, is available from individual PROs. One way PROs are executing their mission to enforce copyrights is by contacting businesses and demanding that they pay required licensing fees. PROs have been targeting the multifamily industry. Depending on what you’re playing, you may need multiple licenses, e.g. a music license with all three PROs. A music license with one PRO allows you to perform only copyrighted music represented by that PRO. Each songwriter or composer may belong to only one performing right organization at any given time, so each PRO licenses a unique repertoire of music. Thus, a BMI license doesn’t cover songs in SESAC’s catalog.\nWait a minute we only play music from satellite, TV, cable, or radio stations at our community. Aren’t they already paying the PROs licensing fees, so we don’t need another license? Various content providers may already be licensed with PROs. However, broadcast licenses don’t authorize the performance of such TV, cable, and radio to the public by businesses and other organizations. Generally, businesses must secure public performance rights for TVs or radios under certain circumstances. With respect to TVs, licenses must be secured if the business is playing more than four TVs; has more than one TV in any one room; if any of the TVs has a diagonal screen size greater than 55 inches; if any audio portion of the audiovisual performance is communicated by means of more than six loudspeakers, or four loudspeakers in any one room, or there are any speakers in an adjoining outdoor space. With respect to radio, licenses must be secured if the business is playing more than six loudspeakers; more than four loudspeakers in any one room or adjoining outdoor space; or playing music on hold.\nTo evaluate licensing requirements, you must answer where what, and how music or other copyrighted material is being played at your community. If music is not being publicly performed (played in public) at your community, then you don’t need a license. Keep in mind that the concept of public performance is broad. If the content of your music is not subject to copyright protection, then you don’t need a license. While there is a fairly extensive catalog of music that is not copyright protected, the reality is that any music your residents would want to hear is likely to be subject to copyright protection. In other words, your residents probably don’t want to hear 1928 elevator music. However, your music list or TV content may be limited to a particular PRO’s catalog. Thus, if you’re only playing BMI music, you don’t need to pay SESAC licensing fees as well. As discussed previously, the size of TVs, the number of TVs, and the number of speakers (both for TV and radio) can also factor into the analysis of licensing fees and rights.\nLicensing options and pricing varies widely depending on content and delivery. Music and TV licenses from the major PROs will run $200 per year and up, per community per PRO. Some alternatives to the PROs exist. Under the alternatives, the licensing passes through so that you can play them without requiring an additional license from a PRO. Such services include SiriusXM Business or Pandora Business, which run from $25 to $30 per month. Other alternatives include MoodMedia (previously Muzak) at about $200 per year (MoodMedia touts itself as the world leader in background music). Royalty free music collections are offered by stockmusic.com and streamlicensing.com ($130 to $200 per year). While these services offer an alternative to the PROs, they may be subject to limitations on content and delivery, and do not cover TV. If you’re going to go with an alternative to a PRO, you should carefully read the fine print for limitations.']"	['<urn:uuid:f3cea90a-50c9-492c-b99f-db14c22175e0>', '<urn:uuid:9be55007-eebb-4ce7-894c-a2933deb2272>']	open-ended	direct	long-search-query	distant-from-document	three-doc	expert	2025-05-13T01:23:33.086345	9	105	2041
95	expert dns deployment need steps propagate new records apache virtual hosts setup procedure	To propagate new DNS records for Apache virtual hosts, first create an A-Record with your hosting provider for your subdomain (like 'dev') pointing to the same IP as your main server. While DNS records propagate globally, create separate Apache virtual hosts for your subdomain. Configure the virtual host with admin email, server name, document root, and log file locations. After configuration, restart the Apache2 service. You can verify DNS propagation by checking if the subdomain is accessible in your browser. If not immediately visible, wait a few hours for DNS updates to complete.	"['In Part 1 of this article, we discussed setting up your git repository server. This post will explain how to use that server to deploy primary and development versions of your site.\nWe’re going to start on a non-git part of the setup: creating your development subdomain: Whenever you’re working on a new subdomain the first thing to do is actually create the A-Record with your hosting provider because these records can take a while to propagate around DNS. If you’re not familiar with A-Records they’re basically just a way for DNS to know where to send requests for any subdomains of your site; here’s Linode’s information on them other hosting providers will obviously have different settings but you’re basically looking to modify your DNS settings. For my development purposes, I created a ‘dev’ subdomain with the same IP as my main server and left TTL as Default. (I also created another one for this ‘blog’ subdomain, but that’s obviously not part of this guide.)\nWhile those new DNS records are being spread around the globe you’ll want to create separate Apache virtual hosts for your dev subdomain (again some Linode documentation). Your new Dev host should look something like this:\n# Admin email, Server Name (domain name), and any aliases\n# Index file and Document Root (where the public files are located)\nDirectoryIndex index.html index.php\n# Log file locations\nCustomLog ~<yourUsername>/public/<yourURL>/log/access.dev.log combined\nOnce you’re done with that remember to restart the Apache2 service so it sees your new config files. Assuming your DNS updates have propagated, you should now be able to see something when you point your browser at dev.<yourURL>. If you don’t, work on the steps below and check again in a few hours.\nAt this point I’m going to assume you already have some sort of working website, no matter how basic, that you want to keep running as is while you tinker with your dev version on the subdomain. If you don’t already have a site just create a basic index.html page and put it up at your main (non-subdomained) URL to get yourself started.\nNow you need to use the gitolite-admin control repo you set up in the first part of this guide to create a new repository for your site. I just called mine ‘site’, but a more descriptive name is probably better. Change your working directory on your Linux server to the DocumentRoot path to your dev site which you set in your Apache config files earlier, and run the following command:\ngit clone git@<yourGitServer>:site\nThis should clone the empty repository to your new development subdomain. Now copy all the files from your production site’s DocumentRoot path to your new development site DocumentRoot, add them all to git, commit them, tag your commit, and push them back to the repository.\ncp -r <prductionDocumentRoot>/* <prductionDocumentRoot>/.* .\ngit add .\ngit commit -a -m ""Initial Commit""\ngit tag ""v1.0""\nIf everything went right, the repo that you created in part 1 should now contain the entire contents of your site. To test pulling it to your production site, create a new folder in the same one as the DocumentRoot of this site, cd into it, and try to clone your site’s repo with the command:\ngit clone git@<yourGitServer>:site\nYou should end up with a second copy of all the files in your site with the addition of the .git folder which has the file history and your repo information, so go ahead and swap this new git-ified version of your site with the main version:\nmv <siteFolder> <siteFolder>.non-git-version\nmv <siteFolder>.gitversion <siteFolder>\nIf everything still works when you try your site, you’re in great shape. Hopefully your subdomain is working as an identical copy of your site now as well. If it still isn’t run the following command on your local machine to check what your DNS server is returning:\nBoth commands should return the same IP address, if not, something is wrong with your config or your DNS servers haven’t updated yet.\nI’d suggest a few small .htaccess file tweaks at this point. First, make sure your .git files can’t be seen by people visiting your site (you should probably apply this to both your dev and primary site’s .htaccess files, or change one and push/pull it to the other with git for practice). Second, on your dev site, you’ll want to limit access by IP address to just your own IPs so people can’t see your less-secure development code. Finally, tell git to ignore your .htaccess files in the future so that you dev version with IP restrictions doesn’t get pushed to your main site.\nWow, that took a while. Stay tuned for Part 3 where I finally describe how to set up Coda 2 with this mess.\nUpdate: still no Part 3, but Part 2.5 is up.\nLiked this post? Follow this blog to get more.']"	['<urn:uuid:56ff2c52-86bc-461a-8dc7-b3028827b7a6>']	open-ended	with-premise	long-search-query	similar-to-document	single-doc	expert	2025-05-13T01:23:33.086345	13	93	814
96	Why is seismic analysis important and what are its benefits?	Seismic analysis is crucial for evaluating structural safety against possible earthquakes and forms an essential step for retrofitting. For instance, in the Taj Mahal's analysis, dynamic analysis was conducted using 3D mathematical models for fixed and flexible base conditions, studying free vibration characteristics and seismic response for bending moments, stresses, and forces. The benefits include protection of life and property, as seismic evaluation helps improve structural performance and determines the need for retrofitting to prevent catastrophic structural failures reported in past major earthquakes.	"[""There is an increasing awareness to protect historical monuments, in future earthquakes. Taj Mahal is one such monument known for its cultural heritage worldwide and must be protected from damage. The strength evaluation of the structure taking into account future earthquake forces besides gravity and other loads is of concern for determining its safety. The seismic evaluation forms the essential step for retrofitting of such structures under occasional earthquake loads. In the present study, dynamic analysis of the Taj Mahal monument has been carried out on the basis of two simplified 3D mathematical models for fixed and flexible base condition using IS code and site dependent spectra. The free vibration characteristics and seismic response for bending moments, bending stresses, torsional moments, shear forces and axial forces have been studied.\nTaj Mahal; monument; 3D mathematical model, setsmtc analysis; response spectra; seismic response; safety evaluation\nThe Taj Mahal in one of the finest architectural monuments of the world and has been included in the cultural treasures of the world heritage. Because of the increasing awareness to protect the historical monuments and its importance, seismic safety evaluation of the marble mausoleum is of great value for its future safety against possible earthquakes.\nTHE TAJ MAHAL MONUMENT\nThe monument lies in the seismic zone lii of India on the bank of river Yamuna in the historical city of Agra. The structure has survived small to moderate earthquakes in its life. The Taj Mahal monument (Figure 1) consists of a central main dome resting on two storey thick brick masonry walls square in plan, four small domes each at corner with four 40m high independent minars at the four corners. The whole monument is resting on large platform of thick brick masonry raft which is approximately ISm above the ground level. The center to center distance of columns is 32m and the storey height is measured as 11 m. The foundation soil consists of a deep alluvial deposit.\n|Figure 1: Sectional Elevation of Taj Mahal|\nMODELING OF THE STRUCTURE\nDynamic analysis of the monument has been carried out for two simplified 3D mathematical models (i) fixed base and (ii) flexible base conditions. The first model is considered fixed at the center of raft while other is considered to be resting on soil springs. Figure 2 shows the 3D mathematical model of the complete structure, each portion of structure wall/ dome are represented by assemblage of beam elements with 6 degree offreedom at each node.\nMODELING OF THE FOUNDATION\nType of Soils\nThere is sandy soil strata beneath the monument having the weight densities 1.8 ton/m3 and 1.6 ton/m3 at masjid site and mehman-khana site respectively. The Poisson's ratio is 0.25. The sandy soil is followed by clayey soil. Both the sandy and clayey soil strata beneath the monument are submerged. No material change in the mechanical properties of the various soil layers would therefore occur due to further impoundment of water in the river Yamuna.\nType of Foundation\nThe raft foundation is solid square platform measuring approximately l00m x l00m in plan and 35m thick The portion of the raft above the ground level is about 18m. The raft is built in brick masonry consisting of thick fire burnt clay bricks and mortar joints of varying thickness. The platform rests on thick sandy layer followed by clayey layer underneath.In the two mathematical models the bottom most vertical members are assumed to be rigid and half the weight of the raft is lumped at the four bottom nodes\nBased on the meager data published in the literature, soil spring constants for an embedded rnassless rigid rectangular foundation which forms a prism have been used. The increase in stiffness for embedment leads to a factor with which the value for surface foundation is multiplied and which is for surface foundation equal to one. For three different values of shear wave velocities, different soil spring constants are calculated and one fourth of each spring constant is used at all the four bottom most nodes as shown in Fig. 2.\n|Fig. 2: Mathematical model of the monument with springs at the bottom nodes|\nMaterials used in the Taj Mahal are sand stone, brick work in lime mortar and marble for cladding. Modulus of elasticity, Poisson's ratio and unit weight densities of all the materials are given in Table 1. The unit weights of soils at masjid site is 1.8 tonfm3 and at mehman-khana site is 1.6 tonfm3. The unit weight of soil for analysis purposes is taken as 1. 7 tonfm3, the average of weight densities at the above two sites. The Poisson's ratio for the soil is taken as 0.25. The analysis has been carried out for three different shear wave velocities, 158 m/sec, 300 rnlsec and 600 m/sec and shear modulii have been calculated accordingly.\n|Table 1: Material Properties of Taj Mahal Monument|\nThe site dependent spectra has been worked out (EQ 92-1 0) at the site of Taj Mahal monument, taking into consideration local soil conditions, geology and seismic history. The zero period acceleration for this spectra is 0.2g while it is 0.1g for code response spectra. The response spectrum analysis has been carried out for both fixed base and flexible base models using IS code spectra and site dependent spectra. For fixed base condition damping of the structure has been taken to be 7% of critical and for flexible base condition 10%. As the city of Agra falls in seismic zone Ill, zone factor is taken as 0.2, importance factor as 1.5 and soil foundation factor as 1.2 for the analysis using code spectra. The digitized values of spectral acceleration (Sa/g) for IS code as well as site dependent spectra are given in Table 2 both for 7% and 10% damping.\n|Table 2: Digitized values of spectral acceleration coefficients (Sa/g) for IS code and site dependent spectra|\nRESULTS OF ANALYSIS\n|Table 3: Time periods of the complete structure for fixed and flexible base conditions|\nThe dynamic displacement& of the structure at critical points namely top and bottom of main dome and at the top of small dome for fixed and spring base conditions are given in Table 4 and 5 using code spectra and site dependent spectra respectively. The maximum displacement is at the top of small dome and is equal to 0.87 cm and 8.44 cm for code and site dependent spectra respectively. While at the top of main dome the displacement is 0.29 cm and 2. 77 cm for code and site dependent spectra respectively. The displacement at top of small dome is more than the top of main dome which is due to the fact that the main dome is more rigid than small dome.\n|Table 4. Displacement& at critical locations in X-direction using code spectra|\n|Table 5. Displacements at critical locations in X-direction using site dependent spectra|\nThe bending stresses at the bottom of main and small domes for fixed and spring base conditions are given in Table 6 and 7 using code spectra and site dependent spectra respectively. It is observed that stresses developed are more in fixed base model as compared to the three cases of the spring base model.\n|Table 6. Bending stresses at critical sections using code spectra|\n|Table 7. Bending stresses at critical sections using site dependent spectra|"", 'Seismic retrofitting is the modification of existing structures so as to improve the system behaviour or its components repair or strengthening up to the performance is expected. The retrofitting of a building requires an appreciation for the technical, economic and social aspects of the issue. Choosing the optimal solution depends on a large variety of criteria, the most important being the total cost, period of construction, the ease of technologies application etc. Furthermore, the seismic behaviour of the buildings is affected due to design efficiency, construction deficiency, additional loads, additional performance demand etc. A case study is conducted in Mani Mandir complex located in Gujarat, to find the impact of various retrofitting techniques. A comprehensive retrofit program was formulated. Conservative principles, minimum intervention and consonance with the heritage character of the building were important considerations in selecting the retrofit program. The retrofit measures include providing a rigid diaphragm behaviour mechanism in existing slabs, introducing stainless steel reinforcement bands in the existing masonry walls, cross pinning and end pinning in walls and pillars and strengthening of arches and elevation features.\nEarthquake creates destruction in terms of life, property and failure of structures. In order to protect from the risk triggered by seismic disaster to the life and property, the performance of the structure must be improved and thus Seismic Retrofitting plays its role. Retrofit involves modifications to existing structures that may improve energy efficiency or decrease energy demand. Seismic retrofitting is the modification of existing structures so as to improve the seismic behaviour or its components repair or strengthening up to the performance it is expected. Retrofitting also proves to be a better option catering to the economic considerations and immediate shelter problems rather than replacement of seismic deficient buildings. Two alternative approaches are conceptually adopted and implemented in practice for seismic retrofitting. The first approach focusses on upgrading the structure to resist earthquake induced forces (i.e. modifying the capacity) and is called Conventional method of retrofitting. The second approach focusses on reduction of earthquake induced forces (i.e. modifying the demand) or Unconventional approach. Seismic retrofitting is the collection of modern techniques for earthquake resistant structure.\nThe presence of soft and weak storey at the open ground floor, in-plane discontinuity out-of- plane offset of the ground floor columns and eccentric mass are commonly observed irregularities in the studied buildings. In absence of collector elements in the slab and proper detailing of the connections with the building frame, there is lack of integral action of the lateral load resisting elements, techniques for earthquake resistant structure. The seismic performance of beam-column joints in an RC framed structure has long been recognized as a dominant factor that affects its overall behaviour when subjected to earthquake forces, as indicated in earlier version of design codes and standards. Unsafe designs and deficient detailing that does not conform to seismic codes within the joint region may result in extra inelastic story drift and excessive post-yield rotation, which likely causes local failure, and may even lead to progressive collapse. The potential problems associated with the design deficiencies of the beam-column joints have been identified in many catastrophic structural failures reported in past major earthquakes.\nFour major objectives are identified to understand the feasibility of seismically retrofitting existing structures. The first objective is to investigate how building location affects the annual probability of attaining or exceeding specified performance levels. The second objective is to develop a framework to determine the economic feasibility of seismic retrofitting. The third objective is to study the effects that achievable loss reduction, investment return period and retrofitting. The final objective is to determine the impact of a modest retrofit strategy applied to identical example buildings.\nTo provide safety to the occupants by reducing the of structural collapse during severe earthquakes. This can be done by strengthening the columns and joints so that their flexural and shear capacities will be adequately stronger. Retrofit strategy refers to options of increasing the strength, stiffness and ductility of the elements or buildings as whole. Several retrofit strategies may be selected under a retrofit scheme of a building.']"	['<urn:uuid:f1de9955-9427-4349-9a8a-0711523ea41d>', '<urn:uuid:3ffebfc4-2fde-44f6-93bb-f4766881b9c2>']	open-ended	direct	concise-and-natural	distant-from-document	multi-aspect	expert	2025-05-13T01:23:33.086345	10	83	1891
97	campfire songs activities safety precautions	While campfire songs like 'This Land Is Your Land' and 'Home On The Range' are great activities for building camp spirit, safety precautions are essential. Campfires must only be built in designated areas like fire rings, and campers should keep water nearby, maintain a reasonable fire level, and completely extinguish fires with water before sleeping. All gear and debris should be kept at a safe distance from flames.	['Summer Camp Songs\nSpending time around the campfire telling scary stories, reminiscing, and eating marshmallows is one of the best parts of camping. It’s also the perfect time to have some fun by singing Summer Camp Songs.\nSinging Summer camp songs is a great activity for many reasons. Songs can build camp spirit, give people a chance to express themselves, and help the group create some great memories. They also give you a chance to share your favorite songs with the group and show off your magnificent voice!\nHowever, to really get the most out of singing around the campfire, you need to find some songs that everyone knows and enjoys. In this post, I’ll be sharing some classic Summer Camp Songs that will make your next outing much more fun.\nIntroducing new songs to the group\nIf you have children in the group, it’s often a good idea to incorporate some actions into your songs. This will help them memorize the lyrics faster and can make singing even more enjoyable.\nSo, when you sing a song like “This Land Is Your Land” and have lyrics like:\nAs I was walking that ribbon of highway,\nI saw above me that endless skyway:\nI saw below me that golden valley:\nThis land was made for you and me.\nYou can use hand gestures that indicate driving for the ribbon of highway, point at the sky for the endless skyway, make a valley gesture with your hands for golden valley, then point at each other for the last line. It really brings some songs to life.\nIf a song has a lot of verses, like the original version of This Land Is Your Land, it sometimes helps to just learn 3 or 4 verses initially. You can then repeat the verses they know to memorize them before gradually introduce new verses.\nAnother great option is sing songs in a round, so the first person starts the song, then after the first line, the second person starts the song. It can be quite fun once everyone gets the hang of it. Here is a video of some girls singing Row, Row, Row Your Boat using this technique.\nThe Best Summer Camp Songs\n#1 – This Land Is Your Land\nThis is one of the most famous folk songs ever written. It was penned by Woodie Guthrie in 1940, using an existing melody from the song “When the World’s on Fire”. Bruce Springsteen one called This Land Is Your Land The greatest song ever written about America. Listen to the original song and check out the lyrics.\n#2 – There’s a Hole In the Bucket\nThis is a classic children’s song that was developed from a German folk song written in the 17th Century. The song is actually a discussion that occurs between two characters, Liza and Henry, about how to fix a hole in a bucket.\nYou could make this song more fun by having all of the males in your group sing the Henry part and having the females sing the Liza part. This Wikipedia page features the lyrics and a recording of the melody. You can also check out a great version of this song recorded by the Sesame Street.\n#3 – The Llama Song\nAlthough The Llama Song is one of the strangest Summer camp songs every created, it is definitely worth learning! It is strange because of the bizarre lyrics about llamas and other animals. The are several versions of this song, but the most common uses these lyrics and hand gestures. This video shows another version which sings about other animals as well.\n#4 – Camp Granada (Hello Muddah, Hello Fuddah)\nCamp Granada is a classic song that has been enjoyed by Summer campers for decades. It is a parody song written by Allan Sherman and Lou Busch in the 1960’s. The lyrics are based on the complaint letters that Allan received from his child while at Summer camp. The letters made him laugh so much that he made this hilarious song.\n#5 – Wheels On the Bus Song\nThis is a great song to sing if the kids are riding the bus to camp. Put this over the radio and encourage kids to sing along to the the lyrics.\n#6 – The Green Grass Grew All Around\nThe Green Grass Grew All Around is a wonderful song to sing along with younger children because they can simply repeat the lyrics as they are sung by an adult. As the song progresses, it becomes more complex and interesting. The song is a great choice for camping because it also encourages children to look at their surroundings and think about the many exciting things in nature. Listen to The Green Grass Grew All Around.\n#8 – Kookaburra\nAlthough “Kookaburra” being about an Australian animal, it is often enjoyed by campers in the United States and Canada. The song was written by Marion Sinclair in 1932, a primary school teacher living in Victoria. The song is easy to learn and perfect for kids of all ages. Check it out.\n#9 – Home On The Range\nThis classic song will be well known to older Americans, but there are many younger people who haven’t been introduced to it yet. If you can strum a few chords on the guitar, it is the perfect song to sing while sitting around a campfire. Listen to Home On The Range.\n#10 – Reese’s Peanut Butter Cup\nThis is a simple repeat-after-me song that kids love. The lyrics are easy to learn and the whole camp will be singing (and dancing) along in no time. Listen to Reese’s Peanut Butter Cup.\n#11 – Oh What A Beautiful Morning\nOh What A Beautiful Morning is the perfect track to be sung in the morning upon waking. It really gets the day off to an exciting start and gets everyone into a great mood. View the lyrics here and listen to Hugh Jackman performing it here.\n#12 – Bed Bug Song\nBoth kids and adults will have a great time singing the bed bug song. The song first became popular in Beech Bluff, Tennessee in the 1920s and quickly spread across the rest of the United States. There are a few alternate lyrics available and you can even add some custom verses yourself! Here is one set of lyrics and a recording of the song.\n#13 – Herman The Worm\nThis slightly strange song remains very popular in Summer camps in the United States. The song is about Herman, a worm who apparently eats other worms (or other animals), depending on the lyrics you use. You can see the lyrics here or watch a performance here.\n#14 – Camptown Races\nEveryone has sung this song at some point in their life! It remains one of the most popular Summer camp songs thanks to its catchy melody and simple lyrics. It is actually a very old song that was published in 1850 by an American singer named Stephen Foster. Foster was one of the most famous songwriters of his time and the quality of this tune says a lot about his skill. Listen to Camptown Races.\n#15 – If You’re Happy\nThis is a classic sing along song for kids. It’s been around since the 1950’s and is dangerously catchy. Listen to the full version of If You’re Happy here.\n#16 – Down By The Bay\nAlthough Down By The Bay was written as a children’s song, it is so enjoyable to listen to that many adults also love it. It’s unknown who wrote the tun, but it was famously performed by Raffi on his album Singable Songs for the Very Young which was released in 1976. Here is a video of Raffi performing his version of the song.\n#17 – She’ll Be Coming Around The Mountain\nThis is a slightly dangerous Summer camp song because of how addictive the melody is. You will struggle to get the tune out of your head after singing it a few times! It is actually a traditional folk song that was first published in 1927 with a melody adapted from the African-American gospel song “When the Chariot Comes”. Listen to She’ll Be Coming Around The Mountain.\n#18 – Take Me Home, Country Roads\nTake Me Home, Country Roads is a classic American song written by Bill Danoff, Taffy Nivert, and John Denver. The song is about the state of West Virginia and is considered an anthem in most of the country.\n#19 – Boom-Shika-Boom\nThis is a hilarious call-and-repeat song from The Learning Station. It can even be made better by incorporating some percussion and playing a funky beat as the group sings along. Check out this video to see Boom-Shika-Boom in action.\n#20 – Oh My Darling, Clementine\nOh My Darling, Clementine is a an American folk ballad that was written by Percy Montrose in 1884. It is a very catchy song that has an easy-to-remember chorus that children enjoy singing. Listening to Oh My Darling, Clementine.\nI hope you enjoyed our article on Summer Camp Songs! For more fun camp activities, bookmark the site.', 'Respecting nature should be a top priority anytime you relax or play in the great outdoors. Camping safety and protecting yourself from common outdoor hazards should be just as important. Each year, thousands of accidents occur in the outdoors as a result of camper and backpacker carelessness. Protect yourself with these camping safety tips.\nCampfire negligence is a leading cause of forest fires and also causes many camping injuries and deaths. Only build your campfires in designated areas. Most campgrounds provide fire rings or pits for campfires or you can use a Coleman fireplace to safely contain your campfire. Keep the area surrounding your campfire free of trash and other debris that could ignite and set your tent, chairs, and other gear a safe distance from the flame.\nMany campgrounds and state parks provide wood for campfires for a small price, but supplies are often low during peak seasons. Bring your own dry firewood to avoid taking wood from the natural area. If you do gather firewood at your campsite, only take pieces already on the ground. Never cut trees or branches within the campground or park and find your wood away from your campsite.\nBegin your fire with a small stack of twigs. Aromatic cedar and pine and other soft woods burn quickly and make excellent fire starters. Use a match to light the dry sticks and add larger pieces of dry hardwood as the campfire strengthens. Burn any garbage or waste that will burn to reduce the amount of waste you carry out and to limit your use of firewood. Before leaving your campsite, completely extinguish your campfire using water. Stir the ashes and pour more water on the embers. Make sure the ashes are cool to the touch before leaving the campsite and remove any debris that didn’t burn.\nAlways keep water close by whenever you have a campfire. Breezes and wind can cause embers to spread quickly. Keep the campfire at a reasonable level and never leave it unattended. Put your campfire out before going to sleep each evening.\nThe wild animals you may encounter in the great outdoors can be beautiful and amazing to watch, but can also be quite harmful and even deadly if you’re not careful. Anytime you see a wild animal, observe the animal from a safe distance. Never try to feed a wild animal and avoid sudden movements or closing in on the animal. Report any strange wildlife behavior to the park ranger to warn of possible rabies.\nTo reduce the risk of bears or other animals in your campsite, never leave food and drinks out in the open. Cover all your food and keep the containers inside your car or suspended from a tree out of animal reach. Garbage should be burned, immediately disposed of, or suspended in the air as well. To prevent bears, hang your bags at least 12 feet high and 4 feet away from the trunk of the tree. Avoid storing food inside your tent and take out any snacks before bedtime.']	['<urn:uuid:3f0043d9-23dc-4a1b-9512-e99f7672efb2>', '<urn:uuid:9ad5c601-d89f-4098-86e8-9bef76788db1>']	factoid	direct	short-search-query	similar-to-document	multi-aspect	expert	2025-05-13T01:23:33.086345	5	68	2031
98	What are the key factors driving the geographic concentration of laser industry development, and how does this compare to pollution-related mortality patterns across different regions?	The laser industry tends to cluster geographically due to regional knowledge in laser sources and the presence of universities with relevant departments. Meanwhile, pollution-related mortality shows stark geographic patterns, with 92% of pollution-related deaths occurring in developing countries. India and China have the highest absolute numbers of pollution-related deaths (2.5 and 1.8 million respectively), although both developed nations like Russia and the US are also among the top ten affected countries.	"['Regional Knowledge and the Emergence of an Industry: Laser Systems Production in West Germany, 1975-2005\nWe analyze the emergence and spatial evolution of the German laser systems industry. Regional knowledge in the related field of laser sources, as well as the presence of universities with physics or engineering departments, is conducive to the emergence of laser systems suppliers. The regional presence of source producers is also positively related to entry into laser systems. One important mechanism behind regional entry is the diversification of upstream laser source producers into the downstream systems market. Entry into the materials processing submarket appears to be unrelated to academic knowledge in the region, but the presence of laser source producers and the regional stock of laser knowledge are still highly predictive in this submarket.\n|Date of creation:||15 Nov 2010|\n|Contact details of provider:|| Postal: Carl-Zeiss-Strasse 3, 07743 JENA|\nPhone: +049 3641/ 9 43000\nFax: +049 3641/ 9 43000\nWeb page: http://www.jenecon.de\nMore information through EDIRC\nPlease report citation or reference errors to , or , if you are the registered author of the cited work, log in to your RePEc Author Service profile, click on ""citations"" and make appropriate adjustments.:\n- Guido Buenstorf & Steven Klepper, 2009.\n""Heritage and Agglomeration: The Akron Tyre Cluster Revisited,""\nRoyal Economic Society, vol. 119(537), pages 705-733, 04.\n- Guido Buenstorf & Steven Klepper, 2005. ""Heritage and Agglomeration: The Akron Tire Cluster Revisited,"" Papers on Economics and Evolution 2005-08, Philipps University Marburg, Department of Geography.\n- Björn Alecke & Christoph Alsleben & Frank Scharr & Gerhard Untiedt, 2006. ""Are there really high-tech clusters? The geographic concentration of German manufacturing industries and its determinants,"" The Annals of Regional Science, Springer;Western Regional Science Association, vol. 40(1), pages 19-42, March.\n- Guido Buenstorf, 2007. ""Evolution on the Shoulders of Giants: Entrepreneurship and Firm Survival in the German Laser Industry,"" Review of Industrial Organization, Springer;The Industrial Organization Society, vol. 30(3), pages 179-202, May.\n- Guido Buenstorf, 2006. ""Evolution on the Shoulders of Giants: Entrepreneurship and Firm Survival in the German Laser Industry,"" Papers on Economics and Evolution 2005-20, Philipps University Marburg, Department of Geography.\n- Buenstorf, Guido & Klepper, Steven, 2010. ""Why does entry cluster geographically? Evidence from the US tire industry,"" Journal of Urban Economics, Elsevier, vol. 68(2), pages 103-114, September.\n- Anselin, Luc & Varga, Attila & Acs, Zoltan, 1997. ""Local Geographic Spillovers between University Research and High Technology Innovations,"" Journal of Urban Economics, Elsevier, vol. 42(3), pages 422-448, November.\n- Diego Comin & Bart Hobijn, 2011. ""Technology Diffusion and Postwar Growth,"" NBER Chapters, in: NBER Macroeconomics Annual 2010, Volume 25, pages 209-246 National Bureau of Economic Research, Inc.\n- Diego Comin & Bart Hobijn, 2010. ""Technology diffusion and postwar growth,"" Working Paper Series 2010-16, Federal Reserve Bank of San Francisco.\n- Diego A. Comin & Bart Hobijn, 2010. ""Technology Diffusion and Postwar Growth,"" Harvard Business School Working Papers 11-027, Harvard Business School.\n- Diego A. Comin & Bart Hobijn, 2010. ""Technology Diffusion and Postwar Growth,"" NBER Working Papers 16378, National Bureau of Economic Research, Inc.\n- Diego Comin & Bart Hobijn & Emilie Rovito, 2008. ""Technology usage lags,"" Journal of Economic Growth, Springer, vol. 13(4), pages 237-256, December.\n- Ron A. Boschma & Rik Wenting, 2004. ""The spatial evolution of the British automobile industry,"" Papers in Evolutionary Economic Geography (PEEG) 0504, Utrecht University, Section of Economic Geography, revised Aug 2004.\n- Guido Buenstorf & Matthias Geissler, 2011. ""The origins of entrants and the geography of the German laser industry,"" Papers in Regional Science, Wiley Blackwell, vol. 90(2), pages 251-270, 06.\n- Guido Buenstorf & Matthias Geissler, 2008. ""The Origins of Entrants and the Geography of the German Laser Industry,"" Papers on Economics and Evolution 2008-14, Philipps University Marburg, Department of Geography.\n- Michael S. Dahl & Christian Ø.R. Pedersen & Bent Dalum, 2003. ""Entry by Spinoff in a High-tech Cluster,"" DRUID Working Papers 03-11, DRUID, Copenhagen Business School, Department of Industrial Economics and Strategy/Aalborg University, Department of Business Studies.\n- Mario Cleves & William W. Gould & Roberto G. Gutierrez & Yulia Marchenko, 2010. ""An Introduction to Survival Analysis Using Stata,"" Stata Press books, StataCorp LP, edition 3, number saus3, September.\n- Jurgen Egeln & Sandra Gottschalk & Christian Rammer, 2004. ""Location Decisions of Spin-offs from Public Research Institutions,"" Industry and Innovation, Taylor & Francis Journals, vol. 11(3), pages 207-223.\n- Audretsch, David B & Stephan, Paula E, 1996. ""Company-Scientist Locational Links: The Case of Biotechnology,"" American Economic Review, American Economic Association, vol. 86(3), pages 641-652, June. Full references (including those not matched with items on IDEAS)', 'pollution might just be humanity’s most pressing problem.\nMore than 9 million deaths were linked to “polluted air, water and soil,” in 2015, according to an explosive new report published in The Lancet, a UK medical journal. To provide some context, this means that pollution accounted for 16 percent of all deaths worldwide, “three times more deaths than from AIDS, tuberculosis, and malaria combined and 15 times more than from all wars and other forms of violence,” according to the report’s executive summary. This figure, as the study’s authors point out, may in fact be conservative, and could be millions higher, “because the impact of many pollutants are poorly understood.”\nThe study drew from the work done in previous studies that “show how pollution is tied to a wider range of diseases than previously thought,” including asthma, cancer, heart and lung disease, and neurological disorders. The vast majority of deaths – a full 92 percent, according to the report – took place in undeveloped or developing countries, a shocking statistic that nevertheless still provides some hope: the study’s authors say that “the big improvements that have been made in developed nations in recent decades show that beating pollution is a winnable battle if there is the political will.”\nOther highlights from the study:\n- Deaths from air pollution “are on track to double by 2050” in southeast Asia;\n- The costs of pollution are $4.6 trillion each year, according to the study’s estimate – “equivalent to more than 6% of global GDP”;\n- Of the various forms of pollution, toxic air contributed the greatest number of deaths, at 7.4 million, water pollution (this means sewage, most often) resulted in 1.8 million deaths, and workplace pollution, “including exposure to toxins, carcinogens and secondhand tobacco smoke,” resulted in 800,000 deaths.\n- Although the highest rates of pollution death are found among small developing countries, it’s India, with 2.5 million deaths, and China, with 1.8 million, that have the greatest number of deaths linked to pollution. Both Russia and the US are in the top ten as well.\nAccording to lead author Dr. Philip Landrigan, a pediatrician and professor of environmental medicine and global health at the Icahn School of Medicine at Mount Sinai, stricter regulation of toxic chemicals can often benefit economies, which “puts the lie to what we hear that controlling pollution is going to kill jobs.” As an example, Landrigan cites the passage in the US of the Clean Air Act, which has brought air pollution levels down 70 percent in the intervening years, even as GDP has increased 250 percent.\n“Pollution is one of the great existential challenges of the Anthropocene era. Pollution endangers the stability of the Earth’s support systems and threatens the continuing survival of human societies.”\nThe Lancet Commission on pollution and health\nThe Lancet Commission on pollution and health\nThe study’s conclusions, along with a just-released report by the nonpartisan Government Accountability Office (GAO) which “found that costs [linked to climate change] may rise to as much as $35 billion per year by 2050,” are surfacing at a particularly transformative moment for the EPA – established the same year, 1970, that the Clean Air Act was amended to give the federal government much greater power to regulate chemical use. Administrator Scott Pruitt, who has not accepted scientific consensus around climate change, and who pressed President Donald Trump to pull out of the Paris Climate Accord, has “embarked on record-setting rollbacks,” which include “plans to repeal pollution in the nation’s waterways [and] delaying rules requiring fossil fuel companies to rein in leaks of methane and greenhouse gases.” Pruitt also refused to ban a farm pesticide, chlorpyrifos, that poses a health risk to children and farm workers, and that has already, since 2000, been banned from most household use. The list goes on: the EPA under Pruitt has moved to suppress research into – and even mention of – climate change, including removing references on its website, canceling public talks, and removing from advisory roles scientists who’ve received federal grants for studies, and politicizing the issue by creating “red team-blue team” exercises between scientists who doubt climate change and those who don’t. Such exercises create false equivalences on a topic that’s already got near-unanimous scientific consensus. Pruitt has also moved to repeal the Obama administration’s signature climate regulation, the Clean Power Plan, in an effort to bolster the coal industry, long in decline due to the rise of natural gas and renewables, and has tasked his agency to respond to permit requests in six months or fewer, which means big, complicated projects with difficult-to-predict outcomes could get the greenlight without receiving thorough vetting.\nWhat this amounts to, in the eyes of many folks on the left, is a full-on assault on the environment, but Pruitt and co. see themselves as reformers, trimming away at bureaucratic fat and demanding the science around disputed chemicals be utterly airtight before acting to regulate, rather than allowing what Dr. Nancy Beck, one of Pruitt’s top deputies, calls “phantom risks” to bottleneck industry. It’s a typically Republican approach, maybe more destructively so in this administration than in previous ones, although this may just be because there’s more to destroy: the Obama administration significantly expanded the role of the EPA in its efforts to address climate change, and also took a rather-safe-than-sorry approach to chemical regulation which, in light of The Lancet report, may prove to be the right one. As Wendy Cleland-Hamnett, who until her retirement last month was “the agency’s top official overseeing pesticides and toxic chemicals,” asserts,\nYou are never going to have 100 percent certainty on anything, but when you have a chemical that evidence points to is causing fatalities, you err more on the side of taking some action, as opposed to ‘Let’s wait and spend some more time and try to get the science entirely certain,’ which it hardly ever gets to be.\nThe New York Times’s The Daily podcast posted a fantastic episode on Tuesday about the perceptions each side of the EPA debate has of itself, which you can find here.\nA new study summarizing observations “across 63 German nature reserves” reveals a disturbing 76 percent decline in flying insect biomass.\n- The study’s authors aren’t sure why this happened, but speculate that pesticide use in adjacent areas, coupled with climate change, has played a significant role.\n- The authors also believe their findings to “be representative of much of Europe and other areas of the globe.”\n- A key quote: “Lack of insects is very likely to be detrimental to the entire ecosystem…insects play a crucial role in ecosystems, being responsible for plant pollination and nutrient recycling as well as acting as a food source for animals such as birds, amphibians, reptiles, bats and small animals.”\nAn 11-year-old motivated by Flint’s still-polluted (honestly, how?) water invented a lead-detecting device.\n- From NPR’s description of the device: “there is a disposable cartridge containing chemically treated carbon nanotube arrays, an Arduino-based signal processor with a Bluetooth attachment, and a smartphone app that can display the results.”\n- What we’d like to know: how does an 11-year-old get her hands on carbon nanotubes?\nChina’s new ban on 24 categories of recyclables and solid waste could mean that much of what we recycle ends up in the trash.\n- Why? “China is the dominant market for recycled plastic,” according to Christian Cole in The Conversation. Here in the US, we export around 1.4 million tons of recycled plastic to China.\n- Double bonus: a tiny Montana company with links to Secretary of Interior Ryan Zinke just landed a $300 million contract to rebuild Puerto Rico’s electrical infrastructure. Whitefish Energy Holdings, which had just two employees when Hurricane Maria made landfall last month in Puerto Rico, is based in Zinke’s hometown and owned by a friend of his.\nThe first-ever floating wind turbines—these five turbines off the coast of Scotland have the potential to power, at peak, up to 20,000 homes.\nAccording to a brand new study in Nature Communications, “water rose rapidly, in punctuated bursts, rather than gradually over time” during the last global warming period more than 10,000 years ago. The researchers “suggest[ed] these past events could be viewed as a kind of ‘analog’ for the future.”\n- Double bonus: a new story in The Atlantic, summarizing the results of a different study, would seem to confirm this: “Climate Change Will Bring Major Flooding to New York Every 5 Years”.\nLet’s end this on a positive note: here’s a livestream from the inside of a little penguin burrow.\nMakayla Esposito contributed to this report.\nheader image: ""pollution,"" possan / flickr']"	['<urn:uuid:8615eac9-65d0-4fca-8133-da478a6d81b6>', '<urn:uuid:5467784a-63f2-4851-b930-f9c8cfc042fd>']	factoid	direct	verbose-and-natural	similar-to-document	multi-aspect	expert	2025-05-13T01:23:33.086345	25	71	2199
99	global drug policy reform recommendations health effects criminalization	Reform recommendations include stopping criminalization of drug use/possession and prioritizing health interventions, while the health effects of drug abuse include cardiovascular issues, sleep disturbances, and cognitive impairment. The current punitive approach must be replaced with evidence-based policies focused on public health, while addressing the medical consequences like potential heart disease, seizures, and mental health disorders.	"['A number of world leaders call for ending criminalization of drug use and possession and responsible legal regulation of psychoactive substances\nFormer Presidents of Brazil, Chile, Colombia, Mexico, Poland, Portugal and Switzerland Join With Kofi Annan, Richard Branson, George Shultz, Paul Volcker And Others To Make Bold, New Recommendations for Major Paradigm Shift in Global Drug Policy on the Road to UN Special Session on Drugs in 2016\nNote: the opinions expressed here are those of the authors and do not necessarily reflect the positions of Dianova International\nOpinion – ""Taking Control"": Pathways to Drug Policies that Work\n(…) A new and improved global drug control regime is needed that better protects the health and safety of individuals and communities around the world. Harsh measures grounded in repressive ideologies must be replaced by more humane and effective policies shaped by scientific evidence, public health principles and human rights standards. This is the only way to simultaneously reduce drug-related death, disease and suffering and the violence, crime, corruption and illicit markets associated with ineffective prohibitionist policies. The fiscal implications of the policies we advocate, it must be stressed, pale in comparison to the direct costs and indirect consequences generated by the current regime.\n(…) The obstacles to drug policy reform are both daunting and diverse. Powerful and established drug control bureaucracies, both national and international, staunchly defend status quo policies. They seldom question whether their involvement and tactics in enforcing drug policy are doing more harm than good. Meanwhile, there is often a tendency to sensationalize each new “drug scare” in the media. And politicians regularly subscribe to the appealing rhetoric of “zero tolerance” and creating “drug free” societies rather than pursuing an informed approach based on evidence of what works. Popular associations of illicit drugs with ethnic and racial minorities stir fear and inspire harsh legislation. And enlightened reform advocates are routinely attacked as “soft on crime” or even “pro-drug.”\nThe good news is that change is in the air. The Global Commission is gratified that a growing number of the recommendations offered in this report are already under consideration, underway or firmly in place around the world. But we are at the beginning of the journey and governments can benefit from the accumulating experience where reforms are being pursued. Fortunately, the dated rhetoric and unrealistic goals set during the 1998 UNGASS on drugs are unlikely to be repeated in 2016. Indeed, there is growing support for more flexible interpretations and reform of the international drug control conventions aligned with human rights and harm reduction principles.\nTaking Control makes seven recommendations which can be summarized as follows:\n- Put health and community safety first through a fundamental reorientation of policy priorities and resources, from failed punitive enforcement to proven health and social interventions.\n- Ensure equitable access to essential medicines, in particular opiate-based medications for pain.\n- Stop criminalizing people for drug use and possession – and stop imposing “compulsory treatment” on people whose only offense is drug use or possession.\n- Rely on alternatives to incarceration for non-violent, low-level participants in illicit drug markets such as farmers, couriers and others involved in the production, transport and sale of illicit drugs.\n- Focus on reducing the power of criminal organizations as well as the violence and insecurity that result from their competition with both one another and the state.\n- Allow and encourage diverse experiments in legally regulating markets in currently illicit drugs, beginning with but not limited to cannabis, coca leaf and certain novel psychoactive substances.\n- Take advantage of the opportunity presented by the upcoming UNGASS in 2016 to reform the global drug policy regime.\nCommission Members: Kofi Annan, former Secretary General of the United Nations and chair of the Kofi Annan Foundation, Ghana – Louise Arbour, former UN High Commissioner for Human Rights, Canada – Pavel Bém, former Mayor of Prague, Czech Republic – Richard Branson, entrepreneur, advocate for social causes, founder of the Virgin Group, cofounder of The Elders, United Kingdom – Fernando Henrique Cardoso, former President of Brazil (chair) – Maria Cattaui, former Secretary-General of the International Chamber of Commerce, Switzerland – Ruth Dreifuss, former President of Switzerland and Minister of Home Affairs – César Gaviria, former President of Colombia – Asma Jahangir, human rights activist, former UN Special Rapporteur on Arbitrary, Extrajudicial and Summary Executions, Pakistan – Michel Kazatchkine, UN Secretary General Special Envoy on HIV/AIDS in Eastern Europe and Central Asia, and former executive director of the Global Fund to Fight AIDS, Tuberculosis and Malaria, France – Aleksander Kwasniewski, former President of Poland – Ricardo Lagos, former President of Chile – George Papandreou, former Prime Minister of Greece – Jorge Sampaio, former President of Portugal – George P. Shultz, former Secretary of State, United States (honorary chair) – Javier Solana, former European Union High Representative for the Common Foreign and Security Policy , Spain – Thorvald Stoltenberg, former Minister of Foreign Affairs and UN High Commissioner for Refugees, Norway – Mario Vargas Llosa, writer and public intellectual, Peru – Paul Volcker, former Chairman of the United States Federal Reserve and of the Economic Recovery Board – John Whitehead, former Deputy Secretary of State, former Co-Chairman Goldman Sachs & Co. and founding Chairman, 9/11 Memorial & Museum – Ernesto Zedillo, former President of Mexico', 'The Effects of Drug Abuse & Addiction\nDrug abuse can result in short- and long-term negative consequences ranging from issues such as disrupted sleep patterns to loss of employment and problems with relationships.1, 2 Problematic substance use can also increase a person’s risk for developing long-term medical issues such as heart disease, cancer, as well as certain co-occurring mental health conditions.1\nWhich specific type of drug or drugs, how much and how they are used, and an individual’s health are some factors that can contribute to the potential range of adverse effects a person may experience.3 While some consequences result directly from drug abuse and addiction, others may occur indirectly in relation to drug use.\nLearn more about the effects of drug abuse and how a substance use disorder (SUD) may negatively impact your life below.\nHealth Effects of Drug Abuse\nThough the associated, substance-related effects may vary from one drug to the next, general short-term health consequences of drug use can include issues such as:3\n- Changes in appetite.\n- Disturbances in sleep patterns.\n- Cardiovascular issues including stroke and heart attack.\n- Overdose toxicity or death.\nLong term use of some drugs may also increase the likelihood of certain cancers; developing heart, liver, and kidney disease; as well as experiencing neurological issues such as seizures.4, 5 Injecting drugs such as cocaine, heroin, and methamphetamine is also associated with an increased risk of contracting hepatitis, HIV/AIDS, and other infections.6,7 Drug use may also negatively impact cognitive functioning, resulting in impaired responses, planning, decision-making, and memory.8\nDrug use can alter the activity of several brain chemicals—or neurotransmitters—including dopamine. Dopamine is a neurotransmitter associated with pleasure, reward, and satisfaction, and several different drugs have been found to influence dopamine pathways in the brain.9\nThough the neurochemical details are complicated, consistently elevated dopamine activity as a result of substance use can strongly reinforce drug seeking behavior and compulsive drug use. Additionally, as someone’s brain and body adjusts to frequently being high, they may experience relatively diminished reward from otherwise normally reinforcing activities like sex or eating food.9\nSubstance abuse and mental illness commonly coincide, with research showing that about half of those who experience a mental illness during their lives will also experience a substance use disorder and vice versa.10 Commonly co-occurring mental health disorders include depression, anxiety, attention-deficit hyperactivity disorder (ADHD), bipolar disorder, schizophrenia, borderline personality disorder, and antisocial personality disorder.10\nImpact of Substance Abuse on Relationships\nDrug abuse can affect more than just the person getting high, it can also be detrimental to their relationships with friends, family members, and coworkers.11 Often, couples in which one or both partners abuse drugs are less happy than couples who don’t.11 Using drugs seems to cause significant conflict between intimate partners; these couples tend to fight more often than couples in which both partners don’t use drugs.11 Unfortunately, substance abuse can sometimes lead to violence in intimate relationships as well. 11\nDrug Addiction and Financial Problems\nThe use of illicit drugs can not only affect an individual’s health and relationships but can cause issues for individuals financially—both in their personal and professional lives. It has been estimated that substance abuse—including alcohol, prescription opioids, illicit drugs, and tobacco—has cost the nation more than $740 billion annually in lost work productivity, health care costs and crime.13\nResearch has shown that drug use is related to a number of problematic work behaviors such as decreased performance levels and increased accident rates.14 Additionally, those using illicit drugs may be prone to absenteeism at work, abuse of work benefits, and low workplace rule abidance.14 These issues, in turn, could result in the loss of employment and difficult obtaining new work in the future.\nWhen it comes to the cost of drugs, paying for them can also begin to be a burden on an individual’s income, especially as increasing amounts of the drugs may become necessary to overcome a growing tolerance to their effects. Since drug use can also result in serious health problems, those struggling with addiction may be faced with increased medical fees and/or insurance premiums.\nDrug use can also lead to costs associated with criminal activities such as DUI charges, bail, attorneys, court-mandated classes, court fines, and public transportation due to the loss of a vehicle or license.\nDangers of Overdosing\nRegardless of how often a person uses certain types of substances—such as cocaine, sedatives, heroin, or other opioids—there is always some risk of overdose, especially when these drugs are mixed with other drugs and/or alcohol. In 2018, more than 65,000 Americans died as a result of overdoses, including those that involved illicit drugs and prescription opioid medications.15\nOverdoses can occur when a drug is used to excess, resulting in toxicity or injury and, in some cases, death.16 Various factors can play into whether someone overdoses on a drug, such as a person’s body weight, gender, tolerance, the potency of the drug, and the other drugs that may have been taken with it.17\nWarning Signs of Overdose\nThe signs and symptoms of overdoses can vary based on the drug. Some signs include:17\n- Pale and/or clammy face.\n- Changes in pulse rate and blood pressure.\n- Shallow, erratic breathing.\n- Loss of consciousness.\n- Severe stomach pain.\nIf you believe someone is experiencing an overdose, call 9-1-1 immediately and wait for medical assistance.\nGetting Help For Drug Addiction\nDrug abuse and addiction can greatly impact your life, resulting in negative consequences to every part of your life. If you feel you’re ready to make a change and quit using drugs and/or alcohol, American Addiction Centers (AAC) is here to help.\nAAC operates AlcoholRehab.com and is a nationwide provider of addiction treatment facilities. Our admissions navigators are available 24/7 to speak with you about your options for treatment today. It’s never too late to get clean and take back control of your life, relationships and finances.\nAll calls are 100% confidential and there’s no pressure to make any decisions right away. We’re here for you and will walk with you every step of the way as you embark on your journey to recovery.\nNot sure if your insurance covers alcohol treatment?\nCheck your insurance coverage or text us your questions for more information.']"	['<urn:uuid:949a1a2c-70d2-4ae6-9d88-7e57c02e1432>', '<urn:uuid:7969a82d-3d90-48b2-8af5-5b6e05263de3>']	factoid	direct	long-search-query	similar-to-document	multi-aspect	expert	2025-05-13T01:23:33.086345	8	55	1916
100	Between routine blood work and molecular diagnostics, which reveals more?	Molecular diagnostics provides more detailed insights compared to routine blood work. While routine blood tests like complete blood counts (CBC) offer important information about red and white blood cells, platelets, and basic health markers, molecular diagnostics examines DNA, RNA, and proteins in blood samples at a much deeper level. This advanced analysis enables the identification of specific genetic mutations, gene expressions, and biomarkers associated with various diseases. For instance, in cancer patients, molecular blood analysis through liquid biopsies can detect circulating tumor DNA and provide detailed information about cancer mutations and treatment responses, offering more precise and comprehensive diagnostic information than standard blood tests.	['Blood analysis is a cornerstone of modern day medication, offering a prosperity of info about a person’s overall health and properly-becoming. This diagnostic instrument plays a pivotal role in determining diseases, monitoring treatment effectiveness, and guiding preventive care. From regimen blood exams to slicing-edge molecular analyses, the energy of blood examination lies in its potential to decode the intricacies of human well being. In this post, we investigate the significance of blood analysis and its extensive-ranging apps in health care.\n1. The Essence of Blood Evaluation\nAt the coronary heart of blood investigation lies the evaluation of blood samples to measure and assess various factors. These factors include purple and white blood cells, platelets, electrolytes, hormones, enzymes, and metabolic byproducts. The insights received from blood analysis enable healthcare professionals to evaluate a patient’s all round health and recognize prospective health troubles.\ntwo. Regimen Blood Tests: A Window to Wellness\nProgram blood exams are between the most typical medical processes, delivering a comprehensive overview of a person’s well being status. A total blood rely (CBC) assesses red and white blood cells, platelets, and hemoglobin stages. Further exams, these kinds of as lipid profiles, liver purpose assessments, and glucose stages, provide vital data about cardiovascular well being, liver perform, and diabetes chance.\n3. Diagnosing Ailments and Problems\nBlood evaluation serves as a crucial tool in diagnosing numerous ailments and problems. Elevated ranges of specified enzymes or markers in the blood can show organ damage, even though irregular cell counts might advise an underlying infection or hematologic issue. Blood investigation performs a important position in the diagnosis of situations ranging from anemia and diabetic issues to autoimmune issues and infectious diseases.\nfour. Monitoring Treatment Usefulness\nFor clients going through healthcare treatment options, blood analysis is crucial in checking remedy efficiency and changing therapies as needed. Most cancers patients, for occasion, count on blood checks to keep track of tumor markers and evaluate the response to chemotherapy. Similarly, blood analysis is essential in managing long-term conditions like diabetes, guaranteeing optimum manage of blood glucose levels.\nfive. Screening for Preventive Treatment\nBlood examination is a cornerstone of preventive care, aiding in the early detection of chance aspects and potential wellness issues. Program screenings, these kinds of as lipid profiles, recognize elevated cholesterol stages that may possibly lead to cardiovascular condition. Blood checks can also reveal markers related with liver or kidney dysfunction, enabling timely interventions to stop difficulties.\nsix. Advanced Blood Investigation: Molecular Diagnostics\nBreakthroughs in health care technologies have offered increase to molecular diagnostics, a subject that examines DNA, RNA, and proteins in blood samples. Molecular blood examination enables the identification of genetic mutations, gene expressions, and specific biomarkers connected with numerous illnesses. It performs a essential role in personalized medication and targeted therapies.\n7. Liquid Biopsies: Non-Invasive Most cancers Testing\nLiquid biopsies are an revolutionary application of blood analysis in most cancers detection. By analyzing circulating tumor cells and tumor DNA fragments in the blood, liquid biopsies can offer insights into cancer mutations and treatment responses. This non-invasive approach has the prospective to transform most cancers analysis and remedy monitoring.\n8. Blood Evaluation in Transplant Medicine\nIn organ transplantation, blood examination plays a vital function in evaluating donor-recipient compatibility and monitoring submit-transplant health. Blood checks evaluate organ perform, detect indicators of rejection, and aid medical professionals enhance immunosuppressive therapies for transplant recipients.\n9. Wellbeing of Blood Examination: Precision and Innovation\nAs technology carries on to progress, the long term of blood evaluation holds remarkable assure. Novel strategies, such as solitary-cell examination and microfluidics, are revolutionizing blood tests with increased precision and sensitivity. These improvements have the potential to uncover deeper insights into human well being and empower earlier illness detection.\nten. The Relevance of Normal Wellness Examine-ups\nNormal health check out-ups and blood investigation are vital for preserving general well-becoming and determining health issues at an early stage. Proactive checking empowers individuals to take demand of their overall health and make informed choices about life style, diet regime, and medical care. Normal blood evaluation ensures that any likely health considerations are instantly resolved, leading to far better health outcomes.\nBlood analysis is an indispensable device in present day healthcare, providing worthwhile insights into a person’s well being and guiding health care determination-producing. From schedule blood checks to sophisticated molecular diagnostics, blood investigation performs a pivotal part in diagnosing ailments, monitoring treatment options, and advertising preventive care. As engineering proceeds to progress, the foreseeable future of blood examination retains even increased prospective to revolutionize medical diagnostics and enhance individual outcomes. Regular overall health verify-ups, complemented by blood investigation, are basic in promoting all round effectively-getting and proactive well being management. In the realm of health care, the power of blood analysis shines as a beacon of hope and development in the pursuit of greater overall health for all.', 'A tissue biopsy is the “golden standard” for molecular profiling that is essential in decision-making regarding treatment for malignant tumors, including primary lung cancer. However, tumor biopsies are associated with several limitations, including invasiveness and difficulty in achieving access. Liquid biopsies have several potential advantages over tissue biopsies, and recent advances in molecular technologies have enabled liquid biopsies to be introduced into daily clinical practice. Cell-free blood-based liquid biopsies to detect mutations in the epidermal growth factor receptor (EGFR) gene in the plasma have been approved and may be useful in selecting patients for treatment with tyrosine kinase inhibitors of EGFR. We herein describe blood-based liquid biopsies and review the current status and future perspectives of plasma genotyping in primary lung cancer.\nThis is a preview of subscription content, log in to check access.\nBuy single article\nInstant access to the full article PDF.\nPrice includes VAT for USA\nSubscribe to journal\nImmediate online access to all issues from 2019. Subscription will auto renew annually.\nThis is the net price. Taxes to be calculated in checkout.\nTorre LA, Bray F, Siegel RL, Ferlay J, Lortet-Tieulent J, Jemal A. Global cancer statistics, 2012. CA Cancer J Clin. 2015;65:87–108.\nHerbst RS, Heymach JV, Lippman SM. Lung cancer. N Engl J Med. 2008;359:1367–80.\nRudin CM, Ismaila N, Hann CL, Malhotra N, Movsas B, Norris K, et al. Treatment of small-cell lung cancer: American Society of Clinical Oncology endorsement of the American College of Chest Physicians guideline. J Clin Oncol. 2015;33:4106–11.\nBunn PA Jr, Minna JD, Augustyn A, Bunn PA Jr, Minna JD, Augustyn A, et al. Small cell lung cancer: can recent advances in biology and molecular biology be translated into improved outcomes?. J Thorac Oncol. 2016;11:453–74.\nNon-Small Cell Lung Cancer Collaborative Group. Chemotherapy and supportive care versus supportive care alone for advanced non-small cell lung cancer. Cochrane Database Syst Rev 2010;12(5):CD007309.\nHanahan D, Weingberg RA. Hallmarks of cancer: the next generation. Cell. 2011;144:646–74.\nSwanton C, Govindan R. Clinical implications of genomic discoveries in lung cancer. N Engl J Med. 2016;374:1864–73.\nHirsch FR, Scagliotti GV, Mulshine JL, Kwon R, Curran WJ Jr, Wu YL, et al. Lung cancer: current therapies and new targeted treatments. Lancet. 2017;389:299–311.\nHanna N, Johnson D, Temin S, Baker S Jr, Brahmer J, Ellis PM, et al. Systemic therapy for stage IV non-small-cell lung cancer: American Society of Clinical Oncology Clinical practice guideline update. J Clin Oncol. 2017;35:3484–515.\nYoneda K, Tanaka F. Molecular diagnosis and targeting for lung cancer. In: Shinomiya N, Kataoka H, Shimada Y, editors. Molecular diagnosis and targeting for thoracic and gastrointestinal malignancy. Singapore: Springer; 2018. pp. 1–32.\nSaito M, Shiraishi K, Kunitoh H, Takenoshita S, Yokota J, Kohno T. Gene aberrations for precision medicine against lung adenocarcinoma. Cancer Sci. 2016;107:713–20.\nBarlesi F, Mazieres J, Merlio JP, Debieuvre D, Mosser J, Lena H, et al. Routine molecular profiling of patients with advanced non-small-cell lung cancer: results of a 1-year nationwide programme of the French Cooperative Thoracic Intergroup (IFCT). Lancet. 2016;387:1415–26.\nSiravegna G, Marsoni S, Siena S, Bardelli A. Integrating liquid biopsies into the management of cancer. Nat Rev Clin Oncol 2017;14:531–48.\nSacher AG, Komatsubara KM, Oxnard GR. Application of plasma genotyping technologies in non-small cell lung cancer: a practical review. J Thorac Oncol. 2017;12:1344–56.\nDiaz LA Jr, Bardelli A. Liquid biopsies: genotyping circulating tumor DNA. J Clin Oncol. 2014;32:579–86.\nHeitzer E, Ulz P, Geigl JB. Circulating tumor DNA as a liquid biopsy for cancer. Clin Chem. 2015;61:112–23.\nTanaka F, Yoneda K. Adjuvant therapy following surgery in non-small cell lung cancer (NSCLC). Surg Today. 2016;46:25–37.\nWan JCM, Massie C, Garcia-Corbacho J, Mouliere F, Brenton JD, Caldas C, et al. Liquid biopsies come of age: towards implementation of circulating tumour DNA. Nat Rev Cancer. 2017;17:223–38.\nZhang W, Xia W, Lv Z, Ni C, Xin Y, Yang L. Liquid biopsy for cancer: circulating tumor cells, circulating free DNA or exosomes? Cell Physiol Biochem. 2017;41:755–68.\nWang J, Chang S, Li G, Sun Y. Application of liquid biopsy in precision medicine: opportunities and challenges. Front Med. 2017;11:522–7.\nRiethdorf S, O’Flaherty L, Hille C, Pantel K. Clinical applications of the CellSearch platform in cancer patients. Adv Drug Deliv Rev. 2018. https://doi.org/10.1016/j.addr.2018.01.011 (pii: S0169-409X(18)30011-5).\nAllard WJ, Matera J, Miller MC, Repollet M, Connelly MC, Rao C, et al. Tumor cells circulate in the peripheral blood of all major carcinomas but not in healthy or patients with nonmalignant diseases. Clin Cancer Res. 2004;10:6897–904.\nTanaka F, Yoneda K, Kondo N, Hashimoto M, Takuwa T, Matsumoto S, et al. Circulating tumor cell as a diagnostic marker in primary lung cancer. Clin Cancer Res. 2009;15:6980–6.\nNaito T, Tanaka F, Ono A, Yoneda K, Takahashi T, Murakami H, et al. Prognostic impact of circulating tumor cells in patients with small cell lung cancer. J Thorac Oncol. 2012;7:512–9.\nDiehl F, Schmidt K, Choti MA, Romans K, Goodman S, Li M, et al. Circulating mutant DNA to assess tumor dynamics. Nat Med. 2008;14:985–90.\nHoldhoff M, Schmidt K, Donehower R, Diaz LA Jr. Analysis of circulating tumor DNA to confirm somatic KRAS mutations. J Natl Cancer Inst. 2009;101:1284–5.\nThierry AR, El Messaoudi S, Gahan PB, Anker P, Stroun M. Origins, structures, and functions of circulating DNA in oncology. Cancer Metastasis Rev. 2016;35:347–76.\nEl Messaoudi S, Rolet F, Mouliere F, Thierry AR. Circulating cell free DNA: preanalytical considerations. Clin Chim Acta. 2013;424:222–30.\nNormanno N, Denis MG, Thress KS, Ratcliffe M, Reck M. Guide to detecting epidermal growth factor receptor (EGFR) mutations in ctDNA of patients with advanced non-small-cell lung cancer. Oncotarget. 2017;8:12501–16.\nGarcia J, Dusserre E, Cheynet V, Bringuier PP, Brengle-Pesce K, Wozny AS, et al. Evaluation of pre-analytical conditions and comparison of the performance of several digital PCR assays for the detection of major EGFR mutations in circulating DNA from non-small cell lung cancers: the CIRCAN_0 study. Oncotarget. 2017;8:87980–96.\nVallée A, Marcq M, Bizieux A, Kouri CE, Lacroix H, Bennouna J, et al. Plasma is a better source of tumor-derived circulating cell-free DNA than serum for the detection of EGFR alterations in lung tumor patients. Lung Cancer. 2013;82:373–4.\nLi X, Ren R, Ren S, Chen X, Cai W, Zhou F, et al. Peripheral blood for epidermal growth factor receptor mutation detection in non-small cell lung cancer patients. Transl Oncol. 2014;7:341–8.\nLuo J, Shen L, Zheng D. Diagnostic value of circulating free DNA for the detection of EGFR mutation status in NSCLC: a systematic review and meta-analysis. Sci Rep. 2014;4:6269.\nReck M, Hagiwara K, Han B, Tjulandin S, Grohé C, Yokoi T, et al. ctDNA Determination of EGFR mutation status in European and Japanese patients with advanced NSCLC: the ASSESS study. J Thorac Oncol. 2016;11:1682–9.\nPinheiro LB, Coleman VA, Hindson CM, Herrmann J, Hindson BJ, Bhat S, et al. Evaluation of a droplet digital polymerase chain reaction format for DNA copy number quantification. Anal Chem. 2012;84:1003–11.\nOxnard GR, Paweletz CP, Kuang Y, Mach SL, O’Connell A, Messineo MM, et al. Noninvasive detection of response and resistance in EGFR‑mutant lung cancer using quantitative next‑generation genotyping of cell‑free plasma DNA. Clin Cancer Res. 2014;20:1698–705.\nDressman D, Yan H, Traverso G, Kinzler KW, Vogelstein B. Transforming single DNA molecules into fluorescent magnetic particles for detection and enumeration of genetic variations. Proc Natl Acad Sci USA. 2003;100:8817–22.\nLi M, Diehl F, Dressman D, Vogelstein B, Kinzler KW. BEAMing up for detection and quantification of rare sequence variants. Nat Methods. 2006;3:95–7.\nYohe S, Thyagarajan B. Review of clinical next-generation sequencing. Arch Pathol Lab Med. 2017;141:1544–57.\nVendrell JA, Grand D, Rouquette I, Costes V, Icher S, Selves J, et al. High-throughput detection of clinically targetable alterations using next-generation sequencing. Oncotarget. 2017;8:40345–58.\nVendrell JA, Mau-Them FT, Béganton B, Godreuil S, Coopman P, Solassol J. Circulating cell free tumor DNA detection as a routine tool for lung cancer patient management. Int J Mol Sci. 2017;18:E264.\nNewman AM, Bratman SV, To J, Wynne JF, Eclov NC, Modlin LA, et al. An ultrasensitive method for quantitating circulating tumor DNA with broad patient coverage. Nat Med. 2014;20:548–54.\nLanman RB, Mortimer SA, Zill OA, Sebisanovic D, Lopez R, Blau S, et al. Analytical and clinical validation of a digital sequencing panel for quantitative, highly accurate evaluation of cell-free circulating tumor DNA. PLoS One. 2015;10:e0140712.\nQiu M, Wang J, Xu Y, Ding X, Li M, Jiang F, et al. Circulating tumor DNA is effective for the detection of EGFR mutation in non-small cell lung cancer: a meta-analysis. Cancer Epidemiol Biomark Prev. 2015;24:206–12.\nKim E, Feldman R, Wistuba II. Update on EGFR mutational testing and the potential of noninvasive liquid biopsy in non-small-cell lung cancer. Clin Lung Cancer. 2017. https://doi.org/10.1016/j.cllc.2017.08.001 (pii: S1525-7304(17)30229-2).\nSingh AP, Li S, Cheng H. Circulating DNA in EGFR-mutated lung cancer. Ann Transl Med. 2017;5:379.\nDuréndez-Sáez E, Azkárate A, Meri M, Calabuig-Fariñas S, Aguilar-Gallardo C, Blasco A, et al. New insights in non-small-cell lung cancer: circulating tumor cells and cell-free DNA. J Thorac Dis. 2017;9(Suppl 13):S1332-45.\nBai H, Mao L, Wang HS, Zhao J, Yang L, An TT, et al. Epidermal growth factor receptor mutations in plasma DNA samples predict tumor response in Chinese patients with stages IIIB to IV non-small-cell lung cancer. J Clin Oncol. 2009;27:2653–9.\nBrevet M, Johnson ML, Azzoli CG, Ladanyi M. Detection of EGFR mutations in plasma DNA from lung cancer patients by mass spectrometry genotyping is predictive of tumor EGFR status and response to EGFR inhibitors. Lung Cancer. 2011;73:96–102.\nHu C, Liu X, Chen Y, Sun X, Gong Y, Geng M, et al. Direct serum and tissue assay for EGFR mutation in non-small cell lung cancer by high-resolution melting analysis. Oncol Rep. 2012;28:1815–21.\nKim HR, Lee SY, Hyun DS, Lee MK, Lee HK, Choi CM, et al. Detection of EGFR mutations in circulating free DNA by PNA‑mediated PCR clamping. J Exp Clin Cancer Res. 2013;32:50.\nKarachaliou N, Mayo-de las Casas C, Queralt C, de Aguirre I, Melloni B, Cardenal F, et al. Association of EGFR L858R mutation in circulating free DNA with survival in the EURTAC trial. JAMA Oncol. 2015;1:149–57.\nKimura H, Suminoe M, Kasahara K, Sone T, Araya T, Tamori S, et al. Evaluation of epidermal growth factor receptor mutation status in serum DNA as a predictor of response to gefitinib (IRESSA). Br J Cancer 2007;97:778–84.\nGoto K, Ichinose Y, Ohe Y, Yamamoto N, Negoro S, Nishio K, et al. Epidermal growth factor receptor mutation status in circulating free DNA in serum: From IPASS, a phase III study of gefitinib or carboplatin/paclitaxel in non‑small cell lung cancer. J Thorac Oncol 2012;7:115–21.\nDouillard JY, Ostoros G, Cobo M, Ciuleanu T, Cole R, McWalter G, et al. Gefitinib treatment in EGFR mutated Caucasian NSCLC: Circulating‑free tumor DNA as a surrogate for determination of EGFR status. J Thorac Oncol 2014;9:1345–53.\nWu YL, Sequist LV, Hu CP, Feng J, Lu S, Huang Y, et al. EGFR mutation detection in circulating cell-free DNA of lung adenocarcinoma patients: analysis of LUX-Lung 3 and 6. Br J Cancer. 2017;116:175–85.\nWeber B, Meldgaard P, Hager H, Wu L, Wei W, Tsai J, et al. Detection of EGFR mutations in plasma and biopsies from non-small cell lung cancer patients by allele-specific PCR assays. BMC Cancer. 2014;14:294.\nMok T, Wu YL, Lee JS, Yu CJ, Sriuranpong V, Sandoval-Tan J, et al. Detection and dynamic changes of EGFR mutations from circulating tumor DNA as a predictor of survival outcomes in NSCLC patients treated with first-line intercalated erlotinib and chemotherapy. Clin Cancer Res. 2015;21:3196–203.\nReckamp KL, Melnikova VO, Karlovich C, Sequist LV, Camidge DR, Wakelee H, et al. A highly sensitive and quantitative test platform for detection of NSCLC EGFR mutations in urine and plasma. J Thorac Oncol. 2016;11:1690–700.\nUS Food and Drug Administration. cobas EGFR Mutation Test v2. 2015. http://www.fda.gov/drugs/informationondrugs/approveddrugs/ucm504540.htm. Accessed 31 Jan 2018.\nYung TK, Chan KC, Mok TS, Tong J, To KF, Lo YM. Single-molecule detection of epidermal growth factor receptor mutations in plasma by microfluidics digital PCR in non-small cell lung cancer patients. Clin Cancer Res. 2009;15:2076–84.\nLee JY, Qing X, Xiumin W, Yali B, Chi S, Bak SH, et al. Longitudinal monitoring of EGFR mutations in plasma predicts outcomes of NSCLC patients treated with EGFR TKIs: Korean Lung Cancer Consortium (KLCC‑12‑02). Oncotarget 2016;7:6984–93.\nSacher AG, Paweletz C, Dahlberg SE, Alden RS, O’Connell A, Feeney N, et al. Prospective validation of rapid plasma genotyping for the detection of EGFR and KRAS mutations in advanced lung cancer. JAMA Oncol. 2016;2:1014–22.\nCouraud S, Vaca-Paniagua F, Villar S, Oliver J, Schuster T, Blanché H, et al. Noninvasive diagnosis of actionable mutations by deep sequencing of circulating free DNA in lung cancer from never‑smokers: a proof‑of‑concept study from BioCAST/IFCT‑1002. Clin Cancer Res 2014;20:4613–24.\nUchida J, Kato K, Kukita Y, Kumagai T, Nishino K, Daga H, et al. Diagnostic accuracy of noninvasive genotyping of EGFR in lung cancer patients by deep sequencing of plasma cell-Free DNA. Clin Chem. 2015;61:1191–6.\nPaweletz CP, Sacher AG, Raymond CK, Alden RS, O’Connell A, Mach SL, et al. Bias-corrected targeted next-generation sequencing for rapid, multiplexed detection of actionable alterations in cell-free DNA from advanced lung cancer patients. Clin Cancer Res. 2016;22:915–22.\nThompson JC, Yee SS, Troxel AB, Savitch SL, Fan R, Balli D, et al. Detection of therapeutically targetable driver and resistance mutations in lung cancer patients by next-generation sequencing of cell-free circulating tumor DNA. Clin Cancer Res. 2016;22:5772–82.\nJenkins S, Yang JC, Ramalingam SS, Yu K, Patel S, Weston S, et al. Plasma ctDNA analysis for detection of the EGFR T790M mutation in patients with advanced non-small cell lung cancer. J Thorac Oncol. 2017;12:1061–70.\nTakahama T, Sakai K, Takeda M, Azuma K, Hida T, Hirabayashi M, et al. Detection of the T790M mutation of EGFR in plasma of advanced non–small cell lung cancer patients with acquired resistance to tyrosine kinase inhibitors (West Japan oncology group 8014LTR study). Oncotarget. 2016;7:58492–9.\nMok TSK, Kim SW, Wu YL, Nakagawa K, Yang JJ, Ahn MJ, et al. Gefitinib plus chemotherapy versus chemotherapy in epidermal growth factor receptor mutation-positive non-small-cell lung cancer resistant to first-Line gefitinib (IMPRESS): overall survival and biomarker analyses. J Clin Oncol. 2017;35:4027–34.\nKarlovich C, Goldman JW, Sun JM, Mann E, Sequist LV, Konopa K, et al. Assessment of EGFR mutation status in matched plasma and tumor tissue of NSCLC patients from a phase I study of rociletinib (CO-1686). Clin Cancer Res. 2016;22:2386–95.\nOxnard GR, Thress KS, Alden RS, Lawrance R, Paweletz CP, Cantarini M, et al. Association between plasma genotyping and outcomes of treatment with osimertinib (AZD9291) in advanced non-small-cell lung cancer. J Clin Oncol. 2016;34:3375–82.\nWu YL, Zhou C, Liam CK, Wu G, Liu X, Zhong Z, et al. First-line erlotinib versus gemcitabine/cisplatin in patients with advanced EGFR mutation-positive non-small-cell lung cancer: analyses from the phase III, randomized, open-label, ENSURE study. Ann Oncol. 2015;26:1883–9.\nOztan A, Fischer S, Schrock AB, Erlich RL, Lovly CM, Stephens PJ, et al. Emergence of EGFR G724S mutation in EGFR-mutant lung adenocarcinoma post progression on osimertinib. Lung Cancer. 2017;111:84–7.\nKobayashi S, Boggon TJ, Dayaram T, Jänne PA, Kocher O, Meyerson M, et al. EGFR mutation and resistance of non-small-cell lung cancer to gefitinib. N Engl J Med. 2005;352:786–92.\nSequist LV, Waltman BA, Dias-Santagata D, Digumarthy S, Turke AB, Fidias P, et al. Genotypic and histological evolution of lung cancers acquiring resistance to EGFR inhibitors. Sci Transl Med. 2011;3:75ra26.\nOhashi K, Maruvka YE, Michor F, Pao W. Epidermal growth factor receptor tyrosine kinase inhibitor-resistant disease. J Clin Oncol. 2013;31:1070–80.\nRamalingam SS, Yang JC, Lee CK, Kurata T, Kim DW, John T, et al. Osimertinib as first-line treatment of EGFR mutation-positive advanced non-small-cell lung cancer. J Clin Oncol 2017;25:JCO2017747576. https://doi.org/10.1200/JCO.2017.74.7576. (Epub ahead of print).\nGoss G, Tsai CM, Shepherd FA, Bazhenova L, Lee JS, Chang GC, et al. Osimertinib for pretreated EGFR Thr790Met-positive advanced non-small-cell lung cancer (AURA2): a multicentre, open-label, single-arm, phase 2 study. Lancet Oncol. 2016;17:1643–52.\nMok TS, Wu Y-L, Ahn M-J, Garassino MC, Kim HR, Ramalingam SS, et al. Osimertinib or platinum-pemetrexed in EGFR T790M-positive lung cancer. N Engl J Med. 2017;376:629–40.\nConflict of interest\nKazue Yoneda has no conflicts of interest; Naoko Imanishi has no conflicts of interest; Yoshinobu Ichiki has no conflicts of interest; Fumihiro Tanaka received a research grant from Astra Zeneca, Chugai Pharmaceutical, Taiho Pharmaceutical and Ono Pharmaceutical and received honoraria from Astra Zeneca, Chugai Pharmaceutical and Taiho Pharmaceutical.\nAbout this article\nCite this article\nYoneda, K., Imanishi, N., Ichiki, Y. et al. A liquid biopsy in primary lung cancer. Surg Today 49, 1–14 (2019). https://doi.org/10.1007/s00595-018-1659-2\n- Liquid biopsies\n- Circulating tumor DNA\n- Plasma genotyping']	['<urn:uuid:c09954fc-ee22-4db1-b900-323db7e83068>', '<urn:uuid:a8de9cf5-abbf-42b6-ae9c-2173180be329>']	open-ended	with-premise	concise-and-natural	distant-from-document	comparison	expert	2025-05-13T01:23:33.086345	10	104	3516
