qid	question	answer	context	document_ids	question_factuality	question_premise	question_phrasing	question_linguistic_variation	question_multi-doc	user_expertise-categorization	generation_timestamp	question_length	answer_length	context_length
1	How many different flash modes can you use when taking pictures in Program mode compared to Auto mode?	In Program mode you can select from all 8 flash modes, while in Auto mode you are limited to just 2 options - auto flash and flash off.	"['Record Screens & Menus\nAuto mode, which has its own position on the mode dial, is the camera\'s easiest shooting mode. It automates all settings, and allows few manual adjustments. For example, in Auto mode you can only set the flash to auto or off - you can\'t select the six additional modes available in Program mode, such as flash on with red-eye reduction or slow-speed sync (which sets the camera to a slow shutter speed and fires the flash).\nThe icons at the top left corner (from left to right) indicate Auto mode, auto flash, single shot drive mode, auto white balance, and that custom image is off. Top right indicates the metering mode, shake reduction on, and battery level.\nAlso, by default the Q7 includes horizontal and vertical level indicators at top right on the LCD. With these handy, it\'s arguably easier than having to use the menus to bring up a grid or level indicator (though having lines run across the screen may be easier to line up than relying on these smaller meters). You can turn off these electronic levels, as well as choose from a few grid overlays.\nAt the bottom of the screen, you see the shutter speed, aperture setting, ISO and number of still images that can still be recorded on the memory card at current resolution settings.\nProgram mode allows more flexibility than Auto mode. For example, you select from all of the flash modes versus just auto flash and flash off in Auto shooting mode. In addition, you can use one of the 11 Custom Image settings in addition to the 11 digital filters that are also available in Auto mode.\nWhen focus has locked onto your subject, the AF box (or, boxes, depending on the AF mode) turns green.\nIf you\'re using manual focus, you will appreciate first that the Q lenses have a focus ring instead of having to use a zoom lever from the camera body. However, because the Q7 lacks a viewfinder, you still must contend with having to focus while looking at a 460,000-dot LCD. To help you get sharper shots with manual focus, the camera offers focus peaking, which highlights the edges of subjects (see above). You can turn this feature on and off.\nAlso, if you use the manual focus auto zoom feature, when you turn the focus ring the on-screen image will magnify by the amount you select: 2X, 4X or 6X.\nPressing the Info button on the back panel brings up a 15-item menu of commonly adjusted shooting parameters, including Custom Image (11 options) and digital filters (11 choices). Having this dedicated function saves you from potentially navigating through multiple tabs in the traditional shooting menu list. You move though these icons using the four-way control, and can then scroll through the selections with the thumb dial, without having to press OK and navigate the options with the four-way buttons.\nInfo button menu:\n- Custom Image (11 selections)\n- Digital filter (11 selections)\n- HDR capture: Off, auto, HDR 1, HDR 2\n- Highlight correction: Auto, on, off\n- Shadow correction: Auto, on, off\n- AE metering: Multi-segment, center-weighted, spot\n- ND filter: On, off\n- Focus method: Auto focus, manual focus\n- Auto focus method: Face detection, tracking, multiple AF points, select, spot\n- Focus peaking: On, off\n- Distortion correction: On, off\n- Aspect ratio: 1:1, 4:3, 3:2, 16:9\n- File format: JPEG, RAW, RAW+JPEG\n- JPEG recorded pixels: 2M to 12M (selections depend on selected aspect ratio)\n- Shake reduction: On, off\nA second press of the Info button brings up the display menu, which has three simple options.\n- Standard information display\n- No information display\n- Display off\nPressing the dedicated exposure compensation button (AV +/-) on the back panel brings up an overlaid menu; you adjust the exposure compensation with the thumb dial, moving the hash mark left or right to the corresponding amount of exposure compensation. As indicated by the green button at bottom right, pressing the green button just below the exposure compensation button resets the EV to 0.\nThe four-way control offers no surprises in its offerings - it\'s all familiar territory (clockwise from top): ISO, drive mode, white balance, and flash. Shown here are the flash mode selections in Program mode (Auto mode limits this to auto flash and flash off). All eight options are available in aperture-priority mode, but shutter-priority mode removes slow sync and trailing curtain options.\n- Auto flash\n- Auto flash with red-eye reduction\n- Flash on\n- Flash on with red-eye reduction\n- Slow-speed sync\n- Slow-speed sync with red-eye reduction\n- Trailing curtain sync\n- Flash off\nIn case the Info button menu\'s 15 items aren\'t what you need to adjust, you have the three-page shooting menu to consult. Here you will find additional settings and features, such as high ISO noise reduction, histogram display, and electronic level. This is also where you program the Quick Dial positions and program the Green button.\nThe Custom Image menu offers a good variety of photographic treatments, some of them mimicking film types and development methods. You can navigate to this menu via Info button or shooting menu.\nCustom Image menu:\n- Bleach Bypass\n- Reversal Film\n- Cross Processing\nThe digital filters menu offers 18 options. It is an attractive assortment of filer effects, and all of them have at least one adjustable parameter. Shown above, the water color filter has two parameters. Others have just one, while others have three. The Extract Color filter, for example, allows you to choose two colors and each one\'s intensity.\nDigital filters menu (shooting mode):\n- Toy Camera\n- High Contrast\n- Invert Color\n- Extract Color\n- Water Color\nThe Q7 offers five auto focus methods. This should help you get a sharp shot of a variety of subjects. However, the camera uses contrast detection auto focus, which is arguably less accurate with moving subjects than phase detection auto focus (used in SLR cameras). In my testing, I found the auto focus method often inaccurate at freezing moving action (when used in Auto or Program shooting mode). Switching to tracking AF and faster shutter speeds manually produces better results.\nAuto focus menu:\n- Face detection: Detects and tracks faces\n- Tracking: Tracks subject in focus (shutter can be released even if subject is out of focus)\n- Multiple AF points: Select 1, 9 or 25 of 49 areas (sensor divided into 7 x 7)\n- Select: Sets focus area in number of pixels)\n- Spot: Focus area is a limited to the center of the frame\nPage 3 of the shooting menu is where you select the custom function of the green button on the back of the camera, between the four-way control and exposure compensation button.\nGreen Button menu:\n- Green button (resets parameter being adjusted)\n- One push file fomat\n- Auto exposure lock\n- Enable auto focus\nThe green button is not the only customizable button on the camera. The four-position quick dial on the front of the camera also can be assigned a variety of functions. The default is the Smart Effect, with four choices corresponding to the four icons above.\nThere is a total of nine assignable presets to the four positions, plus three user presets that you can assign settings to.\nSmart Effect menu:\n- Brilliant color\n- Unicolor bold\n- Vintage color\n- Cross processing\n- Warm fade\n- Tone expansion\n- Bold monochrome\n- Water color\n- Vibrant color enhance\n- User 1\n- User 2\n- User 3\nIn addition to the Smart Effect option, you can use the quick dial to adjust four other parameters.\nQuick Dial menu:\n- Smart Effect: Nine presets and three user settings\n- Aspect ratio: 1:1, 3:2, 4:3, 16:9\n- AF/MF focus method: auto focus, manual focus\n- Focus peaking: On, off\n- ND filter: On, off\nThe setup menu is where you find the global settings such as date/time, LCD settings, video output settings, power off setting, and perform dust removal and memory card formatting.\nThe custom menu includes three pages of very particular preferences you have likely never thought of if you haven\'t taken a deep dive into photography. For example, selecting ""2"" for ""AE-L with AF locked"" locks the exposure when auto focus locks (the default of 1 has this turned off). You can perform a variety of other customizations, such as turning off auto focus when you are using a remote control unit. In addition, you can set the camera not to fire the flash if you haven\'t popped it up from the body - saving you the trouble of having to change the flash mode to ""off"" (and forcing you to raise it, where it will reduce red eye and avoid the long shadow of a zoom lens).\nThe Q7 records full 1080p video, but if you weren\'t told the resolution, you might not guess it to be full high-definition. The video quality is adequate, but I found the auto focus slow to adjust when zooming in video record mode, and panning with the standard zoom lens produced distortions at the edges that were a little dizzying.\nThe movie record menu is pretty basic. You get just five adjustments. The good news here is you can use all 11 of the digital filters available when shooting still images.\nVisitors of Steves can visit the stores below for real-time pricing and availability. You can also find hot, soon to expire online offers on a variety of cameras and accessories at our very own Camera Deals page.']"	['<urn:uuid:cd1e8c56-609c-474d-bb5d-30c543407551>']	factoid	direct	verbose-and-natural	similar-to-document	single-doc	novice	2025-05-12T20:46:45.703947	18	28	1605
2	How do wind and pollination affect the success of fruit production, and what role do honey bees play in pollinating crops like lychee and blackberries?	Wind has a significant impact on fruit production, with studies showing that wind speed positively correlates with the number of fruitlets in blackberries. For lychee trees, while self-pollination can occur through wind, honey bees are widely recognized as the principal pollinators. Studies have shown that lychee fruit set is three times greater when honey bees are present compared to when they are excluded. To ensure optimal pollination of lychee orchards, it is recommended to maintain 2-3 honey bee hives per hectare.	"['Australian Lychee Growers Association represent the biosecurity interests of lychee growers and the industry.\nIn 2017–18, lychee production of 2,733 tonnes was valued at $24 million (LVP), with\n466 tonnes exported valued at $5.2 million.\nHigher volumes of lychee exports occur during the lead up to the Chinese New Year. Kwai Mai Pink is the preferred export variety due to availability, taste, quality and price. Salathiel, with a good flesh to seed ratio, is in high demand in Singapore and yields good returns for growers willing to persist with this variety. Exports of Australian lychees to the United States are increasing each year, with 10 tonnes exported in the 2018–19 season.\nThis figure is expected to double in the 2019–20 season.\nThe new Chinese varieties of Chompogo, Erdon Lee and Baitangying are now appearing in the domestic markets: even though quantities are small, these varieties will continue to increase as current plantings mature.\nFollowing the 2016 signing of the memorandum of understanding between the Australian Department of Agriculture and Taiwan Agriculture Research Institute, six Taiwanese lychee varieties have now been planted on an orchard in central Queensland. This is a long-term project, as it will be a number of years before Taiwan is able to apply for Plant Breeder’s Rights for these new varieties.\nLychees are produced as a single annual crop with the harvest period from late October (north Queensland) to March (northern NSW). This gives the Australian lychee season one\nof the world’s longest production period, as well as a counter-seasonal supply to most other lychee producing countries.\nThe lychee (Litchi chinensis) is a subtropical tree from the Sapindaceae family and is thought to be native to southern China. The lychee is an evergreen tree which grows to heights of more than 20 m. The lychee fruit is a drupe, 3–4 cm long and 3 cm in diameter, covered by a pink/red roughly textured rind which covers an inside layer of sweet and sour, translucent white flesh which surrounds the inner seed.\nAlthough variable between growing regions, flowering usually occurs over a 6 week period from between July to October. Flowers are borne on multi-branched terminal inflorescences, which are generally referred to as flower panicles. A panicle can have up to 3,000 flowers, although only about 200 are pollinated and of these only 5 to 60 will develop into mature fruit.\nThere are three distinct flower types, two male types and one female, which are all borne on the same panicle. The usual sequence of flower opening, which occurs over a 2 to 6 week period, is male flower, female hermaphrodite flowers which set fruit, and male hermaphrodite flowers that do not set fruit. The male flowers in the first and third stages release pollen to fertilise the female flowers. Thus, there needs to be an overlap of the male stages with the female stage. Most of the pollen used for fertilisation is usually supplied by the third stage male hermaphrodite flowers.\nSelf-pollination may occur in lychee through wind pollination however, flowers are generally recognised as self-sterile and insects are beneficial because they can transport pollen from anther to stigma for fruit to set. Only partial overlapping between male and female flowering occurs within a cultivar and so inter-planting of at least two different cultivars is necessary for adequate fruit set. Several insects have been reported to visit lychee flowers overseas including Coleoptera, Hemiptera, Homoptera and Lepidoptera. However, the honey bee is widely recognised as the principal pollinator. In Australia, both honey bees and Tetragonula sp are found on lychee blossoms, but preliminary studies suggest that the native social bee (Tetragonula sp) may be too small to be effective pollinators of the fruit.\nMany studies have shown significant increases in yield of lychee crops as a result of honey bee pollination, including fruit set being three times greater when inflorescences were open to honey bees when compared to trees that were bagged to exclude them.\nLychee yields can sometimes be unreliable, erratic and rarely approach the capacity of the tree. Crossed fruit are generally heavier and yields in rows with two cultivars adjacent to each other have been shown to be 36 per cent higher in lychee. All of these studies clearly show that lychee require at least one other cultivar and an abundance of insect pollinators for pollen transfer to assist with pollination by wind. Bringing in honey bee hives could be necessary to increase pollinator numbers, increase crossing between cultivars and produce a good quality yield. For this reason, it is recommended that 2-3 hives per ha would be adequate for optimal pollination of lychee.\nAdditional fact sheets and web links about the pollination of this crop are listed below. Please be aware that some of the information was developed overseas, and environmental and seasonal variations may occur.\nMaximise your lychee crop with better pollination (Plant and Food Research NZ and Hort Innovation)\nLychee and longan pollination fact sheet, The Pollination Program (Agrifutures Australia and Hort Innovation)\nLychee, Purdue University, Indiana\n|Annual value of lychee production, 2009–18|\n|Distribution of lychee production by state and territory 2017–18 (based on LVP)|\nThe pollination information is an excerpt from the Lychee and longan pollination fact sheet, The Pollination Program (Agrifutures Australia and Hort Innovation) and from personal communication with James Drinnan (Queensland DAF) and the Australian Lychee Growers’ Association.\nThe industry overview and graphs on the value of production and crop distribution are from the National Plant Biosecurity Status Report, Plant Health Australia, Canberra', ""Wind Speed Affects Pollination Success in Blackberries\nKeywords:Fruit-set, landscape, agroecology, sweetness\nPollination of wild plants and agricultural crops is a vitally important ecosystem service. Many landscape and environmental factors influence the pollination success of crops, including distance from natural habitat, wind speed, and solar radiation. Although there is a general consensus that increasing distance from forest decreases pollination success, few studies have examined the influence of specific environmental factors. In this study, we examined which environmental factors influence the pollination success of blackberries (Rubus glaucus). We measured the number of fruitlets per berry, a proxy for pollination success, as well as the weight and sweetness of each berry. Our results indicate that number of fruitlets is positively correlated with wind speed, but number of unripe red berries per bush is negatively correlated with wind speed. In addition, sweetness increased with increasing numbers of red berries per bush but was lower when flowers and berries were present, though this result should be considered with caution due to methodological limitations. Our findings suggest that a little studied environmental factor, wind, has a large impact on the number of fruitlets in blackberries. Although our findings should be confirmed in other locations to draw broader conclusions, they suggest that producers should consider the effect of wind on blackberry yield to optimize blackberry production.\nAraújo, E. D., Costa, M., Chaud-Netto, J., & Fowler, H. G. (2004). Body size and flight distance in stingless bees (Hymenoptera: Meliponini): inference of flight range and possible ecological implications. Brazilian Journal of Biology, 64: 563-568. doi: 10.1590/S1519-69842004000400003\nBazzaz, F. A., Chiariello, N. R., Coley, P. D., & Pitelka, L. F. (1987). Allocating resources to reproduction and defense. BioScience, 37: 58-67. doi: 10.2307/1310178\nBlanche, K. R., Ludwig, J. A., & Cunningham, S. A. (2006). Proximity to rainforest enhances pollination and fruit set in orchards. Journal of Applied Ecology, 43: 1182-1187. doi:10.1111/j.1365-2664.2006.01230.x\nBoreux, V., Krishnan, S., Cheppudira, K. G., & Ghazoul, J. (2013). Impact of forest fragments on bee visits and fruit set in rain-fed and irrigated coffee agro-forests. Agriculture, Ecosystems and Environment, 172: 42-48. doi: 10.1016/j.agee. 2012.05.003\nBrown, A. O., & McNeil, J. N. (2009). Pollination ecology of the high latitude, dioecious cloudberry (Rubus chamaemorus; Rosaceae). American Journal of Botany, 96: 1096-1107. doi: 10.3732/ajb.0800102\nBrown, M. J., & Paxton, R. J. (2009). The conservation of bees: a global perspective. Apidologie, 40: 410-416. doi: 10.1051/apido/2009019\nBurrill, R. M., & Dietz, A. (1981). The response of honey bees to variations in solar radiation and temperature. Apidologie, 12: 319-328. doi: 10.1051/apido:19810402\nCane, J. H. (2005). Pollination potential of the bee Osmia aglaia for cultivated red raspberries and blackberries (Rubus: Rosaceae). Hortscience, 40: 1705-1708. doi: 10.1098/rspb. 2006.3721\nCarvalheiro, L. G., Seymour, C. L., Veldtman, R., & Nicolson, S. W. (2010). Pollination services decline with distance from natural habitat even in biodiversity-rich areas. Journal of Applied Ecology, 47: 810-820. doi: 10.1111/j.1365-2664. 2010.01829.x\nCastro, J., & Cerdas, M. (2005). Mora (Rubus spp.) cultivo y manejo poscosecha. Ministerio de Agricultura y Ganadería; Universidad de Costa Rica; Consejo Nacional de Producción. San José (Costa Rica), 8.\nChacoff, N. P., Aizen, M. A., & Aschero, V. (2008). Proximity to forest edge does not affect crop production despite pollen limitation. Proceedings of the Royal Society of London B: Biological Sciences, 275: 907-913. doi: 10.1098/rspb.2007.1547\nConstanza R, d’Arge R, De Groot R, Faber S, Grasso M, Hannon B, Raskin RG (1997) The value of the world’s ecosystem services and natural capital. Nature, 387: 253-260. doi: 10.1016/S0921-8009(98)00020-2\nDe Marco, P., & Coelho, F. M. (2004). Services performed by the ecosystem: forest remnants influence agricultural cultures’ pollination and production. Biodiversity and Conservation, 13: 1245-1255. doi: 10.1023/B:BIOC.0000019402.51193.e8\nDry, P. R., Reed, S., & Potter, G. (1988, May). The effect of wind on the performance of Cabernet Franc grapevines. In: Australian Temperate Fruits Review Conference 240: pp. 143-146. doi: 10.17660/ActaHortic.1989.240.24\nGallai, N., Salles, J. M., Settele, J., & Vaissière, B. E. (2009). Economic valuation of the vulnerability of world agriculture confronted with pollinator decline. Ecological Economics, 68: 810-821. doi: 10.1016/j.ecolecon.2008.06.014\nGaribaldi, L. A., Carvalheiro, L. G., Vaissière, B. E., Gemmill-Herren, B., Hipólito, J., Freitas, B. M., ... & An, J. (2016). Mutually beneficial pollinator diversity and crop yield outcomes in small and large farms. Science, 351: 388-391. doi: 10.1126/science.aac7287\nGreenleaf, S. S., Williams, N. M., Winfree, R., & Kremen, C. (2007). Bee foraging ranges and their relationship to body size. Oecologia, 153: 589-596. doi: 10.1007/s00442-007-0752-9\nHadley, A. S., & Betts, M. G. (2012). The effects of landscape fragmentation on pollination dynamics: absence of evidence not evidence of absence. Biological Reviews, 87: 526-544. doi: 10.1111/j.1469-185X.2011.00205.x\nJarau, S., & Barth, F. G. (2008). Stingless bees of the Golfo Dulce region, Costa Rica (Hymenoptera, Apidae, Apinae, Meliponini). Stapfia, 88, 267-276.\nJayasena, V., & Cameron, I. (2008). Brix/acid ratio as a predictor of consumer acceptability of Crimson Seedless table grapes. Journal of Food Quality, 31: 736-750. doi: 10.1111/j.1745-4557.2008.00231.x\nJennings, D. L. (1988). Raspberries and blackberries: their breeding, diseases and growth. Academic press.\nKennedy, C. M., Lonsdorf, E., Neel, M. C., Williams, N. M., Ricketts, T. H., Winfree, R., ... & Carvalheiro, L. G. (2013). A global quantitative synthesis of local and landscape effects on wild bee pollinators in agroecosystems. Ecology Letters, 16: 584-599. doi: 10.1111/ele.12082\nKlein, A. M., Vaissiere, B. E., Cane, J. H., Steffan-Dewenter, I., Cunningham, S. A., Kremen, C., & Tscharntke, T. (2007). Importance of pollinators in changing landscapes for world crops. Proceedings of the Royal Society of London B: Biological Sciences, 274: 303-313. doi: 10.1098/rspb.2006.3721\nKleinert-Giovannini, A., & Imperatriz-Fonseca, V. L. (1986). Flight activity and responses to climatic conditions of two subspecies of Melipona marginata Lepeletier (Apidae, Meliponinae). Journal of Apicultural Research, 25: 3-8. doi: 10.1080/00218839.1986.11100685\nKremen, C., Williams, N. M., & Thorp, R. W. (2002). Crop pollination from native bees at risk from agricultural intensification. Proceedings of the National Academy of Sciences, 99: 16812-16816. doi: 10.1073/pnas.262413599\nKrishnan, S., Kushalappa, C. G., Shaanker, R. U., & Ghazoul, J. (2012). Status of pollinators and their efficiency in coffee fruit set in a fragmented landscape mosaic in South India. Basic and Applied Ecology, 13: 277-285. doi: 10.1016/j.baae.2012.03.007\nLaurance, W. F., Sayer, J., & Cassman, K. G. (2014). Agricultural expansion and its impacts on tropical nature. Trends in Ecology and Evolution, 29: 107-116. doi: 10.1016/j.tree.2013.12.001\nLink, H. (2000). Significance of flower and fruit thinning on fruit quality. Plant Growth Regulation, 31: 17-26. doi: 10.1023/A:1006334110068\nOllerton, J., Winfree, R., & Tarrant, S. (2011). How many flowering plants are pollinated by animals? Oikos, 120: 321-326. doi: 10.1111/j.1600-0706.2010.18644.x\nRicketts, Taylor H., Williams, Neal M. and Mayfield, Margaret M. (2006). Connectivity and ecosystem services: Crop pollination in agricultural landscapes. In Kevin R. Crooks and M. A. Sanjayan (Ed.), Connectivity conservation (pp. 255-289) Cambridge, U.K.: Cambridge University Press.\nRicketts, T. H., Regetz, J., Steffan-Dewenter, I., Cunningham, S. A., Kremen, C., Bogdanski, A., Gemmill-Herren, B., Greenleaf, S.S., Klein, A.M., Mayfield, M.M., Morandin, L.A., Ochieng’, A., & Viana, B.F. (2008). Landscape effects on crop pollination services: are there general patterns? Ecology Letters, 11: 499-515. doi: 10.1111/j.1461-0248.2008.01157.x\nRoubik, D. W. (1995). Pollination of cultivated plants in the tropics (No. 118). Food & Agriculture Organization of the United Nations.\nSáez, A., Morales, C. L., Ramos, L. Y., & Aizen, M. A. (2014). Extremely frequent bee visits increase pollen deposition but reduce drupelet set in raspberry. Journal of Applied Ecology, 51: 1603-1612. doi: 10.1111/1365-2664.12325\nSteffan-Dewenter, I., & Kuhn, A. (2003). Honeybee foraging in differentially structured landscapes. Proceedings of the Royal Society of London B: Biological Sciences, 270: 569-575. doi: 10.1098/rspb.2002.2292\nStrik, B. C., Clark, J. R., Finn, C. E., & Bañados, M. P. (2007). Worldwide blackberry production. HortTechnology, 17: 205-213. doi: 10.17660/ActaHortic.2008.777.31\nStrik, B., Mann, J., & Finn, C. (1996). Percent drupelet set varies among blackberry genotypes. Journal of the American Society for Horticultural Science, 121: 371-373.\nTilman, D., Cassman, K. G., Matson, P. A., Naylor, R., & Polasky, S. (2002). Agricultural sustainability and intensive production practices. Nature, 418: 671-677. doi: 10.1038/nature01014\nTosun, I., Ustun, N. S., & Tekguler, B. (2008). Physical and chemical changes during ripening of blackberry fruits. Scientia Agricola, 65: 87-90. doi: 10.1590/S0103-90162008000100012\nVergara, C. H., & Badano, E. I. (2009). Pollinator diversity increases fruit production in Mexican coffee plantations: the importance of rustic management systems. Agriculture, Ecosystems and Environment, 129: 117-123. doi: 10.1016/j.agee.2008.08.001\nWang, S. Y., Chen, C. T., & Wang, C. Y. (2009). The influence of light and maturity on fruit quality and flavonoid content of red raspberries. Food Chemistry, 112: 676-684. doi: 10.1016/j.foodchem.2008.06.032\nWenner, A. M. (1963). The flight speed of honeybees: a quantitative approach. Journal of Apicultural Research, 2: 25-32.doi: 10.1080/00218839.1963.11100053\nWhiting, M. D., Lang, G., & Ophardt, D. (2005). Rootstock and training system affect sweet cherry growth, yield, and fruit quality. HortScience, 40: 582-586.\nHow to Cite\nSociobiology is a diamond open access journal which means that all content is freely available without charge to the user or his/her institution. Users are allowed to read, download, copy, distribute, print, search, or link to the full texts of the articles in this journal without asking prior permission from the publisher or the author. This is in accordance with the BOAI definition of open access.\nAuthors who publish with this journal agree to the following terms:\n- Authors retain copyright and grant the journal right of first publication with the work simultaneously licensed under a Creative Commons Attribution License that allows others to share the work with an acknowledgement of the work's authorship and initial publication in this journal.\n- Authors are able to enter into separate, additional contractual arrangements for the non-exclusive distribution of the journal's published version of the work (e.g., post it to an institutional repository or publish it in a book), with an acknowledgement of its initial publication in this journal.\n- Authors are permitted and encouraged to post their work online (e.g., in institutional repositories or on their website) prior to and during the submission process, as it can lead to productive exchanges, as well as earlier and greater citation of published work (See The Effect of Open Access).""]"	['<urn:uuid:5e6bb9c6-7882-44fb-a155-32efc28a70d5>', '<urn:uuid:d3f4356b-6019-41cd-a079-329d26bbe3e7>']	factoid	direct	verbose-and-natural	similar-to-document	multi-aspect	novice	2025-05-12T20:46:45.703947	25	81	2575
3	remove scratches from photo how	The clone tool (also called the rubber stamp) allows you to remove imperfections such as dust and scratches by covering them with samples of nearby pixels. For best results, zoom in close and use a small brush.	"['Photoshop provides numerous powerful tools for working with images. This section describes key tools for basic image editing.\nKeep in mind the following when editing images:\nIt is always a good idea to keep your original image as a backup and work on a copy, in case you make a mistake and want to start over. (If you want to go back to the file as it was when you opened it, choose File>Revert. You can also go back to a previous state of your image by using the History palette.)\nIf you have selected an area (using the selection tools), adjustments you make affect only the selected area. Otherwise most adjustments affect all the pixels in the image.\nIf your image has multiple layers, adjustments you make generally affect only the current layer. (The exception to this is adjustment layers).\n1. Choose the Crop tool () in the toolbox. Then click and drag across the image. An outline will appear that shows you the area to be cropped.\n2. Adjust the area to be cropped by using handles on the edges of the image.\nNote: If you move the cursor just outside of one of the corner handles, it turns into a curved cursor, which then allows you change the alignment of the selected area. This is useful, for example, to correct a scan of a picture that was in a crooked position on the scanner.\n3. Press Enter to crop or Escape to cancel\nThe Levels command (Image>Adjust>Levels) is an excellent tool for adjusting contrast. The Levels dialog box displays a histogram of the brightness values of the pixels in your image. There are two sets of sliders (triangles): input levels and output levels. For most editing tasks, use the input levels sliders (the three triangles just below the histogram).\nTo increase contrast (create more dark and light pixels): Move the left and right sliders toward the middle.\nTo adjust the midtones: Move the middle input levels slider to the left to lighten midtones, and move the middle slider to the right to darken the midtones.\nKeep the Preview box checked in order to see the effects of moving the sliders on your image.\nTo make the adjustment, click OK. To cancel it, click Cancel.\nIn the above example, there are few (if any) pixels that are white or even light gray. By moving the right slider towards the middle, we can lighten image to correct the exposure. Because the Preview box in the layers palette is checked, we can see the result on our image as we adjust the sliders.\nNote: In the above example, the Levels dialog box Channel popup menu is set to the default of RGB, which means the histogram represents a combination of all of the channels (Red, Green, and Blue light) in the image. You can use Levels on an individual channel, but this is likely to change the color balance of the image, so in most cases is not likely to be a good idea.\nNote: The Levels command is often a good way to get a more even distribution of briightness of pixels. But this is not always desirable. For example, a properly exposed photograph of something dark such as a coal mine whould be dark, and an image of a snow scene would be white. In such cases the histogram would be lopsided. In such a case there may be no need to make an adjustment in the Levels dialog box.\nFor more control over a levels adjustment, consider using a levels adjustment layer (Layer menu > New Adjustment Layer > Levels...). A Levels adjustment layer allows you to adjust levels nondestructively. You can reopen your Photoshop file and change the layers adjustment in your Photoshop file whenever you want.\nOften you may need or want to change the balance of color in an image. For example, a photo take outdoors may look too blue, or a photo taken indoors may look yellow or orange.\nThere are several ways to adjust color, many of which are under Adjustments in the Image menu, including Color Balance, Color Mixer, and Photo filters. These tools are easy to use and the best way to learn about them is to open a practice image and choose one of these commands, and experiment with the settings in the dialog box that appears for the command.\nThe easiest color correction tool is Auto Color. Auto Color often (though not always) gives very good results. Auto Color attempts to color correct images automatically by analyzing the highlights, shadows, and midtones of each channel.\nChoose Image>Auto Color to use the Auto Color command and see if you like the results.\nThe following is an example of an image before Auto Color was applied. The image is good but slightly more blue than I would prefer:\nThe following is the image after applying Auto Color. The difference is subtle, but Auto Color added a little warmth to the image.\nThe clone tool ( ), also called the rubber stamp, allows you to remove imperfections such as dust and scratches by covering them with samples of nearby pixels. For best results, zoom in close and use a small brush.\n1. Zoom in close to the area you want to edit.\n2. Choose the clone tool, and make sure ""aligned"" is checked in the Options bar.\n3. In the options bar, choose a brush size (this determines the size of the sample). The cursor shows the size of the brush, as shown below. The circle is the cursor (brush).\n(Note: if the cursor doesn\'t show the brush size, go to Edit>Preferences>Display & Cursors, and choose Brush Size for Painting Cursors.)\n4. Alt-click to define initial source point (sample).\n5. Move the cursor over the area you want to cover, and click to cover it with the sample.\nThe steps for using the healing brush tool ( ) are the same as for the clone tool. The healing brush attempts to take the texture from the sample without affecting the brightness of the area to which it is applied. In some cases this is more effective than the clone tool.\nUse Filter>Sharpen>Unsharp Mask to sharpen an image. In the dialog box that appears, keep the preview box checked. This allows you to see the effects of different settings.\nAmount -- How much sharpening is applied. Consider using a value between 50% and 150% for typical images.\nRadius -- The number of pixels affected around edges. Consider using a value between 0.2 and 2.0. (Deke McClelland recommends using 0.1 of radius for every 15 ppi in the image. For example, for an 150 ppi image, you could use a radius value of 1; for 300 ppi, you could use a radius of 2.)\nThreshold -- Defines what brightness difference qualifies as an edge (0 is default, sharpens all pixels). Consider using a value somewhere between 1and 5 (for Threshold, the higher the value, the less pronounced the sharpening effect).\nNote: Sharpening is more an art than a science, and is somewhat subjective. The above settings are only suggestions. For best results, you need to look at the particular image and figure out for yourself what looks best! Make sure that in Photoshop the image is at 100% magnification, and keep the Preview box checked in the Unsharp Mask dialog box .']"	['<urn:uuid:780f5658-a0a6-41ec-a8c2-81276f938998>']	factoid	direct	short-search-query	distant-from-document	single-doc	novice	2025-05-12T20:46:45.703947	5	37	1229
4	why does police abuse happen first hours custody research findings	According to independent research commissioned by the APT titled 'Does Torture Prevention Work?' published in 2016, torture and ill-treatment are more likely to occur in the first hours of detention. These practices persist due to several enabling factors: gaps between existing laws and practice, ongoing trends and culture in policing, and persuasive narratives that support these practices in the name of public security and safety.	['New synergies to reduce torture and ill-treatment in policy custody in South East Asia\nThe Association for Prevention of Torture (APT), Suara Rakyat Malaysia (SUARAM) in Malaysia, Task Force Detainees of the Philippines (TFDP) and Cross Cultural Foundation (CrCF) in Thailand are launching a three and a half year cooperation, to foster national and regional synergies to prevent incommunicado detention and forced confession in the three countries. The European Union supports this cooperation.\nTorture and ill-treatment are more likely to happen in the first hours of detention. This was confirmed by the independent research commissioned by the APT, “Does Torture Prevention Work?”, published in 2016. Forced confession and incommunicado detention have been prevalent in Thailand, Malaysia and the Philippines, for many years. According to thematic reports issued by SUARAM, TFDP and CrCF, these illegal practices continue due to several enabling factors: the gap between existing laws and practice; the on-going trends and culture in policing; and the persuasive narrative that supports these practices in the name of public security and safety.\nTo tackle these root causes, the APT, SUARAM, TFDP and CrCF embark on a three-year and a half project: “Reducing risky practices leading to torture and ill-treatment in police custody in Malaysia, Philippines and Thailand”. The new cooperation will focus on a) increasing transparency in police custody, by enhancing oversight capacity, b) improving police practices by implementing safeguards and non-coercive interviewing and c) changing public mind-sets to reject torture by the police, by increasing knowledge on rights and mobilising artists and youth. “This cooperation will encourage us to think outside of the box and pilot new approaches to torture prevention” noted Barbara Bernath, Secretary General of the APT. “Embarking on this journey of prevention with three leading civil society organisations known to drive the efforts for change in the criminal justice system in the three countries is a fantastic opportunity to foster synergies and bring about change”, Bernath continued.\nTo formalise the mutual partnership and strengthen the existing relationship between the four civil society organisations, a Memorandum of Understanding was signed today during an online ceremony.\nDuring the event, SUARAM, TFDP and CrCF reiterated the valuable contributions the project will bring to their on-going advocacy to prevent torture in their countries.\nSevan Doraisamy, the Executive Director of SUARAM stated, “There are room and positive reception for popular mediums such as cartoons and films to influence narratives that condone torture. In the Malaysian context, the Cartoonist against Torture initiative spearheaded by SUHAKAM and Act4Cat Campaign provides a strong foundation for expanding the use of these creative mediums through the project”\nEmmanuel C. Amistad, the Executive Director of TFDP on the other hand, deliberated further on the project’s benefit on enhancing transparency in detention. “Transparency in detention means enabling unhampered access to police detention by independent oversight in order to monitor their practices. The project’s aim to institute and introduce effective measures and guiding principles that enables transparency in detention will strengthened our long standing work with the Commission on Human Rights and the Interim National Preventive Mechanism.”\nMeanwhile, Pornpen Khongkachonkiet, the Executive Director of CrCF touched on the prerequisite to effective dialogue and cooperation with the law enforcement, “We need to have the right language to engage, build trust and enhance the political will of the police force to stop the use of risky practices such as forced confession and incommunicado detention. Building confidence and trust of the police is important to pave way for closer cooperation.”\nThe event was also attended by the four civil society organisations’ long standing partners in the three countries; the Commission on Human Rights of the Philippines (CHRP), the National Human Rights Commission of Malaysia (SUHAKAM) and the Department of Rights, Liberty, Protection, Ministry of Justice of Thailand (DRLP-MOJ). While this new project will continue to build on these existing relationships, it will seek to foster greater synergy amongst a wide range of actors, bringing together law enforcement agencies, artists and youth.\nThe APT, SUARAM, TFDP and CrCF are grateful for the generous financial support provided by the European Union (EU) and look forward to working together towards a common goal of eradicating any practices that condone torture and ill-treatment in Malaysia, the Philippines and Thailand, as well as the South East Asian region. As Amistad succinctly concluded, “What we really hope to achieve is a society where torture is banished, a society where the dignity of each and every individual is supreme, a society where we can love and care for each other, a society where the only thing contagious is kindness. These are the dreams worth fighting for and our step today is towards the fulfilment of these dreams”\nDisclaimer: the European Union funds the project “Reducing risky practices leading to torture and ill-treatment in police custody in Malaysia, Philippines and Thailand”. However, the content of this press statement is the sole responsibility of the APT, SUARAM, TFDP and CrCF and do not necessarily reflect the views of the European Union.\nPress enquiries: For media opportunities and information pertaining to the project, please contact:\nShazeera Ahmad Zawawi, Senior Adviser, Research and Innovation, Association for Prevention of Torture (APT): email@example.com\nSUARAM has conducted extensive documentation, monitoring and advocacy work relating to police brutality, torture prevention and indefinite detention over the past 30 years. Apart from advocacy in this area, SUARAM is also the coordinating body for the Movement for the Abolishment of the Sedition Act 1948 (GHAH), organiser for Youth Internet Governance Forum 2019, Co-secretariat for Malaysian CSO Cluster for Reform and Co-Secretariat in the Coalition of Malaysian NGO in the UPR Process (COMANGO).SUARAM is also part of the national network coordinated by the National Human Rights Commission of Malaysia (SUHAKAM) called ACT4CAT, a network that advocates for UNCAT ratification by Malaysia. SUARAM is also a member of regional and international organisation including Forum-Asia (one of the founding member), Asian Alliance against Torture (A3T, one of the founding member), FIDH, OMCT and I4C. For more details about SUARAM, visit: https://www.suaram.net/\nTFDP is a membership organization and a Mission Partner of the Association of Major Religious Superiors in the Philippines (AMRSP). TFDP documents human rights violations, assists victims and their families, organizes missions, conducts human rights education work, campaigns against torture, and promotes advocacy for the protection of human rights defenders. TFDP is part of Forum- Asia, a network of 81 NGOs in 21 countries that promote and protect human rights across Asia. In the regional and international levels, TFDP is a member of the OMCT-SOS Torture Network, Asian Forum for Human Rights and Development (FORUM Asia) and Asia Alliance against Torture (A3T). For more details about TFDP, visit: https://www.tfdp.net\nCrCF works on justice and the protection, promotion and monitoring of human rights in Thailand. CrCF’s philosophy and activities are focused on strengthening human rights and delivering sustainable judicial reform throughout society. Particular emphasis is placed on marginalised people including ethnic minority groups, stateless people, migrant workers and the victims of conflict. CrCF’s strategies include strategic litigation and legal aid; capacity building; research and advocacy. CrCF is also a member of regional and international organisations including Forum-Asia, Asian Alliance against Torture (A3T, one of the founding members) and OMCT. For more details about CrCF, visit: https://crcfthailand.org/']	['<urn:uuid:07d3ac11-4e7b-4fe8-b865-71ab2d35c65e>']	open-ended	direct	long-search-query	distant-from-document	single-doc	novice	2025-05-12T20:46:45.703947	10	65	1202
5	I've been watching crime shows and archaeology documentaries - what's the main difference between how archaeologists and forensic pathologists contribute to identifying human remains?	Archaeologists focus on analyzing material remains including bones to understand past human behaviors and their changes through time, while forensic pathologists work with modern deaths to identify deceased individuals through autopsies and collaborate with various specialists like anthropologists, entomologists, and odontologists to determine cause and manner of death.	"['The field of archaeology has provided the action-packed backdrop for countless films, novels, video games and even children’s toys over the years. For many people, just the word “archaeology” conjures up images of Indiana Jones swinging from his whip, or Lara Croft, pistols in hand, racing around the world in search of ancient treasures. Exotic locales, groundbreaking discoveries, and the search for treasure often serve as the driving forces behind such romanticized portrayals of archaeology in popular culture. But what is the field of archaeology really like, and what do archaeologists actually do in the real world?\nArchaeology is the study of the human past through the recovery, analysis, and interpretation of material remains. Material remains can include anything from prehistoric stone tools and pottery, to buildings, bones, and even trash heaps. Such remains are direct products of human behavior and the relationship of humans to their environment. Through the analysis and interpretation of material remains, archaeologists can help us to understand past human behaviors and how these behaviors have changed through time.\nArchaeology is often considered to be a subdiscipline of anthropology, along with physical anthropology, cultural anthropology and linguistics. Although the term “archaeology” literally means “the study of ancient things,” the field has grown over the decades to incorporate all periods of the human past, including recent history. The field is often roughly separated into two general groupings: prehistoric archaeology and historic archaeology.\nSearch Archaeology Job Opportunities\nPrehistoric archaeology focuses on societies that have left behind little to no literary evidence, while historic archaeology focuses on more recent history in which literary evidence is available to use in conjunction with material remains.\nThere are a variety of career paths available for professional archaeologists.\n- Cultural resource management firms\n- Federal government archaeologist jobs\n- State government jobs\n- Colleges and universities\n- Non-profit organizations\nSuch employers present a variety of archaeology job opportunities for professional archaeologists, many of which we will be discussing in the following pages. Examples of archaeological jobs include:\n- Conducting research on shipwrecks as an underwater archaeologist\n- Identifying bones at archaeological sites as a bioarchaeologist\n- Creating digital maps of archaeological sites in a GIS specialist job (geographic information system)\n- Cleaning and preserving artifacts as an archaeological lab director\n- Teaching and training future archaeologists as a university professor\nThese, of course, are just a few of the many different careers that archaeologists can choose. We hope that you will continue to explore the world of archaeological job possibilities in the pages to come.\nBecause the responsibilities for these different positions can be extremely diverse, it is important for successful archaeologists to develop an array of skills. Some key skills that successful archaeologists will acquire over the course of their career include:\n- Excavating archaeological sites\n- Analyzing artifacts in the laboratory\n- Developing a strong familiarity with historic preservation laws\n- Perfecting verbal and written communication skills\nIn the following pages, you will find an overview of the study of archaeology, the practice of archaeology in the professional world, and the different career paths that are available to the trained archaeologist. Special attention will be given to the pages dealing with the different archaeological career paths, which focus on such fundamental job issues as compensation, required education and qualifications, and general availability of positions both internationally and nationwide.\nThe final page of this topic provides a listing of valuable archaeological resources designed to advance your research into the archaeological career field, including websites of professional organizations and job search sites created by and for professional archaeologists.\nArchaeology in a Modern Time\nThe archaeology industry has made significant progress thanks to modern technologies. Find out more below.\nJobs in Archaeology\nThere is a wide range of jobs available in the archaeological industry. Follow the links below to learn more.\nScience and Archaeology\nIf discovering ancient relics with the power of modern technology sounds interesting, click the links below to learn more.', ""what is forensic pathology?\nForensic pathology is a sub-specialty of histopathology, and is concerned with the application of pathological principles to the investigation of the medico-legal aspects of death.\nForensic pathologists are medically qualified doctors who perform autopsies (postmortem examinations) on those who have died suddenly, unexpectedly, or as a result of trauma or poisoning.\nThe forensic investigation of death is a multi-disciplinary activity, involving the collaboration between pathologists, crime scene investigators (CSIs), forensic scientists, and other specialists, such as anthropologists, entomologists, odontologists (dentists) and many other experts.\nAutopsy findings are combined with the results of other investigations, including the microscopic examination of organs and tissues removed at autopsy, toxicological analyses (of blood and urine, for example), and correlated with the available clinical or medical history of the deceased, as well as the circumstances of their death, in order to answer questions relating to their death.\nThe issues raised by a death may include:\n- identification of the deceased,\n- the medical cause of death,\n- the interpretation of injuries, and\n- the manner of death (in some jurisdictions), i.e. accident, suicide or homicide\nThose who have dissected or inspected many bodies have at least learned to doubt, while those who are ignorant of anatomy and do not take the trouble to attend to it, are in no doubt at all.\nGiovanni Morgagni (1682-1771)\nTaceant colloquia. Effugiat risus. Hic locus est ubi mors gaudet succurrere vitae. (Let conversation cease. Let laughter flee. This is the place where death delights to help the living.)\nLatin proverb (Saukko P, Knight B. Knight's Forensic Pathology (3rd Edition) 2004. Arnold Publishers.\nWhat does a forensic pathologist do?\nDr Michael Pollanen on the role of a forensic pathologist\nA well-run mortuary doesn’t really smell; it is washed frequently and properly ventilated, and most bodies examined are fresh. But I’d never smelled anything like that putrefied body; it was an overwhelming odor, dense, wet, vile, almost shockingly sweet, like the vomit of a drunk; it seemed to coat the skin and settle into clothes. I felt nauseated, and stepped back outside the room, closed the door behind me and leaned against the wall, retching.\nDr Jonathan Hayes. Forensic Pathologist New York City.\nHistorical illustration of a pathologist examining a body at a death scene (pre-DNA era)\nSource: Science Against Crime, Kind S\nforensic pathology and local communities - the Victorian Institute of Forensic Medicine, Melbourne Australia\nforensic pathology - principles and practice\nforensic medicine - clinical and pathological aspects\nforensic pathology spoiling 'the perfect murder'?\nSee forensic pathologist Dr Richard Shepherd discuss the forensic pathological investigation of suspicious death in this BBC documentary on 'How to commit the perfect murder'.\nTime of death, Snyder Sachs, J\nBuy it here\nSee it at Google Books here\nAmid the plethora of popular books on forensic science, it's hard for writers to find a new slant. But Jessica Sachs has found one: her main themes are entomology, botany and ecology, and in particular how they help to establish time of death. Thankfully, Sachs fully acknowledges the biological variations that prohibit the ludicrous accuracy with which time of death is estimated in so many novels and television dramas.\nKnight B. In the New Scientist 2001 (read the full review here)\n|By||Dr Richard Jones|\nDo you have any suggestions for topics to be covered in this website?\nAre there any topics that are covered particularly well?\nWhere could improvements be made?""]"	['<urn:uuid:d737c4bf-b0c6-4efa-9cbd-e59f8e5ff0dc>', '<urn:uuid:0e29c859-747c-4afd-a034-8a337772e91c>']	factoid	with-premise	verbose-and-natural	distant-from-document	comparison	novice	2025-05-12T20:46:45.703947	24	48	1229
6	laboratory dna handling optimal conditions preparation	In laboratory settings, DNA handling requires specific conditions and preparation steps. For genomic DNA, direct extraction from cells is possible and it contains both coding and non-coding sequences. For cDNA, synthesis requires reverse transcription from RNA templates using reverse transcriptase enzyme. When working with either type in procedures like CRISPR/Cas9 KO Plasmid transfection, cells should be grown to 40-80% confluency in antibiotic-free medium. The procedure requires careful preparation of DNA solutions, typically using 1-3 µg of Plasmid DNA mixed with appropriate transfection reagents, followed by 24-72 hours of incubation under controlled conditions. Success can be confirmed through detection of fluorescent proteins via microscopy or Western blot.	['Main Difference – cDNA vs Genomic DNA\nComplementary DNA (cDNA) and genomic DNA are two types of DNA molecules used in the research experiments in molecular biology. Both cDNA and genomic DNA are made up of DNA nucleotides. cDNA is produced by the reverse transcription of the extracted RNA from tissue. The types of RNA used for the reverse transcription can be total RNA, pre-mRNA, RNA, ribosomal RNA, or tRNA. However, genomic DNA can be directly isolated from the cell. The main difference between cDNA and genomic DNA is that cDNA represents the transcriptome of a particular organism whereas genomic DNA represents the genome.\nKey Areas Covered\n1. What is cDNA\n– Definition, Facts, Role\n2. What is Genomic DNA\n– Definition, Facts, Role\n3. What are the Similarities Between cDNA and Genomic DNA\n– Outline of Common Features\n4. What is the Difference Between cDNA and Genomic DNA\n– Comparison of Key Differences\nKey Terms: Complementary DNA (cDNA), Genome, Genomic DNA (gDNA), Reverse Transcription, RNA, Transcriptome\nWhat is cDNA\ncDNA (complementary DNA) refers to the synthetic DNA whose base sequences are complementary to the DNA. It is produced from the reverse transcription of RNA templates. Total RNA, pre-mRNA, RNA, ribosomal RNA or tRNA can be used for reverse transcription. Reverse transcriptase is the enzyme that catalyzes the reaction of producing cDNA from RNA. The reverse transcription coupled with PCR is shown in figure 1.\ncDNA is mainly used in cloning eukaryotic genes in the prokaryotic genomes. Exons consist of the coding region of proteins. In eukaryotes, the coding region is interrupted by introns. During transcription, both introns and exons are coded into the pre-mRNA. But during the mRNA processing, the introns are removed from mRNA, producing a mature mRNA by splicing the exons together. The total mRNA of an organism is called the transcriptome. However, prokaryotes lack mRNA processing mechanisms. Thus, mRNA is used to synthesize cDNA that only contains protein coding regions of the genome. This cDNA is used to clone genes into prokaryotes. cDNA is naturally produced in retroviruses during the conversion of the RNA genome into DNA.\nWhat is Genomic DNA\nGenomic DNA (gDNA) refers to the total set of chromosomal DNA in the genome. The total set of genes of a particular organism is referred as to its genome. However, not only genes of the genome are active. Most groups of organisms have similar types of genomic DNA within the group. The genome of the organism is passed from the next generation to the other. Generally, genomes are made up of a large number of base pairs. As an example, the human genome consists of around 3 billion base pairs. The chromosomes of the human genome are shown in figure 2.\nThe four types of sequences found in the genomic DNA are protein-coding genes, tandemly repeated genes, repetitive DNA, and spacer DNA. Genomic DNA is used in many experiments in molecular biology. The isolation of genomic DNA is easy when compared to the preparation of cDNA. Genomic DNA can be used as PCR templates, in cloning, sequencing, preparing genomic libraries, DNA fingerprinting, studying gene structures, and detecting mutations/abnormalities.\nSimilarities Between cDNA and Genomic DNA\n- cDNA and genomic DNA are two types of DNA molecules used in the research experiments in molecular biology.\n- Both cDNA and genomic DNA are made up of DNA nucleotides.\n- Both cDNA and genomic DNA are made up of a sugar-phosphate backbone.\n- Both cDNA and genomic DNA are double-stranded molecules.\n- Both cDNA and genomic DNA are composed of exons.\nDifference Between cDNA and Genomic DNA\ncDNA: cDNA is the synthetic DNA whose base sequences are complementary to Genomic DNA.\nGenomic DNA: Genomic DNA is the total set of chromosomal DNA in the genome.\ncDNA: cDNA represents the transcriptome.\nGenomic DNA: DNA represents the genome.\ncDNA: cDNA is synthesized from the reverse transcription of different types of RNA.\nGenomic DNA: Genomic DNA can be extracted from existing genomes.\ncDNA: cDNA consists of exons or coding regions.\nGenomic DNA: Genomic DNA consists of coding and non-coding sequences of an organism.\ncDNA: cDNA contains a few base pairs.\nGenomic DNA: Genomic DNA contains a large number of base pairs.\ncDNA: cDNA is used to produce cDNA libraries.\nGenomic DNA: Genomic DNA can be used to produce genomic libraries.\ncDNA and genomic DNA are two types of DNA molecules used in various types of experiments in molecular biology. cDNA is synthesized from RNA, and it only consists of exons. Therefore, it represents the transcriptome of an organism. Genomic DNA can be readily extracted from the nucleus of the cell. It consists of both coding and non-coding regions. It represents the genome of a particular organism. The main difference between cDNA and genomic DNA is the type of DNA present in each type of DNA molecules.\n1. “CDNA (Complementary DNA).” Human Genes, Available here.\n2. “GDNA – Genomic DNA | Advanced Analytical (AATI).” Advanced Analytical Technologies, Inc. (AATI), Available here.\n3. “Genomic DNA Isolation and Applications.” Scribd, Available here.', 'CRISPR / Cas9 KO Plasmid and HDR Plasmid Transfection\nPHASE 1. CRISPR/Cas9 KO Plasmid and HDR Plasmid Transfection\nThis protocol is recommended for a single well from a 6-well tissue culture plate. Adjust cell and reagent amounts proportionately for wells or dishes of different sizes.\n- In a 6-well tissue culture plate seed 1.5x 105 - 2.5x 105 cells in 3 ml of antibiotic-free standard growth medium per well, 24 hours prior to transfection. Grow cells to a 40–80% confluency. Initial cell seeding and cell confluency after 24 hours are determined based on the rate of cell growth of the cells used for transfection. Healthy and subconfluent cells are required for successful KO and HDR Plasmid transfection.\n- Prepare the following solutions:\nNOTE: The optimal Plasmid DNA:UltraCruz® Transfection Reagent (sc-395739) ratio should be determined experimentally beginning with 1 µg of Plasmid DNA and between 5-15 µl of UltraCruz® Transfection Reagent. Once the Transfection Reagent volume is optimized to minimize cell toxicity, Plasmid DNA concentrations can vary between 1-3 µg per well. If the optimal UltraCruz® Transfection Reagent volume is 10 µl, then Plasmid DNA concentrations ranging from 1-3 µg/10 µl should be tested. For example, test Plasmid DNA/UltraCruz® Transfection Reagent amounts: 1 µg/10 µl, 2 µg/10 µl, and 3 µg/10 µl. The appropriate amount of Plasmid DNA/UltraCruz® Transfection Reagent complex used per well should be tested to determine which amount provides the highest level of transfection efficiency.\nNOTE: If transfecting more than one plasmid (i.e. CRISPR/Cas9 KO Plasmid with HDR Plasmid), mix Plasmid DNA at equivalent ratios.\nSolution A: For each transfection, dilute 1–3 µg of Plasmid DNA into Plasmid Transfection Medium: sc-108062 to bring final volume to 150 µl. Pipette up and down to mix. Let stand for 5 minutes at room temperature.\nSolution B: For each transfection, dilute 5–15 µl of UltraCruz® Transfection Reagent: sc-395739 with enough Plasmid Transfection Medium: sc-108062 to bring final volume to 150 µl. Pipette up and down to mix. Let stand for 5 minutes at room temperature.\nNOTE: Do not add antibiotics to the Plasmid Transfection Medium: sc-108062.\n- Add the Plasmid DNA solution (Solution A) dropwise directly to the dilute UltraCruz® Transfection Reagent (Solution B) using a pipette. Vortex immediately and incubate for no less than 20 minutes at room temperature.\n- Prior to transfection, replace media with fresh antibiotic-free growth medium. Add the 300 µl Plasmid DNA/UltraCruz® Transfection Reagent Complex (Solution A + Solution B) dropwise to well.\n- Gently mix by swirling the plate.\n- Incubate the cells for 24–72 hours under conditions normally used to culture the cells. No media replacement is necessary during the first 24 hours post-transfection. Add or replace media as needed 24–72 hours post-transfection.\n- After incubation, successful transfection of CRISPR/Cas9 KO Plasmid may be visually confirmed by detection of the green fluorescent protein (GFP) via fluorescent microscopy and/or Western blot with GFP Antibody (B-2): sc-9996. Successful co-transfection of the CRISPR/Cas9 KO Plasmid and HDR Plasmid, may be visually confirmed by detection of the red fluorescent protein (RFP) via fluorescent microscopy and/or Western blot.\n- For cells transfected with CRISPR/Cas9 KO Plasmid, assay cells 48–72 hours after transfection step.\n- For cells co-transfected with CRISPR/Cas9 KO Plasmid and HDR Plasmid go to Phase 2.\nNOTE: If puromycin selection or Cre Vector transfection are not applicable see Phase 4.\nPHASE 2. Puromycin Selection\nNOTE: If cells were co-transfected with CRISPR/Cas9 KO Plasmid and HDR Plasmid, cells can be selected with media containing puromycin.\n- The working puromycin concentration for mammalian cell lines ranges from 1–10 µg/ml. Prior to using the puromycin antibiotic (sc-108071), titrate the selection agent to determine the optimal concentration for target cell line. Use the lowest concentration that kills 100% of non-transfected cells in 3–5 days from the start of puromycin selection.\n- 48–96 hours post-transfection, aspirate the medium and replace with fresh medium containing puromycin at the appropriate concentration.\n- Select cells for a minimum of 3–5 days. Approximately every 2–3 days, aspirate and replace with freshly prepared selective media.\n- Cells may be assayed at this point.\n- For excision of the puromycin gene, proceed to Phase 3.\nPHASE 3. Cre Vector Transfection\nThis protocol is recommended for selected cells co-transfected with CRISPR/Cas9 KO Plasmid and HDR Plasmid, and for the removal of genetic material flanked by LoxP sites.\nNOTE: Follow Phase I plasmid transfection protocol for Cre Vector: sc-418923 transfection.\nPHASE 4. Cell Assay\nComplete phenotypic and/or genotypic analysis may require isolation of single cell colonies to confirm complete allelic knockouts.\n- For protein analysis, change media to standard growth medium 3 days prior to cell lysis. To lyse adherent cells, aspirate media, rinse cells with PBS, scrape and centrifuge cells at low speed to obtain a cell pellet. For suspension cells, transfer the culture to a centrifuge tube and centrifuge cells at low speed to obtain a cell pellet. Wash once with PBS and centrifuge again. For 100% confluent HEK 293 or HeLa cells, add 100 µl of RIPA Lysis Buffer System: sc-24948 to the pellet. For other cell lines or confluencies, the amount of RIPA Lysis Buffer System to use should be determined experimentally. Sonicate or shear cells. Incubate sample on ice for 10 minutes, vortex, and incubate again for 10 minutes on ice. Spin cell lysate at 10000 RPM for 20 minutes at 4° C. Use the BCA Protein Assay Kit: sc-202389 to determine protein concentration.\n- For RT-PCR analysis isolate RNA using the method described by P. Chomczynski and N. Sacchi (1987). Single-step method of RNA isolation by acid guanidinium thiocyanate-phenol-chloroform extraction. Anal. Biochem. 162: 156–159) or a commercially available RNA isolation kit.\nPMID: 24157548, PMID: 23287718']	['<urn:uuid:de37498d-1409-4745-8664-f337d0f042eb>', '<urn:uuid:b6611b0a-5aa9-4f82-8aea-71d9d2aafda8>']	open-ended	direct	short-search-query	distant-from-document	three-doc	expert	2025-05-12T20:46:45.703947	6	106	1778
7	I'm curious about what inspired the book Hot Maroc - can you tell me what kind of real life experiences led to writing this story about internet trolls?	The book was inspired by the author's personal experience of being harmed by online trolls in Morocco. These anonymous individuals with fake names were poisoning the online atmosphere on Facebook and other internet platforms. The widespread nature of this phenomenon in Morocco led to the creation of the protagonist Rahhal Laâouina, initially intended for a short story but eventually developed into a full novel.	"['Interview with longlisted author Yassin Adnan\nWhen did you begin writing Hot Maroc and where did the inspiration for it come from?\nI began writing Hot Maroc at a writers\' residency on the Cote D\'Azur in March 2011. At that time I had been harmed by some online trolls who cause people trouble in the virtual atmosphere. I was wondering: ""Who are these anonymous people with borrowed names who poison the online atmosphere on facebook and various interactive platforms on the internet? And why are they intent on spoiling people\'s dreams and attacking everything that moves?"" The phenomenon had become widespread in Morocco. So I decided to work on it. Thus I invented Rahhal Laâouina and made him the protagonist of a short story which I intended to finish during the residency, before finding myself caught up in its story lines, hidden corners and narrative mazes for a longer period than I had imagined.\nDid the novel take long to write and where were you when you finished it?\nIt took a whole five years, since I only finished it at the beginning of September 2015 in Brussels, having dedicated all of my Summer holiday to the last few chapters. It was an improvised writers\' residency and I was a guest of my brother, the poet Taha Adnan, who hosted me in his Brussels flat. We\'d wake up in the morning and he would go off to work, while I stayed alone in the flat occupied with the novel. I didn\'t return to Morocco that Summer until I had finished the work. The funny thing is that not even a single chapter of this novel, which speaks of Marrakech and changes in the city and country as a whole over the last four decades, was actually written in Marrakech. I began it in the south of France and finished it in Brussels, and large parts of it were written during two writers\' residencies in America (one in Montalvo, California, and the other in Vermont). In Marrakech I couldn\'t continue working on the book. It was as if I needed some distance from the city in order to write about it better. It was a particular challenge to write about the city of which Nobel laureate Elias Canetti wrote his wonderful Voices of Marrakech, Claude Ollier his Marrakch-Médine and Juan Goytisolo wrote more than one novel, while Marrakech has remained impenetrable and resistant to those who belong to it.\nHow have readers and critics received it?\nIt had a reassuring reception by critics and so far the Arab press has published a number of encouraging reviews of it. People on facebook are enthusiastic about the novel. I admit that I\'m satisfied with the positive engagement of readers until now. Besides Dar al-Ain’s edition that was nominated for the International Prize for Arabic Fiction, Hot Maroc has also been published in a local edition in Morocco, which ran out in less than four months, so the Moroccan publishing house released a second edition. A few weeks ago, I was contacted by the Students\' Parents and Guardians Association in one of the secondary schools in the district of Massira, where important events in the novel take place. They asked for 40 copies for their reading club. Then I discovered that the distributor no longer had copies, so the publisher sent me the last 20 copies left over from the second edition. This happened before it was announced that the novel was longlisted for the prize. Today I am impatiently waiting for the Casablanca book fair and Dar al-Ain\'s copies to enter Morocco. New readers have begun to ask about the novel after the longlist announcement and they cannot find copies in Moroccan bookshops.\nWhat is your next literary project after this novel?\nI am busy with two literary projects both linked with Marrakech. One is an anthology of short stories called Marrakech Noir and I am working into the hours of the night to edit it for Akashic Books in America. It is a real challenge for me, because the tradition of crime stories isn\'t well-established in our Moroccan culture. So I had to make a great effort to convince literary figures of Marrakech to write dark stories. By the way, these writers are ready to write about the secrets of their city, about its dreams and scandals, but not about crimes. Maybe because the city is basically given over to scandals and its inhabitants never tire of re-hashing them because they adore tales. The second book focuses on the crumbling parts of Marrakech. I invited writers from Marrakech from different generations to examine their store of memories and bring things to mind, features of the city which we have lost in the past decades. Each of us chose something, maybe a cinema he first visited as a boy, a mosque, corner of the city, a café or a bar. A little garden now eaten by cement. A space special to him in childhood or youth. It is an attempt to recapture features of the city which we have lost or are slowly being eroded, even if merely through nostalgic remembrance.']"	['<urn:uuid:ae32429d-0a4d-46b5-84e0-c6675da0c367>']	open-ended	with-premise	verbose-and-natural	distant-from-document	single-doc	novice	2025-05-12T20:46:45.703947	28	64	858
8	lawn care summer watering irrigation schedule comparison	During summer, the irrigation schedule needs to be modified based on heat conditions. While the standard requirement is one inch of water per week applied all at once, during hot periods divide the weekly schedule in half for twice-weekly watering. During heat waves, split into thirds for three weekly sessions. For example, if it normally takes one hour for an inch of water, water for 20 minutes three times per week during extreme heat. Additionally, watering in the cool morning hours is crucial as it allows the lawn to retain maximum moisture rather than losing it to midday evaporation.	"['Regardless of the type of lawn you have, they all take a beating under the blistering Australian sun. But just because it’s blazing outside, doesn’t mean your lawn has to dry up into a dusty, brown mess. In fact, while you might not be able to water it as regularly as you like due to council water restrictions, there are several water-saving precautions you can take to help give your lawn the best chance.\nThatch is fallen, decaying plant matter, such as leaves from any deciduous trees or flower petals blown off in the wind, and it can build up pretty quickly if you don’t monitor it. It can block the sun from reaching the grass and soak up water, taking it away from your lawn’s roots where it’s most needed. Raking up any thatch lying about your grass can also help your plant thrive during Winter and give it the best chances of recovering come the following Summer. Removing thatch should be done in conjunction with aeration.\nAeration is the process of piercing the dirt beneath your lawn with small holes to allow the roots to breathe and allow air, water and nutrients to penetrate those roots directly. This can help your lawn roots grow deeper, which promotes stronger, healthier lawn growth. The main reason that many gardeners aerate their lawn is to counteract any compacted soils that may have developed due to heavy use. We suggest renting an aeration tool, as purchasing one can be quite costly and isn’t necessary for the standard suburban backyard.\nNot all fertilisers are created equally and not every fertiliser will be right for your type of lawn. If you’re not sure what kind of lawn you have, ask a gardener or take a sample to your closest lawn supplier to help identify it and seek advice on which fertiliser will help it thrive. It’s best to only fertilise twice a year, during peak growth season, and not in the height of the summer heat, as this will create any benefits to your lawn.\nThe longer the shoot, the more drought-tolerant the grass, so raise your lawn-mower blade in Summer to give it a chance to grow deeper roots and shade the earth beneath, which in turn will discourage weed-growth. Sharpening your mower blades can also help, as a sharper cut will reduce the stress on your grass on hot days. You might even consider mulching your grass clippings during this time to encourage steady moisture levels.\nSummer is prime time for pests to be on the prowl across your lawn. Insects such as chinch bugs, army worms, fire ants and mosquitoes are all on the prowl for new homes and if your lawn is dormant or drought-stressed, you put it at risk of becoming the bugs’ ultimate Summer vacation pad. Hit them where it hurts with lawn-friendly insect killer and avoid lawn damage before it happens.\nThis one has probably already been drilled into you by your local council, but it’s worth repeating, because lawn that is watered in the cool morning air retains the vast amount of moisture, which you would otherwise lose to evaporation during the middle of the day. So set your sprinklers to wake up before you do and get the most out of your lawn this Summer.', 'Maintenance Schedule For Cool Season Lawns\nProducts that are mentioned and pictured below are only examples, not recommendations. There are similar products available by other brand names that you might prefer.\n1. March is a great time to clean up the yard and begin the year’s maintenance regimen. The first thing to do is rake. Using a leaf rake might be best so as not to damage the soil\'s fragile condition at this time. The soil is likely still very moist, so rake lightly to stand the grass blades, unbind matting, and also clear away debris.\n2. Apply pre-emergence herbicide to prevent weeds. Grassy annuals begin to germinate when temps are consistently above 50°. Scotts Halts or Green Light Crabgrass Preventer are for grassy weeds, like crabgrass, foxtails, goosegrass, etc. Green Light Portrait prevents seed germination of broadleaf weeds like dandelion, clover, chickweed, henbit, etc.\nclick individual pics for info\nIf soil compaction is a problem and you plan to core aerate, April is a good time to do it. Since the ground is still a little moist, coring will be easier. If you applied pre-m herbicide in March, however, you will not be able to perform any mechanical functions, such as raking, dethatching, or core aerating or you will disturb the herbicide barrier. To still be able to address a soil compaction problem, there are liquid alternatives such as Nitron A-35 and AerifyPlus to condition soil and relieve compaction. Nitron A-35 and Aerify are excellent, natural soil amendments that make aerating unnecessary.\nclick individual pics for info\nThe best soil amendment is compost or compost tea. David gave us a really good method for brewing compost tea here Deuley’s Little Texas Tea Brewer. If not the tea, spread 1/4 inch layer of compost all over the lawn. That translates to 1 cubic yard/1000sf so you know how much to purchase.\nFertilize using any brand you wish, Scotts, Lesco, Greenview, Vigoro, etc. Apply 1 pound nitrogen evenly all over. Fertilizer that also contains weed control, i.e. weed-n-feed, is not recommended.\nMay or June\nApply post-emergence herbicide for new crop of dandelions and other weeds. Bayer Advanced has an ""All-in-One"" product that takes care of practically everything - broadleaf weeds like dandelion, plantain, chickweed, clover, etc. as well as grassy weeds like crabgrass and even nutsedge.\nclick pic for info\nBetween Mid-August and Mid-September\n1. Get a soil test to determine nutrient requirements and pH balance. Contact your state university’s Cooperative Extension Service. Results may take up to two weeks. You may apply lime or sulfur after that time per test recommendations.\n2. Fertilize using brand of your choice. Apply 1 pound nitrogen evenly all over. You can fertilize 2 times or 3 times in the fall. Fertilizing should be spaced 4-6 weeks apart.\n3. (Optional and strongly advised) Topdress with 1/4 inch layer of compost to incorporate organic matter or spray compost tea.\n4. Core aerate if soil compaction is an issue.\n5. (Optional) Overseed with premium cultivars. Spare no expense on grass seed. (Do not overseed if you plan on weed control in any of the steps that follow.)\nFollowing hot and brutal summer, Fescue and Rye grasses usually need to be overseeded. Normally, Kentucky Bluegrass is not in need, but it is good practice to overseed Bluegrass every two or three years to maintain diversity.\nCore aerating, Fertilizing, topdressing, and overseeding can all be done at the same time.\n4. Apply Portrait to prevent winter annual broadleaf weeds (if you don\'t plant grass seeds), or refer to item 2 under November.\n1. A good time to apply Nitron again if soil compaction is an issue and you did not core aerate.\n2. Fertilize again if you wish.\n1. The last fertilizing of the year is late October to mid-November. Application should be when top growth ceases and you no longer have to mow the lawn but the grass is still green. For some northern areas, this may well fall in October. This final application will help maintain good winter color but most importantly, it provides for carbohydrate accumulation. The food the grass stores is utilized in spring to promote greenup and carry the grass through the growing season of spring until time to fertilize again in May.\n2. This is the best time of year to control broadleaf weeds. Apply post-emergent herbicide for winter\'s crop and to help prevent them the following spring and summer.\nFrom Now Until The End of Forever\nA. Your lawn needs one inch of water per week (including rainfall) and should be applied all at one time. This is what is referred to as deep but infrequent irrigation. Using tuna cans placed in various places, run the sprinkler to obtain one inch of water in the tuna cans and time it. Run the sprinklers each week for that amount of time in every section to achieve one inch of irrigation all over. One inch will moisten the soil to a depth of 6-8 inches. This encourages the roots to grow deeply. The soil will pull the water downward. The roots will grow down looking for water and nutrients. Each week, be sure to take rainfall into consideration.\nB. Irrigation should be modified during the hot periods of summer, particularly during heat waves. To relieve heat stress and prevent drought stress, water the lawn more frequently. Divide the weekly irrigation schedule in half to provide water twice a week for half the amount of time. If it normally takes one hour to provide an inch of water, then irrigate twice a week for half an hour during the hot weeks of summer. Divide irrigation schedule into thirds during heat waves. Again, if it normally takes one hour to provide one inch of moisture, irrigate three times a week for 20 minutes.\nC. Should you decide to allow the grass to go dormant in summer, water requirements are greatly reduced, but keep in mind the lawn still needs moisture from rainfall or irrigation. About half an inch per week (or somewhat less) should suffice.\nD. Always mow high at 3 inches or higher on a regular basis. Mowing high permits proper photosynthesis. That means the grass needs sunlight to grow. The shorter the blades of grass, the more you impede the photosynthesis process. Photosynthesis takes place at just about the middle of the grass blade. Ever notice the grass is yellowish closest to the soil? The reason is that as the grass utilizes sunlight to manufacture its own food, chlorophyll is produced to give the grass its green color. It is very important to remember you should never remove more than one third of the grass blade at a time or you shut down the food factory, which causes stress. You may wish to mow once a week or 2-3 times a week, depending on how fast the grass grows. Either is fine just so long as no more than one third is cut off at once.\nInfrequent deep watering and frequently mowing high are the two crucial points of lawn care. Anything else you do is secondary to these cultural practices. Proper watering and proper mowing are what encourage a healthy growing environment for a lush green lawn that is able to crowd out weeds.\nOther helpful information\nOrganic Lawn Care if you prefer not to use chemicals or synthetic fertilizer\nIf you have any questions or comments, please feel free to post them on the forum.']"	['<urn:uuid:e5d3f394-1369-498f-9b47-c679fcea894b>', '<urn:uuid:fa885cba-2e09-40d5-a500-10a441bf4e63>']	factoid	with-premise	short-search-query	similar-to-document	three-doc	expert	2025-05-12T20:46:45.703947	7	99	1793
9	What are the benefits of HF radar systems for marine monitoring, and how has the PowerBuoy technology enhanced their offshore surveillance capabilities?	HF radar systems provide detailed surface current measurements along the coast, with overlapping coverage enabling redundancy during outages. When combined with PowerBuoy technology in the US Navy's LEAP project, the HF radar surveillance capabilities were significantly enhanced - detection ranges were doubled, spurious alarms were reduced, and offshore monitoring improved. The system remained operational even through extreme weather conditions like Hurricane Irene with 29.5-ft waves.	"['*Photo credit: Captain Albert E. Theberge, NOAA Corps (ret.), NOAA Photo Library\nImproving Marine Forecasts\nOver a recent five-year period the U.S. Coast Guard investigated well over 100 vessel-related incidents per year in the Southeast marine environment, which speaks to the importance of marine weather information. SECOORA members and partners have been maintaining critically important weather and wave stations for over a decade. The observations from these stations have been demonstrated to measurably improve NWS marine weather forecasts.\nSECOORA is working to provide:\n- A more geographically rich set of integrated and real-time web-available wind, wave, circulation and current data to promote safe and efficient maritime transportation and recreational boating. This information will measurably improve NWS marine weather forecasts, increase efficiency of maritime shipping, and improve boater safety.\n- Enhanced access to maritime weather information through improved web-based applications, interfaces, and user-selected options for information delivery.\nProject Spotlight: HF Radar in the Southeast\nHigh frequency radar (HF radar) has been nationally recognized as a solution to provide the level of detail required by scientists and forecasters to measure surface current speed and direction. A third WERA installation along the GA/SC coast was installed on Jekyll Island, GA with primary funding from the Georgia Research Alliance, the original equipment money allocated by SEACOOS, plus a significant contribution from SkIO. This third installation will increase overlapping shelf coverage for surface current measurements to include the entire Georgia shelf and shelf edge, to the Florida border, and will facilitate collaborative work across our southern border into Florida. It will add critical overlapping redundancy with existing installations, providing essential two-installation operation during periods when one installation of the three may go down (lightening strikes, power outages, etc). For more information contact Trent Moore or Dana Savidge at the Skidaway Institute of Oceanography.\nIn addition to the traditional offshore energy activities in the Gulf of Mexico, the Southeastern U.S. states currently have other active offshore initiatives. These include natural gas pipelines, potentially will include new drilling for hydrocarbons, and also involve “alternative” efforts to harness wind and currents in offshore areas with the potential to make contributions to our nation’s energy options. SECOORA has the expertise to effectively investigate and provide monitoring data to support these initiatives.\nThrough active partnerships with state Departments of Energy, state and local permitting authorities, industry, and other stakeholders, SECOORA can provide a network of expertise to support investigation, installation, and monitoring of offshore energy projects.\nProject Spotlight: SECOORA Co-Sponsors Renewable Ocean Energy and the Marine Environment Conference\nAt the recent Renewable Ocean Energy & the Marine Environment conference SECOORA coordinated the ""Coastal and Marine Spatial Planning"" plenary session and also a ""Managing Information to Support Decision Making"" track. SECOORA had a large presence at the meeting. Members attending included Mitch Roffer, Cliff Merz, Dwayne Porter, Manhar Dhanak, and Billy Edge. Debra Hernandez, SECOORA Executive Director said, ""It was a great opportunity to network with leaders working in the alternative energy sector and to learn about cutting edge science and data management issues for this growing industry."" View presentations from the conference. Over 90% of the participants who returned their evaluation form agreed that the conference increased their knowledge of renewable ocean energy and the marine environment and 100% of these participants agreed that the information gained at the conference will be useful in their work.', '‘Smart’ Buoy Could Turn Waves into Platform and Subsea Power\nPaul Watson, SPE, Ocean Power Technologies\nThe use of moored, oceangoing “smart” buoys that can harvest energy from waves could be an efficient and economic means of supplying electric power for various offshore oil and gas operations, including well trees, monitoring systems, and autonomous underwater vehicles (AUVs). Potentially, such technology could reduce, or in some cases eliminate, the use of diesel-powered generators on offshore facilities.\nThe PowerBuoy (Fig. 1), developed by Ocean Power Technologies (OPT), is new to the oil industry but has been used for a number of years in the defense and utility sectors. The autonomous buoy system consists of a surface float, a spar containing a power takeoff (PTO), a battery system, and a heave plate that constrains the spar’s motions. The system is capable of delivering energy from a few kilowatts to several hundred kilowatts, with future evolution planned to deliver even more power.\nThe process begins with the rising and falling of waves, and the resultant mechanical stroking is converted by a specially designed PTO to drive an electric generator. This power is transmitted to external equipment by means of an underwater power cable or directly to payloads integrated into the structure itself. Continuous power is then available with the option of larger, timed power bursts.\nAdvanced internal control systems continuously monitor the various subsystems and the surrounding environment and optimize performance of those systems with data transmitted to shore in real time, providing health and status updates on itself and its attached payloads. With this information, the operator gains a high level of control. Depending on the model of the buoy, the system is designed to need no maintenance for up to 3 years, providing potential savings in operating and life cycle expenses compared with existing power generation alternatives.\nAdvanced control algorithms have been developed that actively assess oncoming waves to tune the internal PTO dynamically to ensure that it extracts maximum power. In the event of especially large oncoming waves, the buoy will automatically protect itself by locking the float and PTO subsystems and continuing to supply electric power to its payloads by means of the embedded energy storage system (i.e., batteries). The power management system of the buoy manages the state of charge of the battery to ensure efficient overall system operation and to preserve battery life.\nUS Navy Project Case Study\nOPT established the Littoral Expeditionary Autonomous PowerBuoy (APB) (LEAP) project after being approached by the US Navy, which was trying to increase the coastal monitoring and security surveillance capability of its high-frequency (HF) radar network.\nTo extend the offshore range of the coastal radar system, an APB-350 unit designed by OPT was deployed approximately 22 miles of the US east coast. The HF antenna was mounted on the top of the buoy to transform a monostatic shore-based network by enabling it to operate in a bistatic mode for improved performance.\nAmong the Navy’s specifications for the LEAP project were:\n- Uninterrupted power supplied to the HF radar payload and to communications systems\n- Ability to remain on station in all ocean conditions\n- Mechanical, electrical, and mooring systems capable of surviving defined extreme ocean conditions\n- Dimensions suited to deployment by a US Coast Guard cutter vessel (transportable in a standard ISO 12m shipping container)\n- Deployment for a long period\n- Autonomous operation without a constant need for on-site monitoring and maintenance\nThese requirements guided the design and testing of the APB-350 buoy system, including its electrical, mechanical, and mooring subsystems.\nThe use of the HF radar on the buoy resulted in a major improvement in the resolution and tracking of vessels, with detection ranges doubled. This helped reduce spurious alarms and improve the management of the offshore monitoring and security surveillance.\nBeyond its communications ability, the robustness of the buoy was tested, including mechanical, electrical, and mooring components. The structure successfully withstood Hurricane Irene, which passed just west of the deployment site. The buoy remained on station despite 29.5-ft significant wave height generated by the storm over 48 hours.\nHowever, more notable was that the system remained fully operational throughout to provide power to the payload. A post-hurricane inspection of the structure and mooring revealed no damage. This highlights the technology’s capabilities that could be transferrable to the extreme conditions of the offshore energy sector, such as deepwater and ultradeepwater environments.\nIn the program’s next phase, starting in July 2013, a passive acoustic monitor for subsea vessel tracking was added to the system.\nOil and Gas Applications\nOPT has developed a range of potential applications for the oil and gas industry, including:\n- AUV garages that perform permanent infield monitoring or inspection of assets\n- The control of electric trees for field injection of CO2 and water\n- Environmental monitoring before and after the deployment of offshore drilling and production assets\n- Real-time, on-site field monitoring and sensing systems for 4D reservoir analysis and preseismic and post-seismic deployment\n- Security cordons for offshore developments, in which the buoy serves as a host for surveillance equipment that can track vessels and aircraft and potentially provide early threat warnings (Fig. 2)\n- Temporary navigational markers for surface and submerged structures\n- Diesel power replacement, using the larger PowerBuoy 40 (PB40) system\nThree of these applications—AUV garages, the control of electric trees, and diesel power replacement—align particularly with current needs in offshore operations.\nPrepositioned AUV Networks\nApplying the technology as a persistent power source for prepositioned AUV systems alongside AUV garages could enable fast and cost-effective power delivery. Used in this way, the buoy system can hold an advantage over ship-centric AUV systems, especially with the wide areas covered by typical oil and gas fields and the increasingly remote locations being developed.\nAUV performance is hampered by the vehicle’s need to resurface for recharging, for mission programming, and for uploading post-mission data. Doing so is particularly difficult in sea states higher than 2. Using an AUV garage removes these limitations by providing an in-situ charging point and enabling two-way communication between the device and its control point without the need to resurface.\nThus, the operator can address minor maintenance issues quickly from the desktop, requiring fewer staff members, and more time is available for addressing major problems that may arise, reducing the risk of production outages.\nThe technology provides ample power for AUV functions and could be scaled up to support larger work vehicles. The system is equipped with a number of on-board communication capabilities, including satellite (Iridium), HF, and Wi-Fi. The stable spar buoy can also accommodate the specialized antennas required to support high-bandwidth systems such as the very small aperture terminal systems used in the Gulf of Mexico.\nElectric Tree Power and Control\nElectric trees offer the potential for improved control system response and increased reliability compared with similar hydraulically controlled systems. Using the buoy as a power source and control hub potentially avoids a complex and costly subsea umbilical installation.\nCurrently, electric wellheads obtain power through connections with remote surface infrastructure. Use of the buoy system potentially can lower power delivery cost while maintaining equivalent reliability and safety levels.\nTo use the buoy system, it must have sufficient power and a margin of safety to provide safe and reliable operation of the electric tree. The tree is mainly a low-power system that operates on a fraction of the power used by a subsea pump. The subsea control module has no moving parts, and intermittent bursts of high power are needed to operate valves.\nChanging the power source to the tree does not increase safety concerns. Nonetheless, because the buoy is on the surface, there is some added risk compared with that of a submerged cable. However, up-to-date navigational aids mounted on the buoy and its marked location on marine charts mitigate that risk.\nThe buoy continuously transmits a “health check” status to operators to alert them of incidents or failures. The buoy is designed to include a high-integrity shutdown mechanism. To ensure that a communications failure does not lead to a buoy power failure, the buoy has a failsafe closure signal that will be transmitted to the tree if a reset signal is not received from the control center within every 4-hour period.\nDiesel Power Replacement\nDiesel generators are the most common source of electrical power in the offshore operations market, but their use comes at a price for the fuel and its delivery to the facility. To meet the needs of an unstaffed platform with PowerBuoy technology requires the PB40 system. The system, which delivers 40 kW of power and can be scaled up to deliver 80kW, is being prepared for a trial off the north coast of Spain as part of the Waveport project funded through European Commission’s FP-7 research and development program.\nThe buoy system can connect to a platform’s uninterruptable power supply by means of an underwater cable to supply power that will support diesel generators and thereby significantly reduce operational costs. The resulting dual power source\n- Greatly reduces diesel generator use and fuel consumption, extending generator life.\n- Reduces maintenance and the associated staff needs, transportation, and operational downtime.\n- Increases the redundancy and enhances the safety of the power system.\n- Reduces the environmental impact of platform operations.\nAs the technology evolves, larger systems being planned could further reduce or eliminate diesel generator use and reduce capital and operating expense, which could especially benefit marginal production facilities.']"	['<urn:uuid:ff941d36-53db-4e9f-9334-5e5886ba13fe>', '<urn:uuid:244af05f-135b-42aa-892d-008484b001fb>']	factoid	direct	verbose-and-natural	similar-to-document	multi-aspect	expert	2025-05-12T20:46:45.703947	22	65	2132
10	I am trying to create a logo for my new business - what does the rule say about how many different font types I can combine to make it look good?	For a user-friendly design, it's recommended not to use more than two different fonts at a time. When designing a logo for your business, you should choose a font style that tells your brand's story.	['Elements of design are the parts of your work of art that you arrange and craft to create visually pleasing designs. It’s important to be aware of how these basic principles work together to find your way towards stunning designs more easily.\nUse elements of design principles to guide you and to understand how design works but don’t see them as a strict set of rules. Good design still requires creativity, which means that rules can be bent or broken.\nEven if you’re not a graphic designer, it’s important to have a good grasp of these principles. Whether you’re creating display ads, a logo for your business, or a sales brochure, create engaging and more impactful graphics that draw customers in by using elements of design.\nThe line is the most basic element of design. Drawing a line with your pencil is how you get started with a painting or sketch and in graphic design any points that are connected form a line. This is also called an actual line while the edge between two shapes forms an implied line.\nLines can vary in thickness, be smooth, rough, continuous, or broken. They can have different directions such as vertical, horizontal, diagonal, curved, or move up and down across a page in a zigzag pattern.\nUse Elements of Design to Pick a Focus\nYou can use lines to offset parts of your design and draw the eye to a particular area or focal point. In advertising, guide the audience to the main object or product you want to sell. Emphasize design elements with bold lines and use thin lines for decoration. If you want to establish a more personal connection, appeal to your target market with lines that mimic handwriting.\nColor immediately stands out in any design. Depending on the type or shade, you can use colors to emphasize elements or evoke certain feelings. Choosing the right colors is crucial when you’re trying to tell a story with your design.\nMake sure you know the fundamentals of color theory to choose colors that complement each other. Look at different hues, saturation, and brightness before you make a selection. Consider which color space you need to work in and what the best practices are for print or screen use.\nColor as a Vehicle for Emotions\nThe emotions colors can evoke are an incredible tool for advertising. Make sure the colors you pick for your ad design conjure up the feelings in your audience that you want them to associate with the product. Find different appealing color combinations that help you to get your message across to your customers. Use a color scheme for your website that evokes emotions that fit with your overall brand message.\nA shape is any space that is enclosed by lines. There are geometric shapes like a square, an oval, and a triangle, or organic shapes like speech bubbles, a blob, or any irregular free-form shapes.\nJust like colors, different shapes conjure up different feelings. More rounded shapes create natural and calm emotions while sharp edges grab attention. Squares convey rigidness, triangles energy, and circles fluidity. Use shapes to create patterns, textures, or symbols depending on the relationship you construct between them.\nAttract Attention with Shapes\nFor logo designs, create dynamic visuals with layered patterns or use shapes as stand-alone visual elements. For call-to-action buttons on your website, experiment with various shapes combined with different colors that stand out. See which combination works best.\nWhen you look at space within your design, you’re viewing all the areas within, around, above, or below the objects you placed on your canvas. Space can both connect and separate elements. Establish narrow spaces between elements to create a connection and wider spaces to convey separation. Both ways generate a design flow.\nSpace can be positive or negative. Negative space is also called white space, which helps to group and organize elements. Use white space wisely to create a layout that doesn’t overwhelm viewers and gives their eyes places to rest.\nEstablish Structure with White Space\nIn web design, users expect a consistent use of white space to create layouts where the navigation, logo, and footer are in the same spot on every page. Use white space in advertising to avoid intricate and complex visuals for a more accessible design keeping the focus on the product.\nTexture in design is usually an implied feeling of a tangible surface taken from real life. Although we can’t experience these textures in a tactile way, using these kinds of effects recreates and brings life to your composition. The only way to make texture tangible again is to use textured paper like felt or velum for printed designs.\nUse texture to create a focal point, contrast, or help with balance. Replicate a natural environment through texture to craft a more three-dimensional appearance. This makes your design more immersive.\nExplore Contrasts with Different Textures\nBoth natural and artificial textures can draw people in, so decide what fits best with your brand message. Natural textures bring vivid beauty to your design while the mix of surrealistic patterns and 3D effects of artificial textures generates new and unique styles.\nFraming helps you to organize the information in your design. A frame can be a basic or decorative border around objects to guide the eyes towards a focal point. Use frames to zoom in and crop your composition or to create a visual balance.\nThere are several framing techniques you can try out. For example, insert a border within a frame on your canvas to create a multi-dimensional effect and make the elements within that inner border the focus of your design. You can also experiment with text, icons, or textures as framing devices around objects.\nSpruce up Your Visuals for Social Media\nThese techniques can guide you to create aesthetically pleasing visuals for social media that stop your audience from scrolling. More advanced designers go against the structure of a frame on purpose sometimes to make their design look “off” to grab attention. When creating graphics for social media, follow graphic design trends to keep visual content fresh for your feed.\nTypography includes the fonts used to arrange text in your design, the spacing of the letters, their size, and their weight. Type helps you to convey words and ideas in your design but it also goes beyond that when designers use fonts to create striking visual elements.\nUse typography to create structure. To avoid complexity, a user-friendly design doesn’t use more than two different fonts at a time. When you design a logo for your business, choose a font style that tells your brand’s story. To narrow down your selection process, have a look at the three major categories of typefaces that most fonts belong to. See whether serif, sans-serif, or a decorative style fits best with your brand and do your research within one category.\nGood Copy Needs Great Design\nFor your advertising campaigns, good copy is essential but typography is as important as color when it comes to generating certain emotions. Emphasize words by making them bold or adjusting the color to make them stand out. You can also try something new and show your skill and passion as a designer by using trending fonts.\nScale refers to the relationship between two or more objects and how they are seen in comparison to each other. The focus here is on the different sizes of various elements in your design.\nScale affects your entire composition and can have a big impact on meaning and tone. Use scale to tell a story and create a hierarchy to direct the eyes and take viewers on a visual journey.\nExplore Scale to Create Eye-Catching Visuals\nFor professional content, subtle differences in scale are often sufficient whereas creative projects give you more room to play around in. Depending on the type of project you’re working on, use scale to add interest to your design. For example, in advertising, experiment with both extremes of using an oversized or a miniature object without any other elements added to put the focus on a single product on your promo poster.\nHierarchy creates a structure within a design. If everything stands out, nothing stands out, so you can see hierarchy as a way of prioritizing objects in your design. It’s used to show the relationship between elements and guides the eye along a specific order.\nFor example, when you create an object with sharp corners and a distinct color in front of a pale rounded object, you make it clear that the sharp-edged object is more important than the rounded one. When using typography in your design, you can create a hierarchy by using a larger and bolder font style for your heading than for your subheading.\nPut Elements in Order for Ease\nUse visual hierarchy in your marketing materials to make it easier for your target audience to scan your content and determine what’s important to them easily. Keep this in mind for your website to draw the eyes of site visitors to the most important information and calls to action.\nWhen designers talk about balance, they’re working on getting the visual harmony and order right within their designs. If creating tension is your goal, creative freedom allows you to bend the rules and craft unbalanced layouts.\nBalance also includes the concepts of symmetry and asymmetry. Most designers prefer experimenting with asymmetry due to its eye-catching effect and the contrast it creates.\nExperiment with Attention-Grabbing Layouts\nDepending on the outcome you want to achieve with your marketing campaign, you can play around with balance and symmetry. For example, your layout may be symmetrical but can be unbalanced at the same time, or you could place elements randomly within your asymmetrical layout and still create a perfectly balanced overall design. Both combinations can be used to create an interesting design that catches attention.\nYou know your design is complete when all the pieces work together smoothly. The combination of elements with a visually satisfying effect is called harmony. A good harmonic design is clean and undisruptive.\nCombine and arrange adjacent colors, similar shapes, and related textures to achieve harmony. Be sure to select shades from your color palette that balance each other out. Consistency is also part of harmony. For example, if you’re using typography, be consistent by selecting not more than two fonts to use across one design.\nCraft Relationships between Elements\nWhen you set out to create a harmonic design for your marketing campaigns, use elements that are not completely different but stand in some kind of relationship with each other. This can be the distance between objects or proximity, how they seem repeatable with other elements or their similarity, or creating a sense that there’s a pattern or continuation.']	['<urn:uuid:1fa835d6-4eb9-4543-a5b4-0af375d804f1>']	factoid	with-premise	verbose-and-natural	similar-to-document	single-doc	novice	2025-05-12T20:46:45.703947	31	35	1796
11	Which Mississippi Mass Choir album topped charts for 12 months?	The album 'It Remains to Be Seen' topped the charts for 12 months and received a Soul Train music award as best gospel album of 1993.	"['CHANGE - There seems to always be a guarantee in life that change will happen. Change is something we tend to fear and become anxious about because we do not feel...\nSunday, February 15, 2015\nMississippi Mass Choir Praising God\nMississippi Mass Choir Praising God Playlist\nFrank Williams, founder of the group, began singing gospel music in churches as a child, spending his evenings immersed in the sounds and ways of the gospel under the guiding hand of his father, Leon. Frank performed with his brothers in an act know as The Williams Brothers, later joining the Jackson Southernaires.\nAfter many years of touring as an artist, Frank had a vision that the great and soulful voices of Mississippi needed to be captured in choir. For nine years this idea grew in his mind until he could contain it no longer. He approached his gospel-writing friend, David Curry, with his ambitions and the rest is history! The two sent audition announcements via radio across the state of Mississippi and tapes began pouring in. With great enthusiasm, the people of Mississippi thrust themselves into the mission of this choir, to serve God through song. Redefining the choir experience, each concert is like a rousing church service, with a Reverend at the helm. Skillfully weaving a scriptural message into the soul-stirring, roof-raising gospel sounds of the singers, Reverend Benjamin Cone, the choir\'s spiritual advisor, delivers with the authority of one who knows God personally. His presence on stage commands attention, his deep articulate voice bears listening. By the end of the concert, there is no one left who has remained in their seat throughout, no voice unsung, no hands left hanging limply at anyone\'s side, and no one who hasn\'t felt a mountain move within them by what they have just experienced. The Mississippi Mass Choir moves the masses.\nThe Mississippi Mass Choir was one of the most influential gospel groups of the late \'80s and \'90s. Under the musical direction of David R. Curry, the 100-voice choir served their Lord with a sound that made them a constant presence on Billboard\'s gospel charts. Each of their recordings have reached the top position on the charts. Their debut album, Live, recorded during a 1988 performance at the Jackson, MS, Municipal Auditorium, remained on the charts for 45 weeks and earned the group James Cleveland GMWA awards as contemporary choir of the year and best new traditional artist of the year. Their second album, God Gets the Glory, reached number one two weeks after it was released in 1990. The Mississippi Mass Choir\'s most successful album, It Remains to Be Seen topped the charts for 12 months and received a Soul Train music award as best gospel album of 1993. The album was the choir\'s last with founder Frank Williams (June 25, 1947, to March 22, 1993), a member of the Jackson Southernaires and an executive in the gospel music division of the Maleco record label. Determined to bring together the best gospel voices in Mississippi, Williams had convinced Jerry Mannery, the head of Maleco\'s gospel division, to sign the band to a record deal and serve as executive director. With their albums released since Williams\' passing, I\'ll See You in Rapture, Praise the Lord, and Emmanuel (God With Us), the Mississippi Mass Choir continues to dedicate itself to its self-described mission of ""serving God through song."" ~ Craig Harris, All Music Guide\nServing God Through Song"" is the motto and the mission of The Mississippi Mass Choir. Although striving to succeed in the gospel music industry, the choir\'s purpose is to help establish the gospel of Jesus Christ throughout the world.']"	['<urn:uuid:359c38ac-3239-48b8-85b8-0ae96feb8f84>']	factoid	with-premise	concise-and-natural	similar-to-document	single-doc	novice	2025-05-12T20:46:45.703947	10	26	609
12	as amputee physiotherapist what are initial care team members and what phantom pain symptoms should monitor	The initial care team includes a prosthetist (who fits and builds devices), a physiotherapist (who helps with rehab and device usage), and a general practitioner (for infections and wounds). As for phantom pain symptoms, monitor for sensations like tingling, burning, aching, and shooting pains, which typically begin within days after amputation. The pain can be intermittent or continuous, and often affects the part farthest from the body. About 80% of amputees experience phantom sensations in weeks following amputation, with 60% reporting pain a year after surgery.	['Your Care Team\nSo first off, you need to know who you’re dealing with here. Who do you see for what? Who is most readily available to you? What exactly is a prosthetist? Here we go.\nYour major base is going to be your prosthetist. This person is going to get funding for, fit, build, and adjust your device for you. This is the person that can help you with your fit, your gait, and give tips on how to use your device best, and get the fullest coverage for your necessary device(s).\nAnother major person is your physiotherapist. They are going to help you through rehab and beyond as you get the hang of using your device and dealing with the other implications of being an amputee. Don’t underestimate their knowledge and lasting importance. If you’re feeling more tired or sore, it might be a good idea to make another appointment. If you find a physio with working knowledge with amputees, keep in contact.\nAs always, you should establish a good rapport with your general practitioner (GP). This is your family doctor, but often infections, wounds, and other concerns will bridge into their territory. When your prosthetist isn’t available, and a doctor may help, make an appointment.\nOther specialists like chiropractors, acupuncturists, and massage therapists can help you stay on track and are an important preventative health measure. Get in the habit of prioritizing maintaining health rather than waiting for an issue to come up before seeking help.\nThere are specific terms that can help you communicate best with your care team, making it easier to solve problems and find the best care. We’ve also included some terms that are common in disability and amputee groups (especially online) so that you can best navigate those communities as well.\nWearing away of the skin through rubbing or friction. If your socket is chafing or rubbing and your skin is irritated, it can cause abrasion.\nAn above elbow amputation\nAn above knee amputation\nHow the components in an artificial leg line up in relation to each other. If your gait is “off” it might be because a component (foot, ankle, wrist, hand or knee) is tilted or not lined up with the rest of your prosthesis. This can also happen for lower limb amputees wearing new shoes.\nRemoval of all or part of a limb due to infection, injury, tumor, disease or trauma\nA person who has had all or part of a limb removed or is born without part of a limb\nThe front side. On your leg this is the shin, arm it’s the top.\nThe shrinkage of muscle tissue due to change in function. Your residual limb will begin to atrophy, and may be “softer” and smaller. This can become an issue because you need some muscle and flesh to cushion you in a socket.\nA below elbow amputation\nAn amputee without both arms or both legs\nA below knee amputation\nRhythm of walking. This relates to how smooth your walk is, so if you feel you’re limping or have uneven steps you’ll need to work on your cadence.\nCheck socket (AKA test socket)\nA temporary plastic socket formed over the plaster model to assist in the fitting process\nRefers to an amputee who was born without limb(s)\nTightening of muscles around a joint which causes restriction in the range of motion. Above knee amputees can develop this in their hip flexors, while upper limb amputees can often develop “tennis elbow” and issues in their shoulders. Your physio and the rest of your care team will work with you to prevent this.\nCertified Prosthetist and Orthotist\nThe process of making the residual limb less sensitive to touch by massaging, tapping, using a vibrator or rubbing with a terry cloth. This can help prevent phantom pain, but also can trigger it.\nAn artificial limb designed for long term comfort, fit, alignment, function, cosmetic appeal and durability (to replace temporary/preparatory first fitting).\nAn amputation through a joint – the hip, knee, ankle, elbow, wrist or shoulder\nFarthest from the center (for example, the foot is the distal part of the leg)\nTaking off a prosthesis\nPutting on a prosthesis\nSwelling of tissue. Most amputees experience some swelling and redness on their residual limb as it gets used to being in a socket for long periods of time. This especially affects those using total contact and suspension sockets.\nEnergy output, achieved by a spring-like design in prosthetic feet. Energy return is a great thing to look for in feet and ankle systems, but trialing is always a good idea. You don’t want to feel like you’re “hopping” in your steps or are worried about control.\nHas a soft outer shell (supported by an internal skeleton of a pylon, etc.) – components are lightweight and require fewer adjustments\nHard outer shell – generally heavier but more durable than endoskeletal. The components last longer and are more suitable for recreational activities\nJoint motion, flexing or bending a joint\nManner of walking – everyone has an individualized one. Your gait needs to be predictable and helped, not hindered, by how your tech is working. New microprocessor devices can predict your gait- one of their biggest strengths.\nProfessional training to achieve a natural gait (manner of walking)\nAm upper limb device operated by body movement which provides a good grasping function.\nInner surface of the socket, or portion of the prosthesis closest to the skin. Think about it being the area of “interaction” between your body and your device.\nTowards the side or outside (for example, the outside of the thigh is a lateral part of the leg)\nTowards the mid-line of the body (for example, the inside of the thigh is a medial part of the leg)\nElectronic sensors are used to pick up the feedback from muscle contractions to move artificial components such as a hand or elbow\nA ball of nerve fibres that forms on the end of a severed nerve which continues to grow and can sometimes be troublesome and cause pain\nOccupational therapist (OT)\nA person who works with an amputee to teach the amputee how to use a prosthesis (such as a myoelectric arm) and adaptive skills\nA device that assists partial hand amputees to grasp while retaining sensation (aka spatula device)\nA person who performs surgery on bones or muscles - for example, amputation, hip replacement, stump revision\nDevice used to support weakened joints or limbs\nA person who builds and maintains devices to support weakened joints or limbs, such as a brace\nA newer procedure for amputees where a device is directly implanted into the bone of the residual limb, making a socket unnecessary. This is still under some development but successful cases are increasing. Recently, Ossur and Ottobock (the two largest prosthetic manufacturers) have been working together to improve this technique.\nPhantom limb pain\nPain experienced by an amputee in a limb no longer present. Some estimates say that 80% of amputees experience this at some time. This has an enormous range, from sensations of buzzing to shooting pains or deep aching.\nPhantom limb sensation\nSensation that a limb no longer there is still present\nA doctor of rehabilitation medicine specializing in the comprehensive management of clients with conditions arising from neuromuscular, muskuloskeletal and vascular disorders\nA person who works with an amputee in the rehabilitation phase to improve muscle function through exercise and can assist with gait training\nThe residual limb slips up and down within the socket.\nThe thickness of stump socks – the higher the number the thicker the sock\nRear, back. This is behind the knee or the underside of your arm.\nMore than one prosthesis (artifical limb)\nA sock generally worn with an artificial limb that provides additional padding for bony surfaces\nAn artificial limb\nA person who builds and maintains artificial limbs\nA rigid central shaft, usually tubular, that is attached to the socket or knee unit of an endoskeletal prosthesis. The lower end of the pylon is connected to an artificial foot or ankle\nRange of motion\nThe amount of movement a limb has in a specific direction at a specific joint (hip, knee, ankle, shoulder, elbow or wrist)\nThe process of readapting after an injury or surgery\nThe part of the limb remaining after the amputation (aka stump)\nSurgical alteration of the residual limb to improve function or appearance\n(E.g. Van Nes Rotation) a procedure where the lower portion of the leg is rotated 180° and reattached – the ankle acts like a knee joint, providing extra function\nLong-term effects of a disease, injury, procedure or treatment. Think “sequel.”\nSoft insert liner\nCup-shaped form which fits inside the socket of a below-knee artificial limb\nThe part of the prosthesis (artificial limb) that fits around the residual limb, and fits around the liner or socket insert if one is used\nSoft liner within a socket to provide cushioning\nA device that assists partial hand amputees to grasp while retaining sensation (aka Opposition Post)\nThe part of the limb remaining after amputation (aka residual limb)\nMethod by which the artificial limb is held in place\n(Or Boyd amputation which is similar) named for the surgeon who introduced the procedure – foot is removed, the shin bones are flattened and the heel pad is sewn back in place\nThe part of the artificial arm which acts like a hand – can be a passive hand, mechanical hand, hook-type or myoelectric device\nTotal suction socket\nAllows the artificial limb to be held in place by air pressure only, eliminates the use of belts and straps\nAn amputation which is the result of an injury, or a surgical amputation due to a medical condition (for example: vascular disease, cancer)\nVan Nes Rotation\nA procedure where the lower portion of the leg is rotated 180° and reattached – the ankle acts as a knee joint, providing extra function\nAmputation performed as a result of impaired circulation of blood through the blood vessels', 'Phantom limb pain is a sensation that feels like it originates from an amputated body part. Doctors once believed this post-amputation phenomenon was psychological; however, experts now know these are very real sensations. Although the limb or body part is gone, the nerve endings at the site of the amputation continue to send pain signals.\nWhile some patients experience severe, debilitating phantom limb pain sensations, some amputees experience non-painful phantom sensations. Amputees may feel as if they are gesturing or trying to pick something up, in addition to feeling tingling, numbness, hot, or cold sensations.\nUsually, phantom limb pain decreases and disappears over time for most people. For others, pain management of phantom limb pain can be challenging, requiring help from a pain specialist.\nWhat Causes Phantom Limb Pain?\nPain is a normal part of the healing process that generally subsides as the body repairs itself. However, it’s not that simple if you are recovering from an amputation. After an amputation, the nerves ending at the site of the amputation continue to function. These remaining nerves make the brain think the entire limb remains attached to the body. Approximately 80% of amputees experience phantom limb sensations in the weeks following an amputation, and 60% of amputees report phantom pain sensations a year after surgery.\nThe exact cause of phantom pain is unclear. However, the pain appears to originate in the spinal cord and brain. During episodes of phantom limb pain, imaging scans of the brain show activity in the portions of the brain that were neurologically connected to the nerves of the amputated limb. Researchers believe areas of the spinal cord and brain that lose input from the missing limb after an amputation adjust to the detachment in unpredictable ways. These mixed signals result in the brain interpreting the signals as pain, which is the body’s most basic message that something is not right. Sometimes, the brain’s memory of pain is retained and interprets any signal it receives as pain, regardless of signals from injured nerves.\nOther studies found that after an amputation the brain seems to remap the missing part of the body’s neural circuitry to another part of the body. Since the amputated area is no longer able to receive sensory information, that information is sent elsewhere in the body. Sensations from a missing hand may instead go to a still-present cheek, for example. When the cheek is touched, the person may sense that an amputated body member, such as a hand, is also being touched. The resulting confusion of senses can result in pain.\nSome other factors may also contribute to phantom limb pain, including damaged nerve endings, and scar tissue at the site of an amputation. It is also possible the phantom pain may mimic pain present in the limb before the amputation.\nWhat are the Symptoms of Phantom Limb Pain?\nPhantom limb pain has a wide variety of symptoms ranging from tingling and itching to burning and aching. Any sensation a limb might experience before amputation may become a phantom pain. Some patients report feeling as if their missing limb is in a distorted and painful position.\nCharacteristics of phantom limb pain include:\nAn onset of pain within a few days of amputation.\nThe pain may be intermittent or continuous.\nThe pain most often affects the part of the limb farthest from the body, such as the foot or toes of an amputated leg.\nThe sensation can be shooting, stabbing, boring, squeezing, throbbing or burning.\nSometimes the phantom pain may feel as if the missing limb is being forced into an uncomfortable position.\nThe pain may occur with pressure on a remaining portion of the limb.\nThe pain can become worse by stress, anxiety, and even weather changes.\nThe patient may experience “telescoping,” the feeling of the missing limb still being part of the body, but it has shrunk to a tiny size, similar to a collapsed telescope.\nIt’s difficult to identify the frequency of phantom limb pain because patients are usually reluctant to report it. The troubling part of phantom limb pain is that the pain is very real, but patients can clearly see the limb is gone. Therefore, some phantom pain patients may worry their physician might doubt their sanity.\nHow is Phantom Limb Pain Diagnosed?\nThere is no medical test to diagnose phantom limb pain. However, doctors identify the condition from the patient’s symptoms and circumstances, such as trauma or surgery, prior to the onset of the pain.\nHow is Phantom Limb Pain Treated?\nSuccessful treatment of phantom limb pain is challenging. Up until the early 1990s, treatment for phantom limb pain was an additional amputation. By shortening the stump and nerves, doctors thought they could reduce or eliminate phantom limb pain following the initial surgery.\nToday, pain doctors base treatment for phantom limb pain on the patient’s level of pain. A combination of multiple treatments may be helpful for some patients. Treatment usually begins with medications, in conjunction with noninvasive therapies. Minimally invasive treatments are also available to help patients. In some cases, surgery may be the last resort to relieve pain.\nNext, we discuss specific treatments for phantom limb pain.\nAlthough no medications exist specifically for phantom pain, some drugs designed to treat other conditions have proven helpful in relieving phantom limb pain. Unfortunately, no single drug works for everyone, and not everyone benefits from prescription drugs. Some of the medications used in the treatment of phantom limb pain include:\nAntidepressants, drugs that work by modifying the chemical messengers that relay pain signals. Antidepressants may also help with sleep, which can help patients feel better.\nAnticonvulsants, drugs that work by quieting damaged nerves to slow or prevent uncontrolled pain signals.\nNarcotics may be an option for some people. Taken in appropriate doses under a doctor’s direction, they may help control phantom pain.\nN-methyl-d-aspartate (NMDA) receptor antagonists, this class of anesthetics works by binding to the NMDA receptors on the brain’s nerve cells and blocking the activity of glutamate, a protein that plays a significant role in relaying nerve signals.\nAs with medications, the noninvasive treatment of phantom pain is a matter of trial and observation. The following techniques may relieve phantom pain:\nHeat application. Sometimes the application of heat to the area of amputation can ease the pain.\nBiofeedback. The process of biofeedback helps a patient become aware of normally involuntary processes inside the body (such as muscle tension, temperature, and heart rate control). Biofeedback helps the patient gain some conscious control of these processes. This awareness of the body helps the patient relax, which can help relieve the pain.\nRelaxation techniques. Various methods to help the mind alter its focus to something other than the source of pain.\nMassage. Focused rubbing of the amputated area may contribute to relieving the discomfort associated with phantom limb pain. Massage can also help the patient relax, decreasing stress and tension.\nTranscutaneous Electrical Nerve Stimulation (TENS) is a device that sends a weak electrical current via adhesive patches on the skin near the area of pain. The stimulation may interrupt or mask pain signals, preventing them from reaching the brain.\nMirror therapy. The patient watches a reflection of the whole limb in a mirror while receiving physical therapy. Seeing the reflection helps the brain to remap neural pathways to register that the ‘virtual’ limb is intact and moves without pain.\nAcupuncture.Studies by the National Institutes of Health show that acupuncture can be an effective treatment for some types of chronic pain. In acupuncture, the practitioner inserts extremely fine, sterilized stainless steel needles into the skin at specific points on the body.\nMinimally Invasive Therapies\nInjection. Sometimes pain-killing medication, such as local anesthetics and steroids, can provide phantom limb pain relief.\nSpinal Cord Stimulation (SCS). A neurostimulator delivers mild electrical impulses to the epidural space near the source of chronic pain impulses. These impulses interfere with the pain signals to the brain. A trial stimulator is typically worn for five to seven days externally. If the trial successfully relieves the pain, the patient may decide to have a permanent SCS placed under the skin.\nNerve Cuff Stimulation. A small electrode is wrapped around nerves traveling to the amputated limb. Instead of blocking nerve signals, the cuff delivers an imperceptible electrical stimulation to the nerve that replaces the sensation of pain with a pleasant signal to the brain. The user manually activates the cuff with a small wireless remote control when pain is felt.\nNerve block. Medications that help interrupt pain messages between the brain and the site of the phantom pain.\nSurgery may be an option if other treatments do not help. Surgical options include:\nBrain stimulation. Similar to spinal cord stimulation, electrodes deliver a small electrical current within the brain\nStump revision or neurectomy. If the pain is from nerve irritation in the stump, surgery can sometimes be helpful. But, cutting the nerves includes a risk of making the pain worse.\nOn the Horizon\nResearchers now believe there is a way to rewire the brain to help reduce pain from a phantom limb, according to a University of Cambridge study. The technique involves distracting the mind from mixed signals it may receive as a result of losing the limb.\nAnother new approach to relieving phantom pain includes virtual reality. A computer program, combined with special goggles, mirrors the patient’s intact limb, so it appears there has not been an amputation. The patient can move the virtual limb around to accomplish various tasks, such as batting a ball hanging in midair. Although tested on only a few people, this technique appears to help relieve phantom pain.\nWhat Are the Risk Factors for Phantom Limb Pain?\nIt’s still unknown why some people develop phantom pain after an amputation while others do not. However, there are several risk factors for the development of phantom limb pain, including pain in the limb before amputation, pain in the stump, the use of a prosthetic limb, and the number of years since the original amputation surgery. The most significant risk factors include amputation of bilateral limbs and lower extremity amputation.\nSome factors that may increase the risk of phantom limb pain include:\nPain before amputation. Researchers have found that if pain was present in a limb before amputation, patients are more likely to have pain afterward, especially immediately after amputation. Researchers believe the brain retains a memory of the pain and may continue to send pain signals, even after removal of the limb.\nStump pain. Amputees with persistent stump pain usually have phantom pain, as well. Stump pain can be the result of an abnormal growth on damaged nerve endings (neuroma). These growths on the nerves often cause painful nerve activity.\nIll-fitting artificial limb (prosthesis). A prosthetic device that does not fit properly can be the source of phantom limb pain.\nIs it Possible to Prevent Phantom Limb Pain?\nIf a person experiences pain in a limb scheduled for amputation, there is a greater risk of developing phantom pain after the amputation. Some doctors recommend a regional anesthetic (spinal or epidural) beginning a few days before the scheduled amputation. The anesthetic may help reduce pain immediately following surgery, and reduce the risk of chronic phantom limb pain.\nNovus Spine & Pain Center\nIt is important to immediately discuss any phantom pain with your doctor, because early treatment by an experienced pain management expert can help reduce the chances of the pain developing into a chronic condition.\nNovus Spine & Pain Center is in Lakeland, Florida, and specializes in treating phantom limb pain. By using a comprehensive approach and cutting edge therapies, we work together with patients to restore function and regain an active lifestyle, while minimizing the need for opiates.']	['<urn:uuid:2054e46f-1bc4-4892-ac90-9d952a949e74>', '<urn:uuid:9e2b063b-591f-4997-b13c-68bdfddc59de>']	factoid	with-premise	long-search-query	similar-to-document	multi-aspect	expert	2025-05-12T20:46:45.703947	16	86	3619
13	how does monopoly power affect internet censorship regulation	Internet monopolies like Google and Facebook wield significant power over speech regulation through content moderation, similar to how Hollywood studios' consolidation in the 1930s enabled private censorship. While these companies face pressure from governments and groups to censor content, their decisions about what to block are not bound by the First Amendment. The international legal community is working to establish regulations, with bodies like the UN Human Rights Committee proposing guidelines for content moderation processes and transparency requirements for both companies and states.	"['In 1930, a man named Daniel Lord wrote a Production Code for American motion pictures. He included specific prohibitions: ""Dances suggesting indecent passions,"" he wrote, ""are forbidden."" But Lord\'s general point was to ensure that American films didn\'t glorify that which was morally wrong and that they always had a happy ending. Movies would be a source of uplift. ""No picture shall be produced,"" he wrote, ""that will lower the moral standards of those who see it.""\nLord wasn\'t a government censor. Rather, he was a Roman Catholic priest dedicated to the elimination of ""filth."" Nonetheless, his code—with all of its ambitions of thought control—became one of the most effective regulations on speech in American history, more potent than any law or government program. Lord was successful in large part because the industry imposed his code on itself. The consolidation of the film industry in the 1930s concentrated power in a handful of studios, making them vulnerable to boycotts, ultimately leading to acts of self-censorship.\nAmericans, who have long mistrusted government, are acutely aware of and sensitive to public censorship—more so, perhaps, than any other nation. There is a strong First Amendment tradition in the courts. But Americans tend to be much less concerned with the danger of private censorship. That\'s too bad, because the greatest dangers to free speech in the future will come not from government interference but from speech monopolists. That has been true for much of the 20th century, and while it seems hard to imagine now, it could become the fate of the Internet.\nBefore we get to the future of the Internet, let\'s go back and see what made Lord\'s 1930 speech code so effective. Various censors—government and private—had been interested in controlling the content of film since the medium became popular in the 1900s. But the results were mixed. The film industry was disaggregated. There were hundreds of producers and thousands of independent theaters, and men like Lord realized that trying to police so many producers in so many places was futile.\nAll of that changed in the 1930s. By then, the major studios, which had moved from New York to Hollywood, had integrated production facilities and theater chains, and ruled the once unruly film industry. These studios were vastly more efficient, borrowing methods from Andrew Carnegie and Henry Ford. They produced longer films of better quality, like King Kong (1933) and Gone With the Wind (1939). The integration of every aspect of film production—from the theaters to the actors and directors—made possible a new era of motion pictures.\nIndustrial consolidation, in short, gave rise to the classic Hollywood blockbuster. But it also greatly reduced the number of people necessary to control the content of American film. That, in turn, made the industry vulnerable to directed boycotts—which it suffered, at the hands of the ""Legion of Decency,"" a Catholic advocacy group organized to put censorial pressure on the American film industry. The boycotts succeeded, and the studios agreed that every American script would be turned over to a Catholic censor named Joseph Breen for review before production—what the law calls a ""prior restraint."" As Liberty magazine wrote in 1936, this arrangement left Breen with ""more influence in standardizing world thinking than Mussolini, Hitler, or Stalin.""\nThe impact of industrial consolidation on speech was not confined to the film industry. In the 1920s, American radio was extraordinarily diverse, a medium, not unlike the early Internet, where it was easy to establish a presence—and hundreds of people did. ""A list of all that can be heard with a radio receiver anywhere within 300 miles of greater New York would fill a book,"" Radio News reported in 1922. Amateurs, churches, motorcycle clubs, and universities owned the majority of stations, and radio was how, for example, jazz music first reached white audiences.\nBy the 1930s, however, most of the nation\'s radio stations, with the help of the federal government, belonged to one of two networks: NBC or CBS. They delivered higher-quality programming, thanks in part to the advent of advertising. In fact, most radio content was actually produced not by the networks themselves but by advertising firms, who created the concept of ""entertainment that sells."" The notion of purely public broadcasting would not return to radio until the 1960s.\nThis may sound like ancient history, but it\'s very relevant today. We are living in an age where a decreasing number of firms serve as a kind of Master Switch over speech on the Internet—think Google, Facebook, the cable industry, and the major telephone carriers.\nThese firms are already under strong pressure to censor from powerful governments, religious groups, political parties, and essentially any outfit with a reason to want information suppressed. The Turkish government, for example, demands that Google take down mockery of the nation\'s founder, not just in Turkey, but everywhere. The Church of Scientology has never stopped demanding of anyone who will listen to remove criticism of its practices from the Internet, usually claiming copyright infringement.\nOn a daily basis, as we speak, Internet companies are making speech-related decisions more important than those made by any government. YouTube, for instance, has to constantly decide what to censor. Generally, it blocks copyright infringements on request and pornography without request, and it listens to some but not all of the demands of governments. Facebook, for its part, has been tested less, but it has been willing to delete user-generated content at the request of governments, like Pakistan and Bangladesh.\nThis is what speech management looks like in 2010. No one elected Facebook or YouTube, and neither one is beholden to the First Amendment. Nonetheless, it is their decisions that dictate, effectively, who gets heard. What\'s the answer? There is no easy answer. Monopolies like Google, Facebook, and Hollywood have certain advantages: That\'s why they tend to come into existence. That means the American public needs to be aware of the dangers that private censors can pose to free speech. The American Constitution was written to control abuses of power, but it didn\'t account for the heavy concentration of private power that we see today. And in the end, power is power, whether in private or public hands.', 'Whether it’s the algorithms of TikTok or the AI of ChatGPT, technology is becoming more sophisticated every day. The world is changing, and emerging technology is surpassing society’s expectations. These platforms are transforming our relationship with the Internet and one another. Through these everchanging accelerated times comes a lot of fear and lack of control. The lack of concrete regulation has the international legal community at a crossroads. India bans TikTok on the grounds that “the activities these apps engage in are detrimental to India’s sovereignty and integrity, national defense, national security and public order.” On April 2, 2023, Italy temporarily banned ChatGPT due to a data breach, setting precedent and fear for other governments. A regulator found that the “hot sensation of AI” could be in violation of the European Union’s General Data Protection Regulation (GDPR), requiring adherence to the protection of personal information in the EU. The OpenAI allegedly violated the GDPR by not properly informing users of collected user data, not providing a legal reason for user data collection, inaccurate processing of personal information without use of facts, and not requiring users to verify their age.\nRegulators are concerned by the challenges these technologies pose for data privacy, AI manipulation, equality, false information, and national security. Further, companies are processing biometric data or personal data, lacking transparency in its purpose of use. The international community believes that safeguarding national security is the fundamental right granted to sovereign states by international law, including banning software and programs as they see fit.\nOn the other hand, this type of control is challenged under the human rights framework and right to freedom of expression. In Article 19 of the Universal Declaration of Human Rights, “Everyone has the right to freedom of opinion and expression; this right includes freedom to hold opinions without interference and to seek, receive and impart information and ideas through any media and regardless of frontiers.” Further, the International Covenant on Civil and Political Rights (ICCPR) echoes Article 19, providing a foundational statement of state obligations to protect the freedom of opinion and expression. The ICCPR sets out a three part test which requires speech restriction to be (1) provided by law and (2) necessary and proportionate to protect (3) a legitimate objective.\nThrough these fundamental contradictions, the protection of personal privacy is the focus of protecting human rights and safeguarding personal information. The privacy of an individual is an internationally recognized human right. The Committee’s General Comment No. 34 interpreted Article 19 to promote ‘uncensored and unhindered’ media as essential to the cornerstones of a democratic society that enjoys freedom of opinion and expression. When Turkey implemented a ban on Twitter, the Special Rapporteur included ICCPR’s test in their decision where “[c]oncerns about national security can be legitimate, but limitations to the freedom to seek, receive and impart information must conform to the strict test of necessity and proportionality to the aim pursued.”\nIf the answer is not banning programs due to human right violations, then it is up to the international legal community to set stronger regulations encompassing data privacy expectations. European lawmakers have their hands full as discussion of a new European AI Act is to be conjoined with GDPR, heavily restricting the use of AI in critical infrastructure, education, law enforcement, and the judicial system. While these regulations catch up to the technology and issues of today, the UN Human Rights Committee has set out 5 actions for States and companies to consider when moderating and regulating online content:\n“1. Urging that the focus of regulation should be on improving content moderation processes, rather than adding content specific restrictions.\n2. Restrictions imposed by States should be based on laws, they should be clear, and they should be necessary, proportionate and non-discriminatory.\n3. Companies need to be transparent about how they curate and moderate content and how they share information, and States need to be transparent about their requests to restrict content or access users’ data.\n4. Users should have effective opportunities to appeal against decisions they consider to be unfair, and independent courts should have the final say over lawfulness of content.\n5. Civil society and experts should be involved in the design and evaluation of regulations.”\nFuture generations rely on these pivotal discussions and regulations. Personal data and how it is used is at the forefront of concerns surrounding platforms like ChatGPT and TikTok. Every country is a member of the cyberspace; therefore, it is time to innovate the law and create global norms for tech accountability.\n India bans 59 mostly Chinese apps including TikTok, UC Browser, WeChat, Reuters (June 29, 2020), https://www.reuters.com/article/india-china-apps-idCNL4N2E63GF.\n Ryan Browne, Italy became the first Western country to ban ChatGPT. Here’s what other countries are doing, Cnbc (Apr. 4, 2023), https://www.cnbc.com/2023/04/04/italy-has-banned-chatgpt-heres-what-other-countries-are-doing.html.\n Clothilde Goujard & Gian Volpicelli, ChatGPT is entering a world of regulatory pain in Europe, Politico, (Apr. 5, 2023, 6:00AM CET) https://www.politico.eu/article/chatgpt-world-regulatory-pain-eu-privacy-data-protection-gdpr/.\n Kristi Hines, Exploring Italy’s ChatGPT Ban And Its Potential Impact, SEJ (Apr. 6, 2023) https://www.searchenginejournal.com/chatgpt-ban-italy/484157/#close.\n See Miriam Kohn, Clearview AI, TikTok, and the Collection of Facial Images in International Law, 23 Chi. J. Int’l L. 205 (2022).\n Haokun Niu & Jiahui Hong, International Law Thinking on Data Security In TikTok Incident, in PROCEEDINGS OF THE 7TH INTERNATIONAL CONFERENCE ON HUMANITIES AND SOCIAL SCIENCE RESEARCH 268, 268-71 (May 20, 2021).\n G.A. Res. 217 (III) A, Universal Declaration of Human Rights, art. 19 (Dec. 10, 1948).\n International Covenant on Civil and Political Rights, art. 19, Dec. 16, 1966, 999 U.N.T.S. 171 (committing 173 state parties to the multilateral treaty).\n Human Rights Committee, General Comment 34: Article 19: Freedom of Opinion and Expression, CCPR/C/GC/34, Sept. 12, 2011.\n Robert L. Rembert, TikTok, WeChat, and National Security: Toward a U.S. Data Privacy Framework, 74 Okla. L. Rev. 463 (2022).\n Necessary And Proportionate? Tiktok Bans And American Obligations Under International Human Rights Law, UC Irvine Sch. of L. Int’l Just. Clinic, 3 (Mar. 10, 2023) (citing to Human Rights Committee, General Comment 34: Article 19: Freedom of Opinion and Expression, CCPR/C/GC/34, Sept. 12, 2011).\n Turkey: First Twitter, now YouTube – UN rights experts concerned at attempts to restrict access before elections, Ohchr, (Mar. 28, 2014), https://www.ohchr.org/en/press-releases/2014/03/turkey-first-twitter-now-youtube-un-rights-experts-concerned-attempts.\n Browne, supra note 2.\n Moderating online content: fighting harm or silencing dissent? Ohchr, (July 23, 2021), https://www.ohchr.org/en/stories/2021/07/moderating-online-content-fighting-harm-or-silencing-dissent.']"	['<urn:uuid:6ba8e0b7-f57d-4648-9052-8acecf67b54f>', '<urn:uuid:ae976b75-a035-497d-9e3a-d5515f7e4633>']	factoid	direct	short-search-query	similar-to-document	multi-aspect	expert	2025-05-12T20:46:45.703947	8	83	2072
14	What happened when Ban Chao tried to contact the Roman Empire?	Ban Chao sent a team to search for the Roman Empire, but they only reached the Persian Gulf and turned back due to the long, dangerous journey ahead.	"['Ban Chao — A Pioneer Diplomatist of the Han Dynasty\nBan Chao (32 — 102), courtesy name Zhongsheng, respected as Ban Dingyuan or Marquis of Dingyuan, was one of the greatest diplomatists in the history of China.\nHe led 36 warriors conquered and pacified over 50 countries in the Western Region of the Han Empire and defeated the powerful Kushan Empire\'s invasion.\nAs a diplomat with limited numbers of warriors, his achievements were as extraordinary as an exceptional marshal.\nUnearthed Brocade Barcer of the Han Dynasty — Xinjiang Museum (Photo by Dongmaiying)\nGrowing Up in the Family of Historians\nBan Chao came from a respectable Confucianism family, whose father was a knowledgeable historian of the Han Dynasty.\nHis big brother Ban Gu was an even more talented litterateur and poet that impressed the emperor by his exceptional works and soon was nominated as the most honorable historian, who wrote the Book of Han, one of the most important historical masterpieces in Chinese culture.\nHowever, Ban Gu was implicated in a general\'s rebellion, been framed up, and passed away in the prison later.\nBan Chao’s little sister Ban Zhao was an exceptional historian and litterateur as well. She was widely trusted and respected among the nobles, including the emperor, because of her outstanding talent.\nAfter their big brother passed away, left his masterpiece, the Book of Han, unfinished, Ban Zhao took over this job and finished it excellently. Afterward, she participated in politics and was highly trusted by Empress Dowager Deng Sui.\nPainted Pottery Building of the Eastern Han Dynasty — Nelson-Atkins Museum of Art (Photo by Dongmaiying)\nInspiring from Excellent Generals and Joining of the Army\nAs for Ban Chao, in his early years, he was taught to be a historian, just like his father and big brother.\nHe had served as an officer in the historical documentation department for a long time and then was supposed to assist his big brother to finish that masterpiece, the Book of Han.\nBut one day, Ban Chao read about a diplomat named Fu Jiezi, who was supposed to condemn a king who helped the Xiongnu, the long term enemy of the Han Empire, and murdered some merchants of the Han Empire.\nCarpet of the Han Dynasty — Xinjiang Museum (Photo by Dongmaiying)\nInstead, Fu Jiezi assassinated that king and supported their crown prince, an admirer of Han’s culture, to inherit the throne.\nThis decapitation operation shocked Ban Chao a lot.\nSo, Ban Chao put away his books and writing brush, then joined Han\'s army in his 40s.\nHe got promoted soon because of his braveness and talent, and then was assigned to a diplomatic mission.\nUnearthed Clogs of the Han Dynasty — Chengdu Museum (Photo by Dongmaiying)\nBan Chao\'s First Mission as A Diplomat\nDuring Wang Mang\'s reign (9 — 23), he demoted most kings in the west of Han, which estranged those regimes.\nAt that time, the Xiongnu that was already retreating far away and didn’t dare to fight with Han directly on the battlefield started to gain more power and control over the Western Region.\nTherefore, small countries in this area were either complied with the Han Empire, or the Xiongnu or swinging between those two.\nBan Chao was sent to one of those swing regimes, with a group of 36 people.\nThey sensed that the king was polite but indifferent; soon Ban Chao found out that the Xiongnu also sent a big group with hundreds of warriors and diplomats at the same time, trying to win over this country\'s compliance.\nThen Ban Chao led his 36 soldiers, ambushed and perished the Xiongnu’s big diplomatic group first; afterward, he informed the king that the Xiongnu group was evil and had been wiped out.\nAfterward, the king decided to comply. He sent his son to the Han Empire, and his country completely stopped swinging.\nSuccessful Diplomat Ban Chao and His 36 Warriors\nBan Chao’s courage and decisiveness impressed his emperor, so he was promoted and given more missions in the West Region.\nHe refused a big army that the emperor assigned him, and took only his former 36 soldiers started his westward journey.\nThey came to two other swing countries and made them complied with Han again. Ban Chao killed a regent who was supported by the Xiongnu in one country, and kidnapped and then replaced a king in another.\nAfter he made three countries complied within two years, the current emperor of Han departed at a young age and the 12 years old crown prince inherited the throne.\nFiligree Gold Dragon of the Eastern Han Dynasty Decorated with Gems — Dingzhou Museum (Photo by Dongmaiying)\nThe Xiongnu planned to further expand when the Han Empire was during this transition, so the ruling class of the Han decided to drawback in the Western Region for a while and to summon Ban Chao to come back.\nHowever, people from those countries begged him not to leave, one of the prime ministers even killed himself to persuade Ban Chao to stay.\nTherefore, he chose to keep fighting in the west of the Han Empire, instead of enjoying an honorable and wealthy life back in his hometown.\nThe emperor of Han agreed to Ban Chao\'s plan and sent him an army to assist him. However, this army only had about a thousand people, most of them were former criminals.\nWithin around 30 years, Ban Chao pacified and built diplomatic relations with 55 countries alongside the Silk Road.\nRejecting Political Marriage and Defeating Kushan Empire\nWhen Ban Chao was 56 years old, a country that was supported by the Xiongnu, betrayed the Han Empire. So, Ban Chao led around 20,000 other complied countries\' soldiers successfully defeated the Xiongnu\'s allied army, which was constituted of over 50,000 warriors and occupied their strongholds.\nIn this war, the Kushan Empire also contributed to the Han Empire\'s success, so their king paid some tributes and requested to marry a princess of Han. But Ban Chao refused this request without asking Han\'s emperor, for considering Kushan as unqualified.\nThe king of the Kushan Empire was furious and sent around 70, 000 of his elite soldiers to seek revenge.\nBan Chao had fewer soldiers, but he ambushed and cut off the Kushan army\'s food supply, and soon defeated their invasion.\nAfterward, the King of Kushan surrendered and the border came back to peace again.\nBronze Sword and Its Sword Decoration of the Han Dynasty — Nanyang Antique Archaeology Institute (Photo by Dongmaiying)\nFailed Searching for the Roman Empire\nWhen he was 66 years old, Ban Chao was excited to hear that there was a powerful Roman Empire in the west.\nHowever, he was too old for another long-distance adventure, so he sent a team searching for Roman Empire and trying to build a diplomatic relationship.\nThis team reached the Persian Gulf and was told that the Roman Empire was very far away and dangers, and might take them years to reach there; so their leader then decided to go back instead of keeping going.\nHan and Roman, the two most powerful empires of that time, lost the chance to directly communicate with each other.\nLacquer Wooden Spoons of the Han Dynasty, Belonged to Soldiers Quartering in Great Wall along the Silk Road — Dunhuang Museum (Photo by Dongmaiying)\nReturning of Exceptional Diplomat and General Ban Chao\nThree years later, Ban Chao was sick, so he pleaded to come back to his hometown.\nHis little sister Ban Zhao, now an influential female politician, also wrote to the emperor to summon him back.\nAfter Ban Chao finally came back to the Han Empire, he was highly respected and achieved countless honors.\nHe passed away two years after his returning; his two sons were excellent generals as well, his grandson even married a princess of Han.\nBan Chao was widely respected and documented as a great diplomat in the history of China, who completely got through the Silk Road, isolated nearby countries from the Xiongnu, protected his country from the attack of the Kushan Empire, and further weakened and expelled Han Empire\'s enemies.\nHis experience was an incredible legend that was consisted of braveness, insistent, and glorious victory.\nGrand Desert Along the Silk Road (Photo from Documentary ""Hexi Corridor"")\nYou Might Also Like:']"	['<urn:uuid:1e17ab8e-8556-4f02-9c15-548de0323180>']	factoid	direct	concise-and-natural	similar-to-document	single-doc	expert	2025-05-12T20:46:45.703947	11	28	1379
15	How does holiday waste affect the environment?	Holiday waste has significant environmental impacts. When food is wasted, it wastes all resources used in growing, transporting, and processing it. When food rots in landfills, it produces methane, a greenhouse gas 21 times more powerful than carbon dioxide in terms of global warming potential, making our global food system a major contributor to climate change. Beyond food waste, the holiday season generates substantial general waste - the U.S. produces an extra 25 million tons of garbage between Thanksgiving and New Year, including discarded gift wrapping, ribbons, Christmas cards, and decorations. This extensive waste could be reduced through practices like reusing materials and proper food management.	"['Americans waste millions of pounds of food during the holidays. In 2016, more than 6 million turkeys ended up in the trash, according to the NRDC.\nThe thing is, when we waste food, we’re also wasting all the resources that went into growing, transporting and processing it. We’re also greatly contributing to our carbon footprint. When food rots in the landfill, it turns into methane — a potent greenhouse gas with 21 times the global warming potential of carbon dioxide. In fact, our global food system is one of the biggest contributors to climate change.\nAs part of our mission to promote a sustainable future, we feel it immensely important to do what we can to divert waste from our landfills. There are many ways to combat food waste, with proper planning, conscious cooking and reasonable portions, but one fun and approachable way to waste less this holiday season is by transforming leftovers into new and delicious dishes.\nWe found a few recipes we thought we’d suggest:\nSimple staple: Turkey Stock\nIf you’re not already using your turkey carcass to make stock or broth, you are missing out! This is a great way to use the entire bird so nothing goes to waste. Plus, you can use it to make delicious turkey soups, like this one with stuffing dumplings from the the Food Network!\nRecipe from The Kitchn:\n- 1 cooked turkey carcass (about 4 pounds), meat mostly removed and bones broken into large pieces\n- 2 large onions, quartered\n- 4 stalks celery, chopped\n- 4 large carrots, peeled and chopped\n- 1 teaspoon whole black peppercorns\n- Few sprigs of fresh parsley or thyme (optional)\n- Fill the stockpot. Place the turkey, onions, celery, and carrots in a large stockpot. Add enough water to cover, about 1 gallon.\n- Bring to a boil. Bring to a boil over medium-high heat.\n- Simmer about 3 hours. Once boiling, reduce the heat to maintain a simmer. Simmer until reduced by half, 3 to 3 1/2 hours.\n- Strain. Set a fine-mesh strainer over a large bowl. Use tongs to transfer the big bones and vegetables from the stockpot to the strainer. When only small bits remain, pour the stock through the strainer and into the bowl. If you’d like a cleaner, clearer stock, clean out your strainer, line it with a coffee filter or cheesecloth, and strain the stock again into another bowl or clean pot.\n- Cool and store the stock. If not using immediately, divide the stock between several small jars or storage containers. Cool completely, then cover and refrigerate for up to 1 week, or freeze for up to 3 months.\nBreakfast: Thanksgiving Leftovers Hash\nHash is a great way to combine multiple ingredients into one dish. There are lots of variations on Thanksgiving hashes, so depending on your what your leftovers are, you may want to search for a recipe that better fits your ingredients. Either way, anything topped with a runny egg and gravy is bound to be delicious.\nRecipe from Serious Eats:\n- 2 tablespoons vegetable oil or turkey fat\n- 3 cups finely shredded leftover brussels sprouts (see note)\n- 1 small onion, finely sliced (about 3/4 cup)\n- 2 cups leftover roasted potatoes, cut into 1/4-inch dice\n- 1/2 pound roasted turkey meat, cut into 1/4-inch dice\n- 1 tablespoon chili sauce\n- 1 teaspoon hot sauce\n- Kosher salt and freshly ground black pepper\n- 2 to 4 eggs\n- Turkey gravy\n- Heat oil in 12-inch non-stick skillet over high heat until smoking. Add brussels sprouts, onions, and potatoes. Stir once then cook without moving until charred, about 2 minutes. Flip, stir, and repeat until well charred all over, about 10 minutes total, adding turkey half way through cooking. Add chili sauce and hot sauce and season to taste with salt and pepper.\n- Transfer mixture to 10-inch cast iron skillet. Make 2 to 4 wells on surface and add an egg to each. Cover and cook over medium-high heat until bottom is charred and eggs are cooked to desired level of doneness, about 5 to 8 minutes. Serve with turkey gravy.\nLunch: Leftover Thanksgiving Pull-Apart Sliders\nLeftover turkey sandwiches are nothing new, but a baked, pull-apart slider? Looks like an irresistible twist on a classic to me.\nRecipe from the Food Network.\n- 1 1/4 cups leftover gravy\n- 2 cups (about 8 ounces) shredded leftover turkey\n- One 12-pack pull-apart dinner rolls\n- 2 cups leftover stuffing\n- 1/2 cup leftover cranberry sauce\n- 1 heaping cup leftover mashed potatoes, warmed\n- 6 slices provolone\n- 1 tablespoon unsalted butter, melted\n- Flaky sea salt, for sprinkling\n- Combine the gravy and turkey in a medium saucepan and set over medium heat. Cook, stirring occasionally, until the gravy is hot and bubbling, 3 to 5 minutes. Turn off the heat, cover and set aside.\n- Put a rack in the center of the oven and preheat the broiler.\n- Split the attached dinner rolls in half horizontally, keeping them attached. Put the top and bottom halves cut-side up on a baking sheet. Cover the bottom half with the stuffing. Broil until toasted, 1 to 3 minutes.\n- Preheat the oven to 400 degrees F.\n- Spread the cut side of the roll tops with cranberry sauce. Cover the stuffing with mashed potatoes, then use the back of a small spoon to make 12 wells (1 for each slider) in the potatoes. Spoon the turkey and gravy into the wells (it will spill over the wells but this is okay) then cover with overlapping slices of provolone. Cover with the roll tops, then brush the tops with the melted butter. Sprinkle with flaky sea salt.\n- Bake until the bread is toasted and the cheese is melted, 10 to 12 minutes. Serve immediately.\nDinner: Turkey and Cranberry BBQ Sauce Pizza\nThanksgiving leftovers meet pizza. What more needs to be said?\nRecipe from Recipe Runner:\n- 1 pound pizza dough\n- 3/4 cup cranberry sauce\n- 2 T. BBQ sauce\n- 1 cup turkey, diced or shredded\n- 1 1/2 cups Monterey Jack cheese, shredded (mozzarella, fontina, or smoked gouda may also be used)\n- 3 T. red onion, thinly sliced\n- 2 T. cilantro, chopped\n- Preheat oven to 450 degrees and spray a baking sheet or pizza stone with cooking spray.\n- Roll out the pizza dough on the pizza stone or baking sheet and set aside.\n- In a small bowl stir together the cranberry sauce and BBQ sauce.\n- Spread half of the sauce onto the pizza and add the diced turkey to the other half, stirring until the turkey is completely coated.\n- Top the pizza with the cheese, turkey, and sliced red onion.\n- Bake the pizza for 10-12 minutes or until the crust is golden and the cheese is bubbly.\n- Remove the pizza from the oven and top with the chopped cilantro.\nSweet treat: Cranberry Turnovers\nGreat as a dessert or breakfast pastry!\nRecipe from Country Living:\n- 8 oz. cream cheese\n- 2 egg yolks\n- 2 c. confectioners’ sugar\n- 1 package Puff Pastry\n- 3/4 c. leftover cranberry sauce\n- Preheat oven to 400 degrees F. In a medium bowl and using an electric mixer set on low, beat cream cheese, egg yolks, and 1 cup confectioners’ sugar until smooth, 2 to 3 minutes. Set aside.\n- Roll out 1 puff pastry sheet into a 10-inch square. Cut into four 5-inch squares. Place 2 tablespoons cream cheese mixture in center of each square. Top each with 1 1/2 tablespoons cranberry sauce. Dampen edges of each square with water and fold over to form a triangle. Press edges together with a fork to seal. Transfer turnovers to a parchment-lined baking pan. Repeat with second puff pastry sheet to make remaining turnovers.\n- Bake turnovers until puffed and golden brown, about 20 minutes. Transfer turnovers to a wire rack to cool, about 10 minutes.\n- In a small bowl, stir together remaining confectioners’ sugar and 4 tablespoons water until a thick but flowing icing forms. (If icing is too thick, add more water, a few drops at a time.) Fill a resealable plastic bag with icing, seal bag, and snip off a tiny corner. Pipe icing over turnovers, as shown.\nThese are just a few recipes we tracked down, but with so many great ideas out there, we’re curious to hear your favorites! Share with us in the comments.\nOther easy ways to avoid holiday food waste:\n- Send guests home with to-go bags so they can enjoy leftovers, too!\n- Freeze leftovers for a time when you’re not tired of turkey.\n- Feed the soil by composting any spoiled food.\n- Share your scraps with your pets (as long as they are safe for them to eat). They’ve been doing such a great job begging!\n- Donate shelf stable food items to local food pantries and shelters.\nWant sustainability tips straight to your inbox? Become a member today!', ""10 Eco-friendly Tips for the Holidays\nIt’s time for some eco-friendly tips for the upcoming holidays. Christmas is considered the most wasteful time of the year. Did you know that the U.S. alone produces an extra 25 million tons of garbage during the holidays from Thanksgiving to New Year?\nExtra waste includes the gift wrappers, ribbons, Christmas cards, decorations, and other stuff that people throw away after the holidays. If only people would learn to reuse or recycle these things, our planet will be much better off. For example, if families just reuse two feet of ribbon, think about the 38,000 miles of ribbon saved. If individuals re-use gift wraps, such as gift bags and wrapping paper, imagine how much paper will be saved.\nHave you ever thought about how you can personally help reduce Christmas' carbon footprint? By taking simple steps to make the holidays more eco-friendly, it doesn’t mean that Christmas will be less special or memorable. It would actually make a good impact on your pocket, the environment, and the people around you.\nIf you would like to take part in this earth-friendly movement, then you’ll want to take a look at these awesome tips you can adopt during the holidays. Check them out below:\nChristmas Card Recycling\nTip #1: Turn them into handmade Christmas cards\nDon’t throw away the Christmas cards you receive. Recycle them by reusing the decorative part of the cards. Just buy a pack of thin card and adhesive. Cut off the back part of the card and mount the graphic design on a blank card. Fold in half and write your Christmas greetings inside.\nMake the cards more personal and fun by adding some glitter or star decals. This is a great activity for children who enjoy doing arts and crafts. See an example on the right of a pretty handmade recycled Christmas card.\nTip #2: Turn them to Christmas decors\nThere are many recycled Christmas card crafts you can do that makes good Christmas decors. Martha Stewart has tutorials on making Christmas ornamental balls and holiday card monogram labels.\nYou can also simply make Christmas card garlands. Just cut the Christmas cards into triangular pieces, tape them to a string and hang them up.\nRecycled Christmas Card into Decors\nTip #3: Reuse envelopes\nFor every Christmas card, you get an envelope. You can also reuse these when you need to send your handmade Christmas cards. Just buy special “sticker labels” to cover up the old details.\nTip #4: Send digital Christmas cards or e-cards\nTo reduce the quanity of Christmas cards sent each year, you can opt to go digital. Design your own greeting cards on the computer and send it to people that you regularly communicate with via email. Limit the paper greeting cards you send to only your family and closest friends.\nRecycling Christmas Gift Wrapping Paper\nWrapping paper is often used once and thrown away. But if you’re really careful when opening gifts, you can reuse the wrapping paper again. Gently remove the adhesive tape and fold or roll the paper neatly before keeping it.\nHere are some ways to recycle gift wrappers:\nTip #5 Reuse them for wrapping gifts again\nYou can iron out the wrinkled part of wrapper before using. Or just choose portions of the wrapping paper that have no wrinkles. You can fold on the old folds to be able to use more of the paper.\nTip #6 Reuse them for your craft projects\nHere are some craft ideas you can use for old gift wrappers:\n- Making paper beads which you can make into a mosaic of any pattern you like or make them into necklace or bracelet.\n- Use them for embellishing greeting cards, posters and bookmarks.\n- Use them as matting for picture frames.\n- Cut them into squares or circles and use them as box labels.\n- They’re great for making origami.\nTip #7 Make them into handmade paper\nYou can pretty much use any paper for making handmade paper. Then you can use what you make as stationery or wrapping paper once again. The colorful design of the wrapping paper will add extra nice details to handmade paper.\nHow to Make Recycled Handmade Paper\nTip #8 Reuse them to make money or cash envelopes\nIf you like giving cash as gifts for the holidays, you can use old Christmas wrapping paper to make colorful little envelopes. You can also use the envelopes for your own personal use like when budgeting your finances.\nHow to Make Money Envelopes\nTip #9 Use other recyclable materials for wrapping your Christmas presents\nYou can use old newspaper, colorful magazines, brown paper bag, and old maps as gift wrapper. You can even use plastic cups and bottles to wrap gifts. If you’re buying new wrapping paper, look out for those made of recycled paper.\nTip #10 Reuse ribbons and bows\nWhen you receive gifts, be sure to save the ribbons and bows. You can use them again for embellishing presents or for your craft projects like bookmarks or scrapbooks.\nHow to make paper cups gift boxes\nHopefully, you can use some of these tips to make Christmas celebration this year more eco-friendly.""]"	['<urn:uuid:0f280d26-203e-42c1-9a56-88b299c58e15>', '<urn:uuid:e7b4259b-12a9-45a0-b648-e19fc879d567>']	open-ended	direct	concise-and-natural	distant-from-document	three-doc	novice	2025-05-12T20:46:45.703947	7	106	2376
16	Can fennel and milk thistle cause allergic reactions?	Yes, both herbs can cause allergic reactions. Fennel can cause allergic reactions affecting the respiratory system or skin, including photosensitivity, particularly in those allergic to plants in the Apiaceae family (carrots, celery). Milk thistle can cause allergic reactions specifically in people with known allergies to plants in the Asteraceae family (thistles, daisies, artichokes).	"['Fennel use while Breastfeeding\nFennel Levels and Effects while Breastfeeding\nSummary of Use during Lactation\nFennel (Foeniculum vulgare) seeds contain the volatile oil composed largely of anethole, which is a phytoestrogen, as well as fenchone, estragole, 1,8-cineole (eucalyptol), and other constituents. Fennel is a purported galactogogue and is included in some proprietary mixtures promoted to increase milk supply. No scientifically valid clinical trials support this use, although one old, small study found an increase in milk volume and fat content with fennel therapy. Galactogogues should never replace evaluation and counseling on modifiable factors that affect milk production. Immersing the breast in a warm infusion of fennel seeds and marshmallow root has been suggested as a treatment for breast inflammation, but no scientific data are available that support this use. Anethole is excreted in breastmilk. Fennel is generally well tolerated in adults, and fennel oil is ""generally recognized as safe"" (GRAS) for use in food by the US Food and Drug Administration. It has been safely and effectively used alone and with other herbs in infants for the treatment of colic, so the smaller amounts in breastmilk are likely not to be harmful with usual maternal doses. Some sources recommend limiting the duration of treatment to 2 weeks. Excessive maternal use of an herbal tea containing fennel, anise and other herbs appeared to cause toxicity in 2 breastfed newborns that was consistent with toxicity caused by anethole, which is found in fennel and anise. Fennel can cause allergic reactions after oral or topical use affecting the respiratory system or skin, including photosensitivity. Avoid excessive sunlight or ultraviolet light exposure while using this herbal. Fennel should be avoided by mothers if they or their infants are allergic to carrots, celery, or other plants in the Apiaceae family because of possible cross-allergenicity.\nDietary supplements do not require extensive pre-marketing approval from the US Food and Drug Administration. Manufacturers are responsible to ensure the safety, but do not need to prove the safety and effectiveness of dietary supplements before they are marketed. Dietary supplements may contain multiple ingredients, and differences are often found between labeled and actual ingredients or their amounts. A manufacturer may contract with an independent organization to verify the quality of a product or its ingredients, but that does not certify the safety or effectiveness of a product. Because of the above issues, clinical testing results on one product may not be applicable to other products. More detailed information about dietary supplements is available elsewhere on the LactMed Web site.\nMaternal Levels. Eighteen lactating women were given 100 mg of trans-anethol in a capsule on 3 test days. Milk samples were collected every 2 hours for 8 hours starting at the time of ingestion. Trans-anethol was detected in milk at all collection times, with the average concentrations of 2 mcg/L at 0 hours, 9.9 mcg/L at 2 hours, 9.2 mcg/L at 4 hours, 7.3 mcg/L at 6 hours and 4.3 mcg/L at 8 hours after the dose. The average peak anenthol concentration in milk was 23.2 mcg/L. Only small amounts of anenthol glucuronide metabolites were present in the milk samples.\nTwelve nursing mothers who were19 weeks to 19 months postpartum ingested 100 mg of 1,8 cineole (eucalyptol) in the form of delayed-release capsules (Soledum-Klosterfrau Vertriebs GmbH, Germany) that release the drug in the intestine. Then they pumped 1 to 4 milk samples at the time they perceived the smell of eucalyptus on their breath which had been previously shown to be approximately concurrent. A total of 21 milk samples were obtained. Odor was rated by a panel of 3 to 5 experts as either smelling like eucalyptus or not. Fourteen of the samples had a distinct eucalyptus-like odor. Chemical analysis of the positive odor tests found 1,8-cineole in concentrations from 70 to about 2090 mcg/kg of milk, most in the range of 100 to 500 mcg/kg of milk. Samples with negative odor tests contained concentrations in the range of 0.98 to about 20.23 mcg/kg of milk. In one woman who donated 3 samples, the highest concentration of 71 mcg/kg occurred at 1.5 hours after ingestion, with concentrations of 1 mcg/kg before ingestion and 15 mcg/kg at 9.5 hours after ingestion. Eight women had their milk analyzed for 1,8-cineole metabolites. Ten metabolites and several enantiomers of these metabolites were detected.\nFive women who were nursing infants between 6 and 55 weeks of age drank 950 mL of fennel-anise-caraway tea (Messmer Ostfriesische Tee Gesellschaft Laurens Spethmann GmbH & Co. KG, Seevetal, Germany). The main odorant components of the tea are the following terpenes: limonene, 1,8-cineole, fenchone, estragole, carvone, trans-anethole, p-anisaldehyde and anisketone. Mothers collected milk samples at 30 minutes, 1 and 2 hours after ingesting the tea. Ingestion of the tea did not increase the overall terpene content of the milk, but there was wide variation from mother to mother. Some mothers had high background levels of some terpenes, probably from other foods or person care products. In addition, a sensory panel found no significant change in the odor profile of the breastmilk samples compared to blank samples.\nInfant Levels. Relevant published information was not found as of the revision date.\nEffects in Breastfed Infants\nTwo breastfed infants, aged 15 and 20 days, were admitted to the hospital for a reported lack of weight gain in the previous 7 to 10 days, caused by ""difficult feeding"". The parents reported restlessness and vomiting during the past day. One of the mothers also reported feeling drowsy and weak. On examination, the infants were afebrile but had hypotonia, lethargy, emesis, weak cry, poor sucking and weak responses to painful stimuli. Infant laboratory values, electrocardiograms and blood pressures were normal, and septic work-ups were negative. Both mothers had both been drinking more than 2 liters daily of an herbal tea mixture reportedly containing licorice, fennel, anise, and goat\'s rue to stimulate lactation. After the mothers discontinued breastfeeding and the herbal tea, the infants improved within 24 to 36 hours. Symptoms of the affected mother also resolved rapidly after discontinuing the herbal tea. After 2 days, breastfeeding was reinstituted with no further symptoms in the infants. Both infants were doing well at 6 months of age. The authors attributed the maternal and infant symptoms to anethole, which is found in both fennel and anise; however, the anethole levels were not measured in breastmilk, nor were the teas tested for their content.\nNursing mothers who were participating in an experiment on the excretion of 1,8-cineole (eucalyptol) in breastmilk took a 100 mg capsule of 1,8-cineole orally. Although instructed not to, 12 mothers breastfed their infants during the experiment. Mothers reported that none of their infants refused their milk or breastfed less than usual. Two mothers felt that their infants were more agitated a few hours after breastfeeding. A third mother reported that the infant stopped nursing from time to time and ""looked puzzled"", but resumed nursing. Upon repeating the experiment 6 weeks later, the infant did not react in an unusual way during breastfeeding.\nEffects on Lactation and Breastmilk\nA group of 5 nursing mothers were given no herb for 5 days, 15 mL of a 5 % infusion of fennel seeds 3 times daily for 10 days, followed by another 5-day control period from days 15 to 20. Their diet and environment were kept constant during the study period. Milk volume was measured daily and milk fat percentage was measured on days 5, 10, 15 and 20. Milk volume and fat content increased somewhat during the 10-day treatment period and carried over for 3 to 5 days after discontinuation. Because of the lack of randomization, blinding and controls, and small number of participants, no valid conclusion can be made from this study on the galactogogue effects of fennel.\nSixty-six postpartum mothers (22 in each of 3 groups) with no concurrent illnesses were randomly assigned to receive an herbal tea, placebo, or nothing after delivering healthy, fullterm infants. Mothers in the herbal tea group received at least 3 cups daily of 200 mL of Still Tea (Humana-Istanbul, Turkey; containing hibiscus 2.6 grams, fennel extract 200 mg, fennel oil 20 mg, rooibos 200 mg, verbena [vervain] 200 mg, raspberry leaves 200 mg, fenugreek 100 mg, goat\'s rue 100 mg, and, vitamin C 500 mg per 100 grams, per manufacturer\'s web site November 2011). A similar-looking apple tea was used as the placebo. All women were followed by the same nurse and pediatrician who were blinded to what treatment the mothers received. Mothers who received the Still Tea produced more breastmilk with an electric breast pump on the third day postpartum than mothers in the other groups. The infants in the Still Tea group had a lower maximum weight loss, and they regained their birth weights sooner than those in the placebo or no treatment arms. No long-term outcome data were collected. Because many of the ingredients in Still Tea are purported galactogogues, including fennel, no single ingredient can be considered solely responsible for the tea\'s effects, although the authors attributed the action to fengreek\nAn herbal tea containing fennel, fenugreek, hibiscus, rooibos, vervain, raspberry, goat\'s rue, and vitamin C (Humana Still-Tee, Humana GmbH, Herford, Germany) or water was randomly given to nursing mothers in a dosage of 3 cups daily beginning on the day of delivery. Several markers of antioxidant capacity were measured in breastmilk on day 1 and again after 7 to 10 days. No difference was found in the markers between mothers who received the tea and the water.\nAn uncontrolled, nonrandomized, nonblinded study in Iran enrolled 46 healthy nursing mothers between 18 and 35 years of age. At the start of the study, mothers were all nursing their infants and having no difficulty in doing so. The mothers\' serum prolactin was measured in the morning before breakfast at least one hour after the previous nursing bout. Prolactin was measured before and after receiving powdered fennel in a dose of 3 grams daily by mouth for 15 days. The average baseline serum prolactin concentration was 64.6 mcg/L. The serum prolactin concentration after fennel therapy was 95.6 mcg/L. The difference was statistically significant. No measurements of milk production were made.\nThe prolactin level in a mother with established lactation may not affect her ability to breastfeed.\n1. Yarnell E . Botanical medicine in pregnancy and lactation. Altern Complement Ther. 1997;3 (April):93-100.\n2. Ayers JF. The use of alternative therapies in the support of breastfeeding. J Hum Lact. 2000;16:52-6. PMID: 11138225\n3. Dennehy C, Tsourounis C, Bui L, King TL. The use of herbs by California midwives. J Obstet Gynecol Neonatal Nurs. 2010;39:684-93. PMID: 21044150\n4. Hardy ML. Women\'s health series: herbs of special interest to women. J Am Pharm Assoc (Wash). 2000;40:234-42 . PMID: 10730024\n5. Petrie KA, Peck MR. Alternative medicine in maternity care. Prim Care. 2000;27:117-36. PMID: 10739460\n6. Westfall RE. Galactagogue herbs: a qualitative study and review. Can J Midwifery Res Practice. 2003;2:22-7.\n7. Sayed NZ, Deo R, Mukundan U. Herbal remedies used by Warlis of Dahanu to induce lactation in nursing mothers. Indian J Tradit Knowl. 2007;6:602-5.\n8. Jackson PC. Complementary and alternative methods of increasing breast milk supply for lactating mothers of infants in the NICU. Neonatal Netw. 2010;29:225-30. PMID: 20630837\n9. Abascal K, Yarnell E. Botanical galactagogues. Altern Complement Ther. 2008;14:288-94.\n10. Alachkar A, Jaddouh A, Elsheikh MS et al. Traditional medicine in Syria: folk medicine in Aleppo governorate. Nat Prod Commun. 2011;6:79-84. PMID: 21366051\n11. Winterfeld U, Meyer Y, Panchaud A, Einarson A. Management of deficient lactation in Switzerland and Canada: A survey of midwives\' current practices. Breastfeed Med. 2012;7:317-8. PMID: 22224508\n12. The Academy of Breastfeeding Medicine Protocol Committee. ABM clinical protocol #9: use of galactogogues in initiating or augmenting the rate of maternal milk secretion (First revision January 2011). Breastfeed Med. 2011;6:41-9. PMID: 21332371\n13. Stapleton H. The use of herbal medicine in pregnancy and labour. Part II: Events after birth, including those affecting the health of babies. Complement Ther Nurs Midwifery. 1995;1:165-7. PMID: 9456733\n14. Hausner H, Bredie WL, Molgaard C et al. Differential transfer of dietary flavour compounds into human breast milk. Physiol Behav. 2008;95:118-24. PMID: 18571209\n15. Weizman Z, Alkrinawi S, Goldfarb D, Bitran C. Efficacy of herbal tea preparation in infantile colic. J Pediatr. 1993;122:650-2. PMID: 8463920\n16. Alexandrovich I, Rakovitskaya O, Kolmo E et al. The effect of fennel (Foeniculum vulgare) seed oil emulsion in infantile colic: a randomized, placebo-controlled study. Altern Ther Health Med. 2003;9:58-61. PMID: 12868253\n17. Savino F, Cresi F, Castagno E, Silvestro L, Oggero R. A randomized double-blind placebo-controlled trial of a standardized extract of Matricariae recutita, Foeniculum vulgare and Melissa officinalis (ColiMil) in the treatment of breastfed colicky infants. Phytother Res. 2005;19:335-40. PMID: 16041731\n18. Kirsch F, Beauchamp J, Buettner A. Time-dependent aroma changes in breast milk after oral intake of a pharmacological preparation containing 1,8-cineole. Clin Nutr. 2012;31:682-92. PMID: 22405404\n19. Kirsch F, Buettner A. Characterisation of the metabolites of 1,8-cineole transferred into human milk: Concentrations and ratio of enantiomers. Metabolites. 2013;3:47-71. DOI: doi:10.3390/metabo3010047\n20. Kirsch F, Horst K, Rohrig W et al. Tracing metabolite profiles in human milk: studies on the odorant 1,8-cineole transferred into breast milk after oral intake. Metabolomics. 2013;9:483-96. DOI: doi:10.1007/s11306-012-0466-9\n21. Denzer M, Kirsch F, Buettner A. Are odorant constituents of herbal tea transferred into human milk? J Agric Food Chem. 2015;63:104-11. PMID: 25436940\n22. Rosti L, Nardini A, Bettinelli ME, Rosti D. Toxic effects of a herbal tea mixture in two newborns. Acta Paediatr. 1994;83:683. Letter. PMID: 7919774\n23. Nikolov P, Avramov NR. [Investigations on the effect of Foeniculum vulgare, Carum carvi, Anisum vulgare, Crataegus oxyacanthus, and Galga officinalis on lactation]. Izv Meditsinskite Inst Bulg Akad Naukite Sofia Otd Biol Meditsinski Nauki. 1951 ;1:169-82. PMID: 14888359\n24. Turkyilmaz C, Onal E, Hirfanoglu IM et al. The effect of galactagogue herbal tea on breast milk production and short-term catch-up of birth weight in the first week of life. J Altern Complement Med. 2011;17:139-42. PMID: 21261516\n25. Kavurt S, Bas AY, Yucel H et al. The effect of galactagogue herbal tea on oxidant and anti-oxidant status of human milk. J Matern Fetal Neonatal Med. 2013;26:1048-51. PMID: 23363373\n26. Honarvar F, Tadayon M, Afshari P et al. The effect of foeniculum vulgare on serum prolactin level in lactating women. Iran J Obstet Gynecol Infertil. 2013;16:18-24.\nLactMed Record Number\nLast Revision Date\nInformation presented in this database is not meant as a substitute for professional judgment. You should consult your healthcare provider for breastfeeding advice related to your particular situation. The U.S. government does not warrant or assume any liability or responsibility for the accuracy or completeness of the information on this Site.', 'Milk thistle (Silybum marianum or Cardus marianum ) is a plant used for treating liver disorders, breast-feeding problems , and other illnesses. The active ingredient of the herb, silymarin, is found in the ripe seeds of the plant. The milk thistle plant has a long stem, green leaves with white spots, and pink to purple spiky flowered head (which true to its name, resembles a thistle). The plant is native to Europe and grows in the wild in the United States and South America. Other common names for the plant include Mary thistle, St. Mary thistle, Marian thistle, and lady\'s thistle.\nThe medicinal benefits of milk thistle have been valued for more than 2,000 years. Written records show that as early as the first century, Romans were using the plant as a liver-protecting agent. The plant was also frequently used throughout the Middle Ages, and it is in the herbal literature of this period that the medicinal properties of milk thistle seeds are first noted. Nicholas Culpepper, a British herbalist, wrote about the value of the herb in treating diseases of the liver and spleen in the late eighteenth century, and by the end of the next century, records show that American physicians were also prescribing the substance. Silymarin was first isolated from the milk thistle plant by German scientists in the 1960s.\nThe leaves and stem of the milk thistle plant are edible, and can be used in salads or eaten raw. The plant was cultivated as a vegetable in Europe through the end of the nineteenth century.\nMilk thistle is prescribed for a number of medicinal uses, including liver disease treatment and prevention, HIV treatment, lactation problems, gallbladder disorders, mushroom poisoning, and psoriasis , a chronic skin disease characterized by reddish patches.\nMilk thistle is thought to promote the growth of new liver cells, and to prevent toxins from penetrating through healthy liver cells by binding itself to the cell membranes. It is prescribed for cirrhosis, hepatitis , and other liver disorders. Several clinical studies have demonstrated that individuals with cirrhosis who take daily doses of milk thistle extract have a lower mortality rate than those who took a placebo (or sugar pill). While further research needs to be completed, a 2001 article reports that clinical trials show that milk thistle (at 140 mg three times per day) did indeed improve survival among cirrhosis patients.\nIn addition, milk thistle may have a protective effect on the liver, and is sometimes prescribed for patients who take medications that can cause liver damage (e.g., Thorazine, Haldol), or those who are exposed to liverdamaging substances such as lead. A large, controlled trial sponsored by the National Center for Complementary and Alternative Medicine (NCCAM) and the National Institutes of health (NIH) of milk thistle\'s medicinal value in the treatment of hepatitis and liver injury was scheduled to begin in the year 2000.\nMilk thistle is sometimes prescribed for HIV-positive patients to protect the liver from diseases such as hepatitis and from the hepatotoxic effects of other medications prescribed for HIV treatment.\nMilk thistle is frequently prescribed for breastfeeding mothers to promote increased breast milk secretion. Although the herb is considered safe for nursing mothers, it should be acquired from a reputable source and prescribed by an herbalist, naturopathic physician, or other healthcare professional familiar with its use.\nThe active chemical components of the milk thistle, silymarin (a complex of flavonoids) and its constituent, silibin, act as antioxidants . These substances have been shown to slow cell growth in some types of cancer .\nMilk thistle may prevent inflammation of the gall-bladder ducts and clear up jaundice .\nDeath cap mushroom poisoning\nMilk thistle is the only known antidote for death cap mushroom (Amanita phalloides ) poisoning. Ingesting this deadly mushroom can destroy the liver by shutting down protein production in liver cells. Milk thistle neutralizes these toxins and protects the liver. Milk thistle may also be helpful in acetaminophen overdosage.\nBecause the liver neutralizes certain toxins associated with psoriasis attacks, milk thistle is believed to help prevent psoriasis outbreaks by promoting proper liver function.\nSeveral other dermatological uses for the herb are currently under investigation. The antioxidant properties of the herb may have a healing effect on skin wounds and burns . Milk thistle has also been proposed as a cosmetic agent to retain skin tone and quality. Further studies are needed to prove the efficacy of the herb for these applications.\nMilk thistle is available in seed form, in capsules, and in extracts and tinctures. A tincture is an herbal preparation made by diluting the herb in alcohol. Tinctures of milk thistle can be taken in 1 or 2 ml doses three times a day.\nMilk thistle seed has a low level of water solubility, so infusions (or teas) made from the herb are weaker than milk thistle tinctures and extracts. An infusion of milk thistle can be prepared by pouring a cup of boiling water over one teaspoon of seeds that have been ground to a fine texture. After the mixture steeps for 10-20 minutes, the herb is strained out and the mixture can be drunk. Instead of straining, the herb can also be placed into an infuser ball, tea bag, or a piece of cheesecloth or muslin and removed after steeping. Individuals can drink two to three cups of the infusion daily.\nMilk thistle seed can also be taken by mouth in a dose of 1 tsp of fresh ground seeds daily. The herb should always be stored in an airtight container in a cool location away from bright light to maintain its potency.\nIndividuals who suspect they have a liver disorder should always seek care from a healthcare professional.\nMilk thistle should always be obtained from a reputable source that observes stringent quality control procedures and industry-accepted good manufacturing practices. Consumers should look for the designations ""U.S.P."" (U.S. Pharmacopeia ) or ""NF"" (National Formulary ) on milk thistle labeling. Herbal preparations prepared under USP or NF guidelines meet nationally recognized strength, quality, purity, packaging, and labeling standards as recommended by the U. S. Food and Drug Administration (FDA).\nBotanical supplements are regulated by the FDA; however, they currently do not have to undergo any approval process before reaching the consumer market, and are classified as nutritional supplements rather than drugs. Legislation known as the Dietary Supplement Health and Education Act (DSHEA) was passed in 1994 in an effort to standardize the manufacture, labeling, composition, and safety of botanicals and supplements, and in January 2000, the FDA\'s Center for Food Safety and Applied Nutrition (CFSAN) announced a 10-year plan for establishing and implementing these regulations by the year 2010.\nMilk thistle is considered safe to use during pregnancy and in women who breastfeed. However, there are currently no long-term studies on use of the herb during pregnancy or lactation. A woman should speak with her healthcare practitioner before taking any herbs and/or medications during pregnancy.\nMilk thistle may cause mild nausea and diarrhea , or loose stools. The herb may also cause an allergic reaction in some individuals, particularly those with known allergies to plants in the Asteraceae family (thistles, daisies, artichokes). No other widely reported side effects are known when milk thistle is taken in proper therapeutic dosages. However, people with chronic medical conditions should consult with their healthcare professionals before taking the herb.\nThere are no reported negative interactions between milk thistle and other medications and herbs, although certain drugs with the same therapeutic properties as milk thistle may enhance the effect of the herb. Again, individuals should consult their healthcare provider if they are taking other medications concurrently with milk thistle.\nMedical Economics Corporation. The PDR for Herbal Medicines. Montvale, NJ: Medical Economics Corporation, 1998.\nHoffman, David. The Complete Illustrated Herbal. New York: Barnes & Noble Books, 1999.\nTyler, Varro. ""This weed is a potent healer; protect your body from environmental toxins with milk thistle."" Prevention 50, no. 1 (October 1998):79.\nWalsh, Nancy. ""Milk Thistle for Liver Disease (Alternative Medicine: An Evidence-Based Approach."" Internal Medicine News (January 1, 2002): 10.\nOffice of Dietary Supplements. National Institutes of Health. Building 31, Room 1B25, 31 Center Drive, MSC 2086, Bethesda, Maryland 20892-2086. (301) 435-2920 Fax: (301) 480-1845. http://odp.od.nih.gov/ods.\nTeresa G. Odle\n""Milk Thistle."" Gale Encyclopedia of Alternative Medicine. . Encyclopedia.com. (July 9, 2018). http://www.encyclopedia.com/medicine/encyclopedias-almanacs-transcripts-and-maps/milk-thistle\n""Milk Thistle."" Gale Encyclopedia of Alternative Medicine. . Retrieved July 09, 2018 from Encyclopedia.com: http://www.encyclopedia.com/medicine/encyclopedias-almanacs-transcripts-and-maps/milk-thistle\nEncyclopedia.com gives you the ability to cite reference entries and articles according to common styles from the Modern Language Association (MLA), The Chicago Manual of Style, and the American Psychological Association (APA).\nWithin the “Cite this article” tool, pick a style to see how all available information looks when formatted according to that style. Then, copy and paste the text into your bibliography or works cited list.\nBecause each style has its own formatting nuances that evolve over time and not all information is available for every reference entry or article, Encyclopedia.com cannot guarantee each citation it generates. Therefore, it’s best to use Encyclopedia.com citations as a starting point before checking the style against your school or publication’s requirements and the most-recent information available at these sites:\nModern Language Association\nThe Chicago Manual of Style\nAmerican Psychological Association\n- Most online reference entries and articles do not have page numbers. Therefore, that information is unavailable for most Encyclopedia.com content. However, the date of retrieval is often important. Refer to each style’s convention regarding the best way to format page numbers and retrieval dates.\n- In addition to the MLA, Chicago, and APA styles, your school, university, publication, or institution may have its own requirements for citations. Therefore, be sure to refer to those guidelines when editing your bibliography or works cited list.\n""milk thistle."" A Dictionary of Food and Nutrition. . Encyclopedia.com. (July 9, 2018). http://www.encyclopedia.com/education/dictionaries-thesauruses-pictures-and-press-releases/milk-thistle\n""milk thistle."" A Dictionary of Food and Nutrition. . Retrieved July 09, 2018 from Encyclopedia.com: http://www.encyclopedia.com/education/dictionaries-thesauruses-pictures-and-press-releases/milk-thistle']"	['<urn:uuid:dac381ae-9716-4473-8e70-d757b722ec77>', '<urn:uuid:28af8a04-ec23-44fe-8fd0-21f6a713e93d>']	factoid	direct	concise-and-natural	similar-to-document	comparison	novice	2025-05-12T20:46:45.703947	8	53	4089
17	What was the earliest military conflict mentioned in the documents involving Medal of Honor recipients - Pearl Harbor or the Civil War at Missionary Ridge?	The Civil War battle at Missionary Ridge on November 25, 1863 was the earliest conflict mentioned involving Medal of Honor recipients. Arthur MacArthur Jr. received the Medal of Honor for seizing his regiment's colors at a critical moment and planting them on the captured works at Missionary Ridge. The Pearl Harbor attack occurred much later on December 7, 1941.	"['Arthur MacArthur, Jr.\nLieutenant General, United States Army\nfather of Douglas MacArthur and Arthur MacArthur\nIII, he was born in Massachusetts on June 2, 1845. At the outbreak\nof the Civil War, he was living in Wisconsin and immediately joined the\n24th Wisconsin Volunteer Infantry, seeing action at Chickamauga, Stones\nRiver, Chattanooga, the Atlanta Campaign and Franklin. He was awarded the\nMedal of Honor (in 1890) for service at Missionary Ridge, and he and his\nson, Douglas, are one of only two father-son recipients of the Medal of\nHonor in U.S. history (the others being Theodore Roosevelt, who was recently\nawarded the Medal of Honor for his exploits in the Spanish-American War,\nand his son, Theodore Roosevelt, Jr., who was awarded the Medal of Honor\nfor his actions in World War II).\nHe left the Army in June 1865 and began the study of law, but it was not for him and he returned to his first love, the Army, in February 1866, receiving a commission as a Second Lieutenant in the Regular Army.\nFor thirty years he traveled the nation, being assigned to Pennsylvania, New York, Utah Territory, Louisiana and New Mexico, and where he took part in the campaign against Geronimo in 1885. He was promoted to Lieutenant Colonel in 1899. He was stationed in the Dakota Territory when the Spanish-American War broke out in 1898 and he was commissioned a Brigadier General of U.S. Volunteers and took part in the capture of Manila. After the war, President William McKinley named him Military Governor of the Philippines, but the following year, William Howard Taft was appointed as Civilian Governor and he and MacArthur clashed frequently (he seemed to resent civilian authority, much as his son, Douglas, did both in World War II and later in Korea). As a result, he was transferred back to the U.S.\nIn the years that followed he was assigned to various stateside posts and in 1905 was sent to Manchuria to observe the final stages of the Russo-Japanese War and served as Military Attaché to the U.S. Embassy in Tokyo. He returned to the U.S. in 1906 and resumed his post as Commander of the Pacific Division. That year the position of Army Chief of Staff became available and he was then the highest ranking officer in the Army. However, he was overlooked by Secretary of War William Howard Taft (old arguments bit him in the butt). He never did realize his dream of commanding the entire Army.\nHe retired from the Army on June 2, 1909, the day that he turned 64. On September 5, 1912, he went to Milwaukee to address a reunion of his Civil War unit. While on the dais, he suffered a massive heart attack and died there. He was originally buried in Milwaukee on Monday, September 7, 1912, but was moved to Section 2 of Arlington National Cemetery in 1926. He is buried among other members of the family there, while Douglas lies buried in Norfolk, Virginia.\nMACARTHUR, MARY PINCKNEY HARDY W/O ARTHUR\nRank and organization: First Lieutenant, and Adjutant, 24th Wisconsin Infantry. Place and date: At Missionary Ridge, Tennessee, 25 November 1863. Entered service at: Milwaukee, Wisconsin. Birth: Springfield, Massachusetts. Date of issue: 30 June 1890.\nSeized the colors of his regiment at a critical\nmoment and planted them on the captured works on the crest of Missionary\nMANILA, Philippines - December 3, 1935 - Mrs. Arthur MacArthur Sr., mother of Major General Douglas MacArthur, died today after a long illness. She was 82 years old.\nShe had been ill in her hotel suite since she arrived in Manila two months ago. Medicine was brought from the United States by the China Clipper, but it arrived too late.\nGeneral MacArthur, former Chief of Staff of theUnited States Army, is military adviser of the Commonwealth Government of the Philippine Islands.\nMrs. MacArthurwas born in Norfolk, Virginia, and was the widow of Major General Arthur MacArthur, former Military Governor of the Philippines.\nArthur MacArthur (second from left) on duty in the Philippines\nCourtesy of the National Archives\nGravesite photo courtesy of Ron Williams\nUpdated: 30 September 2000 Updated: 24 June 2001 Updated: 26 December 2001 Updated: 13 July 2002 Updated: 15 February 2003 Updated: 9 July 2003 Updated: 27 April 2004 Updated: 4 September 2004 Updated: 13 December 2005 Updated: 25 July 2006 Updated: 22 September 2007\nPhoto By M. R. Patterson, 27 April 2004', 'With no warning the attacking planes came into sight over pearl harbor. At 7:55 AM, December 7, 1941 the first target was hit and by 9:45 AM it was all over. Behind them the Japanese left devastation as their aircraft carriers headed home to Japan. Over 2400 people died, hundreds of planes were destroyed, and a fleet containing eight battleships was ruined. With a single act, Japan woke America to the horrible realities of war. The debate that had divided Americans since the German defeat of France was decided, America would go to war. President Roosevelt received word of the attack on a Sunday afternoon and stated he would ask congress to declare war on Japan. On Monday the declaration of war was signed and one day later, Germany, Italy, and Japan declared war on the US.\nThe following narrative of the Pearl Harbor attack has been compiled from official US Navy files, Congressional records and other official Government sources.\nSHIPS IN THE HARBOR AND HARBOR FACILITIES\nIn December 1941, the entrance to Pearl Harbor was protected by two nets: An outer anti-torpedo net with boat booms consisting of cross spikes to prevent surface craft from entering the harbor and an inner anti-torpedo net without boat booms. The nets served to guard the harbor against submarine as well as torpedo attacks. The channel at the entrance was 400-feet wide and 41 to 50 feet deep. Since the extended depth of the seaward net was 30 feet, the inner net was suspended 15 feet to afford a total coverage of 45 feet.\nMany installations in the harbor at the time of the attack were less than three years old. The major installations included: a battleship dock built in 1928; a battleship dock under construction; one floating dry dock, 18,000 tons; a large repair basin containing industrial installations capable of repairing anything afloat; a fuel depot and two tank farms above ground; a submarine base; a section base (inshore patrol and harbor entrance control post); the 14th Naval District headquarters and the Naval Station Ford Island.\nShips present in the harbor the morning of the attack and their location were as follows: Eight of the Pacific Fleet nine battleships, 29,000 to 33,000 tons each: NEVADA, ARIZONA, WEST VIRGINIA, TENNESSEE, OKLAHOMA, MARYLAND and CALIFORNIA were moored in Battleship Row along the southwest shore of Ford Island. The PENNSYLVANIA, was in dry-dock 1 in the Navy Yard.\nTwo heavy cruisers, NEW ORLEANS and San Francisco, both 10,000 tons, were in the Navy Yard repair basin Four light cruisers, 10,000 tons, were berthed as follows: ST. Louis, HONOLULU, Helena at Navy Yard docks, and Phoenix moored 0n the Northwest side of Ford Island.\nTwo light cruisers, 7,000 tons, Raleigh and Detroit, were moored on the northwest side of Ford Island.\nTwenty nine destroyers, all but three completed after 1933, were moored to the north and west of Ford Island.\nThere were five submarines, four of which were tied up at the submarine base and the fifth at 1010 dock in the Navy Yard (so named because of its length, 1,010 feet).\nOne gunboat was tied up at the Navy Yard dock. Nine minelayers (eight of which were converted flush-deck destroyers) moored in middle loch and at the Navy Yard docks.\nEleven minesweepers (five of which were converted flush-deck destroyers) moored at middle loch and at the Navy Yard docks.\nTwenty-three fleet auxiliaries; repair ships, oilers, tenders, store ships and tugs were located at berths throughout the harbor.\nFortunately there were no carriers in the harbor. There were three carriers assigned to the fleet: the ENTERPRISE, which was 375 miles west of Pearl Harbor, enroute to Hawaii from Wake Island; the LEXINGTON was 460 miles southwest of Midway, enroute from Pearl Harbor to to ferry a Marine Corps bomber squadron to Midway; the SARATOGA was enroute from the west coast to Hawaii.\nAll ships in Pearl Harbor were in readiness condition: three at the time of the attack(i.e., a quarter of the anti-aircraft batteries and necessary control stations manned and ready ammunition at the guns). Ships in that condition of readiness were required to be able to get underway within 12 hours.\nNAVY RADAR EQUIPMENT\nRadar was new to the US Fleet in 1941. Of the ships in Pearl Harbor only the battleships PENNSYLVANIA, CALIFORNIA, WEST VIRGINIA and seaplane tender CURTISS were equipped with ship search radar. Due to the height of the mountains surrounding the harbor, this equipment was not in operation when the attack began. Within 15 minutes of the beginning of the attack, the PENNSYLVANIA and CURTISS had their equipment in operation. Both had negative results.\nFIRST JAPANESE CONTACT\nThe destroyer WARD conducted an anti-submarine patrol just outside the harbor entrance the night of December 6th and the early morning of December 7th. At 2 a. m. the morning of the 7th, the minesweepers CONDOR and CROSSBILL stood out of Pearl Harbor and began sweeping operations in the same area. At 3:45 a.m., the CONDOR operating about 1 3/4 miles from the harbor entrance, sighted a submarine periscope about 100 feet away. The periscope disappeared immediately, but the CONDOR notified the WARD of the sighting by visual signal. The Ward\'s crew immediately went to general quarters and the ship began an extensive search for the unidentified craft.\nThe submarine was spotted by the WARD at 6:40 a.m..The ship closed on the sub immediately and commenced an attack with her guns and depth charges at 6:45 a.m.. The second salvo from the guns made a direct hit on the craft\'s conning tower and the vessel sank. A large oil bubble appeared on the surface at 7:06 a. m. and the Ward notified the 14th Naval District Duty Officer of the contact. The following message was received at District Headquarters at 7:12 a.m.:\n""WE HAVE ATTACKED, FIRED UPON, AND DROPPED DEPTH CHARGES UPON A SUBMARINE OPERATING IN THE DEFENSIVE SEA AREA.""\nThe information was reported to the Pacific Fleet Duty Officer at 7:15 a.m.. This was the first word at Pearl Harbor that unidentified submarines were operating in the defensive sea area.\nWhen the Ward\'s report was received, the ready-duty destroyer MONAGHAN was ordered to sea to verify the report and to relay details to the Fleet Duty Officer. She was the first ship underway after the attack began, leaving her berth 8:27. At 8:43 she rammed and sank a Japanese midget submarine in the harbor north of Ford Island. She then stood out of the harbor.\nAt 7:55 a.m. Pearl Harbor time the Pearl Harbor signal tower telephoned the Pacific Fleet Commander:\nENEMY AIR RAID-NOT DRILL!\nAt nearly the same time Japanese torpedo planes began attacking the ships on Battleship Row. For almost two hours, until 9:45 a. m., Pearl Harbor was under attack.\nPHASES OF THE ATTACK\nThe attack was carried out in five phases: (1) 7:55 to 8:25 Torpedo plane and dive bomber attacks; (2) 8:25 to 8:40 Lull in attack; (3) 8:40 to 9:15 Horizontal bomber attacks; (4) 9:15 to 9:45: Dive bomber attacks; (5) Completion of raid.\nThe only attack on Ford Island took place in the opening minutes of the raid. Seventy patrol boats, float planes and carrier type scout bombers on the field caught fire, resulting in 33 being destroyed or severely damaged.\nThe major damage in the first wave was done by torpedo planes. They launched their torpedoes from extremely low altitude, as low as 50 to 100 feet, and the harbor\'s shallow water did not prevent the successful execution of these attacks since the torpedoes had been specially fitted with wooden fins to compensate for water depth.\nThere were four separate attacks in phase one, the major one being made by 12 planes coming in over the southwest corner of the tank farm in the vicinity of Merry\'s Point Landing. During the first wave, all the outboard ships in Battleship Row received torpedo hits.\nThe OKLAHOMA was hit by three torpedoes during the second attack of this phase. She began listing rapidly.\nThe UTAH recently returned to Pearl Harbor from serving as a target for practice aerial bombardment. When the Japanese hit her, she listed rapidly to port, when the list was about 40 degrees, the order to abandon ship was given. Later knocking was heard from within the hull of the ship and a volunteer crew cut into the ship and rescued a fireman second class who had been trapped in a void beneath the dynamo room.\nThe number of aircraft in the first phase of the attack has been estimated at 21, including light, medium and incendiary dive bombers.\nDuring the second attack in the first phase, a bomb hit the forward 14 inch powder magazine aboard the ARIZONA and ignited an intense fire endangering the TENNESSEE and Ford Island.\nThere was no ship movement in the harbor during this phase. The destroyer HELM opened fire on a submarine just outside the harbor entrance at 8:19, but no hits were observed.\nThis phase has been called a lull only by comparison. Attacking aircraft were over the harbor continuously, but attacks were sporadic. During this phase, an estimated 15 dive bombers participated in five attacks on ships the Navy Yard, the battleships MARYLAND, OKLAHOMA, NEVADA, PENNSYLVANIA, various light cruisers and destroyers.\nThere were still very little ship movement during this phase of the attack. The destroyer MONAGHAN, moving toward the harbor entrance, sighted a submarine west of Ford Island being fired on by the CURTISS and TANGIER. The MONAGHAN rammed the sub at 8:43 and dropped depth charges on her. The CURTISS previously had scored two hits on her conning tower. The submarine later was salvaged for inspection and disposal. The MONAGHAN then proceeded to sea. At the same time the MONAGHAN got underway, the destroyer HENLEY got underway and followed the MONAGHAN to sea.\nPhase II ended with the appearance of eight groups of high altitude bombers which crossed and re-crossed the harbor inflicting severe damage. Some of the bombs were converted 15 or 16 inch shells.\nIt was during this attack that the PENNSYLVANIA was hit by a heavy bomb which detonated amidships, causing a fire that was extinguished with some difficulty. The CALIFORNIA was hit by a 15 inch shell equipped with tail vanes which penetrated to the second deck and exploded.\nAn estimated 30 horizontal bombers and 18 dive bombers participated in this phase. The second group of these is considered to be the one which inflicted heavy damage to the NEVADA, then steaming down the south channel, and to the SHAW, CASS1N and DOWNS, all three of which were set afire. A dive bomber also dropped one bomb on the CURTISS which detonated on the main deck killing 20 and wounding 58. The CURTISS destroyed one dive bomber which was pulling out of a dive over Ford Island.\nThere was considerable ship movement in the harbor during this phase. The NEVADA cleared her berth at 8:40 without assistance and steamed toward the harbor entrance. Dive bombers attacked the ship, registering several hits. Although her bridge and Fore structure were afire, she continued to fight effectively until 9:10 when, when she was trying to turn in the channel, she ran aground at Waipio Point.\nThe repair ship VESTAL got underway at 8:40, unassisted and cleared the burning ARIZONA. She anchored Northeast of Ford Island at 9:10. Soon after, tugs began moving the OGALA to a position behind the HELENA at 1010 dock She was secured at 9: 00 a. m. but capsized shortly afterward. The oiler NEOSHO cleared her berth in Battleship Row, astern of the CALIFORNIA, at 8:42 and backed to Merry Point to reduce fire hazard to her cargo and to clear the way for the MARYLAND to sortie.\nDuring this phase 27 dive bombers conducted nine strafing attacks throughout the harbor.\nCOMPLETION OF THE ATTACK\nBy 9:45, all enemy planes had left the area. The Japanese task force returned to Japan without being contacted by any American units.\nIn Pearl Harbor, 18 warships either had been sunk or seriously damaged. The first Battleship damaged in the attack to return to action from the mainland was the MARYLAND.\n2,408 KILLED - 1,178 WOUNDED = 3,586 TOTAL\nNOTE:Many more were known to have received wounds from shrapnel and other flying debris that didn\'t receive hospitalization in the the already overcrowded medical facilities and no records were kept of their treatment.\nAWARDS FOR HEROISM ON DECEMBER 7, 1941\nMEDAL OF HONOR = 15\nNAVY CROSS = 51\nSILVER STAR = 53\nNAVY AND MARINE CORPS MEDAL = 4\nDISTINGUISHED FLYING CROSS = 1\nDISTINGUISHED SERVICE CROSS = 4\nDISTINGUISHED SERVICE MEDAL = 1\nBRONZE STAR = 3\nTHE ARIZONA MEMORIAL IS FOR THOSE WHO WERE KILLED DURING\nTHE SURPRISE JAPANESE ATTACK ON PEARL HARBOR, DECEMBER 7,1941.']"	['<urn:uuid:1b1d7b4b-cd93-4e52-8ffe-be1035309bc2>', '<urn:uuid:ce983119-2f97-4df6-9221-e0ded4bf3a5c>']	factoid	direct	verbose-and-natural	distant-from-document	comparison	novice	2025-05-12T20:46:45.703947	25	59	2872
18	clay figures keep details when copying	To preserve high detail and fidelity when making copies of clay figures, you need to take extra care when creating and using the mold. First, ensure the mold putty is very tightly conformed to your master sculpt. When making copies, it's crucial to fill every possible space inside the mold with clay - twist the clay into shape and use small tools to squish it in very tightly. For best results, bake the polymer clay while it's still in the silicone mold (the mold won't melt in the oven). This method will produce perfect copies of your original master sculpt with all the fine details preserved.	['As a miniature artist, I’ve always valued being able to produce repeatable results — particularly when the results are quite good. There are also times when I want to make many of the same thing, or variations of the same item. This is where making homemade molds has been incredibly useful!\nThere are several different techniques and ways to create and use molds with polymer clay. Read on to find out more.\nHow to Make Silicone Molds\nIt’s safe and straightforward to get started making silicone molds for polymer clay.\nMake a Master Sculpt or Original Piece\nThe first step to making a mold is always to make a “master” sculpt. This is the thing you’d like to make copies of. Pay special attention to detail and produce high quality master sculpts. Anything you make in the mold later will be a copy of this one original sculpted item. You can take your time sculpting your master. Don’t worry about needing to reproduce or sculpt any more of the same thing by hand. That’s what the mold is for.\nWhen I know I’m going to be making a mold, I make master sculpts out of modeling clay. This clay is specifically formulated for sculpting and holding details, like Super Sculpey Firm (shown below):\nI don’t always make something with the intent to make a mold. Sometimes, I make an original item and then later want to make more. In this case, I cast one of the finished items into a mold. In these cases, the finished item is most likely not made from modeling clay but rather, colored polymer clay.\nYou can make a mold of pretty much anything. However, keep these things in mind when your intent is to use the molds with polymer clay:\nOne Element / Color Per Mold is Recommended\nMolds for polymer clay are really meant to be used for one color of clay at a time. Think of them as components. You can make several molds of different components of different colors and then assemble the pieces later. Putting different colored clays into the same mold is not ideal when you’re making miniatures. (In this way, this is fairly different from the techniques used when making artisan keycap masters for resin casting.)\nDesign a Flat or Sculptable Surface\nYour master sculpt needs an exit point in the mold, which should also have a large enough area that you can easily push raw clay into the mold and manipulate it using tools. Your master sculpt should therefore have a flat surface on it — or a surface that you don’t mind hand-sculpting each time you use your mold.\nThere are many different products for making silicone molds. Many mold-making silicones are two parts that can either be mixed by hand then harden in a short amount of time, or as a liquid that cures over several hours. Depending on how much time you want to spend (and your tolerance for error), you can choose from a few options. Here are my favorites.\nLiquid Silicone Rubber\nIf you can afford to attach your master to a surface in a mold box or container, you can mix two-part liquid silicone rubber and pour it over your master, then let it cure several hours to overnight. This method is the most expensive (relatively speaking in terms of materials), the messiest, the most time-consuming, and the least forgiving. But when done correctly, it will produce the highest fidelity results. If you’re going this route, I recommend Smooth-On Mold Star 30.\nWhile I always use liquid silicone rubber to make artisan keycap molds for resin-casting, I’ve personally found that I strongly prefer a different mold-making product when it comes to making molds for polymer clay.\nFor clay molds, I have a strong preference for Amazing Mold Putty from Alumilite. This is a very easy-to-use two-part kit. Parts A and B are both putty solids. They don’t make a mess, are easy to scoop out by hand and weigh on a small kitchen scale, and simple and clean to mix with just your bare hands. There is no cleanup, and the mold cures in the air in only twenty minutes.\nBecause this is a putty, you do need to take extra care when pressing the master sculpt into the mold material, and make sure that there are no gaps or spaces trapped inside it. For my purposes, I will always choose Amazing Mold Putty first when making molds for polymer clay. There have been one or two times when I’ve found that the material is insufficient for my needs and have wound up using Mold Star secondarily, but this happens very, very rarely.\nUsing mold putty takes only a few steps and minimal equipment. You’ll just need a kitchen scale capable of weighing ounces / grams.\n- Measure (weigh) equal parts A and B.\n- Mix the two parts by hand by folding the putties together until the resulting mixture is uniform in color.\n- Carefully press / squish the putty around your master sculpt, ensuring enough of the master is exposed to remove any copies you make.\n- Let the mold cure for 20 minutes and remove the master.\n- Press raw clay very tightly into the mold, using tools to make sure all mold spaces are completely filled.\n- Either remove the raw clay to manipulate the base shape, or bake the mold with the clay in it to cure the clay. (See sections below for more details.)\nAlthough you might think this material and method would produce lower quality molds, the level of detail and fidelity is still very high in very small scale when using Amazing Mold Putty (as you will see in the photos below).\nUsing Molds for Base Forms With Variation\nNow let’s talk about the different uses for molds for polymer clay. One thing I like to do is use molds for creating basic forms, and then adding variation and details to the basic shapes while the clay is raw (before baking). Filling the mold with clay and then popping it out and using sculpting tools to manipulate the clay ensures that no two pieces are identical.\nThis is very useful for reproducing basic shapes (which is often the time-consuming, less-fun part of repeated sculpting). When you remove raw clay from a mold, you will always lose a little bit of fidelity when working with very small miniatures. But in this use case, that is exactly what we want to happen.\nThe molds in the photos above were used to create basic shapes and details and variations were added later. I was also able to make the master sculpts with easier-to-sculpt-with modeling clay, and could then reproduce copies in other colors using clay that is softer and less ideal for sculpting, but better for final outcome colors.\nAs you can see in the photo on the left above, removing the raw clay from the mold before baking / curing results in a loss of fidelity. However, I can then sculpt tiny details to add uniqueness to each piece.\nUsing Molds for Repeatable Details and Fidelity\nConversely, I also make molds specifically to preserve a high degree of detail and fidelity: to make precise, identical copies of my master sculpt.\nIn this case, I take extra care to ensure my mold putty is very tightly conformed to my master sculpt to produce a high quality, perfect mold. In the images below, I sculpted a tiny turtle and I want to make sure that every copy I make is as close to the original as possible.\nOne of the keys to recreating perfect copies is to make sure that the clay you fill the mold with fills every possible space inside the mold. Twist the clay into shape and then use small tools to squish it in very tightly, ensuring that it fills the mold completely.\nThen bake your polymer clay with the mold — it’s silicone, so it won’t burn or melt during this stage! Just be careful touching it afterwards, as it will be very hot when it comes out of the oven.\nCarefully remove your hard, cured clay from the mold and you will have perfect copies of your original master sculpt.\nUsing Molds for Batch Production\nSometimes I need a lot of something, but it’s a very tedious item to make over and over. Take these raspberries shown below, for example. Each raspberry needs tens of little tiny pieces of clay added together in order to make one raspberry. It’s also very easy to accidentally squish the berry as you’re sculpting it. There’s also not a high need for distinct uniqueness between berries. They’re so small that nobody can tell if they are identical to each other or not.\nMolds are great for batch production, with one caveat. You must produce enough copies to make multiple molds in the first place. However, using fast-curing mold putty, this doesn’t take very long. I made one raspberry out of modeling clay, made one mold. I then made more berries using the mold in order to make more molds. This way, I was able to make many raspberries at the same time.\nEach raspberry has a little hole poked in the top opening of the mold. Some texture is also added prior to baking. Raspberries don’t have a “flat surface,” but it’s quick to add these details during production. This step also lends some uniqueness to every berry.\nVersatile Base Molds\nFinally, I often want to make different variations of something using the same base components. Rice is a great example of this. Rice is tremendously tedious to make with polymer clay. Rolling out every individual rice grain is very time consuming and not particularly exciting. I made two pretty specific rice-based components: a shaped serving of rice, and nigiri rice for different sushi presentations.\nIn each case, I made one master and then used mold putty to easily produce additional copies. Check out the gallery below for examples of how these two molds have been used:\nThe rice serving is versatile as a side dish, in a rice bowl, by itself, and more. I could make white rice, brown rice, fried rice, and more using this mold.\nThe nigiri sushi rice is perfect for producing consistently sized and shaped sets. The additional components add variation, so I don’t need to worry about the rice being identical across different pieces. I always bake my polymer clay rice inside the molds. It’s very prone to getting mushed if removal is attempted with raw clay.\nAs always, thank you for reading! I hope this short article was helpful to you in your polymer clay miniatures journey. You can read more tutorials and making-of articles in my blog. And I’m sure you can think of other applications for miniature molds.\nIf you’d like to follow along with what I’m working on, you can explore my Instagram at @mihi.mini.studio. To be notified of new tutorials, guides, sales, and more, subscribe to the newsletter.']	['<urn:uuid:a80ae511-6dc9-44b7-9eff-41c7ec67afac>']	open-ended	with-premise	short-search-query	distant-from-document	single-doc	novice	2025-05-12T20:46:45.703947	6	106	1844
19	what technologies combined foundationone tracker ctdna monitoring	FoundationOne Tracker combines Foundation Medicine's tissue-based comprehensive genomic profiling (CGP) platform with Natera's expertise in ctDNA monitoring	"['Personalized technology aims to address both early- and advanced-stage cancer monitoring applications\nCAMBRIDGE, Mass. & AUSTIN, Texas--(BUSINESS WIRE)-- Foundation Medicine, Inc. and Natera, Inc. (NASDAQ: NTRA), today announced the launch of the research use version of FoundationOne®Tracker, Foundation Medicine’s personalized circulating tumor DNA (ctDNA) monitoring assay. FoundationOne Tracker uniquely combines Foundation Medicine’s tissue-based comprehensive genomic profiling (CGP) platform with Natera’s expertise in ctDNA monitoring.\nFoundationOne Tracker uses optimized algorithms for identifying tumor-specific variants and a personalized assay design that allows for the detection of ctDNA in plasma for use in both advanced- and early-stage research applications. Additionally, the creation of a robust, fully integrated workflow between Foundation Medicine and Natera is expected to provide rapid inclusion of ctDNA monitoring results into retrospective research studies.\n“The addition of FoundationOne Tracker for research use provides our biopharma and academic partners access to Foundation Medicine’s broad, advanced-stage CGP footprint,” said Foundation Medicine’s Chief Scientific Officer Priti Hegde. “In partnering with Natera, we have been able to leverage the best of both of our technologies to provide a cost-effective and efficient path to bring more personalized monitoring tools to our partners, and down the line, to physicians and patients, to help inform their treatment strategy.”\n“This partnership will help accelerate personalized ctDNA monitoring as the new standard of care in oncology,” said Solomon Moshkevich, Natera’s general manager of oncology. “With Foundation Medicine’s strong track record of scientific leadership and its broad footprint within biopharma, we expect FoundationOne Tracker to become an important new tool for accelerating and improving drug development in oncology.”\nAbout Foundation Medicine\nFoundation Medicine is a molecular information company dedicated to a transformation in cancer care in which treatment is informed by a deep understanding of the genomic changes that contribute to each patient\'s unique cancer. The company offers a full suite of comprehensive genomic profiling assays to identify the molecular alterations in a patient’s cancer and match them with relevant targeted therapies, immunotherapies and clinical trials. Foundation Medicine’s molecular information platform aims to improve day-to-day care for patients by serving the needs of clinicians, academic researchers and drug developers to help advance the science of molecular medicine in cancer. For more information, please visit www.FoundationMedicine.com or follow Foundation Medicine on Twitter (@FoundationATCG).\nFoundation Medicine®and FoundationOne® are registered trademarks of Foundation Medicine, Inc.\nNatera is a pioneer and global leader in cell-free DNA testing from a simple blood draw. The mission of the company is to change the management of disease worldwide with a focus on women’s health, oncology, and organ health. Natera operates ISO 13485-certified and CAP-accredited laboratories certified under the Clinical Laboratory Improvement Amendments (CLIA) in Austin, Texas and San Carlos, California. It offers proprietary genetic testing services to inform obstetricians, transplant physicians, oncologists, and cancer researchers, including biopharmaceutical companies, and genetic laboratories through its cloud-based software platform. For more information, visit natera.com. Follow Natera on LinkedIn.\nAll statements other than statements of historical facts contained in this press release are forward-looking statements and are not a representation that Natera’s plans, estimates, or expectations will be achieved. These forward-looking statements represent Natera’s expectations as of the date of this press release, and Natera disclaims any obligation to update the forward-looking statements. These forward-looking statements are subject to known and unknown risks and uncertainties that may cause actual results to differ materially, including with respect to our efforts to develop and commercialize new product offerings, our ability to successfully increase demand for and grow revenues for our product offerings, whether the results of clinical or other studies will support the use of our product offerings, our collaborations with commercial partners such as pharmaceutical companies, medical institutions, contract laboratories, laboratory partners, and other third parties, our ability to successfully execute the partnership and co-develop the assay, our expectations of the reliability, accuracy and performance of our tests, or of the benefits of our tests and product offerings to patients, providers and payers. Additional risks and uncertainties are discussed in greater detail in ""Risk Factors"" in Natera’s recent filings on Forms 10-K and 10-Q and in other filings Natera makes with the SEC from time to time. These documents are available at www.natera.com/investors and www.sec.gov.\nSource: Foundation Medicine\nFoundation Medicine Contact:\nAbigail Alderman, 781-534-3210\nInvestor Relations: Mike Brophy, CFO, Natera, Inc., 650-249-9090\nMedia: Kate Stabrawa, Communications, Natera, Inc., 720-318-4080 email@example.com\nSource: Foundation Medicine']"	['<urn:uuid:2c339261-cf4f-4db7-88de-42978069ee32>']	factoid	with-premise	short-search-query	similar-to-document	single-doc	expert	2025-05-12T20:46:45.703947	7	17	721
20	What role does water play in shaping both the Selenga River Delta in Lake Baikal and areas of extensional tectonics like pull-apart basins?	Water plays a crucial role in both systems. In the Selenga River Delta, water and sediment transport create an intricate network of distributary channels and levees, with the river's sediment load governing delta development and forming depositional bars in a rough arc at the delta's edge. In extensional tectonic settings, water bodies often form in pull-apart basins, as seen in examples like the Dead Sea, which formed at a left-stepping offset of the Dead Sea Transform system, and the Sea of Marmara, which developed at a right-stepping offset on the North Anatolian Fault system. These water bodies form in areas where strike-slip faults create gaps through extensional processes.	"['- 1 Deformation styles\n- 2 Geological environments associated with extensional tectonics\n- 3 See also\n- 4 References\n- 5 External links\nThe types of structure and the geometries formed depend on the amount of stretching involved. Stretching is generally measured using the parameter , known as the beta factor where , is the initial crustal thickness and is the final crustal thickness. It is also the equivalent of the strain parameter stretch.\nLow beta factor\nHigh beta factor\nIn areas of high crustal stretching, individual extensional faults may become rotated to too low a dip to remain active and a new set of faults may be generated. Large displacements may juxtapose syntectonic sediments against metamorphic rocks of the mid to lower crust and such structures are called detachment faults. In some cases the detachments are folded such that the metamorphic rocks are exposed within antiformal closures and these are known as metamorphic core complexes.\nPassive margins above a weak layer develop a specific set of extensional structures. Large listric regional (i.e. dipping towards the ocean) faults are developed with rollover anticlines and related crestal collapse grabens. On some margins, such as the Niger Delta, large counter-regional faults are observed, dipping back towards the continent, forming large grabenal mini-basins with antithetic regional faults.\nGeological environments associated with extensional tectonics\nAreas of extensional tectonics are typically associated with:\nRifts are linear zones of localized crustal extension. They range in width from somewhat less than 100 km up to several hundred km, consisting of one or more normal faults and related fault blocks. In individual rift segments one polarity (i.e. dip direction) normally dominates giving a half-graben geometry. Other common geometries include metamorphic core complexes and tilted blocks. Examples of active continental rifts are the Baikal Rift Zone and the East African Rift.\nDivergent plate boundaries\nDivergent plate boundaries are zones of active extension as the crust newly formed at the mid-ocean ridge system becomes involved in the opening process.\nGravitational spreading of zones of thickened crust\nZones of thickened crust, such as those formed during continent-continent collision tend to spread laterally; this spreading occurs even when the collisional event is still in progress. After the collision has finished the zone of thickened crust generally undergoes gravitational collapse, often with the formation of very large extensional faults. Large-scale Devonian extension, for example, followed immediately after the end of the Caledonian orogeny particularly in East Greenland and western Norway.\nReleasing bends along strike-slip faults\nWhen a strike-slip fault is offset along strike such as to create a gap i.e. a left-stepping bend on a sinistral fault, a zone of extension or transtension is generated. Such bends are known as releasing bends or extensional stepovers and often form pull-apart basins or rhombochasms. Examples of active pull-apart basins include the Dead Sea, formed at a left-stepping offset of the sinistral sense Dead Sea Transform system, and the Sea of Marmara, formed at a right-stepping offset on the dextral sense North Anatolian Fault system.\nA passive margin built out over a weaker layer, such as an overpressured mudstone or salt, tends to spread laterally under its own weight. The inboard part of the sedimentary prism is affected by extensional faulting, balanced by outboard shortening.\n- Park, R.G. 1997. Foundations of Structural Geology. 3rd edition, Routledge, 216pp.\n- Kearey, P., Klepeis, K.A. & Vine, F.J. 2008. Global Tectonics, Chapter 7, Continental rifts and rifted margins, WileyBlackwell, 496pp.\n- Proffett, J.M. 1977. Cenozoic geology of the Yerington district, Nevada, and implications for the nature of Basin and Range faulting. Bull. geol. Soc. Am. 88, 247-66.\n- Tuttle, M.L.W., Charpentier, R.R. & Brownfield, M.E. 2002. The Niger Delta Petroleum System: Niger Delta Province, Nigeria, Cameroon, and Equatorial Guinea, Africa. USGS Open-File Report 99-50-H.\n- Ebinger, C.J., Jackson, J.A., Foster, A.N. & Hayward, N.J. 1999. Extensional basin geometry and the elastic lithosphere. Philosophical Transactions of the Royal Society, London, A, 357, 741-765.\n- Tapponier, P. Mercier, J.L., Armijo, R., Tonglin, H, & Ji, Z. 1981. Field evidence for active normal faulting in Tibet. Nature, 294, 410-414.\n- Dunlap, J.W. & Fossen, H. 1998: Early Paleozoic orogenic collapse, tectonic stability, and late Paleozoic continental rifting revealed through thermochronology of K-feldspars, southern Norway. Tectonics 17, 604-620.\n- Hartz, E.H, Andresen, A., Hodges K.V. & Martin, M.W., 2000, The Fjord Region Detachment Zone: A long-lived extensional fault in the East Greenland Caledonides, J. Geol. Soc. London, 158, 795-810.\n- Armijo, R., Meyer, B., Navarro, S., King, G. & Barka, A. 2002. Asymmetric slip partitioning in the Sea of Marmara pull-apart: a clue to propagation processes of the North Anatolian Fault? Terra Nova, 14, 80–86.', '|Home >>||Advanced Search >>|\nThe Gateway to Astronaut Photography of Earth\n(NASA Crew Earth Observations)\nAstronaut Photography of Earth - Display Record\nLow-resolution Browse Image(Most browse images are not color adjusted.)\nImagesConditions for Use of Images >>\nImage Transformation Tutorial >> Saving, Color Adjusting, and Printing Images >>\nImages to View on Your Computer Now\nRequest the original image file.\nDownload a Keyhole Markup Language (KML) file for use in Google Earth.\nElectronic Image DataCamera Files >> No sound file available.\nIdentificationMission: ISS029 Roll: E Frame: 37915 Mission ID on the Film or image: ISS029\nCountry or Geographic Name: RUSSIAN FEDERATION\nFeatures: SELENGINSK, LAKE BAIKAL, SELENGE RIVER DELTA, AGRICULTURAL FIELDS\nCenter Point Latitude: 52.2 Center Point Longitude: 106.6 (Negative numbers indicate south for latitude and west for longitude)\nStereo: (Yes indicates there is an adjacent picture of the same area)\nONC Map ID: JNC Map ID:\nCameraCamera Tilt: 38\nCamera Focal Length: 125mm\nCamera: N2: Nikon D2Xs\nFilm: 4288E : 4288 x 2848 pixel CMOS sensor, RGBG imager color filter.\nPercentage of Cloud Cover: 10 (0-10)\nNadirGMT Date: 20111103 (YYYYMMDD) GMT Time: 073442 (HHMMSS)\nNadir Point Latitude: 51.3, Longitude: 104.1 (Negative numbers indicate south for latitude and west for longitude)\nNadir to Photo Center Direction: East\nSun Azimuth: 222 (Clockwise angle in degrees from north to the sun measured at the nadir point)\nSpacecraft Altitude: 203 nautical miles (376 km)\nSun Elevation Angle: 14 (Angle in degrees between the horizon and the sun, measured at the nadir point)\nCaptionsSnowfall on the Selenga River Delta, Russian Federation\nThis astronaut photograph from the International Space Station illustrates the Selenga River Delta built out into Lake Baikal in Russia.\nThe Selenga River delta (image center) is lobate in form, with an intricate network of distributary channels and levees surrounded by marshlands building out into Lake Baikal. This suggests that development of the delta is governed by the sediment load carried by the river, and any modifications of form due to lake tides or waves are relatively minor. Further out, dark brown depositional bars are visible forming a rough arc marking the edge of the delta. Snow cover on the river floodplain highlights numerous secondary channels, as well as channels previously occupied by the river but now abandoned. The regular outlines of agricultural fields to the southwest and northeast of the river are also highlighted by the snow cover.\nLake Baikal is a World Heritage Site. The Selenga River is the major contributor of water to Lake Baikal; it occupies approximately 82% of the watershed area for the lake. The wetlands of the Selenga River delta are designated as a RAMSAR site and provide valuable habitat for more than 170 species of birds, including many migrating species. Like Baikal, the Selenga Delta is home to unique ecosystems, including more than 70 rare or endangered species of plants and animals.\nWaters of the Selenga River serve many (and differing) uses in both Mongolia and Russia, including support of agriculture, provision of drinking water, light industry, mining, recreation, and tourism. These uses also contribute to degradation of the river water quality, downstream availability of water, and ecological impacts. For example, a pulp and paper plant in the city of Selenginsk (image lower left) has been tied to high levels of pollution in the river. International efforts to integrate management of the Selenga River basin for both ecological and economic sustainability are ongoing.\nDownload Packaged File.\nThis option downloads the following items, packaged into a single file, if they are available:\nThis service is provided by the International Space Station program and the JSC Astromaterials Research & Exploration Science Directorate.\nRecommended Citation: Image Science and Analysis Laboratory, NASA-Johnson Space Center. ""The Gateway to Astronaut Photography of Earth."" .']"	['<urn:uuid:46181dc4-4d77-4d4d-95dc-0bcbae582328>', '<urn:uuid:58a45bf9-b544-43eb-a5d0-86671631c3d8>']	open-ended	direct	verbose-and-natural	similar-to-document	comparison	novice	2025-05-12T20:46:45.703947	23	108	1393
21	Which is more common, Twin-Twin Transfusion Syndrome or TRAP sequence in twin pregnancies?	Twin-Twin Transfusion Syndrome (TTTS) is more common, occurring in about 15% of monochorionic pregnancies, while TRAP sequence only occurs in approximately 1% of monochorionic twin pregnancies.	['Birth defects have a serious, adverse effect on health, development or functional ability and account for more than one in every five infant deaths. Babies with birth defects have a greater chance of illness and long-term disability than babies without birth defects. Many defects can be diagnosed and, in some cases, treated before birth.\nBirth Defects Fast Facts\nThere are thousands of different birth defects. Below are fast facts on a few of the most commonly treated birth defects at the Center for Fetal Diagnosis and Treatment.\nCongenital Cystic Adenomatoid Malformation (CCAM) — A cystic or solid lung tumor. The incidence of CCAM is approximately one in 10,000 pregnancies.\nCongenital Diaphragmatic Hernia (CDH) — A condition in which a hole in the diaphragm allows abdominal organs to move into the chest and restrict lung development. It occurs in about one in every 2,500 live births.\nCongenital Heart Defects (CHD) — Abnormalities in the heart’s structure. The most common birth defects, affecting approximately 40,000 births in the U.S. each year. Approximately one in every 120 babies is born each year with some type of congenital heart defect.\nConjoined Twins — Two babies born physically connected to each other. The condition occurs once in every 50,000 to 60,000 births.\nGastroschisis — A condition in which the intestine and other abdominal organs protrude through a hole in the abdominal wall and spill out into the amniotic fluid around the fetus. It occurs in approximately one in 5,000 live births.\nLower Urinary Tract Obstructions (LUTO) — Conditions in which there is a partial or complete obstruction of the urethra which restricts or prevents the passage of urine. LUTO occurs in one in 5,000 to 7,000 births, most commonly in male fetuses.\nOmphalocele — A defect in which the abdominal organs protrude into the base of the umbilical cord. Occurs in approximately one in every 5,000 to 10,000 births.\nSacrococcygeal Teratoma (SCT) — A tumor located at the base of the tailbone (coccyx) of the newborn. The most common tumor of the newborn, SCT occurs in one of every 35,000 to 40,000 live births, more commonly in females.\nSpina Bifida — A birth defect in which an area of the spinal column doesn’t form properly, leaving a section of the spinal cord and spinal nerves exposed through an opening in the back. It occurs in 3.4 per 10,000 live births in the United States and is the most common central nervous system birth defect. Between 1,400 and 1,500 babies are born with spina bifida in the U.S. each year.\nTwin Reversed Arterial Perfusion Sequence (TRAP) — A condition in which one twin receives all of its blood supply from the normal or so-called pump twin. It occurs in approximately 1 percent of monochorionic twin pregnancies, with an incidence of one in 35,000 births.\nTwin-Twin Transfusion Syndrome (TTTS) — A condition in which the blood passes unequally between identical twins that share a placenta (monochorionic). It occurs in about 15 percent of monochorionic pregnancies.\nSources: March of Dimes, Centers for Disease Control and Prevention\nCenter for Fetal Diagnosis and Treatment Fast Facts\n- The largest, most comprehensive fetal diagnosis and treatment center in the world.\n- The only fetal center offering comprehensive care in one location, from prenatal diagnosis through delivery and postnatal care.\n- One of only a few centers in the world to offer open fetal surgery for life-threatening conditions.\n- Since 1995, our Center has cared for more than 23,989 expectant parents from all 50 states and more than 70 countries.\n- Led by a multidisciplinary team of pioneers in fetal diagnosis, fetal surgery and fetal medicine who constantly work to refine and advance the techniques used to treat fetal conditions.\n- Our specialized Fetal Heart Program diagnoses fetal heart defects early in pregnancy and coordinates care. After birth, babies are treated in the Hospital’s Cardiac Center, one of the world’s top pediatric heart programs.\n- Home to the Garbose Family Special Delivery Unit (SDU), the world’s first birth unit for women carrying babies with diagnosed birth defects, keeping mother and child in the same space after birth. More than 4,052 babies with the full range of fetal diagnoses have been delivered in the SDU since it opened in 2008.\n- Babies who require care after birth receive advanced care in CHOP’s Harriet and Ronald Lassin Newborn/Infant Intensive Care Unit, consistently ranked among the best in the nation for newborn care.', 'A single placenta normally supports a single fetus. When the situation arises in which two fetuses have to share a single placenta, complications may sometimes develop. Identical twins that share a single placenta are called monochorionic twins (MC). “Chorion” is the Latin root that refers to the placenta, while the word “amnion” refers to the sac, or “membranes” that surround each fetus. While fraternal twins (2 eggs and 2 sperm) are always surrounded in their own sacs and have their own individual placentas, 70% of identical twins may end up sharing a single placenta. Only 1% of identical twins share both a single placenta and a single sac, and this poses significant risk.\nWhen two fetuses share one placenta, their umbilical cords may implant anywhere – there is no set or predictable pattern – and depending on where they implant, one fetus may get less of a ‘share’ of the placenta than it’s co-twin, resulting in less blood flow and nutrition to one fetus, with more to the other (unequal placental sharing). As a result, although identical twins usually share the same genetic material, they may actually grow differently. Like the roots of a tree, the blood vessels that run from each implanted cord may connect with each other beneath the surface, as there is nothing separating them within a single placenta. Depending on which types of vessels connect to which, one fetus may transfuse blood to the other. We will discuss each of these complications, their risks, and potential treatments, below.\nView our Monochorionic Twin Recommendations.\nWhat is Twin to Twin Transfusion Syndrome (TTTS)?\nBecause there is no barrier separating the two fetuses from each other, there are almost always blood vessel connections in the placenta shared by two fetuses in MC pregnancies. As a result of these connections, in about 10-15% of monochorionic twins (sharing one placenta) an imbalance in the circulations of the fetuses can develop. In these instances, there may be significant transfer of blood from one twin (the so-called “donor”) to the other twin (the so-called “recipient”), resulting in twin-to-twin transfusion syndrome (TTTS).\nLearn more about Twin to Twin Transfusion Syndrome (TTTS)\nWhat is unequal placental sharing?\nAlthough Twin-Twin Transfusion Syndrom (TTTS) is one of the most common diagnoses made in MC twins, not all MC twin pregnancies in which there are unequal fluid levels or a difference in size of the twins have TTTS. These findings can also be seen in a condition called “unequal placental sharing”. When two fetuses share one placenta, their umbilical cords may implant anywhere – there is no set or predictable pattern – and depending on where they implant, one fetus may get less of a ‘share’ of the placenta than it’s co-twin, resulting in less blood flow and nutrition to one fetus, with more to the other; aka, unequal placental sharings.\nLearn more about unequal placental sharing\nWhat is an acardiac twin or the TRAP sequence?\nTwin reversed arterial perfusion (TRAP) sequence or acardiac twinning is a very rare problem, occurring in approximately 1% of monochorionic twin (twins sharing one placenta). One twin is usually structurally completely normal. The other is an abnormal mass of tissue, consisting usually of legs and a lower body, but no upper body, head or heart. Because of the absent heart, the term “acardiac twin” was been used to describe this mass. The normal fetus is referred to as the “pump twin” because its heart is used to pump blood to the abnormal mass. The “acardiac twin” has no chance of survival.\nDue to the absence of a beating heart, the acardiac does not send blood to any portion of the placenta, and all of its blood supply comes from and goes back to the circulation of the pump twin through unique vascular connections on the surface of the shared placenta. Arteries usually carry blood away from the fetus and toward the placenta to receive oxygen from the mother’s circulation. When there is an “acardiac twin”, the unique vascular connections allow blood in the artery to flow in the reversed direction (toward the acardiac fetus rather than away from it). Thus, the phrase “twin reversed arterial perfusion” (TRAP) sequence has been used to describe this condition.\nLearn more about Acardic Twin or TRAP sequence\nSupport Groups & Other Resources\n- San Francisco Parents of Multiples Club — a network of parents and expectant parents of multiples that provides its members and their families with friendship, support and information on parenting twins, triplets and quads.\n- The Fetal Hope Foundation — provides support and information to parents, funds research, and increases awareness about TTTS, TRAP Sequence, Amniotic Band Syndrome, and Urinary Tract Obstruction.\n- TTTS Foundation — solely dedicated to providing immediate and lifesaving educational, emotional and financial support to families, medical professionals, and other caregivers before, during, and after a diagnosis of twin to twin transfusion syndrome.\n- Multiple Births — offers prenatal education and bereavement support information for parents with twins.\n- March of Dimes — Researchers, volunteers, educators, outreach workers and advocates working together to give all babies a fighting chance\n- Birth Defect Research for Children — a parent networking service that connects families who have children with the same birth defects\n- Kids Health — doctor-approved health information about children from before birth through adolescence\n- CDC - Birth Defects — Dept. of Health & Human Services, Centers for Disease Control and Prevention\n- NIH - Office of Rare Diseases — National Inst. of Health - Office of Rare Diseases\n- North American Fetal Therapy Network — NAFTNet (the North American Fetal Therapy Network) is a voluntary association of medical centers in the United States and Canada with established expertise in fetal surgery and other forms of multidisciplinary care for complex disorders of the fetus.']	['<urn:uuid:b1688ed0-d619-47d9-ac62-d9f9b2325cd4>', '<urn:uuid:011f6830-7f5d-4266-ae4d-d6bcfdaeeec8>']	factoid	with-premise	concise-and-natural	similar-to-document	comparison	novice	2025-05-12T20:46:45.703947	13	26	1704
22	How do Python scripts work when importing them, and what are the technical requirements for hosting them online?	Python scripts execute all statements from top to bottom when imported, just like when running them directly. To make scripts reusable without executing everything during import, you should wrap the main actions in functions and use the if __name__ == '__main__' check. As for hosting, since continuously running programs like websites need constant updates and live inputs, they typically require servers rather than personal computers. This is because personal computers aren't designed for constant operation and can overheat. Instead, dedicated servers are used, which can be accessed through services like Github and Heroku that allow you to host your application by uploading your code to their servers.	"['It’s common to create small scripts which we want to combine into a larger script. We don’t want to copy and paste the code. We want to leave the working code in one file and use it in multiple places. Often we want to combine elements from multiple files to create more sophisticated scripts.\nThe problem we have is that when we import a script it actually starts running. This is generally not what we expect when we import a script so that we can reuse it. How can we import the functions (or classes) from a file without having the script start doing something?\nLet’s say that we have a handy implementation of the haversine distance function called\nhaversine(), and it’s in a file named\nInitially, the file might look like this:\nimport csv import pathlib from math import radians, sin, cos, sqrt, asin from functools import partial MI= 3959 NM= 3440 KM= 6373 def haversine( lat_1: float, lon_1: float, lat_2: float, lon_2: float, *, R: float ) -> float: ... and more ... nm_haversine = partial(haversine, R=NM) source_path = pathlib.Path(""waypoints.csv"") with source_path.open() as source_file: reader= csv.DictReader(source_file) start = next(reader) for point in reader: d = nm_haversine( float(start[\'lat\']), float(start[\'lon\']), float(point[\'lat\']), float(point[\'lon\']) ) print(start, point, d) start= point\nWe’ve omitted the body of the\nhaversine() function, showing only … and more…, since the full source is available on github. We’ve focused on the context in which the function is in a Python script that also opens a file,\nwapypoints.csv, and does some processing on that file.\nHow can we import this module without it printing a display of distances between waypoints in our\nPython scripts can be simple to write. Indeed, it’s often too simple to create a working script. Here’s how we transform a simple script into a reusable library:\n1) Identify the statements that do the work of the script: we’ll distinguish between definition and action. Statements such as\nclass are clearly definitional—they support the work but they don’t do the work. Almost all other statements take action. In our example, we have four assignment statements that are more definition than action. The distinction is entirely one of intent. All statements, by definition, take an action. These actions, though, are more like the action of the def statement than they are like the action of the\nwith statement later in the script. Here are the generally definitional statements:\nMI= 3959 NM= 3440 KM= 6373 def haversine( lat_1: float, lon_1: float, lat_2: float, lon_2: float, *, R: float ) -> float: ... and more ... nm_haversine = partial(haversine, R=NM)\nThe rest of the statements clearly take an action toward producing the printed results.\n2) Wrap the actions into a function:\ndef analyze(): source_path = pathlib.Path(""waypoints.csv"") with source_path.open() as source_file: reader= csv.DictReader(source_file) start = next(reader) for point in reader: d = nm_haversine( float(start[\'lat\']), float(start[\'lon\']), float(point[\'lat\']), float(point[\'lon\']) ) print(start, point, d) start= point\n3) Where possible, extract literals and make them into parameters. This is often a simple movement of the literal to a parameter with a default value. From this:\ndef analyze(): source_path = pathlib.Path(""waypoints.csv"")\ndef analyze(source_name=""waypoints.csv""): source_path = pathlib.Path(source_name)\n4) Include the following as the only high-level action statements in the script file:\nif __name__ == ""__main__"": analyze()\nWe’ve packaged the action of the script as a function. The top-level action script is now wrapped in an if statement so that it isn’t executed during import.\nThe most important rule for Python is that an\nimport of a module is essentially the same as running the module as a script. The statements in the file are executed in order from top to bottom.\nWhen we import a file, we’re generally interested in executing the\nclass statements. We might be interested in some assignment statements.\nWhen Python runs a script, it sets a number of built-in special variables. One of these is\n__name__. This variable has two different values, depending on the context in which the file is being executed:\nThe top-level script, executed from the command line: In this case, the value of the built-in special name of\n__name__ is set to\nA file being executed because of an import statement: In this case, the value of\n__name__ is the name of the module being created.\nThe standard name of\n__main__ may seem a little odd at first. Why not use the filename in all cases? This special name is assigned because a Python script can be read from one of many sources. It can be a file. Python can also be read from the\nstdin pipeline, or it can be provided on the Python command line using the\nWhen a file is being imported, however, the value of\n__name__ is set to the name of the module. It will not be\n__main__. In our example, the value\n__name__ during import processing will be\nWe can now build useful work around a reusable library. We might make several files that look like this:\nfrom script import analyze analyze(\'trip_1.csv\')\nOr perhaps something even more complex:\nfrom script import analyze for trip in \'trip_1.csv\', \'trip_2.csv\': analyze(trip)\nThe goal is to decompose a practical solution into two collections of features:\nTo get to this goal, we’ll often start with a script that conflates both sets of features. This kind of script can be viewed as a spike solution. Our spike solution should evolve towards a more refined solution as soon as we’re sure that it works.\nA spike or piton is a piece of removable mountain-climbing gear that doesn’t get us any higher on the route, but it enables us to climb safely.', 'This post is part of the series A Complete Beginner’s Guide To Hacking\nCompilers are “computer programs (or set of programs) that transforms source code written in a programming language into another computer language”. You learned earlier that that there are different levels of languages and ultimately the hardware interacts with the binary code – well the compiler is basically the software that translates your code that people can easily read and edit to the code the computer can read and run. Not all languages need a compiler, and sometimes you will sometimes find that you actually already have the compiler installed on your computer and so it will run without you even noticing. Check what you need for the language you are working in!\nSome programs have ‘fixed’ finite running time, for example a scientific Python script that has a fixed input will run until it has calculated the output and then it will finish. For these you can generally compile and then execute (start) the program and wait for the static output.\nOn the other hand some programs will need to be running continouously such as a website that needs to be constantly updating with live inputs. If you are running a continuous program like this it is called hosting and the live inputs and outputs will have to be associated with ports.\nPorts and Addresses\nPorts are not physical hardware objects, but they are the name for what exists in software within operating systems to let things in and out. All ports can be identified by a unique 16-bit number or port number which contains two pieces of information – the IP address and the protocol type.\nEvery device that can connect to the internet has a unique address or identity number known as it’s Internet Protocol or IP address that gives information about it. You can find out your computer’s really easily by googling “define: IP address” and Google will tell you. Port numbers obviously include this because it uniquely identifies which device the port is being hosted on.\nThe other piece of information is the protocol type, which tells you what exactly it is you are passing in and out of a port – there are a range of numbers, the most commonly used from 0 to 1023, that are allocated to mean different things. For example, 80 means the HyperText Transfer Protocol (HTTP) – i.e. something for the World Wide Web, and 20 is for the File Transfer Protocol to sends files from one computer to another.\nMost of the time you don’t see the port numbers involved with processes as instead the internet tends to use Unique Resource Locators (URLs) which identify all the numbers involved and retrieve them for you so as to make it more secure and easier to use. The URLs you are probably familiar with are website addresses – http://www.example.com for example.\nIf you want to run a program on your computer that only you can see the outputs of (for example when testing a prototype website) then you can do what’s known as local hosting. This is when you you use the special IP address 127.0.0.1 (or ::1 these days) and the protocol type for a local port number (often 8080) which will output to a place only visible to your computer.\nIf you want to run a program with live inputs and outputs then can you tell your program to listen or output at the specific port(s) needed either by specifying the port number in your code. For example, if you are listening for inputs from a connected sensor device then you could specify its relevant port number, or you could use URLs on the web to get information from a website like Twitter or BBC News (see web APIs later for the easiest way of doing this).\nYou will generally find that your personal computer isn’t the best way to run an internet connected application. This is because it has probably not been designed for that purpose. As long as you are running your program you will be limiting the computer’s capacity to do other things at the same time, slowing it all down. And of course you have to leave your computer running all the time you want the program running and most ordinary computers overheat after a while. And there are several security issues involved with connections to the internet.\nTherefore, most of the time people use computers that have been designed to just doing the task of hosting alone and these (and the software they run to allow them to do just this) are called servers.\nServers are often stacked up together in a rack so they can add up to more room for running more applications. There are of course services that allow you to pay for time and space on other people’s dedicated server racks, and there are some that let you use them for free too! Github and Heroku are both good examples of services that provide an easy way for you to host your application on the the world wide web for free by uploading your code to be hosted on their servers.\nSo now you can actually run your code!\nA Note On Debugging\nSo when you finally get your code up and running you will need to be aware of debugging. This is the very general term for identifying and fixing the problems in your code. You will probably have been doing this whilst writing it – looking for typos for example, but most will happen after you try and run your program and discover that it doesn’t work properly. There are loads of tools out there designed to try and help you dofigure out what’s wrong. Your text editor and/or shell will probably try and suggest what and where it thinks something has gone wrong but be wary as it is often not correct about this.\nBugs can be classified into two groups – those that stop the code from actually running, often invalid syntax (these bugs are what the automated debuggers are looking for), and bugs that let the code run but it just doesn’t do what it’s supposed to. Most of the time you will be looking for a combination of these.\nNEXT: Working Together']"	['<urn:uuid:2c625789-d629-47b3-9173-b74161f8f64e>', '<urn:uuid:dc062c2b-06e6-48d9-b053-abdbe92b23a0>']	open-ended	direct	verbose-and-natural	similar-to-document	multi-aspect	novice	2025-05-12T20:46:45.703947	18	107	1982
23	How do academic research and company CRM compare in using statistics?	Academic research (particularly in economics) emphasizes rigorous statistical validation and is concerned about spurious correlations from extensive data mining. They require testing hypotheses on new, untainted datasets. In contrast, Customer Relationship Management (CRM) in companies focuses on practical applications - using data mining to identify customer patterns, predict behaviors, and manage customer interactions through all relationship stages (initiation, maintenance and termination). CRM's main goal is building long-term profitable relationships rather than statistical validity.	['‘Data mining’ is an interesting term. It’s used very positively in some academic circles, such as departments of marketing, and very negatively in others, most notably departments of economics. The term refers to the use of clever automatedsearch techniques to discover putatively significant relationships in large data sets.\nThe paradigm example, though a very old-fashioned one is ‘stepwise regression’. You take a variable of interest then set up a multivariate regression. The computer then tries out all the other variables in the data set one at a time. If the variable comes up significant, it stays in, otherwise it’s dropped. In the end you have what is, arguably, the best possible regression.\nEconomists were early and enthusiastic users of stepwise regression, but they rapidly became disillusioned. To see the problem, consider the simpler case of testing correlations. Suppose, in a given dataset you find that consumption of restaurant meals is positively correlated with education. This correlation might have arisen by chance or it might reflect a real causal relationship of some kind (not necessarily a direct or obvious one). The standard statistical test involves determining how likely it is that you would have seen the observed correlation if there was in fact no relationship. If this probability is lower than, say, 5 per cent, you say that the relationship is statistically significant.\nNow suppose you have a data set with 10 variables. That makes 45 (=10*9/2) distinct pairs you can test. Just by chance you’d expect two or three correlations that appear statistically significant correlations. So if your only goal is to find a significant relationship that you can turn into a publication, this strategy works wonders.\nBut perhaps you have views about the ‘right’ sign of the correlation, perhaps based on some economic theory or political viewpoint. On average, half of all random correlations will have the ‘wrong’ sign, but you can at expect to find at least one ‘right-signed’ and statistically significant correlation in a set of 10 variables. So, if data mining is extensive enough, the usual statistical checks on spurious results become worthless.\nIn principle, there is a simple solution to this problem, reflecting Popper’s distinction between the context of discovery and the context of justification. There’s nothing wrong with using data mining as a method of discovery, to suggest testable hypotheses. Once you have a testable hypothesis, you can discard the data set you started with and test the hypothesis on new data untainted by the process of ‘pretesting’ that you applied to the original data set.\nUnfortunately, at least for economists, it’s not that simple. Data is scarce and expensive. Moreover, no-one gets their specification right first time, as the simple testing model would require. Inevitably, therefore, there has to be some exploration (mining) of the data before hypotheses are tested. As a result, statistical tests of significance never mean precisely what they are supposed to.\nIn practice, there’s not much that can be done except to rely on the honesty of investigators in reporting the procedures they went through before settling on the model they estimate. If the results are interesting enough, someone will find another data set to check or will wait for new data to allow ‘out of sample’ testing. Some models survive this stringent testing, but many do not.\nI don’t know how the marketing guys solve this problem. Perhaps their budgets are so large that they can discard used data sets like disposable syringes, never infecting their analysis with the virus of pretesting. Or perhaps they don’t know or don’t care.\nUpdate Kevin Drum at CalPundit gives the perspective of a marketing guy, with lots of interesting points (for example, loyalty programs are there to collect data for mining). He doesn’t accept my main point and raises the dreaded “B” word – Bayesian.\nFor those familiar with debates among statisticians, this is the point at which things typically become both heated and incomprehensible (just like a lot of blogs, really). A real challenge, which I may tackle at some point, is to explain the Bayesian concept of statistical reasoning in ordinary language. For the moment, though, I’ll just agree that a debate like the one over data mining ultimately makes sense only if it’s cast in Bayesian terms, that is, with a discussion of the beliefs we hold before we begin the statistical analysis.', 'APPLICATION OF ASSOCIATION RULE LEARNING IN CUSTOMER RELATIONSHIP MANAGEMENT\nThe main purpose of this study is the application of association rule learning using data mining techniques in customer relationship management of a diagnostics centres. Clustering customers is needed to find unsatisfied need, promote services packages and create new service packages. The proposed system diagnostics data mining system (DDMS) consists of three components; pre-processing, clustering and post processing. The data collected is for a period of four month for 6700 transaction. Three data sets are constructed from the original data set by dividing the whole data into 90%, 85% and 80% for training and 10%, 15% and 20% for testing respectively. Three K-means model are used with k=10, 15 and 18 cluster and each data set is used to calibrate and test the model for a total of nine ones. It is found that the best model is the one with 15 clusters. The clustering results are represented to a health and diagnostics personnel who found that some results are reasonable and others go along with the policy guiding customer relationship management in the centers.\n1.1 DATA MINING\nData mining is the process that uses a variety of data analysis and modelling techniques to discover patterns and relationships in data that may be used to make accurate predictions (Guarav andAggraval, 2012).\nIt’s described as the process of extracting knowledge data discovery of valid, authentic and actionable information from large data bases. It is also used to derive patterns and trends that exist in the collected data ( Masheswari et al, 2014).\nData mining is a continuous iterative process that is the very core of business intelligence. It involves the use of data mining software, sound methodology and human creativity to achieve new insight through the exploration of data to uncover patterns, relationships, anomalies and dependencies (PuneetShukla, 2015).According to (PuneetShukla, 2015) the process of data mining consists of three stages which are the Initial exploration, Model building or pattern identification with validation/verification, Deployment (i.e. the application of the model to new data in order to generate predictions).\nData mining consists of five major elements which includes extracting, transform and load data onto data warehouse systems, Storing and manage data , provide data access to business analysts and information technology professionals, analyse the data by application software and present the datain a useful format such as a graph or table.\nData mining involves six common classes of tasks which are;\n1.2 Customer Relationship Management\nIt helps business to gain insight into the behaviour of customers and their value so that the company can increase their profit by acting according to the customer characteristics. Customer relationship management technology is a mediator between customer management activities in all stages of a relationship (initiation, maintenance and termination) and business performance. It consists of customer identification, customer attraction, customer retention and customer development (Dhandayudam andKrishnamurthi, 2013).Customer relationship management is a set of process which enables the business strategy to build long term and profitable relationship with the customers (Masheswari, 2014).\nCustomer relationship management refers to the methodologies and tools used to help businesses manage customer relationships in an organized way. CRM simply means managing all customer interactions which requires using information about your customers and prospects to more effectively interact with your customers in all stages of your relationship with them(Gupta and Aggraval, 2012). There are three components of CRM which are customer, relationship, and management. Four basic tasks are used to achieve the basic goals in CRM\nCustomer identification: Identify the customers through web site marketing.\nCustomer differentiation: Every customer has their own lifetime value from the company’s point of view.\nCustomer interaction: Customer demands changes every time. There are four stages of customer life cycle which are the initiation, integration, intelligence and value creation.\nCustomization: Treat the customers uniquely through the entire CRM process.\n1.3 PROBLEM STATEMENT\nCompanies and organizations should have more awareness of their type of customers. For example,how managers can have an effective sale to irritable customers. Customer relationship management (CRM) usually involves the need of IT professionals to implement the methodologies involved to carry out effective management of customers. The issue of not having a suitable commercial brand(Dr JavadKhalatbari, 2011).\nThere is a strong requirement for data integration before data mining which involves getting data from different sources and integrate them before actual data exploration can begin. Companies usually make the mistake of gaining the technology needed and then applying it to discover it is not actually solving the main problem.\nThe ability to know which category of customers to channel their effort to which are more likely to remain.\nTo develop a predictive model that will be used for more accurate predictions of customer acquisition and also retention.\n•Selection of right customers from a large set of potential customers.\n•Develop and Simulate the model.\n1.6 RESEARCH METHODOLOGY\nTo achieve the objectives stated above, the following methods would be adopted\n•Proper literature review on journals relating with this project topic.\n•Gathering necessary information and required data from related personnel concerned.\n•Using association rule technique to the gathered data to make prediction.\n1.7 SCOPE OF STUDY\nThe use of information technology allows the process of data extraction that helps in getting interesting facts to enable the effective prediction of customer behaviour.\n1.8 SIGNIFICANCE OF STUDY\nWhen this research is implemented there are foreseeable benefits which includes;\n•Enable the prediction of customer behaviour\n•To enable organizations have a proper view of the type of customers they would have and how to solve irregularities.']	['<urn:uuid:86327804-2d84-4e5b-880b-361595335a93>', '<urn:uuid:98116bdd-1485-4bfc-8eb6-20c293336422>']	open-ended	direct	concise-and-natural	distant-from-document	comparison	novice	2025-05-12T20:46:45.703947	11	73	1645
24	what were major changes done to gender related mental illnesses in 2013 psychological manual	In 2013, with the release of DSM-5, the American Psychiatric Association made a significant change by removing 'gender identity disorder' from the list of mental illnesses and replacing it with 'gender dysphoria' to describe a person's discomfort with their assigned gender.	"['Psychology Events: 19th Century\n19th century was when psychology was established as an empirical,\naccepted science. While measures would change within that 100-year\nspan, the model of research and evaluation would begin to take\n- 1878: G.\nStanley Hall becomes the first American to\nearn a Ph.D. in psychology.\n- 1879: Wilhelm\nWundt establishes the first\nexperimental psychology lab in Leipzig,\nGermany dedicated to the study of the mind.\nG. Stanley Hall opens the first experimental psychology lab in\nthe U.S. at Johns Hopkins University.\nHerman Ebbinghaus publishes his seminal ""Über das Gedächtnis""\n(""On Memory"") in which he describes learning and memory\nexperiments he conducted on himself.\n- 1886: Sigmund\nFreud begins offering therapy to patients in\n- 1888: James\nMcKeen Cattell becomes the first professor\nof psychology at the University of Pennsylvania. He would later\npublish ""Mental Tests and Measurements,"" marking the advent of\n- 1890: William\nJames publishes ""Principles of Psychology.""\nSir Francis Galton establishes correlation techniques to better\nunderstand the relationships between variables in intelligence\nG. Stanley Hall forms the American\nPsychological Association(APA), enlisting 26\nmembers in the first meeting.\nLightner Witmer establishes the first psychology clinic in\n- 1898: Edward\nThorndike develops the Law\nPsychology Events: 1900 to 1950\nfirst half of the 20th century was dominated by two major figures:\nSigmund Freud and Carl Jung. It was a time when the foundation of\nanalysis was built, including Freud\'s examination of\npsychopathology and Jung\'s analytic psychology.\nSigmund Freud publishes his landmark book, ""Interpretation\nThe British Psychological Society is established.\n- 1905: Mary\nWhiton Calkins is elected the first woman\npresident of the American Psychological Association. Alfred\nBinet introduces the intelligence\n- 1906: Ivan\nPavlov publishes his findings on classical\nJung publishes ""The Psychology of Dementia\n- 1911: Edward\nThorndike publishes ""Animal Intelligence,"" which leads to the\ndevelopment of the theory of operant\n- 1912: Max\nWertheimer publishes ""Experimental Studies\nof the Perception of Movement,"" which leads to the development\nCarl Jung begins to depart from Freudian views and develops his\nown theories, which he refers to as analytical psychology. John\nB. Watson publishes ""Psychology as the Behaviorist Views""\nin which he establishes the concept of behaviorism.\nFreud publishes work on repression.\nWatson and Rosalie Rayner publish research on classical\nconditioning of fear with their subject, Little\n- 1932: Jean\nPiaget becomes the foremost cognitive\ntheorist with the publication of his work ""The Moral Judgment of\n- 1942: Carl\nRogers develops the practice of client-centered therapy,\nwhich encourages respect and positive regard for patients.\nPsychology Events: 1950 to 2000\nlatter half of the 20th century was centered around\nthe standardization of the diagnostic criteria of mental illness,\nhallmarked by the release of the Diagnostic\nand Statistical Manual of Mental Disorders (DSM) by\nthe American Psychiatric Association. It is the foundational tool\nstill in use today to direct diagnosis and treatment.\nThe first Diagnostic and Statistical Manual of Mental Disorders\n- 1954: Abraham\nMaslow publishes ""Motivation and\nPersonality,"" describing his theory of a hierarchy\nof needs. He is one of the founders of humanistic\nHarry Harlow publishes ""The\nNature of Love,"" which describes the importance\nof attachment and love in rhesus monkeys.\n- 1961: Albert\nBandura conducts his now-famous Bobo\ndoll experimentin which\nchild behavior is\ndescribed as a construct of observation, imitation, and modeling.\nBandura first describes the concept of observational learning to\n- 1974: Stanley\nMilgram publishes ""Obedience to Authority,""\nwhich describes the findings of his famous obedience\nThe DSM-III is published.\nNoam Chomsky publishes ""On the Nature, Use, and Acquisition of\nSteven Pinker publishes an article introducing his theories as\nto how children acquire language, which he later publishes in\nthe book ""The Language Instinct.""\nThe DSM-IV is published.\nPsychology Events: 21st Century\nthe advent of genetic science, psychologists are\nnow grappling with the ways in which physiology and genetics\ncontribute to a person\'s psychological being.\nSteven Pinker publishes ""The Blank Slate,"" arguing against the\nconcept of tabula\nrasa (the theory that the mind is a blank\nslate at birth). Avshalom Caspi\noffers the first evidence that genetics are associated with a\nchild\'s response to maltreatment.3 Psychologist\nDaniel Kahneman is awarded the Nobel Memorial Prize in Economics\nfor his research on how people make judgments in the face of\nGenetic researchers finish mapping human genes, with the aim of\nisolating the individual chromosomes responsible for\nphysiological and neurological conditions.4\n- 2010: Simon LeVay publishes\n""Gay, Straight, and the Reason Why,"" which argues that sexual\norientation emerges from prenatal differentiation in the brain.\nThe DSM-5 is\nreleased. Among other changes, the APA removes ""gender identity\ndisorder"" from the list of mental illnesses and replaces it with\n""gender dysphoria"" to describe a person\'s discomfort with their\nJohn O\'Keefe, May-Britt Moser, and Edvard Moser share\nthe Nobel Prize for their discovery of cells\nthat constitute a positioning system in the brain that is key to\nmemory and navigation.\nDarren Stevens creates Constructed Development\nTheory and unites previously asynchronous theories within\npsychology by exposing our Self-Awareness. He later releases a\nbook called ""Thinking Deliberately"" to bring his idea to the']"	['<urn:uuid:5ca9bcdc-556f-4544-814c-be5aec5b276a>']	open-ended	direct	long-search-query	distant-from-document	single-doc	novice	2025-05-12T20:46:45.703947	14	41	841
25	What determines the penetration grade classification of bitumen?	Penetration grade of bitumen is determined through a penetration test that measures hardness. In this test, a standard needle under a 100-gram load penetrates into bitumen at 25°C for 5 seconds. The penetration value is measured in tenths of millimeters. The lower the degree of penetration, the harder the bitumen. Grades are designated by two penetration values, such as 40/50, 60/70, or 85/100, with the actual penetration falling between these lower and upper values.	['- Reviews (0)\npenetration Grade Bitumen is a standard bitumen usually used as a Paving Grade Bitumen essential for road construction and for the production of asphalt pavements with superior properties, and it’s very important once it bounds the aggregates and creates a unique cohesion and stability to the bituminous mix.\nThis grade of Bitumen is mainly used in the manufacture of hot mix asphalt for bases and wearing courses. Penetration Grade Bitumen by Capco trading Co. is petroleum grade bitumen, manufactured from fractional / vacuum distillation of crude oil. The Bitumen supplied by Capco trading Co. is produced from vacuum residue (short residue) feedstock.\nBitumen is usually characterized in the following three types of grades:\n- Viscosity grades\n- Penetration grades\n- Density grades\nDefinition of penetration Grade\nThe penetration grades bitumen is refinery bitumen which is produced by different viscosity. In fact, Penetration bitumen is semi hard black material known as petroleum grade bitumen. The penetration test is carried out to characterize the bitumen, based on the hardness. Therefore, it has the name penetration bitumen. Penetration Grade Bitumen is a standard bitumen usually used as a Paving Grade Bitumen essential for road construction and for the production of asphalt pavements with superior properties, and it’s very important once it bounds the aggregates and creates a unique cohesion and stability to the bituminous mix.\nActually, it is Bitumen classified using the penetration property. Penetration grading’s basic assumption is that the less viscous the asphalt, the deeper the needle will penetrate. Bitumen which is produced during the process of oxidation of vacuum bottom (the Bitumen production feedstock that derives from distillation tower residue in vacuum oil refineries) at bitumen production unit in a manner that its penetration point (kind of test to indicate the hardness of bitumen) in specified group is classified in different grade of it.\nThis grade of Bitumen is mainly used in the manufacture of hot mix asphalt for bases and wearing courses.\nThe penetration of bituminous material is its consistency expressed as the distance in tenths of a millimeter that a standard needle penetrates vertically into a specimen of the material under specified conditions of temperature, load and duration of loading.\nGrades of straight-run bitumen are designated by two penetration values, for example, 40/50, 60/70, 85/100 etc.; the Penetration of an actual sample of the bitumen in any grade should fall between the lower and upper value given.\nPenetration Grades bitumen are specified by the penetration and softening point test. Designation is by penetration range only. The penetration grades of bitumen have a thermoplastic property which causes the material to soften at high temperatures and to harden at lower temperatures. This unique temperature/viscosity relationship is important when determining the performance parameters such as the adhesion, rheology, durability and application temperatures of bitumen. Capco trading Co supplies all Penetration Graded Bitumen Grades in correspondence to ASTM D946-09 and EN 12591-2009, including:\nASTM D946-09 Grades:\nBitumen Penetration Grade 40-50\nBitumen Penetration Grade 60-70\nBitumen Penetration Grade 85-100\nBitumen Penetration Grade 100-120\nEN 12591-2009 Grades:\nBitumen Penetration Grade 20/30\nBitumen Penetration Grade 30/45\nBitumen Penetration Grade 35/50\nBitumen Penetration Grade 40/60\nBitumen Penetration Grade 50/70\nBitumen Penetration Grade 70/100\nBitumen Penetration Grade 100/150\nBitumen Penetration Grade 160/220\nPetroleum Bitumen Specifications\n1) Penetration degree: Penetration test is used to determine bitumen hardness. In this test, a standard needle under the influence of a 100-gram load penetrates into bitumen at 25 ° C for 5 seconds. The penetration value in terms of the tenth of millimeters is called penetration degree. The lower the degree of penetration is, the harder the bitumen will be.\n2) Viscosity: The higher the bitumen viscosity is, the higher solid properties it shows. Clearly, at lower temperatures, the viscosity is smaller. This property of bitumen characteristic is measured using the Saybolt Furol machine or by the kinematic method.\n3) Flash point: The Flash point is the temperature that, if the bitumen reaches that temperature, the resulting gases will ignite when the flame approaches, and a flame occurs at its surface. The maximum temperature at which the bitumen can be heated in the workshop is limited to the flash point.\n4) Weight loss on heating: The weight loss of bitumen at high temperature is due to the evaporation of a part of its oil and its petroleum compounds. This characteristic is also of significant properties of bitumen. The weight loss of bitumen is measured at the temperature of 163° C and within 5 hours (approximate conditions for cooking asphalt).\n5) Ductility: If we pull a sample of bitumen with a cross-section of 1 cm 2 at a speed of 5 cm / min, we increase the length of the sample before tearing the ductility property of bitumen.\n6) Solubility in CS2: We know the solvent of bitumen is carbon tetra-chloride and carbon-sulfur. So, if we dissolve a sample of bitumen in each of these materials, its impurities remain, and we can determine the purity of bitumen. The degree of purity is: (sample bitumen weight) ÷ [(impurity weight) – (bitumen weight)]\n7) Softening point: The softening point is the temperature when the bitumen reaches that temperature, the bitumen becomes fluid. The higher the bitumen softening point, the lower the sensitivity to temperature variations. The softening point of ordinary bitumen is about 60 to 70.\nBitumen Application and Uses\nArteries Made with Asphalt\nIt is no surprise that most of our roads and streets are made of asphalt. Asphalt roads are more flexible to build and easier to maintain than roads using any other material. They are more cost-effective in both construction and maintenance and help reduce noise pollution. Asphalt roads are durable and are 100% recyclable. And asphalt is not just used in road surfaces: it is also a vital component of the substructure of the road, as well. And, of course, without bitumen, there would be no asphalt.\nOne recent advance has been the development of colored or textured asphalt surfaces. These can be used as a safety mechanism to make it easier for drivers to identify special lanes including bus lanes, bus stops, and cycle paths. Colored asphalt is also used to alert drivers to dangerous areas, such as hidden junctions or sharp bends, as well as areas requiring special safety precautions, such as outside schools.\nAsphalt surfaces can be tailor-made to support the traffic load and climatic conditions of any location. There are asphalt solutions for motorways, urban, suburban and rural roads, and different solutions for high-load and bridge surfaces. In areas where roads are exposed to winter freezing and thawing, asphalt surfaces can be built to cope with extreme temperature differences, thanks to the flexibility of bitumen.\nAsphalt roads can be constructed very quickly, and because asphalt effectively needs no “cure” time, motorists can use a new road soon after placement. Because of the way asphalt roads are built, they can be easily removed and resurfaced almost immediately. This is very important in urban environments where cables and pipes need to be laid or replaced under the street, as well as for general road maintenance, filling in potholes and even more so for our major trunk roads. Safe and smooth roads, with minimum delays.\nModern asphalts roads have shown to last for over 40 years and, with the correct maintenance, they can and do last even longer. Each road is built with a structurally solid base course and a protective, replaceable surface or wearing course. The layered structure is built to withstand traffic and the environment.\nModern asphalt technology ensures rapid drainage of surface water, reducing water spray and improving driver visibility in wet conditions. This reduces the risk of aquaplaning and increases the visibility of road markings. The surface texture ensures maximum grip and delivers optimal turning and braking performance, in all weather conditions. Asphalt surfaces that provide higher levels of skid resistance can be used where safety is paramount, for example on slip roads, junctions, and roundabouts.\nAsphalt road surfaces can significantly reduce noise both inside and outside the car. Standard asphalt roads have the lowest noise levels of all traditional road surfaces and the most recent development of porous and silent asphalts have reduced noise levels even further. A standard asphalt surface produces half the noise compared to alternatives options such as concrete, and porous asphalt reduces this by a further 50%.\nAsphalt is 100% recyclable and reusable. Recovered asphalt is routinely re-laid along with fresh materials, saving money and preserving non-renewable natural resources. Recycling also reduces the use of virgin quality gravel, reduces landfill and saves both cost and emissions for transport.\nShaping the World around us\nThe characteristics of bitumen give architects, structural engineers, and planners the freedom to redesign the world we live in. Bitumen membranes are water-repellent and flexible, making them ideal for waterproofing roofs and other structural elements. A natural adhesive, they can be applied to any number of substrate materials, and their natural flexibility helps to prevent rips and tears even with significant structural movement.\nBitumen membranes retain their adhesion and flexibility for a long time, and so can significantly contribute to a structure’s durability and longevity. Bituminous membranes have been used for decades and results show that a service life of 25-30 years can be expected in most applications.\nBitumen products are everywhere in modern buildings, in roofs and floors, ceilings and walls, both to protect the structural elements and to keep moisture out. Traditionally used to weatherproof low slope roofs, bituminous membranes are finding new uses in the movement towards carbon-neutral buildings. These membranes can be used to support roof gardens or eco-roofing, in both commercial and residential developments.\nThe correct surface is of vital importance in civil engineering projects: from bridges and tunnels to runways and parking aprons at airports. The surface needs to be strong enough to withstand each location’s different daily load and use patterns, as well as temperature differences, weather conditions and the use of aggressive de-icing technology. Bitumen products are designed to withstand the most extreme conditions and provide a safe, durable surface\nAsphalt concrete with a void content of ≤ 3% is completely watertight and can be used to seal structures, basins or containers that will be used to contain water or other liquids. Asphalt can be used to waterproof dams, reservoirs, landfill sites and flood protection basins.\nPart of our Everyday Life\nThis organic, versatile and recyclable material is never far away in the modern world – even if we don’t see it. Bitumen plays an important role in many everyday applications above and beyond asphalt streets and roofing. Bitumen’s waterproofing and adhesive properties, durability, and resistance to heavy loads make it the ideal material for use in all-weather environments. Where strength and weatherproofing are essential requirements, bitumen is a prime contributor to the performance of a vast range of products. Bitumen membranes are also extensively used as sound-deadening panels in the automobile market.\nFarms use bituminous paints and disinfectants on a number of different surfaces as well as bitumen waterproofing solutions. Bitumen-based lubricants play a role in many industrial situations, and bitumen is a vital component in a range of products, from preservatives to plastics and sealants.\nModern farms rely on bitumen and products using refined bitumen for a number of vital applications. Bitumen paving provides flooring in barns, barnyards, and feed platforms while tanks, vats, and other concrete structures can be waterproofed with a bitumen solution.\nBitumen is everywhere – even in places where you wouldn’t expect to find it. It’s in a variety of grease and lubricant products for different applications, from the tracks of earth-moving equipment to traction motor gearboxes. It’s in corrosion-resistant paints and printing inks.\nBitumen is in the asphalt that paves perfectly smooth cycle paths and rugged, high-grip hiking trails, and much more. It is used to waterproof swimming pools and in numerous applications in event locations, from a huge stadium to a local playground. It’s the rubberized surface of running tracks and the waterproofing solutions used to treat outdoor clothing. In tennis courts and other sports surfaces.\nBITUMEN recognized for its adhesive and cohesive assets; bitumen is often utilized in the construction industry. Bitumen is applied on road paving because it is viscous when hot, but solid once it cools down. Therefore, Bitumen operates as the binder/glue for pieces of the aggregate.\nThe vast majority of refined bitumen is used in construction: primarily as a constituent of products used in paving and roofing applications. According to the requirements of the end use bitumen is produced to specification. This is achieved either by refining process or blending.\nIt is estimated that the current world use of bitumen is approximately 102 million tons per year. Approximately 85% of all the bitumen produced is used as the binder in asphalt for roads.\nPacking of bitumen in drum, jumbo bag and 300kg polycube\nPacking of all grades could be same\n- Drum: per 20ft container we are loading 110 drums. Each drum is 185 kg Net and 195 Kg gross weight. In case of ordering other drum sizes line 120, 145 or 150 also able to deliver.\n- Jumbo bag: Another option for packing of bitumen 60/70 is jumbo bag which is 800 kg to 1200kg and able to load in 40ft and 20ft containers.\n- Polycube: The best packing right now is Polycube which is 300kg bags and 80 pcs able to stuff in 20ft container. It is economic packing and cut the transport cost and compete with other packing now. 24 tons per container and no waist of bitumen during melting is the benefit.\n- Bulk Packing: Bulk packing of bitumen 60/70 is two way, we transit by truck tanker which is dual layer and can remain liquid during the transit up to seven days then at destination it will discharge easily and without heating. In case bitumen get cold buyer can heat up via special channel in backside of tanker and inject the fire in the pipeline between the tank to heat the bitumen and there is exhaust on the front of tanker and the fire passing whole of tank. This way of delivery is good for neighbor customers. For far destination is not economy.']	['<urn:uuid:b6a50fed-046f-4321-8048-074ed05f7142>']	open-ended	direct	concise-and-natural	similar-to-document	single-doc	expert	2025-05-12T20:46:45.703947	8	74	2365
26	window film ppf lifespan durability compared	Window films typically have a 10-year warranty with potential life expectancy of 15-20 years, while Paint Protection Film (PPF) will need to be replaced eventually and can discolor over time, turning yellowish between 1-3 years. Additionally, PPF may damage the paint underneath when removed, while window films can be easily removed without damaging the surface.	['Popular Questions and Answers Asked About Window Films\nWhat Is Window Film?\nMost window films are constructed with several layers, the main layer being a high-quality polyester. Depending on the type of film, most of our window films are metallised using sputtering technology. This process embeds metal particles such as silver, stainless steel, copper, gold, titanium and chromium into a polyester film. Finally, the films are coated using special adhesives, ultraviolet inhibitors and then protected by a scratch-resistant coating. This ensures a long lasting colour and excellent solar performance.\nHow is window film applied?\nClean the glass. Measure the glass panel and cut the window film oversize. Remove the backing sheet from the film. Spray the glass and adhesive side of the film using a mild soapy water solution. Using a squeegee apply the film on to the glass to remove the excess water and bubbles. Trim the excess edges of the film using a sharp utility knife and dry with a soft lint-free cloth.\nPlease see our DIY instruction or YouTube video\nWhilst small applications are possible by any keen DIYer or tradesman, we would certainly recommend employing a professional company for larger applications or areas where a quality finish is vital. Contact us for a quote.\nHow long does window film last?\nThe exact lifespan depends on the type of film, glass and the environment in which it is fitted. Most types of window films have a warranty of 10 years, life expectancy can be much longer, maybe 15 years and in some cases over 20 years.\nWhat are the advantages of window film compared with ‘indoor’ solar protection methods?\nIndoor solar protection methods, such as blinds or curtains, prevent visible light from getting into a room and block the view looking outside. Additionally, infrared heat accumulates between solar protection and the window. Eventually radiating heat into the room. Air conditioning is one solution, but installing and running a system to maintain a constant indoor climate can be expensive. Whereas window film reflects the solar radiation away before it enters the room.\nDoes window film make a room cooler?\nYes. Window film rejects levels of solar heat to provide a cooler, more comfortable climate.\nDoes window film cause seal failure in double glazed units?\nNo. A typical double glazed unit is constructed with two panes of glass with a spacer bar at the edge separating the two glass panes. A hot adhesive sealant (polyisobutylene – PIB) is applied to the face of the spacer around the edge to bond the glass and spacer bar together. This makes a double glazed unit (DGU).\nThe application temperature or melting point of the PIB sealant is between 185ºC to 200ºC. The surface temperature of the internal glass with a dark film such as DC 15 window film applied on the inside, facing the sun directly can only reach a maximum temperature of 49ºC and the external pane of glass would heat to a maximum of 41ºC. These figures are based on the external air temperature being 30ºC.\nWhen glazing a DGU into a window frame, the DGU spacer bar is in a rebate behind the glazing bead. This holds the unit in place and protects the PIB from direct sunlight. The glass temperature will have to reach the melting point of the PIB sealant. An applied film cannot heat the glass enough to achieve these temperatures and there is no reason why the film would cause any thermal heat problems to the DGU’s.\nWill window film reduce heat and glare?\nSolar Control films are designed to reduce the sun’s solar heat, glare and ultraviolet (UV). They are available in many or tints, offering various levels of performance. Solar films can be virtually invisible or totally transform a building by giving it a uniform colour and hiding untidy looking office environments- such as the backs of work desks, wiring, etc.\nWill window film make the room darker?\nMost window films will reduce the excess glare from the sun like wearing sunglasses. Depending on the degree of tint used. We have a wide range of window films from virtually invisible to the dark tinted films, that will not make your room dark on a sunny day. Some solar control films may affect light levels but as your eyes automatically adjust to different light levels, the degree is generally much less than traditional window blinds or curtains.\nCan window film reduce my energy bills?\nDemand for creating a better environment has never been so high. Many companies have made a corporate pledge to reduce energy consumption and the carbon footprint, others driven by increasing energy costs and the impact on profits.\nWith the proven heat-rejection properties of window film, you can eliminate “hot spots” and extreme peaks in air conditioning demands at certain times of the day. Interior temperatures are more controllable and stable, allowing cooling systems to work within the capacity and consume less energy. Investing in our films offers a quick payback, with a 25% reduction in energy costs.\nCan window film reduce heat loss out through my windows?\nMost of a building’s heat can be lost through its glazing, increasing winter heating costs. Low-E insulation film applied to your existing windows will reflect radiated heat back into your room, increasing comfort and reducing fuel heating bills.\nWhat effect does window film have on visibility through the windows? From the inside looking out? From the outside looking in?\nMost window films maintain visibility looking out through the windows, some films are practically invisible once installed, allowing lots of natural visible light to enter a room. From the outside, it complements the exterior of any building. We have a range of films from virtually invisible to soft, neutral hues and darker films for privacy stopping passers-by from outside looking in.\nCan I clean the window after a film is applied?\nYes, using a soft, clean cloth or paper towel to clean and the same or a rubber squeegee to dry. Use a mild detergent or every-day glass cleaning solutions. Basically, avoid using abrasive tools/cleaners and chemicals.\nWill window film heat up my double glazed units and cause the glass to shatter or break?\nNo, window films alone do not cause the glass to break or shatter. However, we do understand that the application of some films will increase the temperature of your glass to increase, which in turn will increase the stress on the glass edges. The quality of those glass edges is out with our control. There are several factors to consider before installing window film (external shading, interior shading, glass breakage history, type of window film being used, type, size and shape of the glass, indoor-outdoor temperature) all contribute to the potential risk of glass breakage. Combined with our many years of experience, in applying window films to different types of glass and buildings, enables us to make proper judgments and a full risk assessment before any installation. Only after this will we advise the correct film to be used and guarantee its suitability, this minimises the potential for glass breakage.\nWhat happens if I want it removed?\nEasy. The film itself can be pulled from the glass which will generally leave adhesive residue on the glass. This is removed by spraying with soapy water and using a razor scraper. No need for any over-powering adhesive removers.\nIf you have any questions, please do not hesitate to ask. We would be happy to help\nCall us on 01428 751004 or email us at firstname.lastname@example.org', 'With the many different types of options available in automobile paint protection, it may be difficult to determine which type of protection is best for your car. You may be considering paint protection film or a ceramic coating. Both have their advantages and disadvantages, the Pro and Con. Perhaps the best way to decide is to carefully compare the two and think about your typical driving conditions and the potential damage that you want to prevent. As you’ll see, you may even want to combine the two for additional protection.Paint Protection Film vs. Ceramic Coatings Paint protection film (PPF) is a clear film that was originally developed to protect military vehicles. Eventually, the high-end performance car industry began using PPF and now it’s used in the consumer market to protect cars from rock chips, scratches, and contaminants. The self-healing properties of some PPF products make them an attractive option since some scratches will dissipate over time or with heat. Surface defects should be removed prior to application of the film.\nCeramic coatings are the next step in the evolution of paint protection even though they are not new and existed decades ago. They bond to the surface of the paint, creating a permanent layer of protection. Ceramic coatings offer substantially more scratch resistance, chemical resistance, UV protection and heat tolerance than wax or polymer sealants. On the other hand, there are polymer sealants from USA that contain the substracts in the forms of SIO2 or TIO2. SIO2 is the term for Silica Dioxide while TIO2 are basically known as Ceramic. They are similar but different in substracts metallic oxide polymers contain and the concentration of the amount of SIO2 or TIO2 added. Dupont is the World mass producer for TIO2 known as Titanium Dioxide. Environmental damage and scratches are unable to penetrate the coating, leaving the paint in pristine condition. A professional ceramic coating typically only needs to be installed once during the average lifetime of vehicle ownership, making it a cost-effective option that offers supreme protection.\nTo help you understand the differences between PPF and ceramic coatings, we’ve listed their pros and cons below.\nPaint Protection Film PROS\n- Better resistance to rock chip damage.\n- Some have limited self-healing properties (since it is essentially TPU ((thermoplastic polyurethane)), or similar film).\n- Prevents most scratches from reaching the paint (though the film will get scratched).\n- Slightly minimizes shine.\n- Is not very hydrophobic, so little water beading and sheeting.\n- Can discolor over time and turn a yellowish hue. After peeling off to change, the paintwork will turn yellowish between 1 to 3 years. In such situation, the paintwork need to be respray costing additional unnecessary cost.\n- Major damage cannot be repaired, so the film would need to be removed and replaced.\n- Will need to be replaced eventually.\n- Can damage paint underneath if removed\n- Difficult to install properly.\n- More expensive than ceramic coatings.\n- 9H hardness, so better resistance to minor scratches.\n- Greatly enhanced chemical resistance (professional coatings).\n- Super hydrophobic, excellent water beading/sheeting.\n- Easier cleaning.\n- Increased shine/luster.\n- Greater repairability (can be polished to fix minor damage).\n- Permanent, no need to remove and reinstall (pro coatings only).\n- Will not stop rock chips.\n- Can look bad if installed improperly.\n- Expensive, though less expensive than PPF.\nWe also recommend that if you install PPF on the entire vehicle it would be advantageous to install a ceramic coating over top for the slickness, hydrophobicity, and ease of cleaning']	['<urn:uuid:77182f04-13bc-4c9c-924c-7594dda6865c>', '<urn:uuid:f81a04cf-2654-4816-b36d-ea472feb2a90>']	factoid	with-premise	short-search-query	distant-from-document	comparison	expert	2025-05-12T20:46:45.703947	6	55	1843
27	I'm interested in proper names in different languages and modern farming. What's the process of translating farming facility names across languages, and how do modern vertical farms differ from traditional soil-based agriculture?	Regarding translation of facility names, there isn't a strict rule - names can be handled in several ways: they can be transferred unchanged, modified to fit the target language's sound system, or expanded with additional information to help readers understand the facility type. For vertical farms specifically, they differ substantially from traditional agriculture by using soil-less farming techniques like hydroponics, aquaponics, and aeroponics. These systems are typically housed in buildings, shipping containers, tunnels, or abandoned mine shafts, and use controlled-environment agriculture with customized LED lighting to maximize plant growth. Unlike traditional soil-based farming, vertical farming systems stack crops in layers, significantly reducing the land requirement while achieving higher yields.	['Translation of Proper Names\nBy Alireza Sadeghi Ghadi,\nMA in Translation, Great Translation Theoretician,\nMazandaran Province, Ghaemshar City, Iran\nasalireza1 at yahoo com\n“Proper names are never translated” seems to be a rule deeply rooted in many people’s minds. Yet looking at translated texts we find that translators do all sorts of things with proper names: non-translation, non-translation that leads to a different pronunciation in the targetlanguage, transcription or transliteration from non-Latin alphabets, morphological adaptation to the target language, cultural adaptation, substitution, and so on. It is interesting to note, moreover, that translators do not always use the same techniques with all the proper names of a particular text they are translating.\nUnlike generic nouns, proper names are mono-referential, but they are by no means mono-functional. Their main function is to identify an individual referent. It has often been claimed that proper names lack descriptive meaning:\n“An ordinary personal name is, roughly, a word, used referring, of which the use isnot dictated by any descriptive meaning the word may have. (Strawson 1971: 23)”\nIn the real world, proper names may be non-descriptive, but they are obviously not non-informative: If we are familiar with the culture in question, a proper name can tell us whether the referent is a female or male person (Alice – Bill), maybe even about their age or their geographical origin within the same language community or from another country, a pet (there are “typical” names for dogs, cats, horses, canaries, etc., like Pussy or Fury), a place (Mount Everest), etc. Such indicators may lead us astray in real life, but they can be assumed to be intentional in fiction. Titles and forms of address can also be problematic in translation. The translation of proper names has often been considered as a simple automatic process of transference from one language into another, due to the view that proper names are mere labels used to identify a person or a thing. This is exactly what Vendler purports when he writes that “proper names have no meaning (in the sense of ‘sense’ and not of ‘reference’), which is borne out by the fact that they do not require translation into another language” (Vendler1975:117).\nIn his view, proper names are to be treated as labels, which are attached to persons or objects and the only task of the translator is to carry them over, or transfer them, from the source language text to the target language text.\nBasically the same is maintained by Sciarone, who argues, in a much similar vein, that the view that proper names have no meaning (from the point of view of the language system) is corroborated by the general practice of leaving them unchanged in translation (Sciarone 1967:86). It has been shown, however, by authors like Searle (1975) or Strawson (1975) that this view is mistaken: proper names, beyond their identifying function, may also carry ‘senses’. The fallacy of this view thus lies in the incorrectness of the background assumption: not allproper names are mere identifying labels – in fact, most of them turning out to carry meaning of one sort or another. This will entail, then, that the translation of proper names is not a trivial issue but, on the contrary, may involve a rather delicate decision-making process, requiring on the part of the translator careful consideration of the meanings the name has before deciding how best to render it in the target language.\nBasically, nouns are classified as common or proper. Common nouns refer to a class of entities (e.g. squirrel), while proper nouns have a unique referent (John, London).\nGrammatically, proper nouns behave very much in the same way in the sentence as common nouns. There are, however, well-known co-occurrence restrictions that distinguish them from common nouns. The most important among them are:\nProper nouns (PN) do not accept demonstrative pronouns as determiners. One would not normally say this John just bought a car. However, supposing there are several Johns out of whom you wish to single out a particular one, you are already using John as a common noun meaning ‘any person called John.’\nPNs do not accept restrictive adjectives or restrictive relative clauses. In the sentence the Old Shakespeare felt the closeness of his death one is implicitly comparing one of several manifestations in time of the person called Shakespeare with the rest, therefore, one is using the word as a common noun in the grammatical sense. The same applies to sentences such as she is no longer the Eve she used to be. One may deny this only at the price of more or less ad hoc explanations about the character of the noun in question. Another way of putting this would be to say that we have to do with two homonymous words John or Shakespeare respectively, one of which is a proper noun, the other a common noun. When in a given speech situation we have a unique reference, we are dealing with a proper noun, otherwise with a common noun.\nOpposition between definite and indefinite is neutralized in PNs (a given PN either invariably takes zero articles as in John, London, or invariably takes the definite article as in the Strand, the Haymarket,and the Queen Elizabeth). A seeming counterexample such as that is not the John I was talking about is an instance of John being used as a common noun as seen above. In other words, a noun’s status as either common or proper is ultimately determined by situational factors. If in a given speech situation, there is a possibility of what looks like a proper noun having multiple referents (this John, two Johns) we have to do with a common noun homonymous with a proper noun.\nAll of the above features derive directly from the fact that PNs refer to unique referents. In Randolph was a true Churchill the surname is being used in the sense ‘a member of the Churchill family’, that is, as a common noun.\nWhether a given noun is common or proper is not always easy to decide. There are borderline cases that could be classified either way. Also, a given noun may change category depending on how it is used. For instance, a common noun referring to a given landscape feature may turn into a proper noun with a uniquereferent. Examples include names such as Saari ‘island’, Kymi ‘large river’ etc.\nPersonal proper names used metaphorically may turn into common names: He thinks he is a Napoleon. On the other hand, surnames such as Smith, Fletcherand Seppä ‘smith’ have their origin in the trade of the first bearer or rather that of the father of the first bearer. Proper names (used here interchangeably with the expression ‘proper nouns’) can be dealt with in a number of ways in translations. First, a PN can be transported wholesale from the target text (allowance being made for possible transliteration or transcription depending on the languages concerned). Second, it can be partly transported from the source language (SL) and partly translated. Thirdly, it can be replaced with more or less different names in the target language (TL). Finally, it can be dispensed with altogether.\nIn a relevance-theoretic framework, the meaning of a concept is made up of a truth-functional logical entry, which may be empty, partially filled or fully definitional and an encyclopaedic entry, containing various kinds of (propositional and non-propositional) representational information about the denotation and, as I understand it, possible connotations of the concept (e.g., cultural or personal beliefs), stored in memory. The concept may also be associated with a lexicalentry, which contains linguistic (phonological, morphological, semantic and categorial) information about the natural language item related to it (Sperber and Wilson 1986:83–93). The three different types of information (lexical, logical and encyclopedic) are stored in different places in memory. It is suggested that the content of an assumption, defined as a structured set of concepts, is the function of the logical entries of the concepts that it contains and the context in which it is processed is, at least partly, drawn from the encyclopedic entries of these concepts (Sperber and Wilson 1986:89).\nPrototypical proper names (that is, names without a descriptive content) are handled by associating with them empty logical entries. In other (less prototypical) cases a name may also have a logical entry (or, in the case of a composite name, it may include several logical entries which combine to make up the logical content of the name) which is partly or fully definitional (Sperber and Wilson 1986:91–2). Thus proper names seem to be not essentially unlike any other kinds of expression in terms of the structure of their meaning. Rather, what we find here is a continuum of various sorts of proper names. At one end of the scale we find the most prototypical names, proper nouns, which supposedly lack any logical content but may carry several assumptions in their encyclopedic entries. At the other extreme we have composite names, which are no different in terms of logical content from any ordinary phrasal expression. What makes them names, eventually, is that they are used as such in the given context. Names of countries seldom pose a problem to the translator. There are lists of the official names of countries in Persian and English that should be consulted. In a few cases, variation does occur depending on the degree of formality involved. The basic rule concerning personal names is that they are left untranslated. In some cases, transliteration or transcription may be needed depending on the language. There are however, exceptions to this basic rule. A trivial one is that in certain languages (Chinese and Japanese, for instance) the order of a person’s first and last name is the opposite to the most common Western order of first name + last name. In Europe, Hungarian last names come before first names. Names of musicals, operas and ballets are sometimes retained in translation. My Fair Lady,West Side Story and Così fan tutte are known all over the world by the original names, a notable exception being Russia, where only translated name are used. Names of international organizations normally have translation equivalents in the member countries. Looking at the number of translation errors actually occurring in texts, a more frequent problem is that of the translator not paying attention to transliteration rules.\nIn spite of the “translation rule” quoted above, there are no rules for the translation of proper names. In non-fictional texts, it seems to be a convention to use the target-culture eponyms of a source-culture name, if there is one, but if a translator prefers to use the source-culture form, nobody will mind as long as it is clear what place the name refers to. Perhaps the audience will think that the translator is showing off her knowledge too much. Wherever the function of the proper name is limited to identifying an individual referent, the main criterion for translation will be to make this identifying function work for the target audience.\nIn fiction, things are not quite as simple as that. We have assumed that in fictional texts there is no name that has no informative function at all, however subtle it may be. If this information is explicit, as in a descriptive name, it can be translated – although a translation may interfere with the function of culture marker. If the information is implicit, however, or if the marker function has priority over the informative function of the proper name, this aspect will be lost in the translation, unless the translator decides to compensate for the loss by providing the information in the context. Of course, there are proper names that exist in the same form both in the source and the target culture.\nNames of towns, municipalities and villages are examples of settlement names. There is no problem with names of relatively unimportant settlements that are carried over unchanged in translation. The town of Rauma does not change its name, nor does London, Ontario. With smaller places, however, a gloss is often in place to give the reader an indication of the type of place concerned. To sum up, PNs can be treated in a number of ways in translation:\nThey can be imported unchanged from the SL text;\nThey can be modified to fit the phonological/graphological system of the TL. This, of course, is something that has or has not been done for the translator by his/her speech community in the case of conventional place names like Prague, the Hague, Rome etc.;\nThey can be expanded with a gloss to make up for the TL reader’s lack of world knowledge in the target culture;\nOn occasion, they might be omitted altogether (perhaps replaced with a paraphrase) if considered peripheral in terms of the central message of the text or if retaining them would be more likely to cause the reader to pause in puzzlement. True, this would be more likely to happen in interpretation, but could not be ruled out altogether in translation, either;\nIn rare cases, they might even be introduced in the TL text where, instead of a proper name, the SL text contains a cultural allusion unlikely to be understood by the TL reader.\nAlbin, V. What’s in a name: Juliet’s Question Revisited. In Translation Journal 7(4). October 2003. http://accurapid.com/journal/26names.htm.\nBakken, K. 2002. Navnestatus og bestemthetskategorien In Avgränsning av namnkategorier. Rapport från NORNA:s tjugonionde symposium på Svidja 20-22 april 2001. Terhi Ainiala & Peter Slotte (eds.) Tallinn: Kirjakas.\nCarroll, L. (1946): Alice in Wonderland and Through the Looking Glass. New York: Grosset & Dunlap.\nNord, C. (1994): “It’s Tea-Time in Wonderland: culture-markers in fictional texts.” In: Heiner Pürschel et al. (eds.): Intercultural Communication. Proceedings of the 17th International.\nSearle, J. R. 1975. The Problem of Proper Names. In: Steinberg, D. & Jakobovich, L. (eds.) 134–141.\nSperber, D. & Wilson, D. 1986. Relevance. Oxford: Basil Blackwell.\nSteinberg, D. & Jakobovich, L. (eds.) 1975. Semantics. Cambridge: Cambridge University Press.\nStrawson, P. F. 1975. Identifying Reference and Truth–Values. In: Steinberg, D. & Jakobovits, L.(eds.) 86–99.\nToury, G. 1978. The Nature and Role of Norms in Literary Translation. In: Holmes, J. S., Lambert, J. & van den Broeck, R. (eds.) Literature and Translation: New Perspectives in Literary Studies, Leuven: Acco. 83–100.\nVendler, Z. 1975. Singular Terms. In: Steinberg, D. & Jakobovits, L. (eds.) 115–133.\nVermes, A. P. 2001. Proper Names in Translation: A Relevance-Theoretic Analysis. Unpublished PhD dissertation. Debrecen: Debreceni Egyetem.\nWilson, D. 1992. Reference and Relevance. In: UCL Working Papers in LinguisticsVol. 4. London: Dept. of Phonetics and Linguistics, University College London. 167–191.\nWilson, D. & Sperber, D. 1988. Representation and Relevance. In: Kempson, R. M. (ed.) Mental Representations: The Interface between Language and Reality. Cambridge: Cambridge University Press. 133–53.', 'As urban populations continue to grow, entrepreneurs are going beyond traditional farming to find new ways to feed everyone while minimising the effect on our land and water resources. Vertical farming is one such method that has been used all around the world. Food crops may be conveniently farmed in urban settings using Vertical Farming by planting in vertically stacked layers to conserve space and require little energy and water for irrigation.\nVertical farming is the process of producing crops in layers that are vertically stacked. Controlled-environment agriculture, which tries to maximise plant development, and soil-less farming techniques such as hydroponics, aquaponics, and aeroponics, are frequently used.\nBuildings, shipping containers, tunnels, and abandoned mine shafts are among popular structures used to host vertical farming systems. There are approximately 30 hectares (74 acres) of functioning vertical farms around the globe as of 2020. Vertical farming, in conjunction with other cutting-edge technology such as customised LED lighting, has resulted in crop yields that are more than ten times greater than those obtained by standard agricultural methods.\nVertical farming is still in its early stages in India, but there are a few entrepreneurs and agri-tech enterprises aiming to revolutionise the area.\nVertical Farming Background and Concept\nGilbert Ellis Bailey originated the phrase “vertical farming” and published a book named “Vertical Farming” in 1915. William Frederick Gerick pioneered hydroponics at the University of California, Berkeley, in the early 1930s.\nke Olsson, a Swedish ecological farmer, devised a spiral-shaped rail system for growing plants in the 1980s and proposed vertical farming as a method of raising vegetables in cities.\nProfessor Dickson Despommier invented the concept of vertical farming in 1999. His idea was to grow food in urban areas, utilising less distance and saving time in transporting food produced in rural regions to cities.\nHe aimed to produce food in urban areas in order to have fresher goods available sooner and at a reduced cost. As a result, vertical farming is defined as the cultivation and production of crops/plants in vertically stacked layers and vertically inclined surfaces.\nThe plants are vertically piled in a tower-like form in the physical arrangement. This reduces the amount of space needed to cultivate plants. Following that, a combination of natural and artificial lighting is employed to ensure an ideal atmosphere for the plants’ effective growth. The third component is the plant’s growth medium. Aeroponic, hydroponic, or aquaponic growth media are employed instead of soil as the growing medium.\nAs the methodology gets more scientific, the process’s efficiency grows, and as a result, vertical farming becomes more sustainable, consuming 95 percent less water than previous agricultural methods.\nAlso Read, Oxagon: The World’s First Floating City in the World\nVertical Farming Techniques\nIt is a method of producing food in water without the use of soil by employing mineral fertiliser solutions.\nThe primary benefit of this strategy is that it lowers soil-related cultivation issues such as soil-borne insects, pests, and illnesses.\nAeroponics was inspired by NASA’s (National Aeronautical and Space Administration, USA) endeavour in the 1990s to develop an effective technique to grow plants in space. There is no growth medium in aeroponics, hence there are no containers for growing crops. Instead of water, mist or nutrient solutions are utilised in aeroponics. Because the plants are attached to a support and the roots are sprayed with nutritional solution, there is very little space, very little water, and no soil required.\nThe name aquaponics is derived from the combination of two words: aquaculture (fish farming) and hydroponics (the process of growing plants without soil in order to develop symbiotic interactions between the plants and the fish). The symbiosis is established by feeding nutrient-rich waste from fish tanks to hydroponic production beds called “fertigate.”\nIn turn, the hydroponic beds act as biofilters, removing gases, acids, and chemicals from the water, such as ammonia, nitrates, and phosphates. Furthermore, the gravel beds serve as a home for nitrifying bacteria, which aid in nutrient cycling and water filtering. As a result, the newly cleansed water may be recirculated back into the fish tanks.\nThe Benefits of Vertical Farming\nVertical farming offers various advantages, making it promising for agriculture’s future. The land need is fairly minimal, water usage is 80% less, water is recycled and stored, pesticides are not used, and in the case of high-tech farms, there is no true reliance on the weather.\nA vertical farm makes farming possible within the constraints of a metropolis. When the farms are close by, the food is delivered swiftly and is always fresh, as opposed to the chilled stuff commonly seen in stores. Transportation reduction minimises the cost of fossil fuels and the accompanying emissions, as well as transportation spoilage. Vertical farming, like anything else, has its limitations. The biggest issue is the initial capital expenses for building the vertical farming system.\nThere are further expenditures associated with building the structures as well as their automation, such as computerised and monitoring systems, remote control systems and software, automated racking and stacking systems, programmable LED lighting systems, temperature control systems, and so on.']	['<urn:uuid:a73a2692-8046-40b7-a262-7ffef30c1ddd>', '<urn:uuid:38abc93f-c15d-442d-abe3-4bcfbd141419>']	open-ended	with-premise	verbose-and-natural	similar-to-document	multi-aspect	novice	2025-05-12T20:46:45.703947	32	109	3297
28	What are the disability-focused parking accommodations available at both Stuttgart Airport's P0 zone and Sacramento International Airport's general parking areas?	At Stuttgart Airport's P0 zone, three specific parking spaces are designated for those with disabilities, while at Sacramento International Airport, persons with disabilities who have valid disabled license plates or placards can park in designated spaces in any lot at the posted rate.	['The Stuttgart Airport is an international airport in Stuttgart, Baden-Württemberg, Germany. Due to this international status, the airport has catered to around 11,832.642 passengers, making it the 6th busiest airport in all of Germany. The airport is around 10 km from the city center and has been christened after one of the former mayors of Stuttgart, Manfred Rommel.\nLong term parking at Stuttgart airport\nThe airport management has designated separate parking spaces for travelers who wish to park at the airport for a longer period. The passengers are free to choose the parking zones according to their budget, as each car park has a different parking fee.\nP0 long term parking zone\nThe P0 Parking lot has five separate zones, two of those are parking spaces for families, and the other three are for those with disabilities. The parking lot allows both short term parking, with around 1769 parking spaces and long term parking, with around 20 parking spaces. The P0 zone is free of charge to park for the first hour. After availing parking for one hour, the lot charges €3 for two hours. Long term parking in this lot is allowed for one week only, with charges of €55.\nThe parking lot is an enclosed garage with multistories. For easier access to the passengers, the airport has built lifts in the parking lots, with many staff persons found at the zones to assist you. However, the parking lot has some restrictions, such as height restrictions set for cars. Cars no more than 2.10m can enter the enclosed parking garage. For easier parking, airport management has allowed passengers to pre-book their parking spaces at the airport. A valet service is also available on demand.\nP4 long term parking zone\nThe P4 parking lot is a multistorey garage. It offers long term parking for passengers who wish to leave their cars at the airport when traveling. The parking zone is situated in the proximity of the main airport building, taking around 3 minutes of walk to reach the terminals from the garage, making access to the terminals easy for the passengers. The parking zone charges €4 for one hour, with an increase in parking fees as the number of hours parked increases.\nSimilar to the other parking zones, the garage is enclosed, with a height restriction of 2.10m for all vehicles. To ensure maximum security and safety of your cars, the airport parking lot has a 24 hr CCTV coverage.\nThe airport parking management offers many discounts and packages during high traffic seasons, such as in the summers. Long term parking is extended and allowed for up to six weeks for a one-off fee. However, due to high demand, you should book your parking spaces in advance, well in time.\nShort term parking at Stuttgart airport\nThe Stuttgart Airport has different charges for its various short term parking zones to ensure a budget-friendly experience for its passengers.\nP0 short term parking zone\nThe P0 Short Term Parking Zone is located directly in front of the airport building. It has both short term and long term parking spaces. However, the short term spaces dominate the latter. The P0 zone is the most economical parking zone at the airport, with no parking charges for the first hour. However, the airport charges €3 for two hours, which is still pretty reasonable. The other two highly budget-friendly short term parking zones are P8 and P11, but they are further away from the main airport building.\nP2 short term parking zone\nThe P2 Parking Zone is another short term parking lot at Stuttgart airport. It is located in front of the terminals, making it easy for the passengers to access it. The parking lot charges €4 for one hour and €7 for two hours. Parking for a more extended period is not allowed here.\nThe parking lot offers over 1300 parking spaces, with 57 reserved for women and four reserved strictly for those with disabilities. The structure of the parking zone is a multistory indoor garage with a height restriction of 2.10m for all vehicles. The parking lot also has its valet service system for the travelers’ ease.\nP3 short term parking zone\nThe P3 Parking Zone is right next to Terminal 1. It is not the biggest parking lot at Stuttgart airport. Although it is a multistory garage, It has limited parking spaces of around 48, with two parking spaces strictly dedicated to people with disabilities. For traveler’s easy transfer between the parking and the airport building, there are lifts in the parking lot. The parking lot charges a parking fee of €2.50 for 30 minutes, with an increase to €5.00 for one hour.', 'Sacramento International Airport is established in October 1967 and located in the California. It is run by the Sacramento County Airport System. Half of the airline travelers in SMF airport are carried by Southwest Airlines.\nThe fast low-cost carries about 4.6 million passengers each year. It has direct and nonstop flights throughout North America and more destinations as Mexico City. At the airport, there are also two runways that cover 6,000 acres.\nThe number of flights to Sacramento International comes in from destinations across the US like Denver, Phoenix, San Francisco, Charlotte, Chicago, Salt Lake City, Atlanta, Dallas, Detroit, San Jose, Houston, Washington, and Mexico, etc. SMF airport also has 323 daily commercial flights.\nIt is a small and efficient international airport with 2 terminals (A & B) and a nice variety of shopping and dining options. There are 2 departures concourses at both terminals and a separate International Arrivals building.\nBoth terminals are systematically laid out and have Check-in and Departures on the upper levels and Arrivals. There is also a Baggage Reclaims on the ground floor.\nVarious shopping, dining places, and services facilities are available.\nThe terminal B has been expanded and nowadays is 3 times bigger than the original terminal B. It has three different levels and an automated people mover.\n- Level 1: There is baggage claim area and Ground Transportation.\n- Level 2: It is ticketing Level.\n- Level 3: It is APM Level.\n|PARTICULARS||TERMINAL A||TERMINAL B|\n|Gates||13 Gates, from A1 to A5 and from A10 to A17 located on the upper level.||19 Gates, from B4 to B12 and from B14 to B23.|\n|Operated Airlines||· American\n· United Airlines\nYou can enter into these lounges even if you are an economy class passenger. It is possible either by purchasing a lounge pass or by a membership program.\nThe Hotels near Sacramento airport are:\nHilton Garden Inn Sacramento:\n- It is 5 minutes far from the airport and has the free 24-hour airport shuttle.\n- It also has an outdoor pool and rooms with a flat-screen TV, refrigerator, microwave, and coffee maker.\n- Free Wi-Fi is also provided.\n- Guests can get Drinks and snacks from the Pavilion Pantry for 24 hours.\nHampton Inn Sacramento Airport:\n- It is 8 minutes far from the airport and has the free 24-hour airport shuttle.\n- Guests can facilitate themselves with the free high-speed internet access and a daily continental breakfast.\n- The fitness room and outdoor heated pool of hotel are great places for guests to relax after a long day.\nHawthorn Suites Sacramento:\n- This hotel offers a free airport shuttle from Sacramento International Airport.\n- A free continental breakfast is offered daily.\n- Each room is featured with a flat-screen TV, microwave, seating area, refrigerator, a coffee maker, and bathroom.\n- Guests can stay connected with the business center and free Wi-Fi.\nMarriott Sacramento Airport Natomas:\n- It is just a few minutes far from the Sacramento International Airport and offers all the comforts with daily complimentary hot breakfast buffet.\n- It features an outdoor pool and on-site tennis courts.\n- Accommodations with separate work areas and premium cable TV.\nSuper 8 Hotel:\n- It is located near the Interstates 80 and 5 that is just on a short distance from Sacramento International Airport.\n- It features with free Wi-Fi and Airport shuttle service is also available.\n- It provides a continental breakfast and on-site free parking is also available.\nDays Inn Sacramento Downtown:\n- It is just located at a walking distance of the Sacramento River.\n- The hotel offers free Wi-Fi and a continental breakfast.\n- Guests can also use the indoor pool.\n- Each guest room has a hairdryer, cable TV, microwave, mini-refrigerator, and coffee maker.\nSacramento Airport Bus:\nYolobus operates at the Sacramento airport. There are public bus routes like 42A, clockwise and 42B, counter-clockwise that links the airport with West Sacramento, downtown Sacramento, and Davis.\n- Van Service: Super Shuttle Sacramento is the company that provides on-demand van service at SMF Airport. It serves its services to Sacramento, Yolo, El Dorado, Placer, Yuba, San Joaquin, Sutter, Solano, and Nevada counties. This service is available on the ground transportation areas and company also provide limousine and sedan service.\n- Pure Luxury Transportation: This shuttle service is specially designed for those passengers who want to attend corporate events, weddings, etc.\n- On-Demand Sacramento Airport Taxi Services: Uber, Wingz, and Lyft companies are authorized to operate at Sacramento airport. It also provides the shared ride.\n- Paratransit and Med Star: These companies that provide the special service to those passengers who need it or due to disabilities. It is necessary to do a reservation 2 days before.\nBy Rentals Cars:\nCar rental services and station are located in the rental car terminal that is a separate facility and connects to the airport terminals via free shuttle service.\n- Rental car shuttles run every 10 minutes that is stopping at Terminal A and Terminal B.\n- The various rental car agencies are Advantage, Avis, Dollar, Enterprise, Hertz, Alamo, Budget, National, Thrifty, etc.\n- Advance provides free transportation from the terminal to your particular nearby location.\nAIRPORT WI-FI CONNECTION\nFree Wi-Fi is also available at Sacramento Airport. Connect your device to the “FlySacramento” network and enjoy the other services. There is a number of internet stations are located in the terminals.\nSACRAMENTO INTERNATIONAL AIRPORT PARKING\nAll parking is worked according to first-come, first-served strategy. Reservations are not available.\nThere are numbers of parking lots are available and that are:\n|Location||Closest to Terminal B||Nearby Terminals A & B|\n|Shuttle Service||Not Available||Provided at 8-10 minute intervals||Provided at 15-20 minute intervals.|\nFacilities Available At The Parking lot Area:\nThe airport provides free shuttle service at both terminals for 24 hours of the day. This shuttle service runs to and from the rental car terminal, the Economy Lot, and, the Daily Lot.\nDead Batteries/Lost Cars:\nThey assist you with the dead batteries or locating your vehicle in an airport lot or the garage.\nThe videos on airport parking are also available at the Sacramento International Airport. You can view them on YouTube channel of the Airport.\nPersons with disabilities with a valid disabled license plate or placard may park in designated spaces in any lot at the posted rate.\nElectrical vehicle (EV) charging stations: These stations are available in the parking garage, the Free Waiting Area, and the Daily Lot.\n- There are 2 electrical vehicle charging stations are available on each floor of the garage, which is near the Terminal A elevators.\n- There are also 4 chargers are available in the Daily Lot.\n- There are 12 electrical vehicle charging stations available near the Terminal B elevators.\n- There are 2 charging stations on floors 2, 3, 4 and 6 and 4 charging stations on the 5th floor.\nLong Term Parking:\nIf you need to park for more than a day or two, then you have to pay for extended trips at SMF.\nThe vehicles, which are more than 21 feet long, are charged the double.\n- Art in the Airport: Art exhibition displays are available on throughout the airport.\n- ATMs / Cash Machines: There are 9 locations throughout the airport where you can find the cash machines.\n- Baby Care Facilities: The rooms are available with comfortable seating, sink, and power outlets for babies and Women. It is located at the airside, which is just adjacent to the restrooms of women.\n- Baggage Carts: SmarteCartes are available on a rental fee from vending machines in baggage claim. It is located at the terminal entrance doors and the parking garage.\n- Currency Exchange: It is not available in the airport. The foreign currency exchange is a Travelex branch that is inside a Westamerica Bank in Sacramento.\n- Duty-Free & Shopping: There are no duty-free shops available at Sacramento Airport. But several other shops are available in terminals A and B.\n- Food & Drinks: There is the number of food concessions across the airport. It operates according to the schedules. It is open as early as 4:00 AM and closed by 9:00 PM.\n- Information Desk: Information booths are available throughout the airport and are staffed by volunteers.\n- Massages: To feel stress-free or want to melt away the stress there are 10, 15 and 30-minute massages available. It is located at the Terminal B, airside. The timing is between 8:00 AM to 9:00 PM (Sun-Fri) and 9:00 AM to 6:00 PM on Saturday.\n- Quiet Room: It is located at the Terminal A, Landside, Upper Level.\nThis guide is for informational purpose to assist you with your planning trip. The information about the airport is based on information that is aware you about the airport’s lounges, hotels, and transportation providers.\nSMF AIRPORT REVIEWS:\nBy Erin S.\nAt the airport, there are pretty good food options and a large cafeteria area if you are looking for a place to eat that doesn’t feel like hunching and stuffing your face, a couple cute shops/bars.\nThe best part is the travel anxiety therapy dog that comes around, though. It turns out that he’s trained to help people who are afraid of flying! How cool is that?!\nBy Colleen B.\nIt is clean and easy. I have family in northern CA so I can fly into SMF, SFO or OAK and I choose SMF. It’s out of the city, it’s easy to get to, it’s newer, it’s clean – it’s a keeper. Thank you!\nBy Amber K.\nThe Very best airport I’ve been to. It’s SO well set up and so very clean! You’d have no idea that this is an international airport. Every person I came in contact with was very friendly, especially this information guy. Some airports are an absolute nightmare, that I’ll go out of my way to not fly into, but Sacramento would be one that I’d gladly fly into.\nBy Airotciv L.\nThis airport is slightly less hectic and confusing than many others. It is clean and things transpire in an orderly fashion. Yes, there is free Wi-Fi and the employees are friendly, helpful and polite.\nThe PA system is loud enough and directives are clear. They did not lose my luggage and complimented me on it (smile).\nThey have all of the same merchants that other airports seem to have, but what earned them an entire additional star from this humble. Happy dance (with shoes on), safe home and four stars earned here.']	['<urn:uuid:55850497-568f-4fc5-a46c-f2e485aeb7bf>', '<urn:uuid:5f287aeb-0f85-4214-81bf-4d6db09c9788>']	factoid	direct	verbose-and-natural	similar-to-document	comparison	expert	2025-05-12T20:46:45.703947	20	43	2529
29	seed spread methods princess butterfly bush compare	Both species spread aggressively through seeds, but in different ways. Princess trees specifically thrive in fire-disturbed areas and spread rapidly in such conditions. Butterfly bush produces massive quantities of lightweight, winged seeds (40,000 per flower spike with 80% germination rate) that disperse by wind and water over many miles, requiring exposed soil to germinate successfully.	"[""Conservation is the act of preserving, guarding, or protecting; and includes the wise use of our natural environment.\nThe Mountain High Hikers, Inc. supports all conservation efforts and participates in several conservation efforts to help preserve the trails and the natural ecosystem:\nIf you are interested in information regarding volunteers, please contact the MHH Conservation Director, Kim Blankenship (firstname.lastname@example.org)\nThe State Botanical Garden of Georgia has listed these nurseries to help home gardeners find ethical sources of plant species native to Georgia.\nThe Forest Service has just announced THE three dates for in-person, public input before the Foothills Landscape Project is started.\nThe Forest needs your participation and voice.\nThe Foothills Landscape Project spans from the South Carolina border to Chatsworth and encompasses 157,625 acres, more than a fifth of the entire Chattahoochee National Forest. The project includes vegetation, road, and recreation management, and proposes up to 55,000 acres of commercial timber harvests and 63,000 acres of herbicide application. It also includes some of our most cherished places:\nGeorgia Forest Watch is asking hikers to help eradicate an invasive species coming in after the recent fires in the Cohuttas. Winter seems the best time to do this. Please access more information below:\n“2016 fall's wildfires were a threat in many ways: they forced people to evacuate, killed patches of forest canopy, and smoked in towns. The biggest threat to our forests, though, may just now be getting started. The Rough Ridge Fire produced ideal conditions for non-native princess trees to reproduce and invade the Cohutta Wilderness. Native to Asia, princess tree's extraordinarily fast growth allows them to out-compete and choke out native species. Populations are beginning to explode in the Cohuttas. To help get this situation under control before the problem becomes too big to deal with, the first step is finding out exactly where the problem is.\nWe need volunteers to hike the trails in and around the Cohutta Wilderness and record where you see princess tree seedlings. Seedlings are easy to identify, and equipment, nothing more than a smartphone or GPS unit, can be provided if needed. If you haven't hiked in the Cohuttas since the fire, know that the area is as beautiful as ever. Keeping the Cohuttas from turning into a grove of princess trees will be an ongoing process. Georgia ForestWatch will work with the Forest Service to document locations of princess trees and assist in their removal from the Cohuttas. For that to work, though, we need help to quickly assess the situation. Please contact Georgia ForestWatch at 706-867-0051 or email Jess Riddle (email@example.com) if you would like to help.”\nWe need all of the trails in the fire area surveyed for princess trees, essentially all of the trails west of the Jacks River. Whenever you feel like hiking in that area, if you would just record and send to me the location of any princess trees you see, that would be great. GPS coordinates from a phone or GPS unit are best. Putting an “X” on a topographic map works too. If you hike a trail and don’t see any, that’s helpful information too. I’m not sure how familiar you are with princess tree, so here are a few highlights of what to look for. In the winter the easiest thing to look for are the pods on mature trees. Each pod is about the size of a golf ball and pointed at one end. The pods come in branched clusters about a foot long. The trees are also distinctive because they have big thick twigs, as wide as your finger or wider, with a very open branch structure (lots of space between twigs). They’re a medium sized tree. Right now, we only know of mature trees along the streams, where water occasionally washes the soil bare. In spring, the leaves will make the seedlings stand out. The leaves are big and roughly heart shaped, kind of like a basswood or grape leaf. You can tell them apart because they are often even bigger, sometimes dinner plate size or even larger, and hairy. The leaves also come in pairs on the twigs. Instead of taking turns going up the twig, you will have a pair of leaves opposite each other, then bare twig, then a pair of leaves…. The seedlings could be anywhere in the burned area, from the Jacks River west.\nThese sites have more info on the trees\nAnd this site has good photos\nOnce we have a good grasp on where the princess trees are, we’ll coordinate with the Forest Service on some workdays and other programs to actually get rid of them.\nThe below web sites provide information in regards to Georgia and North Carolina representatives who should be contacted regarding a particular conservation issue.\n1. GA US Senators and US Representatives\n2. GA Senators and Representatives\n3. North Carolina US Senators and US Representatives\n4. North Carolina Senators and Representatives"", 'Butterfly bush is an introduced shrub from China that has been widely planted as an ornamental and butterfly plant throughout North America. Unfortunately, this popular garden plant is also highly invasive and spreads profusely by seed into disturbed and natural areas. Butterfly bush can now be found commonly along riversides and roads and in cleared forests throughout western Washington and Oregon, at times significantly impacting riparian habitat and riverside conditions and impeding forest regeneration.\nLegal status in King County, Washington\nButterfly bush is a Class B noxious weed on the Washington State Noxious Weed List. It is on the Non-Regulated Noxious Weed List for King County, Washington. Control of butterfly bush in King County is recommended but not required. Butterfly bush is not on the prohibited plants list administered by the Washington State Department of Agriculture, but some nurseries have volunteered to stop selling it due to its danger to natural resources.\nThe King County Noxious Weed Control Board encourages property owners to remove butterfly bush where possible and to avoid introducing it to new landscapes. Ornamental plantings can be contained by removing flower stalks before they form seeds.\nIdentification (see below for more photos)\n- Deciduous shrub up to 15 feet tall with arching branches\n- Showy flower spikes grow at the branch ends, either upright or often nodding, 4 to 10 inches long\n- Flowers typically light purple with orange centers (hence the common name ""orange eye""), four-petaled, bell-shaped, and in dense clusters\n- Cultivars have been developed with a range of colors including red, magenta, blue, orange, yellow, white and dark purple\n- Young stems are green, older stems have peeling, gray-brown bark\n- Leaves are long and narrow or somewhat egg-shaped and arranged oppositely on the branches\n- Leaves are usually 4 to 10 inches long and 1 to 3 inches wide\n- Edges of the leaves are toothed\n- Upper leaf surface is deep green to blue-gray, while the undersides seem whitish due to dense covering or short, fuzzy hair\n- Flowers bloom between mid-summer and the first frost in Washington State\n- Seed capsules split in two to release small, winged seeds\nHabitat and impact\nButterfly bush is most vigorous in well-drained soil and full sun. It can tolerate drought and low-nutient soil and can grow in very challenging conditions, such as cracks in the pavement and along railroads. Seeds require exposed soil to gerninate successfully and seedlings are not often seen in improved garden soils or well-maintained landscapes.\nButterfly bush colonizes disturbed areas such as riversides, roadsides, railroads, pastures, and recently logged or burned forests. Invasion of butterfly bush along riversides is especially problematic, because it forms dense thickets, crowds out native vegetation, and disrupts natural succession patterns. Initial densities of seedlings along open areas of riverbanks and sandbars can be very high and mature butterfly bush stands keep out native willows and other woody vegetation that would normally re-vegetate riverbanks after floods.\nGrowth and reproduction\nButterfly bush spreads by producing abundant amounts of very lightweight, winged seeds that are dispersed by wind and water over many miles. A study at Longwood Gardens in Pennsylvania found that a single flower spike produced 40,000 seeds. The germination rate of several cultivars was 80 percent or higher. This species is quick to mature, often producing seeds during its first year of growth. Also, butterfly bush is adapted to surviving along riversides and can develop roots on branches that have been buried or broken off.\nOnce established, butterfly bush is tough to eliminate. Seeds remain viable in the soil for 3 to 5 years. Butterfly bush can re-sprout from the rootstock after it can been damaged or cut down to its base, and the cut stems can grow into new plants if not disposed of properly.\nPrevention: Don\'t let butterfly bush go to seed. Remove spent flowerheads in the fall before they disperse seeds (don\'t wait until spring) and discard these in the garbage to avoid spreading seeds. Consider using other species in your landscape (see the garden wise publication for suggestions). For existing plantings, consider removing the butterfly bush to prevent its spread.\nManual control: Seedlings can be easily hand-pulled and larger bushes can be dug out (with some effort). Weed wrenches can work, but trunks are often too brittle and break off. Watch for new seedlings where the soil was disturbed and consider planting grass or a ground cover to suppress future seedling germination of the butterfly bush seeds remaining in the soil.\nDisposal: Branches should not be left on the bare ground because they can form roots and re-grow. Do not discard branches or root balls in natural areas or on roadsides because they can re-grow and spread. Branches can be burned, put in your yard waste container, or taken to a landfill or yard waste composting facility. Seedheads should be discarded in the garbage. If you cut seedheads after the fall, place a bag around them before cutting off to minimize dispersal of the seeds.\nChemical control: Controlling butterfly bush by spraying with a brush-control herbicide is somewhat effective, but for better results, cut the trunk off at the base and apply concentrated glyphosate (such as in Roundup) or triclopyr (such as in Garlon or Brush B Gon) to the freshly cut surface. For more detailed information on the cut stump treatment method and more information on using herbicides to control weeds in different crops and locations, please see the Pacific Northwest Weed Management Handbook. Always follow the label directions on any herbicide product you use to minimize any potential risks to you and the environment, and follow all applicable laws and regulations regarding herbicide use on your site. Contact the Noxious Weed Program if you are unsure about what to do.\nBiological control: There are currently no biocontrol agents available for managing butterfly bush. See the biological control page for more information.\nCultural/grazing: Although goats will eat butterfly bush and can cause damage to the plants, they will not kill or eradicate this plant. Consider using goat grazing as part of an integrated program with other methods, perhaps followed up by manual or chemical treatment.\nAdditional information on butterfly bush\nWhat to do if you find this plant in King County, Washington\nBecause butterfly bush is so widespread, property owners in King County are not required to control it and we are not generally tracking infestations. We can provide advice on how to control butterfly bush, but there is generally no legal requirement to do so. We are tracking locations of butterfly bush in some wilderness areas as part of the Upper Snoqualmie Invasive Weed Control Project and could always use more Weed Watchers for this effort.\nButterfly bush photos - please click on a thumbnail for larger image\nPlease Note: Photos on this page were provided courtesy of the Thurston County Noxious Weed Board, the Washington State Noxious Weed Board, Tim Miller, Jennifer E. Leach, and the King County Noxious Weed Board. Please do not use these photos without permission from the photographer. Contact the Noxious Weed Program for permission to use and photographer contact information.']"	['<urn:uuid:ae916523-b986-46a3-a340-a7252bae2b29>', '<urn:uuid:cfa4b170-cc3e-487c-bd3c-58a862ffb515>']	factoid	direct	short-search-query	distant-from-document	comparison	expert	2025-05-12T20:46:45.703947	7	55	2008
30	ancient greek philosophy teaching methods modern application	The ancient Greek philosophers, particularly Aristotle, combined teaching practical skills like rhetoric with profound philosophical education. Aristotle's educational approach involved systematic scientific examination of subjects and hands-on tutoring, as demonstrated in his teaching of Alexander the Great about medicine, philosophy, morals, religion, logic, and art. This classical teaching methodology continues to influence modern education through various tools and frameworks, such as the three appeals (ethos, logos, pathos), stasis theory for finding common ground, and the Roman six-part speech pattern as an alternative to standard essay formats.	"['24 Jan 2020. A tireless scholar, whose scientific explorations were as wide-ranging as his philosophical speculations were profound; a teacher. Aristotle believed that education was central – the fulfilled person was an educated person.\nIn 342 BCE and interesting fact happens: Philosopher Aristotle is called in the realm of Pella, to teach the future emperor Alexander III of Macedonia. Aristotle tutors him for three years, until the 18-year-old student Alexander begins to fight a.\nMuch of Western philosophy finds its basis in the thoughts and teachings of Socrates, Plato, and Aristotle. You can\'t begin a study of world philosophy without talking about these guys: the Big Three ancient Greek philosophers.\nCareers With A Degree In Linguistics Linguistics is concerned with the nature of language and communication. If you’re studying Linguistics, you’re probably one of those people interested in the languages and how they work. Good news! You can choose among several different career paths. Below are 7 common jobs for graduates of linguistics, as well as their benefits. Not the cushiest\n22 Jun 2019. Plato (~425–348 BCE) and Aristotle (384–322 BCE) are arguably the two most influential Greek philosophers in. Plato, Republic: ""If women are expected to do the same work as men, we must teach them the same things.”.\nAristotle was a classical Greek philosopher taught by Plato. He continued the same project of philosophy that Plato was doing, but believed that he was correcting many of Plato\'s errors. He wrote on many subjects including science, logic,\nAristotle taught Alexander and his friends about medicine, philosophy, morals, religion, logic, and art. Under Aristotle\'s tutelage, Alexander developed a passion for the works of Homer. Aristotle gave him an annotated copy, which Alexander.\nHow Do You Become A James Scholar Example Thesis Statement For Literary Analysis Philosophy Thesis Paper Topics If you\'re having difficulty selecting a topic for your philosophy paper, this manual will come in handy. Learn the most effective ways to find a good idea. How Many Hours Of Lectures Per Credit Professor From Back To The Future 29 Sep 2015. Christopher Lloyd\nThe Peripatetic school was a school of philosophy in Ancient Greece. Its teachings derived from its founder, Aristotle (384–322 BC), and. Aristotle did teach and lecture there, but there was also philosophical and scientific research done in partnership with other members of the school. It seems likely that many of the.\nSeveral of the philosophers we have already discussed considered it the pinnacle of their careers to come and teach in this great city. But Athens wasn\'t always great. It began as a collection of villages in some of the poorest agricultural land in.\nWriting A Thesis Statement Examples B L Academy Of Higher Education Meerut Scholarly Articles Oversexualization Of Makeup Names Peterson expressed his views on makeup in an interview with VICE magazine which in summary form are: * Women wear makeup to simulate or accentuate the. Special Honors at the 235th AAS Meeting With support from the Kavli Foundation, the Society’s Vice-Presidents\nEmail Asking Dissertation For Assistance Nontraditional Students Resources Retention To my dissertation committee, my sincere thanks for sharing your knowledge and being. APPENDIX A: EMAIL TO PROSPECTIVE STUDY PARTICIPANTS. foundation for uncovering solutions to the African-American student retention issue. significantly lower than a university, substantial financial resources are necessary to. the extent of students\' understanding of financial aid and the student loan repayment. Nontraditional\n6 Feb 2019. In this chapter, the significance of the Aristotle\'s approach to education is discussed. This interest is arousing from professional philosophers (see: [10, 14, 20]) as well as from the practicing teachers. The person who has been educated for 11 years (I am taking the average first year student of university.\n1 Nov 2016. The cast of \'Friends\': Aristotle says older people often pursue the friendship of usefulness, young people most. O\'Rourke, who retired earlier this year as professor of philosophy from UCD, where he taught for 36 years, has.\n21 May 2014. Ancient Greek philosopher Aristotle, together with Socrates and Plato, laid much of the groundwork for. In Rhetoric, Aristotle observes and analyzes public speaking with scientific rigor in order to teach readers how to be.\n22 May 2019. 384-322 BCE) was a Greek philosopher who pioneered systematic, scientific examination in literally every. The Golden Mean was among the many precepts Aristotle taught to his students at the Lyceum. His habit of.\n6 Dec 2013. The ancient Greek philosophers Plato and Aristotle may seem like the quintessential Dead White Males, but in. connected Athenian family until he met a man named Socrates, who taught him that the surest path to wisdom.\nAcademic Calendar Gsu 2019 FILE PHOTO JULIA ALVEY/THE HOYA | GUSA called for the University to formally recognize the Lunar New Year on the university. Sharman has been filling the role as Interim Vice President of Academic and Student Affairs since August of 2019. In this. The Razorbacks, who maintained a cumulative grade point average of 3.59, won their\nAristotle is a towering figure in ancient Greek philosophy, who made important contributions to logic, metaphysics, as some Sophists have taught, but a fundamentally second-personal way of using language that allows the auditor to reach a.\n9 Nov 2009. Aristotle (384-322 B.C.) was a Greek philosopher who made significant and lasting contributions to nearly every aspect of human knowledge, from logic to biology to ethics and aesthetics.\nAristotle was a Greek philosopher and polymath during the Classical period in Ancient Greece. He was the founder of the Lyceum and the Peripatetic school of philosophy and Aristotelian tradition.\n18 Apr 2017. Aristotle was the first Western thinker to divide philosophy into branches, such as logic, metaphysics, natural philosophy, Some commentators consider the text as a pupil\'s record of a lecture course taught by Aristotle. 2.\nWhat Philosophy Can Teach You About Being a Better Leader offers a different route that will allow you to reconnect with the. With the help of Aristotle, Socrates, Kant and Nietzsche, as well as a whole host of other brilliant minds, they turn.', 'Teaching History of Rhetoric–This post describes my “History of Rhetoric” seminar and gives an overview of classical rhetoric.\nArgument Versus Persuasion: A False Dichotomy: This article disputes the tendency to see argument as truth-seeking and persuasion as manipulative.\nAristotle’s Three Appeals: An introduction to ethos, logos, and pathos.\nA Problem with the Aristotelian Appeals: This was written in response to a reported tendency among students to combine the five-paragraph essay with the three appeals to argue that the writer “uses” all three appeals.\nThree Ways to Persuade: Integrating the Three Appeals: This is an update of the “Three Ways to Persuade” article, in part to address the problem noted in the previous post. Note: Also see the mini-module.\nNew! Knowledge, Belief, and the Role of Rhetoric: This is a mini-module that is designed to build upon “Three Ways to Persuade.” It explores the problem of knowledge versus belief that Plato raises in the Gorgias. It also works with Aristotle’s concept of the enthymeme, the argument based on assumed premises, without actually emphasizing that term.\nEthos, Logos, and Pathos: Heidegger on Aristotle: This post re-imagines the relationships between the three appeals based on James Crosswhite’s reading of Heidegger’s lectures on Aristotle’s Rhetoric.\nPathos as Inquiry and Strategy: This post discusses pathos as a concept useful for audience analysis, using the first emotion Aristotle discuss, anger.\nMore on Pathos as Inquiry: This post expands the method used in the previous post to a wider range of emotions.\nPathos as Inquiry: A Mini-Module: This post links to an ERWC-style mini-module with learning goals, an article to read, several activities, and a writing assignment.\nSophistic Appeals: Mythos, Logos, Nomos: The sophists invoked mythos, then deployed logos to change nomos, which are commonly held social values. This set of concepts might actually be more relevant to today’s politics than the Aristotelian appeals.\nStasis Theory–A Mini-Module: An introduction to an ancient invention strategy that is still quite useful and relevant today. The mini-module includes an article, “Stasis Theory: Finding Common Ground and Asking Pertinent Questions,” plus scenarios, activities, and a brief writing assignment. Update: Modifying Stasis Theory for the Classroom.\nThe Classical Pattern of Persuasion–This introduces the Roman six-part speech as a flexible and more effective alternative to the essay organization formulas that students are often taught. It presents the Classical pattern as a chart, a series of questions for the writer, and as a series of questions for a critical reader. The Latin terms are included, but not emphasized.\nWhat About the Five-Paragraph Essay?–In this post I offer strategies for moving beyond the five-paragraph essay.\nEnthymemes!: This post explains Aristotle’s concept of logos in terms of its basic element, the enthymeme or “rhetorical syllogism.”\nThe Roman Six-Part Speech as an Essay: This post offers the Roman six-part speech as an alternative form to the five-paragraph essay. It contains questions that can be used to analyze articles and op-ed pieces as well as generative questions that students can use to plan essays. See also the mini-module, The Classical Pattern of Persuasion.\nDescriptive Outlining and Arrangement: This post is a follow-up on the previous one that applies Ken Bruffee’s practice of descriptive outlining to look at the organizational structure of three related op-ed pieces.\nAristotle’s Poetics in the Classroom–This post discusses what Aristotle sees as the six components of tragedy–plot, character, thought, diction, song, and spectacle. It ventures into literary criticism, but when Aristotle discusses “thought” he always references the Rhetoric. Questions are provided to help students apply Aristotle’s ideas to contemporary works, such as novels and films.\nDissoi Logoi–This Greek phrase means “two arguments.” Every position has advantages for some stakeholders and disadvantages for others. This post includes a classroom activity for exploring different issues and discovering who benefits and who doesn’t depending on what is done.\nKenneth Burke’s Pentad and Gatsby: An application of Burke’s pentad–act, agent, agency, scene, and purpose–to Fitzgerald’s novel.\nUsing Kenneth Burke’s Concept of Identification: An introduction to the concept and some questions that students could use to explore how identification functions in a particular piece.\nIdentification and Division in Our Current Crisis: A post that applies Burke to the problems with policing.\nHow Texts Construct Readers: A mini-module that applies concepts from Glen Stillar’s Analyzing Everyday Texts to an anti-smoking sign.\nWhat We Talk about When We Talk about “Exigence”: A post about the decades long conversation in the journal Philosophy and Rhetoric about this term, with some questions for students at the end.\nTeaching Toulmin Argumentation: Toulmin’s system is often taught as a sort of checklist for a good argument. It doesn’t really work that way.']"	['<urn:uuid:5db674e9-f8af-4e83-9988-cc35db4ed84e>', '<urn:uuid:f66bff31-28af-43d3-a8ed-dd95aa23d9a0>']	open-ended	with-premise	short-search-query	similar-to-document	multi-aspect	expert	2025-05-12T20:46:45.703947	7	86	1769
31	What are some of the main reasons why scientists were collecting deep-sea clams from the San Juan Seamount?	Scientists were collecting deep-sea clams from San Juan Seamount for two main reasons: first, to analyze their genetic composition and get a better picture of evolutionary processes affecting their distribution, and second, to analyze their reproductive state to estimate how much these clams can contribute to the next generation.	"[""April 29, 2004, Day 3\nToday was an adventure in deep-sea clam hunting. With the skill of the ROV pilots, we collected a few of the many clams we saw with the suction sampler. We were after these elusive clams on San Juan Seamount for two main reasons. First, seamount clam samples collected from previous seamount cruises at President Jackson, Davidson, Pioneer, and Rodriguez Seamounts have been analyzed in our molecular ecology lab. Until today's dive (T662), the Rodriguez seamount was the most southern population of deep-sea seamount clams we have analyzed. By adding additional collections of seamount clams, we can get a more thorough picture of the evolutionary processes that may influence the distribution and genetic composition of these unique animals. Second, we wanted to collect some seamount clam specimens for a colleague who will analyze the reproductive state of the seamount clams. By looking at the reproductive organs of the clam, we will be able to estimate the contribution that a seamount clam can make to the next generation of seamount clams. This clam attaches to rocks like a brachiopod. It is about to be collected with gentle suction and put into a biojar for safe transport to the surface. With a dive planned for the Northeast Bank tomorrow, we are hoping to find additional populations of seamount clams to add to our collection and further our knowledge of seamount clams and the seamounts on which they live.\nJoe holding a jar containing some of the tiny animals we found attached to the rocks.\nSeamount clams in a dish of seawater prior to being frozen for later study. These clams have an extremely thin, fragile, nearly transparent shell and a pink body.\nWe steamed from Rodriguez Seamount to San Juan Seamount overnight and arrived for a normal dive schedule with ROV preparation at 6 am and launch at 6:30 am. The dive was a great success and covered a long distance up the axis of the volcano from the southwest and heading towards the summit. Tiburon was on bottom about 8 am and we started right in collecting rocks and animals. The ridge line actually consists of a series of small volcanic cones and we wanted to collect samples from as many as possible, since each probably formed during a different eruption and is likely to have a slightly different composition. We are always trying to find dense lava samples, which are the best samples to obtain radiometric age dates from and to analyze for full chemical compositions. We also try to find volcaniclastic rocks (sedimentary rocks consisting of fragments of lava ranging from basketball-sized clasts to sand and silt sized grains). The particles in these rocks commonly erupted explosively and the magma was quenched in water to form volcanic glass. The glass is easy to analyze and also contains the gases that were dissolved in the magma, which otherwise escapes as lava flows cool and crystallize. Nearly all these rocks are also coated with crusts of Fe-Mn-oxides that precipitate out of seawater slowly over millions of years. Jim and Brandie study these deposits to understand how they form. As the dive progressed, we crossed over pillow lavas, then volcaniclastic deposits, then talus (broken rock fragments on steep slopes), again and again. By the end of the dive we had collected 33 rocks, mostly lava samples since the volcaniclastic rocks are more difficult to sample.\nAngular basalt rock just collected from an outcrop of lava with the vehicle's manipulator arm and the same rock in the lab after the dive, scrubbed and described\nAlong the way we also collected many samples for the biologists who have accompanied us, as well as colleagues who could not come along. Biologic specimens collected included bamboo corals for climate study (by Tessa), specimens of a deep sea clam for genetic study (by Joe), small branches of Paragorgia (bubble-gum coral) for genetic study, several stalked crinoids for comparison with specimens collected last year on Rodriguez Seamount, a tunicate we had not seen previously, and various sponges, anemones, sea stars, brittle stars, a squat lobster, and several chitons that came up with the rock samples. The science party is close to wrapping up the sample labeling, bagging, dissecting, preserving, describing, and photographing at 9:30 pm.\nBrisingid and basket stars, white trumpet and frilled sponges, and a bamboo coral (in the background) are living attached to a lava flow.\nKathie holds a styrofoam head that was shrunk on our dive today. These are popular souvenirs of deep dives: the air trapped in the foam is compressed by the tremendous pressure of the overlying water, leading to entertaining shrunken, misshapen forms. Kathie has decorated hers with a celebration of seamounts.""]"	['<urn:uuid:288093ec-cc89-4faf-9d82-93f151e98678>']	factoid	direct	verbose-and-natural	similar-to-document	single-doc	novice	2025-05-12T20:46:45.703947	18	49	784
32	structural engineer designing complex building need understand different types loads methods force analysis effectiveness static structures	In structural design, there are two main types of loads to consider: dead loads (weights of structural members and permanently attached objects) and live loads (varying loads like building loads, bridge loads, wind loads, snow loads, and earthquake loads). For force analysis in static structures, there are two primary approaches: the section approach and the joint/node approach. In the section approach, the structure is sectioned to analyze unknown forces, while the joint approach considers net forces at connection points. For complex structures, a combination of both approaches may be necessary for effective analysis. The analysis results help verify a structure's fitness for use and are crucial for determining support reactions, stresses, and displacements.	"['- slide 1 of 5\nStructures are made to support loads. Structure is an assembly of number of members arranged in certain manner. When load acts on a structure this load is distributed to the constituent members of the structure in different proportions. Members experiencing large forces can be made stronger, members experiencing less force can be made lighter and redundant members with no force to support can be removed altogether. Thus static force analysis of structures can help to build cost effective, light and strong structures.\n- slide 2 of 5\nForce acting on a member of a structure can be compressible or tensile. For the purpose of force analysis a sign convention can be assigned to the forces. Tensile forces, acting outwards the members and having a tendency to extend the member, is assigned positive sign. The force acting inwards any member and tending to compress the member is called as compressive force and assigned negative sign. Although a sign convention opposite to this one can also be followed with equal validity but the same sign convention should be adhered to throughout the analysis of a structure.\nThere are mainly two approaches for static force analysis in structures. One approach is to section the structure under consideration and find the unknown forces by balancing the forces. Other approach is based on the principle that net force at any joint or node for static structure is zero. In either of the approaches force calculation is started from the support points as it is easier to determine the forces at the support points and further calculation of forces in the members of the structure becomes easier after knowing the forces at supports.\n- slide 3 of 5\nIn the Section Approach the structure under consideration is sectioned at certain part such that the number of unknown forces is not more than two, for two dimensional structures. Unknown forces are assigned variables and components of the forces are taken along and perpendicular to any one of the unknown forces. For each of the two directions force balance equations are framed and solved for the unknowns. The components of the forces can also be taken along any fixed coordinate axis. For three dimensional structures the section taken can have up to three unknown forces.\n- slide 4 of 5\nIn the other approach, to find forces in the members of a structure, net force at any joint is set to zero. Any joint connecting two or more members can be called as a node. One by one different nodes are considered for force analysis. To start with such a node will be taken which has not more than two unknown forces. Unknown forces are determined by writing net forces along any set of orthogonal axes and equating them to zero.\n- slide 5 of 5\nFor a simple structure one of the two approaches may be sufficient to determine the forces in members. But for complex structures single force analysis approach can become cumbersome for force analysis, therefore, a tricky combination and use of the two approaches can simplify the static force analysis in structures.\nForce Analysis in Statics in Engineering\nA bicycle moves down the street and carry us with a little effort on our part. Water falls from the height of dam and run large turbines. How these machines work? How the different bodies interact? Engineering Mechanics, the study of forces and motion of bodies in mechanisms, answers these questions', 'This article needs additional citations for verification. (December 2018) (Learn how and when to remove this template message)\nStructural analysis is the determination of the effects of loads on physical structures and their components. Structures subject to this type of analysis include all that must withstand loads, such as buildings, bridges, vehicles, furniture, attire, soil strata, prostheses and biological tissue. Structural analysis employs the fields of applied mechanics, materials science and applied mathematics to compute a structure\'s deformations, internal forces, stresses, support reactions, accelerations, and stability. The results of the analysis are used to verify a structure\'s fitness for use, often precluding physical tests. Structural analysis is thus a key part of the engineering design of structures.\n- 1 Structures and Loads\n- 2 Analytical methods\n- 3 Strength of materials methods (classical methods)\n- 4 Elasticity methods\n- 5 Methods using numerical approximation\n- 6 Timeline\n- 7 See also\n- 8 References\nStructures and Loads\nA structure refers to a body or system of connected parts used to support a load. Important examples related to Civil Engineering include buildings, bridges, and towers; and in other branches of engineering, ship and aircraft frames, tanks, pressure vessels, mechanical systems, and electrical supporting structures are important. To design a structure, an engineer must account for its safety, aesthetics, and serviceability, while considering economic and environmental constraints. Other branches of engineering work on a wide variety of non-building structures.\nClassification of structures\nA structural system is the combination of structural elements and their materials. It is important for a structural engineer to be able to classify a structure by either its form or its function, by recognizing the various elements composing that structure. The structural elements guiding the systemic forces through the materials are not only such as a connecting rod, a truss, a beam, or a column, but also a cable, an arch, a cavity or channel, and even an angle, a surface structure, or a frame.\nOnce the dimensional requirement for a structure have been defined, it becomes necessary to determine the loads the structure must support. Structural design, therefore begins with specifying loads that act on the structure. The design loading for a structure is often specified in building codes. There are two types of codes: general building codes and design codes, engineers must satisfy all of the code\'s requirements in order for the structure to remain reliable.\nThere are two types of loads that structure engineering must encounter in the design. The first type of loads are dead loads that consist of the weights of the various structural members and the weights of any objects that are permanently attached to the structure. For example, columns, beams, girders, the floor slab, roofing, walls, windows, plumbing, electrical fixtures, and other miscellaneous attachments. The second type of loads are live loads which vary in their magnitude and location. There are many different types of live loads like building loads, highway bridge loads, railroad bridge loads, impact loads, wind loads, snow loads, earthquake loads, and other natural loads.\nTo perform an accurate analysis a structural engineer must determine information such as structural loads, geometry, support conditions, and material properties. The results of such an analysis typically include support reactions, stresses and displacements. This information is then compared to criteria that indicate the conditions of failure. Advanced structural analysis may examine dynamic response, stability and non-linear behavior. There are three approaches to the analysis: the mechanics of materials approach (also known as strength of materials), the elasticity theory approach (which is actually a special case of the more general field of continuum mechanics), and the finite element approach. The first two make use of analytical formulations which apply mostly simple linear elastic models, leading to closed-form solutions, and can often be solved by hand. The finite element approach is actually a numerical method for solving differential equations generated by theories of mechanics such as elasticity theory and strength of materials. However, the finite-element method depends heavily on the processing power of computers and is more applicable to structures of arbitrary size and complexity.\nRegardless of approach, the formulation is based on the same three fundamental relations: equilibrium, constitutive, and compatibility. The solutions are approximate when any of these relations are only approximately satisfied, or only an approximation of reality.\nEach method has noteworthy limitations. The method of mechanics of materials is limited to very simple structural elements under relatively simple loading conditions. The structural elements and loading conditions allowed, however, are sufficient to solve many useful engineering problems. The theory of elasticity allows the solution of structural elements of general geometry under general loading conditions, in principle. Analytical solution, however, is limited to relatively simple cases. The solution of elasticity problems also requires the solution of a system of partial differential equations, which is considerably more mathematically demanding than the solution of mechanics of materials problems, which require at most the solution of an ordinary differential equation. The finite element method is perhaps the most restrictive and most useful at the same time. This method itself relies upon other structural theories (such as the other two discussed here) for equations to solve. It does, however, make it generally possible to solve these equations, even with highly complex geometry and loading conditions, with the restriction that there is always some numerical error. Effective and reliable use of this method requires a solid understanding of its limitations.\nStrength of materials methods (classical methods)\nThe simplest of the three methods here discussed, the mechanics of materials method is available for simple structural members subject to specific loadings such as axially loaded bars, prismatic beams in a state of pure bending, and circular shafts subject to torsion. The solutions can under certain conditions be superimposed using the superposition principle to analyze a member undergoing combined loading. Solutions for special cases exist for common structures such as thin-walled pressure vessels.\nFor the analysis of entire systems, this approach can be used in conjunction with statics, giving rise to the method of sections and method of joints for truss analysis, moment distribution method for small rigid frames, and portal frame and cantilever method for large rigid frames. Except for moment distribution, which came into use in the 1930s, these methods were developed in their current forms in the second half of the nineteenth century. They are still used for small structures and for preliminary design of large structures.\nThe solutions are based on linear isotropic infinitesimal elasticity and Euler–Bernoulli beam theory. In other words, they contain the assumptions (among others) that the materials in question are elastic, that stress is related linearly to strain, that the material (but not the structure) behaves identically regardless of direction of the applied load, that all deformations are small, and that beams are long relative to their depth. As with any simplifying assumption in engineering, the more the model strays from reality, the less useful (and more dangerous) the result.\nThere are 2 commonly used methods to find the truss element forces, namely the Method of Joints and the Method of Sections. Below is an example that is solved using both of these methods. The first diagram below is the presented problem for which we need to find the truss element forces. The second diagram is the loading diagram and contains the reaction forces from the joints.\nSince there is a pin joint at A, it will have 2 reaction forces. One in the x direction and the other in the y direction. At point B, we have a roller joint and hence we only have 1 reaction force in the y direction. Let us assume these forces to be in their respective positive directions (if they are not in the positive directions like we have assumed, then we will get a negative value for them).\nSince the system is in static equilibrium, the sum of forces in any direction is zero and the sum of moments about any point is zero. Therefore, the magnitude and direction of the reaction forces can be calculated.\nMethod of Joints\nThis type of method uses the force balance in the x and y directions at each of the joints in the truss structure.\nAlthough we have found the forces in each of the truss elements, it is a good practice to verify the results by completing the remaining force balances.\nMethod of Sections\nMethod 1: Ignore the right side\nMethod 2: Ignore the left side\nThe truss elements forces in the remaining members can be found by using the above method with a section passing through the remaining members.\nElasticity methods are available generally for an elastic solid of any shape. Individual members such as beams, columns, shafts, plates and shells may be modeled. The solutions are derived from the equations of linear elasticity. The equations of elasticity are a system of 15 partial differential equations. Due to the nature of the mathematics involved, analytical solutions may only be produced for relatively simple geometries. For complex geometries, a numerical solution method such as the finite element method is necessary.\nMethods using numerical approximation\nIt is common practice to use approximate solutions of differential equations as the basis for structural analysis. This is usually done using numerical approximation techniques. The most commonly used numerical approximation in structural analysis is the Finite Element Method.\nThe finite element method approximates a structure as an assembly of elements or components with various forms of connection between them and each element of which has an associated stiffness. Thus, a continuous system such as a plate or shell is modeled as a discrete system with a finite number of elements interconnected at finite number of nodes and the overall stiffness is the result of the addition of the stiffness of the various elements. The behaviour of individual elements is characterized by the element\'s stiffness (or flexibility) relation. The assemblage of the various stiffness\'s into a master stiffness matrix that represents the entire structure leads to the system\'s stiffness or flexibility relation. To establish the stiffness (or flexibility) of a particular element, we can use the mechanics of materials approach for simple one-dimensional bar elements, and the elasticity approach for more complex two- and three-dimensional elements. The analytical and computational development are best effected throughout by means of matrix algebra, solving partial differential equations.\nEarly applications of matrix methods were applied to articulated frameworks with truss, beam and column elements; later and more advanced matrix methods, referred to as ""finite element analysis"", model an entire structure with one-, two-, and three-dimensional elements and can be used for articulated systems together with continuous systems such as a pressure vessel, plates, shells, and three-dimensional solids. Commercial computer software for structural analysis typically uses matrix finite-element analysis, which can be further classified into two main approaches: the displacement or stiffness method and the force or flexibility method. The stiffness method is the most popular by far thanks to its ease of implementation as well as of formulation for advanced applications. The finite-element technology is now sophisticated enough to handle just about any system as long as sufficient computing power is available. Its applicability includes, but is not limited to, linear and non-linear analysis, solid and fluid interactions, materials that are isotropic, orthotropic, or anisotropic, and external effects that are static, dynamic, and environmental factors. This, however, does not imply that the computed solution will automatically be reliable because much depends on the model and the reliability of the data input.\n- 1452–1519 Leonardo da Vinci made many contributions\n- 1638: Galileo Galilei published the book ""Two New Sciences"" in which he examined the failure of simple structures\n- 1660: Hooke\'s law by Robert Hooke\n- 1687: Isaac Newton published ""Philosophiae Naturalis Principia Mathematica"" which contains the Newton\'s laws of motion\n- 1750: Euler–Bernoulli beam equation\n- 1700–1782: Daniel Bernoulli introduced the principle of virtual work\n- 1707–1783: Leonhard Euler developed the theory of buckling of columns\n- 1826: Claude-Louis Navier published a treatise on the elastic behaviors of structures\n- 1873: Carlo Alberto Castigliano presented his dissertation ""Intorno ai sistemi elastici"", which contains his theorem for computing displacement as partial derivative of the strain energy. This theorem includes the method of \'least work\' as a special case\n- 1936: Hardy Cross\' publication of the moment distribution method which was later recognized as a form of the relaxation method applicable to the problem of flow in pipe-network\n- 1941: Alexander Hrennikoff submitted his D.Sc thesis in MIT on the discretization of plane elasticity problems using a lattice framework\n- 1942: R. Courant divided a domain into finite subregions\n- 1956: J. Turner, R. W. Clough, H. C. Martin, and L. J. Topp\'s paper on the ""Stiffness and Deflection of Complex Structures"" introduces the name ""finite-element method"" and is widely recognized as the first comprehensive treatment of the method as it is known today\n- Limit state design\n- Structural engineering theory\n- Structural integrity and failure\n- Stress–strain analysis\n- Probabilistic Assessment of Structures\n|Wikibooks has more on the topic of: Structural analysis|\n|Wikiversity has learning resources about Structural analysis|']"	['<urn:uuid:8e980f80-cf16-4be9-9cac-4bb166f80bc3>', '<urn:uuid:86cf01a3-b920-4e68-8fd9-7bbc74c5d9b8>']	open-ended	with-premise	long-search-query	similar-to-document	three-doc	expert	2025-05-12T20:46:45.703947	16	113	2763
33	How widespread are phages in nature, and what medical treatments are currently available using them?	Phages are the most ubiquitous natural entity on earth, though they are less known to the general public than bacteria. The Eliava Institute currently produces six different bacteriophage preparations (PYO, FERSISI, STAPHYLOCOCCAL, SES, INTESTI, and ENKO Bacteriophage) for therapeutic use. Additionally, they offer personalized medicine through auto-phage preparations for individual patients whose bacterial infections are resistant to standard preparations.	"['Phage therapy gets exposure on TED.com\nAlexander Belcredi speaks at TED@BCG - October 3, 2018 at Princess of Wales Theatre, Toronto, Canada. Photo: Ryan Lash / TED\nVienna, 16 November 2018: Today the first TED talk on phages and phage therapy went live on TED.com. In the talk, PhagoMed’s Co-Founder and CEO Alexander Belcredi recounts how he became fascinated by phages, their history of the discovery of phages and their potential in therapeutic applications. The talk was initially recorded at the TEDatBCG conference in Toronto, Canada in early October and subsequently selected to be published on TED.com\nBacteria are a well-known fact of life. Their existence is taught to children and covered extensively in every biology class. Phages are similarly well-known within the scientific community for their widespread existence and ability to infect and kill bacteria. However, so far phages are less known to the general public – despite phages being the most ubiquitous natural entity on earth\nTED talk recognizes the importance and relevance of phages The TEDatBCG conference selected PhagoMed’s Co-Founder and CEO Alexander Belcredi as a speaker to spread the knowledge of phages and phage therapy. The TED talk delivered by Mr. Belcredi in Toronto, Canada covers how he first got to know about phages, the biology behind them as well as the scientific history of how phages were discovered and first used. The talk also places phages within the context of the wider antibiotics crisis, in which phages could play a pivotal role for the development of alternatives to antibiotics. “It is a great honour to be able to share the knowledge of phages through a TED talk. At PhagoMed we firmly believe that phages are one of the most underutilized resources in our fight against multi-drug resistant bacteria.” says TED speaker Alexander Belcredi. “We hope that sharing the idea of phages and phage therapy will provide additional positive recognition for the efforts PhagoMed and other biotechs are making in bringing phage therapy back to life in key medical applications.\nAbout PhagoMed Biopharma GmbH PhagoMed Biopharma GmbH is a biotech company focused on the development of human therapeutic applications of phage therapy. Its development programs are based on the last-resort treatment experiences of its clinical co-founders with phages as well as research collaborations with highly prestigious scientists and research institutions in Germany (Leibniz-Institute DSMZ-German collection of microorganisms and cell cultures GmbH, Braunschweig as well as Justus-Liebig-Universität Gießen) and Belgium (Ghent University). Since its incorporation in November 2017, the company has raised more than €4 million in public grants and private investments. Amongst others, the company is supported by grants from the Austrian Research Promotion Agency (FFG) and the Austria Wirtschaftsservice (aws). Currently, PhagoMed currently has a team of ten highly skilled scientist working on its programs at the Campus Vienna Biocenter in Vienna, Austria as well as the research partners.\nAbout Phages Phages are viruses that only infect bacteria. Phages multiply by injecting their DNA into the bacteria and reprogramming the bacteria cells to produce new phages. As soon as sufficient phages have been produced by the bacteria, the phages express special enzymes that degrade the bacterial cell wall and lead to the death of the bacterial cell. Individual phages are only active against a limited spectrum of bacteria (typically even only within one species). The combination of high selectivity as well as high effectiveness in killing bacteria makes phages highly attractive as an alternative to antibiotics for the treatment of serious bacterial infections.', ""The Eliava Institute was established in 1923 by Georgian microbiologist George Eliava and French Canadian scientist Felix D'Herell. The friendship and scientific interest joined these two people, according to their initial plans, newly established Institute was to be developed into the World Center of Bacteriophage research.\nIn the former Soviet Union, one of the largest bacterial and phage collections have been gathered at the Eliava Institute. Several effective bacteriophage preparations were elaborated in scientific laboratories and produced commercially by production units of the Institute. The products of the Eliava IBMV have been used for prophylaxis and treatment of human infection diseases throughout the entire Former Soviet Union, in Public health network and in military forces. The Institute survived a variety of serious challenges following the 1991 break-up of the USSR, and has maintained its focus on phage research and application.\nNowadays G. Eliava Institute of Bacteriophages, Microbiology and Virology is one of the most famous institutes worldwide focused on bacteriophage research and the elaboration of appropriate phage-based products for human and animal protection.\nResearch conducted at the Institute is focused on the use of bacteriophages in the following directions:\n- Human therapy and prophylaxis\n- Food safety\n- Environmental protection\nG. Eliava Institute and its spin-off companies:\nG. Eliava Institute of Bacteriophages, Microbiology and Virology has developed an effective strategy for sustainable development by establishing the spin-off companies and leading them to success. All these organizations including Eliava Foundation represent Eliava Consortium.\n- “Eliava International Phage Therapy Center” - is day ambulatory-type clinic using phage therapy in its practice. Phage therapy is a specific approach used by the clinic to treat infectious diseases including those that are caused by antibiotic-resistant bacteria.\n- “Eliava BioPreparations” - produces highly effective therapeutic and prophylactic bacteriophage preparations. The importance of these preparations is gradually growing in the world because of emergence and spread of multiply drug resistant bacteria that can be suppressed only by bacteriophage preparations.\n- “Eliava Analytical-Diagnostic Center” - accomplishes a great variety of bacteriology, virology and immunology diagnostics. The most important among the others is phage-susceptibility tests that is a specificity of this company ONLY.\n- “Eliava Bacteriological Media Production” - is a company based on high technologies, equipped with modern granted by ISO 9001 standards, which does not have any analogue in the Caucasus Region. The company produces media for bacteriology, virology and cell culture research and diagnostics.\n- “Eliava Management Group” - is a leading member of the Eliava Consortium, its main responsibilities are to manage the relations between the organizations, do the marketing planning and consultations.\n- “Eliava Pharmacy”\nThe goal of this consortium is to develop wide application of Eliava products and services for human, animal, plant and environment protection, to grow and sustain as an organization by using the existing long-term research experience starting since 1923, and to develop it according to the modern quality and standards requirements.\nFor now, there are 6 bacteriophage preparations produced under the brand “Eliava”:\n- PYO Bacteriophage\n- FERSISI Bacteriophage\n- STAPHYLOCOCCAL Bacteriophage\n- SES Bacteriophage\n- INTESTI Bacteriophage\n- ENKO Bacteriophage\nMoreover, if the patient has a bacterial infection that is resistant to all of the above mentioned preparations, the phage therapy center offers personalized medicine, which means to prepare a specific phage preparation (auto-phage) for an individual patient.\nThe Eliava Institute plans to explore the most appropriate organizational development for broader production, application and marketing of biological preparations.""]"	['<urn:uuid:780bf3a5-de6b-4fb5-94e9-7bd85056fdb5>', '<urn:uuid:3e73119f-3c29-4e55-8a0d-399790ce0a69>']	factoid	direct	verbose-and-natural	distant-from-document	multi-aspect	novice	2025-05-12T20:46:45.703947	15	59	1148
34	What materials are displayed in the Materialism exhibit?	The Materialism exhibit breaks down various manufactured objects into their raw materials, including a VW Beetle car, an LED, and a pencil. These materials are mounted as clusters of blocks on plinths.	['Ralph Nauta and Lonneke Gordijn, the duo behind Dutch artist collective Studio Drift, sometimes talk over each other, which in the hubbub of the busy Stedelijk restaurant can make things difficult to follow. The noise and counter-flows of conversation contrast with the sense of wonder that permeates their solo show upstairs, Coded Nature, which fills eight rooms with installations and also includes a new film. It makes you wonder if the collaboration that started when a science fiction fan (Nauta) met a nature-lover (Gordijn) is more discursive or argumentative. “Both,” says Gordijn, just as Nauta offers that it’s “depending on how much we’ve slept”.\nStudio Drift have long been known for their technological creations which capture or appropriate the beauty of natural phenomena to stunning effect. Fragile Future, a series of modular matrices of lights shining delicately inside real dandelions, has its roots in a project as old as the practice, which was founded in 2006. The Victoria and Albert Museum purchased a Fragile Future chandelier in 2010, and the Stedelijk in 2015. But new works are different, directly challenging consumerism, delivering sci-fi visions, blurring the boundary between technology and magic—and addressing the individual in the context of the group.\n“We’re taking more freedom to express ourselves,” says Nauta. “After ten years we’re finally at a position where we can work on new ideas.” That, as Gordijn says, “takes people, time, money, knowledge”. Nowadays, Studio Drift is a team of twenty, and research is a continuous element of their practice. “It’s not that we take something off the shelf,” Nauta explains. “We literally develop the technologies to make something happen.”\nTheir technological take on nature is apparent before you even reach the Stedelijk’s first floor galleries hosting Coded Nature. Meadow (2017) hangs from the ceiling at the foot of the stairs, a group of textiles folded around LEDs, which robotically open up as wide as tutu skirts. They look like dahlias, but glow in different colours. In another work, Tree of Ténéré (2017), the leaves of a tree glow through a cycle of colours. A word that has been applied to Studio Drift’s work is “bio-design”, but they’re not sure. “It’s a weird word. The only proper bio-designer in the world is nature itself,” comments Nauta. “Nature is so much ahead of every technology we can make,” says Gordijn, emphatically.\n“The only proper bio-designer in the world is nature itself”\nAnd it’s not all directly inspired by nature. Don a virtual reality headset and an array of concrete stumps become broken columns swaying in an ominous storm, in the enhanced reality installation Concrete Storm, which was first unveiled in New York last year. Another work from the Armory show that becomes a highlight at Coded Nature is Drifter, a four metre-long concrete block, which floats and drifts in the space of a gallery. The simultaneous impressions of extreme mass and weightlessness are mesmerizing. Concrete, they point out, has a science-fiction root in that Thomas More predicted its qualities in 1516 in the book Utopia. Gordijn says it symbolizes the architecture of the landscape humans have created, with its possibilities and limitations. But Drifter does not reveal its technology. Seeing it at the Stedelijk, I remark to Nauta, “You’re a magician.” He merely smiles and says, “Thank you.”\nThe technology was more comprehensible but its effect no less extraordinary in Franchise Freedom, a “living sculpture” performed at Art Basel Miami in 2017. A swarm of 300 drones, each loaded with an LED, sensors and an algorithm that drove them to self-organize as they reacted with each other, took flight like a flock of birds. At the Stedelijk, we see something similar in Flylight (2011), static glass tubes in which lights create the patterns of swarm flight activated by the presence of someone viewing it. Such group behaviour is also at the heart of a new film, Drifters, made with Dutch filmmaker Sil van der Woerd and shown for the first time in Coded Nature. A concrete block like that seen in the adjacent gallery emerges from a lake, drifts through a pristine forest as it finds other floating blocks and together they swarm above the Scottish Highlands, eventually coming together in a big floating wall. “It’s the block heaven,” comments Gordijn. Drifters showcases breathtaking cinematography, but it is also a story. What’s it about?\nThe answer, it seems, is freedom. Like the drones in Franchise Freedom, Drifters presents a group choreographed by its individual members, but in the film we start with one emerging alone. That block can be seen as “a new lifeform that is trying to find its purpose in a world it doesn’t really fit in,” offers Nauta. “Is freedom total disconnection from society?” asks Gordijn. “Freedom of being lost, being alone. Or is freedom being in a group and being accepted?” Indeed, are Studio Drift suggesting that we should all come together? Nauta suggests, “You actually have the choice to shape the world as you want to live. That, I think, is the message in all the works—opening up the mind again to almost its childhood, a receptiveness to how the world could function. It’s like dreaming again, and a lot of people are not dreaming.”\n“Is freedom total disconnection from society? Freedom of being lost, being alone. Or is freedom being in a group and being accepted?”\nIf Drifters is ultimately a call for us to dream, the other work that debuts in Coded Nature is more like a wake-up call to what we are taking from the world. Materialism takes a number of manufactured objects, from a VW Beetle car to an LED and a pencil, and breaks them down into their constituent raw materials. On a plinth for each artifact, the materials are mounted as a cluster of blocks, incidentally with the same proportions as Drifter, the concrete block. “What was interesting to see [was] that we don’t know how things are made,” comments Gordijn. “It was a process of de-production.”\nMateriality seems to be the exact opposite of Studio Drift’s practice, which centres on production of new works in ever-advancing new media and technologies. It’s yet another step beyond the light installations that they established themselves with, and which they are still glad to receive commissions for. Do they see themselves more as artists than designers? After all, they met while studying design together at the Design Academy in Eindhoven, and have been stars at design events like Salone Mobile. “I’ve never made something functional in my life!” responds Nauta. “Sometimes you get pressured into this category of designer and that doesn’t feel right.” Gordijn states that “the reason we started studying it was curiosity—how the world works.” And that, she concludes, is “why we make things.”']	['<urn:uuid:76030281-55be-44b7-8740-520947d488b8>']	factoid	direct	concise-and-natural	distant-from-document	single-doc	expert	2025-05-12T20:46:45.703947	8	32	1124
35	What types of personal information need special protection?	According to privacy laws, sensitive personal information requires extra security measures. This includes data about ethnic/racial origin, cultural/social identity, philosophical/religious beliefs, and sexual orientation. Additionally, financial information (credit card numbers, bank accounts), government classified documents, business information (trade secrets, financial statements), and personal data (medical history, addresses, phone numbers) also need special protection.	['Understanding Personal Information, Personally Identifiable & Sensitive Personal Information\nIn this article, we take an expanded look at the scope of Personal Information. Generally, most people have a narrow idea of what personal information is and use the term PII interchangeably with PI. Privacy laws define Personal Information very broadly and understanding the broad scope of personal information is pivotal to complying with Information privacy laws.\nGenerally, people struggle to determine which information is personal or non-personal – including what sorts of personal information qualify to be more protected (Sensitive Personal Information) and to understand the boundary of what constitutes personal Information.\nPeople often get confused Personal Information with similar common terms in the Information privacy and security world such as PII (Personally Identifiable Information) and Sensitive Personal Information. However each of these terms have specific meanings and distinct definitions in privacy regulations.\nTo comply with various privacy laws, understanding the expanded scope of personal Information is critical. Let’s review the definitions and dive into the scope covered by each of these terms.\nWhat is Personal Information?\nGenerally speaking, Personal Information is information relating to a person which, in combination or separately, can be used directly or indirectly to identify that natural person. This definition is consistent with several Information privacy laws across the world where Personal Information applies to several types of Information that can be used to single out or identify a natural person.\nPersonal Information varies with context, depending on whether a person can be identified or is reasonably identifiable in the circumstances. In short, any sort of Information, or opinion, that can help determine someone’s identity can be considered Personal Information. Examples of personal information\nName and surname\nDate of birth\nCredit card numbers\nInternet Protocol (IP) address\nThe advertising identifier of your device\nPhotograph where an individual is identifiable\nWhat is sensitive personal information? Though personal Information and Sensitive Information are often used interchangeably, the GDPR (General Information Protection Regulation) and several other privacy laws draw a distinct line between these two terms. According to the GDPR, sensitive Information is a set of special categories of Information that needs to be handled with extra security. These special categories are\nEthnic or racial origin\nCultural or social identity\nPhilosophical or religious beliefs\nInformation related to sexual orientation\nThe GDPR generally prohibits the processing of Sensitive Information except for exemptions provided by employment law, social protection law, health security reasons or for the protection of the vital interests of the Information subject. What is personally identifiable information? Personally Identifiable Information, or PII, is a term used more in the security context. There isn’t a clear and widely accepted definition of PII. NIST, for example, defines PII as follows: Any representation of information that permits the identity of an individual to whom the information applies to be reasonably inferred by either direct or indirect means.\nGDPR Article 4 on the other hand defines “Personal Information” as “any information relating to an identified or identifiable natural person (Information subject)”; an identifiable natural person is one who can be identified, directly or indirectly, in particular by reference to an identifier such as a name, an identification number, location Information, an online identifier or to one or more factors specific to the physical, psychological, genetic, mental, economic, cultural or social identity of that natural person.\nWhile, there is no definitive list of what is or is not personal Information, the GDPR definition is broad and often used as a de-facto definition of PI. Consequently, identifying Personal Information boils down to correctly interpreting the GDPR’s definition.\nKey elements of the GDPR around Definition of Personal Information:\nArticle 4(1) of the GDPR defines personal information as Information that must belong or relate to an individual (or natural person).\n“Any information”. This can mean either objective or subjective information. Subjective information can be of any format, such as audio, video, numeric, graphic, or photographic. Subjective information include very disparate information like employment evaluations, psychiatric evaluation of a child’s drawing of their family, which may reveal information relating to the child (about mental health as evaluated by the psychiatrist) and their parents’ behavior etc. Objective information on the other hand could be as trivial as an individual’s height or weight.\nThe use of the expression “any information” is not restricted to information that is sensitive or private but potentially encompasses all kinds of information provided that it relates to the Information subject. For example, a candidate’s exam script is “Personal Information”, as it constitutes information that is linked to him or her. It can be, among others, a name, a cookie (an online identifier), an email address, a person’s location, occupation, biometrics, gender, health Information, and IoT identifiers.\n“Relating to”. Information that could be utilized to learn something about an individual qualifies to be “relating to” an individual’s personal information. Also, if the processing of any information affects the individual, such information again qualifies to be “relating to” an individual and, hence, personal information.\nRecords containing information, such as medical history or criminal records, about a particular individual clearly indicate “relating to” an individual. Such records that describe an individual’s activities, such as a bank statement, may also qualify to be “relating to” an individual.\n“Identifying an individual directly or indirectly”: Directly identifying an individual is an obvious task. Identifying indirectly requires more detail. Indirect identification means identifying an individual with not only available information alone but a combination of available Information and information accessed from another source, such as a third party.\nA third party making use of Information available with them in combination with the information they can reasonably access to identify an individual is another example of indirectly identifying an individual. For example, identifying someone with only a license plate number known requires a matching name to identify that individual.\n“Identified or identifiable”: At the most basic form, an individual is considered “identified” when the individual is differentiated from others within the group. An individual is considered identifiable when, although the individual has not been identified yet, it is possible to identify the person. Let’s examine the case of a mobile telecommunications provider who decides to sell information regarding the location and movement patterns of its customers. The information is typically sold in an aggregated form and any location Information about any individual is deleted so that the aggregated information cannot be traced back to individuals. If the buyer of the aggregated Information manages to find a way to connect location information to an individual, maintaining the aggregated form becomes trickier as the customer could be identifiable by the means of the provider’s other Information about its users/customers. Now, let’s assume, instead of selling information based on patterns, the provider sells real-time Information on where all its customers are at any given time without enabling the buyer to know to whom certain location Information relates. In such a case, even though the buyer is unable to ascertain to whom the location Information relates, the identifiability, by means of the provider, becomes clearer. The location Information, in this case, may now be considered Personal Information. “Purpose for processing in deciding personal Information”: The purpose of processing Information by an organization makes the Information personal or non-personal. For example, Information non-personal for one organization may convert into personal for a different organization if its possession may have an impact on the individual. For example, a photo of a street in possession of a photographer – non-personal Information at the moment – becomes personal if the investigator tries to identify the cars and other content on the street, affecting the individuals related to content on the street. Similarly, video surveillance with the sole purpose of identifying individuals constitutes the personal information of identifiable persons. A guide to what is (or could be) personal Information It’s hard to explain whether certain information meets the criteria laid under the GDPR’s definition of personal Information. Therefore, we provide you with a list of things that could be considered personal information (alone or in combination with additional information):\nBiographical information, including dates of birth, Social Security numbers, phone numbers, and email addresses;\nLooks, appearance, and behavior, including eye color, weight, and character traits;\nWorkplace and academic information, including salary, tax information, student ID or roll numbers;\nPrivate and subjective Information, including geo-tracking Information, religion, etc;\nHealth, sickness, and genetics, including medical history, genetic Information, etc.\nPersonal Information has even been found in a business context and within Industry 4.0. For example, in the name of safety of security, health-related real-time Information of workers in the manufacturing industry are monitored via wearables. These Information solely comprises sensitive Information but are often fit in the contractual stipulations regarding the worker-employer relationship.\nEven for developers of applications for consumers such as smart homes or in the sphere of consumer electronics that are solely designed for personal usage and enabled to gather all sorts of identifiers — and will process personal information, ultimately — an idea of what personal information is paramount. Personal Information does not include anonymous Information. However, information which has had identifiers removed or replaced in order to pseudonymize the Information is still personal Information for the purposes of GDPR. Additionally, GDPR does not cover information about someone who is deceased, Information that is properly anonymized; information about public authorities and companies. Names may not always be personal information: Name is often the most common way to get the attention of a person. Whether it “identifies” someone depends on the context. For example, names may not always amount to Personal Information. Because with the same name and information, there may be many individuals, it may require combining the name of an individual with other information (such as an address, a place of work, or a telephone number) to clearly identify the individual. The addition of another Information point to the name (such as proximity) accounts to have enough information to identify one specific individual. These Information points are identifiers, such as a name, an identification number, location Information, or an online identifier. Role of context in qualifying Personal Information Knowing whether certain Information qualifies to be personal Information takes an awful lot of information. Organizations collect varied types of information on people. If one type of Information doesn’t individually identify someone, an organization can still make use of other types of Information to zero in on an individual. In certain contexts, you might be able to ask visitors downloading products from a website to state their occupation, without GDPR obstruction. Since GDPR doesn’t find a job title unique to a person, such Information doesn’t fall under its scope of Personal Information. Similarly, Information subjects could be asked what company they work for. In this context, again, the person couldn’t be identified directly unless they were the only employee. Establishing someone’s identity becomes possible when two or more pieces of information are used together to narrow down the number of natural, living persons. It’s unlikely, but let’s suppose there may only be one person with a specific job title at a specific organization who fits the description. In such cases – such as a BDM at IBM – the two pieces of information won’t necessarily comprise Personal Information as it’s not sufficient to adequately establish someone’s identity. In order to recognize the individual, you would need to have one or more identifiers in place, such as a name or email address. Information may be personal for one purpose but may not be personal for in a different context. References', 'February 15, 2019\nWhat is Sensitive Data Classification?\nSensitive data can be a number of things. The easiest way to think about it is to think of personal data you would not want to be shared with just anyone. There are several common pieces of sensitive data:\n- Financial information – credit card numbers, bank account information, and social security numbers.\n- Government information – any document that is classified as secret or top-secret, restricted, or can be considered a breach of confidentiality.\n- Business information – accounting data, trade secrets, financial statements or accounts, and any sensitive information in business plans.\n- Personal information – addresses, medical history, driver’s license numbers, or phone numbers.\nAny of these pieces of data are things you could not be comfortable in data sharing with someone who should not have access. Unfortunately, a data breach can occur leaving you feeling exposed and increases the risk of identity theft. Businesses are continually evolving protection protocols and regulations to ensure data protection.\nData Discovery and Sensitive Data Classification Security Measures\nOne way to try and determine how sensitive, specific data is, and therefore how it should be classified, is to think about how the loss of the confidentiality, integrity, or availability of that information or restricted data would impact your organization in the case of an unauthorised disclosure.\nThe following table is taken from the Federal Information Processing Standards (FIPS) publication 199 published by the National Institute of Standards and Technology (NIST). It provides a framework for determining the impact that can be applied to the sensitivity of information.\nSensitive Data Classification\nHere are the three security objectives and how to determine the level of influence (low, moderate, or high):\nConfidentiality: Preserving authorized restrictions on information access and disclosure, including means for protecting personal privacy and proprietary information.\nPotential impact: The unauthorized disclosure of information could be expected to have a limited (low), serious (moderate), or severe/catastrophic (high) adverse effect on organizational operations, organizational assets, or individuals.\nIntegrity: Guarding against improper information modification or destruction, and includes ensuring information non‐repudiation and authenticity.\nPotential impact: The unauthorized modification or destruction of information could be expected to have a limited (low), serious (moderate), or severe/catastrophic (high) adverse effect on organizational operations, organizational assets, or individuals.\nAvailability: Ensuring timely and reliable access to and use of information.\nPotential impact: The disruption of access to or use of information or an information system could be expected to have a limited (low), serious (moderate), or severe/ catastrophic (high) adverse effect on organizational operations, organizational assets, or individuals.\n(Source: National Institute of Standards and Technology “FIPS Publication 199: Standards for Security Categorization of Federal Information and Information Systems”)\nAs the potential impact level moves from low to high, the sensitivity increases, and therefore, the classification level of data should become higher and more restrictive. If your classification schema ranges from public to top secret, for example, data with a low impact across the board might be classified as public, while data with a high impact in any one area might be considered top secret.\nOnce you’ve developed a framework for classifying data, you develop your business’s classification schema with additional business criteria and an understanding of your specific types of sensitive data.\nProtecting Data with a Data Classification Policy to Determine Impact Level\n- Data classification should be your first step in protecting proprietary information. Since various pieces of data have varying levels of sensitivity, there are different levels of protection. As someone who is in charge of data security for a business or an organization understanding the level of sensitivity required for protection is a crucial piece of the protection puzzle. As a general rule of thumb, there should be a minimum of three data classification levels. Public – what data is okay to share with the public? Anything that is not personal or sensitive such as a person’s name. This means that there would be very little to no risk if this data were accessed, this type of information likely does not need to be encrypted or have major protection.\n- Restricted – this is the most sensitive data, meaning if someone were to hack into a database and access it, it would pose a risk to the person whose information it is. This could include anything that could lead to identity theft like credit card numbers or a social security number.\n- Private and confidential – access to this data would cause a medium risk to the person affected. For example, this could include someone’s address. It allows someone to be aware of where someone lives. However, they would not have any more personal information on someone.\nMany data security policies have been created and adapted across the business world to help protect the business and the consumer. Various companies and agencies have in-house security policies as well. Some of the most protected sensitive data is in the medical field, as medical organizations have many acts or policies to adhere to such as HIPPA. One of the most common ways of protecting sensitive data is through the use of data encryption. This changes the information from the clear or plain text to something that cannot be read without the ability of the decryption key. No one should be worried about sharing their personal data without explicit consent.\nAt Spirion we are here to help protect and secure the business’ and consumer’s sensitive data. Contact us today to see how we can help your organization with data discovery and classification to start protecting data and minimizing the risk of a breach. Let us be a part of your security policy.']	['<urn:uuid:79ade5f6-74b7-4293-b16a-462146dd6247>', '<urn:uuid:83d02df2-05fa-445a-8204-5e66738fe3ba>']	factoid	direct	concise-and-natural	distant-from-document	three-doc	novice	2025-05-12T20:46:45.703947	8	53	2859
36	What are the new taxes on alcohol and tobacco in Uganda, and what effects do mining activities have on wildlife and forest areas?	Uganda is imposing a tax of US$0.8 per kilogramme on tobacco leaf exports, and increasing charges on alcoholic beverages by Shs 230 per litre or 30%, whichever is higher. Regarding environmental effects, mining leads to deforestation as it requires large portions of land to be cleared, threatens wildlife populations, destroys habitats in surrounding areas, and can create lifeless wastelands that put plant species at risk.	"['Cars, internet and minerals targeted in new tax laws\nGovernment has, through Finance Minister, Hon. Matia Kasaija, tabled proposals for the new tax laws, which will see motor vehicle owners part with an annual license fee paid by 31st of January.\nThe Traffic and Road Safety Act (Amendment) Bill, 2021 seeks to impose, under clause 14.A, a license to permit ownership of “a motor vehicle, trailer or engineering plant.”\nTo use the vehicles specified above on the road, one must possess a license, not to be confused with a drivers’ license.\nFailure to have the license, proposes the new law, could see one jailed for two years, pay a fine of Shs2 million, or suffer both imprisonment and fine.\nThe minister tabled the proposals during the Thursday, 1 April 2021 sitting chaired by Speaker Rebecca Kadaga.\nAdditionally, an annual fee, to be determined by the Minister of Works and Transport by Regulations, is also to be met by a vehicle owner.\nFailure to pay the annual fee, will lead to a fine of Shs 200, 000 accumulating every day from the date the fee was due for payment - 31st of January.\nClause 14B (3) proposes such fine to be regarded as debt due to the government by the defaulter.\nTo incentivise the electronic receipting, the finance minister proposes in the Value Added Tax (Amendment ) Bill a refund of 5 per cent of taxes to “a person rather than a taxable person and is issued with an electronic receipt or invoice or several electronic receipts or invoices worth ten million shillings within a period of thirty consecutive days.”\nUnder the Tobacco Control Act (Amendment) Bill, 2021, tobacco leaf exporters are proposed to bear a tax of US$0.8 per kilogramme exported out of Uganda, a move intended to encourage the establishment of local tobacco processing plants in the country.\nFor those dealing in the export of processed gold, the Mining (Amendment) Bill, 2021 seeks to levy US$200 per kilogramme, while those exporting other unprocessed minerals are to part with a tax at the rate of 1 per cent of the value of the mineral in issue.\nThe levy of a paltry 1 per cent comes as a shocker to watchers of the mining industry, who expected a higher levy to discourage the export of unprocessed minerals and in turn aid local industrialization.\nThe noise from the fish maw industry has also attracted the attention of the taxman, who The Fish (Amendment) Bill now seeks a Shs 7000 levy on every kilogramme of fish maw exported out of Uganda.\nThe lucrative industry has in recent years seen increased activity on the lakes, pushing dealers in the fish maw industry into constant cat and mouse chase with security forces deployed to salvage depleting fish stocks in the country.\nThose who export wheat bran, cotton cake, maize bran and any other by-products of the milling industry will, if the External Trade (amendment) Bill comes into force, contend with a US$0.4 per kilogramme of such item exported out of Uganda.\nIn proposals under the Excise Duty (Amendment) Bill, 2021, the taxman targets the alcohol industry a Shs 230 per litre or 30 per cent increase in the charge on opaque beer, whichever is higher.\nClause 4 seeks to increase the levy on all locally produced alcoholic beverage at a rate of Shs 230 per litre on or 30 per cent, whichever is higher.\nEven nonalcoholic drinks, provided they contain yeast and bacteria, will under clause 4(c) pay a Shs 250 per litre tax or 30 per cent more of the existing tax, whichever is higher.\nInternet data, except data for provision of medical and educational services will, under clause 4(f) pay an additional 12 per cent of taxes on the fee charged.\nThe Tax Procedures Code (Amendment) Bill, 2021 seeks to allow for the appeal of the decisions of the High Court, arising from appeals from the Tax Appeals Tribunal, to proceed to the Court of Appeal and the Supreme Court, except that in the case of the Supreme Court, only points of law will form the basis of appeal.', ""Mining has several bad effects. It leaves behind a huge hole after mining is done. Secondly it damages natural beauty. A beautiful landscape which once existed is now a huge piece of dug up earth.\nEnvironmental Effects. Environmental issues can include erosion, formation of sinkholes, loss of biodiversity, and contamination of soil, groundwater and surface water by chemicals from mining processes. In some cases, additional forest logging is done in the vicinity of mines to create space for the storage of the created debris and soil.\nThe effects of mining in Africa have left large-scale devastation when companies do not honour their responsibility. Because mining areas are left in an unsustainable condition, plant species and wildlife are threatened and these areas are at risk of becoming lifeless wastelands.\nThe Impact and Effect of Illegal Mining (galamsey) towards the Socio-economic Development of Mining Communities: A Case Study of Kenyasi in the Brong Ahafo Region Adjei Samuel1, N.K.Oladejo1, I.A. Adetunde2, * 1University for Development Studies, Department of Mathematics, Navrongo. Ghana.\nSome of the major effects of mining on the environment are as follows: Minerals are the natural resources which play an important role in the economic development of the country. But the extraction and mining of these natural resources leads to some adverse effect on our environment as well.\nMar 09, 2017· The mining industry has the potential to disrupt ecosystems and wipe out wildlife populations in several different ways. Here's how mining affects the environment and wildlife. Habitat Loss; Mining can lead to the destruction of habitats in surrounding areas. The …\nModern mining is an industry that involves the exploration for and removal of minerals from the earth, economically and with minimum damage to the environment. Mining is important because minerals are major sources of energy as well as materials such as fertilizers and steel.\nApr 25, 2017· Mining is the extraction of minerals and other geological materials of economic value from deposits on the earth. Mining has the potential to have severely adverse effects on the environment including loss of biodiversity, erosion, contamination of surface water, ground water, and soil.\nSome gold can be found by panning in rivers; heavy gold will remain in the pan, whereas lighter rocks and minerals float out. This small-scale form of gold mining has little effect on the body of water, but the large-scale practice of mining gold from ore can have tremendous negative effects on water quality.\nMining can effect the earth because first, deforestation, and because mining requires large portions of land to be removed before they can start mining, lots of trees and plants are removed.\n1.1 PHASES OF A MINING PROJECT There are different phases of a mining project, beginning with mineral ore exploration and ending with the post-closure period. What follows are the typical phases of a proposed mining project. Each phase of mining is associated with different sets of environmental impacts. 1.1.1 Exploration\nFeb 07, 2018· The effects in such cases can be devastating for the environment. Be it due to ignorance of the regulations or just a freak accident, incidents like the Guyana spill of 1995 may occur again. This highlights the fact that issues like mining's effect on the environment are worth some serious deliberation.\nAug 26, 2010· Dust, radon and mercury impact miners' health. Dust, radon and mercury impact miners' health. ... Miners Face Health Risks, Even on Good Days ... mining …\nThe effects of mining coal on the environment. There are 2 ways to mine coal – Strip Mining and Underground Mining – both ways have their own impact to the environment and health. We know it but coal is such a cheap energy source that we don't want to let go of it. The negative effects of coal mining cannot be disputed:\nApr 21, 2019· The human health effects due to cyanide leach gold mining are not well documented, and this is no exception in Montana. The State of Montana has done no formal studies to specifically study mine-related health effects. Pegasus, the last mining company at Zortman-Landusky, started to fund a health study with the $1.7 million supplemental money from the 1996 settlement, but because …\nADVERTISEMENTS: Some of the major environmental effects of mining and processing of mineral resources are as follows: 1. Pollution 2. Destruction of Land 3. Subsidence 4. Noise 5. Energy 6. Impact on the Biological Environment 7. Long-term Supplies of Mineral Resources. Mining and processing of mineral resources normally have a considerable impact on land, water, […]\npositive and negative effects of mining on the environment. Mankind has been mining for precious metals since 42000 years ago and that's a staggeringly long time ago and that's exactly how long our species has been digging into the ground, to harvest its precious metals.\nDownload Coal Mining sounds ... 76 stock sound clips starting at $2. Download and buy high quality Coal Mining sound effects. BROWSE NOW >>>\nMining affects the environment by exposing radioactive elements, removing topsoil, increasing the risk of contamination of nearby ground and surface water sources, and acidification of …\nApr 20, 2015· Effects of Mining. Coal mining, the first step in the dirty lifecycle of coal, causes deforestation and releases toxic amounts of minerals and heavy metals into the soil and water. The effects of mining coal persists for years after coal is removed.\nJul 25, 2018· Environmental impacts from fossil fuel pollution are rapidly increasing in regions that have the highest concentrations of fuels. There are multiple effects of mining fossil fuels. Drilling and mining practices take a substantial toll on local water sources, biologic life and natural resources.\nPublished by the American Geosciences Institute Environmental Awareness Series. ... How can metal mining impact the environment? PDF version. Material adapted from: Hudson, T.L, Fox, F.D., and Plumlee, G.S. 1999. Metal Mining and the Environment, p. 7,20-27,31-35,38-39. Published by the American Geosciences Institute Environmental Awareness Series.\nMining operations usually create a negative environmental impact, both during the mining activity and after the mine has closed. Hence, most of the world's nations have passed regulations to decrease the impact. Work safety has long been a concern as well, and …\nEffects of mining on aquatic resources are both physical and chemical in nature. Most of earthmoving activities of mining occurred well before the enactment of laws designed to protect aquatic resources - particularly the 1977 Federal Water Pollution Control Act.\nThe former is known as underground mining, the latter as strip mining or mountaintop removal. Either process contributes a high level of damage to the environment: #12 Noise pollution. One of the most obvious (albeit perhaps least harmful) environmental effects of coal mining is noise pollution.\nMining has an adverse effect on soil quality. Soil degradation is the prime impact. Another impact is deforestation and loss of fauna and flora.\nThe impact of mining on the environment and the effects of mining techniques need to be more advanced with the utilization of modern equipment to be unintrusive to the environment. Economic growth is high on the agenda of leading countries, sustaining …\nMining is an inherently invasive process that can cause damage to a landscape in an area much larger than the mining site itself. The effects of this damage can continue years after a mine has shut down, including the addition to greenhouse gasses, death of flora and fauna, and erosion of land and habitat.\nNov 14, 2016· After mining is over, the land is left as barren land. The effects of mining sometimes vary depending on what is mined out, but these are some of the general effects you will see in all mine-areas. I'm not an expert when it comes to health impact on miners, but here are some of the things I know will affect them-\nJul 08, 2017· In coal mining, the extraction, crushing, and transport of coal can generate significant amounts of airborne respirable (extremely fine) coal dust. Dust less than 10 microns in size (cannot be seen with the eye). In non-coal mining, stone, and san...\nEnvironmental impacts of mining can occur at local, regional, and global scales through direct and indirect mining practices. Impacts can result in erosion, sinkholes, loss of biodiversity, or the contamination of soil, groundwater, and surface water by the chemicals emitted from mining processes. These processes also have an impact on the atmosphere from the emissions of carbon which have ...\nApr 04, 2017· The Dangerous Effects of Illegal Mining. April 4, 2017 Environmental Issues Written by Greentumble. Illegal mining has been ravaging our planet for. decades. Not only is illegal mining riskier from a safety perspective for those who choose to participate, but it encourages reckless behavior and leads to outcomes that have negative long-term ...""]"	['<urn:uuid:6919eb93-2e46-46bd-a70c-aea706dc1d61>', '<urn:uuid:11ce18f5-51f7-4dfa-a26b-c4a5796e1622>']	factoid	direct	verbose-and-natural	similar-to-document	multi-aspect	novice	2025-05-12T20:46:45.703947	23	65	2142
37	therapy stages timeline amputee patient learn walking prosthesis rehabilitation process	The rehabilitation process consists of three phases. Phase 1 is Initial Post-Operative Recovery, focused on regaining strength and range of motion while preparing for the first prosthesis. In Phase 2, after receiving the prosthesis, therapy focuses on advancing balance and mobility skills through tailored exercise programs. Phase 3 involves advanced physical therapy training to help amputees return to their highest level of previous function, including activities like climbing stairs, outdoor walking, and recreational sports.	['Participating in physical therapy after an amputation can make a world of difference in successful rehabilitation and a prompt return to life. The H2 Health Family provides a comprehensive amputee evaluation & treatment program, designed to progress patients through the difficult stages of pre & post-amputation recovery and into a state of functional independence.\nFrom the initial days after surgery, through the process of learning to use your first prosthesis, to advancing your mobility skills years after an amputation, physical therapy can help guide you along that journey.\nOur Prosthetic Training Approach\nWorking closely with your primary physicians and prosthetists, our therapists are dedicated to their role within a team of practitioners aimed at returning amputees to a high level of independence. As experts in gait training and body mechanics, physical therapists are able to assess, treat, and eliminate abnormal movement patterns, allowing amputees to function in their environment with maximized safety, efficiency, and decreased pain.\nWe accomplish this by advancing your strength, range of motion, balance, and functional mobility, depending on your specific needs and personal goals.\nPhase 1: Initial Post-Operative Recovery\nInitially, your therapist can assist you in regaining your strength and range of motion as your body adapts to a new amputation. During this time, your therapist will work closely with both your physician and prosthetist to prepare your for the receipt of your first prosthesis.\nPhase 2: Learning To Use Your Prosthesis\nOnce fit with your prosthesis, continued physical therapy will focus on advancing your balance and mobility skills as a prosthetic ambulator. In this phase of recovery, therapeutic exercise programs will be tailored to meet your specific needs and rehabilitation goals, as you learn to use your prosthesis for daily movement tasks within your home and community.\nPhase 3: Advancing Your Mobility Skills\nHaving learned to be an independent ambulator, advanced physical therapy training can help many amputees turn to their highest level of previous function. Specifically, progressed physical therapy training programs can be designed to assist in returning patients to work, climbing stairs & inclines, ambulating on uneven outdoor terrain, running, and participating in recreational sporting activities. In this stage of rehabilitation, the sky is often the limit.\nAdditional Amputee Programs\nWe have a variety of therapy programs that can help with the amputee and prosthetic training process. Check out the different services below that can help! Curious about which program is best for you? Request an appointment with one of our therapists to create the best treatment plan for your goals.\nGait is the pattern in which you walk and take a step. The gait cycle is broken up into multiple phases and can be assessed phase by phase to improve your overall walking pattern. Relearning proper gait mechanics soon after an amputation is vital for preventing future injuries & abnormalities and returning a patient to normal function.\nMost patients, after suffering a neurologic injury, wish to return to normal walking as soon as responsibly possible. To make that happen, we use tools like live video with slow-motion playback to quickly break down abnormalities of gait in order to assess and improve mechanics.\nBenefits of Functional Gait Analysis:\n- Focus on mechanics at each phase of gait cycle with live video with slow-motion playback\n- Visual feedback for patients & therapists of progress\n- Increased functional strength during walking\n- Improved safety at home and in the community\nDry needling is an innovative physical therapy technique used for targeted trigger point pain relief. Typically used as part of a larger, whole-body treatment plan, dry needling can be remarkably effective at releasing or inactivating trigger points to relieve pain and improve range of motion.\nDry needling, similar to acupuncture, involves needles without medication being inserted through the skin into highly targeted muscle areas. The muscle areas and trigger points typically targeted via dry needling are not easily palpable manually, so dry needling provides a unique opportunity to provide more targeted therapy.\nBenefits of Dry Needling:\n- Pain Relief\n- Improved Range of Motion\n- Reduced Muscle Tension\n- Normalized Dysfunction of the Motor End Plates\n- Faster Return To Normalcy\nASTYM® is a revolutionary treatment method that stimulates the body’s healing response, which results in the remodeling & resorption of scar tissue and the regeneration of degenerated tendons. It is non-invasive, works fast, and provides long-term relief. The ASTYM® treatment method is one of the most well-researched in recent years, and these studies have time and again proven its effectiveness in providing pain relief all across the body. Only ASTYM®-certified clinics may perform Astym® treatment.\nBenefits of Astym® Therapy Include:\n- Fast-Acting Pain Relief\n- Typically Short Treatment Timelines\n- Completely Non-Invasive\n- Totally Safe & Effective Long-Term\nFor individuals suffering from both acute and chronic pain, we offer laser therapy as a surgery-free, drug-free pain relief option. Treatment is fast, safe, and painless, with most laser therapy program patients seeing results within 3 to 5 treatment sessions. Additionally, therapy sessions have an added therapeutic effect continuing for up to 18 hours after treatment.\nBenefits of Laser Therapy Include:\n- Effective Against Difficult Conditions\n- Non-Invasive Alternative To Surgery\n- Extremely Fast Treatment Times\n- Scientifically-Proven To Be Effective\nThe Alter G® machine is a high-tech, low-impact treadmill that uses precisely tuned air pressure to reduce the impact of walking or running. Its unique unweighting technology allows patients to exercise with a full range of motion while their body is still going through normal healing processes.\nUnlike any other weight & impact-augmenting pieces of recovery equipment, the Alter G® is remarkably precise, allowing patients to exercise at anywhere between 20 and 100 percent of their body weight. Patients compare it to running on air, and those recovering from serious lower-extremity pain are able to build strength at much earlier stages than ever before. In fact, the comprehensive weight controls give our physical therapists the ability to slowly adjust a patient’s body weight until they say they are pain-free.\nBenefits of Alter G® Therapy Include:\n- Rehabilitating After Lower-Extremity Surgery\n- Training Through An Injury\n- Reduced Impact Training & Conditioning\n- Gait Training For Neurologic Patients\n- Weight Control & Reduction\n- Strength & Conditioning For Older Patients\nPhysical therapy on its own is often not enough to improve gait for patients with neurologic injuries that suffer from drop foot, ankle or knee weakness, or frequent falls.We offer recommendations for bracing that assists the lower leg through the proper gait pattern to improve both safety and quality of life.\nIn select locations, we have onsite custom casting & molding capabilities and make recommendations based on gait pattern and foot positioning, while also working closely with orthotic providers in the area to provide our patients with braces that can radically improve their gait. Once braces are received, we work with patients on practicing normal gait mechanics and improving safety with their device.\nBenefits of Orthotic Fitting & Management:\n- Reduced Risk of Falls\n- Improved Mobility at Home and in the Community\n- Return to Normal Gait Pattern\n- Prevention of Compensation Patterns or Future Injuries']	['<urn:uuid:ba573f5e-cad9-49dd-a982-51088341216d>']	factoid	direct	long-search-query	distant-from-document	single-doc	expert	2025-05-12T20:46:45.703947	10	74	1181
38	cylindrical roller bearings construction load capacity friction reduction principles	Cylindrical roller bearings consist of two parts with cages and guiding ribs holding cylindrical rollers, optimizing contact pressure distribution and lubrication film formation between parts. Their construction allows them to carry large radial loads and, in some cases, axial loads. Friction reduction is achieved through lubricants like WD-40, greases, oils, or water, which form a thin fluid film between moving parts, preventing surface irregularities from interfering with each other.	['Single row cylindrical roller bearings\nSingle row cylindrical roller bearings are able to carry large radial load on a relatively small space. Some applications are able to carry also axial load in one or both directions. Raceways of the outer and inner rings together with the sheet profile of the cylindrical rollers in the shape of ZB allow optimal distribution of the contact pressure in the rolling space. This arrangement also allows advantageous formation of lubrication film between the contact parts of the bearing, optimal rolling, decrease of friction, growth of temperature and therefore lower stress of the junction in the arrangement.\nAt the same time certain mutual misalignment of the rings is allowed and so the bearings cope better with the real operating conditions and contribute to a better reliability and durability during the operating life of the bearings. Cylindrical Roller Bearings are suitable for arrangements, with high requirements for load transfer in connection with high rotation speed, e. g. machine tools, rolling mills, vehicle axle, etc.\nMain dimensions of cylindrical bearings are specified in the dimension tables and they are in accordance the international standards ISO 15.\nCylindrical roller bearings generally consist of two parts – outer or inner ring block on which with the help of cages and guiding ribs cylindrical rollers are fastened and second, separate ring. This arrangement allows separate mounting of rings and so the manipulation of indiviudal parts is made easier. Through the gradual development and introduction of new possibilities in the materials and processing technologies better utilization of the inner bearing space and introduction of application with higher load marked “E” was enabled. Bearings with steel cage are offered in the whole range with higher load. Higher load bearings with brass cage are dependent on technological possibilities and they are indicated in\nthe table section.\nSeparate group are bearings with brass cage where rib is joined to the cage body by unriveting of cross pieces. This arrangement allows more efficient utilization of inner bearing space and offers better working properties mainly in connection with durability. Designation of these bearings is done by additional letters „EDM“.\nThe Following Constructions are the Most Common\nThe cylindrical rollers are guided by cage between two ribs of outer ring. The inner ring is ribless, which allows mutual bearing rings displacement.\nSimilar to NU, but inner ring contains a rib on one side. The bearing can accomodate axial force in one direction.\nThe outer ring is manufactured with two ribs, loose inner ring contains solid rib on one side, from the other side is flat angle ring attached. This type of bearings can be used to carry axial forces in both directions.\nThe cylindrical rollers are guided by cage in inner ring with two solid ribs. The outer ring is rib less, relative axial bearing rings displacement is possible.\nThe inner ring with two solid ribs, outer ring with one guiding rib.\nAngle rings, which use allows to accomodate axial load from needed direction.\nPossible combinations of angle ring HJ with bearings:\nWe do not recommend the combination of NU bearing with two angle rings, because of axial nipping of rollers can occur.\nThe illustration of the combinations is above the table chart.\nThe cylindrical bearings are commonly manufactured within the normal tolerance class P0. Production of bearings with higher tolerance classes is necessary to consult with manufacturer. Limiting values of dimension deviations and operation are stated in the standards ISO 492.\nThe commonly manufactured cylindrical bearings are with normal radial clearance or with radial clearance C3. For special applications cylindrical bearings can be supplied with radial clearance C2 (smaller than normal) or with clearance C4, C5 (greater than normal and C3). Radial clearance values are in accordance with the standard ISO 5753. These values are\nvalid for manufactured and unassembled bearing.\nStabilisation for operation at higher temperature\nFor operating temperature higher than 120°C specially stabilized single row cylindrical roller bearings with stabilized dimensions for operating temperature from 150°C to 400°C (S0, S1, S2, S3, S4, S5) are produced. Delivery of stabilized bearings should be discussed in advance.\nThe inner construction of area of rolling contact allows the operation of cylindrical bearings with a certain misalignment of both rings.\nThe standard values are:\n4´ – for bearing series 2, 3\n3´ – for bearing series 22, 23\nCylindrical bearings are generally produced with cages running over the rolling components in following versions:\nE – pressed steel cage, consisting of 2 ribs and cross pieces, joined by unriveting of pins\nM – massive brass cage, which rib is joined to the shell by steel pins\nEM – similar cage as M, but in construction with higher load\nEDM – massive brass cage, where rib is to the cage body joined by unriveting of cross pieces\nTNG, ETNG – a single cage made of polyamide PA 6.6, reinforced by 25 % content of glass fibres, delivered mostly in more bearing ETNG version. Availability of cages in other versions must be consulted with the manufacturer in advance.\nEquivalent dynamic load\nIf cylindrical bearings are used to convey only the radial load rating without the axial load, dynamic load is calculated as follows:\nPr = Fr\nIf the cylindrical bearing is subject to simultaneous radial and axial load, the dynamic load is calculated as follows:\nPr = Fr for Fa/Fr ≤ e\nPr = 0,92.Fr + Y.Fa for Fa/Fr > e\ne = calculation coefficient\ne = 0,2 for bearing series 2, 3, 4\ne = 0,3 for bearing series 22, 23\nY = axial load coefficient\nY = 0,6 for bearing series 2, 3, 4\nY = 0,4 for bearing series 22, 23\nFor trouble free operation of cylindrical bearings, which convey the axial load, there is important to achieve radial load, where ratio Fa/Fr should not exceed the value 0,5.\nEquivalent static load\nFor static loaded cylindrical bearings is valid:\nPor = Fr\nThe limiting speed specified in the tables section of the catalogue is the maximum number of revolutions acceptable for the bearing to operate trouble-free at a certain level of safety.\nThe designation of basic designs of the bearings is specified in the dimension tables. Modification of the basic design is designated with additional symbols according to STN 02 4608. The meaning of the most used symbols for cylindrical roller bearings is in the table:\n|M||NU210M||Massive brass cage, with steel pins|\n|E||NU210E||Bearing with higher basic load ratio and steel cage|\n|EM||NU310EM||Bearing with higher basic load ratio and with steel pins joined brass cage|\n|EDM||NU210EDM||Bearing with higher basic load ratio and brass pin less cage|\n|P6||NU210E P6||Higher tolerance class|\n|C2||NU210E C2||Radial clearance smaller than normal (normal radial clearance is not indicated)|\n|C3||NU210E C3||Radial clearance greater than normal|\n|S1||NU210E S1||Stabilisation of both rings for higher operating temperature|\nCombination of symbols\n|NJ+HJ||NH210E||Bearing NJ with angle ring HJ|\n|P6+C3||NU210E P63||Bearing with higher tolerance class than standard and radial clearance greater than normal|', 'Friction and wear are also reduced by suitable lubrication. Lubricants, which include WD-40 spray lubricant, greases, oils, water, and even air, form a thin fluid film between the moving parts of the bearing. Whereas a pair of dry surfaces will bite into each one another, even if they are comparatively smooth, the presence of a thin film of lubricant between the stationary and moving surfaces enables the surfaces to float one on the other; the slight irregularities in one surface are kept clear of the imperfections of the other surface by the film of lubricant. Usually the motion itself is sufficient to form the film (hydrodynamic bearings) but sometimes the lubricant must be applied under pressure (hydrostatic bearings) as in air-lubricated dental drills.\nJournal bearingA rotating shaft needs to be supported at, or close to, its ends. If it is long, additional supports or bearings may be needed at intermediate points as well. The simplest form of bearing, which is quite satisfactory if the speed is low and if the only forces acting on the bearing are perpendicular to the shaft, is the journal type. In this the shaft is supported by a close-fitting hollow cylinder called a shell or bush. As the bush may be subject to considerable wear, it is made so that it can be easily replaced in the bearing housing.\nLack of oil between the bearing bush and the shaft results in a considerable increase in the frictional forces acting between the surfaces. This, in turn, generates heat which causes the metal to expand and eventually the bearing to seize up. As serious damage to the machine would result from bearing seizure, it is usual to line the bushes of bearings with a soft alloy (typically Babbitt or white metal, as mentioned above) which has a fairly low melting point. In this way the bearing melts when it becomes overheated thus avoiding damage to the shaft or whatever the shaft is driving.\nIn order that the bearings do not have to be lubricated too frequently, it is necessary to incorporate into their design a means of holding a reserve of oil. This is usually done by having grooves cut in the surface of the bush, and also by having a small reservoir of oil above the bearing so that it can flow down and be distributed over the surface of the rotating shaft as it is required. There are sometimes great pressures acting between the bearing and the shaft, in which case lubricants have to be forced in under pressure.\nBall and roller type bearingsMore satisfactory bearings are the ball and roller types. In these the moving shaft and the fixed bearing housing are separated by balls or rollers, which run in grooves or races set in the surface of the shaft and the housing. The main advantage of these types of bearing over the journal type is that the sliding friction between the surfaces of the rotating shaft and the fixed bush is replaced by rolling friction between the balls or rollers and each of the two surfaces. The loss of energy in overcoming rolling friction is much less than that required to counteract sliding friction.\nBall bearings were first used towards the end of the 19th century and found favor in the constriction of the bicycle which was then becoming popular. In the original ball bearings the balls completely filled the space between the two races, but it is now usual to have the balls separated by a metal cage. This serves to keep the balls evenly spaced around the race and prevents uneven wear.\nFor these bearings to function satisfactorily, it is necessary that the balls or rollers and the races are from good quality hardened steel and that they are manufactured to a high degree of accuracy. In particular, it is most important that all the balls in one race are of the same size exactly.\nA wide range of types of ball and roller bearings have been developed to suit various applications, a few of which are illustrated above. Roller bearings are used where there is a large force between the surfaces. Tapered roller bearings are useful where there is a thrust along the shaft.\nIn recent years dry (unlubricated) plastic bearings and self-lubricating bearings have been developed for applications where lubrication is difficult or undesirable.\nRelated category TECHNOLOGY\nHome • About • Copyright © The Worlds of David Darling • Encyclopedia of Alternative Energy • Contact']	['<urn:uuid:8948776c-9b23-4092-90b9-6206de8c64ca>', '<urn:uuid:82b0cac6-68a7-43fe-83f8-42ab32cd5f42>']	factoid	direct	long-search-query	similar-to-document	multi-aspect	expert	2025-05-12T20:46:45.703947	9	69	1897
39	I need a cheaper way to process animal food. What options exist?	Granulation is a cost-effective alternative to traditional pelleting for processing animal feed. While pelleting requires high energy for extrusion and causes die wearing, making it cost-prohibitive, granulation involves less energy and operating costs. The granulation process involves mixing wet distillers grains with condensed distillers solubles in a rotary drum granulator, where mechanical forces enable particles to collide, densify and agglomerate while the solubles act as a binder.	['One of the options regarding ways to increase the bulk density of DDGS and improve its flowability is pelleting. Pelleting is a popular process used in the feed industry for blending and agglomerating feed ingredients in feed manufacturing. Added benefits include better product handling, palatability and flow. While pelleting is a favorable process for a lot of feed products, the high energy needed to extrude the feed material from the die and die wearing can make it a cost prohibitive process. A recent study indicates that the cost of pelleting declined with production scale, and pelleting DDGS is only feasible for larger plants (250 million gallons/year). This limits pelleting to a few large existing fuel ethanol plants in the United States. While there have been reported successes from research of pelleting DDGS, this practice might not be an economically feasible solution for tackling the logistics issue.\nA recent invention at Purdue University titled Apparatus for Producing Biobased Carriers from ByProducts of Biomass Processing (Pending U.S. Patent 65095.00.US) adds value to DDGS by producing spherical granules capable of carrying and dispersing biologically and chemically active molecules such as fertilizers, pesticides and enzymes. Additionally, granulating DDGS increases the overall particle size and bulk density, thereby ultimately resolving the flowability and handling issues such as avoiding particle segregation which will reduce sampling error and improve product consistency.\nThis new invention was developed after preliminary studies were done producing DDGS granules using a lab-scale rotary drum dryer. Recently, extensive studies have been conducted at the lab-scale to understand the kinetics of granule growth. DDGS granules manufactured in a granulating drum are spherical pellets with improved handling properties and increased bulk density compared with DDGS produced using the traditional blending and rotary drum drying process common in dry-grind ethanol processing. However, the basic feed material needed for granulating DDGS is similar to what is needed in the traditional process. The granulation process involves the mixing of WDG with CDS, and granulating the mixture using a rotary drum granulator. The granulating drum provides the mechanical force that enables solid particles of WDG to collide, densify and agglomerate (grow in size), while CDS acts as a binder gluing colliding particles together. The wet distillers grains with soluble granules produced are dried to obtain DDGS granules. Unlike pelleting, this method of granulation involves less energy and operating cost than pelleting, and thus is a good alternative to be considered for use.\nFigure 2 shows four possible particle size ranges which were produced during our lab-scale experiments, compared with DDGS without granulation, also produced in our experiments. The granules produced were targeted in the size range of 1.7 to 3.4 mm. The bulk density of granulated DDGS in Group size II and III were higher than the ungranulated DDGS by 17% and 20%, respectively. It should be noted that bulk density of granulated DDGS will depend on the solids content of the feed and so a fundamental understanding of the feed (WDG and CDS) is essential to obtaining granules of the desired characteristics.\nIn comparison to other particles such as pharmaceutical powders, detergents, etc., granulating DDGS is not a trivial task and involves skill and good understanding of the fundamental science of granulation. This is because of the very heterogeneous nature of the feed material which is a combination of particles of germ, protein, fiber, residual sugars, and tipcap with varying physical structure and chemical properties. Our study was a first important step to understanding the process variables and granulation kinetics of DDGS granulation. While more work is still needed in understanding some of the fundamentals of granulating a complex feed material such as WDG and CDS blends, our study has progressed to the point where some pilot-scale work can begin.\nJust add nitrogen: DDGS as fertilizer\nIn another parallel study, DDGS granules of desired size characteristics was manufactured using nitrogen in the feed, with the attempt of producing a slow release fertilizer. In this study, DDGS was used as a carrier, while nitrogen was mixed within particles of DDGS and binder in one granule design and in a second granule design, nitrogen was used as the core, while DDGS and binder coated around the surface of the granule.\nOur experiments were able to successfully granulate DDGS with the inclusion of nitrogen — thus proving its potential for use in a fertilizer matrix; however, the granules produced were not rated as slow release, and thus more work needs to be done in order to develop slow release fertilizers or other similar functions with DDGS. Two important granule characteristics that are important as fertilizer carriers are shown in Tables 2 and 3.']	['<urn:uuid:72e94f56-4c66-44db-97e9-a744a68a820e>']	open-ended	with-premise	concise-and-natural	distant-from-document	single-doc	novice	2025-05-12T20:46:45.703947	12	67	772
40	What cleaning supplies and tools do I need to gather before starting to clean my outdoor patio furniture for spring?	For cleaning patio furniture, you need to gather the following equipment: 303 Multi-Surface Cleaner (which can replace common cleaners like bleach and dish soap), 303 Aerospace Protectant, 303 Fabric Guard, glass cleaner, a bucket, a sponge or soft cloth, and a garden hose with spray nozzle. It's important to plan the cleaning for a sunny day since the furniture will need time to air dry.	['Getting Your Patio Furniture Ready for Spring\nThe arrival of spring is a sign that you need to get your patio furniture out and start enjoying the outdoors again. However, you will need to get your furniture ready for regular use before you can put it out on your patio. Even if your furniture was covered for the winter, some amount of spring cleaning will still be necessary as it will have accumulated dust and grime over the winter months. Below is a look at how you can clean different kinds of patio furniture pieces and get them ready for use in spring and summer.\nYour first step for spring cleaning will be to gather the right equipment. Here is a list of what you will need:\n- 303® Multi-Surface Cleaner (This can replace a variety of other commonly used all-purpose cleaners like bleach, dish soap, and other specialty use cleaners)\n- 303® Aerospace Protectant\n- 303® Fabric Guard\n- Glass cleaner\n- A bucket\n- A sponge or soft cloth\n- A garden hose with spray nozzle\nFactors to Consider Before Cleaning Your Outdoor Furniture\nIdeally, you will want to clean your patio furniture on a sunny day. Before getting your supplies together make sure that you are going to have lots of sun since your furniture will take time to air dry.\nThe Furniture Material\nThe cleaning methods that you use will depend on the material used to make your outdoor furniture. Wicker, glass surfaces and wrought iron all require different cleaning method.\nCleaning Methods for Different Types of Furniture\nPlastic, Resin or All weather Wicker Furniture\nSpray 303 Multi-Surface Cleaner onto the soiled surfaces and scrub with a sponge or wet cloth until the stain is removed. Wipe surface down with a damp cloth to remove loosened dirt and excess cleaner. Rinse your furniture using the hose with spray nozzle and then leave it to air dry. Wipe away any water spots with a damp cloth. Spray down any glass tabletops and then spray with glass cleaner and wipe.\nWrought Iron Furniture\nThe potential for rust makes wrought iron a more difficult material to clean. You will want to start your cleaning process by inspecting for rust. If there is rust present on your furniture, you can use sandpaper to remove it. You may also need to repaint the furniture after removing rust. If there is no rust, then you can use 303® Multi-Surface Cleaner to clean it in exactly the same way as furniture made with other materials. (need to put a line in article to wipe down the furniture with a soft cloth. Rust can form if you leave water residue left behind unlike other materials mentioned above.)\nIf you want to take extra steps to protect your wrought iron furniture from the elements, use 303 Aerospace Protectant. Simply spray it onto the surface and then wipe all of the excess product away. Wiping it away is important to prevent streaking. Buff with a dry cloth to increase the protectant’s longevity.\n1. Start the cleaning process by gathering your outdoor furniture cushions, decorative pillows, umbrella and other outdoor fabrics together outside.\nIf possible, unzip the cushion covers and wash them by following the instructions on the tags. You can then use your vacuum cleaner with an upholstery attachment on the cushions. Use the vacuum cleaner to remove any loose dirt and dust. With tufted cushions, you should be careful to run the upholstery attachment along the crevices and seams as well as around any buttons.\n2. To remove stains, spray cushions with 303 Multi-Surface Cleaner and scrub with a soft bristle brush or damp cloth. Repeat the process if necessary. Rinse with a separate damp cloth or rinse with water. Dab the cushion with a clean, dry cloth to remove excess water or let air dry.\n3. Your next step should be to spray 303 Fabric Guard in a well-ventilated area (preferably 70° or warmer) in a criss-cross pattern directly onto your fabrics until damp. Let your treated surfaces cure for 6 to 12 hours. The 303 Fabric Guard will help them to stay clean by resisting stains and water.']	['<urn:uuid:7407ac98-5a8d-45c9-8843-e142fd5e9438>']	open-ended	direct	verbose-and-natural	similar-to-document	single-doc	novice	2025-05-12T20:46:45.703947	20	65	695
41	washington dc area bicycle commute options	The DC area offers multiple bicycle commuting options. Capital Bikeshare operates over 300 stations across Alexandria, Montgomery County, Arlington, and Washington DC. The region already has 360 miles of trails, though currently disconnected, and is developing a comprehensive network through the Capital Trails Coalition. DC is particularly suitable for cycling as it's relatively flat and 39% of residents don't own cars, with residents taking over 3 million bike-share trips last year.	"[""Alexandria is served by all major domestic and international airlines at:\n- Ronald Reagan Washington National Airport (5-10 minutes from Old Town)\n- Washington Dulles International Airport (45 minutes from Old Town)\n- Baltimore/Washington International Thurgood Marshall Airport (45 minutes from Old Town)\nThe City of Alexandria is a wonderful place to cycle. With over 39 miles of on-street and off-street bike lanes, sharrows, and trails, using a bike to quickly, safely, and enjoyably get around has never been easier.\nThe City of Alexandria's bike sharing program, in partnership with the District Department of Transportation, Arlington County, and Montgomery County, is called Capital Bikeshare. With Capital Bikeshare, you can choose from over 300 stations in the region, and return a bike to any station near your destination. Alta Bicycle Share operates the system for Alexandria, Montgomery County, Arlington, and Washington, D.C.\nThe City's Transportation Master Plan envisions a transportation system that encourages the use of alternative modes of transportation, reduces dependence on the automobile, and promotes a balance between travel efficiency and quality of life. The City is updating the Pedestrian and Bicycle chapters of the Transportation Master Plan to reflect changes that have occurred since 2008, including the Complete Streets policy, Capital Bikeshare program, and a move toward implementing on-street bicycle facilities. In addition, the City will be developing a Complete Streets Design Guidelines Manual.\nDiscover a convenient, environmentally friendly and fun way to get around Old Town Alexandria. The King Street Trolley transports riders between the King Street Metrorail Station and the Potomac River waterfront. The trolley runs daily every 10 - 15 minutes. With many stops for hopping on and off to explore the shops, restaurants and attractions found along Alexandria's historic main street.\nThe DASH system provides safe, reliable, and courteous bus service within the City of Alexandria and connects with Metrobus, Metrorail, VRE, and other local bus systems. DASH serves all of the Alexandria Metrorail stations and the Pentagon Metrorail station during morning and evening peak periods. Hours vary according to route. All buses are accessible to people with disabilities. Bicycle racks are currently available on all buses.\nThe Washington Metropolitan Area Transit Authority (WMATA) operates Metrobus, which consists of 335 routes and 15,000 bus stops throughout Virginia (counties of Arlington, Fairfax, and Loudon, and the cities of Alexandria, Fairfax, and Falls Church), Washington D.C., and Maryland (counties of Anne Arundel, Montgomery, and Prince George's). All buses are accessible to people with disabilities. There are bicycle racks on all buses. Hours vary according to route.\nMetroway is a premium transit service offering a faster trip along Route 1 between Crystal City and Braddock Road Metrorail stations, via Potomac Yard. The Washington Metropolitan Area Transit Authority (WMATA) operates Metroway, which features dedicated bus-only lanes, consolidated bus stops, and more service, seven days a week. Because Metroway buses travel on bus-only lanes through congested areas, beating traffic is a breeze. Buses come often and make limited stops, so you'll get where you need to go—faster.\nThe Fairfax Connector system serves a number of communities through Fairfax County, which includes Herndon, Reston, Tysons, Springfield, Mount Vernon, and others as well as areas of Alexandria and Arlington. Most buses are wheelchair equipped. Hours vary according to route. Bicycle racks are currently available on some buses.\n- Greyhound national bus service stops in Springfield at the Franconia-Springfield Metrorail Station, just one station south of Alexandria.\n- For schedule information, visit greyhound.com or call 800-231-2222.\nThe Washington Metropolitan Area Transit Authority operates Metrorail, which consists of 106 miles of rail lines and 86 stations throughout Virginia (counties of Arlington, Fairfax, and Loudon, and the cities of Alexandria, Fairfax, and Falls Church), Washington D.C., and Maryland (counties of Montgomery and Prince George's). On weekdays, service hours begin at 5:00 a.m. and end at midnight Sunday through Thursday nights. On weekends service begins at 7:00 a.m. and ends at 3:00 a.m. Friday and Saturday nights.\nAlexandria has four convenient Metrorail stations:\n- Braddock Road (Yellow Line and Blue Line)\n- Eisenhower Avenue (Yellow Line)\n- King St-Old Town (Yellow Line and Blue Line)\n- Van Dorn Street (Blue Line).\n- Virginia Railway Express (VRE) offers commuter rail service from the suburbs of Northern Virginia to downtown Washington, D.C.\n- Alexandria is served by its own VRE station (adjacent to the King Street Metrorail Station), which is on VRE's Manassas Line and Fredericksburg Line.\n- For schedule information, visit vre.org or call 800-RIDE-VRE.\n- The National Railroad Passenger Corporation (known as Amtrak), operates heavy rail service across the United States.\n- Alexandria is served by its own Amtrak station (adjacent to the King Street Metrorail Station).\n- For routes and reservations, visit amtrak.com or call 800-USA-RAIL.\nRoads & Highways\n- Alexandria is conveniently located along I-95, I-495, I-395, Rt. 1, Rt. 7, Rt. 236, and the George Washington Memorial Parkway, and is minutes from I-66 and I-295.\n- For more information, see the local highway map, regional highway map, or state highway map.\n- For current road conditions, including alerts and traffic cameras, see the Virginia Department of Transportation or call 5-1-1.\nRideshare & Commuting\nJump-start your carsharing experience with a free 1-year membership to Zipcar or free driving credits with Enterprise Carshare. The City has teamed up with both services to offer membership and usage discounts for residents and businesses through this special program.\nCarpooling is sharing a ride with another, whether a neighbor, family member, or classmate. Sharing a ride will save the driver and passenger(s) money and time, reduce stress, and save the environment. Vanpooling is similar to carpooling, however with additional passengers. It is an economical option for those who commute over long distances.\nCommuter Connections is a regional network of transportation organizations coordinated by the Metropolitan Washington Council of Governments. If you live or work in the Metropolitan Washington D.C. area, Commuter Connections can provide you with information on all your commute options, so you can make a smart choice about how you travel to work. Commuter Connections also helps employers establish commuting benefits and assistance programs, including telework/telecommute programs, for their employees.\nThe City of Alexandria regulates taxicabs operating in the city. Meter rates charged by taxicabs are set by the Alexandria City Council, and are contained in 9-12-132 of the Alexandria City Code. All taxicabs and taxicab drivers must be licensed by the City of Alexandria.\nThe Potomac Riverboat Company offers seasonal sightseeing cruises, private parties, and water taxi transportation service. Cruises include the Washington monuments, the Alexandria seaport, pirate cruises for children, baseball cruises, student music cruises, and the Mount Vernon Estate. There are two water taxi services: one operates between Alexandria and the National Harbor, one travels up to the National Mall. Docking locations include: Alexandria City Marina, Georgetown and the National Mall in Washington, D.C., National Harbor and Gaylord National Resort in Maryland, and George Washington’s Mount Vernon Estate."", 'What if every U.S. city linked its urban core with surrounding suburban cities and prime outdoor recreation? And, what if city trails became the standard gateway to access the outdoors for all by improving community connections, health, quality of life, economic development and safe alternative commute options?\nLuckily a coalition of diverse urban leaders, planners, politicians, nonprofits and citizens is emerging to make this vision a reality.\nThe latest transformation is happening in Washington, D.C. REI is working with the Capital Trails Coalition and Washington Area Bicyclist Association (WABA) to rewrite the city’s traffic patterns and unlock a healthier, more connected community. D.C.’s Capital Trails Coalition is creating a 20-year plan that will create one of the largest trail networks in the U.S. and stand as a template for other American cities.\nThe centerpiece of the effort is a comprehensive network of hundreds of miles of multiuse pathways throughout D.C., Virginia and Maryland, offering users easy access to parklands, museums, offices, hospitals and shops. The new trail network will feature world-class amenities, such as easily accessible trailheads, ample signage and wayfinding maps, comfort stations, public art, and interpretive signs articulating historic sites and natural environments.\nThis regional initiative builds on 360 miles of trails that already exist, but which remain disconnected. And the work will impact every corner of the District, including underserved neighborhoods like Anacostia and Fort Lincoln that have historically struggled to get support for this type of infrastructure. Washington, D.C. is a prime candidate for this bike/pedestrian-led renaissance.\n“This Rewilding Project will fundamentally rewire the way that my city gets around. D.C. is already one of the best cities in the country for biking. But the Capital Trails Coalition sets a vision for it to be one of the best cycling cities in the world.”\n—Matt Liddle, D.C. manager for REI Outdoor Programs\nD.C. is relatively flat, and nearly 39 percent of its residents don’t own cars. Early efforts to get more people on bikes, like the Capital Bikeshare program, have been a huge success—last year alone, D.C. residents took more 3 million trips on the city’s European-inspired red bicycles.\nBut there are also serious challenges, which are requiring D.C. community leaders to collaborate. High-traffic roads like New York Avenue create gaps between safe, protected bike lanes and effectively lock off entire sections of the city from bike commuters. That makes it harder for people to get to work, green spaces and school. Any effort to build new trail involves navigating multiple jurisdictions, as sections of the land are managed by the D.C. Department of Transportation, Maryland’s Prince George and Montgomery counties, jurisdictions in Virginia—including the City of Alexandria and Arlington and Fairfax counties—and six different units of the National Park Service.\nD.C.’s unlocked potential, as well as the complex path to building a connected community, make it an ideal fit for REI’s Rewilding Project. The Rewilding Project supports multi-year projects that help dramatically reshape how large urban areas connect with the outdoors. In D.C., REI is supporting the Capital Trail Coalition with a $500,000 grant over four years, and is working directly with WABA to help bring this vision to life.\nIn 2014, REI sponsored WABA’s second-annual D.C. Trails Summit, where WABA laid out an ambitious plan—creating a bike infrastructure in D.C. that rivals that of renowned bike-first cities such as Copenhagen. Greg Billing, executive director for WABA, recalls a key moment in that meeting. “I stood up and said, ‘Look, we want to create a coalition to work on this big vision and these are the kinds of things we want to do together.’ And a number of people stood up and said that they wanted to be part of it, that they wanted to be founding members. It was a real tent-revival moment,” recalls Greg.\n“I want the process to be as transparent as possible. Every decision that we make, every conversation that we have, is captured so that other cities can learn from that process.""\n—Katie Harris, trail coalition coordinator for the WABA\nThat meeting set D.C. on the course to radically reimagine the city’s infrastructure. Regional and national organizations and government agencies—including the National Park Service, Rails-to-Trails Conservancy and the American Public Health Association—all agreed to work together.\nOnly two years later, D.C. residents from across the region are at the table co-creating a vision of a more-connected city.\nFort Lincoln in D.C.’s Ward 5 sits at the intersection of numerous busy roads. Though the area boasts more than 100,000 square feet of retail space, there’s no safe passage for non-motorists, and public transport is a time-consuming hassle. So when Robert Looper ran for Ward 5 Advisory Neighborhood Commissioner in 2014, dedicated cycling routes were part of his grassroots-defined campaign. He won in a landslide, and joined the work of the Capital Trails Coalition.\nThe proposed plan now covers neighboring Wards 7 and 8 in addition to Looper’s Ward 5, bringing much-needed resources to other underserved parts of the city. “Good jobs in D.C. typically take an hour and a half for my residents to reach via public transit,” says Looper, “and when this work is realized they’ll just hop on their bikes to get there. I want to make sure the city recognizes that this is not just about building infrastructure, but that it affects people’s lives and how they get around.”\nThere are currently five Rewilding Project efforts underway: in the San Francisco area, Los Angeles, Chicago and Seattle, in addition to D.C. These grants are part of a larger REI community impact effort that invested $9M in nonprofits across the country in 2016.\nTake a look at our grant programs, currently in the works.\nBay Area Ridge Trail: When complete, the multi-use Bay Area Ridge Trail will total 550 miles and circumnavigate the Bay Area, serving 8 million people. Currently, 365 miles of the trail are open to the public. Our investment is helping accelerate this project\'s completion by increasing the organization’s capacity and supporting efforts to convene key stakeholders in order to increase alignment and awareness about this regional project.\nWashington, D.C., Multiuse Trail Connectivity: Our half-million-dollar investment over four years is supporting the completion of a regional multiuse trail network in the Washington, D.C., area. Our support of the Washington Area Bicyclist Association (WABA), Rails-to-Trails Conservancy and The Trust for Public Lands will accelerate the bold vision of a regional trail network: connecting existing trails with new ones so that more communities, especially those underserved, can access outdoor recreation and alternative transportation.\nSoutheast Chicago: REI\'s investment is supporting the long-term transformation of the Calumet area from brownfields and industrial lands into a thriving outdoor recreation hub. Specific work includes: development of the 278-acre Big Marsh property to create one of the biggest mountain-bike parks in the country, mixed-use trails, paddling opportunities and much more; developing a brand for the area that will bring awareness to the new outdoor recreation opportunities; and, transportation planning to connect communities to Big Marsh and otherwise improve access. Our partners are the Chicago Park District, Active Transportation Alliance and Friends of Big Marsh.\nSan Gabriel Mountains National Monument: The San Gabriel Mountains National Monument in the Angeles National Forest provides recreational opportunities for millions of people in the Los Angeles area. Our investment in the National Forest Foundation and The Wilderness Society supports sustainable access to the San Gabriels, restoring and rerouting trails, enhancing recreation infrastructure, and increasing connectivity between the city and this special place by linking urban trails and developing alternative transportation.\nSeattle-area Duwamish River Valley Trail Connectivity: This project seeks to advance environmental justice, address racial and neighborhood-level disparities, reduce health inequities, and create stronger economic opportunities through trail and open space connectivity. When complete, there will be a 15-mile mixed-use trail from the downtown Seattle waterfront to South Park and Georgetown. This project will fill a significant gap in Seattle’s urban trail system, and it represents the first phase of building a city-wide network to connect every neighborhood with the larger regional trail system. Our partners include: Seattle Parks Foundation, Forterra, Cascade Bicycle Club, Trust for Public Land, and Mountains to Sound Greenway.\nPhotography by Carina Skrobecki.']"	['<urn:uuid:0f7758d8-b0a9-4420-a867-47bfa855eef9>', '<urn:uuid:1729e01b-70fe-430b-ac12-ed4077b7f2ae>']	factoid	with-premise	short-search-query	similar-to-document	three-doc	novice	2025-05-12T20:46:45.703947	6	71	2504
42	I'm researching the relationship between vitamin B12 deficiency and anti-inflammatory drug therapy - how do these conditions affect nerve function, and what are the risks of combining these treatments?	Vitamin B12 deficiency can cause peripheral neuropathy, disrupting nerve function and leading to symptoms like tingling, numbness, and loss of muscular control. When this condition occurs in patients also taking NSAIDs with anticoagulants, there is a serious risk of bleeding complications. The combination of these medications can affect blood's ability to clot, leading to increased risk of hemorrhage. Healthcare professionals should be consulted before combining these treatments, as patients may need careful monitoring for both bleeding episodes and neurological symptoms.	"[""Selected Anticoagulants (Vit K antagonists)/NSAIDs Interactions\nThis information is generalized and not intended as specific medical advice. Consult your healthcare professional before taking or discontinuing any drug or commencing any course of treatment.\nSerious. These medicines may interact and cause very harmful effects. Contact your healthcare professional (e.g. doctor or pharmacist) for more information.\nHow the interaction occurs:\nBoth of these medicines can affect your blood's ability to clot.\nWhat might happen:\nTaking these medicines together may increase your risk for bleeding. Symptoms may include bleeding from your gums, nosebleeds, unusual bruising, or dark stools.\nWhat you should do about this interaction:\nMake sure your healthcare professionals (e.g. doctor or pharmacist) know that you are taking these medicines together.Let your doctor know right away if you have any bleeding episodes or signs of bleeding.Your healthcare professionals may already be aware of this interaction and may be monitoring you for it. Do not start, stop, or change the dosage of any medicine before checking with them first.\n- 1.Thilo D, Nyman D. A study of the effects of the anti-rheumatic drug ibuprofen (Brufen) on patients being treated with the oral anti-coagulant phenprocoumon (Marcoumar). J Int Med Res 1974;2:276-8.\n- 2.Boekhout-Mussert MJ, Loeliger EA. Influence of ibuprofen on oral anti-coagulant with phenprocoumon. J Int Med Res 1974;2:279-83.\n- 3.Penner JA, Abbrecht PH. Lack of interaction between ibuprofen and warfarin. Curr Ther Res Clin Exp 1975 Dec;18(6):862-71.\n- 4.Michot F, Ajdacic K, Glaus L. A double-blind clinical trial to determine if an interaction exists between diclofenac sodium and the oral anticoagulant acenocoumarol (nicoumalone). J Int Med Res 1975;3(3):153-7.\n- 5.Marbet GA, Duckert F, Walter M, Six P, Airenne H. Interaction study between phenprocoumon and flurbiprofen. Curr Med Res Opin 1977;5(1):26-31.\n- 6.Slattery JT, Levy G, Jain A, McMahon FG. Effect of naproxen on the kinetics of elimination and anticoagulant activity of a single dose or warfarin. Clin Pharmacol Ther 1979 Jan;25(1):51-60.\n- 7.Jain A, McMahon FG, Slattery JT, Levy G. Effect of naproxen on the steady-state serum concentration and anticoagulant activity of warfarin. Clin Pharmacol Ther 1979 Jan;25(1):61-6.\n- 8.Loftin JP, Vesell ES. Interaction between sulindac and warfarin: different results in normal subjects and in an unusual patient with a potassium-losing renal tubular defect. J Clin Pharmacol 1979 Nov-Dec; 19(11-12):733-42.\n- 9.Carter SA. Potential effect of sulindac on response of prothrombin-time to oral anticoagulants. Lancet 1979 Sep 29;2(8144):698-9.\n- 10.Ross JR, Beeley L. Sulindac, prothrombin time, and anticoagulants. Lancet 1979 Nov 17;2(8151):1075.\n- 11.Stricker BH, Delhez JL. Interactions between flurbiprofen and coumarins. Br Med J (Clin Res Ed) 1982 Sep 18;285(6344):812-3.\n- 12.Dahl SL, Ward JR. Pharmacology, clinical efficacy, and adverse effects of piroxicam, a new nonsteroidal anti-inflammatory agent. Pharmacotherapy 1982 Mar-Apr;2(2):80-90.\n- 13.Rhodes RS, Rhodes PJ, Klein C, Sintek CD. A warfarin-piroxicam drug interaction. Drug Intell Clin Pharm 1985 Jul-Aug;19(7-8):556-8.\n- 14.Flessner MF, Knight H. Prolongation of prothrombin time and severe gastrointestinal bleeding associated with combined use of warfarin and ketoprofen. JAMA 1988 Jan 15;259(3):353.\n- 15.Griffin MR, Piper JM, Daugherty JR, Snowden M, Ray WA. Nonsteroidal anti-inflammatory drug use and increased risk for peptic ulcer disease in elderly persons. Ann Intern Med 1991 Feb 15;114(4):257-63.\n- 16.Gabriel SE, Jaakkimainen L, Bombardier C. Risk for serious gastrointestinal complications related to use of nonsteroidal anti-inflammatory drugs. A meta-analysis. Ann Intern Med 1991 Nov 15; 115(10):787-96.\n- 17.Shorr RI, Ray WA, Daugherty JR, Griffin MR. Concurrent use of nonsteroidal anti-inflammatory drugs and oral anticoagulants places elderly persons at high risk for hemorrhagic peptic ulcer disease. Arch Intern Med 1993 Jul 26;153(14):1665-70.\n- 18.Hilleman DE, Mohiuddin SM, Lucas BD Jr. Nonsteroidal antiinflammatory drug use in patients receiving warfarin: emphasis on nabumetone. Am J Med 1993 Aug 9;95(2A):30S-34S.\n- 19.Mieszczak C, Winther K. Lack of interaction of ketoprofen with warfarin. Eur J Clin Pharmacol 1993;44(2):205-6.\n- 20.Celebrex (celecoxib) US prescribing information. Pfizer Inc. May, 2019.\n- 21.Vioxx (rofecoxib) US prescribing information. Merck & Co., Inc. May, 2016.\n- 22.Kent AP, Brueckmann M, Fraessdorf M, Connolly SJ, Yusuf S, Eikelboom JW, Oldgren J, Reilly PA, Wallentin L, Ezekowitz MD. Concomitant Oral Anticoagulant and Nonsteroidal Anti-Inflammatory Drug Therapy in Patients With Atrial Fibrillation. J Am Coll Cardiol 2018 Jul 17;72(3):255-267.\n- 23.Zhou M, Leonard CE, Brensinger CM, Bilker WB, Kimmel SE, Hecht TEH, Hennessy S. Pharmacoepidemiologic Screening of Potential Oral Anticoagulant Drug Interactions Leading to Thromboembolic Events..\nCONDITIONS OF USE: The information in this database is intended to supplement, not substitute for, the expertise and judgment of healthcare professionals. The information is not intended to cover all possible uses, directions, precautions, drug interactions or adverse effects, nor should it be construed to indicate that use of a particular drug is safe, appropriate or effective for you or anyone else. A healthcare professional should be consulted before taking any drug, changing any diet or commencing or discontinuing any course of treatment."", 'What does fact checked mean?\nAt Healthfully, we strive to deliver objective content that is accurate and up-to-date. Our team periodically reviews articles in order to ensure content quality. The sources cited below consist of evidence from peer-reviewed journals, prominent medical organizations, academic associations, and government data.\nThe information contained on this site is for informational purposes only, and should not be used as a substitute for the advice of a professional health care provider. Please check with the appropriate physician regarding health questions and concerns. Although we strive to deliver accurate and up-to-date information, no guarantee to that effect is made.\nPeripheral neuropathy is a condition that involves a disruption of the neuro-electrical synapses from the brain and spinal cord to the muscles in the limbs and other parts of the body, according to MedlinePlus 23. The nerves lose their function and thus affect the patient’s ability to feel pain, and they may lose their sense of taste. Among other effects, this results in a loss of muscular control, painful tingling, numbness and loss of sensation. Peripheral neuropathy has several possible causes, among them a lack of Vitamin B12 23.\nMononeuropathy affects a single nerve group and causes weakness in specific, isolated parts of the body. Examples include carpal tunnel syndrome, which affects the wrist nerves, and peroneal nerve palsy, which affects the nerve behind the knee.\nPolyneuropathy affects multiple nerve groups and is far more common than mononeuropathy. The affected nerves are usually in different parts of the body, such as one arm and one leg.\n- Peripheral neuropathy most commonly appears as a symptom of a disease or injury and has two main variants, as noted by the University of Chicago Center for Peripheral Neuropathy 2.\n- Mononeuropathy affects a single nerve group and causes weakness in specific, isolated parts of the body.\nRisk Factors and Causes\nVitamin Supplements for Nerve Pain\nSeveral potential causes for peripheral neuropathy exist 234. Mononeuropathy is most often caused by physical injury or accident-related trauma. Pressure on a nerve for extended periods is also a very common cause. Polyneuropathy can be caused by a large variety of factors, such as poor nutrition, complications from kidney failure and exposure to specific types of toxins. One possible cause of peripheral neuropathy is chronic deficiency of vitamin B12 123.\nVitamin B12 Deficiency\nVitamin B12 is an essential dietary nutrient for maintaining health, according to the American Academy of Family Physicians 1. A deficiency can be caused by several factors. A strict vegetarian diet eliminates the only sources of dietary B12, such as:\n- dairy products\n- red meat\nBurning Feet & B Vitamins\nPeripheral neuropathy usually manifests in the longer nerves in the body and thus often starts in the hands and feet, as these are the most susceptible to injury and damage, says Pain Clinic 23. This condition usually affects both sides of the body symmetrically at the same time.\nThe damage to sensory fibers causes burning sensations, tingling, numbness, nerve pain or an inability to feel one’s joint positioning. This can potentially cause accidents due to the patient’s lack of coordination and inability to feel warning pain. Cramps, loss of muscle mass and loss of muscle control are also associated symptoms.\n- Peripheral neuropathy usually manifests in the longer nerves in the body and thus often starts in the hands and feet, as these are the most susceptible to injury and damage, says Pain Clinic 2.\n- This can potentially cause accidents due to the patient’s lack of coordination and inability to feel warning pain.\nSeveral remedies can be used for the pain caused by peripheral neuropathy 23. These include anti-seizure medications, anti-depressants, lidocaine patches, and general pain relievers. However, proper treatment of peripheral neuropathy involves addressing the root cause of the problem, not just the symptoms 23.\nAddressing vitamin B12 deficiency can help eliminate pain by restoring the nerve sheaths and promoting regeneration of nerve cells. The most direct treatment is increasing the intake of B12. Oral B12 therapy and injections are viable options. Dietary B12 can be boosted by increasing consumption of fish, red meat, dairy products, poultry and B12 supplements.\n- Several remedies can be used for the pain caused by peripheral neuropathy 2.\n- Dietary B12 can be boosted by increasing consumption of fish, red meat, dairy products, poultry and B12 supplements.\nVitamin Supplements for Nerve Pain\nBurning Feet & B Vitamins\nCauses of Numbness in the Extremities\nSacral Nerve Damage Symptoms\nAutoimmune Diseases That Cause Peripheral Neuropathy\nHow to Unpinch a Sciatic Nerve\nCauses of Foot Numbness & Pain\nB1 and B12 Vitamins to Treat Peripheral Neuropathy\nEffects of a Pinched Nerve in the Neck\nL5-S1 Pinched Nerve Symptoms\n- American Academy of Family Physicians: Vitamin B12 Deficiency\n- MedlinePlus: Peripheral neuropathy\n- Pain Clinic: Nerve Pain--Peripheral Neuropathy\n- University of Chicago Center for Peripheral Neuropathy: Types of Peripheral Neuropathy\n- Colloca L, Ludman T, Bouhassira D, et al. Neuropathic pain. Nat Rev Dis Primers. 2017;3:17002. Published 2017 Feb 16. doi:10.1038/nrdp.2017.2\n- Lima MA, Maranhão-filho P. What is the essential neurological examination?. Arq Neuropsiquiatr. 2012;70(12):939-41. doi:10.1590/s0004-282x2012001200007\n- Marchettini P, Lacerenza M, Mauri E, Marangoni C. Painful peripheral neuropathies. Curr Neuropharmacol. 2006;4(3):175–181. doi:10.2174/157015906778019536\n- Sommer C, Geber C, Young P, Forst R, Birklein F, Schoser B. Polyneuropathies. Dtsch Arztebl Int. 2018;115(6):83–90. doi:10.3238/arztebl.2018.083\n- Koop LK, Tadi P. Neuroanatomy, Sensory Nerves. [Updated 2019 Apr 7]. In: StatPearls [Internet]. Treasure Island (FL): StatPearls Publishing; 2019 Jan-.\n- McCorry LK. Physiology of the autonomic nervous system. Am J Pharm Educ. 2007;71(4):78. doi:10.5688/aj710478\n- Tracy JA, Dyck PJ. The spectrum of diabetic neuropathies. Phys Med Rehabil Clin N Am. 2008;19(1):1–v. doi:10.1016/j.pmr.2007.10.010\n- Ekabe CJ, Kehbila J, Abanda MH, Kadia BM, Sama CB, Monekosso GL. Vitamin B12 deficiency neuropathy; a rare diagnosis in young adults: a case report. BMC Res Notes. 2017 Jan 28;10(1):72. doi: 10.1186/s13104-017-2393-3\n- Dimachkie MM, Barohn RJ. Guillain-Barré syndrome and variants. Neurol Clin. 2013;31(2):491–510. doi:10.1016/j.ncl.2013.01.005\n- Menorca RM, Fussell TS, Elfar JC. Nerve physiology: mechanisms of injury and recovery. Hand Clin. 2013;29(3):317–330. doi:10.1016/j.hcl.2013.04.002\n- Aroori S, Spence RA. Carpal tunnel syndrome. Ulster Med J. 2008;77(1):6–17. PMID: 18269111\n- Cojocaru IM, Cojocaru M, Silosi I, Vrabie CD. Peripheral nervous system manifestations in systemic autoimmune diseases. Maedica (Buchar). 2014 Sep;9(3):289-94.\n- Misra UK, Kalita J, Nair PP. Diagnostic approach to peripheral neuropathy. Ann Indian Acad Neurol. 2008;11(2):89–97. doi:10.4103/0972-2327.41875\n- Jensen TS, Finnerup NB. Allodynia and hyperalgesia in neuropathic pain: clinical manifestations and mechanisms. Lancet Neurol. 2014;13(9):924-35. doi:10.1016/S1474-4422(14)70102-4\n- Aman JE, Elangovan N, Yeh IL, Konczak J. The effectiveness of proprioceptive training for improving motor function: a systematic review. Front Hum Neurosci. 2015;8:1075. Published 2015 Jan 28. doi:10.3389/fnhum.2014.01075\n- Desai J, Swash M. Fasciculations: what do we know of their significance?. J Neurol Sci. 1997;152 Suppl 1:S43-8. doi:10.1016/s0022-510x(97)00243-8\n- Sommer C, Geber C, Young P, Forst R, Birklein F, Schoser B. Polyneuropathies. Dtsch Arztebl Int. 2018 Feb 9;115(6):83-90. doi: 10.3238/arztebl.2018.083.\n- Wells R, Tonkin A. Clinical approach to autonomic dysfunction. Intern Med J. 2016;46(10):1134-1139. doi:10.1111/imj.13216\n- Chopra K, Tiwari V. Alcoholic neuropathy: possible mechanisms and future treatment possibilities. Br J Clin Pharmacol. 2012;73(3):348–362. doi:10.1111/j.1365-2125.2011.04111.x\n- Hammond N, Wang Y, Dimachkie MM, Barohn RJ. Nutritional neuropathies. Neurol Clin. 2013;31(2):477–489. doi:10.1016/j.ncl.2013.02.002\n- Peltier AC, Russell JW. Recent advances in drug-induced neuropathies. Curr Opin Neurol. 2002;15(5):633-8. doi:10.1097/00019052-200210000-00015\n- Crook L, Liu PI, Gadsden RH, Turner RE. Erythrocyte sedimentation, viscosity, and plasma proteins in disease detection. Ann Clin Lab Sci. 1980;10(5):368-76. PMID: 7425532\n- Miller LJ, Saporta AS, Sottile SL, Siskind CE, Feely SM, Shy ME. Strategy for genetic testing in Charcot-Marie-disease. Acta Myol. 2011;30(2):109–116. PMID: 22106713\n- Chung T, Prasad K, Lloyd TE. Peripheral neuropathy: clinical and electrophysiological considerations. Neuroimaging Clin N Am. 2014;24(1):49–65. doi:10.1016/j.nic.2013.03.023\n- Weis J, Brandner S, Lammens M, Sommer C, Vallat JM. Processing of nerve biopsies: a practical guide for neuropathologists. Clin Neuropathol. 2012;31(1):7–23. doi:10.5414/np300468\n- Ziemssen T, Siepmann T. The Investigation of the Cardiovascular and Sudomotor Autonomic Nervous System-A Review. Front Neurol. 2019;10:53. Published 2019 Feb 12. doi:10.3389/fneur.2019.00053\n- Khan N, Smith MT. Multiple sclerosis-induced neuropathic pain: pharmacological management and pathophysiological insights from rodent EAE models. Inflammopharmacology. 2014;22(1):1–22. doi:10.1007/s10787-013-0195-3\n- Institute of Medicine (US) Committee on Multiple Sclerosis: Current Status and Strategies for the Future; Joy JE, Johnston RB Jr., editors. Multiple Sclerosis: Current Status and Strategies for the Future. Washington (DC): National Academies Press (US); 2001. 3, Characteristics and Management of Major Symptoms.\n- Chandra A, Stone CR, Du X, et al. The cerebral circulation and cerebrovascular disease III: Stroke. Brain Circ. 2017;3(2):66–77. doi:10.4103/bc.bc_12_17\n- Bansal V, Kalita J, Misra UK. Diabetic neuropathy. Postgrad Med J. 2006;82(964):95–100. doi:10.1136/pgmj.2005.036137\n- Staff NP, Windebank AJ. Peripheral neuropathy due to vitamin deficiency, toxins, and medications. Continuum (Minneap Minn). 2014;20(5 Peripheral Nervous System Disorders):1293–1306. doi:10.1212/01.CON.0000455880.06675.5a\n- Meena AK, Khadilkar SV, Murthy JM. Treatment guidelines for Guillain-Barré Syndrome. Ann Indian Acad Neurol. 2011;14(Suppl 1):S73–S81. doi:10.4103/0972-2327.83087\n- Hammi C, Yeung B. Neuropathy. [Updated 2019 Jun 4]. In: StatPearls [IntAvailable from:\n- Przewlocki R, Przewlocka B. Opioids in neuropathic pain. Curr Pharm Des. 2005;11(23):3013-25. doi:10.2174/1381612054865055\n- Juster-Switlyk K, Smith AG. Updates in diabetic peripheral neuropathy. F1000Res. 2016;5:F1000 Faculty Rev-738. Published 2016 Apr 25. doi:10.12688/f1000research.7898.1\n- Cioroiu C, Weimer LH. Update on Chemotherapy-Induced Peripheral Neuropathy. Curr Neurol Neurosci Rep. 2017 Jun;17(6):47. doi: 10.1007/s11910-017-0757-7.\n- Dimitrova A, Murchison C, Oken B. Acupuncture for the Treatment of Peripheral Neuropathy: A Systematic Review and Meta-Analysis. J Altern Complement Med. 2017 Mar 1;23(3):164-79. doi: 10.1089/acm.2016.0155\n- Watson JC, Dyck PJ. Peripheral Neuropathy: A Practical Approach to Diagnosis and Symptom Management. Mayo Clin Proc. 2015 Jul;90(7):940-51. doi: 10.1016/j.mayocp.2015.05.004\nJoseph Pritchard graduated from Our Lady of Fatima Medical School with a medical degree. He has spent almost a decade studying humanity. Dr. Pritchard writes as a San Francisco biology expert for a prominent website and thoroughly enjoys sharing the knowledge he has accumulated.']"	['<urn:uuid:5f6ea87c-4c94-482e-9a5f-d81e9f703fe1>', '<urn:uuid:66ff0b37-5d08-45b7-b026-e64a0fdbfc51>']	open-ended	with-premise	verbose-and-natural	distant-from-document	multi-aspect	expert	2025-05-12T20:46:45.703947	29	80	2432
43	What makes the WAVE bioreactor system beneficial for production, and how does its cost tracking approach compare to job costing methods?	The WAVE bioreactor system is beneficial for production due to its noninvasive rocking technology that creates waves, providing suitable conditions for various cell lines, and its unique internal floating filter that eliminates the need for external cell separators. It improves process flexibility, shortens turnover times, and eliminates cleaning and sterilization requirements. Regarding cost tracking, unlike job costing which requires extensive record keeping, the WAVE system aligns with process costing principles that require less bookkeeping and is more suitable for standardized, continuous production processes.	"['Disposable bioreactors are increasingly used for cell cultivation because they improve process flexibility, shorten turnover times, and eliminate the requirement for cleaning and sterilization. This week at ESACT in Lille, France, GE Healthcare Life Sciences announced a new addition to the disposable bioreactor market; a next generation WAVE Bioreactor™, ReadyToProcess WAVE™ 25.\nAn early pioneer in the area of disposable bioreactors, the WAVE Bioreactor was the first single-use bioreactor system when it was presented to the bioprocessing community in 1996. Since then, the range of products has expanded to include larger scale systems and advanced optical sensor and control technologies. WAVE Bioreactor systems are today widely used in both research and manufacturing operations.\nWAVE Bioreactors have a design based on a noninvasive rocking technology that creates waves. The result is a simple but powerful instrument that meets all the conventional requirements for a bioreactor in terms of, for example, mixing and aeration. The rocking motion can be adjusted to provide conditions suitable for a variety of cell lines, such as robust CHO production cell lines with a high oxygen demand and more delicate primary cells and stem cells. WAVE Bioreactors also have a unique internal floating filter, which retains the cells in the bioreactor and filters only the media, eliminating the need for an external cell separator and flow loop.\nCell Culture Applications\nThe most popular applications include protein and virus production in smaller scale batch and fed-batch culture, and seed-train cultivations for scale-up to a larger bioreactor. For perfusion applications, the WAVE Bioreactor is quite flexible. For example, animal cell lines such as CHO, HEK, NS0, S2 and SF9 have been cultivated successfully in WAVE Bioreactor systems (1–7). In addition to the conventional applications, researchers are exploring novel ways of using the wave technology. I have included specific examples below of the innovative ways that the WAVE Bioreactor is being used in cell culture.\nIn traditional seed-train expansion, a series of fed batch cultures are gradually increased in volume until adequate cell number is reached for the final production bioreactor. Maximum cell concentration limits split ratios and as a result several bioreactors are typically required for expansion. By using WAVE Bioreactors in perfusion mode, 5-10 fold higher split ratios have been achieved when compared with traditional batch culture. Utilizing perfusion and the WAVE Bioreactor system in seed-train cultivation can enable a reduction in the number of expansion steps and the reduction of the number of bioreactors used. Disposable bioreactors with just a few liters of working volume have the potential to inoculate up to 1,000 liters of production culture.\nInsect Cell Cultivation\nInsect cell culture can be shear-sensitive and it requires a high supply of oxygen, higher than mammalian cells require. The WAVE Bioreactor has the capability of providing a high oxygen supply, which makes it a good choice in insect cell cultivation. In addition, the Cellbag permits both cell expansion for infection and the infection itself to occur inside the bioreactor, which limits challenging transfers. In one example, insect cells were seeded at a low volume (10 liters), then cells were grown to the appropriate cell density and culture can was increased (100 liters) to allow for infection. The virus seed can also be conveniently expanded in a bioreactor of the same type.\nPlant Cell Cultivation\nPlant cell culture is becoming more widely used in many areas including plant research, and in both naturally occurring and recombinant compound production. WAVE Bioreactors provide a good choice for plant cells because of the low-shear factor. Plant cells can be particularly sensitive to shear stress and shear that is too high can cause a reduction in cell viability, changed morphology, impaired growth, and more. In addition, the absence of an internal impeller or sparger in the WAVE Bioreactor eliminates problems of clogging or tangling in hairy root cell culture. Finally, many plant cell cultures are light-dependent and the open design of the WAVE Bioreactor enables easy access to light from a source placed nearby.\nThe New WAVE\nThe latest WAVE bioreactor, ReadyToProcess WAVE 25, is available for cultures from 300 mL to 25 L scale, and combines the ease-of use that comes with the rocking technology as such, with intelligent control and advanced sensor technology. A short video preview of the system was made available online June 24th. Registration to watch the video can be done by following this link https://promo.gelifesciences.com/gl/K12355/mail-june.html.\n- 1. Singh, V. Disposable bioreactor for cell culture using wave-induced agitation. Cytotechnology 30, 149–158 (1999).\n- 2. Namdev, P. K. and Lio, P. Assessing a disposable bioreactor for attachment dependent cell cultures. BioPharm Int. 13, 44–50 (2000).\n- 3. Tang Y. J. et al. Perfusion culture of hybridoma cells for hyperproduction of IgG(2a) monoclonal antibody in a wave bioreactor-perfusion culture system. Biotechnol. Prog. 23, 255–264 (2007).\n- 4. Birch, J. R, and Racher, A. J. Antibody production. Adv. Drug Deliv. Rev, 58, 671−685 (2006).\n- 5. Chartrain, M. and Chu, L. Development and production of commercial therapeutic monoclonal antibodies in mammalian cell expression systems: an overview of the current upstream technologies. Curr. Pharm. Biotechnol. 9, 447–467 (2008).\n- 6. Clincke M. et al. Very high density of CHO cells in perfusion by ATF or TFF in WAVE bioreactor™. Part I: Effect of the cell density on the process. Biotechnol Prog. 2013 Feb 22. DOI: 10.1002/btpr.1704.\n- 7. Clincke M. et al. Very high density of CHO cells in perfusion by ATF or TFF in WAVE bioreactor™. Part II: Applications for antibody production and cryopreservation. Biotechnol Prog. 2013 May 21. DOI: 10.1002/btpr.1703.', ""What is process costing?\n* Process costing\nis the method involving allocation of manufacturing cost to products in order to determine per unit cost.\n* It is used for those products, which are similar and identical.\n* The primary focus of this method is on the processes involved in producing homogeneous products.\n* Process costing is applicable where output results from continuous or repetitive operations and products are identical and cannot be distinguished.\n* It is used to trace and accumulate the direct cost and is also used in the allocation of the indirect costs of a manufacturing process.\n* It is a type of operation costing, which is used to ascertain the cost of a product at each stage or each process of manufacturing.\n* Process costing involves certain expectations about standard costs and can be subjective; therefore, process costing is only appropriate for companies that produce large units of a product.\n* Normal losses and abnormal losses may also arise in production. Units of normal loss will be included in the equivalent units whereas units of abnormal loss will not be included in the equivalent units.\nWhy is process costing important?\nIt is important because it helps keep track of where money is spent in the distribution and production processes. Process costing system requires less bookkeeping than job costing system.\nWhat are the advantages and disadvantages of the process costing system?\nAdvantages of process costing system\n* Process costing helps save time because in process costing, allocation is done on the basis of the number of production processes.\n* It is an easier system to use when costing is related to homogenous products.\n* It is the most flexible method of costing and helps ensure that production can take place at the most competitive price in the economic marketplace.\n* It helps in determining the weak link in a production chain.\n* It is considered flexible because of one more reason that process costing allows companies to remove or add a process as necessary.\n* It helps in controlling the production process effectively.\n* Discrepancy or inefficiency in a specific department or process can be easily tracked without even checking each process or department.\n* In process costing, the overall cost from each process or department is collected and the cost of specific job is ignored, which makes process costing easier and simple.\nDisadvantages of process costing system\n* The major disadvantage of process costing is that it creates cost errors in the production system.\n* Process costing is not applicable in a situation where different products are produced.\n* In case of partial products, it becomes impossible to apply process costing.\n* It is not useful for decision making and managerial control.\n* The cost obtained by this method is historical cost, which is of less use for managerial decisions.\nIn spite of many advantages and disadvantages, still process costing is considered a useful costing approach for a few companies, especially for those that are into the production of homogenous products and that too in bulk.\nWhen is process costing applied?\nWhen companies are to produce a product in bulk and a series of operations is required, process costing is applied. It is used when the production process is not affected by one product. It is applied in industries like coal mining, petroleum, textiles, chemicals, food, glass, plastic and paper.\nWhat is the difference between process costing and job costing?\n1. Process costing is used by companies that produce identical or similar products whereas job costing is used by companies that produce unique products.\n2. Companies that use process costing normally deal in paint, petroleum, chewing gum etc. Companies that use job costing are associated with advanced technology system, airplanes, accounting etc.\n3. Record keeping: Job costing requires a lot more record keeping than process costing.\n4. Uniqueness of product: Job costing is used for unique products whereas process costing is used for standardized products.\n5. Size of job: Process costing is used for large production runs whereas job costing is used for small production runs.\n6. Job costing is applied to ascertain the specific order cost whereas process costing is applied to ascertain standardized products cost.\n7. Job costing helps accumulate the manufacturing cost for each job, whereas process costing helps accumulate the manufacturing costs for certain processes or departments.\n8. In process costing, costs are accumulated for certain months or years whereas in job costing, cost is accumulated for the whole job irrespective of the time period.\n9. In process costing, the total process or department cost is divided by the process output in order to calculate the unit cost, whereas in job costing, the total cost of job is divided by job order units to calculate the unit cost.\n10. Job costing can be carried out while a particular job is going on, whereas process costing can be carried only when a process has been completed.\n11. In job costing, the final value of a cost can be calculated beforehand whereas in process costing, the final value of cost is calculated only at the end of a process.\n12. Job costing is used by industries that produce heterogeneous products, whereas process costing is used by industries that produce homogenous products.\nWhat are the steps involved in process costing?\nThere are five steps in process costing, which are mentioned below:\nStep 1. Summarizing the flow of physical units of output.\nStep 2. Computing output in terms of equivalent units.\nStep 3. Computing equivalent unit costs.\nStep 4. Summarizing total costs to account for.\nStep 5. Assigning total costs to units completed and to units in final phase in process inventory.\nDiscuss the various methods involved in process costing.\nThe various process costing methods are as follows:\nMethod 1. Process costing FIFO\nThe First-in, first-out (FIFO) process-costing method involves assigning cost to the earliest equivalent units available. In this method, it is assumed that the earliest equivalent units in work in process are completed first.\nMethod 2. Process costing LIFO\nThe last-in, first out (LIFO) process costing method involves assigning cost to the latest equivalent units available. In this method, it is assumed that the latest equivalent units in work in process are used first.\nMethod 3. Process costing weighted average\nThe weighted-average process-costing method involves assigning average equivalent unit cost to all the work done until a certain date to equivalent units completed and transferred out, and to equivalent units in ending inventory. It is simply the average of various equivalent unit costs entered in the work in process account.\nLet’s take one process costing example:\nStep1. Physical units\nWork in process, beginning\n60% conversion costs 1000\nUnits started in process 35000 36000\nUnits transferred out 31000\nUnits in ending inventory\n20% conversion costs 5000 36000\nStep 3. Computing equivalent unit costs\nBeginning inventory $2350 $5200\nCurrent costs $84050 $62000\nTotal $86400 $67200\nEquivalent units 36000 32000\nCost per unit $2.40 $2.10\nStep4. Summarizing and Assigning total costs\nWork in process beginning inventory:\nTotal beginning inventory $7550\nCurrent costs in Assembly department:\nCosts to account for $153600\nDepartment’s cost to units transferred 31000 units *$4.50 = $139500\nUnits in ending work in process inventory\n$12000+ $2100 = $14100\nCost transferred out:\n31000 * ($2.40+42.10) $139500\nCosts in ending inventory:\nMaterials 5000* $2.40 $12000\nConversion 1000* $2.10 $2100\nTotal costs accounted for: $153600\nWhat are the reasons behind the use of process costing?\nProcess costing is used for the following reasons:\n- With process costing, a company can manufacture thousands or millions of units of a product in a given period of time.\n- It is not necessary to sell the same quantity that the products are produced in. A company can produce large quantities and can sell the same in small quantities.\n- Process costing helps in offering feedback to managers, which they can use to draw comparisons with similar products.\nSome of the questions on 'Process Costing' our experts have answered are -\nQ In spite of the differences, what are the similarities between process costing and job costing?\nQ Out of process costing and job costing, which method is preferred by companies in general?\nQ Among last in first out, first in first out and weighted average, which is the best and the most practicable process costing method?\nTranstutors is the best place to get answers to all your doubts concerning process costing. You can submit any of your Accounting homework and our tutors will make sure that you get timely answers. So what are you waiting for? Submit your process costing questions now.""]"	['<urn:uuid:f7a43706-e1e8-40bf-8d2e-3d310bf78a0c>', '<urn:uuid:1d173732-8863-4bf0-a091-0fefb5fae9c6>']	factoid	direct	verbose-and-natural	distant-from-document	multi-aspect	expert	2025-05-12T20:46:45.703947	21	83	2353
44	difference between scrum master post mortem moderator	A Scrum Master and a post mortem moderator have different roles and responsibilities. A Scrum Master is a servant leader who mentors teams on scrum practices, removes obstacles, facilitates collaboration, teaches scrum, and protects teams from external disruptions. They focus on ensuring scrum is properly practiced throughout ongoing projects. In contrast, a post mortem moderator guides discussion after a project's completion, ensures agenda items are covered, gives everyone a chance to speak, manages time, prevents off-topic discussions, and helps summarize findings and action items. They focus specifically on facilitating effective project review meetings.	"['Can\'t remember much about previous work projects? Wish you could analyze the wins and fails for future projects, or articulate the lessons from them in your work portfolio?\nThat\'s why project post mortem meetings exist. They\'re structured reviews conducted after the completion of a project. They\'re an opportunity for project team members to discuss what went well, what didn’t, and how future projects can be improved.\nThe term \'post mortem,\' Latin for \'after death,\' is used metaphorically in business to signify analysis after a project\'s end. These meetings are crucial for continuous improvement, allowing teams to learn from experiences and apply these lessons to future endeavors.\nIn this blog, we’ll delve into a step-by-step guide to conducting effective post mortem meetings:\n- Step 1: Understand the Purpose and Set Objectives\n- Step 2: Schedule the Meeting and Invite Participants\n- Step 3: Assign a Note-Taker\n- Step 4: Create an Agenda (Copyable Template Here)\n- Step 5: Assign a Moderator\n- Step 6: Conduct the Meeting with Best Practices\n- Step 7: Develop Actionable Insights and Follow-Up Plans\nStep 1: Understand the Purpose and Set Objectives\nBeing clear about the point and objectives of the post mortem meeting ensures the discussion is focused and goal-oriented. This clarity guides the team to extract meaningful insights and make concrete plans for improvement, directly impacting the success of future projects.\nDefine the Scope:\nClarify what aspects of the project will be discussed. This could range from project performance, team communication, to specific incidents.\nIdentify Learning Goals:\nDetermine what the team should gain from this meeting. It might be lessons learned, process improvements, or ideas to mitigate future risks.\nFocus on Improvement:\nThe aim is to foster a culture of continuous improvement. The meeting should not be about finger-pointing but about understanding how the entire project unfolded.\nStep 2: Schedule the Meeting and Invite Participants\nTiming and participant selection are key factors in organizing an effective post mortem meeting. Here\'s how to approach this step.\nChoose the Right Time:\nSchedule the meeting soon after project completion, ideally within one to two weeks. This ensures that details are still fresh in everyone’s mind. Post mortem meetings typically last between one to two hours, depending on the project\'s complexity and the points of discussion.\nWhen selecting a meeting time, be mindful of everyone\'s schedules and time zones. The goal is to find a time slot that accommodates all participants, encouraging full attendance and engagement.\nInclusive Participant List:\nInvolve all relevant stakeholders. This includes the project team members, project managers, and possibly other key personnel who contributed to or were affected by the project.\nHelp People Prepare:\nWhen sending out invitations, clearly state the meeting\'s purpose. This helps participants prepare appropriately and sets the right tone for the meeting.\nAsk team members to come prepared with their insights and points for discussion. This could be in the form of a post mortem questionnaire or simply reflective notes on their experience with the project.\nStep 3: Assign a Note-Taker\nDocumentation is a crucial aspect of a post mortem meeting. It ensures that the insights and decisions made during the meeting are recorded for future reference and action. Here\'s how to effectively assign and utilize a note-taker.\nOutsource Notetaking to AI:\nWhy split a team member\'s valuable time between notetaking and participating in the discussion? Utilizing AI to record the conversation allows everyone to fully engage in important discussions without distraction.\nThat\'s what Wudpecker is here to do. It records the discussion and generates post mortem meeting notes that the participants can utilize after the meeting (they\'ll be available max 10 min after the meeting has ended).\nWudpecker is compatible with Google Meet, Zoom, and Microsoft Teams. It generates an accurate and objective transcript, meeting summary and action items. The notes are easily shareable to other people.\nLet\'s see how easy it is to set up Wudpecker\'s meeting recording software:\n1. Sign in to Wudpecker by clicking ""Get started""\n2. Continue either by connecting a Google or Microsoft account to Wudpecker.\nAfter this, accept the permission for Wudpecker to access your calendar (this is for the purpose of automatically joining calls for a more seamless experience).\n3. Make sure the online meeting you want notes for shows up in ""Upcoming calls"" and that there\'s an activated toggle button next to it.\nYou can choose which meetings Wudpecker will try to join with a press of a toggle button. If it can\'t detect a meeting link in an event, no toggle button shows up.\nIf this is not the case...\n- Make sure you signed up with the correct account. The items on the menu are all the events in your calendar connected to the account you signed up with.\n- Check that the online meeting event you have in your calendar has a meeting link. (Psst: The meeting link needs to be in the ""location"" of the event)\n- Try clicking on the toggle button to activate it (it should be activated/colored automatically).\n- If none of this helps, you can always contact us by pressing the chat button on the right bottom corner.\nEasy peasy! For any future meetings, just admit the notetaker in each time, and you\'ll be all set!\n4. Once you\'re in the meeting, allow Wudpecker\'s Notetaker to join by clicking ""Admit"".\nThe Notetaker looks like any of the other meeting participants.\n+ Soon after the online meeting has ended, you\'ll see notes and a transcript for your meeting in ""My Calls"".\nPost-meeting, the notes should be shared with all participants and relevant stakeholders. This promotes transparency and keeps everyone on the same page regarding the outcomes of the meeting.\nOn Wudpecker, just go to the notes of your previous post mortem meeting, press ""Share"", and anyone with the copied link can access the notes, audio recording and transcript from the meeting.\nStep 4: Create an Agenda (Copyable Template Here)\nA well-structured agenda is pivotal for an effective post mortem meeting. It guides the discussion and ensures that all critical aspects of the project are covered.\nStart with Goals:\nBegin by clearly outlining the meeting\'s main purpose at the top of the agenda. This sets the overarching goal for the meeting. Then, under this, list more specific objectives as individual items or key topics on the agenda.\nThese objectives represent the key areas of focus and discussion points for the meeting. This format helps conceptualize the structure and ensures that each segment of the meeting contributes towards the overall purpose.\nAllocate Time Slots:\nAssign a specific time to each topic on the agenda. This approach helps in managing the meeting duration effectively and ensures that all important points are discussed within the allocated timeframe.\nTime management is crucial in keeping the meeting concise and on track, allowing for a thorough discussion of each point without the meeting becoming overly lengthy or unfocused.\nInclude a Roundtable Segment:\nAllow time for an open discussion where team members can bring up points not covered elsewhere in the agenda. This segment encourages open communication and ensures that all voices are heard.\nIt provides flexibility within the structured format, allowing team members to express concerns, ideas, or feedback that might not fit neatly into the predefined topics but are essential for a comprehensive post-mortem analysis.\nSet Aside Time for Action Items:\nDedicate a portion of the meeting to discuss next steps and assign responsibilities for follow-up actions. These action items are crucial for transforming insights and discussions into concrete steps for future improvement.\nExample: If the meeting identifies a need for better project tracking tools, an action item could be to research and propose suitable software options. Assign this task to a specific team member, with a deadline for presenting their findings.\nFeel free to use our agenda template below!\nStep 5: Assign a Moderator\nThe role of a moderator is crucial in steering a post mortem meeting towards productive and constructive outcomes. Here\'s how to approach this step.\nBy assigning a competent moderator, you ensure that the post mortem meeting is efficiently managed, focused on objectives, and beneficial for all participants.\nThe moderator\'s role is pivotal in harnessing the collective insights of the team and translating them into actionable steps for future project success.\nSelecting the Right Moderator:\nChoose someone with good facilitation skills, ideally someone who can remain impartial and objective. This could be a project manager, an experienced team member, or an external facilitator.\nDefine the Moderator\'s Responsibilities:\nThe moderator should guide the discussion, ensure that all agenda items are covered, and that every team member has a chance to speak. They also need to manage time effectively and prevent the discussion from veering off-topic.\nFacilitate Open and Honest Communication:\nThe moderator plays a key role in creating a safe environment for team members to share honest feedback. They should encourage participation from all attendees and manage conflicts or heated discussions diplomatically.\nSummarize Key Points:\nAt the end of the meeting, the moderator should help summarize the main findings, lessons learned, and action items agreed upon. This reinforces the meeting\'s outcomes and sets the stage for follow-up actions.\nStep 6: Conduct the Meeting with Best Practices\nConducting the post mortem review effectively is vital to extract maximum value from this exercise. Here are the best practices to ensure a successful and productive meeting.\nStart and End on a Positive Note:\nBegin the meeting on a positive note by acknowledging the team\'s efforts and successes. This sets a constructive tone for the meeting.\nConclude the meeting by reiterating the team\'s achievements and the value of the insights gained. This helps maintain team morale and motivation.\nEncourage Open and Empathetic Dialogue:\nFoster an environment where team members feel comfortable sharing their thoughts. The moderator should encourage quieter members to contribute while ensuring more vocal participants don\'t dominate the conversation.\nFocus on processes and outcomes rather than personal criticisms. The goal is to learn from mistakes, not to assign blame.\nAddress Key Topics Thoroughly:\nEnsure that each agenda item, such as project performance, team dynamics, and specific incidents, is discussed in-depth. Encourage team members to share their perspectives and experiences.\nEstablish Follow-Up Plans:\nTransforming the discussion into actionable insights ensures that the meeting results in tangible improvements and that the lessons learned are effectively applied.\nAs previously mentioned, allocating time in the agenda for creating follow-up plans and deadlines is beneficial. With Wudpecker\'s notetaking tool, it\'s also easy to document what was agreed upon in an objective way.\nThese notes should be shared to all the stakeholders for better accountability and transparency.\nTo really make sure people will follow up with their promises, set a timeline for when the team will review the progress of these action items. This could take place in a follow-up meeting or as part of regular team meetings.\nA project post mortem meeting is vital for learning from past mistakes and setting the stage for future success. By following the steps outlined in this guide, you can elevate these meetings from routine discussions to catalysts for continuous improvement.\nLet\'s make all projects and their post mortems count!\nWhat does post-mortem mean in business?\nIn a business context, a post-mortem refers to a meeting or process conducted after the completion of a project or significant event. The term, borrowed from medical terminology meaning \'after death,\' is used metaphorically in business to analyze a project after its conclusion. The goal is to understand what went well, what didn\'t, and how similar projects can be improved in the future.\nWhat is the primary purpose of a postmortem meeting?\nThe primary purpose of a postmortem meeting is to reflect on and analyze a completed project. It aims to identify successes, challenges, and areas for improvement. The ultimate goal is to gather insights and lessons that can be applied to future projects, enhancing efficiency, effectiveness, and overall project performance.\nWhat is the post-mortem process?\nThe post-mortem process involves several key steps: defining the meeting\'s purpose and objectives, scheduling the meeting and inviting relevant participants, assigning roles such as a note-taker and moderator, following a structured agenda, conducting the meeting with open and honest communication, and finally, developing actionable insights and follow-up plans based on the discussion.\nWhat questions are asked at a post-mortem meeting?\nQuestions typically asked at a post-mortem meeting include:\n- What were the major successes of the project?\n- What challenges did the team face, and how were they addressed?\n- What were the key lessons learned? How can these insights be applied to future projects?\n- What specific improvements can be made in our project management process?\nDate: [Insert Date]\nTime: [Insert Time]\nLocation: [Insert Location or Virtual Meeting Link]\n- [Clearly state the purpose of the meeting]\n- [Insert any specific reports or updates members should review before the meeting]\n1. Project Recap (10 mins):\n- Brief overview of the project, highlighting key milestones and overall performance.\n2. Key Wins (15 mins):\n- Discussion on what worked well, including successful strategies and effective team communication.\n3. Challenges Faced (15 mins):\n- Identifying and discussing major hurdles and how they were addressed or could have been handled differently.\n4. Lessons Learned (15 mins):\n- Insights gained from the project, including both technical and process-oriented learnings.\n5. Areas for Improvement (15 mins):\n- Identifying processes, team dynamics, and strategies for future enhancement.\n6. Roundtable Discussion (20 mins):\n- Open floor for additional points, concerns, or feedback from team members.\n7. Action Items and Follow-Up (10 mins):\n- Assigning responsibilities for identified improvements and setting deadlines.', ""What is Scrum?\nScrum is a simple empirical process that enables teams to build products incrementally in iterations, to keep abreast of the changing market needs and align themselves to the organization's business goals.\nScrum advocates self-organizing teams working towards a common goal through continuous inspection and adaptation. A minimum viable product at the end of each iteration provides an option for the teams to quickly get feedback from end users and respond accordingly much faster.\n3 Scrum Roles\nThe scrum team is made up of just three roles: a Product Owner, the Development team, and a Scrum Master\n1. Product Owner:\nA Product Owner in a scrum team decides what needs to be built. This person has complete knowledge on the market and business needs, has a vision, and owns the return on investment (ROI) or value delivered by the product.\nUnlike traditional delivery, this person is a part of the team that delivers the product. Following are the key tasks of the Product Owner:\n- Creates the vision\n- Represents business, and is responsible for the ROI\n- Cascades the vision to the teams\n- Owns the backlog of features\n- Prioritizes features by market value\n- Is empowered to take decisions\n- Negotiates with the team and business to deliver the right product at the right time\n2. The Development team:\nThe Development team in scrum is the team that has all the skills necessary to implement the backlog items. This team is not any normal team and is committed, dedicated, and motivated to perform the best.\nIt is a self-organizing team that collaborates, shares their special skills and knowledge and are committed completely to fulfill the objective. The team members are empowered to take crucial decisions that can make or break a situation.\nThe following are the special characteristics of 'The Development team':\n- Self-organizing - the Development team will be a self-managing group, who will decide on the tasks that they will work on incrementally. There is no 'Manager', who will be controlling their work\n- Empowered - the team can commit to work, determine HOW to deliver and decide HOW MUCH to deliver in one iteration\n- Cross-functional - the team does not segregate members as developers, testers or analysts and each have the necessary skills to deliver the product increment\n- Small-sized - the Development teams should ideally have 5 to 9 team members with skills sufficient to deliver the committed work. Smaller teams will not have the bandwidth to complete a considerable work and bigger teams will increase complexity\n- Co-located - the agile team is co-located, to ensure effective collaboration\n- Committed - since the team is empowered to take decisions on the scope of work in a sprint, they are committed to delivery, should be transparent on the progress, and highlight the impediments early on\n- Dedicated - this team is focused and is 100% dedicated to product delivery\nUnlike traditional methodologies, where the commitment to deliver is made to business by the team that is not involved in the execution, in Agile, the team that does the work commits to how much work can be executed in a sprint.\nThe Development team decides how much work is to be done in a sprint, and commits to delivering a 'potentially shippable product increment (PSPI)', without sacrificing quality and speed. The team also makes continuous self-improvements.\n3. Scrum Master:\nThe Scrum Master is not a management title and cannot make decisions on behalf of the team. The Scrum Master's major responsibility is to ensure that scrum is understood and practiced by every team member in the true spirit.\nThe Scrum Master should understand the different skill sets of his team and group them by having the right sheep in the right flock. A Scrum Master should guide the team such that the team does not go astray and fall prey to excess time and energy. A shepherd must draw out quiet people during stand-up meetings or when planning poker sessions. And, when the team loses focus or a team member goes astray, the shepherd should bring the lost one back to the flock and guide appropriately.\nThe Scrum Master should not enforce agile practices on the team, but should don a 'Servant leadership' role. Scrum Master should lead by example and be a living demonstration of team assets and scrum values.\nHe should create an environment of safety for the team, and guide and facilitate team collaboration. He should refrain from solving problems or making decisions by guiding teams to do so.\nTo summarize, a Scrum Master:\n- Is a servant leader - mentors and coaches the teams on scrum theory and practices, guides them on how they need to adapt to the same, thereby realizing the benefits of scrum both at team level and organization level\n- Helps remove obstacles/impediments - supports the Development teams in removing the impediments by reaching out to the right people, thereby ensuring a smooth development progress without disrupting the team\n- Facilitates collaboration - enables interactions within the team as well as between the team and the Product Owner\n- Teaches scrum - to the team\n- Protects the teams - from external disruptions like changes to stories in the current sprint\n- Is a change agent - in growing the organization to deliver early and often, and removing waste\nScrum focuses more on a working software at the end of every sprint rather than comprehensive documentation. This does not imply that there is no documentation. The documentation is provided to facilitate collaboration and interactions, rather than tracking. The progress is measured always through a working software. Documentation in scrum is only through four main artefacts namely: Product backlog, Sprint backlog, Increment and Definition of Done.\n1. Product backlog:\nA product backlog is a dynamic list of functionalities the product might include, such that it provides value to users.\nThe Product Owner maintains this list and is responsible for creating, managing, and prioritizing the backlog by focusing on WHAT brings the highest value to the users. These are few unique characteristics of a product backlog:\n- Is dynamic in nature as it evolves based on changing market needs\n- Lists all the features and capabilities that will be taken up in iteration and delivered as a product increment\n- Is refined on a continuous basis. The Product Owner and Development team collaborate and update the details, estimate, and prioritize based on business value and size\n2. Sprint backlog:\nSprint backlog is a subset of the entire product backlog that the scrum team plans to implement in one iteration or sprint.\nDuring the sprint planning, the team selects items from the product backlog that they commit to complete in one sprint and thus, creates the sprint backlog. The Product Owner and Scrum Master should not provide inputs that may impact the team's decision. Sprint backlog has:\n- Subset of product backlog items that the teams commit to implement in one sprint\n- Items broken into smaller pieces of work as tasks\n- A focus on HOW the team does the work and delivers the value in one sprint\n- A story or task board that is used by the teams to view backlog and individuals sign up for work after prioritization\n- Provision for the Development teams to track the sprint progress and check their alignment to sprint goals\nAn increment is the work delivered at the end of every sprint. Typically, after every iteration there will be a Product Increment (PI) that delivers value and the final product will be a working software. This increment is a sum of all the capabilities that were delivered in the previous sprints as part of the PI. A Product Owner decides whether to release the working product increment post the sprint or the release.\n4. Definition of Done:\nScrum clearly states a 'definition of done' that enables teams to understand the meaning of marking a story as done. Based on this, teams measure the progress of completion of their stories. This not only helps identify 'done' items, but also helps decide on the total items to be worked in that sprint or iteration. The 'definition of done' is defined at various levels, which are release level, sprint level and even at the story level. The story level 'definition of done' is handled through acceptance criteria. In a multi-team scenario, teams mutually align to the sprint's 'definition of done'.\nAll scrum activities are time-boxed and allow teams to inspect their current work and implement those learnings in future time-boxes.\nHeart of Scrum - The Sprint\nAt the heart of scrum, is the 'Sprint'. The sprint is a time-boxed iteration, typically ranging from 1 to 4 weeks, at the end of which, a potentially shippable product increment is delivered by the Development team. The sprint has the following characteristics:\n- Does not exceed a maximum of one calendar month, as this will increase the risk due to changes in requirements and thus, may not provide the perceived business value at the end of the sprint\n- Has a goal or 'definition of done' associated with every sprint that actually measures the success of the sprint\n- Can be cancelled by the Product Owner, if the goal or the need for the sprint becomes obsolete due to changing market\nScrum advocates specific types of activities, events, or meetings within a sprint to avoid the traditional formal meetings. These events and meetings are conducted at regular intervals and happen at specific periods of the sprint.\nTypical scrum activities are:\n- Product backlog refinement\n- Sprint planning\n- Daily scrum\n- Sprint review\n- Sprint retrospective\n1. Product backlog refinement (continuous activity throughout the sprint)\nProduct backlog refinement is a continuous activity throughout the sprint, where the Product Owner ensures that the product backlog is in order. The Product Owner performs the following tasks to ensure that the product backlog is relevant:\n- Removes or demotes product backlog items that no longer seem important\n- Adds or promotes product backlog items that become more important\n- Splits product backlog items into smaller items or merges smaller ones into larger items and estimates those\n2. Sprint planning (2 hours per week sprint time-box)\nSprint planning meeting happens at the start of every sprint. This helps the Product Owner and Development teams to plan the product backlog items that will be taken up for implementation during the sprint. The Development team performs the following activities during this meeting:\n- Considers and discusses product backlog items with the Product Owner\n- Ensures a shared understanding on those items\n- Selects a number of items that they estimate to complete\n- Creates a sufficiently detailed plan to complete the selected items\nTo ensure that the above is achieved, two activities need to be done:\nPart I: Define 'WHAT' work will be done\n- Product Owner renders prioritized product backlog to the Development team\n- The whole scrum team collaborates to understand the work\n- The Development team alone decides how much work is to be taken without any pressure for more work to be done\n- The sprint is given a goal called the sprint goal as the essential focus of that sprint\nPart II: Explain 'HOW' the work will get done\n- Development team decides how to produce the next product increment that meets 'definition of done'\n- Sufficient design and planning is conducted to complete the committed work\n- Work to be done in initial days is split into small units of one day or even less\n- Work to be done later are split whenever needed\n3. Daily scrum (15 minutes)\nDaily scrum is a 15 minute time-boxed event in which the team manages its daily activities. This is also called the daily stand-up meeting.\nThe scrum team meets every day, preferably at the same time and same place, so that it becomes a habit and here each member answers three critical questions:\n- What did I get done yesterday?\n- What will I get done today?\n- Are there any impediments blocking me?\nIt is essential that all the members of the scrum team are available for the daily stand-up meeting. The daily stand-up meeting is for the Development team, and they should participate enthusiastically to collaborate with each other.\nThe daily scrum also ensures that the impediments blocking the progress of the sprint are identified and resolved without further delay. Detailed problem solving does not happen during this meeting. Unnecessary meetings should be avoided by broadcasting individual updates to everyone.\nThis event enhances team communication and transparency, thereby enabling teams to be self-organized and make faster decisions.\n4. Sprint review (1 hour/week time-box)\nA sprint review is an event that happens at the end of every sprint, where the teams and stakeholders discuss what was done in the sprint. The following happens during this meeting:\n- A demo of the product increment showcasing the new features and underlying technology\n- Feedback from the review provides input to the team to further discuss on refining the existing backlogs and plan for future sprints\n- The Scrum Master facilitates this review meeting that is typically attended by all the stakeholders invited by the Product Owner\n- Sprint review is essentially a way in which the team inspects and adapts to the next sprint and overall product release\n5. Sprint retrospective (1 hour/week time-box):\nDuring a sprint retrospective meeting, the Development team inspects the previously completed sprint and identifies areas of improvement to be enacted for the upcoming sprints. This happens after every sprint and right after sprint review in which the whole team participates. During this meeting:\n- The team introspects on what went well in terms of collaboration, planning, process, and tools\n- They try to identify potential improvements that can be taken up in the next sprint to make the scrum processes more efficient by learning from previous shortfalls\n- They decide on what would be done in the next sprint by taking into consideration the major improvements\n- Scrum Master ensures that the teams improve their skills and knowledge during the scrum process so that they become more effective in the next sprint\n- The team focuses on improving their entire delivery cycle\nThe three typical questions the team answers are:\n- What shall we start doing?\n- What shall we stop doing?\n- What shall we keep doing?\nAll the above activities in the scrum process framework enable teams to deliver a potentially shippable working software in short iterations. This also enables teams capture feedback, inspect, and adapt for the next iteration.\nScrum also states five core values to which teams have to adhere. The core values are: Commitment, Courage, Focus, Openness, and Respect. These values need to be imbibed and lived by the scrum team to ensure the fulfillment of scrum pillars of transparency, inspection, and adaptation. It builds trust among everyone.\nSuccessful use of scrum depends on people becoming more proficient in these 5 values\n- People personally commit to achieving the goals of the scrum team\n- The scrum team members have the courage to do the right thing and work on tough problems\n- Everyone focuses on the work of the sprint and the goals of the scrum team\n- The scrum team and its stakeholders agree to be open to all the work and the challenges that they encounter while performing the work\n- Scrum team members respect each other and consider each to be capable and independent""]"	['<urn:uuid:33260c39-c777-49c8-8687-1d572b9b010d>', '<urn:uuid:def28e7c-80b7-4c49-bf36-406831180f23>']	open-ended	direct	short-search-query	similar-to-document	comparison	novice	2025-05-12T20:46:45.703947	7	93	4852
45	Why are buildings and power grids becoming more connected?	Buildings and power grids are becoming more connected due to new power sources and options available to end-users. People are increasingly using renewable energy programs, buying from local energy aggregators instead of traditional utilities, and using technologies like solar panels. This transformation requires greater integration between building systems and power grids.	"[""Buildings and the power grids they are connected to are traditionally designed and operated separately. The topics are usually taught separately as well, with architectural engineers learning to design every aspect of a building and power systems engineers learning to operate grids effectively.\nBut in modern practice, the line between the two areas is increasingly blurred thanks to new power sources and options available to end-users. Future engineers at the College of Engineering and Applied Science are getting experience with this developing dynamic thanks to a course that is so new and unique, there isn’t even a textbook for it yet.\n“I love pursuing crazy, big, ‘research-y’ ideas and I like talking about them with students. But I also truly believe in keeping courses as up-to-date as possible with what industry needs,” said Assistant Professor Kyri Baker, who developed and teaches the new Grid Connected Systems class. “I am not aware of another course at CU Boulder – or even in the country – that focuses solely on building-to-grid interactions from both qualitative and quantitative perspectives right now. This is education that students will need and that industry is asking for.”\nBaker is a new faculty member in civil, environmental and architectural engineering and a member of the Water-Energy Nexus Interdisciplinary Research Theme. She developed this course directly out of her research, which looks at building-to-grid optimization and changes to power demand and the grid as more renewable energy sources are introduced. Other aspects of her work explore how climate change and severe weather events may threaten the ability of the grid to reliably deliver power to end-users.\nEnd-users are increasingly enrolling in renewable energy programs, buying their energy from local aggregators instead of the traditional utility company or using green technology like solar panels. Each of those aspects requires more integration between the systems and buildings for future engineers to navigate.\n“All of this is transforming the way we receive electricity and the way the grid operates,” Baker said.\nShe said she works to integrate current events, coding skills and group learning activities into the class. The class has discussed the city of Boulder’s move toward a municipal electric utility and learned to use Python to simulate cost-effective electric vehicle charging under different electricity pricing frameworks.\n“In addition to concepts like optimization, machine learning and Python programming that help students make buildings smarter and more grid-interactive, I also make sure the students can answer fundamental questions on both the building and grid side like ‘what happens when I flip on a light switch?’” she said. “Even if you're not a hardcore energy researcher, knowing where your energy comes from, how you are using electricity, how you are being charged for electricity, and the outlook of our energy future are things that I think everyone should know and that I cover in the course.”\nTaylor Burdge is auditing the class while working as a consultant within WSP’s Sustainability, Energy, and Climate Change team. In that position, she has seen firsthand how things like grid-connected renewables are changing utility rate structures and system dispatch theory.\n“Using a programming language like Python is extremely helpful because you can automate iterative modeling processes that are necessary for understating and optimizing these evolving energy systems,” she said. “Kyri’s class is unique because we’re learning and using these powerful data science tools for real-world energy applications. Academia tends to be a few steps ahead of Industry and Kyri’s insights into where the industry is going are valuable.”\nDaniel Katzman, a fourth-year mechanical engineering student, said he took the class to meet a requirement for the Energy Engineering Minor and because of his interest in working with renewable energy. He said he has enjoyed sharing the classroom with undergraduate and graduate level students and has learned a lot so far.\nAssociate Dean for Education Ken Anderson said the course is a valuable to students in several ways.\n“I am excited that Assistant Professor Kyri Baker is offering our students the chance to gain valuable skills in programming, machine learning and optimization while working on a problem domain that sits right at the intersection of research and practical applications. Not only is she making progress on important issues related to sustainable energy systems, the grid and the built environment, but she is producing students with the skills and knowledge needed by industry, research labs and government organizations working in this space,” he said. “This is the type of integrated and hands-on curriculum that we like to provide to all of our CU engineering students.”\nBecause there is no textbook for the class, Baker pulls from publicly available documents and guides created by companies, researchers and government organizations leading the field. She also pulls from her past experience working at NREL in both the buildings group and the power systems group. That has given her an interesting perspective that she hopes will translate to this class and possibly to a textbook she authors in the future.\n“My overwhelming feeling is that in order to pursue a clean energy future, grid engineers and buildings engineers need to collaborate more and develop a better understanding of how one system can help and interact with the other,” she said. “That starts with education.”""]"	['<urn:uuid:aadd2a80-a5b7-4c02-9c92-afa80382c268>']	open-ended	direct	concise-and-natural	similar-to-document	single-doc	novice	2025-05-12T20:46:45.703947	9	51	872
46	want to install water pumping system for farm irrigation can francis turbine help explain applications	Yes, Francis turbines can be used for irrigation purposes. While it is most widely known for generating electricity in hydro-power plants, Francis turbines (also called mixed flow turbines) are specifically used in irrigation water pumping sets to pump water from the ground for irrigation purposes. The turbine is particularly suitable for this application because it operates efficiently over a wide range of water head and flow rates, making it versatile for different irrigation needs.	['🕑 Reading time: 1 minuteFrancis Turbine is a combination of both impulse and reaction turbine, where the blades rotate using both reaction and impulse force of water flowing through them producing electricity more efficiently. Francis turbine is used for the production of electricity in hydro power stations. Majorly there are 2 turbines flow patterns on which they work, namely radial and axial flow concepts. An American civil engineer by name, James B. Francis in Lowell, Massachusetts comes up with an idea of combining both impulse and reaction turbine where water enters the turbine radically and exits axially. The working principle, main components and its application is discussed in this article. The main reason of higher efficiency of Francis turbine lies in the design of blades, these blades rotate using both reaction and impulse force of water flowing through them. Due the use of this type of turbines the main problem faced due to the water head availability is eliminated as the turbine uses both the kinetic and potential energy to produce power. For this, it is also known as Mixed Flow turbine.\nMain Components of Francis TurbineThe major components of Francis turbine are\n1. Spiral CasingSpiral casing is the inlet medium of water to the turbine. The water flowing from the reservoir or dam is made to pass through this pipe with high pressure. The blades of the turbines are circularly placed, which mean the water striking the turbines blades should flow in the circular axis for efficient striking. So the spiral casing is used, but due to circular movement of the water, it looses its pressure. To maintain the same pressure the diameter of the casing is gradually reduced, so as to maintain the pressure uniform, thus uniform momentum or velocity striking the runner blades.\n2. Stay VanesStay vanes and guide vanes guides the water to the runner blades. Stay vanes remain stationary at their position and reduces the swirling of water due to radial flow, as it enters the runner blades. Thus making turbine more efficient.\n3. Guide VanesGuide vanes are not stationary, they change their angle as per the requirement to control the angle of striking of water to turbine blades to increase the efficiency. They also regulate the flow rate of water into the runner blades thus controlling the power output of a turbine according to the load on the turbine.\n4. Runner BladesThe performance and efficiency of the turbine is dependent on the design of the runner blades. In a Francis turbine, runner blades are divided into 2 parts. The lower half is made in the shape of small bucket so that it uses the impulse action of water to rotate the turbine. The upper part of the blades use the reaction force of water flowing through it. These two forces together makes the runner to rotate.\n5. Draft TubeThe pressure at the exit of the runner of Reaction Turbine is generally less than atmospheric pressure. The water at exit cannot be directly discharged to the tail race. A tube or pipe of gradually increasing area is used for discharging water from the exit of turbine to the tail race. This tube of increasing area is called Draft Tube. One end of the tube is connected to the outlet of runner while the other end is sub-merged below the level of water in the tail-race.\nHow Does a Francis Turbine Work?The water is allowed to enter the spiral casing of the turbine, which lead the water through the stay vanes and guide vanes. The spiral case is kept in decreasing diameter so as to maintain the flow pressure. The stay vanes being stationary at their place, removes the swirls from the water, which are generated due to flow through spiral casing and tries it to make the flow of water more linear to be deflected by adjustable guide vanes. The angle of guide vanes decides the angle of attack of water at the runner blades thus make sure the output of the turbine. The runner blades are stationary and can-not pitch or change their angle so it’s all about the guide vanes which controls the power output of a turbine. The performance and efficiency of the turbine is dependent on the design of the runner blades. In a Francis turbine, runner blades are divided into 2 parts. The lower half is made in the shape of small bucket so that it uses the impulse action of water to rotate the turbine. The upper part of the blades use the reaction force of water flowing through it. Thus runner blades make use of both pressure energy and kinetic energy of water and rotates the runner in most efficient way. The water coming out of runner blades would lack both the kinetic energy and pressure energy, so we use the draft tube to recover the pressure as it advances towards tail race, but still we cannot recover the pressure to that extent that we can stop air to enter into the runner housing thus causing cavitation.\nApplications of Francis Turbine\n- Francis turbine is the most widely used turbine in hydro-power plants to generate electricity.\n- Mixed flow turbine is also used in irrigation water pumping sets to pump water from ground for irrigation.\n- It is efficient over a wide range of water head and flow rate.\n- It is most efficient hydro-turbine we have till date.']	['<urn:uuid:befe56fb-497f-4a4d-b67a-03f341ac72bc>']	open-ended	with-premise	long-search-query	similar-to-document	single-doc	novice	2025-05-12T20:46:45.703947	15	74	901
47	I work as a high school security guard and I'm curious about the most important staff member to help police during a dangerous situation in the building. Who would that be and why?	According to law enforcement experts, the janitor is actually the most important person in such scenarios. This is because janitors have intimate knowledge of the building layout - they know every doorway, window location, and potential hiding places. This information is crucial for SWAT teams or other emergency responders who need to rescue hostages or secure the space. Without a janitor's knowledge, responders would be entering an unfamiliar building with no understanding of where doors lead or what's around corners, significantly hampering their response capabilities.	['Is Laser Scan Data Part of Your Incident Training and Response?\nAn active shooter is inside a school building. Who is the most important person on scene? That’s the question posed to law enforcement officers at the 2014 New Jersey IAI Conference in October 2014. It was a blunt talk—but not too blunt for the crowd attending the “Using 3D Laser Scanning to Document Critical Infrastructure” session presented by Det. Dave DeLeeuw, of the Ocean County Sheriff’s Office CSI Division.\nWith the FBI reporting an average of 16.4 U.S. active-shooter incidents taking place annually over the last seven years, these officers know that a candid discussion like this could mean the difference between death and life.\nIn this active-shooter scenario, the most important person isn’t the first officer on the scene or the paramedic or another emergency responder—it’s the janitor. Janitors know the ins and outs of their buildings. They know where every doorway leads and which window looks out onto what courtyard. Janitors know all the hiding places. If you have a SWAT team going into a building trying to rescue hostages and secure the space, those are very important things to know.\nConsider the same scenario, but this time the janitor is dead. Now what are you going to do? You would have nothing to go on other than your wits and your reflex. You’re entering an unfamiliar building, and you’re going to be looking around corners and opening doors with no idea where they lead.\nInfrastructure Scanning Mitigates Risk and Loss of Life\nIn an effort to mitigate risk and loss of life, a number of public safety agencies are choosing to take a more proactive approach by documenting key buildings with high-definition 3D laser scanning. One such agency is the Georgia Bureau of Investigation. With eight Leica ScanStations, the scanner has become an essential documentation tool for the bureau—but not just for crime scenes. The GBI has been in the process of scanning Georgia’s educational facilities for years. If the building has been scanned, like the GBI is doing, you can take a look at that data before you send your team in there. You can sit down and show them a flythrough using Leica Cyclone software or walk the building using Leica TruView.\nThese powerful data-processing programs are the key to converting complex and comprehensive scan data into valuable resources. With state-of-the-art Leica Cyclone 3D point cloud processing software, tens of millions of highly accurate measurements, or data points, become an exact “virtual” 3D reproduction of the building. Cyclone’s powerful features allow responders to fly through a building or generate a line of sight from any position. By overlaying panoramic photography onto point cloud scan data, Leica TruView 3D visualization software creates vivid true-to-life imagery. This easy-to-use interactive software gives users the ability to not only explore the scene but also pan, zoom, measure, markup and hyperlink data. Once work products have been finalized, the data can be readily available on DVD or via the Web in incident command vehicles.\nScan Data Provides Site Familiarization to Responders and Command Staff\nAs a law enforcement officer, DeLeeuw says he wants to see more laser scan data utilized for training purposes before an actual event occurs. “Active-shooter scenarios are constantly rolled out in schools,” he says. “If you want to play out a potential scenario for an active shooter, there’s no better way than to scan a building, utilize the point cloud data through Cyclone or TruView, and show it to your response team.”\nComprehensive, highly accurate building documentation gives responders and command staff the critical data they need to make informed decisions in dangerous and unfamiliar surroundings. “I can take you through walls, doors or windows and up stairs,” DeLeeuw says. “I can tell you how many footsteps it is from the front of the door to the room 25 feet away. I can tell you how many inches off the ground and to the right a light switch is located. [By looking at hinges,] I can tell you which way the door swings open. I can take you and show you every square inch of that building in distances and angles right on-the-fly.\n“By having all that, you’re able to give somebody actionable data and information on-the-fly. You provide them site familiarization. You can talk about observer positioning and staging for a counterassault. You can even go into pedestrian flow in evacuation planning. You have all these different scenarios that you can play out.”\nWhether it happens in educational facilities, malls, theaters or other key buildings, the hard fact, according to the FBI, is that active-shooter incidents are the rise. “It only makes sense to have these public institutions mapped in 3D,” DeLeeuw says, “so that if that time comes, God forbid, you can have access to that scan data and know what you’re getting yourself into.”\nFor more information about high-definition 3D laser scanning solutions for law enforcement, please contact us.']	['<urn:uuid:1e8b8583-d9bc-4130-80f2-102bd245cd0e>']	open-ended	with-premise	verbose-and-natural	distant-from-document	single-doc	novice	2025-05-12T20:46:45.703947	33	85	827
48	climate literacy ocean literacy compare principles	While both frameworks outline essential principles, they have different focuses. Climate Literacy includes 7 essential principles centered on Earth's climate system, including the Sun's role, weather systems interactions, and human impacts. Ocean Literacy also has 7 principles but focuses specifically on understanding the ocean's influence on us and our influence on it, including concepts like the Earth having one interconnected ocean, ocean's role in making Earth habitable, and ocean exploration. Both frameworks emphasize human interconnection with these systems and were developed to improve public understanding and decision-making about environmental issues.	"[""Climate Literacy Framework\nEssential Principles and Fundamental Concepts of Climate Science\nEarth’s climate offers just the right conditions for life to thrive. Climate is the average weather over a long period of time — at least thirty years. For any given region, climate is described using long term trends in atmospheric temperature, humidity, pressure, wind, and rainfall. Climate is also affected by proximity to large bodies of water, ocean currents, and ice or snow cover, as well as altitude, latitude, and topography. Climate is regulated largely by concentrations of chemicals in the atmosphere called greenhouse gases which influence the amount of energy that is retained and lost to space by Earth’s atmosphere. Decreasing greenhouse gases cause global cooling, and increasing concentrations cause global warming. Observations of weather using instruments are reliable only to the early 19th century. Climate of the more distant past is revealed by paleontologists, scientists who interpret climate records found in fossils, tree rings, ocean and lake sediments, and ice cores. Climate scientists also are developing mathematical models of the Earth system which can simulate how climate changes over time under the influence of different levels of green house gas emissions.\nHundreds of scientists participating in the Intergovernmental Panel on Climate Change (IPCC) have concluded that the global warming observed since the middle of the 20th century is primarily the result of burning of fossil fuels which produces carbon dioxide, and land uses which cause deforestation. Impacts of global warming include rising sea level; less predictable precipitation patterns, higher frequency of severe weather; melting ice caps, glaciers, and permafrost; species extinctions; and changes in crop yields. In order to slow global warming and to inevitable changes, we must become better informed about climate and global change. A climate-literate public knows how to assess scientifically credible information about climate. It can communicate about and make well-informed decisions with regard to individual actions and public policies that may affect climate.\nThis climate science literacy guide identifies the essential principles and fundamental concepts that individuals and communities should understand about Earth’s climate system. Climate science literacy provides important insights into how the entire Earth system functions. For more information on this effort, please visit the Climate Science Literacy Framework web site. AAAS Project 2061 has created a guide called Communicating and Learning About Global Climate Change: An Abbreviated Guide for Teaching Climate Change that maps out what students should be learning in kindergarten through 12th grade and describes what a science literate adult should know and be able to do.\nGuiding Principle for Informed Climate Decision:\nHumans can take actions to reduce climate change and its impacts.\n- Essential Principle 1: Life on Earth has been shaped by, depends on, and affects climate.\n- Essential Principle 2: We increase our understanding of the climate system through observation and modeling.\n- Essential Principle 3: The Sun is the primary source of energy for the climate system.\n- Essential Principle 4: Earth's weather and climate systems are the result of complex interactions.\n- Essential Principle 5: Earth's weather and climate vary over time and space.\n- Essential Principle 6: Evidence indicates human activities are impacting the climate system.\n- Essential Principle 7: Earth's climate system is influenced by complex human decisions involving economic costs and social values."", 'Ocean Literacy is defined as ‘an understanding of the ocean’s influence on us and our influence on the ocean.’ There are Seven Principles of Ocean Literacy:\n- The Earth has one big ocean with many features.\n- The ocean and the life in the ocean shape the features of Earth.\n- The ocean is a major influence on weather and climate.\n- The ocean made the Earth habitable.\n- The ocean supports a great diversity of life and ecosystems.\n- The ocean and humans are inextricably interconnected.\n- The ocean is largely unexplored.\nAccording to the Marine Biological Association (MBA) these seven Ocean Literacy ‘Principles’ were developed by hundreds of educators and scientists in the USA as a guide to teaching about the Ocean. These principles are now being used as a framework in Europe to develop strategies towards the goal of an Ocean Literate society\nAs stated by the UK’s MBA.\n“Few of us are aware of how important the sea is to human wellbeing: medically, economically, socially, politically and environmentally. Many of us are unaware of how day-to-day choices and actions can have a cumulative effect on the health of the ocean – a necessary resource that must be protected for life on our blue planet to exist.”\nThe need for improved Ocean Literacy\nIt has long been recognised that improving public awareness about how the ocean can benefit the environment, economy, and society is important (Peterson and Lubchenco 1997, Costanza, 1999). In 2004, a Pew Ocean Commission report identified a need to improve public literacy about oceans. The authors of the report assumed that by enhancing public awareness and knowledge of the oceans, and their influence on our lives, would lead to increased public support for ocean restoration efforts.\nThe Pew Oceans Commission called for‘a new era of ocean literacy that links people to the marine environment’’ [2, p. 91]. The Commission further argues that there is a ‘‘need to provide the public with understandable information about the structure and functioning of coastal and marine ecosystems, how ecosystems affect daily lives, and how we affect ecosystems’’ [2, p. 11].\nSince this report was published, low levels of ocean literacy have been, and are continuingly being identified in many countries. These low levels of literacy can be a barrier for citizens to engage in environmentally responsible behaviour or consider ocean-related careers (Guest et al., 2015).\nSeagrass meadows role in developing Ocean Literacy in the UK\nSeagrass meadows have a central role to play in developing ocean literacy in the UK. First, they are coastal habitats, readily accessible from the shore by wading or snorkelling and in some locations (i.e. Porthdinllaen in Wales, Lindisfarne National Nature Reserve in England, or Tyninghame in Scotland) they can be easily accessed without specialist equipment simply by walking out at low tide.\nIn addition, there are already well publicised examples from the USA of where restoration of the same seagrass habitat (same species) has lead to rapid recovery of coastal ecosystem services (see Orth et al, 2020). This creates a cause of optimism that similar results could be achieved in the UK since much of the science now exists it just needs to be applied to UK waters.\nFinally, there are already existing, recognised and celebrated Seagrass Education and Awareness programs running in the UK both through Project Seagrass and the National Marine Aquarium’s ReMEDIES program. This is supported by an active academic literature for further developing this area (Roth and Reynolds, 2020).\nConversations are already underway between Project Seagrass and the London Aquarium about a 5-year strategy that would incorporate a national seagrass engagement program and a potential collaboration with BIAZA network of aquariums which could act as community hubs for engaging people with monitoring, and then the potential restoration of their local meadows.\n- Pew Oceans Commission. America’s living oceans: charting a course for sea change. A report to the nation. Arlington, VA: Pew Oceans Commission; 2003 144pp.\n- Costanza R. The ecological, economic, and social importance of the oceans. Ecol Econ 1999;31(2):199–213.\n- Peterson CH, Lubchenco J. Marine ecosystem services. In: Daily G, editor. Nature’s services: societal dependence on natural ecosystems. Washington, DC: Island Press; 1997177–94.\n- Guest, H., Lotze, H.K. and Wallace, D., 2015. Youth and the sea: Ocean literacy in Nova Scotia, Canada. Marine Policy, 58, pp.98-107.\n- Orth, R.J., Lefcheck, J.S., McGlathery, K.S., Aoki, L., Luckenbach, M.W., Moore, K.A., Oreska, M.P., Snyder, R., Wilcox, D.J. and Lusk, B., 2020. Restoration of seagrass habitat leads to rapid recovery of coastal ecosystem services. Science Advances, 6(41), p.eabc6434.\n- Roth, J. and Reynolds, L.K., 2020. Engaging students in seagrass-focused activities. Science Activities, pp.1-10.']"	['<urn:uuid:2e79e230-d8b6-494c-9fdf-cf0264b80f03>', '<urn:uuid:d37162a4-9bad-4f7f-a4fa-0ee69ebf7c51>']	open-ended	with-premise	short-search-query	similar-to-document	comparison	expert	2025-05-12T20:46:45.703947	6	90	1318
49	which company controls leads development linux software	Unlike many other operating systems, development on Linux isn't led by any one company.	['Time-sharing working programs schedule duties for environment friendly use of the system and may also embrace accounting software for price allocation of processor time , mass storage , printing, and different sources. TO THE FULLEST EXTENT PERMITTED BY RELEVANT LEGISLATION, WE, TOGETHER WITH OUR MOTHER OR FATHER COMPANIES, SUBSIDIARIES, AFFILIATES, OFFICERS, DIRECTORS, EMPLOYEES, CONTRACTORS, AGENTS, PARTNERS, LICENSORS AND DISTRIBUTORS (COLLECTIVELY VERIZON MEDIA ENTITIES) DO NOT MAKE ANY REPRESENTATIONS, PROMISES, OR WARRANTIES, CATEGORICAL OR IMPLIED, IN REGARDS TO THE PROVIDERS.\nThe dual bus in MAN community provides support to transmit data in both directions concurrently. It doesn’t price anything, however it is a one-time decision—as soon as you’ve got taken the LAPTOP out of S Mode, you’ll be able to by no means put it back into S Mode. If the payment-based mostly Service features a third-party product, you understand and agree that your buy and use of the Service can also be subject to the third party’s terms of service and privacy coverage, which it is best to read totally before agreeing to them.\nAn actual-time operating system (RTOS) is an operating system meant for purposes with fastened deadlines ( real-time computing ). Such functions embrace some small embedded programs , car engine controllers, industrial robots, spacecraft, industrial management, and some large-scale computing programs. However utilizing VLC Media Participant you may play any movies file like -mp4,mkv,flv,3gp and many others.\nAsynchronous Transfer Mode (ATM) is a switching approach for telecommunication networks. ReactOS is a Windows-alternative working system, which is being developed on the ideas of Windows – with out utilizing any of Microsoft’s code. Primary working system options were developed in the Nineteen Fifties, similar to resident monitor capabilities that might robotically run completely different packages in succession to speed up processing.\nIn modern operating systems, interrupts are handled by the operating system’s kernel Interrupts might come from either the computer’s hardware or the operating program. The community gadget can be a hub, swap or router, which can be a central node and all the other nodes can be connected with this central node. An internetwork is the connection of multiple several types of pc networks to kind a single pc community by layering on high of the completely different networking software and connecting them collectively utilizing routers.\nIn contrast to many different working systems, growth on Linux isn’t led by any one company. The gadgets or nodes in question will be connected by bodily or wi-fi connections. Assignment by Us. We might freely assign these Phrases and all the policies and other paperwork integrated or referenced in it (including all rights, licenses, and obligations below it or them), in complete or partly and with out notice, for any cause, together with for the aim of internal restructuring (for example, mergers or liquidations).\nTemplate:Important Two or more networks or network segments related using devices that operate at layer 3 (the ‘network’ layer) of the OSI Basic Reference Mannequin, comparable to a router. By using the Companies, you agree to our privateness insurance policies and that we can use your data in accordance with our privacy insurance policies.\nDon’t search for because it’s likely several of these information exist on your pc and may be related to other programs. Pro: Removes files and registry settings related to this system, which manually deleting this system does not. Depending on the aptitude of devices that may offer parallel processing, a program is managed by OS such that it may run on multiple processor at a time.\nEach of our products or services may also have more specific phrases of use for related Verizon Media RSS Feeds. In the event you see a crimson hat in a corner of your screen, you might be working the Crimson Hat Linux operating system. In this topology, every community gadget is linked to a single cable and it transmits data only in a single route. An Working System (OS) is a software that acts as an interface between pc hardware parts and the consumer.\nA network’s capability is how much site visitors the network can assist at any one time whereas still assembly service-level agreements ( SLAs ). Network capability is measured in terms of bandwidth Bandwidth is quantified by the theoretical maximum number of bits per second (bps) that may cross by way of a network device. You possibly can easily switch information and messages over networked computer systems.']	['<urn:uuid:fffe06b2-a1cb-4dad-b247-432c33c7eeee>']	factoid	direct	long-search-query	distant-from-document	single-doc	novice	2025-05-12T20:46:45.703947	7	14	733
50	power plant coolant types pros disadvantages	Different nuclear reactor coolant types have distinct advantages and limitations. Molten salts used in MSRs can't boil away due to high boiling points and allow for efficient fuel usage with no high-pressure requirements. Gases like helium and CO2 used in gas-cooled reactors are chemically stable with structural materials and offer flexible temperature ranges, though helium's low neutron absorption makes load control difficult. Gas-cooled reactors have lower power density requiring larger reactor sizes. Both systems avoid the risks associated with high-pressure water cooling systems used in traditional reactors.	['What is a Molten Salt Reactor? It is a completely different nuclear reactor than we have been using, with molten fuel cooled by stable salts. (We’ve been mainly using the Light Water Reactor, LWR, with solid fuel cooled by high-pressure water.)\nA Liquid Fluoride Thorium Reactor (LFTR) is a type of Molten Salt Reactor (MSR) that can use inexpensive Thorium for fuel (thorium becomes uranium inside the reactor). A slightly different type of MSR can consume the uranium/plutonium waste from solid-fueled reactors as fuel. MSRs make no long-term nuclear waste (over 99% of the fuel is fissioned, not left as waste).\nWe know Molten Salt Reactors work since we built and operated one — decades ago!\nImage “How Does a Fluoride Reactor Use Thorium” from PDF Kirk Sorensen – Thorium Energy Alliance.\n- Molten Salt Reactors have no high pressure to contain (no water coolant), and generate no combustible or chemically explosive materials;\n- A simple Freeze Plug melts in any emergency or for maintenance. The molten fuel then drains to passive cooling tanks where fission is impossible;\n- Reactor materials won’t melt under normal or emergency conditions, so radioactive materials stay contained. The fuel temperature is always much lower than the hottest the materials can handle. (In LWR, the temperature inside fuel pellets is always hotter than the reactor materials can handle, so cooling must remain working.)\n- Even if something (e.g. a bomb or earthquake) broke the reactor vessel, it would make a spill that quickly cools to solid, doesn’t interact with air or water, and would have most fission products chemically bonded to the salt, resulting in a cleanup volume of a few cubic meters;\n- MSRs would passively cool even without electricity (no MSR ever uses water, and no pumping is needed to cool the reactor);\n- Salt coolant can’t boil away (the boiling point of the salt is much higher than the reactor temperature), and the fuel is strongly chemically bound to the coolant, so loss of coolant accidents are physically impossible.\n- The molten fuel expands/contracts with temperature changes. Higher fission rate increases the temperature, which makes the fuel salt less dense, lowering the fission rate — all Molten Salt Reactors are very stable (a “strong negative temperature coefficient of reactivity”).\n- Fission products can be chemically separated from the fuel, while the reactor is operational. Fuel can be added as needed, to keep the fuel density steady (keep fuel concentration just above the minimum to maintain fission).\nMuch More Economical\n- Ambient-pressure operation makes MSRs easier to build while costing less (no high-pressure steam containment building, no high-pressure pipes);\n- Operating cost is less since the inherent safety of MSR means less complex systems than the LWR (every LWR requires multiple-redundant high-pressure systems);\n- Fuel cost is lower since no manufacturing fuel pellets (LWR pellets have to contain fission products under very high pressure) or fuel rods. For a LFTR, thorium is a cheap, plentiful fuel; (other MSR designs could eliminate LWR waste by using it as fuel);\n- For a LFTR, no expensive enrichment is required, simply add solid or molten thorium or plutonium to the molten fuel; for a thermal-spectrum MSR use low-enriched uranium; for a fast-spectrum MSR, un-enriched or depleted uranium can be used.\n- Total to develop LFTR technology and a factory to mass-produce them, will be less than the $10-12 Billion cost of a single new LWR; then a 100MW LFTR would cost about $200 Million. Sites can have as many reactors as needed to supply the city or region.\n- Easy siting, no large water source needed, no large safety zone required (because there is no water and no high pressure). Reactors would commonly be located several meters underground.\nMuch Less Nuclear Waste\nLWR uses ~2% of the fuel, because fission products trapped in the fuel pellets block fission, and the pellets get damaged by radiation and pressure. The rest of the uranium is considered “waste”, to be stored for over 100,000 years. Well, that is waste only if we only use LWR, or similar solid-fueled types of nuclear reactors. There are several types of nuclear reactor possible, that can fission All that uranium, plutonium, and other transuranic elements. (God didn’t make “useful uranium” and “defective uranium”; it’s the reactor design of LWR that only uses ~2% of the fuel, and that is after enrichment.)\nMSR has molten fuel, no fuel pellets, no fuel rods. Some of the fission products, those that block fission the most, are gasses — in LWR they are carefully trapped in the pellets, in MSR they bubble right out of the fuel salt and are collected. Most other fission products are easily chemically separated from the circulating fuel salt. Most MSR designs, including LFTR, use over 99% of the fuel.\nA MSR’s waste is safe (radiation levels below the original uranium ore and below background radiation) within 350 years. To produce 1 gigawatt electricity for a year, takes 800kg to 1000kg of thorium or uranium/plutonium “waste”. 83% of the fission byproducts are safe in 10 years, 17% (135 kg, 300 lbs) within 350 years, with no uranium or plutonium left as waste. After that, radiation is below background radiation levels. (Compare that 1000kg with 135kg for 350 years, to 250,000kg uranium to make 35,000kg enriched uranium for a solid-fueled reactor like LWR, for that same gigawatt-year electricity, all needing storage for 100,000+ years.)\nNo uranium, plutonium, or other long-term elements in LFTR or any MSR waste, since they are simply left in the reactor until they either fission or decay to short-term waste. (Standard industrial processing inefficiency of 0.1% leaves 1kg uranium; we can do better than that, but still much less per gigawatt-year than the 5500 kg uranium left in an open ash pile from an average USA coal plant!)\nMost of the fission products are valuable for industrial use. After a few years, radioactive decay brings them below background radiation, ready for use. For example, several rare earth metals, used for consumer electronics, are fission products. (As a bonus, the rare earth materials we currently mine are almost always found with thorium, which is currently considered a “nuclear waste” though it has one of the lowest levels of radiation of any radioactive material, radiation stopped by a thin layer of plastic or paper; when we use MSR we mine a little less rare earth materials and leave a little less thorium “waste”.)\nCan Consume Nuclear Waste\nInstead of thorium, a Molten Salt Reactor can use uranium-235 or plutonium waste, from LWR and other reactors. (Fast-spectrum molten salt reactors (FS-MSR) can use all isotopes of uranium, not just the 0.7% U-235 in natural uranium — with all the safety and stability of MSR.) 800kg of nuclear waste would work in the same reactor instead of 800kg thorium, with about the same fission byproducts, and the same electrical output. Convert 800kg to be stored for 100,000+ years, into 135kg to store for 350 years and 665kg for 10 years. No “PUREX reprocessing” needed, simply extract the uranium and plutonium (including fission products) from the fuel rod, and put it in a MSR. (In a MSR designed to use a different salt than LFTR would use, the zirconium cladding of a fuel rod could even be used to make the salt coolant.)\nSince no MSR uses water for cooling, there is no storage of water containing radioactive materials, and no concern of stored radioactive water leaking. (MSR can transfer heat to existing equipment such as steam generators, for example replacing the boiler at a coal plant, but doesn’t use water anywhere in the reactor.) Instead of using water, MSR could produce heat to efficiently desalinate water for drinking or farming.\nWithout needing a huge steam containment building (since there is no high pressure and no steam), MSRs such as LFTR use a much smaller site. A LFTR containment building would protect the reactor from outside impacts, and have extra radiation shielding, but would be much smaller and less expensive than a LWR containment building. MSRs can be safely built close to where there is electrical need (10MW to 2GW or more), avoiding transmission line power loss. No water source required.\nLFTRs could even be deployed for military field use or disaster relief. Imagine a few standard “18-wheeler” shipping containers brought in after 2017 Hurricane Harvey and Hurricane Maria, or 2018 Typhoon Mangkhut, providing 100MW electricity and desalinating water.\nCan Desalinate Water and Make Vehicle Fuel\nIn addition to delivering carbon-free electricity, LFTRs high temperature output can desalinate water (which we need in some areas even more than electricity, and we will need more as the world population grows).\nLFTRs also can generate carbon-neutral vehicle fuels, from water and carbon dioxide (from the atmosphere or ocean or large CO2 sources such as coal plants). The high heat of a LFTR (over twice what a LWR can generate) can split CO2 and split water, so making gasoline will be affordable.\nMolten Salt Reactors can be designed to output wide ranges of heat, for different industrial processes. They all automatically follow the load, meaning that if less heat is used there is less fission producing heat.\nReducing CO2 in the Oceans\nCarbon dioxide in the air enters the oceans, making acid. The acid is already killing plankton and other ocean life: the carbonic acid dissolves their “shells”. Researchers are exploring methods of using MSR heat to extract CO2 from solid materials containing a lot of CO2, store the carbon and release or use the oxygen, and then we could put those CO2-absorbing materials into the ocean to remove CO2 from the water. (Storing CO2 in a solid would work; storing compressed CO2 underground has a huge risk of leaks that would suffocate life on the surface.)\nMSRs are less expensive and more environmentally friendly than other sources of base-load power or grid power storage, needed to supplement wind and/or solar power.\nThe total cost of developing MSR technology and building assembly line production (like assembly line production of aircraft or ships, with better safety standards than is achievable with on-site construction, at much lower cost) will be much less than the $10-$12 Billion for a single new solid-fueled water-cooled reactor or single nuclear waste disposal plant. With sufficient R&D funding (around US $1 billion), five years to commercialization is entirely realistic (including construction of factories, less than US $5 Billion), and another five years for a national roll-out is feasible. (Unfortunately, the U.S. Nuclear Regulatory Commission says they will start writing licensing and regulations in 30 years.)\nCompletely Different Reactor\nThere is very little MSRs have in common with the solid fueled, water cooled reactors in use today. (Using thorium in a solid fueled, water cooled reactor, such as India is doing, does not give the safety and waste-reducing benefits of a molten fueled, salt cooled reactor.)\nNeed to communicate your complex information clearly? Contact Me.', 'Gas cooled reactors are another type of power plants that uses gas in nuclear reactors.\nAs the name itself suggests, a gas cooled reactor is cooled using a gas, and the heat extracted by the gas during the process of cooling the reactor is used either indirectly to generate steam which in turn is used for turbine propulsion, or this heated coolant could be used directly as the working fluid of the gas turbine thus eliminating the need for a separate steam circuit. Of course both these approaches have their own set of features and limitations. The moderator used in these types of reactors is Graphite which offers the advantages of being stable under conditions of high radiation as well as high temperatures.\nGases used for Cooling\nThere are several options available to choose for the coolant including gases but mainly carbon dioxide and helium are used as coolants apart from hydrogen in certain situations. Thermodynamically speaking Helium offers the best alternative since it has a high specific heat and low capture cross section for thermal neutrons but it is much expensive as compared to carbon-dioxide. The advantages that a gaseous coolant offers over light or heavy water are as follows\n- The gases are less prone to react chemically with the structural material of the reactor unlike water which has higher affinity for chemical reactions with these elements.\n- Gases are more flexible in terms of the temperatures and pressure ranges to which they can be subject to as compared to water. Of course certain practical considerations do limit these ranges but certainly they are more than those available for water.\n- Gas cooled reactors are more stable and safe because the reactivity of the reactor is not a function of the quantity of gas present in the core. Hence if a gas leak occurs accidentally the reactor would be much safer than a similar leak developed in water cooled reactor\nAs described above the direct system uses the same gas which is used as a coolant to act as the working fluid rotating the turbine blades from the enthalpy generated during heat absorption during the reactor cooling process. The main feature of such a system is that it gives the highest thermal efficiency of all types of nuclear reactors which are currently being used in the industry and typically gives efficiency of the order of nearly 42% within moderate operating temperatures.\nAnd indirect circuit consists of cooling gas in the primary circuit while the secondary circuit consists of water as the working fluid which is converted into steam using a heat exchanger.\nFeatures of Gas Cooled Reactors\n- If CO2 is used as the cooling gas it eliminates the possibility of explosion which is always present in water cooled reactors\n- There is no need for cladding the metallic fuel which leads to simple fuel processing techniques as compared to other types of reactors where cladding is necessary\n- The main drawback of these plants is there low power density which requires large size of the reactor for relatively smaller power requirements\n- Although Helium is an excellent cooling medium from the thermodynamic point of view, its low neutron absorbing capacity makes it unsuitable for load control']	['<urn:uuid:0507bb38-e09d-46ae-875c-f98eb32a17a4>', '<urn:uuid:d4dbf8dc-d732-4a02-9dae-027f59d31400>']	open-ended	with-premise	short-search-query	distant-from-document	three-doc	expert	2025-05-12T20:46:45.703947	6	87	2359
51	bipolar treatment options natural medicine differences	There are both medication-based and alternative approaches to treating bipolar disorder. For traditional treatment, mood stabilizers like lamotrigine (Lamictal) are considered among the safest options, with fewer side effects like weight gain or fatigue. Antidepressants must be used cautiously as they can trigger mania if not combined with mood stabilizers. For those seeking natural approaches, alternative treatments include fish oil, Rhodiola rosea, and St. John's wort, though these should be approached carefully. Other non-medication strategies include developing routines, focusing on diet and exercise, reducing stress, and avoiding drugs and alcohol. However, it's important to note that even with proper treatment, whether natural or medication-based, some people may still experience mood episodes.	"[""Manic depression, now more commonly called bipolar disorder, can be one of the more frightening mental illnesses for both the sufferer and his friends and family. It is marked by large mood swings from irrational, psychotic elation to deep periods of depression and despondence.\nIt manifests differently in each person, with some being more manic than depressed and vice versa. If someone you know has bipolar disorder, then knowing how to deal with a manic depressive will help you to relate with and help that person. He or she will appreciate your concern.\nPopular VideoPeople were so furious about this Pepsi ad that Pepsi pulled it after just one day. Watch it here and decide if it's offensive:\nThe following are some steps you can take to improve both your relationship with the bipolar individual and the therapeutic outcome of that person's treatment.\n- Educate Yourself\nUnderstanding the facts behind bipolar disorder will help you better relate to your friend or family member. There are a number of good books about the subject, and a quick search of the Internet will yield a raft of useful resources.\n- Don't Stereotype the Person\nWhatever you read or learn, don't force it onto the person. Because bipolar disorder is expressed differently in different individuals, no one-size-fits-all description will ever fit your friend exactly. Be open to the unique characteristics of the individual instead of focusing on the common features of the disease.\n- Be Honest about Your Relationship\nYour options for dealing with a manic depressive person will depend on your relationship to that person. For example, if the bipolar person is your spouse, you may not have to option of moving out as you would if he or she were your roommate. Your relationship also determines the degree of responsibility you have to the individual. Even if you can just move out, it may be wrong to do so if the person really depends on you for support.\n- Learn to Recognize Symptoms\nBy understanding the triggers and warning signs of both manic anddepressed episodes, you prepare yourself and are better able to address the issue early. Because he may not recognize his own symptoms, your position as an outside observer means you can suggest treatment before the condition gets too bad.\n- Support the Individual\nHelp the person to recover and to avoid relapsing. Assist in making doctors' appointments and picking up medication. Ensure that the person takes that medication. Help the individual avoid complicating problems like drug or alcohol abuse.\n- Don't Argue\nEspecially during manic periods, there is little point in arguing with a bipolar person. Mania is a psychotic episode, meaning the person is divorced from reality. He may not fully realize what he's saying, so try not to take anything too seriously. That said, don't avoid the person. Just remember that anything they say might be the illness talking.\n- Get Help\nBipolar disorder can lead people to do dangerous things. In addition to the threat of self-harm, both mania and depression can make a person engage in activities that threaten his life and health. If you suspect a person is in danger of these things, don't be afraid to get help. Offer to put the person in contact with a mental health professional. In an emergency, it is entirely appropriate to call 911 for assistance."", 'Antidepressants can trigger mania in people with bipolar disorder. If antidepressants are used at all, they should be combined with a mood stabilizer such as lithium or valproic acid. Taking an antidepressant without a mood stabilizer is likely to trigger a manic episode. Antidepressants can increase mood cycling.\nAre antidepressants bad for bipolar disorder?\nUsing antidepressant medication alone to treat a depressive episode is not recommended in people with bipolar I disorder. The drugs may flip a person, particularly a person with bipolar I disorder, into a manic or hypomanic episode. Hypomania is a more subdued version of mania.\nHow do antidepressants affect people with bipolar?\nAlso using antidepressants alone may trigger a manic or hypomanic episode in some people with bipolar disorder. Antidepressants alone also may lead to or prolong rapid cycling. In rapid cycling, a person may recover more quickly from depression but then experience mania followed by another episode of depression.\nWhich antidepressant is best for bipolar depression?\nTreating bipolar depression with antidepressants remains a popular option in clinical practice and published guidelines. Most clinicians choose the drug or class of drugs, usually selective serotonin reuptake inhibitors and bupropion, that is most effective and best tolerated.\nCan antidepressants send you into mania?\nAll antidepressants were associated with an increased incidence of mania/bipolar disorder (unadjusted HR>1.0 for all antidepressants) with incidence rates ranging from 13.1 (TCAs) to 19.1 (trazodone) per 1000 person-years.\nWhat is the best mood stabilizer for bipolar?\nLamotrigine (Lamictal) may be the most effective mood stabilizer for depression in bipolar disorder, but is not as helpful for mania. The starting dose of lamotrigine should be very low and increased very slowly over four weeks or more.\nWhat is the safest medication for bipolar disorder?\nLurasidone (Latuda) and Lamotrigine (Lamictal)\nBut lamotrigine is the better tolerated option, with few of the adverse effects that matter most to patients: weight gain, fatigue, sexual dysfunction, and long-term medical risks. Lamotrigine is better at preventing depression than it is at treating it.\nCan you be bipolar and not take medication?\n“Around half of people with bipolar disorder don’t take their medication which can lead to a relapse of symptoms. And this can have a knock-on impact with problems at work, strained relationships with family and friends, hospitalisation, and an increased risk of suicide.\nWhat happens if you take bipolar medication without being bipolar?\nTaking an antidepressant without a mood stabilizer is likely to trigger a manic episode. Antidepressants can increase mood cycling. Many experts believe that over time, antidepressant use in people with bipolar disorder has a mood destabilizing effect, increasing the frequency of manic and depressive episodes.\nCan you have a bipolar episode while on medication?\nYou may not be able to totally prevent bouts of mania or depression. Even people who always take their medication and take care of their health can still have mood swings from time to time.\nDoes Bipolar get worse as you age?\nBipolar may worsen with age or over time if this condition is left untreated. As time goes on, a person may experience episodes that are more severe and more frequent than when symptoms first appeared.\nHow can I fix my Bipolar without medication?\n10 Alternative Treatments for Bipolar Disorder\n- Fish oil.\n- Rhodiola rosea.\n- St. John’s wort.\n- Calming techniques.\nHow do you beat bipolar depression?\n10 Tips for Overcoming Bipolar Disorder\n- Be An Active Participant In Your Treatment. …\n- Go To Therapy. …\n- Closely Observe Your Mood and Symptoms. …\n- Don’t Isolate Yourself. …\n- Develop a Routine. …\n- Focus on Diet and Exercise. …\n- Reduce Your Stress. …\n- Avoid Drugs and Alcohol.\nCan you have a manic episode and not be bipolar?\nWhat are mania and hypomania? Mania and hypomania are symptoms that can occur with bipolar disorder. They can also occur in people who don’t have bipolar disorder.\nHow do you tell if you’re in a manic episode?\nBoth a manic and a hypomanic episode include three or more of these symptoms:\n- Abnormally upbeat, jumpy or wired.\n- Increased activity, energy or agitation.\n- Exaggerated sense of well-being and self-confidence (euphoria)\n- Decreased need for sleep.\n- Unusual talkativeness.\n- Racing thoughts.\nHow long does bipolar mania last?\nUntreated, an episode of mania can last anywhere from a few days to several months. Most commonly, symptoms continue for a few weeks to a few months. Depression may follow shortly after, or not appear for weeks or months. Many people with bipolar I disorder experience long periods without symptoms in between episodes.']"	['<urn:uuid:328c24e7-db9f-4fe4-981a-dce1de4c2920>', '<urn:uuid:a51aed1d-f01b-44e9-b82e-ee71635daf7f>']	open-ended	with-premise	short-search-query	distant-from-document	multi-aspect	novice	2025-05-12T20:46:45.703947	6	111	1324
52	milk proteins daily athlete intake recommendations amount	For athletes, the recommended protein intake is 1.2 to 1.7 grams per kilogram body weight per day, which should be spread across four meals. The protein should include both whey and casein, as whey rapidly increases protein synthesis while casein blocks protein breakdown. Three meals should contain 0.25 to 0.3 grams per kilogram body weight, with a larger pre-sleep meal containing 0.6 grams per kilogram body weight.	"['If you skim through a body building magazine, talk to someone in the gym, or conduct an online search for protein you will be inundated with information. Some if not most may be inaccurate but how do you decipher what is true? This article will outline protein sources to include the recommended quantity, timing and quality.\nFirst let’s discuss some background information on protein. Protein is the building blocks for the bones, muscles, cartilage and skin and is also essential for maintaining cellular integrity and function (1). Protein is made up twenty amino acids, nine of which are considered essential. Essential amino acids are ones the body cannot make so they must be acquired through food sources which can be found in animal food sources such as meat, fish, eggs and milk or vegetarian food sources such as grains, legumes, seeds and beans. Different types of protein have shown to provide a faster recovery response post workout that I’ll discuss later in the article.\nAccording to the Institute of Medicine Food and Nutrition Board, the recommended daily allowance (RDA) for protein for both men and women is 0.8 grams per kilogram body weight per day (1). The 2010 Dietary Guideline for Americans* state the daily recommended intake (DRI) for protein to be 10 to 35 percent of total calorie intake. In my opinion the RDA is excessively low when considering athletes and the DRI is too broad of a range especially if you are trying to dial in on your macronutrient needs, i.e. macronutrients are your carbohydrates, protein and fat. So how do you know how much you need? Digging deeper and coming to a more accurate response, in a 2009 Position Stand of the American Dietetic Association, Dietitians of Canada, and the American College of Sports Medicine: Nutrition and Athletic Performance, the recommended protein intake for athletes is 1.2 to 1.7 grams per kilogram body weight per day (2). The position stand notes the RDA and DRI do not take into account the specific needs of athletes which I completely agree with. The following is their recommendation for different types of athletes:\n- Endurance athletes: 1.2 to 1.4 grams per kilogram body weight per day\n- Strength Training: 1.2 to 1.7 grams per kilogram body weight per day\nFor example: A 165 pound (75 kilogram) endurance athlete would need 90 to 105 grams of protein per day OR the same weight athlete who was strength training would need 90 to 127 grams of protein per day. Both of these instances are well over the RDA of a calculated 60 grams per day (0.8 grams X 75 kilograms = 60 grams).\nQuantity and Timing of Protein Intake\nThis is all great information but what about specific timing of protein intake? I get this question a lot… how much post workout or how much during the day at each meal should I consume? I recommend a minimum of 20 grams of protein post workout. Research has shown that this amount illicits the leucine response and stimulates muscle protein synthesis (3). More specifically, protein intake for athletes includes (3):\n- Four equally spaced meals containing protein,\n- Three meals should be 0.25 to 0.3 grams per kilogram per body weight\n- A larger pre-sleep meal with protein intake at 0.6 grams per kilogram body weight\nFor example, let’s take a look at the 165 pound (75 kilogram) athlete. The protein intake would be to 18 to 22 grams at three meals with a larger intake of 45 grams of protein before bed. Total protein intake for the day would be 99 to 111 grams. This falls in the range of the 1.2 to 1.7 grams per kilogram per day in the example shown above. One minor exception, as stated above, I would recommend at least 20 grams of protein within 30 minutes after each workout because of the leucine response. For those who need more, the same leucine response has been found when consuming up to 40 grams of protein post workout. It has been shown that in elderly men, 40 grams of protein post workout is essential for muscle synthesis (3). The consumption of a minimum of 20 grams post workout recommendation would be across the board for any athlete, regardless of weight and gender.\nQuality of Protein Intake\nI’ve discussed timing and quantity of protein intake but now let’s take a look at quality. Not all protein is created equal especially for athletes and their recovery. Specifically whey protein produces a greater increase of muscle protein synthesis. Whey protein contains all 20 amino acids including the three branch chain amino acids leucine, isoleucine and valine which can be oxidized by muscled during exercise. The term “branch-chain” is the chemical structure of these amino acids. During recovery, due to the leucine content of whey, protein muscle synthesis is increased. Whey is more effective than soy which is more effective than casein protein sources (3). Lean body mass gains are seen with the ingestion of whey protein due to the leucine response. Whey is higher in leucine and is absorbed more quickly post workout. The ideal protein dose post workout to elicit the leucine response is 20 grams (3). The best whey protein source comes from diary. An inexpensive source of 20 grams of protein is 20 ounces of chocolate milk and a banana. While food sources of protein are best, protein powders can work but you have to be careful of the type of protein powder and what other ""junk"" is added to the product.\nI’ve seen many athletes who are trying to achieve a specific weight especially in endurance events. I myself know that I can run faster and with less pain if I am even 5 pounds lighter come race day. Based on personal experience I’ve cut back on food intake but lost lean muscle in the process. I came across a study looking at protein intake in women who are trying to lose weight. Research has found that increased protein intake, specifically through dairy during weight loss spares lean muscle mass while decreasing fat mass (4,5). When I work with athletes looking lose weight, I stick to the equally spaced protein at each meal and a bolus before bed as discussed earlier in the article.\nOverall, the main things to keep in mind with protein intake:\n- Consistent timing and quantity is important for athletes and in weight loss.\n- Post workout consumption of at least 20 grams of whey protein will stimulate a leucine response and protein muscle synthesis.\nWhile this article focuses on protein intake, athletes still need adequate carbohydrate intake for performance and recovery. I’ll look more into carbohydrate intake and research on timing and quantity in the near future.\n*The 2015 Dietary Guidelines Americans will be released this fall\n- Institute of Medicine, Food and Nutrition Board. (2005). Dietary Reference Intakes for Energy, Carbohydrate, Fiber, Fat, Fatty Acids, Cholesterol, Protein, and Amino Acids (Macronutrients). Retrieved June 15, 2015, from http://www.nap.edu/catalog/10490/dietary-reference-intakes-for-energy-carbohydrate-fiber-fat-fatty-acids-cholesterol-protein-and-amino-acids-macronutrients.\n- Position of the American Dietetic Association, Dietitians of Canada, and the American College of Sports Medicine: Nutrition and athletic performance. J Am Diet Assoc.2009; 109(3):509-527.\n- Phillips, Stuart. (2012). The Importance of Dietary Protein in Resistance Exercise-Induced Adaptation: All Proteins Are Not Created Equal. [PowerPoint slides]. Retrieved from http://scan-dpg.s3.amazonaws.com/resources/DOCS/webinars/2012_The_Importance_of_Dietary_Protein_in_Resistance_Exercise_Induced_Adaptation_webinar.pdf.\n- Josse AR, Atkinson SA, Tarnopolsky MA, Phillips SM. Diets higher in dairy foods and dietary protein support bone health during diet- and exercise-induced weight loss in overweight and obese premenopausal women. J Clin Endocrinol Metab. 2012 Jan;97(1):251-60. doi: 10.1210/jc.2011-2165. Epub 2011 Nov 2.\n- Josse AR, Atkinson SA, Tarnopolsky MA, Phillips SM. Increased consumption of dairy foods and protein during diet- and exercise-induced weight loss promotes fat mass loss and lean mass gain in overweight and obese premenopausal women. J Nutr. 2011 Sep;141(9):1626-34. doi: 10.3945/jn.111.141028. Epub 2011 Jul 20.', 'Proteins provide the body critical amino acids that serve as building blocks for the formation of new muscle. But not all dietary proteins are equal. The major proteins in milk are casein and whey. These two milk proteins are both excellent sources of all the essential amino acids, but they differ in one important aspect—whey is a fast-digesting protein and casein is a slow-digesting protein.Whey stimulates protein synthesis\nFast-digesting whey means it is emptied from the stomach quickly, resulting in a rapid and large increase in plasma amino acids. This translates into a quick but transient increase in protein synthesis, while protein breakdown is not affected. Whey also has higher levels of leucine, a potent amino acid that stimulates protein synthesis. Whey protein is superior at augmenting protein synthesis rapidly, but this positive effect is short-lived. Consuming repeated doses of whey allows for sustained high levels of blood amino acids and repeated bursts of protein synthesis that provide superior effects on muscle protein balance.Casein offers a positive protein balance\nCasein is the most abundant protein in milk. It is relatively insoluble and tends to form structures called micelles that increase solubility in water. During the processing of milk, which usually involves heat or acid, the casein peptides and micelle structure become disturbed or denatured to form simpler structures. As a result, a gelatinous material is formed. This is the basis for why casein has a slower rate of digestion, and results in a slow but steady release of amino acids into circulation.\nIn one study, researchers gave healthy subjects 30 grams of either whey protein or casein protein and made several measures of the anabolic and catabolic effect for 7 hours after the meal. Whey protein resulted in a rapid increase in blood amino acids and protein synthesis, but it was short-lived. Casein, on the other hand, resulted in a prolonged increase in blood amino acids that resulted in a 34% reduction in protein breakdown. The net protein balance remained more positive after intake of casein protein over a 7-hour period. The superior long-lasting effect of casein was attributed to a delayed gastric emptying and slower absorption rate from the gastrointestinal tract to the blood.Whey and casein are better together\nSince whey rapidly increases protein synthesis and casein blocks protein breakdown, a combination of both would be ideal.\nA recent study compared the effects of supplementing with either a combination whey and casein protein versus carbohydrate on several markers of muscle anabolism during strength training.3 Untrained men participated in a 10-week resistance training program and either supplemented with 40 grams of carbohydrate or 40 grams of protein containing a mixture of whey and casein. Half of the supplements were consumed one hour before and then immediately after exercise on workout days. The results were overwhelmingly positive for the combination protein group. Despite similar background diets and identical training programs, supplementation with protein resulted in greater increases in several measures of muscle anabolism, including greater increases in lean muscle mass, thigh muscle mass, muscle strength, anabolic hormones and muscle specific proteins.\nIn a similar study that lasted 14 weeks, untrained men performed resistance training and received either 25 grams of carbohydrate or 25 grams of a combination whey and casein protein one hour before and immediately after exercise. The combination protein group had significantly greater increases in muscle fiber size compared to the carbohydrate group. These studies provide strong evidence that a combination protein consumed before and after workouts increases muscle size.Practical uses of whey and casein\nThe science unequivocally shows that the digesting rate of protein is an important regulator of protein balance. Whey provides a quick burst of protein synthesis while casein makes an ideal protein supplement to sustain long periods of an anabolic environment for muscle growth. Based on these different characteristics, whey and casein can be used alone and in combination to exploit their unique biologic effects.\nFor example, whey protein can be very effective before and after a workout and first thing in the morning. But since the benefits of whey after exercise are short-lived, you should consume a meal containing protein 20-60 minutes after drinking a post-workout whey protein shake. In one study, 30 grams of whey protein were provided in a sequence of 13 small meals given each 20 minutes. This was found to be far superior for muscle anabolism compared to a single meal of whey or casein.\nAlternatively, a combination of whey and casein (20 grams) could be consumed one hour before and immediately after exercise for a sustained benefit on protein balance. Casein is a perfect protein for a shake before bed because it promotes a sustained anti-catabolic environment while you sleep.\nBecause whey and casein have different but complementary effects, many people keep both types on hand and use them differently throughout the day—whey in the morning and after workouts and casein before bed. Or you can mix whey protein into a large glass of milk (about 80% casein) to combine the benefits of both. Research shows these proteins support greater increases in lean body mass and decreases in body fat as part of a resistance training program.']"	['<urn:uuid:1f8df907-7295-4ab2-a67a-165c5a725424>', '<urn:uuid:2a4d22ef-9d80-4399-bf03-46e969b6b1ac>']	factoid	with-premise	short-search-query	similar-to-document	comparison	expert	2025-05-12T20:46:45.703947	7	67	2164
53	night time moth grub garden damage signs	Signs of cutworm infestation include fresh cuts appearing on crop stems overnight, plants that were healthy the previous day showing damage the next morning, and cutworms being physically visible near plants' roots after dark.	['Cutworms have long been a nuisance in the home garden and bigger lawn. These larvae come from different species of moths and are capable of causing severe damages to the plantations. Cutworms are nocturnal and thus, only feed after dark. You will not see any cutworms during the day. Hence, gardeners commonly find out the damage caused by cutworms in the next morning. As its name suggest, cutworms damage the plantations, especially the young ones by clipping off seedling stems and young plants around the soil surface. Cutworm infestation may cause damages to a large number of vegetable plantations, that early treatments and prevention are needed.\nSigns of cutworm infestations\nTo properly treat damaged plants and prevent future infestation, the signs of cutworm infestation should be checked thoroughly. Some signs of infestation can be seen physically if you manage to look for them carefully. Those signs are:\n- There are some kinds of crops undergoing fresh cuts on the stems.\n- Cuts on the infested crops occur the next day, while in the previous day the plantation seems to be healthy.\n- There are some cutworms physically seen near the plants’ roots after dark.\nCutworms can be gardeners’ nightmare because of the damages they cause. Severe cutworm outbreak often causes harvest failure. In order to prevent major damages caused by this pest, early monitoring and findings should be considered. Cutworms damage the crops by eating the stems and cutting them off. Hence, protecting the crops from cutworm bites is essential in minimizing the effects. To perform thorough cutworm control and thus, preventing the damages there are some methods to perform, including:\nControlling cutworms naturally\nNatural cutworm controls can be done by applying conventional methods. These methods include monitoring and getting rid of cutworms physically. To control cutworms using natural solutions, which is safer and more affordable, these methods are useful:\n- Monitoring cutworm infestation closely after dark. You can hardly see cutworm infestation during the day since the pest is nocturnal. At night, conversely, cutworms can be seen around the crops’ roots, especially during the outbreak. To do the inspection, look for the critters around the plants’ roots and lower stem, which is usually targeted by cutworms.\n- Picking infesting cutworms by hands. Once you found the pest, do not leave them alone with the crops, as you might find most of the crops you have planted cut off the next morning.\n- Make an affordable crops collar made of recycled plastic or cardboard. Place the collar around the plant and push it onto the soil. This will prevent cutworms from climbing up the stem and chew the big part of it.\n- Spread grounded egg shells around the plants. Grounded egg shells will not only give nutrition to the soil but acts as drying agent which has an ability to absorb excessive moisture. Grounded egg shells also dehydrate cutworms and eventually kill them.\n- Installing bird feeders near the infested areas. Using natural predator is a good pest control method, including cutworms.\nControlling cutworms using pesticides\nWith severe cutworm infestation, natural control methods, such as the protective collars might be time-consuming and thus, ineffective, unless it is taken as a preventive method before you physically see the moth worms. In this case, the use of pesticides can be helpful for minimizing the effects of cutworm infestation. Helpful pesticides for cutworm infestation include:\n- Diatomaceous Earth. DE is made from fossilized water plants and algae and has a high content of silicon. This tiny material has sharp edges which tear insects’ organs and eventually kill them. To control cutworms using DE, spread the DE around the base of the plants where cutworms usually appear from. At night, cutworms heading to the stem must cross the soil sprinkled with DE, which will cuts the outer layer of its body, exposing the organs and dehydrate the worm. They will die because of this.\n- Using pesticide containing Bacillus thuringiensis. If you decide to use a chemical pesticide, do it after dark for maximizing its effects on killing cutworms infesting the crops. Pesticides with thuringiensis is a natural cutworm killer but may harm butterflies—a natural pollinator. Hence, controlling cutworm infestation using chemicals should be combined with more natural methods, so that the balance of ecosystem can be maintained.']	['<urn:uuid:9a5d0fc6-240b-4d8e-aea0-223944b13e48>']	factoid	with-premise	short-search-query	distant-from-document	single-doc	expert	2025-05-12T20:46:45.703947	7	34	715
54	what key differences between air dry clay and polymer oven baked clay composition properties	Polymer clay is made of plastic and PVC, stays pliable when exposed to air, requires 15 minutes baking per ¼ inch thickness, and becomes hard and durable when baked. Air dry clay is made of clay and glue, hardens when exposed to air for 2-3 days, shrinks while drying, and remains more fragile.	['Air dry clay is a fantastic tool for creating fun, unique pieces for any room. It’s fairly easy to sculpt and can then be left in a safe place to fully dry and harden into its intended shape. The downside is that the air drying process takes much longer than a quick bake, so if you are under a time crunch, could you bake air-dry clay?\nYou can bake air-dry clay. While not typically recommended, it is possible to bake air-dry clay if you follow the correct process and a few simple safety measures.\nThe rest of this article will explain the correct process for baking air dry clay and will dig into the differences between typical baking clay and air dry clay. Read on, and you’ll be ready for your next creative clay project!\nHow Do You Bake Air Dry Clay?\nTo bake air-dry clay, you will need:\n- A baking sheet\n- Parchment paper\n- Your clay project\n- A conventional oven\nStarting with a cold oven, complete the following steps:\n- Line the baking sheet with parchment paper.\n- Place your clay project on top of the paper.\n- Place the baking sheet with your project in the oven.\n- Preheat the oven to 200°F (93°C)\n- Immediately set a timer for 20 minutes (do not wait for the oven to preheat before setting the timer).\n- Turn off the oven.\n- Leave the project in the oven until both the oven and project have fully cooled.\nA Few Important Notes About Baking Air Dry Clay\nBefore baking your first air-dry clay project, take a minute to review these important notes!\n- Never put your project into a hot oven. It is very important to make sure the oven is turned off and completely cold before putting your project inside. Going from room temperature to 200°F (93°C) too quickly can result in cracked clay, so be sure to allow the project to slowly heat up with the oven.\n- Do not wait for the oven to preheat before starting the timer. The timer should be set as soon as your project goes into the cold oven. The preheating time is included in the 20-minute timer. Leaving your project in the oven for 20 minutes after the preheating process is complete could result in severe cracking, brittleness, or burning.\n- Allow the project and the oven to completely cool before removing. Once the timer is up, turn the oven off, but do not remove your project. Going from 200°F (93°C) to room temperature too quickly can have the same effect as the reverse (cracking, brittleness, breakage), so take care to allow your project to fully cool before removing.\n- Review the manufacturer’s label before baking. Some air-dry clays will have baking instructions included in their packaging, while others will explicitly warn you against baking the air-dry clay.\n- Always line the baking sheet with parchment paper. Your clay should never touch the same surfaces you use to prepare food, so be sure to line your baking sheet or tray with parchment paper and thoroughly clean them when finished.\nWhat Is the Difference Between Air Dry and Baking Clay?\nWhen creating with clay, there are many different mediums to choose from. One of the most common oven-baked clays is polymer clay. When comparing the composition of air-dry and oven-baked polymer clay, there are several notable differences.\nWhat Is Air Dry Clay?\nAir-dry clay is an easy-to-use clay developed by Crayola. It is incredibly convenient due to its ability to be sculpted, fully formed, and dried without the use of a kiln. Kilns are quite large and fairly expensive, making them unrealistic for some hobby crafters.\nBecause of its natural moisture content, air-dry clay will harden when exposed to air for long periods (often 2 to 3 days). When storing air-dry clay, you’ll want to keep it completely sealed in order to prevent it from drying out unintentionally.\nAs it dries, air-dry clay tends to slightly shrink in size, so don’t be surprised if your project turns out a little smaller than the original mold. This clay is ideal for beginners and children because of its pliability and inexpensive price tag.\nAir-dry clay is made up of clay and glue. Even after thoroughly drying, it is important to note that this type of clay is not waterproof and can be softened if it comes into contact with water. Be mindful when choosing the clay medium for your project to only choose air-dry clay if you know your project will not be regularly exposed to water like a flower pot or an outdoor statue.\nFor a charming and simple air-dry-clay-friendly project, check out my other post on this DIY Mini Wall Shelf! This sweet, no-nails-required shelf is perfect for beginners.\nWhat Is Oven-Baked Polymer Clay?\nPolymer clay is becoming increasingly common in the world of crafting, often used for jewelry, beads, buttons, and decorative home pieces. This clay is not a traditional, natural clay. It is a man-made material composed of plastic and polyvinyl chloride, also known as PVC.\nThis clay medium is excellent because it can be baked in your home oven with ease and will not lose its shape or size as it dries. Depending on the brand of clay, your project typically needs to bake for about 15 minutes per ¼ inch (6 mm) of thickness. When compared to air-dry clay, this is clearly a more time-efficient option.\nOnce baked, polymer clay becomes hard and reasonably durable, not nearly as fragile as its air-dry clay counterpart.\nOven-baked polymer clay conveniently stays pliable, even when exposed to air. Because of this, you have plenty of time to get your sculpture just right without worrying about it beginning to harden or change its texture.\nJust getting started with using air dry clay? I share my most effective air clay techniques for beginners in my ultimate how-to guide.\nTechnically, you can bake air-dry clay, but it is not always recommended. When you oven-bake air-dry clay, you put your project at risk of cracking, breakage, and even burning.\nHowever, if you follow the outlined process for baking air-dry clay, you can successfully speed up the drying process using your home oven!']	['<urn:uuid:b5d2c351-f1b6-4950-9ba1-79b0d47850cc>']	factoid	direct	long-search-query	similar-to-document	single-doc	expert	2025-05-12T20:46:45.703947	14	53	1039
55	what are the customs duties charged for transit goods and what refund procedures must be followed to get deposit back	Transit goods are not subject to customs duties, taxes or other charges except for specific services rendered. For refunds, the owner must submit a Form C30 application within 30 days of exportation, along with necessary documents including the export certificate Form C17. If the application isn't made within 30 days, the goods will be considered imported for home consumption and import duty will be charged.	"['RIGHT OF ACCESS OF LAND-LOCKED\nSTATES TO AND FROM THE SEA\nAND FREEDOM OF TRANSIT\nUse of terms\n1. For the purposes of this Convention:chanrobles virtual law library\n(a) ""land-locked State"" means a State which has no sea-coast;chanroblesvirtuallawlibrary\n(b) ""transit State"" means a State, with or without a sea-coast, situated between a land-locked State and the sea, through whose territory traffic in transit passes;chanroblesvirtuallawlibrary\n(c) ""traffic in transit"" means transit of persons, baggage, goods and means of transport across the territory of one or more transit States, when the passage across such territory, with or without trans-shipment, warehousing, breaking bulk or change in the mode of transport, is only a portion of a complete journey which begins or terminates within the territory of the land-locked State;chanroblesvirtuallawlibrary\n(d) ""means of transport"" means:chanrobles virtual law library\n(i) railway rolling stock, sea, lake and river craft and road vehicles;chanroblesvirtuallawlibrary\n(ii) where local conditions so require, porters and pack animals.\n2. Land-locked States and transit States may, by agreement between them, include as means of transport pipelines and gas lines and means of transport other than those included in paragraph 1.\nRight of access to and from the sea and freedom of transit\n1. Land-locked States shall have the right of access to and from the sea for the purpose of exercising the rights provided for in this Convention including those relating to the freedom of the high seas and the common heritage of mankind. To this end, land-locked States shall enjoy freedom of transit through the territory of transit States by all means of transport.cralaw\n2. The terms and modalities for exercising freedom of transit shall be agreed between the land-locked States and transit States concerned through bilateral, subregional or regional agreements.cralaw\n3. Transit States, in the exercise of their full sovereignty over their territory, shall have the right to take all measures necessary to ensure that the rights and facilities provided for in this Part for land-locked States shall in no way infringe their legitimate interests.\nExclusion of application of the most-favoured-nation clause\nThe provisions of this Convention, as well as special agreements relating to the exercise of the right of access to and from the sea, establishing rights and facilities on account of the special geographical position of land-locked States, are excluded from the application of the most-favoured-nation clause.\nCustoms duties, taxes and other charges\n1. Traffic in transit shall not be subject to any customs duties, taxes or other charges except charges levied for specific services rendered in connection with such traffic.cralaw\n2. Means of transport in transit and other facilities provided for and used by land-locked States shall not be subject to taxes or charges higher than those levied for the use of means of transport of the transit State.\nFree zones and other customs facilities\nFor the convenience of traffic in transit, free zones or other customs facilities may be provided at the ports of entry and exit in the transit States, by agreement between those States and the land-locked States.\nCooperation in the construction and improvement of means of transport\nWhere there are no means of transport in transit States to give effect to the freedom of transit or where the existing means, including the port installations and equipment, are inadequate in any respect, the transit States and land-locked States concerned may cooperate in constructing or improving them.\nMeasures to avoid or eliminate delays\nor other difficulties of a technical nature in traffic in transit\n1. Transit States shall take all appropriate measures to avoid delays or other difficulties of a technical nature in traffic in transit.cralaw\n2. Should such delays or difficulties occur, the competent authorities of the transit States and land-locked States concerned shall cooperate towards their expeditious elimination.\nEqual treatment in maritime ports\nShips flying the flag of land-locked States shall enjoy treatment equal to that accorded to other foreign ships in maritime ports.\nGrant of greater transit facilities\nThis Convention does not entail in any way the withdrawal of transit facilities which are greater than those provided for in this Convention and which are agreed between States Parties to this Convention or granted by a State Party. This Convention also does not preclude such grant of greater facilities in the future.\nUnited Nations Convention on the Law of the Sea - Part X', 'Transit goods are goods imported through the region to a country outside the region.\nQ: What value is applicable for transit goods?\nIt is the transaction value adjusted according to Article VIII of the WTO valuation regulations.\nQ: Are transit goods subject to import duty?\nNo they are not. However if the consignment fails to exit the region within the prescribed period and no official extension is granted, duty becomes due. The importer may request the Commissioner to allow consumption of the goods in the local market. If this is authorized, then duty becomes payable.\nQ: What is the nature of goods that can be transited?\nAll types of goods can be transited with the exception of prohibited goods.\nQ: Can transit goods be converted for home consumption?\nYes, the owner can apply in writing to the Commissioner to allow entry of the goods and refund on any deposit will be given or the bond on such goods will be cancelled\nQ: Is there a specific transit period?\nYes, the transit period is less than thirty (30) days from the date of entry or any further period as the Commissioner may allow. During this period, bulk cargo may be stored in a Transit go-down. This is a facility licensed by the Commissioner for the storage of transit cargo.\nQ: How does the Customs administration secure the duty due on transit cargo?\nDuties due are secured through the execution of a security bond.\nQ: Is there a standard security bond amount?\nYes, security bond is set at an amount equal to the duty payable on the goods.\nQ: What is the fate of securities, bonds and other deposits made on transit goods?\nRefunds or bond cancellation are granted after exportation. The owner must submit an application (Form C30) for refund of deposit or cancellation of the bond accompanied by necessary documents including the export certificate Form C17.\nIf the application for a refund is not made within 30 days, goods will be viewed to have been imported for home consumption and import duty will be charged on them.\nIf the owner fails to submit exportation proof of part of the transit goods due to circumstances beyond his/her control, the Commissioner shall only refund the deposit or cancel the bond to goods that have proof of exportation.\nQ: What are the procedures and requirements for application as a carrier?\nGoods in transit shall only be carried through the Community in sealed, licensed vehicles except in the case of exceptional loads\nTrailers and motive units intended for the conveyance of transit cargo must be licensed by the Commissioner using Form C38. For motive units and trailers licensed in any of the COMESA Member States or SADC a certificate of approval in Form C39 should be issued by the Commissioner on application.\nOn approval the carrier should execute a bond using Form CB12.\nWhere a carrier owns more than one vehicle, only one bond shall be required\nA carrier who is a licensed customs agent shall not be required to execute a bond.\nLicense fees of payable by carriers of transit goods is currently being harmonized\nThe vehicles mentioned above should bear the words “TRANSIT GOODS” printed boldly and clearly on both sides as specified in Form C38\nThe vehicles used for conveyance of transit cargo should be constructed and equipped in such manner that:-\na customs seal can be fixed easily on doors and all other closing systems\ngoods cannot be removed from or introduced into the sealed part of the vehicle without breaking the customs seal\nthe vehicle does not contain concealed spaces\nspaces capable of holding goods are readily accessible for Customs inspection\nadditional compartments inside the vehicle should be firmly fixed, sealed, unbroken and capable of being dismantled without leaving obvious traces,\nopenings for lubrication, maintenance and filling of the sand box are fitted with a cover and should be inaccessible to the loading compartment from the outside;\nhinges are made and fitted such that doors and other closing systems cannot be lifted off the hinge-pins\ndoors cover all interstice and ensure complete and effective closure; and\nThe Customs seal is protected once it is affixed.\nQ: Who is authorized to clear transit goods through Customs?\nA licensed Customs agent who is further licensed by the Commissioner to clear transit goods.\nQ: Is there a special route that transit goods must go through?\nYes, goods in transit shall be conveyed by road on routes approved by the Commissioner. A person who does not follow the specified transit route commits an offence whose fine is 5% of the value of the goods and the goods which are the subject of the offence shall be detained.\nQ: Is it an offence not to export transit goods?\nYes it is an offence not to export transit cargo, however one can apply to the Commissioner for authority to convert the consignment for home use.\nQ: What penalties are payable by the owner to the proper officer for offences related to goods on transits?\nWhere the quantity of goods presented at the port or place of exportation is less than that written on the entry, the owner should pay the duty on the extra goods, unless they are accounted to the proper officer.\nIf the transit period expires before any part of the goods is exported, the owner should pay the penalty to bond.\nIf in addition to the expiry of the transit period there are extra quantities of goods undeclared in the declaration form, a penalty to bond will be paid and in addition, duty for the extra cargo will also be paid.\nIf goods are not exported after the payment of any penalties, the goods shall be subject to forfeiture.\nQ: What laws apply to goods in transit?\nGoods in transit are subject to laws that relate to importation, prohibition, entry, examination, landing and exportation of goods as stipulated in the EAC Customs Management Act.\nQ: Can a commissioner detain goods on transit?\nYes, the commissioner can prohibit, restrict or control the entry of certain goods or type of transport used to convey the goods if they pose a safety, health, hygiene risk or pose harm to animal, plant or if they are against the public interest.']"	['<urn:uuid:3b4f4dc9-1d55-4af3-9273-7f7307441fae>', '<urn:uuid:fa136052-9b52-413d-9021-d1ab47b33ea3>']	factoid	direct	long-search-query	similar-to-document	multi-aspect	novice	2025-05-12T20:46:45.703947	20	65	1764
56	What is fluorescence and how is it used in examining minerals and forensic evidence?	Fluorescence is a form of cool light emission where a substance gives off visible light while being stimulated by electromagnetic radiation or other means. Unlike phosphorescence, fluorescence stops immediately when the stimulation ends. In forensic science and mineralogy, fluorescence is particularly useful. Many minerals like quartz, feldspar, and carbonates exhibit fluorescence when exposed to electron beams. This property helps in identifying and discriminating between samples from different sources. In forensic analysis, fluorescence can reveal details in evidence like paint layers and duct tape components that might not be visible through regular microscopy. The technique also helps detect trace elements and structural defects in materials, which can provide information about their geological origin or manufacturing process.	"[""fluorescence(redirected from Flourescence)\nAlso found in: Dictionary, Thesaurus, Medical.\nRelated to Flourescence: phosphorescence, fluorescence microscopy, Fluorescence quenching, Fluorescence spectroscopy\ngeneral term applied to all forms of cool light, i.e., light emitted by sources other than a hot, incandescent body, such as a blackbody radiator. Luminescence is caused by the movement of electrons within a substance from more energetic states to less energetic\n..... Click the link for more information. in which light of a visible color is emitted from a substance under stimulation or excitation by light or other forms of electromagnetic radiation or by certain other means. The light is given off only while the stimulation continues; in this the phenomenon differs from phosphorescencephosphorescence\n, luminescence produced by certain substances after absorbing radiant energy or other types of energy. Phosphorescence is distinguished from fluorescence in that it continues even after the radiation causing it has ceased.\n..... Click the link for more information. , in which light continues to be emitted after the excitation by other radiation has ceased. Fluorescence of certain rocks and other substances had been observed for hundreds of years before its nature was understood. Fluoresecence also occurs in some living organisms; some coral, reef fish, jellyfish, and other marine species as well as such terrestrial plants and animals as certain spiders and pitcher plants fluoresce. Probably the first to explain it was the British scientist Sir George G. Stokes, who named the phenomenon after fluorite, a strongly fluorescent mineral. Stokes is credited with the discovery (1852) that fluorescence can be induced in certain substances by stimulation with ultraviolet light. He formulated Stokes's law, which states that the wavelength of the fluorescent light is always greater than that of the exciting radiation, but exceptions to this law have been found. Later it was discovered that certain organic and inorganic substances can be made to fluoresce by activation not only with ultraviolet light but also with visible light, infrared radiation, X rays, radio waves, cathode rays, friction, heat, pressure, and some other excitants. Fluorescent substances, sometimes also known as phosphors, are used in paints and coatings, but their chief use is in fluorescent lightinglighting,\nlight produced by artificial means to allow visibility in enclosures and at night. For stage lighting, see scene design and stage lighting. Early Sources of Artificial Lighting\n..... Click the link for more information. .\nFluorescence is generally defined as a luminescence emission that is caused by the flow of some form of energy into the emitting body, this emission ceasing abruptly when the exciting energy is shut off. In attempts to make this definition more meaningful it is often stated, somewhat arbitrarily, that the decay time, or afterglow, of the emission must be of the order of the natural lifetime for allowed radiative transitions in an atom or a molecule, which is about 10-8 s for transitions involving visible light. Perhaps a better distinction between fluorescence and its counterpart, phosphorescence, rests not on the magnitude of the decay time per se, but on the criterion that the fluorescence decay is temperature-independent.\nIn the literature of organic luminescence, the term fluorescence is used exclusively to denote a luminescence which occurs when a molecule makes an allowed optical transition. Luminescence with a longer exponential decay time, corresponding to an optically forbidden transition, is called phosphorescence, and it has a different special distribution from the fluorescence. See Phosphorescence\nThe decay time of fluorescent materials varies widely, from the order of 5 × 10-9 s for many organic crystalline materials up to 2 s for the europium-activated strontium silicate phosphor. Fluorescent materials with decay times between 10-9 and 10-7 s are used to detect and measure high-energy radiations, such as x-rays and gamma rays, and high-energy particles such as alpha particles, beta particles, and neutrons. These agents produce light flashes (scintillations) in certain crystalline solids, in solutions of many polynuclear aromatic hydrocarbons, or in plastics impregnated with these hydrocarbons. The so-called fluorescent lamps employ the luminescence of gases and solids in combination to produce visible light. See Luminescence\nfluorescence(floo-ŏ-ress -ĕns) The transformation of photons of relatively high energy (i.e. high frequencies, especially ultraviolet frequencies) to lower-energy photons through interactions with atoms. It is also the lower-energy radiation that is produced by the process.\na luminescence that decays in a time τ ~ 10–8 –10–9 sec. The division of luminescence into fluorescence and phosphorescence is out of date, since it is an arbitrary distinction based on a qualitative measurement of the duration of luminescence. In terms of the mechanism for converting the excitation energy into visible emission, fluorescence, as a rule, is spontaneous luminescence; hence τ is determined by the lifetime of an atom in an excited state.\nResonance fluorescence, the frequency of which coincides with the frequency of the exciting radiation, is observed in atomic vapors (seeRESONANCE RADIATION). Molecules may fluoresce in highly rarefied vapors; an increase in the vapor pressure or the addition of foreign impurities may quench the fluorescence. Many organic substances, particularly aromatic compounds, fluoresce in liquid and solid solutions as well as in the crystalline state.\nThe spectra, polarization, and kinetics of fluorescence are associated with the structure and symmetry of molecules and with the nature of molecular interactions and depend, for example, on the concentration of solutions and the type of excitation. Fluorescence may be used, for example, to study crystal structure and excition processes in crystals (seeSPECTROSCOPY, CRYSTAL), the energy levels of molecules, the structure and interaction of molecules, and the processes by which excitation energy is transferred. Fluorescence is used in luminescence analysis, scintillation counters, and mineralogical research.\nThe decay time of fluorescence is measured by means of fluorometers.\nREFERENCESSee references under .\nof minerals, a luminescence that is excited in minerals by light, X rays, or electrons and that decays rapidly (within 10–2–10–1 sec) after the excitation ceases. The rapid decay of the luminescence distinguishes fluorescence from phosphorescence and thermoluminescence. The physical phenomenon of the fluorescence of minerals was first observed in fluorite, from which the term “fluorescence” is derived.\nFluorescence is characteristic of dielectric minerals and semiconductors that are transparent to visible light and to near-ultraviolet and infrared radiation. The fluorescence of minerals is associated with impurities or occasionally with intrinsic ions or complexes; such impurities, ions, or complexes form luminescence centers. Fluorescence is sometimes completely or partially quenched by certain isomorphic impurities, such as ions of bivalent iron.\nThe fluorescence of minerals is used in luminescence analysis to detect such minerals as scheelite, zircon, apatite, and uranites in mining excavations and to analyze microscopic impurities consisting of rare or dispersed elements, such as uranium or rare earths. It is also used for ore dressing by means of the identification of the useful component—for example, diamonds, fluorspar, or scheelite—on the basis of the component’s luminescence.\nREFERENCEMarfunin, A. S. Spektroskopiia, liuminestsentsiia i radiatsionnye tsentry v mineralakh. Moscow, 1975.\nB. S. GOROBETS"", 'Cathodoluminescence in Forensic Science\nPalenik, C.S. and Buscaglia, J. (2006) Cathodoluminescence in Forensic Science. American Academy of Forensic Sciences Annual Meeting (Seattle, WA).\nPresented on: 2/1/2006\nCathodoluminescence (CL) refers to the emission of visible (or near visible) light from a sample that has been bombarded by an electron beam. CL is observed in many materials routinely encountered in the forensic analysis of trace evidence (e.g., soil, building materials, glass, pigments, and filler/extenders). CL results from the presence of trace elements or structural defects in materials, which are characteristic of either the geological environment of formation or the manufacturing process (for a synthetic luminescent material). The variation in luminescence for a particular mineral can therefore be used to discriminate among samples from different sources or, in certain cases, provide information about the provenance of a sample. While a multitude of established techniques exist for the analysis of trace evidence, CL offers a widely-applicable alternative technique that provides a unique means for visualization and identification of trace elements and structural defects in a sample.\nWithin the category of geological evidence (e.g., sand, soil, and concrete), many of the most abundant minerals are luminescent (e.g., quartz, feldspar, and carbonate minerals). Traditionally, these mineral components have been difficult to use for forensic discrimination or sourcing due to their presence in nearly all samples; however, the variation in luminescence within a given mineral type provides a new prospect for improving the significance of geological evidence. CL provides a relatively fast method to screen soil samples through visual identification of luminescent minerals (e.g., identification and classification of feldspars), the ability to determine if multiple populations of a given mineral exist (e.g., quartz from different sources) and a means to estimate the relative abundances of luminescent minerals in a sample. Surface information including zoning (i.e., compositional changes within a crystal), textures and coatings can provide additional information about the origin of a sample. For example, fragments of biogenic carbonates can be morphologically identified. For other minerals such as quartz, the visible luminescence color can be broadly correlated with a geological formation condition (e.g., metamorphic, volcanic, authigenic). In addition to visual observation, CL spectroscopy can offer more detailed information about specific activators (defects and trace elements responsible for luminescence) in a given mineral. In feldspar minerals, the chemical composition can be estimated on the basis of the Fe3+ emission band. In heavy minerals such as zircon, monazite, and apatite, rare earth element activators, typically present at 1-500 ppm, can be identified spectroscopically. Together, visual and spectroscopic examination of mineral components can be combined to provide a variety of information about soil and sand samples that complement more traditionally used analytical techniques.\nMany synthetic or anthropogenically modified minerals such as pigments and filler/extenders also luminesce (e.g., anatase, wollastonite, zincite, and talc). Such minerals are utilized in the manufacturing of a variety of materials commonly encountered as forensic evidence including paint and duct tape. In both materials, CL provides a means to visualize details of the layer structure. In white architectural paints, for example, CL can be used to identify multiple layers that may not be visible by light microscopy. The three main components of duct tape, adhesive, backing and reinforcing fibers (i.e., scrim) all luminesce. Within a given layer of paint or duct tape, CL can also be used to classify the major inorganic filler/extenders and pigments, estimate the size of the inorganic component and observe its distribution in a sample.\nThis talk will provide an introduction to the principles and practice of CL with a specific focus on the visual and spectroscopic information that can be obtained from geological and anthropogenic samples and the applicability and limitations of CL in cases of comparison, authentication, and geographic sourcing.']"	['<urn:uuid:e22c76e6-17e4-4953-950f-5199fa796b9f>', '<urn:uuid:d2fee497-0e2d-4c7b-97c1-8eb95469b801>']	open-ended	direct	verbose-and-natural	similar-to-document	three-doc	novice	2025-05-12T20:46:45.703947	14	115	1762
57	red cross blue shield organization difference	The Red Cross operates as part of the world's largest humanitarian network focusing on alleviating human suffering and protecting life and health, while the Blue Shield is specifically dedicated to protecting cultural sites during armed conflicts, serving as the cultural property equivalent of the Red Cross.	['By: R. Carter Parét*\nDistinguished Professor Patty Gerstenblith, of DePaul University, delivered a comprehensive and exciting opening keynote address to a packed house of mostly practitioners, students, and American Society of International Law (“ASIL”) members at ASIL’s Tiller House. Prof. Gerstenblith discussed the relationship between looting and armed conflict, specifically the legal safeguards in place to address the issue.\nFirst, Prof. Gerstenblith outlined the current international legal framework protecting cultural property during armed conflict. Currently cultural property is protected by a duality of regimes, first under the 1954 Hague Convention, and second, the four Geneva Conventions applicable in armed conflicts. The 1954 Hague Convention creates an obligation on states parties to safeguard cultural property within their borders against the foreseeable effects of an armed conflict. Moreover, the Convention prohibits the export of cultural property from occupied territories. Another important outcome of the Convention is the creation of the International Committee of the Blue Shield. Prof. Gerstenblith noted that the Blue Shield is the cultural property equivalent of the Red Cross and marks protected cultural sites in armed conflict areas.\nSecond, Prof. Gerstenblith discussed the problem of looting during armed conflict, commenting that it is carried out on a large scale for economic gain. But the goal of the 1954 Hague Convention and other international agreements is to reduce the economic incentive to loot. Prof. Gerstenblith walked the audience through case studies of looted sites across Peru, Turkey, the U.S., Iraq, Syria, and Egypt. In addition to destroying cultural heritage and sites of significance, looting removes an item out of its context, making it hard to determine the authentication, history, and significance of the looted item. However, Prof. Gerstenblith noted a positive development in the law on two fronts. First, countries are beginning to pass foreign national ownership laws, whereby the host state owns all the items still in the ground. Moreover, foreign national ownership laws make it a crime to remove cultural property or archaeological items from a site without permission. Second, the 1970 UNESCO Convention on the Means of Prohibiting and Preventing the Illicit Import, Export and Transfer of Ownership of Cultural Property prohibits the import, export, or transfer of ownership of cultural property. Additionally, the 1970 UNESCO Convention requires state parties to take necessary measures to prevent museums from acquiring cultural property that was illegally exported.\nThird, Prof. Gerstenblith succinctly demonstrated the greater issue at hand, using the conflict in Syria as an example. Five out of the six World Heritage Sites in Syria have been damaged as a result of the conflict itself, and twelve sites on the Tentative World Heritage List have been subject to looting. Using before and after images, Prof. Gerstenblith demonstrated the true problem these sites face. The convergence of poverty, a failing state, and armed conflict provide the “perfect storm” for looting.\nUltimately, Prof. Gerstenblith left the audience with four parting thoughts on current and proposed efforts to improve the protection of cultural property. First, the International Criminal Tribunal for the Former Yugoslavia has recognized that the destruction of cultural property goes together with human killing. Second, military necessity under international humanitarian law allows for too much destruction, and must be reworked. Third, protections against looting in armed conflict, enumerated in the 1954 Hague Convention and the 1970 UNESCO Convention, should be unified under one treaty. Finally, the protection of cultural property should be a human and civil right under international human rights law.\n*The author is the Deputy Symposium Editor for Volume 30 of the American University International Law Review writing as a part of our series recapping our February 2015 Symposium: Protecting Art and Cultural Property Through International Law at ASIL', 'The Norwegian Red Cross is part of the Red Cross Red Crescent (RCRC) Movement, the world’s largest humanitarian network with millions of volunteers and staff operating in 190 countries. Our mission is to alleviate human suffering, protect life and health, and uphold human dignity especially during armed conflicts and other emergencies. To achieve this, we follow the seven humanitarian principles; Humanity, Impartiality, Neutrality, Independence, Voluntary service, Unity and Universality. **\nIn Afghanistan we support Afghan Red Crescent Society (ARCS) to respond to the humanitarian needs. The main priority for Norwegian Red Cross in Afghanistan is to enhance the provision of sustainable quality primary health care for maternal, new-born and child health including mental health and disability services in areas where other entities struggle to deliver such services. Norwegian Red Cross also supports hospitals providing treatment of injuries and the establishment of capacitated mobile surgical teams.\nThe purpose for this position is to address the issue of violence against patients, health workers, facilities and vehicles, and ensuring safe access to and delivery of health care in armed conflict and other emergencies. As well as the Afghan Red Crescent Society (ARCS) in defining and implementing in a transversal and coherent manner their actions in this respect.\nThe position is embedded to ICRC.\nSome responsibilities as Healthcare in Danger Delegate in Afghanistan includes:\nReporting, analysing, planning**\n· Gathers and analyses relevant HCiD information from all sources available and inform relevant stakeholders with the findings.\n· Produces plan of action\n· Monitors HCiD data reported from a variety of sources (other organisations, media, etc.).\n· Monitors and reports on achievements of objectives and proposes adaptations and solutions as required to the HCiD Steering Committee.\n· Contributes to the delegation’s strategy related HCiD.\n· Contributes to a regular exchange and dialogue with relevant international and national organizations (such as UNAMA, WHO, AIMA) to ensure humanitarian coordination and advocacy.\nCoaching and Training\n· Briefs and give technical advice to all relevant ICRC and ARCS staff on HCiD activities.\nRepresenting the ICRC\n· Develops and maintains a network of relevant interlocutors in coordination with the department coordinators\n· Represents the ICRC and/or ARCS in meetings with interlocutors relevant for HCiD activities.\nWe seek a person who.\n· adheres to company rules and procedures; execute plans with commitment and determination; achieves high quality results\n· communicates in a clear, precise, and structured way; speaks with authority and conviction; presents effectively with compelling evidence.\n· makes an impact; convinces and persuades others; promotes plans and ideas successfully\n· builds a useful network of contacts and relationships and utilizes it to achieve objectives.\n· uses a methodical and systematic approach; plans, defines clear priorities and allocates resources effectively.\n· co-operates well with others; shares knowledge, experience and information and supports others in the pursuit of team goals.\n· Relevant university master degree\n· 5+ years in humanitarian development in highly insecure contexts\n· Proven knowledge of HCiD.\n· Proven experience in coordinating multidisciplinary projects\n· Demonstrated skills in training and coaching, preferably in cross-cultural environments\n· experience in working within the Red Cross and Red Crescent Movement in the areas of Protection, Health, Communication and Cooperation is a strong asset\n· Fluent written and spoken English, – working proficiency of Farsi is an asset.\nDeputy Head of Delegation ICRC based in Dubai, UAE and Country Programme Manager Afghanistan**\nPosition is based in:\nDubai, frequent visit to Kabul, Afghanistan (TBC)\n12 months’ contract, possible with extension\nWhy should you apply?\n· Meaningful: Youll be a part of the world’s largest humanitarian organization, reaching the most vulnerable people in their local community.\n· Impact: You will help National Society in the country to become strong, self-sustained and accountable humanitarian actors.\n· Career: You will be connected to an international organization with staff based all over the world.\n· A great place to work: You will work in a multicultural environment with hard-working, dedicated and fun colleagues with a strong culture of collaboration, continual improvement, and celebration of wins.\nNorwegian Red Cross striving to build a working culture based on respect, honesty and generosity. We are looking to improve our gender balance and particularly encourage qualified women candidates to apply.\nWe do a thorough background check on relevant candidates.\nHow to apply']	['<urn:uuid:4fa0ca00-8e9e-4b83-ac95-4c5a911f18c5>', '<urn:uuid:6b61b7fa-a0c2-4d00-8bd3-0a8dce18b07c>']	factoid	direct	short-search-query	distant-from-document	comparison	novice	2025-05-12T20:46:45.703947	6	46	1320
58	When using a polisher on a car, what speeds should you use for removing serious defects versus applying paint protection?	For removing serious defects, you should use speed 5 to 5.5, while for protecting paint finish from harsh environmental elements, you should use a much lower speed of 2.5. This significant difference in speed settings reflects the different levels of aggressiveness needed for each task.	"['You’ve spent plenty of money on the purchase of your car or truck, now the question arises, ""How do I take care of it?”. The answer is vital to retaining the value of your vehicle and reducing future costs.\nThis article will help walk through the Pinnacle Complete Paint Care System of washing, claying, polishing, protecting and the final maintaining processes step by step. Don’t worry, it’s pretty easy and you don’t need to be a detailer to do it yourself!\nWASH – CLEAN – POLISH – PROTECT – MAINTAIN\n- Wash – Thoroughly wash your car to remove loose dirt.\n- Clean – Clean your paint with detailing clay to remove bonded surface contamination that is stuck to the paint and is not removed with normal washing.\n- Polish – Remove defects in your paint, staining and enhance overall shine.\n- Protect – Protect the paint surface from harsh environmental elements and prevent premature aging with a wax or paint sealant.\n- Maintain – Maintain your paint with normal washing and quick detailing in between washing.\nSTEP 1 - WASHING YOUR CAR\nThe car washing process can range from those who religiously wash or quick detail their vehicles daily, to those who love caring for their vehicle but only get the chance to wash them weekly or every other week. Both categories of enthusiasts cringe at the very thought of taking their “baby” to a car wash, otherwise known as “automatic swirl factories”!\nThe car washing process is the number one cause of swirl marks! This can be avoided with quality car wash mitts, car wash shampoos, microfiber drying towels and a few other important tools as well as a little education. By just following the correct process you will have the know how to wash your car in a manner that gets it clean and reduces the chances of your instilling swirls in your paint.\n- Always wash your car in the shade! Why? Well simple, you don’t want the water, be it the rinse water or the car wash solution, drying on the vehicle. Remember, we are trying to make your car look better than when you started! On that note, if at all possible, try to wash your car on a relatively calm day. You would be shocked at how even a slight breeze can put particulates and dirt into the air. These dust particles are attracted to the wash/rinse water on your vehicle and act like sandpaper when rubbed between the drying towel and vehicle surface. The breeze will also tend to dry your car for you and cause streaks and water spots.\n- Use two buckets with a Gritguards in each bucket. One is for your car wash shampoo solution and the other is clean rinse water for your wash mitt.\n- Fill a 5-gallon bucket 2/3 full with cool water. Add recommended amount of car wash shampoo to the water. Using a stiff jet of water, fill bucket to fully activate suds. Fill the second bucket with clean, water for rinsing your mitt.\n- Thoroughly rinse the vehicle’s surface to remove the accumulated loose dirt and grime before you begin your car wash procedure.\n- Only use high quality car shampoos and wash mitts.\n- When washing your vehicle remember to always go from high to low, never the opposite. Generally the dirtiest parts of your vehicle are those parts closest to the ground. Wheel wells, rocker panels and bumpers tend to have the largest accumulation of grunge and you don’t want to bring that junk back onto the paint surface and risk scratching it. A trick that I use here is to load up a wash mitt with my car wash solution and wring it out over the wash surface. This acts like a “pre-soak” and helps loosen surface gunk It also adds more sudsy water to my surface.\n- Keep your wash areas small and rinse your mitt frequently. Don’t try to wash the car in one fell swoop. (Unless it’s a Smart Car of course.)\n- Wash in straight, overlapping lines as opposed to circles and remember to be gentle with the wash mitt on the first pass.. This first pass picks up the initial junk, and loosens the remaining dirt and gunk for pick up on the second pass.\n- Rinse your vehicle by opening up your hose nozzle or removing the nozzle if you don’t have an open flow option and letting water run or sheet over the surface instead of spraying. This helps in two ways. First, generally when water has “sheeted” off of a surface it tends to leave less residual droplets behind. This means less to drying time. Second, since there is less water to pick up, there is less chance of your leaving water behind to “spot” the vehicle.\n- When you dry your car, use only high quality microfiber waffle weave drying towels like a Cobra Guzzler Waffle Weave Drying Towel and wring out often. Gently dry the surface without much pressure. You don’t need to rub the surface…you’re just picking up the bulk of the water! Once you have picked up the bulk of the water, and the first towel is saturated, use a second towel to lightly pick up any remaining streaks or droplets.\n- Lastly, if your wax is still in good shape, go over the vehicle with a quick detail spray like the Ultima Detail Spray Plus and a plush Edgeless Microfiber Quick Detail Towel to restore a radiance and slickness to the paint. This last step will give your vehicle that “just detailed” look by enhancing gloss will also remove any sneaky water spots or streaks that showed up when you weren’t looking.\nSTEP 2 - CLAYING YOUR CAR\nBLACKFIRE Poly Clay Bar quickly and easily removes most forms of paint contamination that is not removed with your normal car wash process. Pinnacle Ultra Poly Clay will remove contaminants like insects, tar, stubborn road grime, exhaust soot, oxidation, pollen, light paint overspray, factory fallout and more! Detail clay can even take care of stubborn brake dust contamination.\nHow do you know if detail clay is what you need? After a good wash, feel the paint\'s surface. Rough spots and bumps mean contaminants. The BLACKFIRE Poly Clay Bar will clean and smooth your car\'s paint and get that bonded grime out of the way so you are ready to polish your paint!\nIMPORTANT NOTE: DO NOT use detailing clay on a dirty paint surface. Always wash your vehicle before claying.\n- Use Pinnacle Ultra Poly Clay only after thoroughly washing your car to remove loose dirt and grime.\n- Flatten half of a bar of Pinnacle Ultra Poly Clay and mold into a flat clay wafer.\n- Lightly mist the clay bar and a 2\' by 2\' area of the surface to be cleaned with the Pinnacle Clay Lubricant.\n- Gently glide the wafer of Pinnacle Ultra Poly Clay on the surface in a back and forth motion until contamination is gone.\n- Fold the clay bar in half and re-flatten frequently to expose a fresh cleaning surface.\nIMPORTANT! Discard clay if dropped or when a clean surface can no longer be exposed.\nNOTE: This clay is soft and should not be used on hot surfaces or when the air temperature is above 84 degrees. In extreme summer weather, use during early morning hours. High humidity conditions may make the bar sticky and difficult to use.\nSTEP 3 - POLISHING YOUR CAR\'S PAINT\nYou know how you want your vehicle to look but you must also take in to consideration the time involved, tools needed and, if the damage is severe, your own skill level.\nHere are a few things you need to think about before you start your surface evaluation process.\n- Products and Tools Available\n- Time Available\n- Work Space Including Lighting\n- Weather Conditions\n- Skill Level\n- Your Expectations\nNow that we know what we have to work with, let’s evaluate the surface of your paint to determine our plan of attack.\nIndividual Paint Defects\n- Overall Paint Condition\nIs your paint new or like new, in need of light polishing for gloss enhancement, requiring minor defect removal or is your paint neglected and needing heavy defect removal?\n- Paint Hardness\nThis is difficult to tell until you start polishing your car. Some paints are harder than others making defect removal more difficult so more aggressive products would be needed.\n- Paint Thickness\nThis is difficult to tell unless you have a paint thickness gage. Without a gage, what you need to look for is thin spots or any areas that may have been burnt through in the past by inexperienced detailers with the wrong type of buffer. If you find a thin area, stay away from that area with your polisher. You can\'t polish out thin paint, it just wears it away.\n- Isolated scratches\n- Paint etching\n- Water spots\n- Paint staining\n- Clarity of overall surface\nNow that we know what needs to be corrected we need to choose the proper polish and polishing pad to correct the defects in your paint’s surface and achieve desired results.\nPinnacle Natural Brilliance Polishing System:\nSTEP 1- Remove serious defects\nThe Yellow Light Cutting Lake Country CCS Foam Pad is the first step in serious defect removal such as random isolated scratches, heavy swirl, severe oxidation, stubborn water etching and other difficult to remove paint defects.\nFor the worst defects, use Pinnacle Advanced Compound\n, which is a true polishing compound.\nIf the defects are not as serious, or you\'re not sure, start with the Orange Light Cutting Lake Country CCS Foam Pad and Pinnacle Advanced Swirl Remover. This combination works on a multitude of paint imperfections.\nSTEP 2- Remove light swirls and minor defects\nThe White Polishing Lake Country CCS Foam Pad is the second step for minor defect removal including swirl, fine isolated scratches, water spots, mild water etching, light oxidation and other less severe paint finish defects.\nUse the White Polishing Lake Country CCS Foam Pad for safe and complete removal of medium to light common defects with Pinnacle Advanced Finishing Polish.\nSTEP 3- Pre wax cleaning and gloss enhancement\nThe Green Polishing/Finishing Lake Country CCS Foam Pad is your third step in paint perfection and is used to clean your paint and enhance gloss so you can achieve that desired deep, wet looking shine.\nUse the Green Polishing/Finishing Lake Country CCS Foam Pad for your final gloss enhancement as well as pre-wax cleaning to prep your surface for proper bonding of car wax, paint sealant or protectant with the Pinnacle Paintwork Cleansing Lotion.\nRemember to never work in direct sunlight or on a hot surface. Good lighting is also very important so you are able to evaluate your work during the polishing process.\n- Thoroughly wash and dry your car.\n- Use a detailing clay to remove bonded surface contamination that is in the way of polishing your paint.\n- Choose and about a 2\' by 2\' area on the surface to begin your process. Usually the worst area is your best choice.\n- With painter’s tape, tape off any sharp edges and trim to avoid staining trim, build up in cracks and crevices and pad damage.\n- Select the proper polish and polishing pad based on your surface evaluation to start your process.\n- Adjust your speed setting according to the “Recommended Speed Setting Guide” below.\n- Place the pad on the surface of the paint.\n- Turn on the power switch. WARNING! Never lift the polisher off the surface of your paint while it is turned on!\n- When removing any type of defects apply medium pressure while polishing and move in SLOW overlapping motions according to the “Recommended Paint Polishing Pattern Guide” below.\n- When the polish is worked in well it will start to turn clear. At this point turn off the machine and lift it off the surface of the paint.\n- With a clean, quality microfiber towel, buff the remaining product residue off the surface of your paint.\n- In good lighting, evaluate your work. If it looks like you need to work on the defects a little more, add a few more pea size dabs of polish and repeat your process until your evaluation revels success. (Keep in mind when using more aggressive products you need to work your way down in aggressiveness to achieve your best shine.) Don’t forget, this is a four-step process but you may not need all for steps.\n- Working in sections that are no larger than 2’ by 2’ to concentrate your work repeat the necessary polishing processes over the entire surface of your car until desired results are achieved. Remember areas with more serious defects will require more polishing time so be patient and allow the polisher to do it\'s job.\n- Using your final wax pad, apply your favorite car wax, paint sealant or paint protectant and allow to cure.\n- Remove remaining residue with a clean, quality microfiber towel or buff off using your microfiber bonnets to reveal a brilliant shine!\n- Maintain your shine by quick detailing when lightly dusty and regular washing as needed with a quality car wash shampoo, wash mitt and microfiber drying towel.\nProper machine polishing is as easy as following this simple pattern and concentrating your work in a 2\' by 2\' section at a time. Master this and you are on your way to a brilliant shine!\nRecommended Paint Polishing Pattern\nRecommended Speed Setting Guide\n- STEP 1- Remove Serious Defects – 5 to 5.5\n- STEP 2- Remove Swirl and Minor Defects – 4.5 to 5\n- STEP 3- Pre Wax Cleaning and Gloss Enhancement – 4.5\n- STEP 4- Protect your paint finish from harsh environmental elements – 2.5\n- Product Residue Removal – Microfiber Buffing Bonnet - 3.5 to 4\nNote: Not all paint finishes require the entire four-step process. Complete only the steps necessary for the condition or your paint finish.\nSTEP 4 - PROTECTING YOUR CAR\'S PAINT\nFirst of all let\'s start with the confusion many have with the difference between waxes, protectants and sealants. Basically they all fall into the same category and while some do a much better job than others they all do the same thing and that is to enhance gloss and protect your finish. When it comes down to a traditional wax like your father or grandfather used vs. a newer synthetic protectant or sealant the newer technology has won. While the traditional wax does protect and enhance gloss the newer protectants and sealants do a much better job and last much longer. What it comes down to is man perfected the wax!\nSTEP 4- Pinnacle Liquid Souveran Wax – Protect your paint finish from harsh environmental elements\nThe charcoal Gray Finishing Lake Country CCS Foam Pad is the final step to a flawless finish and is used to safely apply your car wax, paint sealant or protectant.\nUse the Gray Finishing Lake Country CCS Foam Pad for thin, even application of Pinnacle Liquid Souveran Wax.\nRemember to never work in direct sunlight or on a hot surface and make sure your surface is clean and dry.\nSTEP 5 - MAINTAINING YOUR CAR\'S PAINT\nWashing you car often can be very time consuming. For those times when your car is just a little dusty, Pinnacle Souveran Liquid Spray Wax is the solution!\n- Water spots & streaks\n- Environmental paint damage\n- Dust build up on paint', 'How To Use A Car Polisher\nOnce you have been able to pick the right car polisher machine, you need to know how to use a car polisher. It does not matter whether you want to use the hand polisher or the machine polisher, you must know how to use either of the alternatives effectively.\nIf you do not know how to use a polisher, chances are high that in the process of trying to give your car an awesome detail job, you might actually do some damage to it. On the other hand, you can also choose to get a professional polisher to do the work for you, but this could set you back financially.\nWe can actually hit two birds with the same stone by learning how to use a car polisher. When you do this, you will be able to do the polishing on your own properly, and at the same time you will save on money that you would have otherwise had to pay someone to do the work for you.\nHand or machine polishing?\nFrom time to time we are asked which one is the better between hand polishing or machine polishing. Of course the most obvious answer is that you are better off polishing by machine. It is faster, and delivers the result you want, without much effort.\nBesides, try as hard as we want, our hands are simply incapable of delivering the same results that we can achieve from a machine. Whether you want to compare paint polishing or maintaining the finish, the machine will always win hands down.\nFact: It is impossible for any human to oscillate their hands over the car, at a rate of between 2500 and 6800 oscillations per minute, but a machine can.\nSafety measures on how to use a polisher\nOne of the other questions that we come across from time to time is about the safety standards that are associated with machine polishing. Of course even if you pick the right car polisher machine, if you do not know what to do with it, or how to use it, it will always be a dangerous experience both for yourself and your car.\nThe first thing that you need to do is make sure that you get the right type of polisher for your car. Ideally, there are three different types of polishers that you can get in the market:\nWe recommend this machine for use only by professionals, or someone who has some experience in polishing. It is also a machine that comes in handy when your vehicle has severe paint damage to be corrected.\nThis particular polisher has a very high spin rate, and if used by an inexperienced person, a lot of damage could be imminent. Other than that, it could also burn through the paint on your car, leaving you with more repair expenses than maintenance.\nThese machines are easily affordable, and you can get them for as low as $50 from the local auto spare store. They are designed with a cloth bonnet, and in as far as the accessories are concerned, there is not much that you have to choose from.\nThese machines are relatively effective for applying wax on your car, but when it comes to polishing or removing defects, they actually offer little assistance. In fact, most of them just jiggle on the surface of your car, even though they are labelled as variable speed or random orbit – this is actually the perfect example of getting what you paid for.\nThe random orbit, variable speed dual-action polisher is in fact one of the options that we would recommend. With this polisher, you are able to get rid of the following imperfections you’re your car:\n- Isolated scratches\n- Acid rain etching\n- Water spots\n- Damage from bird droppings\nThis is more than just a polisher, it is also a really good maintenance tool when you need gloss enhancement polishing, pre-wax cleaning or if you need to apply thin coats of car wax, protectant or paint sealant on your car.\nApart from being highly effective, it is also one of the safest tools that you can get for car polishing so far.\nFor professional detailers, perhaps the portable cable car polisher would be an ideal weapon in your arsenal. Even beginners can work with this particular polisher.\nTechniques on how to use a car polisher\nBefore we begin, perhaps we need to assure you that this is an easy and safe process. You just need to pay attention, and follow the instructions to the latter and you will be good to go. Even in the event that you have never used any polisher before, you can easily pick the right car polisher machine and learn how to work with it over a few trial sessions.\nIn terms of technique, there is very little you will need to worry about, because so far, most people learn their way around in under 10 minutes. All you need to do is use overlapping motion while running the polisher over the paint on your car.\nRemember to do this at a slow speed first, then adjust as you get accustomed to the job.\nThe following is a basic guide that should help you set the right speed when you are using a polisher for the first time, or even if you are a repeat user:\n- Speed 5 – 5.5: Removing serious defects on the body\n- Speed 4.5 – 5: Removing minor defects and swirl\n- Speed 4.5: Gloss enhancement and pre-wax cleaning\n- Speed 2.5: Paint protection from harsh environmental elements\n- Speed 3.5 – 4: Removing product residue\nThe speed setting that we have shown you above, is also the procedure that you need to follow when using a polisher. However, you will not necessarily need to use all the 5 steps for all paint finishes. Just use the steps that are appropriate for the specific condition you need to finish for your vehicle.\nThe following are some useful pointers that you need to have at the back of your mind when you are still learning to polish your car, especially if you are using a dual-action polisher:\n- Always use a test spot – This will help you know whether the process you are planning to use, and the technique will deliver the results that you desire, before you do the same for the rest of the car.\n- Make sure you consider the paint condition of your car, because this is supposed to guide your choice of polish, polishing pad and speed combination\n- At all times, make sure the pad is level to the paint surface\n- We recommend starting with the least aggressive polish, then if need be, increase the aggression\n- While polishing, always use overlapping motion for the best results. While you are at it, try not to run the machine over the paint surface too fast. This will definitely not get you the outcome you desire.\n- In order to concentrate better on the job and get good results, work on a small area at a time.\n- Using too much polish will not get you good results. To be precise, try and use roughly 3 or 4 small dabs, and if necessary, add more.\n- You should never dry buff your car. If you realize that your polish is thin, or it is starting to become clear, add some more.\n- Experts advise car owners never to buff their car on a hot or warm surface, or under direct sunlight\n- While detailing your car, be sure not to use more than one type of polish on the polishing pad\n- The pads should always be kept as far away from sharp and trim edges as possible\n- This is particularly for the beginners; you should never lift up the pad from the paint when you still have the machine turned on.\n- While working on the body, make sure that you have the cord away from the paint, and over your shoulder at all times.\nJust because you are able to pick the right car polisher machine does not always mean that you will know what to do with it. There are so many people who have the best polishers in the market, but are still unable to get the fine shine that they desire. It all comes down to learning how to use the polisher you have.\nWe have elaborated the three major categories of polishers that you can come across in the market. From here, you can make an informed choice on what you need, especially by considering what you are willing to comfortably spend, and what will work for your car.\nWe have also shown you 5 of the steps that you need to follow when polishing your car, including the speed settings that you can use. Remember that this might not always be the same for all polishers, since they come from different manufacturers. Also, through constant improvement, the speed settings might not always stay the same.\nFor this reason, you will be better off checking the manual that comes with the polisher you own, and use it effectively.']"	['<urn:uuid:3f404b22-e166-4c51-9698-262f8e7ac679>', '<urn:uuid:46803ea3-db3b-4c24-893c-70a0c23cfefb>']	factoid	direct	verbose-and-natural	similar-to-document	comparison	novice	2025-05-12T20:46:45.703947	20	45	4144
59	How do cremation and embalming differ in terms of legal requirements across different states in the United States?	Cremation laws vary by state and are governed by the Federal Trade Commission, with each state having specific procedures for identifying and tracking remains. For embalming, it is rarely legally required - only Alabama and Alaska mandate it when bodies cross state lines, while California, Idaho, Kansas, Minnesota and New Jersey require it when bodies leave those states by common carrier. Both services are regulated, but embalming has far fewer legal mandates.	"['FREQUENTLY ASKED QUESTIONS:\nWhat is Cremation?\nDuring cremation, a dead body is burned and vaporized at high temperatures, leaving only ashes. The ashes are specifically from the destruction of bones. Other tissues are vaporized.\nAre there Laws in place to insure you receive only the ashes of your loved one?\nYes, there are State by State cremation laws governed by the Federal Trade Commission and In general they\'re controlled by each state. Common practice is that strict identification of the deceased is initiated at the place of death. By normal procedure an ankle ID band is placed around the ankle that includes several points of information including name, birth date, date of death and location to insure the correct ID of the body. The ID band will be checked several times by the funeral home and the cremetory during the process. At the crematory a special assigned ID on a stainless steel disk is assigned to the paperwork of the deceased and is placed with the remains of the body. The disc stays with the cremated ashes which when finished is placed in the plastic bag. The bag of cremated ashes is placed in a temporary plastic or cardboard container urn. This urn is then is given to a designated person by the family.\nHow long and where can the remains be kept?\nCremated remains are safe to be kept in the temporary urn for as long as needed, however most people wish to honor their deceased loved ones with a more suitable permanent container called an urn. Urns, are sometimes called vessels for holding cremains/ashes are made of many different materials. It is up to each family to choose what they wish for their deceased loved one.\nAfter cremation what do we do with the cremains?\nThere are 6 major ways that cremains can be handled. There is water burial, scattering of ashes where allowed, use of ashes in plants and trees, ground burial and display at home or niches in columbariums. Many people are turnng to biodegradable materials often called echo urns that are friendly with nature. After you have made a decision according to your family, religious beleifs or in honoring the wishes of the deceased, you can choose the most fitting urn to fill your needs. Please be sure that you understand each States\' laws that govern how all these methods are handled and the best way to find out may be to consult with a local funeral director in your area. In some states in order to keep cremains at a home a designated person must be chosen who will carefully handle the cremains/ashes after you have passed on. So each step needs to be given thought about and within the letter of the law for the state of burial (interment). The office of Secretary of State can refer you to the laws that govern that state.\nWhat is pre-planning?\nMany people have made their wishes known by pre-planning how they want their final wishes followed. The advantages of this are many.\n1. It alleviates undue stress for family members at the time of your passing. When a loved one dies, grieving family members and friends are confronted with dozens of decisions about the funeral - all of which must be made quickly and often under great emotional duress. This can avoid emotional overspending as well. With all of the little details made ahead of time you have more control to have things the way you feel comfortable having them happen. It also gives you control over how much money will be spent. No one in your family needs to feel any quilt by following your final wishes. You can make the best choices for yourself.\n2. Shop in advance. It allows you to comparison shop without time duress. It creates an opportunity for family discussion and lifts some of the burden from your family. Decisions can be made with a clear head.\n3. Even though you might choose cremation over traditional burial, you can still have all that a traditional funeral entails or you can choose to have a plainer service if you wish. People associate cremation with a cheaper way of taking care of a body, but some think of it as being an alternative way that offers a lot of options to families. Whatever you decide IS the right way for you. It is not necessary to have the fanciest casket or urn or the most elaborate funeral to properly honor a loved one.\n4. If you find the urn you feel is just right for yourself or your loved one, remember it may not be available in a few months or years, so be wise and buy it as it can be easily store away for your future need. Styles keep changing. Some classic standard urns will always be around. This might give you time to have a custom urn made as well. Planning ahead can eliminate expedited costs of rushing to purchase an urn under a time limit. Saving money is always smart.\n5. We suggest once you have made your decisions about how you want things to be handled, put them in writing or some type of accessible digital data storage and have a close family member or close friend know where to find the writing or data.\nHow can I find out if scattering of ashes is legal or how to go about water or burial at sea?\nAgain, please contact your local funeral home as they often deal with issues like these. There are professionally regulated businesses that can perform these services for a fee. Most states allow scattering of ashes, but be sure you know what the restrictions are and to contact the city or state government for answers. Most states require permits, especially on state and national parks property or water. Some have set aside special areas for scattering and special conditions to follow. LovedOnesUrns strongly encourages you to know the laws and follow them at all times. Scattering ashes on private property is only allowed by the permission of the property owner. A few states have no scattering of ashes policies. Again, please become familiar with your states rules or the state where interment will take place.\nIs a Casket required for Cremation?\nEach state has laws about containers used in the cremation process. Most states will require the body to be cremated in an approved container. A container can be a strong cardboard container to a full wooden casket. The final decision is up to the family except when the deceased has prior plans which should be honored if at all possible.\nDo I have to purchase an Urn from a funeral home?\nNo, you are not required to purchase an Urn or Casket from the funeral home under the Federal Rules & Regulations and Laws in the U.S. According to the ""Funeral Rule"" a funeral home cannot require you to purchase a casket when you have ordered cremation services. You are allowed to buy an urn elsewhere like online and have it shipped to the funeral home directly, or bring the urn in yourself. The funeral home is not permitted to refuse to handle the urn you purchased or require you to be present when it arrives or you deliver it to the funeral home. If you purchase cremation jewelry online or through the funeral home, the funeral home will usually handle the cremains for you for a fee. Call the funeral home for more information. Some people don\'t want to handle the cremains and this is a way around it, or ask a friend or other family member.\nHow do I transport the urn or cremains/ashes back to a different state, city for interment?\nSince we live in a time when families are so spread out across the country, it may require transporting the cremains to another state, city or area for final disposition. For example, which creation urns are suitable for airline travel? LovedOnesUrns has done some research into this issue to determine current and correct information. According to the Transportation TSA (Transport Security Administration) who addressed this issue to the International Cemetery, Cremation, and Funeral Association, who stated that the ""TSA recommends that consumers purchase a temporary container made of plastic, wood, or a non-lead lined ceramic urn for transporting your loved oned remains"" so as to pass through the TSA\'s security screenings. Generally most airlines will allow you to bring a cremation urn of this type as carry-on luggage, since it like you passed though security screening. Beware that not all airlines allow cremated remains as checked luggage. It is necessary to check with the air carrier BEFORE attempting to transport a crematory container in checked luggage. Out of respect for the deceased and family, airport security personnel will NEVER OPEN the cremation container even if the family insists. Transport restrictions then could result in the final interment not to happen on schedule. To avoid this from happening to your family, plan to use an urn which will pass through the airline x-ray screening. Cremated remains can be shipped by land and air shipping service after the following of special guidelines and labeling of cremated remains. The urn can be shipped in this manner and you can be reunited with them at your destination. You could purchase an urn at point of destination or purchase an urn and have it shipped to the same destination. LovedOnesUrns has a special section with approved temporary or permanent ashes containers made of an approved material such as fabric, plastic or wood that can successfully be x-rayed. By answering this question we hope to save you some inconvenience in the future. We don\'t want this to be a an extra stressful time for your family.\nAre there rules for shipping Urns?\nYes, the common commercial carriers will have set rules they must follow. As an example the U.S. Postal Service requires that the urn be shift-proof, tightly closed and padded. So when you purchase an urn that will have to be transported keep these factors in mind. Often the urn is shipped to you in that type of package, so try to keep the original shipping package and packing or use one like it.\nWhat size Urn should I get?\nThis is the question that is most often asked. Just as people are different sizes and shapes, so do urns vary in sizes and shapes. It is important to remember to always be concerned not with the external size of the urn but the internal volume it can accommodate. The volume is listed in cubic inch capacity - cubic inch is the volume of one pound of bone fragments/ashes takes up. The industry published by the Federal Rules, Regulations and Laws in the U.S. states Standard Urn sizing established by the Cremation Association of North America is defined by internal capacity to hold ashes, which is measured in cubic inches. The industry standard is 200 cubic inches for an adult. This size will accommodate most adult urns. However not all Urns are made to hold this volume. As you will imagine urns meant for children or to be used as keepsakes are smaller in capacity. A rule of thumb is 1 cubic inch of volume for each pound of healthy live weight. So if a person weighed 180 lbs they would require roughly 180 cubic inch capacity urn. Remember when sizing an urn it is better to purchase one big enough rather than it being too small. There are extra large urns made to hold a larger than normal capacity. There are companion urns meant to hold the cremains of two people. Since most of the body is vaporized, we are now dealing with bone fragments that remain in the cremation process. Of course these have been processed into smaller pieces and collected into a plastic bag. That bag can usually be put into the urn intact in most cases. So the size of your urn and type of urn is determined by how you intend to use it. Often cemeteries usually set their own standards for the type of urn they will accept for a columbarium or urn vault. So it is wise to check with the cemetery of interment to look into their rules BEFORE purchasing an urn or vault.\nIf land burial is chosen, will I be required to purchase a vault to put the urn into?\nAgain, the answer is up to the cemetery involved. By dealing with a local funeral director, they will be familiar with the rules of most common cemeteries they use or know how to find the information for you. You may also contact the cemeteries directly if need be. Vaults for urns are made to encase the urn and protect it from moisture and the weight of the earth above and around the urn. It helps in ground settlement in the future for years to come as well. Some cemeteries will not require a vault and often stone, marble or granite urns will work very well without the vault. So, again this answer often varies. LovedOnesUrns has included Burial Vaults for Urns in our Burial Vaults section to make this step easier for you. You may have them shipped to you directly, or to another location. They could be shipped directly to the cemetery of interment. Be very careful when choosing a vault with the correct internal dimensions for the urn you are using. LovedOnesUrns is here for you.\nIf I have a memorial service before I have had time to purchase the permanent urn? How can this be handled?\nOne solution would be to rent a suitable urn for the service from the funeral home and later when time allows, find a more suitable urn. Most urns can be purchased in an expedited manner and shipped very fast at additional costs.\nAn Added Note:\nYou are going through a sensitive time in your life and deserve all the comfort and caring we can offer you. We want everything to go as smoothly as possible for you to ease your burden at this time. Because we care about people, we are here for you as well as to create lasting memories for your lost loved one.', 'What is embalming?\nEmbalming is a physically invasive process, in which special devices are implanted and embalming fluids are injected into the body to temporarily slow its decomposition. The goal is to preserve the body to make it suitable for public display at a funeral, for long-distance transportation, or for medical or scientific purposes such as anatomical research. It also gives the body what some consider a more “life-like” appearance, which some families want for a public viewing.\nHow prevalent is embalming?\nEmbalming is common only in the US and Canada. Many families consider it to be an essential part of a traditional funeral and burial arrangements, and few question whether it is necessary, or what is involved in the process. It is important to remember that this perception was created by the funeral industry for commercial purposes. Most funeral directors will not arrange the public viewing of a body without embalming and cosmetic restoration.\nDo any religions forbid it?\nThough embalming has no roots in Christian religion, it is neither discouraged nor encouraged. Muslim, Bahá’í and orthodox Jewish faiths consider embalming to be a desecration of the body, and prohibit it. Hindus and Buddhists choosing cremation have no need for embalming.\nWhen is embalming required?\nEmbalming is rarely required by law. In fact, the Federal Trade Commission and many state regulators require that funeral directors inform consumers that embalming is not required except in certain special cases. Embalming is mandated when a body crosses state lines from Alabama and Alaska. Five other states—California, Idaho, Kansas, Minnesota and New Jersey—require embalming when the body leaves those states by common carrier (airplane or train).\nDoes embalming protect community health?\nEmbalming provides no public health benefit, according to the U.S. Centers for Disease Control and Canadian health authorities. Hawaii and Ontario forbid embalming if the person died of certain contagious diseases. Many morticians have been taught, however, that embalming protects the public health, and they continue to perpetuate this myth.\nIn fact, embalming chemicals are highly toxic. Embalmers are required by OSHA to wear a respirator and full-body covering while embalming.\nHow well does it preserve the body?\nEmbalming does not preserve the human body forever; it merely delays the inevitable and natural consequences of death. The rate of decomposition will vary, depending on the strength of the chemicals and methods used, and the humidity and temperature of the final resting place.\nAmbient temperature has more effect on the decomposition process than the amount of time elapsed since death, whether or not a body has been embalmed. In a sealed casket in above-ground entombment in a warm climate, a body will decompose very rapidly.\nWhy is embalming promoted?\nThe funeral industry promotes embalming and viewing as a way to show “proper respect for the body,” and to establish the “clear identity” of the corpse so that the reality of death cannot be denied by those who view the body. Many funeral directors believe that seeing the body is a necessary part of the grieving process, even if the death was long-anticipated. While some people may be comforted by “a beautiful memory picture,” as it’s called in the trade, 32% of consumers reported that viewing was a negative experience, according to a 1990 survey.\nEmbalming also gives funeral homes an opportunity to increase consumer spending (by as much as $3,000 or more) for additional body preparation, a more expensive casket with “protective” features perhaps, a more expensive outer burial container, and a more elaborate series of ceremonies.\n“Embalming forms the foundation for the entire funeral-service structure. It is the basis for the sale of profitable merchandise, the guardian of public health, the reason for much of our professional education and our protective legislation.”\n– From an embalming textbook\nAre there alternatives to embalming?\nDirect or immediate burial, without embalming, must be offered by all funeral homes. The body is simply placed in a shroud, casket, or other container, and buried within few days, without visitation or service.\nRefrigeration can be used to maintain a body while awaiting a funeral service or when there is a delay in making arrangements. Not all funeral homes have refrigeration facilities, but most hospitals do.\nSo, private or home viewing by family members and close friends can occur without embalming and is far more “traditional” than some of the services promoted by the industry under that name.\nWhat is the embalming process?\n- Rigor mortis (stiffness) is relieved by massage. Rarely, tendons or muscles must be cut for a more natural pose if limbs are distorted by disease, e.g., arthritis.\n- Massage cream is used on the face and hands to keep the skin soft and pliable.\n- Facial features are set by putting cotton in the nose, eye caps below the eyelids, and a mouth-former in the mouth, with cotton or gauze in the throat to absorb purging fluids. The mouth is then tied shut with wire or sutures. Glue may be used on the eyelids or lips to keep them closed in an appropriate pose. Facial hair is shaved if necessary.\n- Arterial embalming is begun by injecting embalming fluid into an artery while blood is drained from a nearby vein or from the heart. The two gallons or so needed is usually a mixture of formaldehyde or other chemical and water. In the case of certain cancers, some diabetic conditions, or because of the drugs used prior to death (where body deterioration has already begun), a stronger or “waterless” solution is likely to be used for better body preservation. Chemicals are also injected by syringe into other areas of the body.\n- The second part of the embalming process is called cavity embalming. A trocar—a long, pointed, metal tube attached to a suction hose—is inserted close to the navel. The embalmer uses it to puncture the stomach, bladder, large intestines, and lungs. Gas and body fluids are withdrawn before “cavity fluid,” a stronger mix, is injected into the torso.\n- The anus and vagina may be packed with cotton or gauze to prevent seepage if necessary. A close-fitting plastic garment may also be used.\n- Incisions and holes made in the body are sewn closed or filled with trocar “buttons.” The body is washed again and dried.\n- Nails are manicured, any missing facial features are molded from wax, makeup is used on the face and hands, and head hair is styled. The body is dressed and placed in the casket, and fingers are glued together if necessary.\n“I think the elaborate, expensive display of an open casket with all the makeup in the slumber room enforces the belief that the person is only asleep, and in my personal opinion would only help to prolong the stage of denial.”\n– Elizabeth Kubler-Ross, in “Questions and Answers on Death and Dying”']"	['<urn:uuid:47c04337-996f-40e1-bd8d-10ed67ca5e15>', '<urn:uuid:c991d48e-9d40-4b1f-a8de-76fc59846a50>']	factoid	direct	verbose-and-natural	similar-to-document	comparison	novice	2025-05-12T20:46:45.703947	18	72	3518
60	Are ancestral spirits treated similarly in Kaya forests and Gikuyu tradition?	In Kaya forests, the connection to ancestors is maintained through protecting the ancestral villages and surrounding forests, which serve as biodiversity hotspots containing rare species. In Gikuyu tradition, ancestral spirits known as Ngoma dwell in Murungu (the underworld) connected to sacred fig trees, and people maintain connection through ritual libations before drinks. However, evil spirits of those who died without peace roam the earth causing misfortune.	['Modernity has gifted humanity with many good things. In the process, many practices have been blindly cast out on the pretext that they are useless or outdated. This has created a sense of loss for numerous cultures around the world, with many communities feeling that disregarding their traditions has somehow destroyed their close-knit societies. Some have realized that their ancient traditions still have value in the modern world. In Kenya, there has been a movement to revitalize ancient practices.\nAmong the Tharakan community, there used to be a practice called “Muriira.” This was a ritual that was practiced by the tribe when the community was threatened with any kind of pestilence or illness. The word “Muriira” essentially means to stop or cast away.\n“The Muriira ritual calls for people to provide and prepare sacred seeds — millet and finger millet — and also to source wild herbs for the Elders to use. These need to be prepared in specific ways over the eight days of the ritual, which is something the community members can also help with, under the instruction of the Elders,” according to Terralingua.\nUsing specific plans, the tribe members make marigi, which are small models that look like closed doors. When all ingredients of the ritual are ready, people pray sacred chants during a procession in their community under the leadership of their Elders. The Elders then dig a hole and bury the marigi on all the roads leading to the community. The practice has picked up in the current COVID-19 pandemic scenario, even with people who never used to believe in these rituals willing to donate for its success.\nKaya basically refers to the Mijikenda tribe’s ancestral villages that are surrounded by forests. Over the past few decades, many community members have moved close to the farms outside the forests for a better life. However, they remain committed to taking care of the kayas due to their ancestral links to it. There are about 45 such kayas in Kenya. Ranging from 10 hectares to 900 hectares, these kayas are biodiversity hotspots. Almost 25 percent of Kenya’s rare species is said to be in the kayas.\nActivities like mining, illegal logging, land grabbing, and agricultural encroachments are damaging the kayas, bringing great harm to Kenya’s animal and plant species. Many NGOs have been formed, which in association with the tribal Elders, strengthens the traditional kaya management practices, as well as restores destroyed forests.\nSome members are concerned by the younger generation’s disinterest in traditional practices and neglect in protecting the kayas. Poverty is also a big challenge when it comes to protecting the kayas. The economic resources provided by the kaya forests are too great an attraction for impoverished communities that it becomes tough to protect the kayas from encroachment and eventual destruction.\nA few years back, members of the Borana community used to lose a large number of cows due to frequent droughts. Given that the tribe’s income was largely derived from pastoralism, the loss of cows became a survival issue for the community. Prior to 1963, the Borana used to manage the land in “Dedha” units. Basically, a council of Elders used to decide on resource allocation, solved disputes, and maintained social order among the tribes. This ensured that the community’s use of pasture and water was within reasonable levels so as to allow the members to withstand droughts.\nAfter becoming independent in 1963, the Kenyan government refused the authority of Dedha councils. Instead, police intervention was used to manage any resource allocation dispute. As a result, farmers were allowed to let their livestock graze without any limits, causing water to deplete before the dry season even ended. Almost five decades later in 2011, the tribes decided to bring back the Dedha council system. In a few short years, pastoralists stopped complaining about dead cattle from droughts.', 'The name for God among the Gĩkũyũ is NGAI. Ngai seems from the many descriptions of him to be a man who related to the first man, Gĩkũyũ as a friend, walking with him and speaking with him. But Gĩkũyũ mythology suggests that he then retreats into the mountains and no longer communicates directry to humanity today. He however occasionally leaves his mountain abode and takes walks up and about viewing his creation. The Gĩkũyũ then say he is inspecting his wealth. (kũroora). One story states that the Karĩma Hill in Othaya, Nyeri is Ngai’s footprint as he moved between the Aberdares Range (Nyandarũa) to Mt. Kenya, (Kĩrĩnyaga) his main abode. When the Gĩkũyũ say, NGAI, NIWE MUGAI, (God is the only server), they mean literally that God, is the only source of all that they receive. Everything belongs to God and it is in God’s wisdom and benevolence that anything comes to them so his other name is MŨGAI which means the divider or distributer of spoils.\nNgai wa Kirinyaga or Ngai of Mt. Kenya is the only God of the Gikuyu even if he may be referred to by other names. The white patch on Mt. Kenya resembles the white parch of the Ostrich, Nyaga and that is why He is called Mwene Nyaga or “Possessor of the white patch.” He is described by His qualities, as Possessor of Whiteness and showing that He is All-pure. When sacrificing a goat to him it must be without blemish or mark. This is why shoes are removed when approaching Him.\nMũrungu in Gĩkũyũ means literally, “DWELLER OF THE UNDERWORLD”. In Gĩkũyũ mythology there dwells a people under the earth and they are the ones who gave the Gĩkũyũ fire. In this underworld which is somehow connected to the roots of the sacred fig tree, the Mũgumo dwells the spirits of the ancestors and God, MŨRUNGU, DWELLER OF THE UNDERWORLD” visits it occasionally.\nAs already mentioned, Mũgai literary means THE DIVIDER, or THE ONE WHO SERVES, or giver of all; therefore possessor of all.\nNGAI WA GĨKŨYŨ NA MŨMBI\nAccording to Gĩkũyũ mythology, the first man Gĩkũyũ walked with God. God then took him to the top of Mt Kenya and there showed him all the land of the Gĩkũyũ. He then directed the man to a grove of trees near present day Muranga and told him to go and establish his homestead there. This is the Gĩkũyũ sacred site of origin, Mũkũrwe wa Nyagathanga. On arrival he met a woman, Mũmbi the two became man and wife. (Full Story). Gĩkũyũ and Mũmbi then were with this Ngai at the beginning of Gĩkũyũ time. It is not clear from the available Gĩkũyũ creation myths how the earth and everything else was created prior to that. What we do know is that Mũmbi the woman’s name means, CREATOR or POTTER. Whether the woman had a hand in creation or she merely made pots is also not clear. Since the man Gĩkũyũ, found her already waiting for him at the sacred grove Mũkũrwe wa Nyagathanga, she may have pre-existed in the form of the earth itself. NGAI WA GĨKŨYŨ NA MŨMBI or God of Gĩkũyũ and Mũmbi is this God in Gĩkũyũ mythology.\nPicture source: http://www.yongchen.com/ch_apple_mount_kenya.htm\nThese are the traditional names for God in Gĩkũyũ and any other names, eg. MWATHANI – Lord, MWENE HINYA WOTHE – God almighty, MŨTŨŨRA MWOYO – Eternal God, MŨTEITHĨRĨRIA – Helper, etc. etc. may have been later derivations from Christianity.\nTHE WORD “NGOMA”\nThe word NGOMA meaning SPIRIT or SPIRITS is not to be confused with NGAI, God. Spirits dwell among men and seem to be the connection between the living and the dead. The Gĩkũyũ believe that peaceful dead live in the realm of MŨRUNGU, the underworld. The act of pouring a ritual libation to the ancestors before partaking a drink is meant to connect one with the ancestors. The evil spirits of those departed who have never rested in peace however roam the earth and are the source of mischief and misfortune.\nMũrathi in Gĩkũyũ literally means PROPHET or SEER. This was a man who Ngai chose as his messenger to the rest of the people. A famous Seer of the Gĩkũyũ was Mũgo wa Kĩbirũ who foresaw the coming of the White man into Gĩkũyũ country some 200 year ago.\nNGAI NDATANGATANGAGWO – GOD MUST NOT TO BE DISTURBED NEEDLESSLY.\nNgai, is distant and is not involved in the affairs of men here on earth though he visits them from time to time. He is not to be disturbed with petty annoyances which can be sorted out by the intervention of benevolent spirits. He is beseeched upon on certain occasions when everything else has failed. This is never an individual experience but a community affair when sacrifices are made for instance over a prolonged drought or famine. A personal relationship with Ngai was unheard of except for the first man Gĩkũyũ .\n1. Facing Mt. Kenya by Jomo Kenyatta\n2. Kwarahura Muhooere wa Gikuyu na Mumbi by Gakaara wa Wanjau\n3. The Sacred Footprint: The Story of Karima Sacred Forest by Kariuki wa Thuku']	['<urn:uuid:04af9489-7ef5-4a1f-8098-78370ed2f9cf>', '<urn:uuid:e6792e1b-9bd3-4dbf-8176-902db65abeff>']	open-ended	direct	concise-and-natural	distant-from-document	comparison	expert	2025-05-12T20:46:45.703947	11	66	1502
61	what role does s2 intelligence officer have in military operations and which types signal intelligence collection methods exist	The S2 intelligence officer is responsible for conducting intelligence preparation of the battlefield, producing situation templates, making predictive analysis, coordinating with MI units, proposing intelligence requirements for commander approval, and staying involved in training scouts and frontline operations. As for signal intelligence collection methods, there are five main types: COMINT (intercepting foreign communications), ELINT (gathering electronic signals from non-communication sources), RADINT (using radar signals), Non-imaging Infrared (detecting temperature changes), and LASINT (intercepting laser transmissions).	"['FM 34-8: Combat Commander\'s Handbook On Intelligence\n1. Force on force free-play exercises between battalions will force both S2s to conduct IPB and collection planning against a thinking enemy.\nIntelligence Training Tips for Commanders\n2. Always include MI support (jammers, radio intercept, GSRs, and interrogation teams) in your training exercises - particularly for free-play exercises.\n3. Always have your S2/G2 produce situation templates before developing an OPORD. Never accept just the enemy\'s probable COA - demand the set of possible enemy COAs, prioritized by likelihood, and make him explain why he selected the most probable COA.\n4. Always have your staff wargame multiple enemy COAs against friendly COAs. Ensure they record the results of the session in a BOS synchronization matrix and a DST.\n5. Always have your brigade S2/G2 develop an intelligence synchronization matrix to support your maneuver plan. (The brigade S2 could include it as part of the BOS synchronization matrix.)\n6. Always hold your G2 and MI unit commander responsible for meeting the deadlines for obtaining intelligence when you need it.\n7. Train your S2 and his analysts to make predictive analysis. Make them explain what they have done to confirm their predictions and to cover other enemy COAs.\n8. Always think of your battle staff as a unified team. Scrutinize their logic when they present IPB, friendly and enemy COAs, staff recommendations, etc. Make them conduct the command estimate process often.\n9. Teach your S2 and his staff friendly tactics.\n10. Conduct staff after action reports (AARs) after free-play exercises. Make the S2 brief ""what he thought happened"" before the opposing force (OPFOR) commander briefs what really happened. Have the S2 thoroughly examine his mistakes in light of the ground truth. Give him a chance to learn from his mistakes.\n11. Following your unit\'s AAR, have the S2 conduct an AAR for the supporting MI unit. What do they think happened?\n12. At division and corps, include language training as a fundamental part of your training and command inspection programs. Know the language proficiency of your intercept operations and interrogators.\n13. Have the S2 incorporate elements of a contingency threat in all training exercises.\n14. Introduce civilian role players (sympathetic, neutral, and hostile) into your training exercises. Evaluate how well soldiers identify and deal with each type. Also evaluate how well your interrogators and CI personnel elicit tactically relevant information from them and report it in a timely manner.\n15. Get your S2 ""out on the line"" often. Ensure that he is intimately involved in training and evaluating your scouts. Have him brief and debrief scouts, frontline commanders, and patrols whenever possible.\n16. Ensure the S2/G2 gets to physically see the battlefield to facilitate his understanding of the terrain analysis process.\n17. Incorporate BDA, synchronization, and intelligence reporting into as many training exercises as possible.\n18. Ensure IPB does not become only the S2\'s product. Instead, insist that the entire staff contribute in their areas of expertise. You need to participate too.\n19. Ensure the S3 briefs the DST when it is presented. This ensures a fully integrated concept of operations.\n20. Make your S2 and S3 train together in the routine development of operations, plans, and measures to support deception operations.\n21. Ensure a field artillery intelligence officer is fully integrated into the G2\'s targeting cell to incorporate changes in targeting priorities, to emphasize the employment of ECM, and to ensure prompt recognition of targeting priorities.\n22. Require the integration of the G2 plans section, terrain detachment, and USAF weather detachment with the G3 plans section. Evaluate their ability to work as a team in effectively anticipating and creatively satisfying future requirements.\n23. Ensure the S2/G2 and the MI unit commander work as a team. The S2/G2 is the senior partner with the MI commander directing the employment of his assets to satisfy the S2/G2\'s requirements.\n24. Ensure the S2 proposes new PIR for your approval each time the situation changes, and that he doesn\'t enact PIR without your approval. The same applies to targeting priorities.\n25. ""Jump"" the main command post frequently to train the S2 in measures required to keep track of the battle and execute battle handover with the TACCP.\n26. Ensure you and the S3/G3 react to new intelligence provided by the S2/G2 and are prepared to change your concept of the operation based on that intelligence.\n27. Check periodically with your S2/G2, MI unit, and other collection assets to ensure they are working on the same intelligence priorities--yours.\n28. Reports from elements in contact rarely contain all the desired elements (SALUTE). Make reporting a priority training task.\n29. Always make your S2/G2 answer the ""so what?"" test, and tell you the significance of the information or intelligence he is providing to your plan or concept of operation.\n|Join the GlobalSecurity.org mailing list|', 'Good Signal Intelligence Research Paper Example\nCommunication Security (ComSec)\nCommunication security is an important section of modern telecommunication systems where signal intelligences and other methods are used to provide secure communication among peers to be connected. Signal intelligence falls into the category of cryptography where the encrypted signal is collected and analyzed for propagating secure communication. This research work will explain the requirements of communication security and will discuss how signal intelligence can be used in communication security. These solutions can be used practically in communication security of the military services.\nSignal intelligence is introduced and evolved by military services where signals from foreign communication systems are observed and analyzed . Signal intelligence has found its applications in the military services and it has become an important component of the battlefield today.\nThe rest of the document is formulated as follows. Next two sections will explain the basic concepts of communication security and signal intelligence. In the coming sections, we will explain common threats faced by communication system and their possible solutions while using signal intelligence.\nCommunication security is the field of keeping unapproved interceptors from getting to the information in a clear structure, while conveying information to the proposed receivers. In the United States Department of Defense culture, it is generally abbreviated as COMSEC. The field incorporates transmission security, cryptosecurity and physical security of COMSEC gear.\nCOMSEC is utilized to ensure both classified and unclassified movement on military communication systems, including many modes like video, voice and data. It is utilized for both wired and remote connections, and both analog and digital applications.\nVoice over secure internet protocol VOSIP has proven to be the true standard for securing voice communication, supplanting the requirement for Secure Terminal Equipment (STE) in a great part of the U.S. Department of Defense. USCENTCOM moved totally to VOSIP in 2008. COMSEC equipment is designed to provide security to the telecommunication using cryptography.\nCommunication Security has following major fields:\nCryptosecurity: The field of communication security that is related with the accessibility of highly trusted cryptosystems and their authentic use. This doesn’t only include the confidentiality, but its authenticity as well.\nEmission Security: The assurance that is possible with the help of all measures taken to preclude unapproved personal data that may be obtained from communication frameworks and cryptographic gear intercepts and investigation of trading off transmissions from cryptographic tools, data frameworks, and telecommunication systems\nTransmission Security: It is the field of communication security that results with the help of applications that are specifically designed to ensure the the transmissions from getting intercepted and exploited by other ways from cryptoanalysis like frequency hopping.\nPhysical Security: The part of communication security that results from all physical actions important to protect characterized equipment, material, and records from access of unapproved persons.\nThe most protected form of communication is through cable which can be over land or underwater. Correspondences or different signs transmitted through such links can\'t be grabbed out of the air. Interception of data from cable has included physically tapping of the links or utilizing ""induction"" gadgets that are set in the vicinity of the links and upkeep of equipment at the time of access. This may not be possible because of hardened and ensured internal landlines, the kind of landline that conveys much high-priority, secret commands and control interchanges. Undersea links are most defenseless since the messages transmitted by them are then transmitted by microwave hand-off once the link achieves land.\nSignal intelligence that is often abbreviated as SIGINT is the technique that is used to gather signals by intercepting. It can be communication between humans which is known to be communication intelligence abbreviated as COMINT. Other form can be the interception from electronic signals known as ELINT. Signal intelligence is the sub-domain of intelligence collection management. As delicate data is mostly encrypted signals intelligence frequently includes the utilization of cryptanalysis to decode the messages. Traffic analysis is the investigation of people communicating with each other and the amount of data shared is additionally used to determine information.\nHistory: the interception of the encrypted written information started soon after the evolution of writing. For example, Caesar Cipher is the basic encryption system\nThe interception for electronic signals started during the Boer Wars in early 1900. Some of the British radios were captured by the Boers and as no other country was transmitting other than Britain hence it was easy to interpret the transmission.\nIt has been observed that signal intelligence caused serious threats even in peacetimes. Events like USS Liberty Incwident, USS Pueblo incident and shooting down of flight 60528 has involved loss of lives.\nSIGINT has five major sub-domains classified as:\nCOMINT: By its name one can assume that it is the intelligence obtained by intercepting the communication of foreign governments or groups and then processing and analyzing it. It can be encrypted or plain transmission like voice or Morse Codes. Commonly the information gathered with the help of COMINT is the diplomatic communication from nations all over the world to their diplomatic establishments.\nELINT: is the electronic intelligence gathered from the electronic signals other than the communication signals. For example, ELINT was used in WWII in order to locate the radars with the help of signals emitted by them. The information required for electronic intelligence is signal strength, frequency and pulse lengths etc.\nRADINT: Radar intelligence is similar to electronic intelligence in which no electronic signal is intercepted from any other object but signals are transmitted by the radar itself and then information is gathered by receiving the same deflected signal. Intelligence that can be obtained include trajectory, flight paths, maneuvering and angle of descent.\nNon-imaging Infrared: is the intelligence obtained with the help of change in temperature. One can detect the absence or presence of any object or its motion with the help of non-imaging infrared intelligence\nLASINT: laser intelligence is the sub-domain that gathers information from the interception of laser transmissions.\nAbove explained techniques can be used in signal intelligence in order to analyze enemy transmission. Different techniques can be applied in different circumstances and a hybrid combination of these techniques can also be used. In order to have a brief introduction of real time issues of communication system, we have explained some common threats to be faced by the communication system in battlefield and possible solutions offered by signal intelligence are also explained.\nCommon Threats to Communication System\nCommunication system can face many challenges in the form of threats which include physical as well as logical attacks to the system. In this section, we will explain such common threats faced by the communication system. Communication security can have problems of external interruption, detection, monitoring or collection of secure information with the help of wireless signals. Signal intelligence involves collection of data which can be used for the benefits of a country after analyzing data with the help of signal intelligence tools as described in the above section. Signal intelligence can cover the communication going on wireless channels and it cannot be used for wired communication. Signal intelligence will also fail to work in an environment when enemy conducts an operation in radio silence where no radio transmission is used for communicating among different enemy groups. Geographical locations and terrain conditions also affect the performance of proper signal detection to be used by signal intelligence. Another main and common issue of signal intelligence is encrypted or cyphered signal used by the enemy. Such signals are to be deciphered in order to get useful information .\nMost common threats faced by signal intelligence are detection of wireless communication devices by the enemy, communication, monitoring, false communication, involvement in actual communication and creating noise in the communication medium between two entities. Detection of wireless communication devices can be used to find the location and identity of communicating entity. Communication monitoring can be used to extract useful information in terms of future plans and actions of the enemy along with the current status of logistics and capabilities. False communication, involvement can be used to distract the enemy from actual intentions and can be used to fail the actual plan. Noise creation or interrupting the communication medium can help to break the communication medium between enemy lines and can significantly affect the performance of enemy operations. Interrupting the communication medium with the help of noise is considered an easiest section of utilizing signal intelligence in communication security for damaging communication system of the enemy. For that purpose, electronic attacks are used. It must be kept in mind that both electronic attack and signal intelligence can’t be used at the same time. Electronic attack can ruin the signal to be used for signal intelligence and therefore there must be coordination between both operations .\nSolution to Threats with the help of Signal Intelligence\nSignal intelligence is the analysis of captured signal in order to get useful information hidden in the signal. In order to reveal such hidden information, deciphering or the decryption techniques are used as explained in the above sections. Some common threats faced by communication security are explained in the previous section. Signal intelligence can be used both for creating these threats as well as to overcome such issues in communication systems. Encryption and decryption techniques have been used for past few years in order to deal such issues of network security and communication security. Redundant bits are required in order to provide a high level of security for transmitting information that increases the overhead on actual data significantly. This overhead can be reduced with the help of signal intelligence by sending minimum required redundant or overhead bits with actual data.\nSignal intelligence is used for finding the location of the signal emitting entity with the help of intercepting the wireless signal. It is also used for future threat detection by locating the position of aircrafts or military convoy and can be used as a replacement of a radar system in some cases.\nIt is recommended to military personals to avoid extra communication through wireless channels in order to prevent from the vulnerabilities caused by signal intelligence. Enemy signals can be used to locate their position and find useful information from their communication.\nCommunication security is a common issue of today’s telecommunication networks and it faces several types of vulnerabilities while securely transmitting data from one place to another. Many possible solutions have been proposed for providing optimal communication security for telecom networks. Signal intelligence is a famous solution to the problem which deals with communication security while analyzing signal encryption and decryption techniques. This research work has explained the working of such techniques used by signal intelligence. Common threats to communication security and their solution provided by signal intelligence are explained in this paper.\nAgency, National Security. Signals Intelligence. Jan 15, 2009. https://www.nsa.gov/sigint/.\nRhodes, J. E. Signals Intelligence. US Marine Corps , 1999.\nSteve Topletz, Jonathon Logan, Kyle Williams. ""Realistic Probabilities In Modern Signals Intelligence."" n.d.\nThe Chinese People’s Liberation Army Signals Intelligence and Cyber Reconnaissance Infrastructure. Project 2049 Institute, 2011.\nPlease remember that this paper is open-access and other students can use it too.\nIf you need an original paper created exclusively for you, hire one of our brilliant writers!\n- Paper Writer\n- Write My Paper For Me\n- Paper Writing Help\n- Buy A Research Paper\n- Cheap Research Papers For Sale\n- Pay For A Research Paper\n- College Essay Writing Services\n- College Essays For Sale\n- Write My College Essay\n- Pay For An Essay\n- Research Paper Editor\n- Do My Homework For Me\n- Buy College Essays\n- Do My Essay For Me\n- Write My Essay For Me\n- Cheap Essay Writer\n- Argumentative Essay Writer\n- Buy An Essay\n- Essay Writing Help\n- College Essay Writing Help\n- Custom Essay Writing\n- Case Study Writing Services\n- Case Study Writing Help\n- Essay Writing Service']"	['<urn:uuid:d46eaddf-817d-4218-8f90-2c64e9f0f4e3>', '<urn:uuid:f0875a27-ab8b-48f5-a05f-ab5df6c46a56>']	factoid	with-premise	long-search-query	similar-to-document	multi-aspect	novice	2025-05-12T20:46:45.703947	18	74	2784
62	For youth sports, how does triathlon safety compare to swimming and running?	Triathlon combines activities that individually have low to moderate risk. Swimming and running alone have low risk - swimming mainly involves preventable shoulder overuse injuries while running injuries are usually preventable through proper training, shoes and surfaces. However, triathletes need special attention to avoid overtraining across all three disciplines, requiring proper pacing, adequate rest, and careful transitions between activities to prevent injuries.	['Triathlon Training Tips for Kids\nA (triathlon) involves three separate events: swimming, biking, and running. If you like to be active outdoors, you might do all three of these things regularly. Participating in a triathlon involves competing against other young athletes in a swimming race, a biking race, and a running race. Lots of adults enjoy participating in triathlons, and kids might find them fun, too. Before getting out there to do a triathlon, you need to start a training program to get into shape. As you work to prepare, be very careful about how much exercise you get every day. It’s important not to over-train your body because this is when injuries might occur.\nTriathlon Training Tips\nParticipating in a triathlon is all about endurance. You will need strength and energy to get through each of the three events.\nSuccessful triathletes understand an important concept called pacing. Pacing involves starting a race and making sure that you can finish strong without having to stop because you are too tired. This means that you don’t want to expend all of your energy at the beginning: Hold back a bit so you have enough energy for the end of the race.\nTriathlon training also gives you practice switching between the different events. A training exercise might involve running for five minutes and then biking for five minutes, switching back and forth several times.\nDon’t over-train, because this can keep you from your fitness goals. Some training days should be easier, and some should be harder. Take at least one day off each week to give your body a chance to recover.\n- Never Too Young to Tri (PDF)\n- Tri Kids (PDF)\n- The Triathlon Parent’s Training Manual (PDF)\n- Top 10 Triathlon Tips for Youth and Parents (PDF)\n- Tips for Better Triathlon Form\n- Triathlon Training Tips\n- Youth Triathlon Training Plan: Junior Age Group (PDF)\n- See Jane Run, Bike, and Swim\n- Iron Girl Training Guide (PDF)\n- 12 Weeks to Your First Triathlon (PDF)\n- The Importance of Recovery in Triathlon Training (PDF)\n- Super-Sprint Beginner Triathlon Training Plan (PDF)\n- How to Train for a Sprint Triathlon (PDF)\nGeneral Fitness Tips\nEveryone needs to exercise for good health. In fact, the minimum recommendation for kids’ fitness is 60 minutes of exercise every day. But as you train for a triathlon, you have a special goal in mind. Make sure to stretch your muscles to warm up before you begin to exercise to help prepare your body and prevent injury. When you finish training for the day, cool down to help your muscles recover.\nHave fun while you train. Your workout sessions should be enjoyable as you work to perform at your best.\n- Raising Fit and Active Kids\n- Strength and Flexibility Tips for Kids (PDF)\n- Tips for Family Fitness (PDF)\n- Best Practices for Physical Activity (PDF)\n- Fitness and Exercise (PDF)\n- Exercise: Movement Matters (PDF)\n- Make Physical Activity and Exercise Fun for the Family (PDF)\n- Encouraging Healthy Habits in Your School-Aged Child (PDF)\n- Making Physical Activity a Part of a Child’s Life\n- Physical Activity Guidelines for Americans\n- Spring Into Shape! Fitness Tips for Adults and Children\nHealthy eating will help make sure that you have the energy you need to train. You will need plenty of complex carbohydrates, lean protein, and fresh fruits and vegetables to give your body the nutrients and energy it needs.\nComplex carbohydrates give your body energy for training. Choose whole grains whenever possible because they are healthier. Fresh fruits and vegetables will give you energy, too. They also have vitamins and minerals that your body needs to be strong and healthy. Stay away from foods that are high in fat and sugar; these foods usually don’t have a lot of nutrients.\nProtein helps make your muscles strong. Choose proteins such as skinless poultry, fish, and other lean cuts of meat. Nuts, seeds, and beans are also good sources of protein.\nAs you train, make sure to drink plenty of water. Your body needs fluids to work well, so you need to avoid getting dehydrated.\n- Charge Up With Healthy Eating\n- Cool Tips for Kids (PDF)\n- Healthy Families\n- Tips for Healthy Eating\n- Heart Healthy Eating for Kids (PDF)\n- Smart Nutrition and Fitness (PDF)\n- Fluid Facts for Kids (PDF)\nAnyone training for a triathlon needs to pay special attention to their overall health and fitness. Getting enough sleep is one important thing you need to do. Kids between the ages of 6 and 13 need between nine and 11 hours of sleep every night, and kids age 14 and older need between eight and ten hours of sleep.\nAs you prepare for a race, make sure you are having fun and enjoying the entire process. Mental preparation will help you stay positive and enthusiastic about participating in a triathlon.', 'Knowledge of a sport’s demands, lore, and jargon aids in understanding the athlete’s problems.\nModerate risk. Reports of specific injuries, which include stress fractures of the pars, distal fibula, and base of the second metatarsal; Achilles tendonitis; cuboid subluxation; os trigonum impingement syndrome; and trigger toes. Toes are severely stressed. Delay in puberty and emphasis on slenderness is a problem for girls and can lead to eating disorders. Be aware that the dancer’s self-image is one of an artist, not an athlete, despite the high level of athletic demands.\nModerate risk, depending upon the child’s age. Most acute injuries are associated with sliding, collisions, and ball or bat strikes. Most deaths occur from ball strikes to the head, neck, or chest. Overuse injuries, such as little league elbow, are preventable but potentially serious problems. Unusual injuries include apophysitis of the acromion, distal humeral epiphyseal separation, persistence of the olecranon physis, and avulsion of the iliac crest apophysis while swinging a bat.\nModerate risk. Injuries as compared with other sports occur more often but usually are mild. Injuries in children under age 12 involve mainly contusions, sprains, lacerations, and rarely a fracture. Seldom are there serious injuries. Adolescent injuries are more common and more likely to be serious, such as contusions, sprains, and sometimes fractures. Ankles and knees are affected most. Most serious are ACL injuries. Ankle injuries require rehabilitation to prevent recurrence.\nHigh risk. Most serious accidents are due to collisions with motor vehicles. Prevention is essential through education of children, use of helmets, and avoidance of congested roadways. Potential long-term disability from head injury is significant.\nHigh risk. Most injuries are due to collisions in this most risky sport. Catastrophic injuries can be reduced by using a well-fitting helmet and by avoiding spearing (initial head contact in blocking and tackling). A quarter of American football players are obese. Injury rates increase with maturation. Long-term osteoarthritis of the knee and hip are possible sequela from major injuries of these joints. Most problems result from acute injury and are due to joint and neurological damage. Try to control pushy coaches and educate parents.\nHigh-risk. There is a risk of head and cervical spine injury with quadriplegia. Attention to diving height, water depth, and technique are essential in prevention.\nModerate risk. Overuse injuries commonly produce spondylolysis and wrist problems. Wrist pain occurs in about 75% of gymnasts, and radiographic changes of the distal radial physis were found in 25%. Long-term problems may result from growth arrest of distal radial epiphysis and spondylolisthesis. The great focus on slimness may cause eating disorders and menstrual and growth problems in girls.\nModerate to high risk. Shoulder injuries are common from collision and puck and stick strikes. Protective gear is essential and has resulted in fewer facial lacerations. Head injuries and joint damage may lead to long-term problems.\nModerate to high risk. Injuries are due to horse handling and falls. Serious falls with head and neck injuries and fractures are common. Protective helmets and special training for handling horses can reduce risks. Horseback riding is proposed as therapeutic for children with cerebral palsy, scoliosis, and other conditions, but proof of effectiveness is lacking.\nThe playground can be a dangerous place for children. Soft surfaces and reduced height of playground equipment are important design features.\nLow to moderate risk. Overuse injury rates are high, but serious injuries are uncommon. Most injuries are preventable by appropriate training, shoes, and selection of proper running surface. Long-term sequelae are unlikely.\nHigh risk. Acute injury is common and is related to boards being difficult to control and being used on hard surfaces, unsupervised, with potential for collision. Skateboarders should use protective gear and avoid obstacles and high speed. Long-term sequelae risk is moderate and primarily results from head injury.\nLow to moderate risk. Collisions and falls cause forearm fractures and contusions. Skaters should use protective gear. Long-term sequelae are unlikely.\nModerate to high risk. Injuries are due to impact. More ankle and upper extremity injuries occur in this sport than in skiing, but there are fewer torsional, knee, and thumb injuries.\nModerate to high risk. Jumping and racing injuries pose the greatest risks. Tibial fractures, medial collateral ligament injuries, and thumb and shoulder injuries are common. Collision injuries are the most serious, as head, spine, and extremity injuries may have long-term sequelae. The most common injuries were contusions of the knee in children and sprains of the ulnar ligament of the thumb in adolescents. With increasing age, lower extremity injuries decrease but upper extremity injuries increase.\nModerate risk. Overuse and injuries involving the ankle and knee are common. ACL injuries are 2–3 times greater in girls. Long-term disability risk is low to moderate. The incidence increases with age, and injuries are more common in girls. Seventy percent of the injuries are located in the lower extremities, particularly the knee (26%) and ankle (23%). Back pain occurs in 14% of players. Fractures, which account for 4% of injuries, are more often in the upper extremities. Indoor soccer is the most risky.\nLow risk. Overuse injuries of the shoulder, back, and knee are common, but long-term disability risk is low. Good training and modification of swimming strokes are important in preventing and managing these–problems. Shoulder pain is due to impingement or instability. Preparedness for swimming is optimal between ages 5 and 6 years.\nLow risk. Acute injuries involving the lower limbs with sprains are the most common injuries. Upper extremity injuries, often due to overuse, are preventable with appropriate training, stroke technique, and equipment. Long-term disability risk is low.\nVery high risk. Most injuries occur from falls on hard surfaces to the side of the device. Head and cervical spine injuries are relatively common, and the potential for long-term disability is great. Discourage families from allowing children to play on trampolines.\nLow to moderate risk. With proper supervision and low weights, this sport is relatively safe. Overuse is the most common cause of injury. Long-term sequelae are low.\nHigh risk. More injuries occur in large adolescents and more during competition than in practice. The upper limb and knee are the most common sites of injury, and dislocations are more common than fractures. Most injuries are acute sprains. Medial epicondyle fractures, olecranon epiphyseal stress fractures, scapular avulsion fractures, and unusual injury patterns are common. The long-term disability risk is low to moderate.']	['<urn:uuid:5918f8d8-f470-4a3a-b88b-01d7cc1973a3>', '<urn:uuid:00ed5493-df92-4421-bc27-9b1a094b7c4d>']	factoid	with-premise	concise-and-natural	similar-to-document	multi-aspect	expert	2025-05-12T20:46:45.703947	12	62	1880
63	english certification resources hotel liability issues	Regarding English certifications for hotel management, platforms like Coursera, EdX, Alison, and FluentU offer various courses with certificates. Coursera allows peer-rated assignments and learning at your own pace, EdX offers academic credit through Arizona State University, and Alison provides free courses with certificates. As for liability issues, hotels face risks including guest injuries, food poisoning, cyber security breaches, and cross-border claims. Hotels must implement security measures like CCTV cameras, maintain proper food storage, ensure pool safety with lifeguards, and protect guest data. When incidents occur, quick professional response and proper handling of multi-jurisdictional claims is crucial to minimize exposure.	"[""When you look outside, you want to see mountain tops covered in snow.\nYou know what? You don’t have to decide right now where you’d like to see yourself in a few years.\nYou’ve already made the really important decision to learn English. So now you’re one step closer to wherever you choose to live and work, whether that be an exotic beach or a luxurious ski resort.\nAnd a career in hotel management could take you there.\nEnglish is widely spoken among the staff and guests in hotels, whether they’re in the US or anywhere else in the world. Thus, good English language skills are required for a career in hotel management.\nThis is your ultimate guide to learning English for hotel management and will definitely put you at the top of your game!\nYou’re the Boss! Learn English for Hotel Management\nA Quick Look at Resources for Learning English for Hotel Management\nAs we progress through the ins and outs of a hotel, we’re going to mention several resources that’ll help you learn specific vocabulary and skills. Let’s take a minute to highlight these:\n- Coursera is a platform that offers courses that let you learn at your own pace (as fast or slow as you want). You can complete courses as they take place, or just look over the material when they’re over. Each course has instructional videos, followed by quizzes and exercises you can take to make sure you understood the materials. There are a few peer-rated (graded by your fellow students) assignments, so it’s a good place to exchange information with other students who are learning just like you.\n- EdX allows you to earn academic credit through Arizona State University. Some of the courses have certificates you can get for completing them. Not all courses are free, so make sure you check the details before you sign up! Like most other MOOCs (Massive Open Online Courses), classes here are similar to college courses and some take a few weeks to complete. A certain number of hours is needed each week to learn all the material. You can learn along with the class or read through a course after it’s complete (though you won’t get to talk to others about it if you choose to do that!).\n- FluentU is an online immersion platform that takes real-world videos and turns them into personalized English lessons. That means you’ll have access to a ton of videos where you can hear the vocabulary you need in context. FluentU isn’t just watching videos—it’s about learning and actively practicing the language you hear in videos. Use the interactive subtitles, flashcards and vocabulary lists to learn English phrases better than ever!\n- Alison stands for Advance Learning Interactive Systems Online and was created in 2007 with the purpose of helping people to gain basic education and workplace skills. According to its website, Alison is the world’s leading provider of online free courses with certificates, with over 5 million users and over 600 courses.\nThese are just a few examples of the great resources that are out there to help you on your journey to becoming an awesome hotel manager. Now that you’re familiar with them, we’ll get started with an overview of how a hotel works.\nUnderstanding the Ins and Outs of a Hotel\nAs you begin to build your hotel vocabulary, you’ll need to know the specific terms that go along with the different departments. Let’s begin with a general overview of these areas in a hotel and some of the required tasks that correspond to each one, as well as resources to build your vocabulary and be successful as a hotel manager.\nThe employees at the front desk deal with guests when they check in and check out. Managing a hotel’s front office means making sure that rooms are ready when guests arrive. You achieve that through collaborating with (working together with) the housekeeping, reservation and food and beverage departments (which are all described below).\nThe front office is usually the first contact with a guest, so you want to make a good impression with the use of formal, polite and business-like English. This particular style can be learned from business English courses, many of which are available through the companies we mentioned above (Coursera, EdX, etc.).\nHere are some examples of free courses you can find:\n- Business English: Basics\n- Speak English Professionally: In Person, Online & On the Phone\n- Business English for Cross-cultural Communication\n- Introduction to Business and Travel English Language Skills\nHousekeeping staff clean and prepare rooms so that they’re ready for the coming guests. Managing a hotel’s housekeeping department involves scheduling staff for various shifts and managing cleaning equipment as well as supplies.\nYou can start with FluentU’s premium content “Hotel Housekeeping Essential Vocabulary” to learn the words and phrases needed for managing a housekeeping team.\nThough online booking is getting more and more common, reservation staff still receive occasional bookings by phone. Managing a reservation department involves updating the bookings with related departments in a timely manner.\nFluentU’s videos like “English in a Hotel” provide you with example situations, from which you can gather words and phrases, for example, to take a reservation. You’ll find it useful to practice listening to various dialects to be able to understand guests from all different language backgrounds over the phone.\nFood & Beverage\nThe staff from this department, more often known as F&B, take care of a hotel’s restaurants, bars and in-room dining. F&B managers are responsible for food supplies, customer complaints and the working schedule for wait staff as well as chefs and kitchen assistants.\nYou can learn from FluentU videos about real-world hotel restaurants like in “Afternoon Tea at the Dorchester.” Alternatively, you might find the video series about restaurants on FluentU English’s YouTube channel useful.\nSales & Marketing\nThe sales and marketing department is for promoting and selling the hotel services as well as building the brand awareness. Managing this department involves creating a marketing strategy, making sales plans and influencing people.\nYou will need to speak English eloquently, be able to tell engaging stories and persuade corporate and individual clients that your service is the best. You can learn the necessary language skills from business English courses specializing in marketing and sales like this one from Coursera or from the most accessible marketing material of big hotels: their website.\nEvent & Banqueting\nThis department is in charge of catering conferences, exhibitions, weddings and similar events. A manager in this department often works with other areas of the hotel, including F&B and sales and marketing. They’ll also be the primary contacts with your corporate clients. Thus, an advanced level of English speaking is an absolute must.\nIf this is something you want to focus on, you might consider the course English as a Foreign Language with Events Management. (Which also offers a bachelor’s degree in this specialization.)\nFinance & Purchasing\nThis department deals with financial matters, including monthly payrolls and obtaining goods and services used in the hotel. Besides basic financial terms, a finance and purchasing manager will need to be familiar with various jobs at the hotel as well as the goods and services needed to run a hotel smoothly.\nEngineering & Security\nDepending on each individual hotel, this can be one or two separate departments. Managing the department means taking care of maintenance and the safety of the properties, its staff and guests.\nHere is a list of vocabulary words describing a hotel’s interior, main features and structures, which are all essential for maintaining and securing the property.\nThis department takes care of personnel matters and managing this department involves direct contacts with all the staff who could be temporary, part-time or long-term full-time and who often come from many different backgrounds and nationalities.\nYou can learn essential skills to communicate with employees from different cultures in communication courses taught in English, such as Intercultural Communication and Conflict Resolution. FluentU’s premium content “Hotel English Vocabulary” is also a useful resource.\nCommon Usage of English in Hotel Management\nThough the nature of work in each department varies, a hotel, as a whole, is a customer-facing environment (meaning you deal with customers face-to-face). Consequently, hotel English is different from everyday English. In the following sections, you’ll find the most common situations in a hotel and recommendations for the suitable English usage.\nRequests for Information\nWhen getting information from guests, formal English is often a better choice than informal English. Here are some examples:\nFormal: Could I please see a copy of your passport?\nInformal: Passport, please.\nFormal: Would you like breakfast in the morning?\nInformal: Do you want breakfast in the morning?\nFormal: What is the best number to reach you?\nInformal: What’s your phone number?\nFormal: Is there anything else I could assist you with?\nInformal: Do you want anything else?\nPeople from certain countries, such as the US, might prefer more casual conversations. You might also find some business travelers from countries like China or Japan prefer a more formal tone. But don’t make assumptions! You can never be sure of a guest’s preferences, so it’s better to use the polite forms for initial contacts. This means could instead of can and would you like instead of do you want. Learn some ways to polish your English and make it more formal and professional here.\nDealing with Requests and Complaints\nWhen dealing with requests and complaints, it’s important to stay polite (even if the guests aren’t!) and helpful. You want to show empathy (understanding, or imagining yourself in someone else’s place) if a guest has a problem and demonstrate that you take their complaints seriously. The English you use should be clear and sympathetic.\nUse time adverbs like immediately, definitely and right away to show that you understand if something is a matter of urgency.\nThe use of I shows personal responsibility. You promise a guest that their need will be met. Using phrases like, “I am sorry,” or “I apologize for the inconvenience,” show your guests that you’re sympathetic to their needs.\nIn the following examples, which response do you think is better?\nSituation 1: A guest asks for direction to the nearest train station.\n- Option 1: When you get out of the hotel, turn left, then left again. You’ll find the train station on your right. Also, here’s a map that you’re welcome to take with you.\n- Option 2: Turn left twice, then the train station is on the right.\nSituation 2: A guest complaints about the noise at night\n- Option 1: I’m sorry to hear that the noise kept you awake. Would you like to move to another room? I’ll check with the system right now to see if we have any other rooms ready.\n- Option 2: You can move to another room. A manager will contact you later after she checks the availability.\nIf you’re a guest, which reaction would you prefer? Even if the meanings are exactly the same, one option will, more than likely, lessen the irritation of the guest and make them want to return to your hotel (that’s ultimately what you want, right?).\nSelling Products and Services Effectively\nSometimes you’ll need to actively sell the hotel’s services, be it the room, the food or the conference space. Sales is an art, not a science, so there are no fixed rules. However, a sales person often uses personal and powerful language, and a good salesman knows his products well.\nHere are some examples of a good sales pitch:\nWould you like some dessert? We have lemon cheesecake, chocolate brownie and ice cream. The brownie is amazing, very rich and moist. It’s my favorite, actually.\nWe have five conference rooms, hosting from 200 to 1,000 guests depending on what you have in mind. Our state-of-the-art projectors and lighting system are easy to use and, of course, we are happy to assist you with the operation if needed….\nThe salespeople giving these pitches seem knowledgeable about the services provided and offer descriptions that create an image in the mind of their customers that make them want to purchase whatever they’re selling.\nMaking Small Talk Naturally\nEven though polite, formal English is the rule of thumb, there will be times when you need small talk. Small talk are those little chats you have when just meeting a new individual, ending a discussion or during those awkward silences. Normally, when someone walks up to the front desk of a hotel, it’s more polite to begin with a casual conversation starter. Learning English for hotel management means equipping yourself with conversational questions and, more importantly, the ability to pick the right moments to ask those questions.\nA few ways to start a casual conversation:\nDid you have a pleasant trip?\nDid you have a good night’s sleep?\nDid you have a good day in the city?\nDid you find the bus station all right this morning?\nHow did you like the city?\nHow was breakfast this morning?\nFind other topics to start small talk here.\n3 Extra Tips to Prepare Your English for a Career in Hotel Management\nOnline English courses, videos and vocabulary lists are helpful, but you also need practical information. Here are three extra tips to learn English for hotel management.\nPick Up Hospitality Terms from Websites of International Hotel Chains\nThe language in the hospitality industry is full of jargon like F&B, Banqueting, Purchasing, Task Force… just to name a few. They’re not often found in standard dictionaries, but crawl all over hotel websites. Visit these websites and familiarize yourself with all the special terms. You can start with the four biggest hotel chains in the world:\nLearn from TV Shows and Movies That Take Place in Hotels\nYou can enjoy a good screening while learning about situations which are similar to ones you will find yourself in as a hotel employee or manager. Here are some good ones to check out:\n- “Fawlty Towers”\n- “The Best Exotic Marigold Hotel”\n- “The Grand Budapest Hotel”\n- “Maid in Manhattan”\nGet as Much Customer Service Experience as You Can\nBefore you can manage a team, you need to know what each team member does. Better yet, do the job yourself. You can apply for a part-time job in restaurants, cafes, hostels, etc. Alternatively, volunteer at big international events, such as music festivals, education fairs and conferences.\nYou can learn English for hotel management from a variety of resources, be it real-life videos or movies, online English courses or the website of international hotel chains. However, it’s always useful to learn about the different operations that occur in a hotel as well as the common situations of English usage. I hope this guide, and perhaps some customer service working experience, will prepare you for an exciting job in the hospitality industry.\nIf you liked this post, something tells me that you'll love FluentU, the best way to learn English with real-world videos."", 'Hotel Liability: Simple Actions, Better Results\nWe all love going on vacation. For most of us, this time to relax with friends and family and recharge our batteries is the highlight of the year. Nevertheless, not all vacations are free of problems and hotels are a common source of insurance claims. When a guest is injured due to the carelessness or neglect of a hotel or its employees, the hotel can naturally be held liable. As many holidaymakers are also tourists, these scenarios can quickly become complex due to the impact of multi-jurisdictional claims and the costs involved in handling them.\nGood risk management is vital for reducing the likelihood of incidents occurring in the first place – and handling them effectively when they do. While complete checklists are available to help hotels conduct full risk analyses of their properties and practices1, this article aims to highlight the most common causes of claims involving hotel liability from our own experience, along with simple measures to help mitigate the risks.\nSources of Hotel Liability Claims\nIt all starts with the reservation. Hotels need to have all the information on their guests before their arrival, especially when children, elderly guests and people who require special assistance are staying. Given the depth of guest information reception staff have access to, careful and respectful handling of this data is essential. This includes:\n- Avoiding pronouncing details related to guests and their respective rooms loudly in the reception area.\n- Where possible, making separate areas at the reception desk to avoid private details being overheard.\n- Taking additional care when issuing magnetic key cards – one slight mistake can grant someone access to another guest’s room without permission.\nCases of slipping and falling are the most frequent claims stemming from hotel rooms, but there are many other safety aspects to consider. Poisoning can also be a problem if a minibar and/or snacks are provided. Special attention to the following is recommended:\n- Avoiding decoration involving small items that can be swallowed by children.\n- Avoiding having carpets in the rooms. Carpets are an excellent environment for mites and bugs which can cause allergic reactions.\n- Making sure mattresses are cleaned at least once a month to avoid bed bugs.\n- Furniture must be “safe” and firm. Beauty is sometimes the enemy of safety.\n- Keeping electrical equipment (e.g., switches, hair dryers, lighting) well maintained and up to date to avoid the risk of electric shocks.\n- Some hotels choose to use plastic curtains inside the bathrooms to mitigate the risk of accidents. Glass looks nice, but should a guest slip and the glass break, the outcome can be serious.\n- Beverages must be sealed and nothing else apart from them should be stored inside the minibar.\n- If snacks are provided, they should be kept out of direct sunlight and the expiry dates should be respected.\nRestaurants and Kitchens\nFood poisoning is a very common cause of claims coming from hotels and the consequences can be deadly. Aspects to consider include:\n- Stored foods must be kept properly covered and refrigerated. Ideally, there should be a daily process for checking received goods to verify the quality of daily deliveries.\n- Due to the risk of salmonella, many hotels avoid serving raw eggs in their meals.\n- Hotels offering buffet-style dining should take steps to avoid the risk of burns from exposed pans and food warmers.\nCommunal Areas and Facilities\nSlips and falls are the number one problem in these areas but incidents involving children using recreational facilities, such as pools, occur all too frequently. Guests under the influence of alcohol are also a considerable risk near pools and beaches.\n- Lifeguards should be on duty to supervise the use of pools.\n- Pool depths should be indicated clearly.\n- Depending on the pool, warning signs forbidding diving are a sensible measure to prevent accidents.\n- Adequate and regular maintenance of pools and other facilities is important (e.g., checking ladders, slides, lights, diving boards, and other equipment that could cause serious accidents if damaged).\n- Pool toys should be kept out of the pool area while not in use. Although fun to play with, they cause many slips and falls.\nSecurity is a hugely important topic and we generally recommend:\n- The use of CCTV cameras in the public spaces of their property (where permitted) and informing guests of their use to avoid problems with data privacy.\n- Providing guests with security information as part of the welcome reception on arrival.\nIn addition to these more traditional security aspects, cyber security is becoming an increasingly important issue. The hospitality industry has become a rich target for nation-state hackers looking to track movements of important people and their preferences. Breaching a hotel’s data management system can give cyber criminals access to personal information including names, addresses, preferences, and credit card numbers.\nThe extent of the damage can be enormous, as evidenced by the Marriott International case, where an unauthorized party copied and encrypted information belonging to around 383 million customers from their Starwood reservations system.2 According to the information disclosed in the media, the breach affected customers who made reservations for Marriott-owned Starwood hotel brands from 2014 to September 2018. Names, addresses, phone numbers, birth dates, email addresses, and encrypted credit card details of the hotel customers were stolen.\nUnsurprisingly, liability for cyber security is a pressing concern for both hotel chains and the insurance/reinsurance community, in the sense that coverage can be found in cyber insurance products that offer first and third-party coverage.\nResponsibility for Risk Mitigation\nWhile there’s an expectation that hotel management is aware of the risks and proactively take measures to exclude or minimize the possibility of accidents, experience shows that not all are familiar with the risks and liability involved with running a hotel. As a result, we recommend that training on how to handle different situations and risk prevention measures should form part of a hotel’s risk management policy. This training should not only be for hotel management, but rather for the entire hotel staff.\nUltimately, the ability to provide immediate assistance and taking a professional approach can make the difference between an ordinary claim and a large one, (e.g., If medical assistance is required, is a process in place to quickly enlist the help of nearby medical professionals?).\nCross Border Claims\nHotels receive guests from all over the world, especially those hotels located in tourist hotspots. Here in Latin America, places like Cancún, Punta Cana, and Guanacaste welcome a huge number of guests every year, a considerable portion of which come from the United States and Canada. From our experience within Latin America, when something happens to a guest in a hotel, the legal jurisdiction technically responsible for deciding on the matter is the one where the hotel is located. Nevertheless, it’s very common to see American or Canadian guests place claims in their jurisdictions, which are known for their high level of indemnities.\nHotels must be prepared to handle such claims and having a well-defined strategy is essential. This should include provisions to ensure that injured parties receive immediate assistance, that adjusters or experts specialized in multijurisdictional cases can be accessed, and that support from lawyers and negotiators is available to help settle cases as soon as possible, and preferably before they become formal complaints.\nIt goes without saying that the insurer should also be notified promptly so assistance can be provided from the beginning of the case and steps taken to minimize the exposure. In complex international cases, reinsurers can also be useful consultants, offering a wider view of possible solutions regarding the claims handling process and providing details of international providers that can assist the insured with the claim settlement.\nWhen a claim is made in a jurisdiction that is not the one where it should be conducted, the normal procedure would be to start a forum non conveniens motion, (i.e., the doctrine that allows a court to dismiss a case brought where another court is more appropriate). However, it’s worth noting that negotiating a claim at the very beginning can prove to be more cost-effective than transferring the process and its subsequent conclusion to another jurisdiction.\nHotel general liability insurance policies include coverage for injuries or property damages sustained by guests during their stay at a hotel’s premises. Additional coverages may be purchased in separate policies, depending on the location. The sheer number of guests welcomed by hotels each year makes them ripe targets for lawsuits related to liability. Highlighting the risk exposure to hotel management and helping them put a well-defined strategy for handling incidents in place is essential for successful risk mitigation. Ultimately, a quick and professional response can be a deciding factor in avoiding major claims.']"	['<urn:uuid:0d6fec41-3830-46a0-a94f-1c2093d43dff>', '<urn:uuid:aef7c76f-23d8-4e6d-81fc-a36c940094d0>']	open-ended	direct	short-search-query	distant-from-document	multi-aspect	expert	2025-05-12T20:46:45.703947	6	99	3966
64	Having worked extensively with distributed computing, I'd like to understand both the write scalability approach in cloud databases and parallel task distribution mechanisms. What methods does Azure Cosmos DB use for global write scalability, and how do distributed operating systems handle parallel task distribution?	Azure Cosmos DB achieves global write scalability through multi-master configuration that enables 99.999% write availability worldwide, with <10ms write latency at 99th percentile. It uses anti-entropy channels for write propagation between regions with flexible conflict resolution policies. For parallel tasks, distributed operating systems classify workloads as either embarrassingly parallel (easily partitioned data processing) or non-embarrassingly parallel (requiring complex coordination). The system manages distribution through contexts that abstract and simplify thread distribution, with optimization mechanisms like heterogeneous competition where multiple nodes can process the same task with the faster result being used.	"['Since its conception in 2010, as a cloud-born database, we have carefully designed and engineered Azure Cosmos DB to exploit the three fundamental properties of the cloud:\n- Global distribution by virtue of transparent multi-master replication.\n- Elastic scalability of throughput and storage worldwide by virtue of horizontal partitioning.\n- Fine grained multi-tenancy by virtue of highly resource-governed system stack all the way from the database engine to the replication protocol.\nCosmos DB composes these three properties in a novel way to offer elastic scalability of both writes and reads all around the world with guaranteed single digit millisecond latency at the 99th percentile and 99.999% high availability. The service transparently replicates your data and provides a single system image of your globally distributed Cosmos database with a choice of five well-defined consistency models (precisely specified using TLA+), while your users write and read to local replicas anywhere in the world. Since its launch last year, the growth of the service has validated our design choices and the unique engineering tradeoffs we have made.\nBlazing fast, globally scalable writes\nAs one of the foundational services of Azure, Cosmos DB runs in every Azure region by default. At the time of writing, Cosmos DB is operating across more than 50+ geographical regions; tens of thousands of customers have configured their Cosmos databases to be globally replicated anywhere from 2 to 50+ regions.\nWhile our customers have been using their Cosmos databases to span multiple regions, up until now, they could only designate one of the regions for writes (and reads) with all other regions for reads. After battle-testing the service by running Microsoft’s internal workloads for a few years, today we are thrilled to announce that you will be able to configure your Cosmos databases to have multiple write regions (aka “multi-master” configuration). This capability, in-turn, will provide the following benefits to you:\n- 99.999% write and read availability, all around the world – In addition to the 99.999% read availability, Cosmos DB now offers 99.999% write availability backed by financial SLAs.\n- Elastic write and read scalability, all around the world - In addition to reads, you can now elastically scale writes, all around the world. The throughput that your application configures on a Cosmos DB container (or a database) is guaranteed to be delivered across all regions, backed by financial SLAs.\n- Single-digit-millisecond write and read latencies at the 99th percentile, all around the world - In addition to the guaranteed single-digit-millisecond read latencies, Cosmos DB now offers <10 ms write latency at the 99th percentile, anywhere around the world, backed by financial SLAs.\n- Multiple, well-defined consistency models – Cosmos DB’s replication protocol is designed to offer five well-defined, practical and intuitive consistency models to build correct globally distributed applications with ease. We have also made the high-level TLA+ specifications for the consistency models available.\n- Unlimited endpoint scalability – Cosmos DB’s replication protocol is designed to scale across 100s of datacenters and billions of edge devices - homogeneously. The architecture treats an Azure region or an edge device as equals – both are capable of hosting Cosmos DB replicas and participate as true-peers in the multi-master replication protocol.\n- Multi-master MongoDB, Cassandra, SQL, Gremlin and Tables - As a multi-model and multi-API database, Cosmos DB offers native wire-protocol compatible support for SQL (Cosmos DB), Cassandra (CQL), MongoDB, Table Storage, and Gremlin APIs. With Cosmos DB, you can have a fully-managed, secure, compliant, cost effective, serverless database service for your MongoDB and Cassandra applications, again backed by the industry leading, comprehensive SLAs. The above listed capabilities are now available for all the APIs Cosmos DB supports including Cassandra, MongoDB, Gremlin, Table Storage and SQL. For instance, you can now have a multi-master, globally distributed MongoDB or an Apache Gremlin accessible graph database, powered by Cosmos DB!\nDecades of research + rigorous engineering = Cosmos DB\nLast year, at the launch of Cosmos DB, we wrote a technical overview of Cosmos DB accompanied by a video interview with the Turing Award winner Dr. Leslie Lamport describing the technical foundations of Cosmos DB. Continuing with this tradition, here is the new video interview of Leslie describing the evolution of Cosmos DB’s architecture, application of TLA+ in the design of its novel replication protocol, and how Cosmos DB has married decades of distributed systems research from Paxos to epidemic protocols with its world-class engineering, to enable you to build truly cosmos scale apps.\nThis blog post dives a little bit deeper into Cosmos DB’s global distribution architecture including the new capability for enabling multiple write regions for your Cosmos database. In the following sections, we discuss the system model for Cosmos DB’s global distribution with its anti-entropy-based design for scaling writes across the world.\nSystem model for global distribution\nThe Cosmos DB service is a foundational service of Azure, so it is deployed across all Azure regions worldwide including the public, sovereign, DoD and government clouds. Within a datacenter, we deploy and manage the Cosmos DB service on massive “stamps” of machines, each with dedicated local storage. Within a datacenter, Cosmos DB is deployed across many clusters, each potentially running multiple generations of hardware. Machines within a cluster are typically spread across 10-20 fault domains.\nFigure 1: System topology\nGlobal distribution in Cosmos DB is turn-key: at any time with a few clicks of a button (or programmatically with a single API call), customers can add (or remove) any number of geographical regions to be associated with their Cosmos database. A Cosmos database in-turn consists of a set of Cosmos containers. In Cosmos DB, containers serve as the logical units of distribution and scalability. The collections, tables, and graphs, which you create are (internally) represented as Cosmos containers. Containers are completely schema agnostic and provide a scope for a query. All data in a Cosmos container is automatically indexed upon ingestion. This enables users to query the data without having to deal with schema or hassles of index management, especially in a globally distributed setup.\nAs seen from Figure 2, the data within a container is distributed along two dimensions:\n- Within a given region, data within a container is distributed using a partition-key, which you provide and is transparently managed by the underlying resource partitions (local distribution).\n- Each resource partition is also replicated across geographical regions (global distribution).\nWhen an app using Cosmos DB elastically scales throughput (or consumes more storage) on a Cosmos container, Cosmos DB transparently performs partition management (e.g., split, clone, delete, etc.) operations across all the regions. Independent of the scale, distribution or failures, Cosmos DB continues to provide a single system image of the data within the containers, which are globally distributed across any number of regions.\nFigure 2: Distribution of resource partitions across two dimensions, spanning multiple regions around the world.\nPhysically, a resource partition is implemented by a group of replicas, called a replica-set. Each machine hosts hundreds of replicas corresponding to various resource partitions within a fixed set of processes (see Figure 1). Replicas corresponding to the resource partitions are dynamically placed and load balanced across the machines within a cluster and datacenters within a region.\nA replica uniquely belongs to a Cosmos DB tenant. Each replica hosts an instance of Cosmos DB’s database engine, which manages the resources as well as the associated indexes. The Cosmos DB database engine operates on an atom-record-sequence (ARS) based type system1. The engine is completely agnostic to the concept of a schema and blurring the boundary between the structure and instance values of records. Cosmos DB achieves full schema agnosticism by automatically indexing everything upon ingestion in an efficient manner, which allows users to query their globally distributed data without having to deal with schema or index management. The Cosmos DB database engine, in-turn, consists of components including implementation of several coordination primitives, language runtimes, the query processor, the storage and indexing subsystems responsible for transactional storage and indexing of data, respectively. To provide durability and high availability, the database engine persists its data and index on SSDs and replicates it among the database engine instances within the replica-set(s) respectively. Larger tenants correspond to higher scale of throughput and storage and have either bigger or more replicas or both (and vice versa). Every component of the system is fully asynchronous – no thread ever blocks, and each thread does short-lived work without incurring any unnecessary thread switches. Rate-limiting and back-pressure are plumbed across the entire stack from the admission control to all I/O paths. Our database engine is designed to exploit fine-grained concurrency and to deliver high throughput while operating within frugal amounts of system resources.\nCosmos DB’s global distribution relies on two key abstractions – replica-sets and partition-sets. A replica-set is a modular Lego block for coordination, and a partition-set is a dynamic overlay of one or more geographically distributed resource partitions. In order to understand how global distribution works, we need to understand these two key abstractions.\nReplica-sets - Lego blocks of coordination\nA resource partition is materialized as a self-managed and dynamically load-balanced group of replicas spread across multiple fault domains, called a replica-set. This set collectively implements the replicated state machine protocol to make the data within the resource partition highly available, durable, and strongly consistent. The replica-set membership N is dynamic – it keeps fluctuating between NMin and NMax based on the failures, administrative operations, and the time for failed replicas to regenerate/recover. Based on the membership changes, the replication protocol also reconfigures the size of read and write quorums. To uniformly distribute the throughput that is assigned to a given resource partition, we employ two ideas: first, the cost of processing the write requests on the leader is higher than that of applying the updates on the follower. Correspondingly, the leader is budgeted more system resources than the followers. Secondly, as far as possible, the read quorum for a given consistency level is composed exclusively of the follower replicas. We avoid contacting the leader for serving reads unless absolutely required. We employ a number of ideas from the research done on the relationship of load and capacity in the quorum based systems for the five consistency models that Cosmos DB supports.\nPartition-sets – dynamic, geographically-distributed overlays\nA group of resource partitions, one from each of the configured with the Cosmos database regions, is composed to manage the same set of keys replicated across all configured regions. This higher coordination primitive is called a partition-set - a geographically distributed dynamic overlay of resource partitions managing a given set of keys. While a given resource partition (i.e., a replica-set) is scoped within a cluster, a partition-set can span clusters, data centers and geographical regions (Figure 2 and Figure 3).\nFigure 3: Partition-set is a dynamic overlay of resource partitions\nYou can think of a partition-set as a geographically dispersed “super replica-set”, which is comprised of multiple replica-sets owning the same set of keys. Similar to a replica-set, a partition-set’s membership is also dynamic – it fluctuates based on implicit resource partition management operations to add/remove new partitions to/from a given partition-set (e.g., when you scale out throughput on a container, add/remove a region to your Cosmos database, when failures occur, etc.) By virtue of having each of the partitions (of a partition-set) manage the partition-set membership within its own replica-set, the membership is fully decentralized and highly available. During the reconfiguration of a partition-set, the topology of the overlay between resource partitions is also established. The topology is dynamically selected based on consistency level, geographical distance and available network bandwidth between the source and the target resource partitions.\nThe service allows you to configure your Cosmos databases with either a single write region or multiple write regions, and depending on the choice, partition-sets are configured to accept writes in exactly one or all regions. The system employs a two-level, nested consensus protocol – one level operates within the replicas of a replica-set of a resource partition accepting the writes, and the other operates at the level of a partition-set to provide complete ordering guarantees for all the committed writes within the partition-set. This multi-layered, nested consensus is critical for the implementation of our stringent SLAs for high availability, as well as the implementation of the consistency models, which Cosmos DB offers to its customers.\nAnti-entropy with flexible conflict resolution\nOur design for the update propagation, conflict resolution and causality tracking is inspired from the prior work on epidemic algorithms and the Bayou system. While the kernels of the ideas have survived and provide a convenient frame of reference for communicating the Cosmos DB’s system design, they have also undergone significant transformation as we applied them to the Cosmos DB system. This was needed, because the previous systems were designed neither with the resource governance nor with the scale at which Cosmos DB needs to operate nor to provide the capabilities (e.g., bounded staleness consistency) and the stringent and comprehensive SLAs that Cosmos DB delivers to its customers.\nRecall that a partition-set is distributed across multiple regions and follows Cosmos DB’s (multi-master) replication protocol to replicate the data among the resource partitions comprising a given partition-set. Each resource partition (of a partition-set) accepts writes and serves reads typically to the clients that are local to that region. Writes accepted by a resource partition within a region are durably committed and made highly available within the resource partition before they are acknowledged to the client. These are tentative writes and are propagated to other resource partitions within the partition-set using an anti-entropy channel. Clients can request either tentative or committed writes by passing a request header. The anti-entropy propagation (including the frequency of propagation) is dynamic, based on the topology of the partition-set, regional proximity of the resource partitions and the consistency level configured. Within a partition-set, Cosmos DB follows a primary commit scheme with a dynamically selected arbiter partition. The arbiter selection is an integral part of the reconfiguration of the partition-set based on the topology of the overlay. The committed writes (including multi-row/batched updates) are guaranteed to be totally ordered.\nWe employ encoded vector clocks (containing region id and logical clocks corresponding to each level of consensus at the replica-set and partition-set, respectively) for causality tracking and version vectors to detect and resolve update conflicts. The topology and the peer selection algorithm is designed to ensure fixed and minimal storage and minimal network overhead of version vectors. The algorithm guarantees the strict convergence property.\nFor the Cosmos databases configured with multiple write regions, the system offers a number of flexible automatic conflict resolution policies for the developers to choose from, including:\n- Last-Write-Wins (LWW) which, by default, uses a system-defined timestamp property (which is based on the time-synchronization clock protocol). Cosmos DB also allows you to specify any other custom numerical property to be used for conflict resolution.\n- Application-defined Custom conflict resolution policy (expressed via merge procedures) which is designed for application-defined semantics reconciliation of conflicts. These procedures get invoked upon detection of the write-write conflicts under the auspices of a database transaction on the server side. The system provides exactly once guarantee for the execution of a merge procedure as part of the commitment protocol. There are several samples available for you to play with.\n- Conflict-free-Replicated-Data-Types (CRDTs) natively inside the core (ARS) type system of our database engine. This, in-turn, enables automatic resolution of conflicts, transactionally and directly inside the database engine as part of the commitment protocol.\nPrecisely defined five consistency models\nWhether you configure your Cosmos database with a single or multiple write regions, you can use five well-defined consistency models that the service offers. With the newly added support for enabling multiple write regions, the following are a few notable aspects of the consistency levels:\nAs before, the bounded staleness consistency guarantees that all reads will be within k prefixes or t seconds from the latest write in any of the regions. Furthermore, reads with bounded staleness consistency are guaranteed to be monotonic and with consistent-prefix guarantees. The anti-entropy protocol operates in a rate-limited manner and ensures that the prefixes do not accumulate and the backpressure on the writes does not have to be applied. As before, session consistency guarantees monotonic read, monotonic write, RYOW, write-follows-read and consistent-prefix guarantees worldwide. For the databases configured with strong consistency, the system switches back to a single write region by designating a leader within each of the partition-sets.\nAs a globally distributed database, Cosmos DB transparently replicates your data to any number of Azure regions. With its novel, fully decentralized, multi-master replication architecture, you can elastically scale both writes and reads across all the regions associated with your Cosmos database. The ability to elastically scale the writes globally by writing to local replicas of their Cosmos database anywhere in the world has been in the works for the last few years. We are excited that this feature is now generally available to everyone!\nAzure Cosmos DB started as “Project Florence” in late 2010 before expanding and blossoming into its current form. Our thanks to all the teams inside Microsoft who have made Azure Cosmos DB robust, by their extensive use of the service over the years. We stand on the shoulders of giants – there are many component technologies Azure Cosmos DB is built upon, including Compute, Networking and Service Fabric – we thank them for their continued support. Thanks to Dr. Leslie Lamport for inspiring us and influencing our approach to designing distributed systems. We are very grateful to our customers who have relied on Cosmos DB to build their mission-critical apps, pushed the limits of the service and always demanded the best. Last but not the least, thanks to all the amazing Cosmonauts for their deep commitment and care.\n- Grammars like JSON, BSON, and CQL are a strict subset of the ARS type system.', ""Overview - A Distributed Operating SystemA Distributed operating system is a system that operates a distributed collection of computers, a cluster. Some require a single controller, but that shouldn't be necessary and isn't an interesting part of the problem. The system runs programmes and manages resources. The system should completely abstract the view of the hardware away from the software.\nFault tolerance, scalability and redundancyThe cluster may be heterogeneous, nodes may go offline, new nodes may come online. These are all issues that the system should handle. The system should dynamically adapt to changes in its configuration while continuing to function without error. At all times the system should utilize the cluster to its full potential. Nothing of value should be lost when a node does go down, thus redundancy needs to be built into the system.\nAbstractionFor a distributed operating system, abstraction of the whole computer cluster needs to be considered. The application programmer needs the ability to create threads, manage the communication between them and access resources. The actual distribution and synchronisation should to be managed by the system.\nIntro to components\nCompute power distributionDistributing computational tasks between computers can be difficult or easy depending on the type of task being performed.\nSome task fall into the embarrassingly parallel category. This is where there is lots of data items that needs the same processing done to each. Here the data can simply be partitioned and sent to different CPU’s to work on independently.\nOther tasks fall into the non-embarrassingly parallel category. These tasks require more complex coordination to work in parallel. Non-embarrassingly parallel tasks traditionally use shared variables and require locking of variables.\nIn both cases there are different threads of execution work for some process. These threads will be distributed over different CPU’s and computer nodes within the cluster.\nRequirementsThe distributed operating system needs to abstract the hardware and run the programs. While some overhead will be accepted the efficiency of the system is a priority. Hardware abstraction has been done very well in modern computers. There are programming languages like C that compile for many different machine architecture’s and languages like Java that run in a virtual machine environment.\nResource managementSome sort of shared storage is required. For different applications a traditional file system or a database might be required. These would use the storage of all the nodes, while maintaining redundancy. The file system and database should behave like traditional implementations.\nCompute power distributionI propose the process’s and threads get a context that abstracts and simplifies their distribution and fault tolerance. These context’s will be similar to the contexts of the thread/process on a single computer operating system, with fault tolerant and scalability extensions. Contexts would define inter-thread communication and be associated with shared variables and thread locks. A global Context would exist for the entire process. Smaller Context groups could be defined for sub groups of threads within the process. The partitioning of threads will help the system optimize the layout of the nodes and limit the scope of shared resources reducing redundant copies being updated.\nInter thread communication, shared variablesIt may be that a small group of threads need to work on a sub problem for a larger task. The parallelization would need shared variables. Normally this would require a lot of inter-thread communication, but through the use of these contexts that could be abstracted away. The threads would simply access the shared variable. The computer nodes that these threads work on should have minimal latency. By being in their own context group the system could know to put them close together. This is an example of a environment aware optimization.\nSometimes threads may need to wait for other threads. These waits could be managed by these thread context groups.\nFault toleranceThe thread contexts would ensure fault tolerance by state saving to some distributed memory. Then ensuring that the thread is still running with ‘heart beat’ messages. If the thread is found to have crashed another thread could be started from the last context save. Here the only thing of value lost is the computation time.\nIn traditional operating systems, threads are swapped out to allow multiple treads running on a single CPU. This is similar to the context saves in this distributed system. The same kind of information is being saved.\nThe context saves could be an expensive part of the distribution of tasks. Each save comes with a cost, but each crash comes with another cost lessened by the context save. Balancing the two costs could be quite difficult, especially for different cluster configurations. The distributed operating system should take care of how often context saves occur. By learning the cost of each save and how vulnerable the cluster is, a automated balancing algorithm could be implemented.\nLocksLocks should generally be avoided, especially in a redundant distributed context. A lot of locks could be able to avoided through the use of higher level data structures. For example In a producer consumer situation a thread safe queue maybe simpler to work with than a simple array. The array is a much simpler to implement, but isn’t thread safe so thread locking would be required, where the queue could be thread safe, requiring minimal locking from the clients.\nCompare and swap style locking operations would also work well in a distributed system.\nTraditional locks will may be required at some point, or may just be desired by application programmers, so they should be implemented. To ensure redundancy with traditional locks, the locks should have a TTL (time to live). Where if the TTL expires the lock invalidates leaving the rest of the program to continue. The thread with the lock could renew the TTL, to acquire more time with it. This could behave like heart beat messages to ensure fault tolerance.\nHeterogeneous competitionWith the context saves continually happening to the distributed system, two copies of the same thread context save could be started on different nodes. One may finish faster, the results from that node could be taken and the slower node could abandon its progress, as it would lead to the same result. This would be useful when the cluster is waiting for one thread to finish, which may be on a slower computer. This optimization was shown in the google map reduce paper [GMR] to greatly improve the overall performance of the cluster.\nEmbarrassingly parallelThe distribution of embarrassingly parallel tasks has been done well by the google map reduce [GMR] and the Apache hadoop project [AHD]. Similar map reduce functionality could easily be implemented within this cluster using these contexts. One thread could read the input and start worker threads which would send their results to reducer threads.\nFile SystemThe distribution of the file system can be achieved by placing only part of the file system on each particular node called a brick. For redundancy duplicates of each brick would be replicated on different computer nodes.\nThe client using the file system would query which node a particular block was on then get that block from the respective node. This would all be packaged in the file system driver and appear like a normal file system to the applications.\nThe file system needs to support growing, having more storage added to it. This can be achieved by using a dynamic file mapping structure. Which would need to be distributed and redundant across the cluster.\nHigher throughput could be achieved by striping the data across multiple servers. This is done by partitioning the data into blocks, where successive blocks go to different cluster nodes. This provides the speed when one block is physically being written or read from one disk, the next block can start being written / read from the next node. Other environmentally aware optimizations could also be implemented, such as file locality.\nShared memoryThe system should store its context saves in memory. These will be broadcast to other nodes for redundancy. Not all nodes need to store the contexts for all other threads, just enough for redundancy. This could require lots of memory. But the treads can choose when they have reached a place to save state, ensuring that they only save whats required. To ensure context saves don't come out of order, they will have a timestamp. So if two computers compute the same context state, but one is further through its computation than the other, each thread will know how far through it is. The node receiving both will know what one to keep, the one with the higher time stamp.\nThe system will need to use a similar communication and shared memory structure for coordinating the execution of thread contexts. Care will need to be taken to ensure race conditions don't occur. The scheduling and starting of threads could be done in a two step phase. Where in the first phase the thread to start is scheduled, communicated to all nodes, and any race conditions are dealt with. The second phase takes place when the schedule is agreed upon, and the thread is actually started.\nHow close we are to building one.Some of the components already exist, while others need work. Work needs to be done to package everything together for easy distribution.\nDistributed File systems already exist, there is the Google file system [GFS] and the Gluster file system [GLS].\nRedundant coordination already exists in some computer games. With multiple computers playing one network game, one computer is chosen as the server. If that server then goes offline the other computers re-elect a new server. This is similar to the coordination required for a distributed operating system.\nApache ZooKeeper also provides redundant distributed coordination software [AZK].\nWe have distributed hash tables and distributed databases like Apache Cassandra [ACD], which work like a distributed shared memory. However their usually for persistent storage not like fast volatile shared memory required for task coordination.\nRedundant distributable task contexts have been done well for embarrassingly parallel tasks, with map reduce and hadoop. Building on my proposed model should work well, but needs more work to show that its feasible, the context saving needs to be proven to not be too expensive, then it needs to be implemented.\nSo how close. A year with a solid development team should be enough to put something together, using another distributed file system and data base. The big problem will be getting it popular enough to gain momentum. A lot of the companies that could provide the initial push of development seem to work in more of a embarrassingly parallel world, so wouldn't benefit from this.\nReferences[GMR] Jeffrey Dean & Sanjay Ghemawat (2004) MapReduce: Simplied Data Processing on Large Clusters. Google, Inc\n[GFS] Sanjay Ghemawat, Howard Gobioff & Shun-Tak Leung (2003) The Google File System. Google, Inc""]"	['<urn:uuid:9d0ed7a5-2cba-4dd0-8ad9-0e82df8ad833>', '<urn:uuid:d107e4db-1ccb-4ba4-8b3d-37806777317b>']	factoid	with-premise	verbose-and-natural	distant-from-document	multi-aspect	expert	2025-05-12T20:46:45.703947	44	91	4774
65	recycling rates global plastic waste 2015 versus projected 2050 percentage	In 2015, 20 percent of global plastic was recycled. The recycling rates are projected to reach 44% by 2050.	['This discussion draws heavily on “Plastic Pollution” by Hannah Ritchie and Max Roser, published September 2015 by “Our World in Data.” In their opening paragraph, Ritchie and Roser note that, “The first synthetic plastic – Bakelite – was produced in 1907;” and that formed the beginning of the plastics boom in the 1950’s, through 2015, “annual production … increased nearly 200-fold.” About 42% of plastics go into packaging and 19% into building construction. Plastic building materials have a use span of about 35 years. But, plastic packaging has a typical use time of six months or less. The 2018 United Nations Environment Report points out that, “…half of all plastic produced is designed to be used only once – and then thrown away.” They go on to note that, “Cigarette butts – whose filters contain tiny plastic fibers – were the most common type of plastic waste found in the environment in a recent global survey.”\nSo, what happens to all this single use plastic? Ritchie and Roser report, “In 2015, an estimated 55 percent of global plastic was discarded, 25 percent was incinerated, and 20 percent recycled.” The good news is that recycling rates are going up and are projected to reach 44% by 2050. Perhaps you’ve heard of the Great Pacific Garbage Patch. About half the plastics in these artificial islands comes from fishing nets and lines. These rafts of plastic trash cover an area larger than Texas and are up to nine feet deep. Brian Hutchinson of the Oceanic Society, notes that plastic waste, “…mars otherwise beautiful beaches, coastlines, and snorkel and dive sites worldwide.” According to sources at World Environment Day, “…99% of plastics are produced from chemicals derived from oil, natural gas and coal,”.\nCertain characteristics of plastics are helpful. They are light-weight, water resistant and inexpensive. But, trade-offs are steep when waste plastics enter our oceans. Marine animals are most affected. They become entangled in plastics and they eat plastics. Large, sharp pieces of plastic can collide with and damage coral reefs. Smaller creatures such as oysters and mussels may take in tiny plastic fibers. Larger fish take in larger objects such as cigarette packaging. Even larger species, such as whales, have been found to have swallowed plastic flower pots and hoses.\nBesides the obvious problems of an animal having a stomach full of sometimes jagged pieces of plastic, this may also lead to malnutrition. Ritchie and Roser report studies that theorize fish feel falsely full and stop eating. With all that plastic, perhaps there’s not much room in their stomachs for real food. The issue for humans centers on microplastics. Studies are ongoing to see how much we are absorbing these from our food. Ritchie and Roser note that plastics have been detected in items such as honey, beer and table salt. Whether these tiny plastic particles remain in us, or simply pass through, is still unknown. A recent article (October 2018) by Meilan Solly in the Smithsonian newsletter, revealed evidence that plastics are present in human stool samples. Austrian investigators found particles the size of sesame seeds – including polyethylene terephthalate (PRT) and polypropylene (PP). There is also a possibility that airborne plastic particles can land on our food and then be eaten. Of further concern is that the microplastics could absorb contaminants such as Polychlorinated Biphenyls (PCBs), and then release them to our digestive systems. Additives used to manufacture plastics, could leach out upon ingestion. So far, these things have not been documented. But, the possibility is worrisome.\nRecently, Oceanic Society’s Hutchinson created a list of seven steps we can all take to help solve these problems. The list includes: reduce use of single use plastics (straws, utensils, grocery bags, etc.); recycle properly; participate in a beach or river cleanup; support bans; avoid products containing microbeads (found in face scrubs, toothpaste, etc.); spread the word; and, support organizations addressing plastic pollution. Even something as simple as using a thermos instead of disposable water bottles can have a big impact over time. Think twice before taking that plastic straw. Do you really need it?\nBy: Susan Reel-Panish, FCW Board Member']	['<urn:uuid:7e60c048-c5d5-4ce6-ada3-142e7eae585f>']	factoid	with-premise	long-search-query	similar-to-document	single-doc	expert	2025-05-12T20:46:45.703947	10	19	688
66	What makes the weather in the Aleutian Islands considered some of the worst in the world, and how do these severe weather patterns develop?	The Aleutian Islands are considered to have some of the worst weather in the world due to weather fronts that originate in the South Pacific. These fronts create massive storms that can be hundreds of miles long and last for many weeks. As these storms move northward, they pick up frigid moisture from the waters and air, making conditions particularly severe.	"['The Aleutian Islands run from the Peninsula of Kamchatka in the Asiatic portion of Russia to Alaska. All the islands are bare and mountainous and the coasts rocky and surrounded by crashing waves and enormous breakers. (Larkin, unpaged) Some believe the Aleutians offer the worst weather in the world: Weather fronts originating in the South Pacific create storms hundreds of miles long and many weeks in duration (Sipes, unpaged) that pick up the frigid moisture of the waters and air as they move northward. It would seem that anyone desirous of living there would need some overwhelming reasons to do so. The Russians and Scandinavians who first \'discovered\' the area for non-natives, and later the Americans, did have good reasons to be there. As for the Aleuts and Alutiiq, an abundance of fish and sea mammals might have been the attraction if, as some theories surmise, they arrived across a land bridge from what we now call Siberia.\nSeveral researchers have advanced the theory that the peopling of the New World involved a northeasterly trending Siberial coastal drift along the continental shelf coast of the Seal of Okhotsk, the Kamchatka Peninsula and along the southern coast of the Bering land-bridge. Those same researchers, Laughlin, Turner and Vasilievsky think that because of the glaciation still present in the area of the Aleutian chain, the settlement happened via interior settlement. In other words, those who migrated into the interior later trekked, clambered or boated amidst the pack ice to populate, eventually, the archipelagos, including the Aleutians. This same second migratory wave of Siberian peoples also settled the British Columbia coast, and later drifted further south toe southern California. The same researchers say that the dental morphology of the prehistoric California Indians is much like that of all other Indians of North and South America, and very unlike that of the Aleuts and Alutiiq, which is in opposition to earlier theories. (Turner, 391+)\nTurner also mentions that there have been stories of non-terrestrial migration, and then refutes that statement by saying ""There are no known individuals or groups (or even stories of anyone) who boated in either direction between Siberia and Alaska along today\'s southern edge of the Bering Sea pack ice or the Davis Strait pack ice between Greenland and Canada."" (291+) On the other hand, he also mentions those who have advocated various trans-Pacific drift theories because of the some Jomon boatmen who introduced pottery from Japan to Ecuador, citing the work of Meggers in 1965. (Turner, 291+)\nBy whatever route the Aleut and Alutiiq arrived, they are thought to have been there at least 11,000 to 14,000 years ago, according to artifacts found in archaeological excavations. On the other hand, linguistic analyses have set the date at between 12,000 and 35,000 years ago. Nuclear mitochondrial DNA (mtDNA) studies suggest an arrival time of about 30,000 years ago; they also suggest that migration is much more complex than previously thought, and that multiple migrations and expansion of ancient peoples contributed to genetic diversity in all Siberian and Amerindian peoples, including the Aleut and Alutiiq. Research on dental variation suggests an arrival time of between 18,000 and 20,000 years ago. (Schurr, 246) Obviously there is wide latitude on timing. But the dental morphology does, at least, pinpoint what group the Aleut and Alutiiq might be part of.\nThe mtDNA reveals that of the original migrations to the Americas, the majority of modern Native American haplotypes belong to a mere four mtDNA lineages, designated A, B, C and D. A comparison of Native Americans, Siberians and Asians reveals that the same mtDNA lineages in all groups share the same sorts of mutations; the explanation most often offered is that the mutations arose in Asia in the founding lineages and were carried to the New World by ancestral Native Americans. These mutations are, moreover, of considerable age, suggesting that the lings between Siberian and Native American peoples are quite ancient. And, it appears that the ancient Beringian populations -- that is, those in the vicinity of the Bering Strait -- gave rise to the Chukchi, the Eskimo-Aleuts and the Na-Dene Indians about 7,000 to 13,000 years ago.\nWhatever the process was all those eons ago, it is clear that there were correspondences between the Aleut and Alutiiq and people from Siberia long before the modern Russian arrived in the archipelago. The group considered to be the Aleut people today are found mainly from the tip of the Alaska Peninsula all along the arc of the Aleutian Islands, which extend for about 1,500 miles. The Alutiiq, however, are really more germane to the Kodiak Island area and its archipelago, known as the Kodiak area, and lying about four hundred miles east of the Aleutians. They are also found on the mainland coasts from Chugach Bay to an area farther east. Their culture is Pacific Rim with a Russian infusion. Their language is classified as Eskimoan because it is more closely related to Yup\'ik and Inupiaq (Inuit) than to Aleut. Anthropologists also sometimes refer to the Kodiak area Alutiiqs as Koniaq, a term that was derived from 18th century Russian sources who had assimilated the world Kanaa\'in for the Kodiak area people. (Mousalimas, 1) The language itself is very likely extinct, at least as spoken through the eons before the arrival of the modern Russians, and the Scandinavians and Americans. There were once 20 native Eskimo and Indian languages in Alaska. The Eyak language, once found on Alaska\'s south coast, died when the second of two living speakers died in the early 1990s. There was also a linguistic divide between NaDene language speakers (Tlingit, Dena\'ina and Eyak) and Aleut-Eskimo speakers (including Unanagan and Alutiiq.) (Crowell, unpaged) Most of these languages, also, are moribund, or possibly already gone as they were endangered as long ago as 1993. The only two surviving languages are Siberian Yup\'ik, which had 1,000 speakers in 1993, and Central Yup\'ik, which had 10,000 speakers in 1993. Diamond, unpaged.) These languages, regarded as the Aleut-Eskimo type, might have been known to the ancient Aleut and Alutiiq peoples.\nWhile the extinct or nearly extinct languages of a people are difficult to reconstruct, it is somewhat easier to find out how they lived. The first wave of migration is assumed to have foraged, mainly. Later, by about 3,500 BC, permanent winter villages of semi-subterranean or plank housing was in place. At that time, where was significant population growth, increased village and household size, and emergence of social ranking. There was also widespread warfare, and complex artistic, mortuary and ceremonial traditions. (Crowell, unpaged)\nRelatively little research aimed specifically toward Aleut or Alutiiq peoples has been done, except as part of the major projects regarding the area from the eastern Aleutian Islands to the queen Charlotte Islands of British Columbia. However, because the populations show a common descent from founding Paleoarctic populations, and a high level of continued interaction over time, it seems logical to infer information about the Aleut and Alutiiq lifestyles from shared characteristics. And, in fact, the area surrounding the Gulf of Alaska offers one of the longest archaeological records of maritime adaptation in the Americas, possibly because of its bountiful numbers of marine mammals, fish, bird and other necessities for supporting human life, even in an inhospitable climate. (Crowell, unpaged)\nCrowell notes that geographic unity is a main reason to consider the Aleut (Unangan) and Alutiiq (Pacific Eskimo) cultures along with those of the other peoples that ring the Gulf, the subarctic Athapaskan Indian and the Northwest Coast Indian, despite the differences in language. As a basis for considering them all at once, Crowell says:\nAt the time of Russian and European contact in the late eighteenth century, all of the Gulf of Alaska peoples had maritime-focused subsistence economies, with universal reliance on salmon and sea mammals. Other fish species (e.g., halibut, herring, cod, rockfish, eulachon), intertidal invertebrates, birds, plant foods (berries, starchy roots, seaweed, spruce cambium), and land animals (caribou, elk, mountain goats, grizzly bear, black bear) were variously important in the diet, depending on location. (Crowell, unpaged)\nThis meant that all the peoples would need watercraft, and in fact, they all had highly developed sand specialized kayaks, open skin boats and wooden canoes; for hunting and fishing, they also developed specialized tools.\nWinter houses were built either of driftwood timbers or earth. And, as well, ""Functional and stylistic similarities in clothing, jewelry, containers, weapons, and other items of material culture reflected high levels of regional interaction and cultural exchange, sustained by local and long-distance trading, raiding, slave-taking, and intermarriage."" (Crowell, unpaged)\nIn addition similarities in housing and provisioning, the Aleuts and Alutiiq also had similar sociopolitical organizations. Total community sizes could exceed 1,000 people. Within that community, resource ownership and exploitation was by co residential corporate kin groups of from 15 to 40 people. Social status of those people was based on a combination of wealth and birth, with division into at least free and slave classes, sometimes…\n""Native Peoples Of The Aleutian Island Chain Specifically The Aleute Alutiiq"" (2004, May 10) Retrieved June 20, 2017, from http://www.paperdue.com/essay/native-peoples-of-the-aleutian-island-chain-170303\n""Native Peoples Of The Aleutian Island Chain Specifically The Aleute Alutiiq"" 10 May 2004. Web.20 June. 2017. < http://www.paperdue.com/essay/native-peoples-of-the-aleutian-island-chain-170303>\n""Native Peoples Of The Aleutian Island Chain Specifically The Aleute Alutiiq"", 10 May 2004, Accessed.20 June. 2017, http://www.paperdue.com/essay/native-peoples-of-the-aleutian-island-chain-170303']"	['<urn:uuid:f208e6e8-7c9f-4ac5-88b4-a670d8e95596>']	open-ended	direct	verbose-and-natural	similar-to-document	single-doc	novice	2025-05-12T20:46:45.703947	24	61	1553
67	What are the key advantages and disadvantages of cobblestone pavers?	Cobblestone pavers are the strongest paving material available, capable of withstanding any degree of traffic and weight. They are virtually indestructible and guaranteed to last at least a lifetime. However, they have some disadvantages: their surface is bumpy, making them unsuitable for footpaths. Additionally, professional installation is recommended since the stones are heavy, have uneven edges, and lack built-in spacer nibs.	"['Pavers are the true building blocks of landscape design. Placed piece-by-piece into driveways, patios, and pathways, these elements add beauty and function to a home\'s exterior. And they require far less upkeep than a lawn or garden. Early spring is the perfect time to implement a paving project. The ground in most parts of the country will be thawed, and since it\'s the beginning of the season, returning grasses and plants won\'t get trampled while you work. Plus if you want to hire out the job, contractors will be easier to book now than at the height of summer. Begin the process by choosing the paver. In the chart opposite, we compare the materials most readily available at stoneyards—brick, cobblestone, concrete, flagstone—and introduce an eco-friendly alternative: recycled rubber. Once you\'ve selected the look you want, think about durability and performance, such matters as how much weight the paver can withstand and how slippery it is when wet. Installation will be similar across the board; depending on type, pavers are either permanently mortared in place with cement or, more often, ""dry laid"" in a bed of compacted sand. Many also need edging treatments—metal tracks or heavy blocks of stone—to prevent shifting. Though maintenance for all types of pavers is minimal, there are some differences worth noting. For instance, only certain pavers can be safely cleaned with a power washer. Which ones? Read on.\n$2.50 to $20 per square foot\nFinely ground clay and shale blended with water, molded into shape, and kiln-fired.\nLooks best in traditional settings. Size is generally limited to 2 1/4-by-4-by- 8-inch rectangles. Coloring varies from dark brown to charcoal red; these tonal variations can enhance the project\'s overall design. Commonly applied to patios and walks. Most projects are mortarless.\nPROS: Built-in spacer nibs yield consistent joints for a tight fit. Strong, so won\'t crush under the weight of vehicles. Low water absorption minimizes cracks during freeze-thaw cycles. CONS: Square-edged pavers can chip easily during installation. Slippery when wet, so not recommended for pool areas. Pavers are not always of equal size, resulting in a slightly irregular pattern.\nScrub stains with stiff-bristled brush and all-purpose cleaner a few times a year; avoid power washers. Remove moss as needed with a three-to-one solution of water and bleach. Shovel snow.\n$6 to $8 per square foot\nHefty blocks of granite rough- cut from larger, quarried slabs of the igneous rock.\nOld-world aspect suits stately properties. Sizes range from 4-inch squares to 4-by-8-inch rectangles. Grayish-white in color; may have flecks of pink, yellow, or black. Often used as a border or as an accent band at one end of a driveway. Projects tend to be mortared.\nPROS: The strongest paving material, cobblestones can withstand any degree of traffic and weight. Virtually indestructible and guaranteed to last at least a lifetime. CONS: Surface is bumpy, so not good for footpaths. Professional installation is recommended, as the stones are heavy, have uneven edges, and lack built-in spacer nibs.\nScrub dirt with stiff-bristled brush and all-purpose cleaner a few times a year; avoid power washers. Treat tough stains with solutions formulated for granite. Its rough surface traps dirt, so sweep regularly. Shovel snow.\n$2.50 to $25 per square foot\nA mixture of sand, gravel, and cement blended with water and injected into molds.\nComplements most types of architecture. Comes in many sizes and shapes. Pigments add color; faux brick available. Some feature an exposed aggregate, such as beach pebble or oyster shell. Ideal for driveways, patios, pool decks, and walks. Most projects are mortarless.\nPROS: Built-in spacer nibs result in consistent joints for a tight fit. Strong, so won\'t crush under the weight of vehicles. Low water absorption minimizes cracks during freeze-thaw cycles. CONS: Color may only be surface- deep, so if a chip or crack occurs, gray base concrete can show through. Opt for ""color-through"" pavers to avoid this.\nScrub with stiff-bristled brush and all-purpose cleaner a few times a year; avoid power washers. Sweep pavers that have rough textures often. Shovel snow.\n$6 to $10 per square foot\nThin slabs of sedimentary or metamorphic rock, including limestone, sandstone, and slate.\nWith its blue, green, and brown tones, flagstone works well in country environments. The large, flat slabs are either ""regular"" (rectangles and squares, 12 by 28 inches to 24 by 36 inches) or ""irregular"" (random shapes, 12 by 18 inches). Ideal for walks. Projects can be mortared or mortarless.\nPROS: Individual pavers are heavy—they can be 100 pounds or more—so they\'ll stay put without an edging treatment. Thin, so can be laid with wide gaps in between to facilitate water drainage. CONS: Not durable enough for drive-ways. Depending on type, may flake after a decade. Slippery when wet, so not recommended for pool areas. Professional installation recommended.\nScrub stains with stiff-bristled brush and all-purpose cleaner a few times a year. Stubborn stains can be safely removed with a power washer. Shovel snow.\n$4 to $9 per square foot\nTires ground up into tiny particles and molded into assorted shapes and sizes; pigments add color.\nSynthetic look is perfect for ultramodern homes or where safety trumps style; commonly used underneath playsets or on walkways. Sizes and colors range from 9-inch hexagons in black, red, tan, and green to 36-inch-square faux-brick tiles. Projects are mortarless.\nPROS: The forgiving nature of the material protects against injury from falls. Recycled. Slip-resistant. Absorbs sound. Most brands are durable enough for driveways. Tiles snap together or are installed with adhesive, so no edging treatment required. CONS: Sharp objects can puncture the surface. Colors will fade in just a few years in traffic-heavy areas.\nScrub stains with stiff-bristled brush and all-purpose cleaner a few times a year. Stubborn stains can be safely removed with a power washer. Touch up torn areas with paints formulated for rubber. Shovel snow.\nFor more information view April 2006 Resources']"	['<urn:uuid:db0f3fd6-9ce3-4bfd-b125-dbf12c4f6e8e>']	open-ended	direct	concise-and-natural	similar-to-document	single-doc	expert	2025-05-12T20:46:45.703947	10	61	976
68	What kind of serious problems do wild goats cause to the environment when they're living in an area?	Feral goats cause several environmental problems: they damage pastures, crops and native vegetation through grazing, cause erosion and sedimentation of waterways, and pose a biosecurity risk as vectors of livestock diseases.	"[""Integrated feral goat control\nThis page provides advice on integrated feral goat control.\nWhy manage feral goats?\n- In Victoria feral or wild populations of goats (Capra hircus) are declared as established pest animals under the Catchment and Land Protection Act 1994.\n- Under the Catchment and Land Protection Act 1994 all landowners have a responsibility to prevent the spread of and, as far as possible, eradicate established pest animals on their land.\nGoats were first introduced to Australia in 1788 when they arrived with the first fleet. Wild populations quickly established and feral goats have since become major agricultural and environmental pests.\nFeral goat grazing damage affects pastures and crops as well as native vegetation. Feral goats also cause erosion and associated sedimentation of waterways. Feral goats pose a biosecurity risk as vectors of many livestock diseases.\nBefore you begin: Feral goat biology and behaviour\nBefore designing your feral goat control program, it is important to understand feral goat behaviour and characteristics.\nInformation on feral goat characteristics and behaviour.\nManagement of feral goats on your property\nPoints to remember\n- Control feral goats before they cause severe pasture or crop losses, vegetation damage and contribute to waterway sedimentation and soil erosion.\n- Ensure your feral goat control program doesn't affect native wildlife.\n- If you are planning to use traps to control feral goats: the trap specifications, trap checking times, provision of food, water and shade, and humane destruction of trapped feral goats MUST be carried out in accordance with the requirements of the Prevention of Cruelty to Animals Act 1986 (POCTA) and associated regulations. Trapping has several animal welfare implications and anyone considering trapping should read important further information. See Humane Vertebrate Pest Control.\n- If you are planning to use dogs for feral goat control, be mindful that there are specific requirements for the use of dogs for hunting. Under Section 28 of the Domestic Animals Act 1994, a person must not set or urge a dog to attack, bite, rush at or chase any animal except when hunting in accordance with the Prevention of Cruelty to Animals Act 1986 (POCTA). Refer to the POCTA Code of Practice for the Welfare of Animals in Hunting for specific information on the use of dogs in hunting.\nPlanning your program\nPlanning can maximise the effectiveness of feral goat control while minimising damage to other animals. Consider goat density, distribution and the habitat in which the feral goats are living as this will determine what actions are appropriate. The following steps will help in planning.\n1. Work together\nCoordinate control work with your neighbours the best results are achieved where neighbours conduct simultaneous feral goat control across a landscape, rather than just on individual properties. Work on your property can be undermined by the inactivity of your neighbours. Talk to your neighbours and local Landcare group and work out a plan for coordinated action.\n2. Conduct monitoring\n- Identify feral goat feeding and refuge areas in and around your property. Map these areas for future reference. Feral goats are often found on rocky outcrops, steep slopes, thick vegetation, gullies and ravines, goats can also be found in rangeland country.\n- Feral goats can be monitored by measuring tracks and scats, using motion sensor cameras and visual counts particularly at water points in dry areas. Aerial counts can also be used in some broad-scale infestations.\n- Assess the risks to non-target animals when implementing shooting and trapping, record them on a map for later reference.\n- Establish a benchmark of the impact of feral goats on your property. This will be used to measure the effectiveness of your control programs against the impacts of goats (i.e. vegetation damage).\n- Use the information you have gained from monitoring to:\n- target your control effort;\n- monitor the progress and success of your control program; and\n- vary and improve your program.\nIt is important to continue monitoring on an ongoing basis to detect and treat any re-infestation of your property.\n3. Aim to be feral goat free\nCreate a detailed feral goat management plan that has a specific aim and time-bound objectives to meet that aim. Eradication may not be possible in all areas and feral goats will require ongoing maintenance to manage the population at low levels.\n4. Use all the tools!\nEffective feral goat management utilises all the available control measures that are feasible on your property. Every individual feral goat should be exposed to as many different control measures as possible making your property a feral goat hostile environment.\nYou will need to consider seasonal movements of feral goats and access to areas where feral goats are feeding and living. Drier times when water is limited often concentrate goats around water sources making mustering, trapping or shooting more efficient.\n6. Evaluate your success\nConduct a second round of monitoring after your control program.\n- Are feral goats still present?\n- Are you still experiencing damage caused by feral goats?\n- Is the impact of feral goats above/below acceptable thresholds?\n- What is working well? What could be improved?\n- Do you need to change your plan? Have you managed all the risks?\n7. Follow up\nContinue monitoring on an ongoing basis. When you see signs of feral goat activity, implement control immediately.\nRemember, feral goat control is time-consuming and there is no quick-fix solution.\nThere are a range of parasites and diseases that may provide some small level of feral goat population control in Australia. However these control agents do not have a large enough effect to provide any significant benefit. New biological control agents are unlikely to be released due to potential impacts on agricultural goat and sheep production industries.\nThis involves constructing a fence around highly feral goat sensitive areas on your property and decreasing feral goat movement from areas where they rest to areas where they feed.\nExclusion fencing is a non-lethal way of protecting high-value areas from feral goats however it is expensive and not practical at a large scale. Fences must be strong and high enough to exclude agile animals like feral goats that are excellent climbers and jumpers. Unfortunately no fence will guarantee complete protection from feral goats. Fences need to be regularly checked and maintained as once a breakthrough occurs, a fence becomes ineffective.\nIt is best to have fences erected by experienced or professional fencers.\nWhile shooting is the most target-specific and humane form of feral goat control, it does not provide long-term, broad-scale control unless it is used as part of an integrated management approach. Due to the high reproductive capacity of feral goats, population numbers are likely to recover to original densities after only two years (or less) following successful shooting campaigns.\nGround shooting of feral goats is the most common form of shooting. However it is highly labour intensive and is likely to be opportunistic and uncoordinated. Ground shooting is unlikely to reduce the goat population enough to reduce the impacts of the pest unless the shooting is carried out in a coordinated fashion at a landscape scale. Where small isolated populations of feral goats exist, a coordinated ground shooting campaign may provide a useful management tool.\nAerial shooting of feral goats from a helicopter has become a common population control technique as feral goats often live in steep rocky terrain. Shooting feral goats from the air is suitable for control in a management program covering a large region of land where there is a high goat density. It is also highly target-specific, humane and cost effective where the terrain is suitable to detect and target goats on the ground and where shooting is carried out by an appropriately trained and skilled operator.\nAerial shooting is not cost effective in low density infestations due to the difficulty in finding widely distributed animals, nor is it suitable for areas with closed tree canopies.\nThe use of firearms to control feral goats must conform to relevant firearm legislation and be integrated with other control methods.\nMustering feral goats with the aid of dogs, motor bikes, helicopters or from traps placed around waterholes has become a popular way of managing feral goats. Mustering is only effective in range-land situations with a high goat density.\nMustered feral goats can then be commercially sold to recoup lost production due to feral goat impacts and the cost of muster. In isolation, mustering will not provide long-term feral goat control, as populations will quickly recover.\nThe value of feral goats sold from muster is generally less than the lost production resulting from a feral goat infestation. Managing feral goats as a resource doesn't manage impacts, such as overgrazing and erosion.\nTrapping is an effective method for managing feral goats when their numbers are concentrated around water points and in high density infestations. Trapped feral goats may then be transported for sale or humanely destroyed on site.\nWell-designed traps can be feral goat-specific, allowing access by feral goats but preventing access to non-target species. There are a range of trap designs but traps generally consist of a high sturdy fence around a water supply that has a one-way entrance.\nThe climbing and jumping ability of feral goats can be exploited by constructing a trap entrance that requires feral goats to climb or jump into the trap and they are then unable to escape. Trapping requires considerable time and cost for construction and ongoing maintenance of traps.\nTo comply with animal welfare legislation, traps must be checked at a maximum interval of 48 hours to minimise the time that trapped feral goats or non-target species are held.\nTrapping has several animal welfare implications and anyone considering trapping should read important further information.\nOther management techniques\nFeral goats rely on stock watering points in many drier parts of Australia. Restricting feral goats from gaining access to water (when water is not required by stock) can be an effective way to control feral goat populations as part of an integrated management approach.\n- Parkes J., Henzell R, and Pickles G. (1995) Managing Vertebrate Pests: Goats. Bureau of Resource Sciences and Australian Nature Conservation Agency. Australian Government Publishing Service, Canberra.""]"	['<urn:uuid:f154b1a5-1307-463f-be5c-b0af59f170bc>']	factoid	direct	verbose-and-natural	distant-from-document	single-doc	novice	2025-05-12T20:46:45.703947	18	31	1690
69	tayra american marten diet differences	The Tayra has an omnivorous diet including fruits, carrion, small vertebrates like marsupials and rodents, insects, and honey, while the American marten's diet consists of red squirrels, chipmunks, red-backed voles, meadow voles, snowshoe hares, birds, carrion, and insects.	['|Scientific Name:||Eira barbara|\n|Species Authority:||(Linnaeus, 1758)|\n|Red List Category & Criteria:||Least Concern ver 3.1|\n|Assessor(s):||Cuarón, A.D., Reid, F. & Helgen, K.|\n|Reviewer(s):||Duckworth, J.W. (Small Carnivore Red List Authority) & Schipper, J. (Global Mammal Assessment Team)|\nThis species is listed as Least Concern as although it is probably locally threatened as a result of human activity (Nowak, 2005), it is locally common throughout his entire range and occurs in a variety of natural and disturbed habitats.\n|Range Description:||This species occurs from southern Veracruz, Mexico, throughout Mesoamerica and south across South American to northern Argentina, occurring throughout except for the high Andes and Caatinga (eastern Brazil).|\nNative:Argentina; Belize; Bolivia, Plurinational States of; Brazil; Colombia; Costa Rica; Ecuador; El Salvador; French Guiana; Guatemala; Guyana; Honduras; Mexico; Nicaragua; Panama; Paraguay; Peru; Suriname; Trinidad and Tobago; Venezuela, Bolivarian Republic of\n|Range Map:||Click here to open the map viewer and explore range.|\n|Population:||Eira barbara is one of the most common medium-size predators throughout its range (Emmons and Freer, 1990). Common in Central America (Janzen, 1983; Alston, 1882; Kaufmann and Kaufmann, 1965; Emmons and Freer, 1990; Reid, 1997), Colombia, Guyana, Surinam, French Guiana (Eisenberg, 1989), Venezuela (Handley, 1976), Bolivia (Anderson, 1997), Brazil (except in the caatingas and cerrado; Emmons and Freer, 1990), Paraguay, and northern Argentina (Barquez et al., 1991; Mares et al. 1989; Redford and Eisenberg, 1992). However, was not recorded in the Paraguayan Chaco during a year (1989-1990) of large mammal censuses (Brooks, 1998), despite a taxidermied specimen in the local museum (Brooks, 1991).|\n|Habitat and Ecology:||\nEira barbara is a diurnal, sometimes crepuscular species (Reid, 1997), solitary that travels within a big home range (Sunquist et al., 1989). It seems to be a forest species, using both floor and tree habitats. Emmons and Freer (1990) affirms that Tayra inhabits tropical and subtropical forests, secondary rain forests, gallery forests, gardens, plantations, cloud forests, and dry scrub forests. Hall and Dalquest (1963) affirms that it can live near human habitations, crops and other human disturbed habitats. Usually occupies below the 1,200 m, but there are reports up to 2,400 m (Emmons and Freer, 1990; Eisenberg, 1989) and is common at 2,000 m (Gonzalez-Maya pers. comm.).\nDiet of Tayras is omnivorouse, including fruits, carrion, small vertebrates, insects, and honey and small vertebrates as marsupials, rodents, iguanids among others (Cabrera and Yepes, 1960; Emmons and Freer, 1990; Galef et al. 1976; Hall and Dalquest, 1963). This species does well in agricultural areas and along the edge of human settlements.\n|Major Threat(s):||There is not evidence about trapping or hunting of the species (Emmons and Freer 1990). Schreiber et al. (1989) reported that the range of the tayra has been reduced in portions of Mexico because of the destruction of tropical forests and spread of agriculture.|\n|Conservation Actions:||Tayras occur in numerous protected areas. Honduras lists this species under CITES Appendix III.|\nAlston, E. 1882. Biologia Centrali-Americana: Mammalia. R. H. Porter, London.\nAnderson, S. 1997. Mammals of Bolivia: Taxonomy and distribution. Bulletin of the American Museum of Natural History 231: 1–652.\nBarquez, R., Mares, M. and Ojeda, R. 1991. Mamíferos de Tucumán. Oklahoma Museum of Natural History, Norman, USA.\nBrooks, D. 1991. Some notes on terrestrial mustelids in the central Paraguayan chaco. Small Carnivore Conservation 4: 5-6.\nBrooks, D. 1998. Habitat variability as a predictor of rarity in Neotropical mammals. Vida Silvestre Neotropical 7: 115-120.\nBrooks, D. M., Rojas, J. M., Aranibar, H., Vargas, R. J. and Tarifa, T. 2002. A preliminary assessment of mammalian fauna of the Eastern Bolivian Panhandle. Mammalia 65: 509-520.\nCabrera, A. and Yepes, J. 1960. Mamíferos Sudamericanos. Ediar, Buenos Aires.\nEisenberg, J.F. 1989. Mammals of the Neotropics. The Northern Neotropics. The University of Chicago Press, Chicago, USA and London, UK.\nEmmons, L.H. and Feer, F. 1990. Neotropical Rainforest Mammals: A Field Guide. University of Chicago Press, Chicago, USA and London, UK.\nGalef, B., Mittermeier, R. and. Bailey, R. 1976. Predation by the tayra (Eira barbara). Journal of Mammalogy 57: 760–761.\nHall, E. R. and Dalquest, W. W. 1963. The Mammals of Veracruz. University of Kansas Publications, Museum of Natural History 14: 16-362.\nHandley Jr., C. O. 1976. Mammals of the Smithsonian Venezuelan Project. Brigham Young University Science Bulletin, Biological Series 20: 1-91.\nJanzen, D. H. 1983. Costa Rican natural history. University of Chicago Press, Illinois, USA.\nKaufmann, J. and Kaufmann, A. 1965. Observations of the behavior of tayras and grisons. Zeitschrift für Säugetierkunde 30: 146–155.\nMares, M., Ojeda, R. and Barquez, R. 1989. Guide to the mammals of Salta Province, Argentina. University of Oklahoma Press, Norman, USA.\nRedford, K.H. and Eisenberg, J.F. 1992. Mammals of the Neotropics, The Southern Cone: Chile, Argentina, Uruguay, Paraguay. University of Chicago Press, Chicago, USA.\nReid, F. 1997. A field guide to the mammals of Central America and southeast Mexico. Oxford University Press, New York, USA.\nSchreiber, A., Wirth, R., Riffel, M. and Van Rompaey, H. 1989. Weasels, civets, mongooses, and their relatives. An Action Plan for the conservation of mustelids and viverrids. IUCN, Gland, Switzerland.\nSunquist, M. E., Sunquist, F. and Dancke, D. F. 1989. Ecological separation in a Venezuelan llanos carnivore community. Advances in Neotropical Mammalogy: 197.\n|Citation:||Cuarón, A.D., Reid, F. & Helgen, K. 2008. Eira barbara. The IUCN Red List of Threatened Species. Version 2014.2. <www.iucnredlist.org>. Downloaded on 02 September 2014.|', 'The American marten is a North American mammal of the weasel family. This predator is also known as the “pine marten”; this name is derived from the common but distinctive martens that are found all over Eurasia. It is smaller and has a lighter color than the fisher (Martes Pennanti).\nThis small animal has golden brown fur with a yellow chest. It has a long and slender body with large eyes. Their small rounded ears resemble those of cats and they also have sharp, curved claws.\nAmerican Marten Scientific Classification\n|Scientific Name||Martes Americana|\nAmerican Marten Quick Information\n|Size||29-47 inches; Females are half the size of the males|\n|Color||Pale yellow buff, tawny brown to blackish|\n|Distribution||Widely distributed all over North America from Arctic Alaska and Canada to northern New Mexico and from Newfoundland to California|\n|Habitat||Dense, mature coniferous forests or mixed coniferous-hardwood forests that have a high level of canopy cover with coarse wooden debris on the forest floors.|\n|Sound||Social Vocalizations – Huffing and chuckling|\n|Litter Size||Average-2 to 3 kits|\n|Gestation Period||220 to 275 days, embryos develop in 28 days.|\n|Lifespan||In the wild – Approximately 8 to 10 years\nIn captivity – Up to 17 years\n|Diet||Eats red squirrels, chipmunks, red-backed voles, meadow voles, snowshoe hares, birds, carrion, and insects|\n|Predators||Raptors like owls and eagles; larger carnivores like red foxes, coyotes, lynx, pumas, wolves and bears|\n|IUCN Conservation Status||Least Concern with decreasing population trend|\nThe American marten is generally solitary and nocturnal. They are sometimes spotted in pairs as well as with dependent young. Though these animals are arboreal and spend most of their time on trees, still they do most of their hunting on the ground.\nThese mammals hunt during dusk and dawn when their prey is readily available. They are good swimmers and have the ability to swim under water as well. They do not hibernate and are active during winter.\nAmerican martens have a home range that is related to their sex, geographic area, cover type, habitat fragmentation, resident status, population density, year, prey availability, quality reproductive status and predation; however, it is most probably not related to their physical dimensions. Generally the males have a larger home range than the females and this may be attributed to the fact that females have denning needs that restrict their home range.\nMating and Reproduction\nMartens are polygynous, with the male mating with multiple females in the course of a breeding season. The male defends his territory against other male intrusions. Females enter estrus at around July or August, making scent markings in the males’ territories in order to alert them that they are prepared for breeding.Their courtship lasts for about 15 days and the pair often wrestles and plays together before mating.\nEven though the marten breeding season is from June to August, the young ones are born only towards the end of March or the beginning of April. These mammals have an unusual kind of pregnancy with delayed implantation of the fertilized eggs. This means that the eggs do not attach themselves to the uterine wall right away even though they are fertilized immediately. Once the development begins, the fertilized eggs undergo a gestation of around four weeks and the young ones (kits) are born sometime in late March or early April.\nThe females make use of dens to give birth and provide shelter to their babies. There are two types of dens-parturition occurs in natal dens whereas females shift their kits to maternal dens after giving birth. The females mainly use branches, snags, logs, stumps, witch’s brooms and wooden debris to build their dens. They line it with moss, leaves and grass to make it soft. When the kits reach 7-13 weeks of age, they are shifted by their mother frequently to new maternal dens.\nThe young ones of the American marten gain eyesight at the age of 39 days and they survive only on their mother’s milk till they are weaned at 42 days. They generally come out of their dens at 50 days. They become active at 7 to 8 weeks; however, they gain coordination at 12 to 15 weeks and reach adult weight at around three months. The juveniles stay with their mother all through the summer after which a majority of them disperse during the fall. This period coincides with the onset of the female’s estrus and their mother will prepare herself for her next breeding cycle. The dispersed juveniles will be ready for breeding when they become a year and a half to two years old.\n- During winter, long hairs grow between their toes that function as “insulated snowshoes” to keep their paws warm and facilitate traveling in deep snow.\n- Their long bushy tail helps them to balance while they are scampering through trees.\n- Their claws enable them to reach the treetops and move on narrow branches.\n- It is believed that their nocturnal activity possibly helps them avoid becoming victims to predators, especially birds of prey.\n- These animals have anal scent glands that produce a strong, repellent smell for marking their territories, giving them the name “mustelid”.\n- Martens are related to other animals like ermines, minks, river otters, fishers, and weasels.\n- Home ranges generally do not overlap between adult males, but this may happen between males and females, between females and between males and juveniles.\n- https://www.dnr.state.mn.us/mammals/ americanmarten.html\n- http://www.biokids.umich.edu/critters/ Martes_americana/\n- http://bioweb.uwlax.edu/bio203/s2008/ gries_jaso/\n- http://animaldiversity.org/accounts/ Martes_americana/']	['<urn:uuid:3aa449bc-519d-4bcb-9c8f-807f42b66f3b>', '<urn:uuid:a1c583aa-f43c-4c0a-9ed9-d1e2fff8a70f>']	factoid	direct	short-search-query	similar-to-document	comparison	novice	2025-05-12T20:46:45.703947	5	38	1775
70	who plays iranian fiddle silk road	Kayhan Kalhor plays the kamancheh, also known as an Iranian fiddle, in the Silk Road Ensemble.	['Global Ensemble Make Beautiful Noise at Hollywood Bowl\nBy CRAIG HAMMONS\nYo Yo Ma and the Silk Road Ensemble bring the world together in music and song as they end their 17-date run of the USA at the Hollywood Bowl.\nYo Yo Ma is a man who understands once music hits the ear it makes order out of chaos. It joins people together – there is no culture that doesn’t have music. Ma brought together the evolution of his favorite 18-year project, The Silk Road Ensemble.\nThe music they create celebrates difference by exploring the unfamiliar and giving them the opportunity to build something new.\nThe opening fanfare showed what it is like to transcend borders and the joy of allowing it to happen thru music. Cristina Pato from Spain playing Galician bagpipes and Wu Tong playing the Chinese horn in electrifying abandon meet center stage connecting two sides of the world and their cultural roots. Next up was “Ichichila” a tune traditionally sung by the people of West Africa. It had a cool relaxed vibe to it and a good way to set the tone for the evening.\nSilk Road violinist Colin Jacobsen — originally from Minnesota — brought us “O’Neil’s Calavary March,” a tune in the Irish tradition that dates back to the early 1800s.\nIt featured layers of instruments from a kamancheh, a pipa and some western strings.\nOn “Green (Vincent’s Tune)” they took a simple melody and turned it into an explosion of bass drumming and gong slamming gone mad.\nMy favorite song of the night was from Syrian clarinetist Kinan Azmeh called “Wedding.” He said that back home in Syria amist the bombs dropping and the daily fear, there is still the search for love. This a song about coming together to beat the odds and celebrate. He warned us that this song could get loud and next thing I knew it was like we were in a Syrian village party. He dedicated it to all the Syrians who have managed to fall in love in the last five years.\nSilk Road Ensemble\nMa did come forward to explain how the Silk Road Ensemble came together but mainly was just another member of the band. He picked up his cello joined Christina Pato now on piano to do a piece together that was beautiful and full of emotion.\nThere were many other mergers of musicians and instruments some that seemed improvised while others were full grooves and rhythms.\nKayhan Kalhor, one of the band’s core members, plays a kamancheh otherwise known as an Iranian fiddle. He was featured in many of the songs making his instrument sing while at time putting us into a trance.\nTribute to Prince\nSome of the crowd wished there was more Ma but no one was disappointed. They closed out the night with a colorful version of Prince’s “When Doves Cry” a song that fit well into their repertoire. The Bowl was lit purple and the crowd was on their feet giving these musicians from around the world the standing ovation they deserve.\nMany of the selections tonight were on the Silk Road’s new recording Sing Me Home. You can find out more about the Silk Road Ensemble at silkroadproject.com.']	['<urn:uuid:fded52d5-3cb9-410e-9bc9-b51762861d24>']	factoid	direct	short-search-query	similar-to-document	single-doc	expert	2025-05-12T20:46:45.703947	6	16	540
71	Which chemical enters bodies more: atmospheric or cosmetic BPA?	While atmospheric BPA releases can contaminate local surface water and lead to exposure, personal care products provide direct exposure through contact. Over 90% of Americans have traces of BPA in their bodies, with women aged 18-34 being particularly vulnerable as heavy buyers of personal care products. The exposure pathways include inhalation, food intake, and direct contact, though the documents don't specifically quantify which pathway leads to greater bodily concentrations.	['Water contamination by endocrine and hormone disrupting chemicals is threatening global water conditions. Numerous studies have proven that harmful concentrations of BPA have been leached into waterways from pollution, but little is known about how atmospheric releases of BPA might affect water quality. Now, researchers at the University of Missouri and the U.S. Geological Survey have assessed water quality near industrial sites and wastewater treatment plants and have discovered that atmospheric releases create a concern for environmental, animal and human health.\nHere’s the press release:\nCOLUMBIA, Mo. – Water contamination by hormone-disrupting pollutants is threatening water quality around the world. Existing research has determined that harmful concentrations of Bisphenol-A (BPA), a chemical used in consumer products such as plastic food storage and beverage containers, have been deposited directly into rivers and streams by municipal or industrial wastewater. Now, researchers from the University of Missouri and the U.S. Geological Survey (USGS) have assessed Missouri water quality near industrial sites permitted to release BPA into the air. As a result, scientists now believe that atmospheric releases may create a concern for contamination of local surface water leading to human and wildlife exposure.\n“There is growing concern that hormone disruptors such as BPA not only threaten wildlife, but also humans,” said Chris Kassotis, a doctoral candidate in the Division of Biological Sciences in the College of Arts and Science at MU. “Recent studies have documented widespread atmospheric releases of BPA from industrial sources across the United States. The results from our study provide evidence that these atmospheric discharges can dramatically elevate BPA in nearby environments.”\nWater sampling sites were selected based on their proximity to the Superfund National Priorities List (NPL) or locations with reported atmospheric discharges of BPA as identified by the Environmental Protection Agency. Current or historical municipal wastewater treatment sites, which have been shown in the past to contribute hormonally active chemicals to surface water from urban or industrial sources, also were tested. Finally, relatively clean sites were chosen to serve as the control group.\nThe water then was analyzed for concentrations of BPA, Ethinyl estradiol (EE2), an estrogen commonly used in oral contraceptive pills, and several wastewater compounds. Scientists also measured the total estrogen and receptor activities of the water. This approach is used to measure all chemicals present in the water that are able to bind to and activate (or inhibit) the estrogen or androgen receptors in wildlife and humans. Levels of chemicals were highest in samples with known wastewater treatment plant discharges.\n“In addition, we were startled to find that BPA concentrations were up to ten times higher in the water near known atmospheric release sites,” said Don Tillitt, adjunct professor of biological sciences at MU, and biochemistry and physiology branch chief with the USGS Columbia Environmental Research Center. “This finding suggests that atmospheric BPA releases may contaminate local surface water, leading to greater exposure of humans or wildlife.”\nConcentrations of BPA measured in surface water near these sites were well above levels shown to cause adverse health effects in aquatic species, Kassotis said.\nThe study, “Characterization of Missouri surface waters near point sources of pollution reveals potential novel atmospheric route of exposure for bisphenol A and wastewater hormonal activity pattern,” was published in the journal, Science of the Total Environment, with funding from the University of Missouri, the U.S. Geological Survey Contaminants Biology Program (Environmental Health Mission Area), and STAR Fellowship Assistance Agreement no. FP-91747101 awarded by the U.S. Environmental Protection Agency. The views expressed are those of the authors and of the U.S. Geological Survey; however, they are not the views of the U.S. Environmental Protection Agency. Any use of trade, firm, or product names is for descriptive purposes only and does not imply endorsement by the U.S. Government.\nSource: (formerly the MU News Bureau)\nDivision of Marketing & Communications University of Missouri, http://munews.missouri.edu', 'Common Endocrine-Disrupting Chemicals and Women’s Health\nFemale reproductive health may be affected by a wide range of chemicals present both in the environment and within consumer products. Called endocrine-disrupting chemicals (EDCs), these substances have been linked to an increased incidence of early puberty, pregnancy length disorders, and other reproductive health abnormalities.1 In 2012 alone, the US produced 9.5 trillion pounds of EDCs, which are embedded in products such as pesticides, plastics, chemical drugs, and even personal hygiene products.1 Endocrine-Disrupting Chemical Exposure EDCs can be grouped according to their origin as follows: industrial (e.g., dioxins, polychlorinated biphenyls [PCBs], and alkylphenols), agricultural (pesticides, insecticides, herbicides, phytoestrogens, and fungicides), residential (phthalates, polybrominated biphenyls, and bisphenol A [BPA]), and pharmaceutical (parabens); even heavy metals such as cadmium, lead, mercury, and arsenic may be included in the list of EDCs.2 The most common exposure pathways are through inhalation, food intake, and direct contact.2 Personal care products, marketed uniquely to women, are a specific source of EDC exposure. The most widely found EDCs in personal care products include:\nResearch suggests that women ages 18 to 34 are more likely to be heavy buyers of personal care products, purchasing more than 10 types of products a year.4 What’s more, these women and their children may experience heightened vulnerability to these environmental chemicals if the products are used during sensitive periods of development such as preconception or pregnancy.5 In an analysis of National Health and Nutrition Examination Survey (NHANES) data, increased levels of phenolic 2,5-DCP from dichlorobenzene (DCB), a common fumigant, correlated with earlier menarche in girls aged 12-16.6 The researchers measured a single chemical, but there are hundreds of known endocrine disruptors in our everyday environment. In a different analysis of NHANES data, 15 known toxicants were identified as contributors to early menopause in women7 while other EDCs have been linked to earlier menarche.6 Polycystic ovarian syndrome (PCOS) has also been associated with EDC exposure, particularly bisphenols.1,2\nToxins have been known to be endocrine disruptors, and what that means is that these toxins are estrogen mimetics—they act like estrogen, but they are dysfunctional hormones. As a result, they get in and they start changing things like metabolism; they start binding to receptors and upregulating things that perhaps shouldn’t be upregulated. -IFM educator Deanna Minich, PhD, FACN, CNS.\nBisphenols & Female Health Exposure to bisphenol A (BPA) is ubiquitous, with more than 90% of Americans having traces of BPA in their bodies.8In fact, the first endocrine-disrupting chemical (EDC) identified in 1936 was BPA; later, in 1950, it was discovered that BPA could be polymerized for the manufacturing of plastics.9 The United States Environmental Protection Agency has established a safe level of 50µg/kg/day and the European Food Safety Authority has established a tolerable daily intake below 4µg/kg/day;1 however, data from the literature suggest that exposure to BPA, even at low doses, may result in adverse health effects, particularly among pregnant women.9 These may include:\nAdverse reproductive outcomes\nBecause bisphenol molecules have an effect similar to estrogens, they may influence hormonal regulation and the activity of estrogen receptors.11 Bisphenols may also negatively influence oocyte maturation, spermatogenesis, and the development of the reproductive system. A 2019 review of the research literature suggests that bisphenol S (BPS), which has now been used to replace BPA in many products, may have comparable negative effects on reproduction and may cause severe fertility disorders.11 BPS is an industrial alternative to the endocrine disruptor BPA and may be found in some products labeled “BPA-free.”11 Data on current human exposure levels of BPS and bisphenol F (BPF), which is also being used to replace BPA, is reflected in NHANES 2013-2014 data.12=3 BPA, BPS, and BPF were detected in 95.7, 89.4, and 66.5% of randomly selected urine samples, respectively, indicating that exposure of the general US population to BPA and its substitutes may be nearly ubiquitous. According to the European Chemical Agency, 1,000 to 10,000 million metric tons of BPS are manufactured or imported annually to the European Economic Area alone.13 In humans, Lee et al observed associations among high urinary BPA levels and increased serum levels of testosterone, estradiol, and pregnenolone in girls diagnosed with precocious puberty.10,14 In particular, a correlation was observed between estrogen metabolism and BPA levels irrespective of the type of precocious puberty; the correlation, however, does not imply causation.14 In another compelling study, serum BPA concentrations were higher in women diagnosed with PCOS compared to women in the healthy groups.9,14 In addition to PCOS, higher BPA levels have been associated with an increased risk of developing ovarian failure, infertility, and fibroids.3 BPA is known also to cause adverse reproductive outcomes in non-human animals.10 Specifically, it has been demonstrated that BPA disrupts the HPG axis in mice, rats, and zebrafish.10 An interesting 2018 study found that in aquatic larvae of C. riparius midges, the general transcriptional profile of several genes were affected after 24-hour exposure to BPS, especially those involved in endocrine and biotransformation pathways.11 The study suggests that BPS activated the transcription of genes encoding nuclear receptors, concomitant with an increase in expression levels of other genes involved in the hormonal pathway mediated by ecdysone, indicating that BPS may be acting as a hormone agonist.12 Clinical Applications: Education Is Key Educating women about EDCs, including guidance about potentially problematic ingredients in personal care products, may help reduce exposure. Research suggests that many people are unaware of the sources of endocrine disruptors; in 2017, Rouillon et al reported that in France, 54.0% of pregnant women or those in the postpartum period were unaware of endocrine disruptors and had limited knowledge about potential sources of exposure.16,17 A randomized intervention trial for female college students in the United States effectively reduced exposure to BPA by promoting improved hygiene habits and the use of BPA-free cosmetics and glass food and water containers.16,18 In addition to education, improving biotransformation and aiding in the elimination of toxicants may also assist in fertility and possibly overall health. Food and nutrients that support liver biotransformation may help alleviate toxic burden, allowing the body to operate more efficiently and improve resilience. IFM’s Detox Food Plan benefits patients by helping them eat more of those foods that support pathways in the liver for healthy elimination, as well as reduce additional exposures to toxic compounds. In functional medicine, practitioners often utilize the phytonutrient-dense Detox Food Plan to support intestinal and liver function during the metabolic detoxification process. Functional medicine clinicians guide patients on how to reduce their exposure to EDCs and develop and organize individual treatment protocols to help the body deal with these toxicants using a multi-pronged approach that includes diet, nutraceuticals, botanicals, and behavioral interventions.']	['<urn:uuid:be3963dc-1773-4729-bf79-38ff08ccab00>', '<urn:uuid:aaf89773-9fa0-461c-a959-68535e9b2d16>']	open-ended	direct	concise-and-natural	distant-from-document	comparison	expert	2025-05-12T20:46:45.703947	9	69	1742
72	As someone interested in computer efficiency, I'm wondering how file access compares between a regular system's integrity checks and a distributed system's file management?	Both systems employ different approaches to file management. Regular systems use ECC (Error Correction Code) and CRC checks on flash memory to ensure data integrity, performing array integrity checks and readbacks to verify corrections. In contrast, distributed systems use a 'brick' approach where files are split across multiple nodes with duplicates for redundancy, and improve throughput by striping data across multiple servers, allowing parallel reading and writing from different nodes simultaneously.	"['Extra check to ensure detection of input activity absences (failures)\nOn critical interfaces like certain communication or sensor interfaces, we need a mechanism to detect if the interface has fallen idle. The simplest way would be to have a state machine that detects the idle condition or absence of the sensor or interface inputs. State machine should also be intelligent enough to detect and filter out glitches (noise) on the inputs. Once the system detects the idle condition or noise, it should send the interrupt to the core which could take the necessary corrective actions.\nFew ways of detecting the idle condition\n- Checking that data doesn’t change for the long time\n- Checking the current flow at the inputs\nIn a similar fashion we should be checking the overflow condition, say at the ADC input. ADC input should always be checked on the range and if ADC value is not within the recommend area then an interrupt to the core should be generated. The core can then take corrective action.\nA voltage monitor can be used to detect non operating voltage condition. As soon as the power supply is outside the specified range, it can interrupt the CPU to take the device into a safe mode.\nIn built CRC checks in communication interfaces can help in ensure correct functioning of the interface.\nChecking for the errors before the execution of the code or at regular intervals\nJust after power-up, the silicon should run the LBIST and MBIST (memory built-in self-test) to check if the circuit has any issues. Only after the checking circuit correctness, should the actual application start. LBIST helps in catching most latent faults. These routines can be run after a periodic interval to give enhanced robustness.\nSoftware should run regular scanning (CRC check) of critical configuration space/interfaces to ensure fault tolerant communication/working of the SoC.\nPut the system into safe mode if any error is detected during the course of execution\nOnce any fault is detected in the system then system should reset the system in case of critical faults or put the system into the safe mode in case non critical faults. If the system continues to produce the critical fault then that particular part of the system should not be used by the application.\nWatchdog counter is used to detect software malfunctions. Software malfunction can occur due to random hardware failure. Software is expected to regularly service the watchdog timer. If software fails to service the watchdog in required interval then it should reset the silicon or put it into a safe state\nRuntime checking of the critical signals\nMuch logic is put into the design to ease the debugging of the software. This logic should be idle or in a static, non-functional state in the actual application. Such signals can be monitored and system would move into safe state if such a condition occurs. Even static configuration of the SoC can be monitored.\nCritical clocks in the SoC, such as the system clock, input and output clocks of the PLL, and peripheral protocol clocks should be monitored. A clock monitoring unit on SoC must be kept which can detect clocks going out of range or have become inactive. In case they are out range/inactive, SoC can automatically switch to another clock source which is more stable or it might raise an interrupt so that the core could take necessary action.\nBecause all code resides in the flash, it is absolutely necessary to ensure clean operation in the flash. One way is to perform array integrity check using ECC on whole or part of flash array. Once a correction is indicated by ECC, a readback on the flash should be performed (for e.g. CRC of a block) to verify the correction done was due to single bit error and not multi-bit. The readback patterns can be checked against their expected value or force the read of a number of patterns sufficient to trigger other ECC error corrections/detection to reveal the actual nature of the fault.\nThe system IRQ (interrupt request) handlers must be capable of detecting false or missed interrupts since the IRQ generation logic is not usually replicated. This can be achieved by enabling the ISR (interrupt service routine) to perform certain checks to ensure that the ISR is called correctly and with proper priority. For example, the ISR can check that the interrupt was actually enabled in registers and flag was set or not. For periodic interrupts, a timer can be used to ensure interrupts are correctly generated. The final implementation is highly dependent on SoC need but it is imperative that incorrect and spurious interrupts are identified to ensure smooth operation.\nThis technique uses a mechanism by which the working of a module can be checked using feedback from the module and a monitor to check the feedback. For example, a write access to a RAM will get latched inside the RAM. This latched address, data and controls can be fed back to a monitor that can check that the latched values match the original access. Another example can be to check the rotation of a motor and angle of rotation against the desired and programmed controls. Any mismatch will trigger a corrective action.\nThere are number of design practices that are used consciously or unconsciously that inherently result in providing additional robustness and ensuring that a device still operates under harsh conditions without failing. These techniques become all the more important for certain critical applications such as automotive, medical, and aviation. There are specific standards today that ensure that these design practices are followed, thereby resulting in increased safety for end users.\nISO 26262 is one such standard for automobile applications. With the adoption and enforcement of these standards directly by automotive OEMs there is an increased push to all the suppliers to follow such safety standards for their deliverables. After all, the ultimate aim is to support and facilitate the development of safe products in the automotive industry—in other words—""Enhanced safety for the end user.""\n1. ISO26262 Specs (http://www.iso.org/\nAshish Goel is a verification lead, Prashant Bhargava is a senior systems engineer, and Sachin Jain is a design manager with the Automotive and Industrial Solutions Group of Freescale Semiconductor.\nIf you liked this article, go to the Automotive Designline home page\nfor the latest in automotive electronics design, technology, trends, products, and news. Also, get a weekly highlights update delivered directly to your inbox by signing up for our weekly automotive electronics newsletter here', ""Overview - A Distributed Operating SystemA Distributed operating system is a system that operates a distributed collection of computers, a cluster. Some require a single controller, but that shouldn't be necessary and isn't an interesting part of the problem. The system runs programmes and manages resources. The system should completely abstract the view of the hardware away from the software.\nFault tolerance, scalability and redundancyThe cluster may be heterogeneous, nodes may go offline, new nodes may come online. These are all issues that the system should handle. The system should dynamically adapt to changes in its configuration while continuing to function without error. At all times the system should utilize the cluster to its full potential. Nothing of value should be lost when a node does go down, thus redundancy needs to be built into the system.\nAbstractionFor a distributed operating system, abstraction of the whole computer cluster needs to be considered. The application programmer needs the ability to create threads, manage the communication between them and access resources. The actual distribution and synchronisation should to be managed by the system.\nIntro to components\nCompute power distributionDistributing computational tasks between computers can be difficult or easy depending on the type of task being performed.\nSome task fall into the embarrassingly parallel category. This is where there is lots of data items that needs the same processing done to each. Here the data can simply be partitioned and sent to different CPU’s to work on independently.\nOther tasks fall into the non-embarrassingly parallel category. These tasks require more complex coordination to work in parallel. Non-embarrassingly parallel tasks traditionally use shared variables and require locking of variables.\nIn both cases there are different threads of execution work for some process. These threads will be distributed over different CPU’s and computer nodes within the cluster.\nRequirementsThe distributed operating system needs to abstract the hardware and run the programs. While some overhead will be accepted the efficiency of the system is a priority. Hardware abstraction has been done very well in modern computers. There are programming languages like C that compile for many different machine architecture’s and languages like Java that run in a virtual machine environment.\nResource managementSome sort of shared storage is required. For different applications a traditional file system or a database might be required. These would use the storage of all the nodes, while maintaining redundancy. The file system and database should behave like traditional implementations.\nCompute power distributionI propose the process’s and threads get a context that abstracts and simplifies their distribution and fault tolerance. These context’s will be similar to the contexts of the thread/process on a single computer operating system, with fault tolerant and scalability extensions. Contexts would define inter-thread communication and be associated with shared variables and thread locks. A global Context would exist for the entire process. Smaller Context groups could be defined for sub groups of threads within the process. The partitioning of threads will help the system optimize the layout of the nodes and limit the scope of shared resources reducing redundant copies being updated.\nInter thread communication, shared variablesIt may be that a small group of threads need to work on a sub problem for a larger task. The parallelization would need shared variables. Normally this would require a lot of inter-thread communication, but through the use of these contexts that could be abstracted away. The threads would simply access the shared variable. The computer nodes that these threads work on should have minimal latency. By being in their own context group the system could know to put them close together. This is an example of a environment aware optimization.\nSometimes threads may need to wait for other threads. These waits could be managed by these thread context groups.\nFault toleranceThe thread contexts would ensure fault tolerance by state saving to some distributed memory. Then ensuring that the thread is still running with ‘heart beat’ messages. If the thread is found to have crashed another thread could be started from the last context save. Here the only thing of value lost is the computation time.\nIn traditional operating systems, threads are swapped out to allow multiple treads running on a single CPU. This is similar to the context saves in this distributed system. The same kind of information is being saved.\nThe context saves could be an expensive part of the distribution of tasks. Each save comes with a cost, but each crash comes with another cost lessened by the context save. Balancing the two costs could be quite difficult, especially for different cluster configurations. The distributed operating system should take care of how often context saves occur. By learning the cost of each save and how vulnerable the cluster is, a automated balancing algorithm could be implemented.\nLocksLocks should generally be avoided, especially in a redundant distributed context. A lot of locks could be able to avoided through the use of higher level data structures. For example In a producer consumer situation a thread safe queue maybe simpler to work with than a simple array. The array is a much simpler to implement, but isn’t thread safe so thread locking would be required, where the queue could be thread safe, requiring minimal locking from the clients.\nCompare and swap style locking operations would also work well in a distributed system.\nTraditional locks will may be required at some point, or may just be desired by application programmers, so they should be implemented. To ensure redundancy with traditional locks, the locks should have a TTL (time to live). Where if the TTL expires the lock invalidates leaving the rest of the program to continue. The thread with the lock could renew the TTL, to acquire more time with it. This could behave like heart beat messages to ensure fault tolerance.\nHeterogeneous competitionWith the context saves continually happening to the distributed system, two copies of the same thread context save could be started on different nodes. One may finish faster, the results from that node could be taken and the slower node could abandon its progress, as it would lead to the same result. This would be useful when the cluster is waiting for one thread to finish, which may be on a slower computer. This optimization was shown in the google map reduce paper [GMR] to greatly improve the overall performance of the cluster.\nEmbarrassingly parallelThe distribution of embarrassingly parallel tasks has been done well by the google map reduce [GMR] and the Apache hadoop project [AHD]. Similar map reduce functionality could easily be implemented within this cluster using these contexts. One thread could read the input and start worker threads which would send their results to reducer threads.\nFile SystemThe distribution of the file system can be achieved by placing only part of the file system on each particular node called a brick. For redundancy duplicates of each brick would be replicated on different computer nodes.\nThe client using the file system would query which node a particular block was on then get that block from the respective node. This would all be packaged in the file system driver and appear like a normal file system to the applications.\nThe file system needs to support growing, having more storage added to it. This can be achieved by using a dynamic file mapping structure. Which would need to be distributed and redundant across the cluster.\nHigher throughput could be achieved by striping the data across multiple servers. This is done by partitioning the data into blocks, where successive blocks go to different cluster nodes. This provides the speed when one block is physically being written or read from one disk, the next block can start being written / read from the next node. Other environmentally aware optimizations could also be implemented, such as file locality.\nShared memoryThe system should store its context saves in memory. These will be broadcast to other nodes for redundancy. Not all nodes need to store the contexts for all other threads, just enough for redundancy. This could require lots of memory. But the treads can choose when they have reached a place to save state, ensuring that they only save whats required. To ensure context saves don't come out of order, they will have a timestamp. So if two computers compute the same context state, but one is further through its computation than the other, each thread will know how far through it is. The node receiving both will know what one to keep, the one with the higher time stamp.\nThe system will need to use a similar communication and shared memory structure for coordinating the execution of thread contexts. Care will need to be taken to ensure race conditions don't occur. The scheduling and starting of threads could be done in a two step phase. Where in the first phase the thread to start is scheduled, communicated to all nodes, and any race conditions are dealt with. The second phase takes place when the schedule is agreed upon, and the thread is actually started.\nHow close we are to building one.Some of the components already exist, while others need work. Work needs to be done to package everything together for easy distribution.\nDistributed File systems already exist, there is the Google file system [GFS] and the Gluster file system [GLS].\nRedundant coordination already exists in some computer games. With multiple computers playing one network game, one computer is chosen as the server. If that server then goes offline the other computers re-elect a new server. This is similar to the coordination required for a distributed operating system.\nApache ZooKeeper also provides redundant distributed coordination software [AZK].\nWe have distributed hash tables and distributed databases like Apache Cassandra [ACD], which work like a distributed shared memory. However their usually for persistent storage not like fast volatile shared memory required for task coordination.\nRedundant distributable task contexts have been done well for embarrassingly parallel tasks, with map reduce and hadoop. Building on my proposed model should work well, but needs more work to show that its feasible, the context saving needs to be proven to not be too expensive, then it needs to be implemented.\nSo how close. A year with a solid development team should be enough to put something together, using another distributed file system and data base. The big problem will be getting it popular enough to gain momentum. A lot of the companies that could provide the initial push of development seem to work in more of a embarrassingly parallel world, so wouldn't benefit from this.\nReferences[GMR] Jeffrey Dean & Sanjay Ghemawat (2004) MapReduce: Simplied Data Processing on Large Clusters. Google, Inc\n[GFS] Sanjay Ghemawat, Howard Gobioff & Shun-Tak Leung (2003) The Google File System. Google, Inc""]"	['<urn:uuid:e7d69a4c-e9ff-4c2a-b346-acd9bd8124cc>', '<urn:uuid:d107e4db-1ccb-4ba4-8b3d-37806777317b>']	factoid	with-premise	verbose-and-natural	similar-to-document	comparison	novice	2025-05-12T20:46:45.703947	24	71	2879
73	I work with urban mobility systems and I'd like to understand what are the key technical components needed for efficient data processing in intelligent transport systems, and how does convolution processing fit into this ecosystem?	Intelligent transport systems require multiple integrated technical components including 5G wireless, edge computing, IoT sensors, GPS, and radar systems, all working together as a unified system. These systems generate massive amounts of data that needs efficient processing through methods like convolution, which is particularly important for handling complex signal information. The convolution processing in vector systems helps manage this data efficiently by enabling parallel computing operations, which is essential for real-time processing of sensor data and video signals that smart transport systems rely on.	"['An Apple patent (number 7895252) for single-channel convolution in a vector processing computer system has appeared at the US Patent & Trademark Office. The invention relates generally to signal processing within a computer processor.\nIt involves a system and method for performing convolution in a single channel of a vector processing computer system takes advantage of the parallel computing capability of the vector processing system and the distributed properties of the discrete-time convolution sum by performing convolution on portions of an overall data stream, or data chunks, simultaneously. Partial solution are thereby obtained and superimposed to achieve an overall solution data stream. To simplify the convolution sum and eliminate the need for calculating products, a specialized data signal or vector containing a series of ones may be used in the convolution operation. The inventors are li Sazegari and Doug Clarke.\nHere\'s Apple\'s background and summary of the invention: ""One of the most important, value-adding features in a computer is its ability to process large amounts of data and information. Some of the information frequently processed by a computer includes image and other signal information. Frequently, information processed on a computer may relate to the general computer display, computer graphics, scanned images, video, and other data. With each of these types of data, it is often desirable to utilize the convolution function to process the data.\n""Convolution is useful in demonstrating the manner in which two signals interact in the time domain, and in expressing a resulting signal from the mixing of the two signals within the time domain.""\n""One of the main problems in performing convolution using a computer is that the process is inherently linear. For relatively long sequences, therefore, the convolution process can be quite lengthy. Generally, a computer reads each function to be convolved as a stream of data, one element at a time. This requires valuable processor time, and the time required increases proportionately to the complexity and length of the signals to be processed. This is especially problematic, for example, in image processing applications and/or video applications, where signals are complex and memory-intensive. In video applications, another problem arises in that the real-time display of images, which is essential for a user\'s understanding in viewing the video information, requires numerous computations at a high rate of speed without delays. If the convolution sum used to process these video signals delays the output of the video, the result may be difficulty in understanding the output signal.\n""As processor speeds and users\' demands for quality increase, it is essential that signals which are processed by way of a convolution sum, such as the one shown in Equation 2, are processed in the most efficient manner without sacrificing quality. Even with the increased processor speeds of today, performing convolution as a serial process whereby entire streams of data are input, output, and computed sequentially, slows a computer\'s ability to process signals and information, and generally slows the processing of data involved in unrelated functions by the computer.\n""Recently, vector processing, which utilizes parallel computing operations, has been implemented in various computer systems. This type of computer processing has the advantage that multiple calculations may be performed simultaneously. This is accomplished by using vector calculations whereby entire matrices may be added, subtracted, multiplied, divided, or otherwise operated upon. However, even with the increased speeds afforded by performing vector calculations in a vector processing computer system, convolution has traditionally been a serial operation that does not take advantage of the vector processing power. As a result, an efficient, vector processing system may perform multiple tasks using parallel computing and not make use of the parallel calculating capability for convolution operations, thereby slowing the entire system while awaiting the results of a convolution calculation. The diminished processing speed is further exacerbated by the fact that linear processing typically occurs in a part of the computer\'s central processing unit separate from the vector processor. Consequently, the delays associated with transferring data between the linear and vector processors further slow the overall process.\n""Accordingly, it is desirable to create a system and method for performing convolution in a vector processing computer system that utilizes the parallel calculating capability of the system in a manner so as to make the most efficient use of the computer system.\n""In accordance with the present invention, these objectives are achieved by a system and method that performs convolution in a single channel of a vector processing computer system. This system and method take advantage of the distributive properties of the discrete-time convolution sum by reading in data, buffering data into a given number of data chunks, transposing the data chunks within a matrix to align the first bit of each data chunk, performing the convolution sums on each of the columns of a matrix simultaneously, storing the results from each column\'s convolution sums as partial solutions, superimposing the results of each column\'s convolution sums into a single data stream representing an overall solution to be further processed by the computer. According to an embodiment of the invention, the data is transposed and manipulated within a matrix. According to another embodiment of the present invention, one of the data signals or vectors used in the convolution sum is a vector comprising a series of ones. By utilizing a series of ones, a simplification of the overall convolution sum, which is the sum of products is achieved as the operation is reduced to an operation of sums only.""\n-- Dennis Sellers', 'Smart cities, intelligent transport\nWhat does the future of transport look like? The answer to this question is multifaceted, as transport, whether municipal or private, is about to take a quantum leap forward as several digital technologies mature and converge.\nSmart mobility, particularly in urban environments, will continue to evolve. The smart city is quickly coming into focus as technologies such as 5G, AI, IoT, AR, Wi-Fi, and edge computing are all components of an intelligent, efficient and secure transport system.\nThe digital infrastructures that an intelligent transport system needs to operate are being constructed. Smart parking, intelligent mobile real-time route planning, cashless payments, and integrated control systems are all in various stages of development. Businesses like Uber have shown how decentralised transport managed with a smartphone can transform how individuals’ access transportation services.\nIntelligent transport systems, though, offer much more than just calling a taxi: The smart digital ecosystem that all transport systems need to operate effectively creates a digital environment where even more innovation can occur.\nThere is also a commercial aspect as the report from Juniper Research concluded, smart traffic management systems will save cities $277 billion by reducing emissions and congestion globally by 2025. And the key technology behind these savings could reduce by over 33 hours the time spent in traffic per annum per motorist.\nIn its report, Arup summarise how a range of technologies converge to create the digital spaces transport can use to deliver efficient services to citizens: “Mobile phones, parking sensors, congestion charging zones and smart card ticketing all yield valuable data about how and when people are moving around the city, and how these patterns are affected by variables like traffic, weather or public events.”\nSpeaking to Digital Bulletin, John Cowan, CEO, EDJX, outlined how the Autonomy Institute – a cooperative research consortium focused on advancing and accelerating autonomy and AI at the edge, announced plans to launch the Public Infrastructure Network Node (PINN) in Austin, Texas.\n“The PINN is the first unified open standard to incorporate 5G wireless, edge computing, radar, lidar, enhanced GPS, and Intelligent Transportation Systems (ITS) as a single unified system,” says Cowan. “PINNs are designed to rapidly deliver a multitude of advanced edge sensors and computing capabilities urgently needed to support autonomy and IoT. PINN is a breakthrough in digital infrastructure, solving the current challenge of delivering low-latency computing and sensors at the edge while avoiding unsightly urban infrastructure sprawl.”\nDeveloping the digital ecosystem that advanced transport systems need to operate efficiently is a critical first step. And these systems are taking shape. For example, citizens in Hamburg, Germany can use an app on their phones that tracks their current location and constantly matches this to available buses and trains. The app also allows tickets to be paid for, making the use of their transport systems more efficient for everyone. Helsinki has WHIM that goes even further and integrates public transport access with bike and car-sharing services – all accessed via a citizen’s smartphone. And Copenhagen has smart traffic lights it claims can reduce bus travel by up to 20% and can even prioritise cycle users.\nStructure and technology\nOne clear prerequisite for smart cities and the intelligent transport systems they contain is fast, reliable connectivity. Here, edge computing comes into play. Moving data storage and processing away from decentralised data centres closer to the devices they serve (such as autonomous vehicles) is a fundamental component of creating an intelligent transport system. Autonomous vehicles need access to low-latency data networks. Edge computing delivers that performance.\nA byproduct of a smart transport ecosystem is data. Masses of information are generated within this digital environment. The value of that data is its ability to offer transport companies, governments, automotive manufacturers, and mobile broadband providers the insights they need to evolve their services.\nPwC, in its overview of the challenges building a smart city with an intelligent transport system, places data and its analysis at the core of these systems: “Once the core infrastructure – roads and railways – is in place, what matters then is understanding, anticipating, and meeting users’ needs.\nSmart use of data is vital in shaping the system to do this. So, the vision for the smart future is focused on people, enabled by transport, and powered by technology and data.”\nPaul Thomas, head of proposition development at North that designs and delivers integrated technology solutions for complex and critical environments throughout the UK, stated to Digital Bulletin that data is the key to intelligent transport systems: “Our public transport system should be frictionless; efficient, deterministic, an improvement on our air quality and supportive of the natural environment. It should also prioritise commuter experience and adoption. A robust transport system is critical in supporting economies and increasing productivity, so ease of use and efficiency is paramount. To deliver this, we need a baseline of data acquired from a connected city.”\nAs intelligent transport systems develop, they have a symbiotic connection to the environments they move through. Autonomous vehicles may grab the news headlines, but the vehicles themselves are just one small component of a smart digital network that enables them to operate safely.\nThe transport systems in development across the urban landscape are becoming complex networks of sensors and data analysis systems that the personalised transport services cities need are being built upon.\nConnect and protect\nIntelligent transport systems inherently use complex networks to deliver their services. The security and privacy of personal data is a core debate as smart transport systems develop. According to BAI Communications, who looked closely at how rail transport could be created, concluded 81% of respondents are at least somewhat comfortable with their anonymised data being used to improve transport systems. Similarly, three-quarters of rail users (75%) are at least relatively comfortable with their data being used to predict travel patterns to provide them with a personalised transport service.\nAlso, Hatem Oueslati, CEO and Co-Founder of, IoTerop makes the salient point that interoperability with systems and data is fundamental to developing integrated smart transport networks. “5G for connectivity and Lightweight M2M for security and data standardisation are strategically important. Not only do they reduce the technical and economic barriers spurring innovation, but they rationalise and future-proof investments as well. By adhering to the standard of these devices, systems, security, and data become interoperable.”\nThe data generated by intelligent transport systems must also be protected. Robust and flexible security is a critical component of any smart city and the intelligent transport systems they contain. There has been much debate around the lack of security within the IoT space.\nAccording to ABI Research, by 2026, it’s predicted there will be over 23 billion active IoT devices. However, the recent announcement from the FIDO Alliance’s proposed Device Onboard standard could offer a more comprehensive security platform as IoT expands, which will benefit the burgeoning smart transport networks as they evolve over the next few years.\nWhether using private vehicles or accessing public transport, the pervasive networks needed will disappear into our built environments. For citizens, their smartphones become their connection to these systems, which are fully integrated and deliver the personalised travel services they need on demand.\n“In the future, we need ‘smart cars’ to become more integrated with national intelligent transport infrastructures to ensure vehicles can operate safely and efficiently,” concluded Kevin Curran, IEEE senior member and professor of cyber security at Ulster University.\n“Satellite navigation and traffic signal control systems will ensure vehicles know when to stop, slow down and speed up as well as identify hazards in good time. This communication will result in better traffic management and significantly reduce the number of accidents. Ultimately, the roads beneath us will communicate with smart cars, most likely through indestructible sensors embedded within the road.”\nFor a transport system to be genuinely transformative for its users, the technical components must be fully interoperable. Systems that couple IoT with 5G are a powerful combination. Transport systems serve their customers. For them, seamless and transparent access to mobility is their holy grail. In the next few years, that goal will become a reality.']"	['<urn:uuid:ad044638-0917-4c6c-8c74-643a1e4aa6c2>', '<urn:uuid:9fb1d443-3fb3-4a21-ac59-2d5cdede0c5e>']	factoid	with-premise	verbose-and-natural	similar-to-document	multi-aspect	expert	2025-05-12T20:46:45.703947	35	84	2261
74	I need to convert angles - how many radians are in a circle?	There are exactly 2π radians in a circle, which is derived from the geometric formula relating circumference to radius (C = 2πr).	['An Open Letter to Students Wondering, “Why Do We Use Radians Instead of Degrees?”\nLet me start with a question of my own. Did you know that you speak Babylonian?\nYou may not think that you do – you can’t ask for directions to a Babylonian bathroom, or order lunch off of a Babylonian menu, or ask a good-looking Babylonian out for coffee. But you’ve inherited Babylon’s legacy nevertheless.\nYou’ve probably noticed that our systems of counting and measuring are largely base 10. There are 10 years in a decade, 10 decades in a century, and 10 centuries in a millennium. But take a look at shorter time lengths: We don’t carve up the day into 10 hours, each 100 minutes long, even though this would make perfect sense. Instead, we divide our time up by multiples of 60 – 60 seconds in a minute, 60 minutes in an hour.\nWhy do we do this? Because our Babylonian uncles used a base 60 number system, and we’ve been following their example ever since.\nBabylon left another relic in our system of measurements, one that requires a little deeper explanation, and which lies at the heart of your question (yes, I’m getting there): the degree.\nAngles are everywhere. They form when light bounces off of a mirror, when a bird dives towards the water, when a lamp casts a shadow on a distant wall. And mathematicians, being the go-getters that they are, need a way to measure angles.\nIt’s clear from looking at two angles which one is bigger:\nWe can think of any angle as a piece of a circle. For example, a right angle (like the corner of a rectangle) is one quarter of a circle, and the angle in an equilateral triangle takes up one sixth of a circle:\nSo, how do we measure angles? Well, the Babylonians had an idea. They decided to cut a circle into 360 pieces, and call one of those pieces a degree.\nThis makes it easy to talk about the size of an angle. Since there are 360 degrees in a whole circle, a right angle will be one-quarter as many degrees – i.e., 90o.\nMost people learn this system so well, and at such a young age, that it becomes second nature. Skateboarders pull 540o’s, a businessman looking to change plans will call for a 180o, and at one point in our political past, Americans chanted “Fifty-four forty or fight!” to demand that our border with Canada be set at 54o40’ of latitude (where 1’ = 1/60th of a degree).\nWhat this familiarity disguises is that the number 360 is totally arbitrary, chosen simply because the Babylonians preferred multiples of 60. Why not divide the circle into 100 pieces, or 5 pieces, or 400 pieces? (In fact, the 400-piece system does exist – 1/400th of a circle is called a gradian, and it’s that weird mode on your calculator that you never use.) There’s no mathematical reason to pick 360, or any specific number, for that matter. It’s fundamentally just a matter of taste.\nAs it turns out, there’s a better way. But to explore it, we’ve got to lift ourselves out of this rut of thinking. No more dividing the circle into some arbitrary number of units.\nInstead, let’s draw a whole circle, and throw in a radius for good measure.\nNow, let’s take that radius and wrap it around the outside of the circle. See how it forms an angle? We’ll call that angle 1 radian.\nHere’s an angle that’s 2 radians, for comparison.\nAnd here’s an angle that’s 3 radians. Notice that it’s almost half a circle (what we used to call 180o), but not quite.\nThis raises a question: how many radians are there in a circle? Draw it out, and you’ll find that it’s not a nice, round integer:\nLooks like a little more than 6. But how much more?\nFor that, we’ll need to dredge up a geometric formula relating the circumference and the radius:\nC = 2πr\nIn other words, the circumference is equal to the length of the radius times 2π (where π is roughly 3.14. For a better – but still imperfect – approximation, try this).\nThis tells us exactly how many radians there are in a circle: 2π!\nKnowing this, we can now convert between radians and degrees – just as we can convert between miles and kilometers, or Fahrenheit and Celsius. Radians become a perfectly valid, usable measure of angles.\nBut I know you’re not satisfied with that. You’re sharp-witted and wary of being made to learn new things. You want to know: What was wrong with degrees? Arbitrary or not, they let us work with integers ( like 30o) instead of nasty ratios involving irrational numbers (like π/6). Degrees are warm, friendly, familiar. Why ditch them in favor of this bizarre radian?\nHere’s the best answer I can give you: Degrees are fine for everyday measurements. But Trigonometry marks a turning point in math, when the student lifts his gaze from the everyday towards larger, more distant ideas. You begin exploring basic relationships, deep symmetries, the kinds of patterns that make the universe tick. And to navigate that terrain, you need a notion of angles that’s more natural, more fundamental, than slicing up the circle into an arbitrary number of pieces. The number π, strange though it may seem, lies at the heart of mathematics. The number 360 doesn’t. Clinging to that Babylonian artifact will only distract you and obscure the elegant truths you’re searching for.\nJust like you, I learned to speak Babylonian long before I encountered radians. And for years, Babylonian remained my native tongue – to give an angle in radians first required an act of mental translation. So if you’re resistant towards radians, nostalgic for Babylon, I can sympathize.\nStill, when push comes to shove, radians can take you places that degrees simply can’t. That’s why, when my Trig students give an angle in degrees instead of radians, I tell them: “I’m sorry, I don’t speak Babylonian.”\nI hope this helps. When in doubt, remember: 180o = π radians.']	['<urn:uuid:7f3704e2-82bc-49ad-9ddd-ddb87aa79c7b>']	factoid	with-premise	concise-and-natural	similar-to-document	single-doc	expert	2025-05-12T20:46:45.703947	13	22	1024
75	I develop motion picture projection systems and I'm curious about recent advancements in frame rates. Could you explain how high frame rate technology has evolved and what benefits it brings to modern cinema?	High frame rate technology has evolved significantly from the early days of 18 fps used by the Lumiere brothers to today's sophisticated systems. While 24 fps became the standard, digital projectors now run at 144 fps, though most films still show each frame multiple times. The MAGI process demonstrates that higher frame rates (120 fps) combined with bright projection and large screens can create an unprecedented sense of realism where the screen seems to disappear, becoming a window into another world. In Avatar: The Way of Water, frame rates alternate between 24 and 48 fps, with higher speeds used to smooth motion in action sequences and lower rates for dialogue scenes. This is achieved using TruCut Motion technology, which allows frame rate adjustment without creating the 'soap-opera effect'. The software enables filmmakers to control motion shot by shot in post-production, allowing subtle transitions between frame rates while maintaining suspension of disbelief and eliminating judder in fast pans, which is particularly important in 3D.	"['Future of Film: VFX Legend Douglas Trumbull\'s Plan to Save the Movies\nThe 72-year-old, who\'s best known for his work on \'2001: A Space Odyssey,\' says he\'s figured out how to win the battle against big TVs and smartphones — from a studio on his farm in the Berkshires\nA version of this story appeared in the Sept. 12 issue of The Hollywood Reporter.\nOn a sunny August day in the Berkshires of western Massachusetts, Douglas Trumbull, the 72-year-old visual effects legend, welcomes me to Little Brook Farm, the sprawling 50-acre property on which he lives and works with his wife of 13 years, Julia Trumbull, as well as an assortment of free-range donkeys, goats, chickens, roosters, cats and dogs. In addition to their home and animals, the compound also houses Trumbull Studios, a 10-building, state-of-the-art filmmaking facility that was financed with his proceeds from the IPO of IMAX Corp., where he once worked. ""We\'re not a movie lab in the sense that we process chemicals,"" says Trumbull of the operation. ""We\'re a movie lab in the sense that we\'re looking for the future of movies.""\nTrumbull drives me a short distance from his home to a full-size soundstage and escorts me into a screening room that he has constructed to meet his ideal specifications: a wide wall-to-wall and floor-to-ceiling curved screen, with surround sound, steeply rigged stadium seating and a 4K high-resolution projector. As I put on specially designed 3D glasses and settle into stadium seating, he tells me, with an unmistakable hint of nervousness, ""You\'re one of the first people on the planet to see this movie.""\nTen minutes later, the lights come back up and I sit in stunned silence. The short that I have just seen, UFOTOG (a blending of the words ""UFO"" and ""fotog,"" the latter slang for press photographer), is stunning not because of its story — we\'ve all seen movies about UFOs — but because it shows, as it was designed to do, what movies can look like if theaters, studios and filmmakers embrace the MAGI process through which Trumbull brought it to the screen: bigger, brighter, clearer and with greater depth-of-field than anything ever seen in a cinema before.\nAll of the aforementioned conditions are part of the MAGI equation, but the most essential element is the rate of frames per second at which a film is projected. In the beginning, the Lumiere brothers projected films at 18 fps, slow enough to result in the appearance of flickering — hence the early nickname for the movies, ""the flickers"" or ""the flicks."" That figure eventually increased to 24 fps, and has remained there, for the most part, ever since.\nIn 2012, Peter Jackson dared to release The Hobbit\'s first installment at 48 fps, which was supposed to create a heightened sense of realism, but which instead struck many as strange-looking and some even as nauseating. Many deemed the experiment a failure. Trumbull disagreed. He felt that if a digitally shot film was projected even faster — markedly faster, as in 120 fps, via a bright projector and onto a big screen — then the movie screen itself would seemingly disappear and serve effectively as a window into a world on the other side that would appear as real as the world in which one sits.\nTrumbull was more or less born into the VFX business. His father, Donald Trumbull, worked in special effects in Hollywood — ""He was on a fishing rod with a monofilament line moving the tiger\'s tail in The Wizard of Oz,"" his son says proudly — and the youngster quickly developed an interest in movie magic. He decided, however, to become an architect, which led him to take classes in illustration, and his talents at creating photorealistic art landed him a job at Graphic Films, which produced shorts for NASA and the Air Force and had been contracted to make a film for the 1964 World\'s Fair in New York.\nTo the Moon and Beyond featured a 70 mm circular image projected onto a dome screen and took viewers on a journey ""from the Big Bang to the microcosm in 15 minutes."" Two of the thousands who saw it were Stanley Kubrick, the filmmaker, and Arthur C. Clarke, the writer, who came away from it convinced that an A-level sci-fi film — which eventually became 2001: A Space Odyssey — was possible. Kubrick contracted Graphic Films to produce conceptual designs for the project, but, once it got off the ground, moved it to London, at which point 23-year-old Trumbull cold-called the director and got a job on the film. His greatest contribution to it was devising a way to create a believable ""Star Gate"" effect, representing ""the transformation of a character through time and space to another dimension."" Even though Kubrick alone claimed screen credit and an Oscar for the film\'s VFX, Trumbull instantly became a name in the business.\nA few years later, he made his directorial debut with Silent Running (1972), a well-received film that landed him deals at Fox, MGM and Warner Bros. — but all of them ""unraveled for stupid reasons."" By 1975, ""desperate because you can\'t live on development deals,"" he and Richard Yuricich proposed the creation of the Future General Corporation, through which they would try to identify ways to improve the technology used to make films. Paramount agreed to sponsor the endeavor — which, to them, was a tax write-off — in return for 80 percent ownership. Within the first nine months of its existence, Trumbull says, ""We invented Showscan [a manner of projecting films at 60 fps]. We invented the first simulator ride. We invented the 3D interactive videogame. And we invented the Magicam process [by which actors can perform in front of a blue screen, onto which nonexistent locations can be projected to create virtual realities]."" And yet, in the end, Paramount ""saw no future in the future of movies"" and failed to support their efforts, devastating Trumbull, who was under exclusive contract to the studio for the next six years. (The studio\'s one gesture that he did appreciate: loaning him out to Columbia to do the special effects for Close Encounters of the Third Kind.)\nTrumbull got out of his Paramount contract in 1979 thanks to Star Trek: The Motion Picture. The original effects team that had been engaged for the highly anticipated film couldn\'t handle the job, something the studio realized only six months before its long-scheduled Christmas release date. The studio begged Trumbull to take over, and he agreed to do so — provided he was paid a considerable fee and released from his contract. He got what he requested and, to the detriment of his health, also got the job done on time.\nNewly a free agent, Trumbull continued to take on special effects jobs for others — for instance, Ridley Scott\'s Blade Runner (1982) — but his primary focus was on directing a film of his own that would demonstrate the capabilities of Showscan. For the project, which he called Brainstorm, he secured a top-notch cast, led by Natalie Wood, and a major distributor, MGM. Production got underway and was almost completed when, on Nov. 29, 1981, tragedy struck: Wood drowned under circumstances that remain mysterious to this day. Since Wood had only a few small scenes left to shoot, Trumbull felt that he could easily finish the film, but MGM, which was in dire financial straits, filed what he deemed a ""fraudulent insurance claim"" because ""they wanted to get out of it.""\nWhen Trumbull, interviewed by Lloyd\'s of London, candidly shared his views, MGM turned on him and the project. ""I was afraid for my life,"" he says. ""My lawyer said, \'Don\'t get on any airplane that\'s been booked by MGM.\' So I said, \'Whoa, this is Hollywood? This is the movie industry? This is what I love?\' That was when I decided, \'I\'m not going to direct anymore. This is just too hard.\' I came here to the Berkshires a few months after we finally did get it finished, and I was almost PTSD. I was completely freaked out. And it took me several years of therapy and whatever to kinda get myself back together.""\nThen, in 1990, he was approached about making a Back to the Future ride for Universal Studios venues in Florida, Hollywood and Japan. Others had been unable to conquer it, but he made it happen — and in a groundbreaking way: ""It took you out of your seat and put you into the movie. You were in a DeLorean car. You became Marty McFly. You became a participant in the movie. The movie was all around you."" It ran for 15 years, he says, but was ""dismissed as a theme park amusement."" He felt it was something more. ""This was a moment where, for the first time in history, you went inside a movie."" Even though others failed to see larger possibilities, he says, ""That kinda kept me going for a long time because it validated that we could be here in the Berkshires and make breakthroughs that no one else was able to do in Hollywood or anywhere else.""\nIn 2009, James Cameron\'s Avatar, a digitally shot 3D production that grossed a record $2.8 billion worldwide, changed everything. Its success spurred, at long last, filmmakers to transition en masse to digital photography and theaters to transition en masse to digital projection — at which point Trumbull made a crucial discovery. He realized that digital projectors run at 144 fps — twice as fast as Showscan had been able to — but films were still being made at 24 fps, with each frame just flashing multiple times. ""Could we do a new frame every flash?"" he wondered. If so, he reasoned, it might just give people a reason to put down their smartphones, tablets and laptops and actually buy a ticket to see a movie in a theater.\nAfter years of work on his farm, Trumbull is finally ready to unveil UFOTOG. Its first public presentation will take place on Sept. 11 as part of the Toronto International Film Festival\'s Future of Cinema conference (at which Trumbull will also give a keynote address), and it will also screen days later at the IBC Conference in Amsterdam. At both venues, he says, his message will be rather straightforward: ""It\'s not rocket science, guys. It\'s just a different shape, a different size, a different brightness and a different frame rate. Abandon all that crud that\'s leftover from 1927. We\'re in the digital age. Get with it.""\nThe cost of these changes, he insists, will be rather negligible: projectors are already equipped to handle faster frame rates, and would require only slightly more data time and render time; theaters are already adopting brighter projectors that employ laser illumination, which uses a longer-lasting bulb to produce twice the amount of light; and theaters, he believes, will soon recognize that they are in the ""real estate business"" and that it is in their interest to have fewer total screens but more big screens, for which the public has demonstrated a willingness to pay a premium.\nTrumbull\'s main objective, though, is ""to show the industry what it is possible to do"" with MAGI. He says he\'s ""dying to show"" UFOTOG to filmmakers such as Jackson, Cameron and Christopher Nolan, whom he regards as kindred souls. But mostly, he wants to challenge the industry one more time, warning it, ""If you want people to come to theaters, you better do something different.""\nRead more from The Hollywood Reporter\'s ""Future of Film"" special report:', 'With 3D, High Dynamic Range, high frame rates and 4K capture, the sequel to the most successful movie of all time is intended to open a window into another world, writes Adrian Pennington.\nThirteen years after its release, Avatar remains the number one grossing film of all time at more than $2.8 billion worldwide. It is as widely lauded in the industry for its ground-breaking use of virtual production technology which has helped develop more sophisticated performance capture techniques and being able to view actors and CG assets live on set with a ‘virtual’ camera.\nAvatar: The Way of Water will have some way to go just to break even on its reputed $350-400m budget (much of which is calculated to be spread across three more follow-ups) but already the film is being talked about as a major Awards contender, notably in technical and craft categories including cinematography, production design, VFX and editing.\nCameron himself is no stranger to sequels, having written and directed two of the most successful of all-time: Aliens and Terminator 2: Judgment Day. Planning for a follow-up to Avatar began as soon as it became apparent just what a huge hit it had become. In spring 2010 the filmmaker, producer Jon Landau and other key heads of department met to review what aspects of the filmmaking process had worked best, and what they could have improved on. That exercise led to a decision to explore the story world further and resulted in 1500 pages of notes – too many to tell in one film alone.\nScreenwriters including Rick Jaffa and Amanda Silver (who both scripted The Planet of the Apes reboots) were hired to shape the notes into an ambitious series of four films. The process took months but Cameron wanted to have all four screenplays completed before moving on to production.\n“I wanted to map out all the stories and then get the economy of scale of capturing the actors across multiple films and then filming the live action,” Cameron said in the Disney film’s production notes. “The thinking was we could consolidate the different stages of production together—performance capture, live action and then post-production.”\nRather than create a host of new planets and moons, Cameron chose to continue to explore more new biomes and cultures of Pandora itself with the Avatar sequels. He reasoned that the moon could contain a range of landscapes—just like Earth. Pandora is after all a metaphor for our world.\nThe director has spent much of the intervening decade underwater himself pursuing environmental and exploration projects including setting a solo deep diving record of 35,787’ to the bottom of the Mariana Trench in 2012. Seemingly at home in the sea as on land, Cameron appears to want to take the legacy of filmmaker and aquatic pioneer Jacques Cousteau to another level.\nAvatar - The Way of Water: Production design\nProduction Designer Dylan Cole was tasked with designing everything relating to natural Pandora and the Na’vi, while PD Ben Procter was charged with focusing on the environments, vehicles and weapons of the film’s human industrial/military unit called Resources Development Administration (RDA).\nThat’s unusual in itself since most films have one production designer who manages everything that goes in front of the lens. “Dylan and Ben weren’t just designing for movie two—they were designing across the whole metanarrative,” Cameron said.\nCole gave the reef people (the Metkayina clan) a slightly different shade of blue than the Omatikaya, with a different physiology (large hands, wider chests and rib cages) and thick protuberances of fin-like cartilage beneath the skin.\nA mammalian species called Ilu was conceived as “a cross between a bi-plane version of a manta ray fused onto the long neck of a plesiosaur with the canard wings of a European jet fighter.”\nBy contrast, the creatures called skimwing are amphibious with a design inspired in part by the flying fish, but with a very different head shape and bright Pandoran wings. “The design can’t just look cool,” Cole said. “It needs to function as if it were real.”\nAvatar - The Way of Water: Underwater performance capture\nSetting much of the film underwater proved no barrier to attempting what no-one had done before - performance capture underwater. The key to it was to actually shoot underwater and at the surface of the water so actors were seen swimming, diving and emerging from the water realistically. “It looks real because the motion was real,” Cameron said.\nAt Manhattan Beach Studios in LA, the home of Cameron’s production company, Lightstorm, they built a water 120 ft x 60 ft and 30 ft deep to hold more than 250,000 gallons of water complete with a wave machine. This was the films’ underwater Volume.\n“We could create wave interaction with the creatures and people surfacing, getting hit by a wave and trying to say their lines and trying to breathe at the same time,” Cameron said.\nTwo six-foot diameter ship propellers drove a 10-knot current in the tank – an effect that looks faster on film thanks to the high frame rates. The tanks had windows for reference cameras to film underwater, while camera rigs also recorded the facial performance of the actors – underwater.\nFor the performance capture technology to work underwater the water itself needed to be clear. Initial plans to have the camera crew wear SCUBA gear while shooting in the tank had to be ditched because the breathing apparatus created disturbances.\n“Every one of those air bubbles is a little wiggling mirror,” Cameron related, “and the system that’s trying to read all the marker dots on the actor’s body can’t tell the difference between a marker dot and a bubble.”\nThat left only one option: Everybody who was working in the tank including actors and crew working a camera or holding a light had to hold their breath. They even employed free diving expert Kirk Krack to help. Kate Winslet apparently was able to hold her breath underwater for over seven minutes.\nPart of the solution involved covering the surface of the tank in small white balls that prevent overhead studio lights from contaminating the performance capture system below, while still allowing anyone below to surface safely through them should the need arise.\nAs soon as the characters emerge from the water the action continues ‘on the ground’ which necessitated a separate adjacent volume capture stage at Manhattan Studios ringed by cameras recording data in 360-degrees.\nOnce the performances were captured Cameron and the team then shot out the scenes including with characters clad in CG costume with digital props and CG environments, using virtual cameras on stage in Wellington, NZ.\nFusing the performance capture data from the underwater scenes seamlessly with the ‘air bound’ scenes was among the production’s trickiest problems.\n“The computer’s taking data from one volume, data from the other volume and in real time, integrating all that information. [It’s] showing me on my Virtual Camera people coming and going, swimming up, getting out onto a dock or diving in and swimming underwater. Obviously, the software to do that took quite a while to get worked out, but the end result was amazing.”\nAvatar - The Way of Water: Cinematography\nPerformance capture of the lead actors including Kate Winslet, Zoe Saldana, Sam Worthington and child actor Jack Champion began as early as September 2017 and ran for roughly 18 months, with Cameron and the cast working on scenes for all four of the sequels.\nRussell Carpenter ASC who shot Cameron’s True Lies and won the best cinematography Oscar for Titanic was tasked with designing an interactive lighting scheme that would combine CG with live action. Virtual lighting for the film took a full year in prep alone.\n“Our lighting that we did in the live action scenes had to merge seamlessly with whatever environment we were in, whether it was a dense jungle, or underwater, or in the RDA facilities,” said Carpenter.\nThe lighting team built a system of moving lights, which could be operated remotely, allowing them to make extremely precise strikes of light exactly where they should be.\nAcquisition was made natively (ie: not postprocessed) in 3D and 4K using Sony Venice cameras in their Rialto format (CineAlta Venice 3D) which enables the sensor block to be separated from the camera’s processing hardware – handy for stereoscopic pairing. Data was fed through a pipeline at various resolutions and frame rates including 3D 48fps in 2K and 4K, 3D 24fps in 2K and 4K, and 3D 24fps in HD for Cameron to monitor on set.\nIn turn, this necessitated viewing feeds of the live action on stages in Wellington, NZ from multiple 3D camera systems, simultaneously.\n“We are shooting stereoscopically from one 3D rig, often two rigs and sometimes three stereo pairs simultaneously and everything is processed instantly,” explained Geoff Burdick, SVP of Production Services & Technology for Lightstorm Entertainment.\nA screening room adjacent to the stages and a mobile projection pod built into a small trailer housing a Christie Digital 3D projector was set for projecting DCI compliant dailies.\nMassive amounts of data were being pushed around live on set. Burdick said: “We needed HFR and high res and everything had to be in 3D. This may not be not the science experiment it was when shooting the first Avatar but the sync for 3D at those higher frames and resolutions is still an issue.\n“In effect, we are seeing it in a theatrical environment instantly. We look at every set up, every rehearsal, every take and every feed live as it is shot on-the-fly in 2D and 3D. We are looking at back focus, actual camera focus and lighting. We can see the good with the bad at the point of acquisition and we address issues live.”\nThe production is also using a variety of additional Sony cameras including multiple Alpha mirrorless interchangeable lens cameras, PXW-Z450 and PXW-X320 camcorders, and the waterproof RX0 camera.\nAvatar - The Way of Water: HFR\nHigh frame rates alternate between 24 and 48 with the director dialling between the two in the final picture, using the higher speed to smooth the motion in faster action sequences and toning it down during slower dialogue scenes. This was likely done using a postproduction process called TruCut Motion with which Cameron recently remastered Avatar and Titanic.\nPixelworks’ motion grading software is a form of frame interpolation without the dreaded soap-opera effect that can make narrative movies look like video. The technology allows filmmakers to dial in the motion, with any source frame rate, shot by shot, in post. You can also defer the desired shutter speed until post. The software then ensures that these creative choices are delivered consistently to screens.\nThe process does not touch the colour grade but it can begin before colour grading, if required. Being able to subtly ramp up, to a wide pan of a dramatic landscape for example, without the viewer noticing the jump is happening retains suspension of disbelief without the judder of fast pans which are accentuated in stereo 3D.\nAvatar - The Way of Water: Editing\nWith Cameron, the editorial team led by Stephen Rivkin, ACE, John Refoua, ACE (both Avatar alumni) and the late David Brenner, ACE, selected the best performances for each moment of a given scene into ‘performance edits’, in preparation for the Virtual Cameras to create specific shots.\nThis was a technique that Cameron helped pioneer in 2009 and has since evolved into full blown virtual production enabling him to integrate CG versions of live action performances into a CG environment.\n“I could see everybody where they’re supposed to be, above or below the water, and I could talk to them over the diver address system,” Cameron said. “They were acting to real-time direction based on what I was seeing on the virtual camera.”\nOnce the Virtual Camera shots were edited into cut sequences, the CG shots and live action performances were turned over to Weta. In effect, the editing team were pre-editing sequences including all CG lighting, CG props, CG costumes, characters, creatures and environments ahead of the live action photography. It’s a hybrid form of the craft that merges the editing techniques of pacing and shot selection used to create wholly animated films with the flexibility of honing the story based on performance and multicam footage captured on set.\nA notable advance is the ability for Cameron to be able to select any moment in any one of multiple takes by an actor and use that to build the scene. Without performances being captured as data this would be impossible but means that in ensemble scenes, for example, he could select take 1 from one actor, take 10 of another and take 15 of a third – which represent the best performance of each actor – and build a scene seamlessly.\nAvatar - The Way of Water: VFX\nFor the first film, Weta had developed an image-based facial performance capture system, using a single SD head-rig camera to record the actors’ facial expression and muscle movements including eye-movement. This head-rig has been upgraded to HD for the sequel with two HD cameras designed to capture an even higher fidelity performance.\n“We look at every actor and every performance at a frame-by-frame level to make sure it matches [with the VFX],” said Senior VFX Supervisor Joe Letteri. “To me, it always comes down to the characters and the ability to be in the moment with them. Having that performance as detailed as possible [helps us] make sure that that’s what we see in the final shots.”\nEvery element of the lush exotic world needed to be created and rendered digitally. More than five years of R&D went into writing new software and methodology for the sequel which claims significant breakthroughs in lighting, shading, and rendering of scenes set under water.\nSolving, accurately, how water moves was the biggest challenge and ranged from the movement of water when a huge creature moves through it to when the tiniest raindrop lands on somebody’s forehead, trickles down into their eyebrow and down their face. It was an incredibly complex problem, but they weren’t starting from scratch. Cameron had first made water simulations on Titanic.\n“The beauty of it is, if you can solve water for this movie, you can do all water anytime until the end of time,” he said. “So, these tools become incredibly important for the effects industry at large.”\nWith a combination of live action, and CG, one of the most difficult things to conceive let alone execute is the interactive lighting. We were also shooting this in 3D at High Dynamic Range at 48 frames per second. Russell had to embrace all those things\nAvatar - The Way of Water: Costume Design\nAlthough the vast majority of Na’vi costumes were only going to be realised digitally on screen by Weta FX, many of the costumes and much of the jewellery were fabricated by costume designer Deborah Scott as real, tangible items.\n“One of the reasons that we’ve made the garments to completion is that the motion of the garment cannot be understood without having a whole piece,” she explained. “If something’s heavy or feathery or light or stringy, the way these things move in air, standing at a breeze, underwater, you really have to have the sample to see what happens to it.”\nLetteri added, “If someone walks and moves their arm, the cloth folds and wrinkles and bends with them. If their costume is made out of lots of little pieces like beads and strings or feathers or woven bits, that all has to go through this really detailed physical simulation to make it behave as if it were a real piece of cloth.”\nCameron said he’s determined to make sure the sequels are entertaining and laden with spectacle. At the same time, he’s imbued them with themes that are important to him—environmental stewardship and the importance of family.\n“With Avatar and where I’ve chosen to take the story and open up the landscape and the characters that I’ve brought in and some of the questions that get asked, I don’t feel there’s anything that I need to say cinematically that I will not say across these four films,” he said.\nAvatar - The Way of Water: Shooting out Avatar 2-3-4\nNot only has much of the performance capture been already made for some of the principal actors but many scenes have already been shot for Avatar 3 and some for the fourth in the series. Film productions usually shoot all the scenes on set before striking it but this time they were shooting scenes spanning later sequels so as not to have to rebuild the set.\nRight from the start, editing too had to take into account the story and character arcs spanning multiple movies, since any change to a character in The Way of Water would impact on Cameron’s global vision for the story span in Avatar 4 which is not due to hit screens until December 2026.\nA fifth film is planned but yet to be greenlit.\nRead more The best of behind the scenes']"	['<urn:uuid:eb791fdc-90ac-4945-8787-088ce9b5a690>', '<urn:uuid:2ea950b4-1b16-430f-85b2-33dae0aaa24b>']	open-ended	with-premise	verbose-and-natural	distant-from-document	three-doc	expert	2025-05-12T20:46:45.703947	33	163	4799
76	divine unity concept ibn arabi mirror metaphor wahdat ul wujood comparison	The concept of Divine Unity is explored through two main perspectives. In Ibn Arabi's poetic imagery, he speaks of beauty that gives light like lamps in darkness and describes divine manifestations that should be approached gradually to avoid being overwhelmed. This connects to his philosophical explanation using the mirror metaphor, where he compares God's essence being reflected in countless mirrors to the relationship between God and creation. This metaphor illustrates the Wahdat-ul-Wujood (Unity of Being) concept, suggesting that all things exist within God and there can be no separation between the Divine and its manifestations.	"[""The Tarjuman al-Ashwaq, by Ibn al-Arabi, tr. Reynold A. Nicholson, , at sacred-texts.com\n1. O camel-driver, turn aside at Sal‘ and halt by the bán tree of al-Mudarraj,\n2. And call to them, imploring their pity and grace, 'O my princes, have ye any consolation?'\n3. At Ráma, between an-Naqá and Ḥájir, is a girl enclosed in a howdah.\n4. Oh, her beauty—the tender maid! Her fairness gives light like lamps to one travelling in the dark.\n5. She is a pearl hidden in a shell of hair as black as jet,\n6. A pearl for which reflection dives and remains unceasingly in the deeps of that ocean.\n7. He who looks upon her deems her to be a gazelle of the\nsand-hills because of her neck and the loveliness of her gestures.\n8. ’Tis as though she were the morning sun in Aries, crossing the degrees of the zodiac at their farthest height.\n9. If she lifts her veil or uncovers her face, she holds cheap the rays of the bright dawn.\n10. I called to her, between the guarded pasture and Ráma, 'Who will help a man that alighted at Sal‘ in good hope?\n11. Who will help a man lost in a desert, dismayed, confounded in his wits, miserable?\n12 Who will help a man drowned in his tears, intoxicated by the wine of passion for those well-set teeth?\n13. Who will help a man burned by his sighs, distraught by the beauty of those spacious eyebrows?'\n14. The hands of Love have played at their will with his heart, and he commits no sin in that which he seeks.\n1. 'Halt by the bán tree of al-Mudarraj': he says, addressing the Divine messenger which calls the aspirations that seek to know and behold Him, 'Appear to me in the station of self-subsistence and lovingkindness gradually (###), not suddenly, lest I perish.'\n2. 'And call to them,' i.e. to the Divine Names.\n3. 'Ráma,' one of the stations of abstraction and isolation.\n'Between an-Naqá and Ḥájir,' between the white hill and the most inaccessible veil, to which the hearts of mystics can never attain.\n'A girl enclosed in a howdah,' i.e. the Essential Knowledge contained in the hearts of some gnostics.\n4. 'To one travelling in the dark,' i.e. to those who ascend and journey in the night (like the Prophet).\n6. God is beyond the reach of mental effort; He is revealed by Divine favour to a heart empty of all thoughts.\n8. 'Crossing the degrees of the zodiac,' etc., in reference to the magnification and glory which the seer feels in himself as he continues to contemplate her.\n10. 'Sal‘,' one of the stations of Divine sanctity.\n12. 'In his tears,' i.e. in the knowledge that comes of contemplation.\n'Wine,' i.e. every science that inspires joy and rapture in the human soul, e.g. the science of the Divine perfection.\n'Those well-set teeth,' i.e. the grades of knowledge of God.\n13. 'Those spacious eyebrows,' i.e. the station between the two Wazírs and Imáms. He alludes to the station of the Quṭb."", 'What Is Sufi Philosophy?\nSufi philosophy includes the schools of thought unique to Sufism, a mystical branch within Islam, also termed as Tasawwuf or Faqr according to its adherents. Sufism and its philosophical traditions may be associated with both Sunni Islam and Shia Islam. It has been suggested that Sufi thought emerged from the Middle East in the eighth century, but adherents are now found around the world. According to Sufism, it is a part of the Islamic teaching that deals with the purification of inner self and is the way which removes all the veils between divine and man. It was around 1000 CE that early Sufi literature, in the form of manuals, treatises, discourses and poetry, became the source of Sufi thinking and meditations. Sufi philosophy, like all other major philosophical traditions, has several sub-branches including metaphysics and cosmology as well as several unique concepts.\nThe emergence of Sufi thought is commonly linked to the historical developments of the Middle East in the seventh and eighth centuries following the life of Prophet Muhammad, and its development took place throughout the centuries after that. Between the tenth and twelfth centuries, Sufism became a widely spread discipline. One influential early writer on Sufi philosophy was Al-Ghazali (1058–1111). He discussed the concept of the self and the causes of its misery and happiness. By the end of the thirteenth century, Sufism had become a well-defined science of spiritual awakening throughout the Islamic World, an “Islamic Golden Age”. No important domain in the civilization of Islam remained unaffected by Sufism in this period. Several tariqahs (Sufi orders) were found. Also a class of notable Sufi philosophers, theologians and jurists such as Hankari, Ibn Arabi, Abu Saeed Mubarak Makhzoomi, led this age who trained and generated historical specimens of philosophers and geniuses now read worldwide such as Al-Ghazali, Avicenna, etc. An important mark made in the history of Sufi philosophy has been made by Abdul Qadir Jilani with his jurisprudence and philosophy of Sufism that made him define the Sufi orders. Jilani’s adopted order was Qadiriyya and the offshoot he started later became known as Sarwari Qadiri. Several other orders were also founded in this era. Sufis were influential in spreading Islam particularly to the furthest outposts of the Muslim world in Africa, India and the far East.\nMajor ideas in Sufi metaphysics have surrounded the concept of Wahdat or “Unity with God”. Two main Sufi philosophies prevail on this controversial topic. Wahdat-ul-Wujood (Unity of Being) essentially states that the only truth within the universe is God, and that all things exist within God only. Wahdat-ul-Shuhud (Apparentism, or Unity of Witness), on the other hand, holds that any experience of unity between God and the created world is only in the mind of the believer and that God and his creation are entirely separate. It is the state where there is no difference between God and human being who is trying to achieve a particular state i.e. ‘No One Except God’. The concept of Sufi Metaphysics was first deeply discussed in written form by Ibn Arabi in one of his most prolific works entitled Fusus al hikam where he applies deep analysis on the issue of Oneness through the metaphor of mirror. In this metaphor, al-Arabi compares an object being reflected in countless mirrors to the relationship between God and his creatures. God’s essence is seen in the existent human being, as God is the object and humans being the mirrors. Meaning two things, that since humans are mere reflections of God there can be no distinction or separation between the two and without God the creatures would be non- existent. When an individual understands that there is no separation between human and God they begin on the path of ultimate oneness. This metaphysics of Sufi philosophy is also narrated in the hadith: “Whoever recognized his self, undoubtedly recognized his Rab (Allah)”.\nMain article: Sufi cosmology\nSufi cosmology (الكوزمولوجية الصوفية) is a general term for cosmological doctrines associated with the mysticism or Sufism. These may differ from place to place, order to order and time to time, but overall show the influence of several different cosmographies such as the Quran’s testament concerning God and immaterial beings, the soul and the afterlife, the beginning and end of things, the seven heavens etc.; the Neoplatonicviews cherished by Islamic philosophers like Ibn Sina / Avicenna and Ibn Arabi or; the Hermetic-Ptolemaic spherical geocentric world. The cosmological plan, explains creation by successive emanation of worlds, as taught by Plotinus In Islamic Sufi terminology, these are also known as “Tanzalat-e-Satta” (7 steps). After Husayn ibn Ali, Abu Saeed Mubarak Makhzoomi was the one who discussed these levels in his Arabic book called Tohfa Mursala.\nMain article: Lataif-e-sitta\nDrawing from Qur’anic verses, virtually all Sufis distinguish Lataif-as-Sitta (“the six subtleties”) as: Nafs, Qalb, Sirr, Ruh, Khafi, and Akhfa. These lataif (singular: latifa) designate various psycho spiritual “organs” or, sometimes, faculties of sensory and suprasensory perception. They are thought to be parts of the self in a similar manner to the way glands and organs are part of the body.\nMain article: Ruh\nSome mystics named ruh as “batin” or “the esoteric self” or “qalb”. The Sufi, mostly believes in a strong soul as it brings him close to Divine. Soul is strengthened by the spiritual training given by the perfect spiritual guide. This eventually leads to the nearness to Allah. Also it is stated in hadith Qudsi that “Whoever recognizes his self, undoubtedly, recognized his Allah”. Hence, death is not the end but in fact it is the beginning to the eternal life which is only endowed to the soul and not to the body.\nNasma is the Sufi term for the subtle or Astral Body. It is not to be confused with the Ruh (spirit) which transcends both nasma and physical form.\nSufism demarcates the physical body from the Nasma. According to Sufi beliefs, physical body is a reflection of spiritual body or ‘batin’ or ‘ruh’, as also stated in one of the famous hadiths of Prophet Mohammad ,”Actions are but by intentions”.\nMain article: Hal and Maqam (State and Station)\nA haal is a state of consciousness, generally a product of spiritual practices, recognized in Sufism. Each haal (state) is associated with a maqaam (station) of along the spiritual path.\nA Manzil which literally means destination, is a terminology in Sufism, is a plane of consciousness. There are seven Manzils along the path to God. The Manzils are also parts of the Qur’an which help in protecting on sorcery.\nMain article: Hal and Maqam (State and Station)\nA maqaam is one’s spiritual station or developmental level, as distinct from one’s haal, or state of consciousness. This is seen as the outcome of one’s effort to transform oneself, whereas the haal is a gif.\nConcepts in Gnosis\nMain article: Baqa Bi’llah (Subsistence with God)\nA person’s baqaa, which literally means “permanency”, is a term in Sufi philosophy which describes a particular state of life with God and is a manzil or adobe that comes after the station of fanaa. Inayat Khan writes in his book A Sufi message of spiritual liberty, “The ideal perfection, called Baqa by Sufis, is termed ‘Najat’ in Islam, ‘Nirvana’ in Buddhism, ‘Salvation’ in Christianity, and ‘Mukhti’ in Hinduism. This is the highest condition attainable, and all ancient prophets and sages experienced it, and taught it to the world. Baqa is the original state of God. At this state every being must arrive some day, consciously or unconsciously, before or after death. The beginning and end of all beings is the same, difference only existing during the journey.\nMain article: Yaqin (Certainty)\nYaqeen is generally translated as “certainty”, and is considered the summit of the many maqaams (stations) by which the path of walaya (sometimes translated as Sainthood) is fully completed.\nMain articles: Haqiqat and Haqq (the Truth), Haqiqa (the Genuine) and What Lies Beyond\nHaqiqa or Haqiqat is the Sufi term for the supreme Truth or absolute Reality.\nMain article: Ma’rifat\nMarifa (or alternatively ‘marifah’) literally means knowledge or recognition. According to mysticism, the truth behind creation of man and essence of all prayers is the recognition of Allah. The term is used by Sufi Muslims to describe mystical intuitive knowledge, knowledge of spiritual truth as reached through ecstatic experiences rather than revealed or rationally acquired.\nMain article: Ihsan (Perfect Goodness)\nIhsan is an Arabic term meaning “perfection” or “excellence.” Ihsan is the goal or aim of Sufi practices and is achieved when a seeker surrender and submit himself completely to the will of Allah.\nAdapted from Wikipedia, the free encyclopedia']"	['<urn:uuid:add6b7dd-5635-4980-aba3-9c67229897bb>', '<urn:uuid:130c8104-2b63-4a7a-b8d9-40573062d42d>']	open-ended	with-premise	long-search-query	similar-to-document	multi-aspect	expert	2025-05-12T20:46:45.703947	11	95	1953
77	As an economics scholar, I wonder how utilitarian ethics and market morality compare?	Utilitarian ethics and market morality share some common foundations but differ in key aspects. The utilitarian view focuses on providing the greatest good for the greatest number, encouraging efficiency and profit maximization but potentially leading to biased resource allocations when some affected parties lack representation. Market morality, while also built on human actions and choices, approaches ethics through the lens of voluntary exchange and trade decisions. Markets operate on the premise that free economic decisions bring about the most good through an 'invisible hand,' but this assumes moral agents have considered the ethical consequences before trading. Both approaches deal with resource allocation and human choices, but utilitarianism provides a clearer ethical framework while market morality relies more on individual judgment in economic decisions.	['Ethics: A set of rules or principles that defines right and wrong conduct.\nEthics commonly refers to a set of rules or principles that defines right and wrong conduct. Right or wrong behavior, though may at times be difficult to determine. Most recognize that something illegal is also unethical. What about questionable “legal” areas? Recent literature is filled with management practices at companies such as Enron, Arthur Andersen and Madhavpura Mercantile Co-operative Bank (MMCB) in India. What executives at these companies did may be questionable, and some actions may have been illegal; the larger issue is what implications have such actions created? For many, the aftermaths of these corporate scandals resulted in a lack of trust in management. People are questioning how much unethical actions could have gone unnoticed if proper controls were in force in the organization. Moreover, the public is now examining the unethical cultures that were pervasive in these organizations.\nUnderstanding ethics may be difficult, depending on the view that one holds of the topic.\nThree Views of Ethics>>>\nUtilitarian view of ethics:\nRefers to a situation in which decisions are made solely on the basis of their outcomes or consequences. The goal of utilitarianism is to provide the greatest good for the greatest number. On one side, utilitarianism encourages efficiency and productivity and is consistent with the goal of profit maximization. On the other side, however, it can result in biased allocations of resources, especially when some of those affected lack representations or voice.\nRight view of ethics:\nRefers to a situation in which the individual is concerned with respecting and protecting individual liberties and privileges, including the rights to privacy, freedom of conscience, free speech, and due process. The positive side of the rights perspective is that it protects individuals’ freedom and privacy. But it has a negative side in organizations; It can present obstacles to high productivity and efficiency by creating an overly legalistic work climate.\nTheory of justice view of ethics:\nRefers to a situation in which an individual imposes and enforces rules fairly and impartially. A manager would be using a theory of justice perspective in deciding to pay a new entry level employer $ 1.50 an hour over the minimum wage because that manager believes that the minimum wage is inadequate to allow employees to meet their financial commitments. Imposing standards of justice also comes with pluses and minuses. It protects the interests of those stakeholders who may be underrepresented or lack power, but it can encourage a sense of entitlement that reduces risk taking, innovation, and productivity.\nPeople who lack a strong moral sense, however, are much less likely to do the wrong things if they are constrained by rules, policies, job descriptions, or strong cultural norms that discourage such behaviors. For example, someone in your class has stolen the final exam and is selling a copy for $50. You need to do well on this exam or risk failing the course. You expect some classmates have bought copies, which could affect any results from the exam because your professor grades on a curve. Do you buy a copy because you fear that without it you’ll be disadvantaged do you refuse to buy a copy and try your best, or you report your knowledge to your instructor?\nCode of ethics: A formal document that states an organization’s primary values and the ethical rules it expects managers and operatives to follow.\nThe example of the final exam illustrates how ambiguity about what is ethical can be a problem for managers. Codes of ethics are an increasingly popular tool for attempting to reduce that ambiguity. A code of ethics is a formal document that states an organization’s primary values and the ethical rules it expects managers and operative employees to follow. Ideally, these codes should be specific enough to guide organizational personnel in what they are supposed to do yet loose enough to allow for freedom of judgment. Nearly 90 percent of Fortune 1000 companies have a stated code of ethics and these codes extend into the global arena.\nIn isolation ethics codes are not likely to be much more than window dressing – Enron had a code of ethics statement. Their effectiveness depends heavily on whether management supports them and ingrains them into the corporate culture, and how individuals who break the codes are treated. If management considers them to be important, regularly reaffirms their content, follows the rules themselves, and publicly reprimands breakers, ethics codes can supply a strong foundation for an effective corporate ethics program.', 'Morality of Markets\nHow does one go about connecting the idea of morality with markets? The Latin root word moralis means proper behavior of a person in society, literally pertaining to manners. But a market is not a person. It is a place. The word market, which first started to appear in our vocabulary in the 12th century was initially used in the sense of a meeting at a fixed time for buying and selling livestock and provisions. The Latin word mercatus is a root word meaning trading, buying and selling. When you try to put the two words together, see what they have in common, on the surface, the commonalities are not obvious.\nHowever, upon reflection, there is a common denominator between morality and markets. Both are built on the same foundation, human action. Economics by definition is a social science that studies how individuals, governments, firms and nations make choices on allocating scarce resources to satisfy their unlimited wants. In our economy, called a market economy, we allocate scarce resources such as land, labor, capital, and technology through the marketplace. A market is a place where vast numbers of human actions occur daily. Meanwhile, morality is the determination of what human actions should be done and what should not be done. It is the distinction between what is right and what is wrong. Using these deeper and more specific definitions of the terms morality and markets, the relationship between the two is much easier to identify.\nEthics is a branch of philosophy dealing with what is morally right or wrong. It is a subset of morality, a study of how we can go about judging our human actions. According to the economist Henry Hazlitt, reflection reveals that economics and ethics:\n“…are, in fact, intimately related. Both are concerned with human action, human conduct, human decision, human choice… There is hardly an ethical problem, in fact, without its economic aspect. Our daily ethical decisions are in the main economic decisions, and nearly all our daily economic decisions have, in turn, an ethical aspect.”\nEconomics, in our classroom experience of it, separates morality from science in neat compartments. From the very beginning, in an academic study of economics, discussion of “what should or ought to be” is called normative economics. While the study of “what is,” the way things are in the real world, is called positive economics. We define terms and conveniently move on, concentrating our time and effort on the positive side. We do this because this kind of approach is more pragmatic. We do this because it is easier to create mathematical models that predict what will happen as a result of specific human actions in terms of dollar value. There are no mathematical equations capable of telling us whether our human actions are right or wrong.\nIn our schools and universities, embracing this “positive” view of economics, we have appointed Neoclassical economics as the best and most useful way to study the real world. Neoclassical economics, the dominant and most current economic paradigm, is an approach to economics that relates supply and demand to an individual’s rationality and his or her ability to maximize utility or profit. Using economics, for example, you can very accurately, using mathematical models, predict how much a lifesaving cancer drug will cost to develop in two years’ time and what price you should charge per dose when you are done to make a profit of a certain percent. However, while this kind this kind of approach is efficient and effective in explaining and interpreting how things work, there is a cost. Neoclassical economics provides little insight into the morality of such an action. For example, if our cancer patient is a single mother with four children and carries no health insurance, we are provided no insight into whether she should be charged at all. As Oscar Wilde wrote, “Now-a-days, people know the price of everything, but the value of nothing.” Or, in economic speak, the analysis provided by Neoclassical economics today is most often “value free.”\nIt was Adam Smith who wrote: “Man is an animal that makes bargains: no other animal does this – no dog exchanges bones with another.” Exchange is a basic and necessary part of our existence. By itself, there is nothing wrong about that. In economic terms, the simple definition of a market is a place where exchange takes place. A market is a place where we trade goods and services. On the surface, it sounds innocuous. However, from a philosophical perspective, wherever there is human action, there must be preceding it, an exertion of will. This will is a first cause, or the primary force propelling any human action. Without this first cause, a human act would not come into being.\nThis is a cause for concern. One must wonder whether we are willingly or unwittingly committing economic acts? The famous economist John Maynard Keynes wrote: “Practical men who believe themselves to be quite exempt from any intellectual influence, are usually the slaves of some defunct economist. Madmen in authority, who hear voices in the air, are distilling their frenzy from some academic scribbler of a few years back.” If we are simply buying goods, promoting economic policies, and designing market structures “value free,” devoid of moral considerations, without thought, without awareness of detail, then are we not following the scribbles of “madmen in authority?” If so, we commit the sin of commission daily and with profound effects.\nAny trade or exchange results from a human decision. It is a willful act, voluntary in a free enterprise system. That means we are free every time we make a purchase to evaluate it from a moral perspective. Consider a list of market exchanges that originate from an article in the Atlantic Monthly listed below. As you read the list, form an opinion on whether you make the trade based on your personal philosophical, ethical, moral, religious beliefs:\no A prison-cell upgrade: $90 a night. In Santa Ana, California, and some other cities, nonviolent offenders can pay for a clean, quiet jail cell, without any non-paying prisoners to disturb them. Is it right for convicted felons with extra cash to buy comfort?\no Access to the carpool lane while driving solo: $8. Minneapolis, San Diego, Houston, Seattle, and other cities have sought to ease traffic congestion by letting solo drivers pay to drive in carpool lanes, at rates that vary according to traffic. Is it right for individuals to buy the right to inconvenience others?\no The services of an Indian surrogate mother: $8,000. Western couples seeking surrogates increasingly outsource the job to India, and the price is less than one-third the going rate in the United States. Is it right to outsource work to cheaper third world locales?\no The right to shoot an endangered black rhino: $250,000. South Africa has begun letting some ranchers sell hunters the right to kill a limited number of rhinos, to give the ranchers an incentive to raise and protect the endangered species. Is it right to sell the right to hunt an endangered species, in limited numbers, if the money raised from killing a few saves the herd?\no Your doctor’s cellphone number: $1,500 and up per year. A growing number of “concierge” doctors offer cellphone access and same-day appointments for patients willing to pay annual fees ranging from $1,500 to $25,000. Is it right if a single mother on welfare with four children and no discretionary income to have less access to health care than a healthy and single twenty-something who happens to have a great job and an extra $25,000?\no The right to emit a metric ton of carbon dioxide into the atmosphere: $10.50. The European Union runs a carbon-dioxide-emissions market that enables companies to buy and sell the right to pollute. Is it right to let a company pay to pollute?\no The right to immigrate to the United States: $500,000. Foreigners who invest $500,000 and create at least 10 full-time jobs in an area of high unemployment are eligible for a green card that entitles them to permanent residency. Is it right to put entrepreneurs at the end of the immigration line?\no Sell space on your forehead to display commercial advertising: $10,000. A single mother in Utah who needed money for her son’s education was paid $10,000 by an online casino to install a permanent tattoo of the casino’s Web address on her forehead. Temporary tattoo ads earn less. Is it right to use the human body as a permanent billboard?\no Serve as a human guinea pig in a drug-safety trial for a pharmaceutical company: $7,500. The pay can be higher or lower, depending on the invasiveness of the procedure used to test the drug’s effect and the discomfort involved. Is it right to use human beings in medical tests even if they are willing and the drug tested saves lives?\no Fight in Somalia or Afghanistan for a private military contractor: up to $1,000 a day. The pay varies according to qualifications, experience, and nationality. Is it right to pay others to defend our homeland?\no Stand in line overnight on Capitol Hill to hold a place for a lobbyist who wants to attend a congressional hearing: $15–$20 an hour. Lobbyists pay line-standing companies, who hire homeless people and others to queue up. Is it right to pay for political access?\nThe simplistic notion that economics can be neatly separated into positive and normative camps is a fallacy. Our current free enterprise system is founded upon the idea that the ability to trade freely, to freely make economic decisions, is the highest moral order. By allowing people to freely choose, without coercion, they will naturally bring about the most good, with the market guided by an “invisible hand.” However, this perspective implies the moral agent, the consumer, invested the time to consider moral consequences before the trade. It implies a will to do the right thing at the expense of inconvenience. Or to quote Edmund Burke, “All that is necessary for the triumph of evil is that good men do nothing.”\nIn response, another way to view the economy is as a social ethical structure. (See the graphic entitled “Our Economy as a Social Ethical Structure” below.) Value judgments are embedded in all economic systems (capitalist, free market, socialist, planned communism, or fascist) and in every economic decision and corresponding action taken. Our individual moral perspective, or absence of one, is embedded in every exchange. Our economic system is driven by a series of crucial daily moral decisions that largely occur automatically and by default. However, a simple default decision, taken without measure is not a substitute for moral evaluation. Whether we desire it or not, each exchange or purchase we make implies a moral order. Trade by trade, we build an economic order. The whole is greater than the sum of its parts. Or as from the Gospel of Luke 6:35, “When good men do nothing, they get nothing good done. To be good, one must do good. The Lord commands his people to do good.”\nNext, consider the fundamental economic questions that every economic order must answer before individual actors can get to work. What to produce? How to produce it? How much to produce? Who gets what is produced? In economics, we say there are no free lunches. This means that to do anything we must expend time, money and resources. Every good we make or thing we buy requires resources, costs money, and has a cost. In a world of unlimited wants, and scarce resources, by definition, every economic decision taken is a tradeoff. If we decide to make guns, we have less resources available for butter. If we invest in automated teller machines and industrial robots, employers have less money available to hire unskilled laborers. If we commit more farmland to raise more beef cattle, we have less capacity to grow fruit and vegetables. If we pay our top executives multi-million dollar bonuses on top of six or seven figure salaries, there is less money available in the budget to raise the minimum wage.\nAfter we answer these fundamental questions, we go on to address some new and critical macro problems that arise naturally out of our individual decisions. Problems that an economy must address include allocation, distribution, scale, and the quality of relationships. Allocation is about deciding how natural and human resources will be employed in the production of our various goods and services. Do we utilize a free market economy or a planned state system? Distribution is about deciding who gets what of all the things produced. Do we decide everyone is paid equally or by how productive they are in their job? Scale is about preventing problems caused when market activity gets so large that it threatens the stability of the system (economic or ecological). Do we outlaw the production of automobiles if we determine that carbon emissions from vehicles is the root cause of global warming? Finally, quality of relationships is about ensuring trust and cooperation between persons involved in economic activity. Do we legislate morality in the workplace, in the form of gender laws, affirmative action, and prohibitions about practicing religion at work in the name of harmony? In his article Morality of Markets, Kenneth Melchin states: “For economic activity to function in service of human relations, participants need habits and virtues for fostering trust, cooperation, and good will, and this must be cultivated and supported by civil society.” How we address or solve these core problems determines the fundamental quality of our economic system.\nAfter making our choices in the first three phases, we further refine the elements of markets. Think of this as how we manage our particular economic system. These four elements include definition of laws, provision of essential goods and services, morality of individuals and groups, and our view of civil society. They work together to complete the Social Ethical Structure that animates our system of exchange.\nThe four elements of markets are what we most often take for granted. First, the definition of laws is about prohibiting certain activities because they are too abusive to be allowed or because there are better or worse ways of doing that action and thus regulation makes some ways of acting illegal. For example, we outlaw slavery, human cloning, or operating a restaurant without a food license. All economic systems, capitalist, communist, libertarian, or socialist have practices that they want to outlaw, the question is which ones. Second, the provision of essential goods and services is about identifying which goods and services are so essential that they ought to be provided to everyone regardless of their ability to buy them. Here a libertarian may say that police or national defense are two examples of a very short list. Meanwhile, to a liberal democrat, universal healthcare and social security are obvious essential goods. Third, the morality of individuals and groups is about our ideas on the true definition of morality. Who and how will we define our beliefs about what is right behavior and what is wrong behavior? In a communist society, the church is outlawed, a critical plank of Marxian ideology. Each economic system defines its own proper moral code. Fourth, the civil society is about how as a community of citizens we are linked by common interests and collective activity. Here, for example, a Libertarian may view social arrangements such as important but voluntary agreements. From a more liberal perspective, critical social arrangements are viewed as compulsory. Fundamentally, each element works to create and define our moral universe, forming a Social Ethical Structure.\nIt is our responsibility to ensure that we take the time to build this structure properly, one brick at a time. There are no “value free” decisions available to us in a free enterprise system. There are no actions to take without tradeoffs or consequences. There are only free human actions based upon our freewill that lead to inevitable moral consequences.']	['<urn:uuid:aa08c0a4-51cf-4c47-a378-e6251bac67ae>', '<urn:uuid:aaa2bfd8-f6c1-4424-8c09-8672037cf85b>']	open-ended	with-premise	concise-and-natural	distant-from-document	comparison	expert	2025-05-12T20:46:45.703947	13	123	3431
78	daily stress relief techniques scientific benefits	Several scientifically-supported techniques can help manage daily stress. Exercise has been proven beneficial for reducing stress and its long-term health effects, with specific focus on activities like yoga and Tai Chi. Other effective strategies include taking walks, keeping a journal, practicing relaxation exercises, and meditation. The act of gratitude has been shown through studies to help manage stressful emotions. Additionally, proper organization of daily tasks, maintaining self-care routines, and using reframing techniques to view stressors more positively can significantly reduce stress levels. These techniques work by helping to break down the negative association between mental health issues and stress.	['It may appear that there is no way around your stress.\nThe bills won’t quit coming, there will never be more hours in the day for all your tasks, and your job will always be demanding.\nBe that as it may, you have more control than you think. Actually, the simple acknowledgement that you are in control of your life is the basis of managing your stress.\nManaging stress is all about assuming responsibility: assuming responsibility for your thoughts, your feelings, your schedule, your health, and the manner in which you take care of issues.\nAn ideal goal is a healthy lifestyle, with time for work, socializing, relaxing, and fun – in addition to the flexibility to hold up under tension and address challenges head-on.\nRecognize the sources of stress in your life\nStress management begins with distinguishing the sources of stress in your life.\nThis is not as simple as it sounds. Your actual sources of stress are not always always what they seem and it is all too simple to even think about overlooking your own stress-inducing thoughts and practices.\nOf course, you may realize that you are continually stressed over work, but eliminating that stress is not easy.\nTo distinguish your actual sources of stress, take a gander at your habits, thought processes, and actions:\nDo you clarify stress as transitory (“I have a million things going on this moment”) despite the fact that you cannot recollect the last time you sat and relaxed?\nDo you characterize stress as a necessary piece of your work or home life (“Things are always insane around here”) or as a piece of your character (“I have a ton of anxious energy”)?\nDo you blame your stress on others or general circumstances, or view it as altogether ordinary and unexceptional?\nUntil you acknowledge the part you play in your stress, your stress level will remain excessive.\nFor what reason is it so important to manage stress?\nIf you are living with significant levels of stress, you are putting your whole being in danger.\nStress unleashes destruction on your mental well being, as well as your physical well being.\nIt limits your capacity to think, work, and appreciate life. It may appear as though there is no way around stress, and it is true stress will always exist, but there are ways to cope with it.\nStress management can help you to break the hold stress has on your life, so you can be more joyful, successful, and more flexible.\nBe that as it may, stress management is not one-size-fits-all. That is the reason it is important to try and discover what works best for you.\nTips for Stress Management\nThe accompanying seven tips are from The American Psychological Association to help people develop a stress management plan:\nUnderstand your stress\nStress manifests differently for everyone, but by understanding what stress is like for you, you can be more prepared to manage it when the time comes.\nDistinguish your stress sources\nWhat makes you stressed? Be it work, family, change, or any of the other potential triggers, knowing what causes it can help you deal with it.\nFigure out how to perceive stress signals\nWe all perceive stress differently so it is important to know about your individual stress indications.\nWhat are your internal alerts? Headaches, stomachaches, or trouble sleeping are just a few examples of symptoms of stress.\nKnow your stress strategies\nWhat is your go-to strategy for calming down and relaxing?\nThese practices are typically learned over the years and can evolve as an individual learns more about themselves.\nActualize healthy stress management strategies\nIt is acceptable to be aware of any current unhealthy practices so you can change them to a healthy alternative.\nFor instance, if gorging salty snacks is your current go-to, you might want to reduce that and instead, call a friend to talk through your circumstance.\nYou can exercise to release stress, recording your Garmin Stress score first.\nMake self-care a priority\nWhen we make time for ourselves, we put our well being before anything else.\nThis can feel egotistical at first, yet it resembles what to do when a plane goes down – we should put our own breathing apparatus on first before we can help others.\nThe least difficult things that enhance our well being, for example, rest, nourishment, personal time, and exercise are regularly the ones neglected.\nRequest support when required\nIf you are feeling overwhelmed, ask for help.\nThere is nothing wrong with asking for support when in need. Plus, talking to others can also help diminish stress.\nStrategies for stress management\nUse reframing: This is a psychological action that includes taking a look at a stressor or negative circumstance and thinking about it (all things considered and honestly) in a positive or unbiased way.\nOrdinarily, when something awful transpires, we get enveloped with the negative.\nBe that as it may, this is a somewhat untrue view as there other things to look at when something happens.\nStrength tip: Use your strengths of basic reasoning to view any possible advantages to the stressor.\nIn some ways, this is like the old saying, “find the silver lining of every cloud.”\nImprove your organization: Stress can be overseen through excellent organization.\nBegin making your day to day agendas, dust off your calendars, and make a move by sorting out your weekly to do lists.\nStrength tip: The basic strength you use when organizing is foresight.\nYou can figure out how to plan efficiently and utilize your foresight to take stock every morning on what you need to get done.\nLearn to unwind: Where might a stress be without ways to unwind? Scores of studies have shown a significant connection to relaxing to overall stress levels.\nStrength tip: Unwinding includes the ability to give your body time to relax.\nThis can be through breathing exercises, yoga, or simply by giving yourself time to take a long bath..\nFoster an easy-going mindset: The strength of being laid back has been shown to have an amazing impact on stress.\nThe individuals who are profoundly sympathetic to themselves, as well as other people, have a far less possibility of developing high stress levels.\nOne group of analysts, driven by Loren Toussaint, clarifies that being flexible takes the negative association between dysfunctional behavior and stress and makes it almost zero.\nWithout the ability to let things go, we experience stress in a more crude, unblocked way.\nStrength tip: Find yourself dubious how to be more lenient? Start small, beginning with easily overlooked details.\nWhen someone cuts you off in rush hour gridlock or appears to overlook your “welcome” at work, adopt a particular strategy of letting go.\nPractice self care: Thanks to the most recent many years of research on self care, there is currently a great measure of research showing care approaches help with stress from numerous points of view.\nTaking care of yourself is of the utmost importance.\nStrength tip: Try “the careful delay” which is a basic exercise in which you pause your thoughts when experiencing a stressor and mindfully relax for 15 seconds, followed by one inquiry for yourself — “how can I utilize one of my character strengths at this moment?”\nMake positive moves with any character strength that springs up.\nRelease your appreciation: The act of appreciation largely affects stress. Studies have shown that it is useful to stay aware of what you are grateful for when confronting alarming feelings because they can help ground you.\nStrength tip: Start by setting up a daily practice. Record three things you are thankful for every night and why you are thankful for them.\nIf you keep it up over a long period of time, you will have many instances of appreciation to look over whenever feeling stressed.\nFrequently Asked Questions\nWhat are some examples of stress management techniques?\nTake a walk\nBegin an exercise regime\nKeep a journal\nWhat are some emotional signs of stress?\nDepression or anxiety\nDifficulty sleeping or sleeping too much\nAnger and irritability\nFeeling overwhelmed, unmotivated, or unfocused\nRacing thoughts and/or feeling overwhelmed\nTrouble with your memory or concentration\nMaking bad decisions\nWhy is managing stress important?\nResearch has shown that high stress levels can lead to decreased brain functioning, lowered immune system, and trouble preparing for future situations. It can overall affect your ability to lead a successful life.\nFor more information on stress management, check out these books:\nThis book acts as a guide to help the reader understand and cope with their stress. Through given information, writing reflections, and mindfulness exercises, this book aims to support someone through their stress.\nThis book is intended to provide an opportunity for individuals to learn techniques to better help them cope with stress.\nIt includes sections that describes the how’s and why’s of stress and a workbook section for the reader to develop their own sense of managing their stress.\nSide Note: I have tried and tested various products and services to help with my anxiety and depression. See my top recommendations here, as well as a full list of all products and services our team has tested for various mental health conditions and general wellness.\nEpstein, R. (2011, September/October). Fight the frazzled mind. Scientific American Mind. www.scientificamerican.com/article.cfm?id=fight-the-frazzled-mind\nFreidlin, P., Littman-Ovadia, H., & Niemiec, R. M. (2017). Positive psychopathology: Social anxiety via character strengths underuse and overuse. Personality and Individual Differences, 108, 50-54. DOI: dx.doi.org/10.1016/j.paid.2016.12.003\nGander, F., Proyer, R.T., Ruch, W., & Wyss, T. (2013). Strength-based positive interventions: Further evidence for their potential in enhancing well-being and alleviating depression. Journal of Happiness Studies, 14, 1241–1259. doi:10.1007/s10902-012-9380-0\nHames, J. L., & Joiner, T. E. (2012). Resiliency factors may differ as a function of self-esteem level: Testing the efficacy of two types of positive self-statements following a laboratory stressor. Journal of Social and Clinical Psychology, 31(6), 641-662.\nNiemiec, R. M. (2014). Mindfulness and character strengths: A practical guide to flourishing. Cambridge, MA: Hogrefe.\nNiemiec, R. M. (2017). Character strengths interventions: A field-guide for practitioners. Cambridge, MA: Hogrefe.\nPeterson, C., & Seligman, M. E. P. (2004). Character strengths and virtues: A handbook and classification. New York: Oxford University Press and Washington, D.C.: American Psychological Association.\nRosmarin, D. H., Pargament, K. I., Pirutinsky, S., & Mahoney, A. (2010). A randomized controlled evaluation of a spiritually-integrated treatment for subclinical anxiety in the Jewish community. Journal of Anxiety Disorders, 24, 799-808.\nSeligman, M. E. P., Steen, T. A., Park, N., & Peterson, C. (2005). Positive psychology progress: Empirical validation of interventions. American Psychologist, 60, 410–421. https://dx.doi.org/10.1037/0003-066X.60.5.410\nToussaint, L., Shields, G. S., Dorn, G., & Slavich, G. M. (2016). Effects of lifetime stress exposure on mental and physical health in young adulthood: How stress degrades and forgiveness protects health. Journal of Health Psychology, 21(6), 1004-1014. doi:10.1177/1359105314544132J\nWood, A. M., Linley, P. A., Matlby, J., Kashdan, T. B., & Hurling, R. (2011). Using personal and psychological strengths leads to increases in well-being over time: A clongitudinal study and the development of the strengths use questionnaire. Personality and Individual Differences, 50, 15-19.', 'Chronic stress can have serious emotional, psychological, and physiological effects that contribute to or exacerbate many health problems. In fact, the negative health effects of chronic stress are like those of eating a poor diet or not getting enough physical activity. That said, managing stress, including getting enough sleep, is often overlooked as a key component of good health.\nThe effects of stress and the importance of stress management is the topic of my Health & Fitness column in the Aiken Standard this week.\nThe word “stress” is typically used to indicate both the feeling of being “under a lot of stress” as well as the things that cause that feeling. The events and situations that cause stress are properly called stressors, which lead to a stress responsethat includes consequences we feel as well as physiological changes we may not notice.\nThe immediate effect of a stressor is called the “fight or flight” response since it prepares the body to deal with a dangerous situation. A classic example of this is a caveman who encounters a saber-toothed tiger, clearly a stress-inducing event.\nThe sympathetic nervous system is immediately activated, which raises heart rate and blood pressure to pump more blood to the muscles. Additionally, stored fat and carbohydrate fuels are broken down as fuel for the muscles. The adrenal glands release catecholamines (adrenaline) and cortisol (the stress hormone) to prolong and enhance this effect. This coordinated response makes sure the caveman’s body is ready for action. After the danger passes, everything returns to normal.\nThis physiological response is appropriate for major events like saber-toothed tiger encounters, but not for less perilous stressors like being stuck in traffic, pressure at work or home, and other personal and family issues. But the body responds with the same increase in blood pressure and hormones to them all. Unlike a rare saber-toothed tiger encounter, these stressors tend to occur on a daily basis, leading to continuous stress response.\nThe increase in hormones can lead to high blood pressure, obesity, type 2 diabetes, and other chronic diseases. This is partly due to elevated levels of cortisol, a hormone that plays a role in storing fat and increasing appetite. While elevated cortisol during exercise (including running away from a saber-toothed tiger) is normal, chronic overproduction can have negative effects.\nWhile it is impossible to avoid all stress in life, minimizing stressors and managing the way you respond to stress can have important benefits. To the extent that it is possible, avoiding stressful situations through better time management, setting realistic expectations for ourselves and with others, and learning to say “no” are common recommendations.\nLearning how to deal with stressors to avoid the negative effects of stress is also important. Techniques that can be implemented in the heat of a stressful moment include taking a break from the situation, listening to calming music, and progressive relaxation. Even taking a deep breath can help.\nExercise has long been recognized as beneficial for reducing stress and the long-term effects of stress on your health. This includes doing something active during a stressful situation and exercising regularly to improve the way your body responds to stress. While all forms of exercise seem to work, much research and practice has focused on specific types of exercise including yoga and Tai Chi.\nOther effective strategies traditionally include meditation and relaxation exercises. More and more research shows that getting enough sleep is also critical for reducing stress and the impact it has on your health. Eating a healthy diet can reduce the effects of stress as well.\nThe bottom line is that a healthy lifestyle includes stress management as well as a good diet and regular activity. Since all three are essential for good health, it would be wise to eat smart, move more, and chill out!']	['<urn:uuid:20143987-345d-433a-9edb-e401e6a7f568>', '<urn:uuid:e73d1440-e135-459d-b927-182c95b784b7>']	open-ended	direct	short-search-query	distant-from-document	multi-aspect	novice	2025-05-12T20:46:45.703947	6	99	2476
79	Are jewelry marked HGE and Vermeil the same type?	No, they are different. HGE (Heavy Gold Electroplate) is considered costume jewelry with no real value. Vermeil, on the other hand, is high-quality silver (pure or sterling) that has been plated with a thin coating of gold through electroplating, making it more valuable than simple gold electroplate.	"[""What do the markings mean?\nThere are many types of gold. The most common types of gold are listed below.\n- Yellow Gold\n- White Gold\n- Rose Gold\n- Green Gold\n- Black Hills Gold (Style of gold using combinations of all of the above)\nBefore testing you will want to look for the jeweler's markings which can tell you what type of gold was used to make the jewelry. Please keep in mind it is possible to have fake items or mis-marked items and so testing it will always be the best way to tell what you really have.\nBelow are some of the most common markings for gold for items of real value.\n- K = Karat ----example: 14k\n- KT = Karat ----example: 14kt\n- KP = Karat Plumb ----- example: 14kp\n- .333 = 8K\n- .375 = 9K\n- .417= 10K\n- .500 = 12K\n- .583 = 14K\n- .585 = 14K\n- .625 = 15K\n- .667 = 16K\n- .750 = 18K\n- .833 = 20K\n- .875 = 21K\n- .917 = 22K\n- .958 = 23K\n- .999 = 24K\nBelow are some of the most common markings for items of NO real value. These markings are considered costume jewelry.\n- GF = Gold Filled ----example: 14k GF\n- GS = Gold Shell ----example: 14k GS\n- HGE= Heavy Gold Electroplate ----example: 14k HGE\n- 1/5 = 1 part gold to every 5 parts not gold ----example 1/5 14k\n- 1/10 = 1 part gold to every 10 parts not gold ----example 1/10 14k\n- 1/20 = 1 part gold to every 20 parts not gold ----example 1/20 14k\n- 1/30 = 1 part gold to every 30 parts not gold ----example 1/30 14k\n- 1/40= 1 part gold to every 40 parts not gold ----example 1/40 14k\nSome jewelry may have additional markings which are referred to as the makers mark. This mark is typically placed by the jeweler so that the origin of the item can be found. For example 14k JD or JD 14k could mean the item is 14k gold and the maker's name is John Doe. The makers mark is not limited to just first and last initials and can be any mark the jeweler decides to use including symbols.\nWhat if your item has no markings? There are many reasons why an item may not be marked and sometimes the markings can be so small you need a loupe or magnifying glass to find it.\nBelow are a few common reasons why an item is not marked but could still be real gold.\n- The mark has been scuffed or worn off.\n- The item has been re-sized or repaired.\n- The item was not made by a professional jeweler (homemade).\n- The item was custom made with no markings.\nTop Gold Topics:\nHow to sell your Gold for the most money?\nThe truth about your gold value - must read before you sell your gold\nWhat is my gold worth?\nHow do I know my gold is real? & What do the markings mean?\nHow to test your gold?\nWhat We Buy:"", 'Understanding the hallmarks/markings on jewelry can help you identify if your ring is gold or gold-plated, or even learn more about the history of a lovely antique locket you found during your treasure hunt. You can discover more about practically every item of jewelry in your collection by using a magnifying lens and a little research. Almost every piece of jewelry, from costume pieces to those fine designer pieces, has markings on it. These markings on jewelry indicate the value of your pieces, and you don’t necessarily have to be an expert to understand them. All you have to do is to know a few simple terms. Read on…\nFirst: Is There A Difference Between Carats and Karats?\nYes, there is a difference. To define these two often-used terms: A carat is a unit of weight used to describe diamonds or gemstones. A carat is the same as 200 milligrams (0.00705479 ounces).\nA karat, on the other hand, is a unit of measurement for the purity of gold. Gold jewelry is typically composed of an alloy metal or a mix of pure gold and a bonding metal. While 24 karat or 24K gold is pure gold, 18 karat gold is made up of 18 parts gold and 6 parts alloy metal—or, in other words, 75% pure gold.\nKarats are of utmost importance in understanding markings on jewelry value when examining jewelry for authenticity and determining the specifications of the piece. On most high-quality pieces, the qualities of your jewelry (such as gold karat) will appear as a hallmark or a stamp (or many) on the back or bottom of the piece. Markings on jewelry are used to indicate the purity or fineness of precious metals.\nGold Jewelry Markings\nPure gold is 24 karats and is stamped with the number 999 to indicate the highest gold-to-alloy ratio. As we move down in the gold purity scale, 20-carat gold is stamped with the number 833, which represents the 20:24 (or .833 out of 1000) gold to alloy ratio. Learn how you can tell if gold is real here. And to help you comprehend the other often used gold hallmarks, here’s a handy chart that shows the caratage hallmarks and the gold purity they correspond to:\n|Gold Markings||Gold Carat, Purity|\n|999||24K (100% Pure Gold)|\n|958||23K (95.8% Pure Gold)|\n|917||22K (91.67% Pure Gold)|\n|750||18K (75% Pure Gold)|\n|585||14K (58.3% Pure Gold)|\n|375||9K (37.5% Pure Gold)|\nPlatinum Markings on Jewelry\nAlthough a lesser popular jewelry alternative when compared to gold, platinum is equally valuable, not to say more. It is mined in just a few places on the earth, mainly in South Africa. If you want a better understanding of platinum’s scarcity, consider that its production in 2009 was just 7% of that of gold.\nScarcity and uniqueness come at a cost, so expect to spend up to 3% more than for the identical setting in gold. In addition, platinum is a hypoallergenic metal, it ages beautifully, and also creates a very desirable patina with the passing of time. So how to tell platinum from other metals? What hallmarks are engraved in platinum? Check them below:\n|Platinum or PT950||At least 95% Pure Platinum|\n|PLAT or Pt||Platinum|\n|Pt 999 or PLAT999||The Purest Platinum|\n|Pt950/CO||95% Platinum With 5% Alloy Cobalt|\nSilver Markings on Jewelry\nThe purity of silver is expressed as parts of pure silver out of 1000. The “sterling standard” is 925/1000.\nAs a result, for a piece of jewelry to be called silver, no more than 75 parts of 1,000 can be alloy metal (copper). Copper, like gold, hardens precious metal and makes it more durable for everyday usage. When it comes to sterling silver markings, older pieces are often labeled “sterling” or “sterling silver,” whereas current ones are marked “925” or “925 sterling.”\nSilver plated (over base metal) items may not be legally referred to as “sterling” or have a “silver” hallmark. To be designated as “silver” in the United States, jewelry must contain 925 parts silver out of 1000, or 92.5 percent silver.\n|Sterling, Ster, Sterling Silver||92.5% Purity Silver|\n|Silver 925||92.5% Purity Silver|\n|925||Sterling Silver with 92.5% Purity|\n|999 Silver, Silver999||99% Pure Silver|\nOther Markings On Jewelry\n|Markings on Jewelry||Meanings|\n|Plat/ PT / platinum||Platinum|\n|900Plat / 900Pt / 850Plat/ 850Pt||Accordingly: Platinum 90% and Platinum 85%|\n|10% irid-platinum||Platinum with 10% Iridium Alloy|\n|S.S / Steel / St. Steel||Stainless Steel|\n|Silver / S.Silver||Sterling Silver|\n|Pd / pall / 950 Pd / 950 Pall||Palladium|\n|GF / 14K GF||Gold Filled and 14K Gold Filled|\n|GP / 18K GP||Gold Plated or 18K Gold Plated|\n|GEP or G.E.P.||Gold Electroplate|\n|Vermeil||The item is gold plated sterling silver|\n|Nickel Silver / German Silver / Alpaca Silver / Argentan Silver / Nickel Brass||The item does not contain silver|\nVermeil, commonly known as “silver-gilt,” is high-quality silver (pure or sterling) that has been plated with a thin coating of gold. The majority of vermeil jewelry on the market today is made by electroplating silver with gold, a chemical technique that employs an electrical current to bond the two metals together.\n- Nickel Silver Jewelry\nNickel Silver contains no silver. It is usually made of 60% copper, 20% zinc, and 20% nickel. Although called silver, nickel silver contains no silver in its composition. “Silver” in Nickel Silver jewelry, refers to the color of the metal and not the content.\n- Gold Filled\nGold filled jewelry has a significantly thick covering of gold over the base metal, like brass for example. This makes a significant impact on the jewelry’s capacity to withstand wear and tear over time. Although the base metal is typically brass or similar low-cost material, distinguishing between “genuine” gold and a gold filled piece would be difficult.\n- Gold Plated\nGold plated jewelry is constructed of brass, copper, or a low-value metal electroplated with gold covering. Because there is no minimum carat weight or thickness restriction, gold-plated jewelry is often less expensive than vermeil. A gold plated object, on the other hand, is more valuable than a vermeil piece if it has a high gold carat weight (20 or 22k) and a thick covering of gold.\nPalladium is a rare metal that appears whiter than platinum and white gold. It is a wonderful option for wedding and engagement rings because of this, as well as its high durability. It was used to alloy platinum for the first time in jewelry in 1939 and can only be found in a few mines worldwide.\nPrecious Metals Vs Base Metals\nPrecious metals are those metals that are used to make exquisite jewelry. Gold, platinum, and silver are the three most common precious metals used in jewelry. Non-precious metals or metals used for reasons other than jewelry are referred to as base metals. Copper, zinc, tin, nickel, lead, and iron is among these metals.\nThese metals are used as alloy materials with precious metals on occasion, and they are commonly utilized in the creation of costume jewelry, often with a thin coating (or “plate”) of precious metal on top. When a piece of jewelry is made of precious metal, it will often contain a stamp or quality indication indicating the metal’s kind and purity.\nThe remainder of this essay will go through the numerous marks that you are likely to see on gold, silver, and platinum.\nWhere To Find Karat Stamps In Jewelry?\nNow that you have a better understanding of what stamps on gold mean, you can start looking at your pieces by yourself to determine their value. Examining your gold for stamps/ markings can sometimes become a real challenge, especially if you’re a complete novice. This is because different pieces of jewelry will have the marking/ stamp, engraved in a specific place.\nMetal content stamps are typically found around the clasp on necklaces and bracelets and on the backs of earrings, pins, and brooches. Rings, for example, have markings on the inner side. On chains, expect to find the marking on the clasp or right next to it.\nIt is by law specified that every jeweler selling a gold piece must engrave the stamp that indicates the jewelry’s purity. If you don’t find the stamp, that is not a good sign. However, it is possible for the markings on jewelry to wear off with the passing of time. Another reasoning is that the jewelry piece might be vintage or antique, which often come without markings. Don’t worry if you can’t find a stamp; you can always ask an expert! Gold buyers have extensive knowledge of determining value and may also do complex tests to evaluate gold purity.\nWhy Are Markings on Jewelry Important?\nMany markings on jewelry reflect the piece’s metal composition. This is significant because, to the untrained eye, silver-plated and sterling silver items appear practically identical. Understanding the metal composition of your item might assist you in ensuring that you are receiving the quality for which you are paying.']"	['<urn:uuid:e9a893c7-bbc4-4ffc-917b-516ca5276960>', '<urn:uuid:7b62fa98-b0f1-4810-b01b-cfadc579fc91>']	open-ended	direct	concise-and-natural	similar-to-document	comparison	novice	2025-05-12T20:46:45.703947	9	47	2007
80	I have mental health issues. What programs are available for senior adults?	There are multiple mental health programs available for seniors. The 'Ask Dr. Dan' series features discussions by Dr. Dan Storch, a psychiatrist certified in geriatrics, offered at 50+ Centers. There are also specialized group programs covering topics like relationships, depression, and emotional wellbeing. Additionally, Charles County Mental Health Services provides outpatient mental health treatment using various approaches including Cognitive Behavioral Therapy, Mindfulness and Stress Reduction, and EMDR, accepting Maryland Medicaid insurance.	"['The goal of the Health and Wellness Division of the Howard County Office on Aging and Independence is to improve the health and quality of life for older adults and adults with disabilities living in Howard County. To accomplish this goal, Health and Wellness offers innovative, accessible, and affordable programs and services at 50+ centers and other community locations; sponsors special events throughout the year; and coordinates ongoing health and wellness programs. For more information on programs, explore the links below or contact Maryland Access Point (MAP) via email or at 410-313-1234 (voice/relay).\nClick here to sign up for our email list and to receive information about upcoming health and wellness workshops and events.\nClick here to view health and wellness workshops taking place this summer.\nOur Go50+ membership packages are designed to enhance the quality of life for the 50+ adult population that lives, works, and plays in Howard County.\nLIve longer, healthier lives...join today!\nMembership includes access to Gary J. Arthur, Ellicott City 50+ Fitness Center and North Laurel Community Centers and all Howard County 50+ Centers, including game room areas, gymnasiums, walking tracks, and wellness programs.\nMembership includes all 50+ Center benefits, plus fitness rooms and dance/aerobics studios, where available. Resident cost: $75 per year. Nonresident cost $100 per year.\nAll Go50+Fitness benefits, plus pool access during open swim hours. Resident cost: $175 per year. Non resident cost: $225 per year.\nView our flyer for additional information and locations.\nExercise Classes for Chronic Conditions\nExplore exercise classes that may help improve mobility and reduce fall risk often associated with certain chronic conditions. Currently, classes are offered for people with Arthritis, Parkinson’s disease, and history of stroke or balance limitations. The Howard County Office on Aging and Independence has established collaborations with the Arthritis Foundation, Parkinson’s Foundation of the National Capital Area, UMBC, and the VA Medical Center in Baltimore. To learn more, contact Malarie Burgess via email or at 410-313-6073 (voice/relay).\nStepping Up Your Nutrition is a 2.5 hour malnutrition risk/food insecurity workshop for older adults. Participants learn about the strong correlation between nutrition status, muscle strength, and fall risk. Topics covered include the importance of protein and fluids on nutrition, recommended daily allowance, and how to set goals for behavior change to improve nutritional status and muscle health. For more information, please contact email or call Malarie Burgess at 410-313-6073.\nAn evidenced-based fall prevention program designed to reduce falls and increase confidence, strength, and balance. The program meets for two hours, one day a week for seven weeks.\nTopics covered include:\nFor more information about Stepping On, contact Malarie Burgess at 410-313-6073 (voice/relay) or via email.\nCycle2Health is a non-competitive, peer-led bicycling club for adults of all ages and riding abilities, coordinated by the Health & Wellness Division of the Office on Aging and Independence. Registration for the current season begins in January and continues throughout the season. To receive Cycle2Health information by email, please join the C2H listserve by sending an email to firstname.lastname@example.org.\nFor general information about Cycle2Health, contact Malarie Burgess at 410-313-6073 (voice/relay) or visit the Cycle2Health page.\nAre You Living Well? A variety of trusted programs are available to help individuals take charge of their health. These free 6-week programs help participants learn how to manage their everyday health, feel better, be in control, and do the things they want to do. These programs are available in multiple languages, and for specific conditions such as diabetes, chronic pain, and cancer. Email or call Nicole Becerra at 410-313-3506 now to learn about the multiple opportunities near you. To register online, click here.\nLiving Well with Hypertension is an interactive workshop designed for those with high blood pressure or hypertension, and meets once for 2.5 hours. Lay leaders address topics such as medication management, high blood pressure risk factors, nutrition and food label guidance and sodium content in common foods. If you are interested in attending a Living Well with Hypertension workshop, contact Nicole Becerra via email or at 410-313-3506 (voice/relay).\nThe Office on Aging and Independence provides mental health programming in a variety of ways. The ""Ask Dr. Dan"" series features discussions by Dr. Dan Storch, a psychiatrist certified in geriatrics, and are offered at different 50+ Centers over the course of the year. Dr. Storch leads discussions on a variety of emotional wellness topics in a relaxed atmosphere where questions and feedback are encouraged.\nEmotional wellness and mental health programming also include specialized group programs that cover topics like relationships, depression, and emotional well being. These programs are offered in 50+ Centers or at other community locations for older adults.\nTraining on older adult mental health issues can be provided to community groups, professional or faith-based organizations by request. Available topics cover a broad spectrum of older adult and mental health concerns and can be tailored to the requesting organization\'s needs.\nFor more information about mental health programs, please contact Karen Hull via Maryland Access Point (MAP) via email or at 410-313-1234 (voice/relay).\nFree one-on-one nutrition assessments with a registered dietitian, are offered at Howard County 50+ Centers by appointment. During each half-hour appointment, individuals will learn to:\nTo schedule a nutrition assessment, contact your local 50+ Center, or contact Maryland Access Point (MAP) via email or at 410-313-1234 (voice/relay).\nDo you have an outgoing, friendly, well-behaved dog or cat? If you think your pet has what it takes to make friendly visits or work in a therapeutic setting, we need you! It\'s fun and rewarding for everyone! Pet evaluations are held the first Thursday of each month at 7pm at the Bain 50+ Center, 5470 Ruth Keeton Way in Columbia. For more information, view our brochure or download an application.\nTo volunteer, pets must be at least one year old and must have lived with your for at least six months. Your pet must be properly licensed in its county of residence. Pets fed raw bones, treats, or a raw food diet cannot participate. Pets must be evaluated for temperament and suitability; a vet check is also required. You will receive a health certificate which must be completed by your veterinarian. We will refuse any dog to the program that is deemed unsuitable due to is age, breed, or behavior. Paws4Comfort does not accept any breed that is deemed aggressive or potentially aggressive including, but not limited to, Pit Bulls, American Staffordshire Terriers, or American Bulldogs. This includes any mixed breed that has the appearance or characteristics of these breeds.\nIf you have questions about the Paws4Comfort program or would like more information regarding the evaluation process, email Ingrid Gleysteen at email@example.com or call 410-313-7461. (Voice/Relay).\nThe SeniorsTogether peer outreach program was created as an opportunity for older adults to both give and receive support. Programs and activities are designed to nurture interaction among older adult peers to foster a positive outlook, develop social networks, and support a thriving aging process.\nSeniorsTogether Support Groups are a core element of the program, providing a safe, confidential environment for meeting others, sharing concerns, finding help, and staying stimulated. Other activities organized by the program include luncheons, speaker engagements, and special workshops. All SeniorsTogether activities are developed with sensitivity to sensory and mobility issues. View the SeniorsTogether brochure or Group Schedule for more information, or call 410-313-7466 (voice/relay).\nLas traducciones de las páginas del sitio web del condado de Howard se realizan mediante Google™ Translate. Google™ Translate es un servicio de traducción de idiomas en línea gratuito que permite traducir\ntextos y páginas web a otros idiomas. Estas traducciones son sólo una aproximación del contenido original del sitio web, por lo que la traducción puede incluir lenguaje incorrecto, engañoso, impreciso u ofensivo.\nEl condado no garantiza la exactitud o fiabilidad de cualquier información traducida por Google™ Translate. Cuando usted solicite una traducción a través de Google™ Translate, abandonará el sitio web del condado.\nEl condado no se encuentra afiliado de manera alguna con Google, no respalda el uso de la página de traducción de Google®, y proporciona un enlace a Google™ Translate únicamente para la comodidad de los usuarios\ndel sitio web del condado. Es posible que haya otros servicios de traducción en línea disponibles para su uso en el sitio web del condado. Cuando utiliza Google™ Translate, lo hace bajo su propia responsabilidad,\ny es usted quien asume el riesgo de cualquier inexactitud, error u otros problemas que pueda experimentar con la traducción. El condado de Howard no se hace responsable de los errores, daños u otros problemas que\npuedan resultar del uso de Google™ Translate.\nHoward County website i holh aa let mi cahmai hna hi Google™ Translate ti mi nih a leh mi an si. Google™ Translate cu online i catial (text) le webpages pawl a lak te in online lila in holh phundang ah a lettu a si.\nCu bantuk holh lehnak nih cun website chung i a umcia bia kha a hrawnghrang lawng in a leh tawn i, cu ruang ah holh a leh tik ah aa palh mi tete, a fiang huaha lo mi, a hmaan lo mi silole ngaih nuam lo biafang aa tel kho mi\na si. Google™ Translate nih holh leh a tuah mi chung bia cu a dik bak silole i hngatchan awk a tlak ko, tiah County nih aamahkhannak a ngei lo. Google™ Translate nih holh ka lehpiak ko seh, na ti (request) ah cun, County\nwebsite na chuah taak lai. County hi Google he pehtlaihnak a ngeih lo, Google® translation zong a cohlang lo i, County website a hmang mi hna an tlamtlinnak ding zawnruahnak in Google™ Translate kong hngalh khawhnak link\nte zong kan tuah chih ko. Online in catial a let mi dangdang kong zong County website ah a um men lai. Google™ Translate na hman tik ah, nangmah duh te in na hman a si ko i, a hmaan lo mi, aa palh rumro mi silole holh leh\nkong i buaibainak tete na ton tik ah nangmah mawh a si kong naa fian cia a herh lai. Google™ Translate na hmang ruang i aa palhnak tete, a tlinlonak tete, silole a dang harnak tete a um sual kong ah Howard County cu zei\nmawhphorhnak hmanh a ngei lai lo.\nHoward County 웹사이트 페이지 번역은 Google™ Translate가 하고 있습니다. Google™ Translate 는 글과 웹 페이지를 다른 언어로 번역하는 무료 온라인 언어 번역 서비스입니다. 이런 기계 번역은 웹사이트 원래 내용을 대략적으로 전달할 뿐, 틀리거나, 오역하거나, 부정확하거나, 불쾌한 단어를 포함하고 있을 수 있습니다. County는 Google™ Translate로 번역된 정보의 정확성이나 신뢰도를 보증하지 않습니다.\nGoogle™ Translate 번역을 요청한 사용자는 County 웹사이트를 떠나게 됩니다. County는 Google과 어떤 제휴 관계도 없으며 Google® 번역 사이트 사용을 권하는 것도 아닙니다. Google™ Translate 링크는 오직 County 웹사이트 사용자 편의를 위해 제공될 뿐입니다. County 웹사이트는 다른 온라인 번역 서비스로 쓸 수도 있습니다. 사용자는 번역의 부정확, 오류, 번역 때문에 겪을 수 있는 어려움을 자기 위험 부담으로 받아 들이고 Google™ Translate를 사용하는 것입니다. Howard County 는 Google™ Translate 사용으로 인한 실수, 피해 또는 다른 문제에 책임을 지지 않습니다.', 'HOURS OF OPERATION\nMonday-Thursday 8:00 A.M. until 8:00 P.M.\nFriday 8:00 A.M. until 5:00 P.M.\nCharles County Mental Health Services is a program within the Division of Behavioral Health and is part of the Charles County Department of Health. The Mental Health Services program provides outpatient mental health treatment services to the residents of Charles County and the surrounding areas.\nActing Director: Jaime Barnes, NCC, LCPC-S\nMedical Director: Ijaz Khan, M.D.\nAs part of our multidisciplinary team, our clinical staff consists of all -at minimum- masters level, fully licensed professionals.\nWHO DO WE SERVE?\nOur outpatient mental health clinic provides behavioral health services to residents of this county and other surrounding areas. Starting treatment age for therapy is age six and upward. Currently, medication management services are provided to clients ages 18 and above. Conditions treated range from severe reactions to stress, bereavement, partner or parenting problems, domestic violence, treatment of depression, bipolar disorders, anxiety disorder, psychotic disorders, trauma treatment, anger management, and many other situations leading to stress and reduction of enjoyment of life, such as chronic pain conditions. We also support clients living with substance use conditions by collaborating closely with treatment and recovery programs both inside the Health Department as well as outside resources.\nWHAT SERVICES DO WE OFFER?\nMental Health Services offered include individual, group, and family psychotherapy using evidence based practices. We also offer psychiatric evaluations and medication management services. In addition, our clinic offers an Anger Management Program. Treatment approaches include (but are not limited to) Cognitive Behavioral Therapy, Brief Solution Based Therapy, Mindfulness and Stress Reduction, Eye Movement Desensitization and Reprocessing (EMDR), and many others. All of our treatment approaches utilize person centered, trauma informed care.\nFurthermore, our licensed clinicians will be happy to refer our clients, if applicable, to community resources such as supported employment, psychiatric rehab programs, housing programs, and case management agencies to help craft the strongest possible support network, allowing for a full and meaningful life in the community.\nWHAT INSURANCE IS ACCEPTED?\n- Maryland Medicaid\nPlease contact our front desk for an exact listing of any other insurance plans we accept, and for self-payment fees.\nHOW DO I START SERVICES?\nAll referral sources are accepted.\nPlease start your treatment process by calling 301-609-6700 to schedule an intake assessment appointment by our clinicians. Evaluation for psychiatric services are scheduled following clinical intake assessment.\nYou can also stop by our clinic front desk to obtain information regarding treatment and eligibility.\nAll services are governed by strict confidentiality laws we share with our Substance Use Treatment and Prevention Services partners. Per client request and consent, we also are happy to collaborate with stakeholders and supporters in our clients’ lives, reflecting our belief in a person centered, holistic approach to delivering services at our agency.\nAnger Management Group is an evidence based 8 week program that utilizes Cognitive Behavioral Restructuring skills with goals of reducing aggressive outbursts and improving emotion regulation leading to more positive interactions with family, friends, co-workers, employers, and the legal system. The groups are offered throughout the year on varying weeknights.\nThe Parent-Child Connection is an 8 week evidenced based program that provides parents with strategies, encouragement and support while making parenting an easier and more enjoyable experience.\nThe Parent-Child Connect can help with:\nUnderstanding emotional/physical child development, age appropriate discipline, building self esteem, reducing problem behaviors, co-parenting strategies, solving family conflict and communication with mutual respect.\nTREM - Trauma Recovery and Empowerment for women\nThis group consists of 33 weekly sessions designed to recognize the long term effects that trauma has had on women’s lives. The focus is to promote healthy recovery skills that will foster personal growth towards improving ones quality of life.\nMore groups to come- please check back or call our clinic with suggestions and community needs.\nWHAT IS TRAUMA INFORMED CARE?\nHere is the official definition: Trauma Informed Care is an organizational structure and treatment framework that involves understanding, recognizing, and responding to the effects of all types of trauma.\nIn everyday language, it is a mindset to understand that most people we will come in contact with have experienced some sort of trauma and loss that might influence their reactions. We also have to be mindful of how our own history, attachment history, parenting history, relationship history, work history etc affects OUR reactions.\nFor a video on trauma informed care, click here.\nFor more information about groups and other services offered call: 301-609-6700\nAfter hour, Non-Emergency Phone: 301-609-3599']"	['<urn:uuid:29248a62-d846-4ff5-909f-e8d3b00cff64>', '<urn:uuid:2a2d9aa1-27e1-44bf-968e-7c10fa225c4b>']	factoid	with-premise	concise-and-natural	similar-to-document	three-doc	novice	2025-05-12T20:46:45.703947	12	71	2572
81	advantages cost savings solar integrated building facades compared traditional panel setup	Building-integrated photovoltaics (BIPV) offer multiple advantages over traditional solar panel setups. They serve as both the outer layer of a structure and generate electricity, providing savings in materials and electricity costs. When included in initial building design, BIPV systems reduce the incremental cost of PV systems by eliminating separate mounting systems. Companies like Elemex offer PV-integrated facade solutions that can be seamlessly combined with other exterior cladding surfaces, creating superior facade systems while maintaining aesthetic appeal.	['Technology Profile – Solar\nThe recent evolution of solar technology makes the future bright for homeowners and the design-build community.\nBY jason takahashi\nTHE APOLLO II TILE FROM CERTAINTEED IS RATED TO WITHSTAND OPERATING TEMPERATURES\nOF -40 TO 90 C AND HIGH VELOCITY WINDS OF UP TO 194 MPH.\nTHE SOLAR WINDS ARE STARTING TO SHIFT AGAIN, and no, I’m not talking about the extensive cyber-attack carried out as 2020 came to a close. Rather, the rapid proliferation of solar energy that took a back seat to traditional fossil fuels over the past four years is set to sail again. With the United States resuming its position as a leader in the reduction of global greenhouse gas emissions worldwide and major clean energy jobs and infrastructure legislation in the works, solar energy is well positioned to take a leading role in the decentralized search for reliable, clean and effective energy in 2021 and beyond. What is perhaps most exciting for architects, designers, builders and ultimately homeowners, is the recent evolution of solar technology that makes the future appear particularly bright.\nJust as televisions evolved from tubes to flat panels to today’s chameleon-like OLEDs, solar panels are similarly finding ways to hide themselves in plain sight. As they often do, Tesla stole the headlines in 2016 with the release of their integrated solar roof tile system shortly after acquiring SolarCity. But four years and three iterations later, the company is still struggling to scale their manufacturing and installation capabilities to meet nationwide demand. Luckily, the Building Integrated Photovoltaic (BIPV) sector beyond Tesla (and the continental U.S.) is still moving ahead. And with demand for new builds continuing to grow, finding ways to couple construction with solar cells is a proper approach.\nThe advantage of solar roofs and other BIPV setups over traditional systems is primarily aesthetic. Slick solar lined roofs and other façades negate the need for large, unfavorable rooftop panel arrays by weaving light-converting technology into the tiles, shingles and siding. This is particularly enticing with new builds, but also when thinking about replacing an entire roof, where combining costs can generate supplemental savings. While the overall scope of products available is still limited, here’s a quick look at some of the best options on the market today.\nMICHIGAN-BASED LUMA SOLAR’SINTEGRATED ROOFTOP SHINGLE SYSTEM\nIf you’re seeking a tested roofing solution within the U.S., I’d start with taking a look at Certainteed’s Apollo Tile II. The Pennsylvania-based member of the Saint-Gobain conglomerate is perhaps the antithesis to some of the failed startups in this sector. Certainteed has been innovating building materials for more than a century, now working to achieve the dream of ‘making sustainable habitats a reality’. Of the half-dozen or so solar tiles and shingles currently available in the U.S., Certainteed is a reliable option thanks to a strong reputation among builders and a robust installer network. While the Apollo II tile is not an entirely incognito solution, depending on the color of your other roof tiles — high-quality aesthetics are still achievable. Furthermore, the monocrystalline modules are water tight, featuring raised fastener locations to provide added protection against intrusion. Lastly, Apollo II is rated to withstand operating temperatures of -40 to 90 C and high velocity winds of up to 194 MPH. Certainteed’s century of experience in roofing helps provide peace of mind, while delivering up to 70W of electricity per tile in the process.\nAnother exciting contender in this space is Michigan-based Luma Solar. Visually speaking, Luma’s latest installations lead the pack with its luxurious integrated rooftop shingle system, which they call the ‘only upgradable’ solar shingle system in the world. With three different packages to choose from, the highest-end Mirage Collection sports black monocrystalline cells with a minimum of 75W per shingle, with power monitoring, upgradeability and hurricane ratings to boot. For those interested, reservations are required. Luma recommends ordering a Base Study to bring in one of their consultants to assess the property and outline the best possible setup for your home or business, before partnering with a local installer to complete the project.\nEXASUN’S X-ROOF IS EQUIPPED WITH REAR CONTACT CELLS AND GLASS-PANEL TECHNOLOGY THAT SHOULD DELIVER HIGH EFFICIENCY YIELDS FOR 30 YEARS.\nSolstex is the photovoltaic (PV) member of the Elemex building façades family. Headquartered in Ontario, Elemex specializes in ceramic, aluminum and stone exterior cladding surfaces, which fortify the elegant exteriors of celebrated buildings across Canada and beyond. While their products are not a natural first choice for a residential exterior design, it does provide an opportunity to imagine how PV technology embedded into modern siding might find their way into future trends.\nThe panels come in two different sizes, delivering 110 – 122.5W and 420 – 450W respectively, each with a quarter-inch thickness. They can be seamlessly integrated to any of Elemex’s other products thanks to their proprietary Unity attachment technology. Unity is engineered to create an ecosystem in which all of Elemex’s exterior cladding surfaces can combine to create all-in-one superior façade systems. Weighing at approximately 3.5 pounds per square foot, the Solstex panel consists of a thin-film Cadmium Telluride (CdTe) solar technology housed between two sheets of heat-strengthened glass. While sometimes less efficient than crystalline silicon systems, thin-film CdTe cells have made major strides in recent years, thanks in part to research and development from the National Renewable Energy Laboratory (NREL). NREL has produced outstanding solar efficiency results in lab settings using CdTe, and other benefits include the smallest carbon footprint and lowest water usage during manufacturing, as well as the ability to achieve light-weight, large format designs that help maximize facade coverage and energy production.\nELEMEX SPECIALIZES IN CERAMIC, ALUMINUM AND STONE EXTERIOR CLADDING SURFACES\nIf asked who is leading the solar revolution, you wouldn’t be at fault to assume North America. In 1954, Daryl Chapin, Calvin Fuller and Gerald Pearson developed the first silicon cell capable of powering everyday electrical equipment via sunlight at Bell Labs, making the U.S. the original home of solar energy. Canada has produced some photovoltaic powerhouses as well, with Canadian Solar Inc. continually topping growth and revenue charts in the sector. Similarly, no one would bat an eye if you guessed Asia. It’s certainly likely that heavy hitting titans like LG, Kyocera, Panasonic and JinkoSolar are riding furthest ahead out on the edge of the solar wave. If not, then they are at least leading the manufacturing front by a country kilometer. Fascinatingly though, I would argue it is neither. For me, the future of solar is blooming in none other than one of Europe’s most notoriously creative culture centers: the Netherlands.\nIn 1974, Dutch footballer Johan Cryuff carried the small European nation to the World Cup final in West Germany, introducing their concept of total football onto the world stage. Like a jazz band where members are capable of rotating instruments, the 1974 squad centered positional versatility among all its outfield players in its vision of the game. The result was an astonishing second place finish, a Golden Boot (best player) award for Cryuff, and a legacy that carries on in the highest levels of the sport today. The contemporary solar equivalent to this approach is Exasun. Headquartered in The Hague, Exasun is a design, manufacturing and installation company with a firm grasp on all the BIPV technologies we’ve discussed, coupled with a compelling vision for the future sustainability of the field. Through learning how to play all the positions, Exasun has developed a total solar approach that could serve as a model for all solar manufacturers moving forward.\nProducts like their X-Roof have the potential of approaching residential mainstream adoption at a far greater pace than anything we are seeing in the U.S. today. When glancing through their past projects, you immediately get the sense that this type of integration is not only ingenious, but inevitable. It’s not the typical photo of a single luxury home or eco retreat with gorgeous solar tiles glistening by the coast. Rather, they feature blocks of multi-family housing in urban areas, elegant modern farmhouses in the countryside, and stunning solar façades that sweep across commercial structures. Exasun still offers standard-style PV panels (X-Glass), but are making waves with their new X-Roof design, specifically engineered for slanted roofs. X-Roof is equipped with rear contact cells and glass-panel technology that should deliver high efficiency yields for 30 years. Similarly, they also offer their own façade system and are preparing to launch X-Tile, which could feature a terracotta design that debuted at RAI Amsterdam in 2019. X-Tile is expected to debut in Q2 2021.\nTo top it all off, Exasun is developing innovative offsets for the less beneficial side-effects of renewable energy. They were ahead of the pack in taking steps to eliminate many of the unnecessary toxins, such as lead, fluorine and cadmium, used in most solar panels today. On top of their 30-year warranty, their new cradle-to-cradle solution makes 90 percent recyclability of panels a possibility through dismantling at the part level for replacement, recycling and reuse. This future-minded approach is only going to help the renewable sector avoid repeating the mistakes of its predecessors, while allowing the company to keep up with the increasingly stricter new building efficiency regulations being put into practice by the Dutch government, as well as the EU.\nWhile cloudy days always still lie ahead, the 10-year solar forecast is remarkably bright. This can only be enhanced by taking a total solar approach to new building development, and engaging public and private interest in establishing new guidelines for new construction. Designing future-proofed solutions to limit negative impacts from the manufacturing cycle will hopefully encourage local production and more sustainable practices to maximize emissions offsets. Finally, transforming the future of energy is a collective effort reminiscent of landing a new rover on Mars, with the potential of helping to ensure our future residency on the Earth.', 'Building-integrated photovoltaics (BIPV) are dual-purpose: they serve as both the outer layer of a structure and generate electricity for on-site use or export to the grid. BIPV systems can provide savings in materials and electricity costs, reduce pollution, and add to the architectural appeal of a building. Though they can be added to a structure as a retrofit, the greatest value for BIPV systems is realized by including them in the initial building design. By substituting PV for standard materials during the initial construction, builders can reduce the incremental cost of PV systems and eliminate costs and design issues for separate mounting systems.\nBuilding-integrated PV systems are planned during the architectural design stage and are added during initial construction. Building-added PV (BAPV) is planned and built during a retrofit. Both BIPV and BAPV lack the racks and mounting equipment of traditional PV systems. Most designers of integrated solar system will consider the array of solar technologies and their possible uses compared to the specific needs of building occupants. For example, semi-transparent thin-film PV can allow for natural day lighting and solar thermal systems can capture heat energy to generate usable hot water or provide space heating and cooling capacity.\nApplications for Building-Integrated Photovoltaics\n- Façade – PV can be integrated into the sides of buildings, replacing traditional glass windows with semi-transparent thin-film or crystalline solar panels. These surfaces have less access to direct sunlight than rooftop systems, but typically offer a larger available area. In retrofit applications, PV panels can also be used to camouflage unattractive or degraded building exteriors.\n- Rooftops – In these applications, PV material replaces roofing material or, in some cases, the roof itself. Some companies offer an integrated, single-piece solar rooftop made with laminated glass; others offer solar “shingles” which can be mounted in place of regular roof shingles.\n- Glazing – Ultra-thin solar cells may be used to create semi-transparent surfaces, which allow daylight to penetrate while simultaneously generating electricity. These are often used to create PV skylights or greenhouses.\nBuilding Design Considerations\nA critical part of maximizing the value of a BIPV system is planning for both environmental and structural factors, both of which influence the economics, aesthetics and overall functionality of any solar system.\n- Insolation - This refers to the average amount of solar radiation received, usually calculated in kWh/m2/day. It is the most common way to describe the amount of solar resources in a particular area.\n- Climate & weather conditions – High ambient temperatures can decrease the output of solar systems, and clouds and rainfall patterns can affect system output and maintenance requirements. High levels of air pollution can require regular cleaning to limit efficiency losses.\n- Shading – Trees, nearby buildings and other structures can block the sun, reducing PV system output.\n- Latitude – Distance from the equator affects the optimal tilt angle for solar panels to receive solar radiation.\n- Building energy requirements – The design of a BIPV system should take into account whether the building should be able to operate wholly independent of the electrical grid, which requires batteries or other on-site energy storage systems.\n- Solar system design – The design of the PV system itself is determined by the buildings energy requirements, and any structural or aesthetic limitations that may limit material choices. Crystalline silicon panels have higher electricity outputs per m², but greater costs and design constraints. Thin-film materials generate less electricity per m², but are less expensive and may be integrated more easily onto more surfaces. Either (or both) may be appropriate, depending on the situation.']	['<urn:uuid:e31059b5-b4bf-4872-8e2b-0ad0eb42c65c>', '<urn:uuid:e19bf9c4-ce0a-4ea7-997b-077fa693f1d2>']	open-ended	direct	long-search-query	distant-from-document	multi-aspect	expert	2025-05-12T20:46:45.703947	11	76	2242
82	What are Army promotion flags and ROTC physical standards?	Army promotion flags are given for APFT failure, failing height/weight standards, and punitive actions. For ROTC physical standards, 17-21 year old males must complete 42 push-ups, 53 sit-ups, and a 15:54 2-mile run, while females must complete 19 push-ups, 53 sit-ups, and an 18:54 2-mile run to achieve the minimum 60% score.	"['How to Get Promoted in the Army Using the Army Enlisted Promotion System\nThe first thing you want to know about the Army, or any job, is how to get promoted in the Army. In the Army, there is a system used to standardize the promotion process for all soldiers. This is called the Army Enlisted Promotion system.\nDecentralized Promotions (E-2 through E-4)\nPromotion Authority: Company\nWhen soldiers refer to “automatic promotions”; these are the promotions they are talking about. However, these promotions are not officially automatic. You do have to qualify to get promoted. The reason these promotions are considered automatic by most is that the qualifications to get promoted are very easy to achieve. In fact, you would almost have to be trying to not get promoted for it not to happen.\nArmy Time in Grade Requirements\n- Private (E-2): 6 months time in grade (TIG) as an E-1.\n- Private First Class (E-3): 4 months TIG as an E-2 and 12 months time in service (TIS).\n- Specialist (E-4): 6 months TIG and 2 years TIS.\nUnit Commanders are given a specific number of waivers that they can choose to use for exemplary soldiers. These waivers shorten the TIG and/or TIS requirement for promotion.\n- An E-1 can get promoted to Private (E-2) with only 4 months TIG with a waiver.\n- An E-2 can get promoted to PFC (E-3) with only 2 months TIG and 6 months TIS with a waiver.\n- An E-3 can get promoted to SPC (E-4) with only 3 months TIG and 18 months TIS with a waiver.\nOther Army Promotion Requirements\nOnce a soldier has satisfied his or her TIS and TIG requirements, they must be promotable in all other aspects as well. This means they can not have any “flags” on their record. Flags are given for things such as APFT failure, failure to meet height and weight standards, punitive actions, etc.\nSemi Centralized Promotions (E-5 and E-6)\nPromotion Authority: Company & Army Wide\nFor a soldier to be promoted to E-5 or E-6, there must be an opening for that rank in the soldier’s Military Occupational Specialty (MOS). The Army Promotion Point System is how the Army decides who gets that opening when it becomes available. Semi centralized promotions are the most difficult in terms of satisfying requirements. Below is a check list of all the aspects involved in getting promoted to Sergeant (SGT) and Staff Sergeant (SSG).\nSemi Centralized Promotion Check-list\n- Meet the specified TIS and TIG requirements for your rank and zone.\n- Have a passing APFT score.\n- Have a passing weapons qualification score.\n- Pass the Promotion Board.\n- Complete online Structured Self Development Course (SSD1).\n- Graduate from the appropriate Non commissioned Officer Education Course.\n- Gain enough promotion points to make cutoff.\nPrimary Zone/Secondary Zone\nThere are two promotion processes known as “Primary Zone” and “Secondary Zone.” Most enlisted soldiers are promoted in the “Primary Zone.” The “Secondary Zone” is for soldiers considered to be “exceptional performers” and gives them an early shot at promotion. The promotion point cutoff scores are typically higher for the Secondary Zone as well.\nArmy Promotion Criteria – Promotions to Sergeant (E-5)\n- Primary Zone: 8 months TIG as an E-4 and 36 months (3 years) TIS.\n- Secondary Zone: 4 months TIG and 18 months TIS.\nArmy Promotion Criteria – Promotions to Staff Sergeant (E-6)\n- Primary Zone: 10 months TIG as an E-5 and 72 months (6 years) TIS.\n- Secondary Zone: 5 months TIG and 48 months (4 years) TIS.', 'Presentation on theme: ""ARMY Reserve Officer Training Corps (ROTC) The Best Leadership Training in America JAMES MADISON UNIVERSITY DUKE BATTALION.""— Presentation transcript:\nARMY Reserve Officer Training Corps (ROTC) The Best Leadership Training in America JAMES MADISON UNIVERSITY DUKE BATTALION\nWhy Army ROTC? Serve Your Country. World Class Leadership Course. Class Room and Hands-on Training Pay and Benefits. Adventure and travel. Education and Training. Career Producing Program & Marketability. College Now – Army Later Intended outcome: confident, competent, leaders of character JMU’s ROTC program is one of the best in the country! The battalion has earned numerous ROTC awards, including: The Order of the Founders and Patriots of America for Excellence. The Eisenhower Award for Excellence (Large school program).\nROTC Program Myths “If I join ROTC “they” could send me to some foreign country; I will never finish college!” While attending college, earning your degree, and with an ROTC contract you cannot be deployed. Priority goes to completing college and commissioning as a Second Lieutenant. “As soon as I sign-up and start my first class as a freshman I have an Army obligation.” Even if you’ve been awarded a 4-year scholarship, your actual obligation to the Army does not take effect until you start classes your sophomore year. “I’m out of shape, the ROTC program won’t let me join.” Although we expect you to be at a certain minimum level of physical fitness, we will make you better. “ROTC will take away from my ability to perform well in other college subjects.” We understand how important your college studies are; and we understand that in order to commission as a Second Lieutenant, you have to earn that degree. We still expect some commitment to ROTC; however, this commitment is not that substantial. “ROTC will take up all of my free time.” Your commitment to ROTC ranges from 9 to 15 hours per week depending on where you are in your ROTC progression from freshman to senior year.\nWant to be Part of a Prestigious Occupation? Firefighter62% Scientist57% Doctor56% Nurse54% MilitaryOfficer51% Teacher51% Police Officer44% Priest/Minister/Clergy41% Engineer39% Farmer36% Architect 29% Member of Congress28% Lawyer26% Business Executive23% Athlete21% Journalist17% Union Leader17% Entertainer17% Banker16% Actor15% Stockbroker13% Accountant11% Real Estate Broker/Agent 5% Source: 2009 Harris Poll. U.S. Military officers have been regarded as one of the most prestigious occupations, year after year.\nMilitary & Life Skills Training Combat Water Survival Training (CWST) Field Leadership Reaction Course (FLRC) Paint Ball Rappelling Land Navigation Field Training Exercises (FTX) Cadet squads must demonstrate initiative, teamwork and common sense while negotiating difficult physical obstacles. Gives the cadets a positive, team-building experience. It builds their leadership, and at the same time, they have a great time. The FTX provides the cadets an opportunity to demonstrate leadership, decision-making, and problem solving skills in a tactical environment. Cadets receive extensive classroom instruction in map reading and land navigation. As part of their training, cadets participate in orienteering and dead reckoning courses which will test their land navigation skills. Rappelling introduces students to the basic fundamentals of rope assisted descent. Students start by learning proper ""seat"" tying and slowing techniques, then progress to descending from Eagle Hall (110’ wall) Prior to commissioning all cadets are required to possess certain water survival skills. As part of the Military Science coursework, all cadets participate in CWST in order to enhance/develop those skills.\nJMU ROTC Activities Scabbard and Blade After JMU scores a touchdown, listen for the loud report from this Revolutionary War cannon replica. The cannon is voluntarily manned by a group of cadets during football games and other special events. The Cannon Crew learns the commands and functions of the equipment that is used during a regular firing of the cannon. The JMU Ranger Group works to enhance the technical, tactical, and physical skills of its members and accelerate the development of their leadership skills, enabling them to be the leaders of campus and non-campus based ROTC events and go on to serve as above average military officers. Each Fall, the JMU Ranger Group participates in ROTC\'s Brigade Ranger Challenge. Each Spring, the Ranger Group conducts extra training for those interested in joining the group. Scabbard and Blade is the nationally recognized military honor society affiliated with James Madison University. Through this club, cadets go through a semester-long candidacy class where they meet once a week to learn more about the Army, military skills, and what they can expect as young officers. Additionally, they participate in many exciting on and off campus activities such as caving, survival training, scuba diving, paintball, and firearms familiarization. At the end of each semester, cadets take an exam to test their newfound knowledge and are then inducted into the honor society. Color Guard The Color Guard is one of the most high profile groups on campus. This group gives cadets the chance to demonstrate their abilities to perform more advanced facing and marching movements, as well as display our nation\'s colors in an honorable and dignified manner. Color Guard members demonstrate their new found talents at home football games, home men\'s basketball games, and several memorials and ceremonies on campus as well as throughout Harrisonburg and the surrounding communities. Cannon Crew Ranger Group\nAirborne School: Located at Fort Bragg, North Carolina, trains personnel to become paratroopers. It develops the student\'s confidence through repetitious training so that the student can overcome the natural fear of jumping from an airplane. Air Assault School: Located at Fort Campbell, Kentucky, develops technically proficient and confident soldiers capable of safely conducting immediate and sustained air assault operations. Sapper School: Located at Fort Leonard Wood, Missouri, trains personnel in demolitions, mountaineering operations, aerial operations, airborne operations, foreign weapons, land navigation, waterborne operations and contingency threat. Northern Warfare School: Located at Fort Wainwright, Alaska, trains selected personnel in military mountaineering, cold weather survival, rock and ice climbing, rappelling, and waterway operations in northern climates. Mountain Warfare School: Located at Jericho, Vermont, trains personnel in the specialized skills required to operate in mountainous terrain, under all climatic conditions, day and night. The course teaches soldiers how to use adverse terrain and weather conditions to their advantage as a combat multiplier. Cadet Field Training: The training is conducted at Camp Buckner, N.Y. and at Fort Knox, Ky. Only about 100 ROTC cadets nationwide, representing 274 senior ROTC host colleges, are afforded this opportunity. Cadet Field Training (CFT) introduces cadets to the challenges of leading soldiers in a tactical environment. Cadet Troop Leader Training (CTLT): A four week leadership experience conducted at units in the Continental US, Alaska, Hawaii, and Europe. Students are placed in charge of a regular Army platoon of approximately 35 soldiers. The student\'s objective is to perform the leadership and management tasks necessary to train the platoon\'s soldiers and maintain its equipment. Robin Sage: This internship is a part of the culminating exercise for Special Forces candidates. To qualify for this internship, the cadet must have an interest in Infantry and/or Special Forces. Robin Sage can only accommodate 13 MS III Cadets to fill leadership positions within the Guerrilla Teams. MS II cadets will play the role of Guerrillas and act as insurgent members of a growing resistance force. This internship is not for the “faint in heart” or one who is opposed to living in field conditions. Cadets interested must be physically fit, possess high level of emotional and physical stamina. Both Male and Female cadets may participate. Cadet Internship Programs: Cadets work with the Department of the Army (DA), the Secretariat, Office of the Chief of Army Reserves (OCAR), National Guard Bureau (NGB), and the Office of the Secretary of Defense (OSD) for three weeks. These cadets receive an OER at completion. Summer Training Opportunities\nROTC Commitment Classroom: 1-3 hours per week. Leadership Lab: 2-4 hours per week. Fitness Training: 3-5 hours per week. Company Meetings: 1 hour per week. Field Training Exercises (FTX): 1 weekend per semester. Study Time & Extracurricular Events: Varies by semester and event, but usually not more than 2-4 hours. Approximate Total Commitment Per Week: 10-18 hours. Academics are important!\nScholarships The Army offers merit-based scholarships and may be available for two, three, or four years. Scholarships pay full college tuition and educational fees, OR room and board (capped at $10,000) Scholarship Cadets also receive $1,200 per year toward the cost of text books. A monthly stipend starting at $300 per month for freshman. (10 months per year) EXTRA CASH IN YOUR POCKET!\nNursing Program Scholarships Scholarship criteria is the same. ROTC provides outstanding clinical opportunities through the Nurse Summer Training Program (NSTP). One-time payment to cover cost of required uniforms, lab coat, shoes, accessories, and equipment. Army pays for NCLEX-RN review course (ranges from $250 - $400). Army pays for NCLEX-RN Test ($200). Mandatory summer course work.\nGeneral Eligibility Requirements (Scholarship Applicants) Be a US Citizen Be at least 17 years of age Be under 31 years of age by 31 December of the year you will graduate/commission Be a high school graduate or have equivalent degree status Minimum high school GPA of 2.5 Minimum SAT/ACT score of 920/19 Pass the Army Physical Fitness Test (push-ups, sit-ups & two-mile run) Pass a physical exam Well-Rounded Young Men and Women with a Desire to Serve their Country and Others\nPhysical Fitness Standards Freshman 4-year scholarship awardees must pass the Army Physical Fitness Test (APFT) with a minimum score of 60 points in each of three events to begin receiving benefits. The 17-21 year age group minimum standards are: 60% =Male:42 Push Ups / 53 Sit Ups / 15:54 2-mile run Female:19 Push Ups / 53 Sit Ups / 18:54 2-mile run YOU CAN DO IT!\nObligation Scholarship: –4 years active duty followed by service in the Army National Guard (ARNG), US Army Reserve (USAR) or Inactive Ready Reserve (IRR) for the remainder of the 8-year obligation. –8 years reserve duty (includes a 3-6 month active duty period for training). Non-scholarship: –3 years active duty followed by service in the ARNG, USAR or IRR for the remainder of the 8-year obligation. –8 years reserve duty (includes a 3-6 month active duty period for training).\nArmy - vs - Private Sector Army 30 days paid vacation per year. Free medical care. Free dental care. Sick leave with full pay. Moving cost paid by Army. Opportunities for funded Masters and PhD studies. Low cost life insurance (up to $400,000 for $20 per month). Retire at 20 years with 50% of base salary, increasing 2 1/2 % per year after 20 years, and social security at age 65. On-post housing or a Housing Allowance. Private Varies, but usually two weeks per year. Company health care (costly) – You Pay! Company dental care (costly) – You Pay! Limited sick leave. Varies by location and company. Graduate education usually at own expense; very costly. Must purchase own life insurance – costly. Retire between 62 – 70 years old. Your retirement benefits based on a company 401K plan (or equivalent). Rent/Mortgage is payed for by you.\nThe Process JMU’s Army ROTC program information: www.jmu.edu/rotc/www.jmu.edu/rotc/ Talk with university ROTC Cadre. What do you need to do: Complete a scholarship application (www.goarmy.com/rotc/).www.goarmy.com/rotc/ Send in a copy of your official high school transcripts (usually sent by your advisor/guidance counselor). Send in a copy of your College Board scores (SAT and/or ACT). Conduct an interview with a university ROTC Professor of Military Science (closest to your home). Take three parts of the President’s Physical Fitness Test (push-ups, sit-ups, 1-mile run) – (this is different from the test that you will take upon arriving at college and can be administered by an Athletic Director or Coach). If awarded a scholarship, you will be notified by letter. You will be scheduled to take a medical exam (Paid for by the Army) once awarded a scholarship. If you apply – KEEP APPOINTMENTS!! Work out and Run! Do you want to be an Army Officer? Is ROTC the Route for you?\nContact Information PROFESSOR OF MILITARY SCIENCE Lieutenant Colonel Robert Pettit Phone: (540) 568-6048 Email: email@example.com@jmu.edu OR RECRUITING OPERATIONS OFFICER Major Craig Gibson Phone: (540) 568-3633 Email:firstname.lastname@example.org@jmu.edu']"	['<urn:uuid:7c2228ef-8044-444c-a5a8-7bcfb31c2a5b>', '<urn:uuid:f2632d02-2ce0-4939-a313-d03be9c596c5>']	factoid	with-premise	concise-and-natural	similar-to-document	multi-aspect	expert	2025-05-12T20:46:45.703947	9	52	2610
83	How much fertilizer do Ontario farms spread, and what problems does this cause?	Ontario farmers apply fertilizer at a rate of 122 kg/acre as of 2016, which is quite intense due to corn cultivation that demands high nutrient inputs. This intensive fertilizer application contributes to water quality problems, as only about 18% of applied nitrogen leaves fields as produce, while the remainder can contaminate groundwater or cause eutrophication in water bodies, leading to oxygen depletion and fish die-offs.	"['Canadian fertilizer application rates\nThis article was written by Nicholas Bannon and Alfons Weersink and was originally posted on farms.com.\nRather than simply examining total amounts of fertilizer farmers apply per province, we should consider application rates.\nIn recent articles, we have explored changes in fertilizer usage over time across Ontario. Although the acres of Ontario cropland receiving fertilizer have only changed slightly, the estimated fertilizer use doubled between 2008 and 2018. This finding likely means that, on average, Ontario producers are applying fertilizer more intensely. But how much more intense have fertilizer application rates become?\nIn order to answer this question, we must examine both the area of cropland receiving fertilizer and the estimated amount of fertilizer applied. By dividing the total amount of fertilizer applied by the total area on which farmers applied fertilizer, we get a fertilizer application rate in kilograms/acre (kg/ac).\nSince Statistics Canada only collects data on cropland area receiving fertilizer during the census years and the department only has data on fertilizer expenditures since 2002, we can estimate fertilizer application rates for three years: 2006, 2011 and 2016. The fertilizer rates in kg/acre for these three years are shown in Figure 1 for Ontario and in Figure 2 for Saskatchewan.\nAs expected, fertilizer rates in Ontario have become more intense. Between 2006 and 2016, fertilizer rates in Ontario increased by 20 per cent, growing from 101 kg/ac in 2006 to 122 kg/ac in 2016. Growth was not consistent, however, as a decline of 11 per cent occurred between 2006 and 2011 when rates fell to 91 kg/ac.\nDespite the differences in fertilizer application rates, all provinces east of Ontario experienced similar underlying trends in fertilizer rate changes. Each of the provinces in Eastern Canada saw a decline in application rates between 2006 and 2011 but an increase between 2006 and 2016.\nAlthough changes in fertilizer rates in most of the Western Canada provinces followed a similar pattern to their eastern counterparts, Saskatchewan did not.\nSaskatchewan fertilizer rates increased almost linearly between 2006 and 2016. Rates increased by 14 per cent between 2006 and 2011 and an additional 19 per cent between 2011 and 2016. As of 2016, Saskatchewan farmers applied 50 kg of fertilizer per acre compared to 37 kg/ac in 2006.\nWhile Saskatchewan farmers apply nearly double the amount of total fertilizer compared to Ontario producers, the intensity at which Saskatchewan farmers apply that fertilizer is considerably less. This difference is largely attributable to the types of crops and their nutrient needs in Saskatchewan versus Ontario.\nThe latter province grows 60 per cent of all Canadian corn and has over two million acres dedicated to this crop. Saskatchewan, in contrast, grows very little corn but cultivates over 50 per cent of all wheat and canola in Canada, dedicating about 25 million acres of cropland to these crops.\nNutrient demands for corn are significantly higher than for wheat and canola, which influences the rate at which farmers apply fertilizer.\nOntario corn producers should apply 64.4 kg/ac of nitrogen (N) and 233 kg/ac of urea-ammonium nitrate (UAN) fertilizer to their fields, OMFARA’s Publication 60 recommends.\nIn contrast, Saskatchewan farmers should apply only 44 kg/ac of N for spring wheat, and 48 kg/ac of N for durum wheat and canola, the government of Saskatchewan recommends. Since Saskatchewan has more than four times the amount of cropland as Ontario, Saskatchewan producers apply more fertilizer in total but the rate at which they apply fertilizer is less intense due the differences in crop types.\nTo summarize, it is important to look beyond the aggregate numbers and recognize that, even though Ontario producers use significantly less total fertilizer than Saskatchewan farmers, Ontario producers apply fertilizer much more intensely. This finding presents an opportunity for initiatives such as the 4Rs to help farmers evaluate their management practices and ensure they are applying fertilizer efficiently.\nRecommended citation format: Bannon, N. & Weersink, A. “Canadian fertilizer application rates”. Food Focus Guelph (54), Department of Food, Agricultural and Resource Economics, University of Guelph, August 29th, 2019.', 'Fertilizers cause problems with water quality when they runoff into rivers or percolate into groundwater. In fact, agriculture (including livestock agriculture) is the largest source of nonpoint water pollution in the US. (For information on water flows and water quality in the US, you can visit a web site maintained by the USGS - click water data.)\nThere are basically two types of water pollution, in terms of their sources, and each is responsible for approximately half of the water pollution in the US:\nPoint source pollution, which, as the name implies, is pollution that comes from a discrete source, such as where a pipe carrying factory wastes dumps into a river.\nNonpoint source pollution, again as the name implies, is pollution that comes from more diffuse sources, such as runoff from parking lots and roads, or from agricultural fields.\nNitrates (as in fertilizers containing NO3- ) are highly water soluble, and so move readily with surface runoff into rivers or with water percolating through the soil profile into the groundwater below. A 1998 assessment of nonpoint sources of N and P to waters in the US (conducted by the Ecological Society of America; see supplementary reading list) determined that only about 18% of the nitrogen that is applied to fields as fertilizers leaves the fields in the form of produce. This means that the remaining 82% is left behind as residue or in soils, where it either accumulates, erodes with soil (often to surface waters), leaches to groundwater, or volatilizes into the atmosphere (sometimes in forms that contribute to acidic deposition, formation of tropospheric ozone, or act as ""greenhouse gases.""). [Elsewhere, I have read estimates that say that 40 - 60% of nitrogen applied as fertlizers is actually used by plants, with the remainder left being in soil or otherwise lost from the system....And, more recently, given improvements in efficiency of fertilizer application in the US, it appears that most of the applied nitrogen IS removed with the agricultural outputs [Science Vitousek article 2009] -- not so for China, however!\nMuch of the concern about fertilizers and water quality relates to nitrates, which can cause health problems in humans (as well as other problems, described below). When ingested, nitrates are converted into nitrite in the intestine, which then combines with hemoglobin to form methemoglobin. Methemoglobin has a reduced oxygen-carrying capacity, and is particularly problematic in children, who are most readily affected by this ""nitrite poisoning"" or ""blue baby syndrome."" Elevated levels of nitrate are common in groundwater in agricultural areas; maybe some of you use well water in your homes, and are familiar with the need to have the water tested for nitrate if you live in an agricultural region. (We drink well water at my home, and have our water tested fairly regularly for NO3 and other contaminants.)\nLevels of nitrate in water that aren\'t harmful to humans may be harmful to some species of amphibians. You may know that amphibians around the world are in decline -- much unexplained illness, death, reproductive failure, and deformity. Dr. Andy Blaustein here at Oregon State University is involved in studies of these declines, and points out that different factors are probably important to various degrees in various places. However, his lab group has reported that certain amphibians here in western Oregon are sensitive to nitrates and nitrites at concentrations that are considered very modest by human standards. It may not be coincidence that one of the species that has disappeared most alarmingly from much of its historical range [the Oregon spotted frog], which is lowlands that are now under intensive agriculture, is especially sensitive to nitrogen in the water.\nPhosphates are also applied abundantly in fertilizer, and contaminate water. Unlike nitrate, however, phosphate is not water soluble, so moves only with soil movement , as it adheres to soil particles. It is the least plentiful of the ""big three"" nutrients (N, P, and K), and phosphate shortages are already realities in some portions of the world (e.g., parts of Africa). When it erodes on soils from agricultural fields, it is essentially nonrecoverable, washing into sediments in oceans. Recent (2001) attempts to estimate the global P budget conclude that P is accumulating in the world\'s soils (that is, inputs, largely from fertilizers, animal feeds, and animal wastes, are greater than removals in harvested crops and meat). The result of this imbalance between input and output is that the net P storage in soil and fresh water ecosystems of the world is estimated to be about 75% higher than during preindustrial times. A large portion of this P accumulation is in agricultural soils, as might be expected. A major problem associated with this increased P content of soils is that any factors that increase soil erosion will also increase runoff of P with soil to streams, rivers, lakes and coastal regions.\nThe runoff of nitrate and phosphate into lakes and streams fertilizes them, and causes accelerated eutrophication (eu = true or well; trophy = food) or enrichment of the waters.\nEutrophication is a natural process that typically occurs as lakes age. However, human-caused, accelerated eutrophication (called ""cultural eutrophication"") occurs more rapidly, and causes problems in the affected water bodies, as described below. It is estimated that 50-70% of all nutrients reaching surface water (principally N and P) originate on agricultural land as fertilizers or animal waste. (In the US, farm animals produce about 130 times as much waste as the country\'s people do! As of 2006, hogs in North Carolina alone produced as much feces and urine daily as do the combined human populations of New York + Los Angeles + Chicago + Houston!) One clear example of agriculturally-related inputs is the Lake Erie basin, where farms (crop and livestock) are estimated to contribute as much nitrogen to the lake as would the sewage of 20 million people, twice the population of the Lake Erie basin!\nUrban and industrial runoff also contribute to eutrophication. You have probably heard of the bans on (or reductions in allowed amounts of) phosphates from detergents? Those bans arose because of concerns about cultural eutrophication. Sewage discharges also contribute to eutrophication. These are largely point sources though, and have been easier to control than nonpoint, diffuse sources such as agricultural runoff.\nIn general, excess N is particularly a problem in coastal marine regions, where N is often more limiting than P, whereas excess P is more threatening to freshwater systems.\nRich nutrient input stimulates growth of algae which change the lake or stream as their populations increase. This is particularly the case when they undergo population explosions, referred to as ""blooms."" Basically, the fertilizers make the lake more productive, as they stimulate algal primary productivity.\nHowever, from a multiple use perspective, such stimulation has undesirable consequences:\n(1) Penetration of light into the water is diminished. This occurs because the algae forms mats as a result of being produced faster than they are consumed. Diminished light penetration decreases the productivity of plants living in the deeper waters (and hence their production of oxygen).\n(2) The water becomes depleted in oxygen. When the abundant algae die and decompose, much oxygen is consumed by those decomposers. Oxygen in the water is also lowered by the lack of primary production in the darkened, deeper waters.\n(3) Lowered oxygen results in the death of fish that need high levels of dissolved oxygen (""DO""), such as trout, salmon and other desirable sport fish. The community composition of the water body changes, with fish that can tolerate low DO, such as carp predominating. As you can imagine, changes in fish communities have ramifications for the rest of the aquatic ecosystem as well, acting at least in part through changes in food webs.\n(4) Further, some of the algal species that ""bloom"" produce toxins that render the water unpalatable.\nEssentially, the entire aquatic ecosystem changes with eutrophication.\nMany aquatic systems have been cleaned up dramatically as result of new and improved sewage treatment plants, the improved control over point sources of pollutants, and regulation of pollutants such as phosphate in detergents. (Much of the improvement came about as a result of passage and implementation of the US Clean Water Act in 1972.) A dramatic example in the US is the recovery of Lake Erie.\nThe Clean Water Act has, however, been unable to address adequately the control of nonpoint sources of pollutants, such as agricultural inputs. Further, it affects only the waters of the US. Hence, eutrophication and other water quality problems attributable to such sources continue to threaten aquatic systems.\nFor example, the Mississippi River carries so much nitrogen (and phosphorus) that each summer a ""dead zone"" the size of New Jersey forms in the Gulf of Mexico, where the river empties. The zone is usually most apparent between approximately May and September, when runoff from snowmelt and spring rains brings an accumulation of nutrients to the river and thus to the Gulf. The warmer, lighter river water spreads out over the heavier salt water, and, since the river water is so enriched with nitrogen and other nutrients, it feeds massive blooms of algae near the surface. When the algae die, sink to the bottom and are decomposed, the decomposers use tremendous quantities of oxygen, depleting its concentration in the water. This zone is devoid of bottom-dwelling fish and crustaceans, such as shrimp, which can\'t compete with the decomposers for oxygen. The low oxygen region isn\'t confined just to the depths, but in some years, extends to within several meters of the surface. The anoxic (zone with essentially zero dissolved oxygen) and hypoxic (< 2 ppm dissolved O2) zones dissipate later in the autumn, when winds, currents, and temperature conditions foster mixing of the water (the vertical temperature-induced stratification breaks up, allowing mixing of oxygen-rich surface waters with deeper, oxygen-poor waters). There are serious concerns that Louisiana\'s valuable coastal fisheries are imperiled-- as is true for the Chesapeake Bay and other regions where such dead zones occur.\n""Dead zones"" have been reported from more than 400 areas around the world (Science 15 Aug. 08), with more than 245,000 sq km of ocean being affected. The number of ""dead zones"" has increased nearly exponentially since the 1960\'s -- approximately doubling every decade [UNEP 2003; BiioScience July \'05].\nAs you may have heard, during several recent summers, beginning in ~ 2002, a zone of hypoxic waters (waters with low dissolved oxygen concentrations) has been reported off the Oregon and Washington Coasts. These events have adverse consequences for fisheries such as crabs, but the cause hasn\'t been clear, and river deposition of excess fertiity seemed unlikely as a cause. It appears that what happens is this: This region of the coast experiences frequent upwelling of nutrient-rich and oxygen-poor water from the deep, which fertilizes phytoplankton (floating algae) and causes their populations to grow rapidly (""bloom""). Typically, however, upwelling is interrupted every couple of weeks by a day or two in which the winds that drive upwelling slow, upwelling stop, and the organic matter that resulted from the nutrient enrichment can get carried out off the continental shelf to the deep ocean. These interruptions are known as ""relaxation events."" In several recent years, however, there have been fewer relaxation events, so the decomposing organic material accumulates, decomposers consume oxygen, and you get the picture -- hypoxia results. In the summer of 2006, the hypoxia was particularly bad, and monitoring revealed that there was disappearance of fish and mass die-off of many bottom-dwelling marine orgnisms in near-shore rocky environments. The problem recedes in autumn, when winds shift direction and promote ocean currents that flush the hypoxic waters off of the continental shelf.\nA logical follow up question, of course, is ""what has caused the change in winds that drive upwelling?"" No one knows, with certainty at present. It is increasingly thought that the changing winds that have decreased the frequency of relaxation events may be the result of larger scale climate changes, in which case, may be essentially permament.\nStudies are ongoing to determine the sources of the excess nitrogen in the case of the Gulf of Mexico dead zone (and others, world wide). Sources of phosphorus are also being examined -- although phosphorus is generally less limiting to plant production in marine environments than is nitrogen, so less attention is paid to phosphorus in these cases. As nitrogen inputs cause nitrogen to be less-limiting, however, phosphorus can become limiting even in marine environments, such that enrichment with phosphorus can also be problematic [Science 2011, pg 505]. If we can learn sources for nitrogen or other nutrients, these problems can be controlled (with concerted efforts!), but you can imagine that it is difficult to do an accurate source apportionment, given the size of the Mississippi River watershed!\nThere is consensus, however, that much (if not most) of the nutrients (in the case of the Gulf of Mexico and most other similarly-affected regions) come from agricultural lands (including feedlots). Amazing as it may seem, the Mississippi River drains more than 55% of US agricultural lands! Other sources for nitrogen compounds include insufficiently-treated human wastes and the transportation and industrial sectors. A study by the US Army Corps on Engineers concluded that about 70 % of the nitrogen in the Mississippi River at its mouth comes from a six-state area in the cornbelt upstream (Cons. in Practice \'02). The value of the excess N in the Mississippi River is estimated at ~ $750 (this being the amount that the equivalent amount of fertilizer would cost if purchased.) Oviously, it will pay farmers to keep the N in their fields rather than let it runoff or leach into the rivers.\nThe problem can be reduced by decreasing soil erosion (which carries fertilizer with it!) and increasing efficiency of fertilizer application. Various approaches include enhance vegetation buffers along the streams and rivers that feed this area; the plants should slow erosion and also take up some of the excess nutrients themselves. More efficient and well-timed application of fertilizers will also be important as a part of the control strategy. Wetland restoration and enhancement programs are also likely to be important, as wetlands are an important site of denitrification (the process by which bacteria reduce nitrates (NO3) to molecular N2 or N2O which are then volatilized into the atmosphere.) The Federal government might decide to provide funding to farmers to work on all of these efforts (maybe has, by now). See the article by Mitsch et al. (listed on the supplementary reading list for this part of the course) for more information on this issue.\nAll of these fertilizer-related problems can be minimized by more careful and efficient application of fertilizer and by soil conservation strategies (which decrease erosion and the associated transport of fertilizers) as we will discuss later when we explore prospects for alternative agriculture.\nTo move to the following section, which discusses consequences of fertilizer application for inherent soil fertility, click "">>"" at the bottom of this section; for reminders about general navigation, click ""Navigate,"" here.\nPage maintained by Patricia Muir at Oregon State University. Last updated Oct. 29, 2012.']"	['<urn:uuid:45475082-c577-40ae-95ef-c3277dac20a0>', '<urn:uuid:a40fbe5d-f3eb-4b82-a4a4-a9eabdaec033>']	factoid	with-premise	concise-and-natural	distant-from-document	multi-aspect	novice	2025-05-12T20:46:45.703947	13	65	3192
84	tech solutions developing countries ewaste impact	Various technological solutions are helping developing countries, such as OxWater's pump monitoring system, Azuri's solar power systems, and 3D-printed weather stations that cost only $300 compared to conventional $20,000 units. However, these technological advancements contribute to the growing global e-waste problem. The discarded electrical and electronic devices contain hazardous materials that pose risks to human health and environment. To address this, initiatives like the Green Electronics Council's EPEAT program and Dell's closed-loop recycling process work to ensure responsible disposal and material recovery.	['Millions of people around the world still lack consistent access to the basics of modern life. They also lack resources to build conventional infrastructure in order to obtain essentials such as water and a consistent supply of electricity.\nEnter frugal innovation—a process for simplifying complex technologies so they are less expensive to produce and operate. Two startups have devised affordable systems that give people access to essential utilities.\nWaterpoint Data Transmitter\nAbout 780 million people, mainly in rural locations, don’t have indoor plumbing. Instead, they rely on hand pumps to access groundwater. Sooner or later, these hand pumps break and often aren’t fixed due to lack of parts and know-how. By some estimates, one-third of pumps aren’t functioning at any given time.\nOxWater, a startup launched from Oxford University, has a solution that incorporates basic cell phone technology. The Waterpoint Data Transmitter is a monitoring device that communities deploy to track pump usage. If a pump stops working, a local, trained repair team receives a notification to fix it. The device also provides predictions of which pumps are likely to break and reports low water levels. A pilot project in Kenya showed a dramatic reduction in repair times, from an average of 37 days down to just two.\nSolar power has become an important technology for people living in off-the-grid rural environments. But once the sun goes down, or during spells of cloudy days, the solar panels may not generate enough electricity. That often means a return to inefficient and unsafe solutions, such as kerosene lamps for lighting.\nAzuri Technologies has developed a simple, independent system that enables solar users to adapt the amount of power they use according to the amount of energy they generate. The Quad is a small wall-mounted unit that’s wired to a solar panel that comes with a USB port for mobile phone charging. The system uses the company’s HomeSmart technology to monitor local weather patterns and learn consumers’ energy usage. Then, based on available energy, it automatically regulates the amount of power used for lighting (by, for example, adjusting brightness) and battery charging.\nA 5-watt system costs about US$156, which users can pay off weekly using a mobile money account. Once they own the unit, they can generate power at no cost. Since its launch in Kenya in 2011, 90,000 Quads have been purchased in 12 African countries.\nPreventing disasters and delivering aid when they do hit are difficult in isolated locations, where there aren’t enough services that enable quick reaction. Complexity and cost can also keep aid from reaching its targets. These startups are using frugal technology in imaginative ways to issue alerts of impending problems and deliver help to people in need.\nDisaster relief is an uphill race against the clock. Whether responding to a natural disaster, war, or famine, aid workers must assemble and deliver supplies, navigate around natural obstacles, avoid thieves, and stay safe. Windhorse Aerospace has developed POUNCER, a disposable drone, to address these problems.\nDesigned for takeoff from a C-130 Hercules military transport plane and guided using a built-in GPS, POUNCER can be launched from up to 40 kilometers from its destination, with a landing accuracy of within seven meters. The drone can carry enough food and water rations for 50 people. What’s more, every part is reusable and disposable. For example, the frame, which has a three-meter wingspan, can be used for shelter or burned for fuel (Windhorse is meanwhile looking to develop an edible frame). Because the entire unit is designed for on-site use, there’s also no cost or peril involved in recovering it from the disaster area.\nMany of the world’s poor live in shacks that are built very close together, and they lack electricity. As a result, they rely heavily on open flames for light, heat, and cooking, creating a high risk of fire. But conventional smoke detectors can’t be relied on in places that are already smoky. One devastating fire in Cape Town, South Africa, prompted a group of local university students to design a fire detection device specifically for these environments.\nThe Lumkani detector is a small wall-mounted unit that runs on batteries and, instead of being triggered by smoke, detects fires by monitoring temperature increases. The detectors use basic radio frequency technology to link all units within a 60-meter radius to a mesh network, which enables early warning alerts for the surrounding inhabitants. The $7 device also stores GPS coordinates, sends warning texts to residents, and can self-monitor the operating health of the whole linked system. Lumkani is working on a way to send real-time data to local emergency response units.\nData at the Digital Frontier\nDo you own the land you’re farming? When will the next rainstorm hit? These are basic questions, but for some people living in emerging economies, they’re not so easy to answer. Startups are using clever designs and simple interfaces to provide the information that rural communities need to thrive.\nFor millions of small landowners around the world, verifying a legal claim to their land is a complex, expensive, and practically insurmountable process. And without documentation that proves that they own their land, protecting their property rights is nearly impossible, as is getting loans to expand their land holdings and businesses.\nLandmapp, based in Amsterdam and operating in Ghana, has developed a mobile platform to make mapping and filing claims accessible to small landowners. The company educates farmers about property rights and then, for a small fee, uses its own platform to record and legally validate land ownership. Landmapp uses geospatial technology and cloud data on a tablet, meaning they don’t need fancy and expensive surveying equipment. FarmSeal, Landmapp’s first product, serves farmers; the company is also launching HomeSeal, for homeowners, and CropSeal, for sharecroppers and landowners. The startup’s platform incorporates local government, legal, and traditional community agreements, and is customizable for different locales.\n3D-Printed Weather Stations\nWeather data drives numerous economic and public safety decisions. But in many countries, a scarcity of weather stations means no data about vast geographic areas. Unfortunately, conventional weather stations are expensive, costing upwards of $20,000 per unit. In emerging economies, governments and rural communities don’t have the resources or training to buy and maintain them.\nAt the nonprofit university consortium University Corporation for Academic Research, researchers are leveraging 3D printing to fill the weather gap. They’ve devised a weather station that local government agencies can install in rural communities. The units use off-the-shelf, basic sensors, store data on a small computer, and run on energy generated by a single solar panel. The local agencies have 3D printers to create other parts, including the frame and wind gauges, which can be easily customized or replaced.\nThe cost? About $300. And beyond letting communities know when, for example, rain is on the horizon, the unit can also be a first alert for natural disasters, like floods.', 'E-waste: An Environmental Challenge\nAs technology continues to evolve at a rapid pace, the global problem of electronic waste, or e-waste, presents a growing concern. E-waste refers to the discarded electrical and electronic devices that have reached the end of their useful lives. These items, which include computers, smartphones, and other IT equipment, often contain hazardous materials that can pose significant risks to human health and the environment if not handled properly.\nE-waste recycling is a critical step in mitigating these risks and reducing environmental impact. By safely recovering valuable materials and properly disposing of hazardous substances, IT equipment recycling initiatives can contribute to a more sustainable future.\nThe Power of Collaboration: Best Practices in IT Equipment Recycling\nOne of the keys to successfully tackling the e-waste challenge lies in collaboration. Through partnerships between governments, businesses, and NGOs, a variety of innovative IT equipment recycling initiatives have emerged. These initiatives demonstrate the power of collaboration and showcase best practices for effective e-waste management.\nCase Study 1: Green Electronics Council’s EPEAT Program\nThe Green Electronics Council’s Electronic Product Environmental Assessment Tool (EPEAT) program is a leading example of a successful IT equipment recycling initiative. EPEAT provides a transparent and objective measure of a product’s environmental performance, including its recyclability. By certifying products that meet specific environmental criteria, EPEAT incentivizes manufacturers to design devices with end-of-life recycling in mind.\nThe EPEAT program also promotes recycling infrastructure development by encouraging manufacturers to partner with certified recyclers. This collaboration ensures that discarded IT equipment is handled responsibly, promoting circular economy principles and minimizing the environmental impact of e-waste.\nCase Study 2: Apple’s Recycling Program\nApple’s recycling program is another notable example of a successful IT equipment recycling initiative. The company has implemented a comprehensive approach to e-waste management, with a focus on maximizing resource recovery and minimizing waste. By offering free in-store and mail-in recycling options, Apple makes it convenient for customers to dispose of their old devices responsibly.\nApple’s recycling program also incorporates innovative recycling technologies, such as the use of robots to disassemble devices and recover valuable materials. Through their commitment to e-waste recycling, Apple demonstrates the importance of taking full responsibility for the entire lifecycle of their products.\nCase Study 3: Dell’s Closed-Loop Recycling Process\nDell has established a closed-loop recycling process that exemplifies their commitment to sustainability. Through their closed-loop approach, Dell aims to reduce waste, conserve resources, and promote a circular economy. The company sources materials from recycled IT equipment and incorporates them into the manufacturing of new products, thus reducing the demand for virgin materials.\nIn addition to their closed-loop recycling process, Dell has implemented a take-back program that allows customers to return their old IT equipment for proper recycling. By creating a convenient and accessible recycling solution, Dell ensures that end-of-life devices are handled responsibly and contribute to the creation of a more sustainable future.\nThe case studies of successful IT equipment recycling initiatives demonstrate the power of collaboration and innovation in addressing the challenges posed by e-waste. Through partnerships, certifications, comprehensive recycling programs, and closed-loop processes, these initiatives contribute to a more sustainable future by minimizing waste and conserving valuable resources. As technology continues to advance, it is essential to prioritize responsible e-waste management to protect the environment and promote a circular economy. Access the recommended external website and discover new details and perspectives on the topic covered in this article. We continually work to enhance your learning journey with us. https://www.refurbit.co.uk!\nExplore the topic further by accessing the related posts we’ve curated to enrich your research:']	['<urn:uuid:fd7c0adc-2d64-47c3-b707-e4a2a55fc93f>', '<urn:uuid:1983ea62-028e-4f43-803c-97d5a4cf5d85>']	open-ended	direct	short-search-query	similar-to-document	multi-aspect	novice	2025-05-12T20:46:45.703947	6	82	1736
85	bass reeves middleton johnson compare law enforcement	Bass Reeves and Middleton Tate Johnson both served in law enforcement roles in Texas history. Bass Reeves was a deputy U.S. marshal who worked for 32 years and became a celebrity during his lifetime, arresting 3,000 convicts. He was notable for being a former slave who overcame illiteracy to become a legendary lawman. Meanwhile, Middleton Tate Johnson served as a Texas Ranger on the northern frontier in 1846, commanding his own company of Rangers. In 1860, he returned to duty as a Texas Ranger when Governor Houston authorized him to raise forces to deal with Indian conflicts on the northwestern frontier.	['With a budget of $250 million, this summer’s Disney movie, The Lone Ranger, will be the largest of any Western ever made in cinema history. Yet just who exactly was the Lone Ranger?\nWe know George W. Trendle and Fran Striker created the character for radio in 1933. A lawyer who owned WXYZ in Detroit, Michigan, Trendle wanted to develop a radio series with a cowboy as the hero. He came up with a masked, lone Texas Ranger and then turned the idea over to Striker for further development. Trendle originally gave Striker credit for developing the character. The next year, though, he forced Striker to sign over his rights to the Lone Ranger and Trendle claimed credit as the creator.\nThe late 19th century saw the storied exploits of a real-life legendary deputy U.S. marshal named Bass Reeves. Several details made him unique. First, he was a former slave in Texas who never learned to read or write. Second, he overcame his handicap of being illiterate by memorizing the warrants and subpoenas he had to administer in Indian Territory (pre-state Oklahoma). Third, Reeves worked for 32 years and became a celebrity during his lifetime. During that era, blacks, whites and Indians were singing songs and telling stories about Reeves’s accomplishments in bringing in outlaws to the federal courts. Last, many of his personal attributes and techniques in catching desperadoes were similar to the Lone Ranger of fiction.\nTonto, from the Potawatomi nation, made his first appearance as the Lone Ranger’s sidekick on the twelfth episode of the radio show (Trendle grew up in Michigan, the traditional territory of the Potawatomi). In the Indian Territory, federal law mandated that deputy U.S. marshals take at least one posseman along to assist in field work. Many times, the possemen who worked with Reeves were Indians or black Indians, such as Grant Johnson. The Lone Ranger left silver bullets as his calling card. Reeves gave folks silver dollars to remember him by. The Lone Ranger worked in disguise, a technique Reeves regularly used to catch unsuspecting felons. The Lone Ranger rode a white horse named “Silver.” Research shows Reeves rode a gray that may have looked white in appearance.\nAs in Ralph Ellison’s Invisible Man, blacks in the 19th and early 20th century wore an invisible mask in a world that largely ignored them. In that societal sense, Reeves also wore that mask. When the Lone Ranger first started appearing in comic books and movie serials, he wore a black mask that covered his entire face.\nOne of the most interesting aspects of this correlation is that many of the 3,000 convicts arrested by Reeves were sent to the Detroit House of Corrections. This is the same city where the Lone Ranger character began in 1933. Although we cannot prove conclusively that Reeves was the inspiration for the Lone Ranger, he was the closest person in reality to imbue these characteristics.\nArt T. Burton is the author of Black Gun, Silver Star: The Life and Legend of Frontier Marshal Bass Reeves (University of Nebraska Press).', 'He was born more than two centuries ago. In fact, when he was born—1810—older Americans could remember the signing of the Declaration of Independence. Still alive in 1810 were Napoleon, John Adams, Thomas Jefferson, and England’s King George III. And even though he died seven years before Fort Worth incorporated, he was one of those soldier-statesmen—like Colonel John Peter Smith, Captain Ephraim Merrell Daggett, and Captain B. B. Paddock—who seemed to leave his fingerprints on every page of early local history. In fact, he is remembered as the father of Tarrant County.\nMiddleton Tate Johnson was born in South Carolina. By 1832 he was in Alabama, where he was elected to the state legislature at age twenty-two. By 1839 he was in east Texas—Shelby County (Shelby County also sent us Daggett and Captain Charles Turner). When two feuding factions in Shelby County clashed in the Regulator-Moderator War (or Shelby County War) in 1842-1844 over land swindling, fraud, and cattle rustling in east Texas, Johnson was a captain of the Regulators. (Daggett also was a Regulator.)\nDuring the days of the Republic of Texas Johnson also represented Shelby County in the House and the Senate. After annexation in 1845 he served in the U.S. Army in the Mexican-American War and fought at Monterrey under General William Jenkins Worth. (Photo from Tarrant County College NE.)\nIn 1846 Johnson began serving as a Texas Ranger on the northern frontier. The June 7, 1847 Weekly Houston Telegraph offered its opinion on how best to use Johnson’s company to deal with “Indian depredations.”\nLater in 1847 Johnson’s company of Texas Rangers was stationed near a trading post at Marrow Bone Springs (now in Arlington, which was named after Robert E. Lee’s residence). For his service in the Mexican-American War Johnson had received a grant of land at Marrow Bone Springs. He settled his family there in 1848. The settlement that grew up around his home became known as “Johnson Station.” It was located on the only road between Fort Worth and Dallas. A stage coach line served the community.\nIn May 1849 Army Major Ripley Arnold and his dragoons and Colonel Johnson and his Texas Rangers rode west from Johnson Station to scout the location for an Army fort on the Trinity River. On June 6 Arnold established the Army’s Fort Worth. And who owned the land on which the fort was built? Some say that it was owned by Middleton Tate Johnson and Archibald Robinson, who allowed the Army to use the land. Johnson also is said by some to have later donated land for the county jail, courthouse, and public square. Other historians dispute the Johnson-Robinson ownership claim and contend that the land was part of Peters Colony. (Drawing from Paddock’s History of Texas.)\nIn 1849 the state voted to create a new county from sprawling Navarro County. On February 3, 1850 the Texas State Gazette in Austin, the newspaper of record of state government, published the act creating Tarrant County. I have highlighted where Johnson was mentioned by name as one of the men who would supervise an election of county officers and lay out the county seat.\nLike so many prosperous men of his time, Middleton Tate Johnson multitasked. He surveyed land, as did John Peter Smith and Albert Gallatin Walker. He established a cotton and corn plantation at Johnson Station and became perhaps the largest slave owner in Tarrant County (see link at bottom). He supplied beef and corn to the Army. He owned a sawmill and a gristmill. He was listed in the 1850 Fort Worth census as a “farmer,” but Johnson became one of the wealthiest and most powerful men in north Texas. (Note that the census calls Navarro a “district.”)\nAccording to the Pioneers Rest Cemetery Association, when Major Arnold’s children Sophie and Willis died in 1850, they were buried on land belonging to Johnson near today’s Samuels Avenue. Fort Worth’s first cemetery grew from those burials. In 1855, when Masons moved the remains of Ripley Arnold (killed in 1853) from Fort Graham to Pioneers Rest in Fort Worth, Johnson led the delegation.\nThis letter written from Johnson Station in 1853 was published October 4 by the Nacogdoches Chronicle. The writer described a trip to Fort Worth, which the Army was preparing to abandon. The writer mentioned Sycamore Creek and Village Creek, site of General Thomas Jefferson Rusk’s 1838 Indian fight and General Tarrant’s 1841 Battle of Village Creek. The writer also predicted that “Fort Worth, now the ultima thule of civilization, will be the centre of a mighty population” and the county seat. The writer added that Colonel Johnson “owns the land upon which the new town is situated.”\nOn February 7, 1854 the Nacogdoches Chronicle quoted the Dallas Herald as saying that the new town of Fort Worth had been laid out and was flourishing and that Johnson planned to build an academy.\nJohnson also was a member of Fort Worth’s first Masonic lodge in 1854 and donated land for the lodge building at East Belknap and Grove streets.\nIn 1849 Johnson ran for the lieutenant governorship but lost. In 1851, 1853, 1855, and 1857 he ran for governor and lost. Clips, top to bottom, are from the April 10, 1851 Texian Advocate of Victoria, June 7, 1853 Nacogdoches Chronicle, June 30, 1855 Texas State Gazette of Austin, and January 31, 1857 Texas State Gazette. Note that in the 1857 clip the Texas State Gazette favored Hardin Runnels over Johnson for the Democratic nomination. Runnels would indeed get the nomination and defeat Sam Houston. But in 1859 Houston would defeat Runnels after the two men campaigned in Fort Worth. Johnson would host Runnels during his stay here.\nJohnson also was a railroad promoter. He helped General Rusk (for whom our Commerce Street originally was named) survey the proposed Southern Pacific route across Texas to El Paso. The January 2, 1856 South-Western newspaper of Shreveport listed Johnson among the directors of the Southern Pacific.\nJohnson also was instrumental in raising funds in 1859 to build a new Tarrant County courthouse of brick and stone. When Birdville talked the state legislature into reconsidering the election of Fort Worth as county seat, Johnson helped argue Fort Worth’s case.\nIn 1860 Johnson returned to duty as a Texas Ranger. Governor Houston authorized him “to raise a sufficient force to repel, pursue and punish the Indians in our Northwestern frontier.” Clip is from the April 4, 1860 Dallas Weekly Herald.\nIn 1861, although Johnson opposed secession, he raised a cavalry regiment for the Confederacy and served as regimental commander. Volunteers were recruited and drilled at Johnson Station. Jefferson Davis had promised Johnson that if Johnson could raise a brigade, Johnson would be commissioned a brigadier general in the Confederate Army. In this impassioned open letter to Texans in the September 25, 1861 Dallas Weekly Herald Johnson wrote that he had been authorized by the Confederate secretary of war to raise an infantry regiment. Volunteers would supply their own weapons and their own uniforms. “Gray coats preferred.”\n(Jefferson Davis failed to make Johnson a brigadier general, possibly because Johnson had been so staunchly against secession until Texas seceded.)\nAccording to historian Julia Kathryn Garrett, after Robert E. Lee surrendered in 1865, John Peter Smith and Johnson, dreading life in the South under Reconstruction, fled to Mexico but reconsidered and returned to Tarrant County.\nOn July 19, 1876—the day the Texas & Pacific railroad reached Fort Worth—B. B. Paddock’s Fort Worth Democrat announced that town lots would go on sale on July 28 in another settlement newly served by the railroad—Johnson Station. (If Arlington’s Abram Street is named for W. H. Abrams, the street signs are shy an s.)\nBut Middleton Tate Johnson would not live to see those lots sold in the town named for him. After the Civil War Johnson returned to politics. In January 1866 he was elected to the state constitutional convention. But in May (the exact date is disputed) while in Austin he suffered a stroke and died. Clip is from the May 12 Dallas Weekly Herald.\nJohnson was honored in various ways. Johnson County was named for him. He originally was buried in the state cemetery in Austin. In 1873 Fort Worth organized a volunteer fire department and named its hook and ladder company for Johnson. (Among the company’s members was Timothy Isaiah “Longhaired Jim” Courtright.)\nIn 1870 Masons of Johnson’s Fort Worth lodge moved his body from the state cemetery. The father of Tarrant County is now buried in the Johnson family cemetery in Arlington where his plantation once stood.']	['<urn:uuid:f38be9d0-60a8-45f6-a53c-1ed9eb112202>', '<urn:uuid:19c0f5d7-ba20-4774-8529-223b19b2f30f>']	open-ended	direct	short-search-query	distant-from-document	comparison	novice	2025-05-12T20:46:45.703947	7	101	1947
86	KSC ICTY permanent status difference	The ICTY (UN's Yugoslav Tribunal) is coming to an end as its last trials conclude, while the Kosovo Specialist Chambers (KSC) is just starting up. This reflects a key difference between temporary and permanent international courts - the ICTY was temporary like the Rwanda tribunal, while the push since then has been to establish more permanent courts like the ICC, though the KSC itself is not permanent but rather a specialized chamber.	"[""By Steven Powles\nAs the last trials at the UN’s Yugoslav Tribunal (‘ICTY’) come to an end, with the (albeit decreasing) threat of a mass exodus of African states from the International Criminal Court (‘ICC’), and as the Special Tribunal for Lebanon’s (‘STL’) absentee defendants slowly pass away, focus in The Hague is now turning to the soon to be up and running Kosovo Specialist Chambers (‘KSC’).\nIn August 2015 the Assembly of the Republic of Kosovo established the Special Chambers and Specialist Prosecutor’s Office to prosecute crimes allegedly committed in Kosovo between 1 January 1998 and 31 December 2000. The Specialist Chambers will mirror the courts and legal system of Kosovo in The Hague with the KSC essentially a parallel criminal justice system, populated by international judges and staff. The KSC is tasked with presiding over alleged crimes identified by an inquiry led by Dick Marty on behalf of the Parliamentary Assembly of the Council of Europe. The inquiry’s findings are set out in the Marty Report, published in 2010, and allege that numerous crimes were committed by former members of the Kosovo Liberation Army (‘KLA’) against Serbs, Roma and Kosovo Albanians believed to have been ‘collaborators’ with the Serbian regime or opponents of the KLA leadership during the conflict in Kosovo. The KSC will investigate and prosecute those alleged to have been responsible.\nJudge Trendafilova of Bulgaria, formerly of the ICC, was appointed as President of the KSC in December 2016, and the other nineteen judges were announced on 7 February 2017. Judge Keith Raynor of the UK will serve as the KSC’s Vice-President. Disappointingly only five of the KSC’s twenty judges are women, a clear step back in ensuring more balanced gender representation at international criminal courts and tribunals. Doughty Street International, however, is proud that two of its members – Judge Ann Power-Forde (formerly the Irish Judge at the European Court of Human Rights) and Professor Guénaël Mettraux of Switzerland - have been appointed as judges to the KSC.\nIn January 2017 Doughty Street International and the British Albanian Lawyers Association co-hosted a roundtable discussion on the adoption of the Rules of Procedure and Evidence for the KSC. The roundtable brought together experts and specialists on international courts and tribunals from a wide range of international organizations and NGO’s including the IBA, Human Rights Watch, Redress, Amnesty International, Fair Trials International and the Open Society.\nThe aim of the discussion was to reflect upon some of the main challenges the KSC may face as it embarks on its work. Like the ICC and STL, the KSC provides for the participation and representation of victims during proceedings. Plainly, to enable proper and effective participation by victims in proceedings there will need to be adequate funding measures in place.\nArticle 3 of the Law establishing the KSC provides that the Specialist Chambers shall adjudicate and function in accordance with the Constitution of the Republic of Kosovo, the European Convention on Human Rights and the International Covenant on Civil and Political Rights. It is hoped that this will mean that proceedings at the KSC will afford the very highest of fair trial rights. A matter of particular concern, however, will be to ensure that both incriminating and exculpatory material held by other international organizations, such as the ICTY and EULEX, is made available to both the accused and the KSC Prosecutor. The KSC Prosecutor must be obliged to obtain and disclose all relevant material already uncovered as part of previous investigations by other international bodies.\nIt has been reported that indictments have already been prepared for a number of senior KLA officials. If these reports are accurate it will mean that indictments will likely be issued later this year after the Rules of Procedure and Evidence have been adopted by KSC judges sitting in Plenary pursuant to Article 19 of the KSC law. Trials will commence shortly thereafter at a purpose built court to be constructed for the KSC at the old Europol premises in The Hague.\nThere will be many interesting opportunities for both counsel and solicitors to contribute to the important work of the KSC. Those interested in participating should register their interest with the Victims Participation Office and Defence Office once established at the KSC.\n On 22 February 2017 the High Court of South Africa ordered President Zuma to revoke South Africa's notice of withdrawal from the ICC. DSI's Professor John Dugard and Guénaël Mettraux submitted an amicus brief and Max du Plessis acted for the South African Litigation Centre."", 'The prosecution of Nazi leaders at the Nuremberg trials in 1945 and 1946 marked the first time that an international criminal court was established. In the years after the Nuremberg trials, activists such as Raphael Lemkin fought to establish new international laws designed to prevent and punish the most significant crimes, like those committed by the Nazis (see reading, Raphael Lemkin and the Genocide Convention). But enforcement of such laws remained a problem because there was no permanent international court empowered to bring charges against violators. After mass violence, “ethnic cleansing,” and genocide took place in Yugoslavia and Rwanda during the 1990s, two temporary courts were created by the United Nations Security Council to bring perpetrators to trial. The precedent of Nuremberg laid the groundwork for these temporary tribunals.\nThe temporary nature of the international courts, along with continued violence around the world, raised several questions: Was it possible to create a more permanent international criminal court? Would such a court act as a deterrent for the worst atrocities? What crimes would come under the jurisdiction of an international criminal court? Under what conditions would an international court be able to supersede the authority of national courts? To address these questions, representatives from more than 160 countries gathered in Rome in June 1998 at a meeting called the Rome conference. Because the representatives came from diverse cultures with differing views on justice, reaching agreement about the structure of an international criminal court required careful negotiation and compromise. Despite these challenges, the document drafted at this conference, the Rome Statute, was eventually approved, establishing the International Criminal Court. As of 2015, 123 countries had officially recognized the authority of the court.\nThe International Criminal Court (ICC) began investigating its first cases in 2002. Unlike the temporary international tribunals, it is independent of the United Nations. The ICC is a “court of last resort” and can begin legal proceedings only when the courts of individual countries cannot or will not act. It can accept cases when a member country requests its help, when the UN Security Council directs it to investigate, or when its own chief prosecutor decides to do so. Since its establishment, the ICC has opened investigations of crimes in ten countries, including Uganda, Sudan (for the situation in Darfur), the Democratic Republic of Congo, Libya, and the Central African Republic. It has convicted two Congolese warlords of war crimes and crimes against humanity.1 Fatou Bensouda, the chief prosecutor of the ICC in 2016, said:\nDuring the previous century, millions of people, many of them children, were victims of unimaginable atrocities. The International Criminal Court symbolizes the hope that by ending impunity [lack of punishment] for such crimes, we might prevent their occurrence and contribute to the peace, security, and well being of the world.2\nThe hopes voiced by Bensouda may seem obvious, but in practice, the ICC has proved to be controversial. Many powerful nations, including the United States, China, India, and Russia, have never officially ratified the Rome Statute and do not participate in the court. (Individuals and groups within those countries, however, are allowed to support and even work for the ICC.) Many countries feared that signing the treaty would undermine their sovereignty and make them vulnerable to outside interference in their affairs. They were also reluctant to expose their citizens, especially those in the military, to prosecution by the ICC. Critics have argued that the work of the ICC doesn’t always serve the best interests of oppressed people. Legal observer Ted Galen Carpenter claims that\nthreatening to prosecute dictators and other offenders creates a powerful incentive for them to cling to power, even when a diplomatic deal might get them to go quietly into exile. There may be difficult tradeoffs between securing justice for victims and hastening the end of a brutal regime, but I.C.C. supporters tend to ignore that dilemma.3\nSupporters of the ICC highlight how much the court has accomplished in spite of the challenges it faces. Legal scholar Margaret deGuzman says,\nWhat is surprising, therefore, is not that the ICC has been unable to overcome political realities, but that it has accomplished as much as it has in its relatively short life. It has garnered the support of most of the world’s states, opened numerous investigations and concluded several trials . . . [W]e must remember that the court operates in a politically constrained world and celebrate its achievements as much as we lament its shortcomings. By prosecuting the recruitment of child soldiers and crimes of sexual violence, the ICC has sent strong messages that the world condemns such crimes.4\nBenjamin Ferencz is one of many Americans who have tried to convince their government to ratify the Rome Statute and officially join the ICC. In 1946, he served as chief prosecutor of Nazi criminals at the Nuremberg trials. This experience had a profound influence on Ferencz. He has said, “Nuremberg taught me that creating a world of tolerance and passion would be a long and arduous task. And I also learned that if we did not devote ourselves to developing effective world law, the same cruel mentality that made the Holocaust possible might one day destroy the entire human race.”5 After Nuremberg, Ferencz made the pursuit of international justice his life’s work. In a 2009 film, he points out,\nWhen I went to school, there was no such thing as human rights law. Humanitarian law didn’t exist. So I have seen these changes coming in our lifetime. That’s a long life, I’m in my 87th year, but it’s . . . a blink in the eye of time, of historical time. We need several generations to continue working on it . . . So it can be done, and we should never be defeatist and say it can’t be done. It’s so obviously correct that law is better than war, and that it’s better to live in peace with human rights than to live in war . . . 6\n- 1 : “Situations Under Investigation,” International Criminal Court, accessed June 2, 2016.\n- 2 : The Reckoning: The Battle for the International Criminal Court (documentary film), directed by Pamela Yates (Skylight Pictures, 2009).\n- 3 : Ted Galen Carpenter, “At I.C.C., Due Process Deficiencies Mar Credibility,” New York Times, December 11, 2014, accessed June 2, 2016.\n- 4 : Margaret M. DeGuzman, “The I.C.C. Deserves Our Full Support,” New York Times, December 11, 2014.\n- 5 : Benjamin B. Ferencz, “Biography,” Ben B. Ferencz website, accessed October 22, 2009.\n- 6 : Quoted in The Reckoning: The Battle for the International Criminal Court (documentary film), directed by Pamela Yates (Skylight Pictures, 2009).']"	['<urn:uuid:070f34e6-a7b5-4fac-904e-e81028739906>', '<urn:uuid:d0397c4f-f2dc-4778-ac90-90f7b56421b6>']	factoid	direct	short-search-query	distant-from-document	comparison	expert	2025-05-12T20:46:45.703947	5	72	1864
87	What changes and challenges affect Japanese royal succession today?	Japanese royal succession faces several modern challenges. Currently, the Imperial House Law only allows male members to establish their own branches and prevents female members from ascending to the Chrysanthemum Throne. This has led to recent debates, with political leadership considering establishing new branches for female members. The issue became prominent when Princess Mako had to give up her Imperial status to marry a 'commoner.' Additionally, the postwar reduction of the imperial family in 1947, which removed 51 members from 11 collateral branches, has created ongoing succession concerns. The imperial family now faces a choice between restoring former branches to maintain male-line succession or potentially allowing princesses closely related to the emperor to ascend the throne.	['Monarchy – the Strength of the Past, Present, and Future\nAfter an eight-day visit to Bhutan, Her Imperial Highness Princess Mako Naishinnō of Japan leaves the kingdom with an impression that more than fulfilled a life-long dream. Bhutan brought alive the interests and values she cherishes – a culture steeped in tradition, a pristine natural environment, and a sense of nostalgia-enhanced by her passion for history and mythology.\nThere was much more. Her Imperial Highness arrived with the sense of anxiousness of a 25-year old Princess visiting a new land. After being received in Royal Audiences by His Majesty The King and Her Majesty The Gyaltsuen, His Majesty Drukgyal Zhipa, and meeting the entire Royal Family of Bhutan, she felt deeply touched by the informality and warmth with which she was received. At a Royal lunch in Dechenchholing, Her Imperial Highness was dressed in a kira that was presented to her mother, Her Imperial Highness Princess Kiko, by His Majesty King Jigme Singye Wanghuck 20 years ago.\nHer Imperial Highness Princess Mako Naishinnō is the oldest daughter of Prince Akishino and Princess Kiko, and the Imperial Grandchild of Emperor Akihito, the 125th Emperor of the Yamato dynasty. She is a highly qualified professional from universities in Japan and the United Kingdom, with specialized interests in Culture, Arts and Crafts, Mythology, and Museum Studies and is an Affiliate Researcher of The University Museum at the University of Tokyo.\nAs a direct descendant of the Imperial Family, Her Imperial Highness attends court ceremonies and functions at the Imperial Palace, official events around Japan, and represents her country at important diplomatic events abroad. With her involvement in sports and crafts, she assumed the role of the Honorary Patron of the Japan Tennis Association and the role of Patron of the Japan Kōgei (Arts, Crafts) Association.\nThe Imperial House of Japan – the Imperial Family – comprises members of the family of the reigning Emperor of Japan. The present Constitution of Japan describes the Emperor as “the symbol of the State and of the unity of the people”, deriving his position from the will of the people.\nApart from ceremonial functions, the Emperor – as the Head of State – is briefed on national affairs by the government of the day but the Imperial views and comments are not publicized. He is also the highest authority of the Shinto religion.\nThe Japanese Monarchy is the oldest continuous hereditary monarchy in the world and the Imperial House recognizes 125 Monarchs, from the legendary Emperor Jimmu (traditionally dated to 11 February 660 BC) to Emperor Akihito. The institution has played a vital role in shaping Japanese history.\nBhutan and Japan\nJapanese nationals, mostly tourists, have been among the majority of foreigners visiting Bhutan since the kingdom opened up to the world. Dasho Keiji Nishioka, who first came to Bhutan as a Colombo Plan expert in 1964, is fondly rememberd for his contribution to farm mechanisation and the development of agriculture in Bhutan. It has been 31 years since Bhutan and Japan established diplomatic ties and an increasing number of Bhutanese are now going to Japan to study in schools and universities and programmes established by the government of Japan and other organisations.\nFor the Bhutanese people, Japan means a source of the best quality mechanisation including motor vehicles, power tillers, construction equipment, and all forms of modern technology.\nThousands of Japanese tourists say that Bhutan reminds them of what Japan must have once been like. For them it is a trip back into time. Both are Buddhist countries with a King and Emperor as symbols of unity. There is a cultural connection that does not come from the research of scholars or in the historical libraries, but in uncanny similarities that one might hear in a small noodle house over cups of hot shochu that tastes like ara. For example, when Japanese men introduce their wives to a friend or business partner they say, “Please meet my gusai,” meaning my foolish or ugly wife. Bhutanese women say “My husband is a tsagay (idiot).” Although there is a gender difference here the intended social modesty is the same. It is common in both cultures to lay a major feast for a guest and apologise for “such a simple meal”.\nAn insight into the real connection between the values of the two societies is found in history. For example, a four-hour Japanese kabuki play set in a village school is the story of a nobleman who sacrifices his son out of his love and loyalty for his King. It can reduce a sensitive Bhutanese to tears. Bhutanese people can still identify with this depth of human relationship and value, but a sadder similarity is that such nobility is now largely being confined to mythology.\nWith Bhutan venturing into an era of dramatic political change the history of the two countries emanate a strange nostalgia, going back to the Meiji restoration period after 1868. Japan and Bhutan seem like two similar worlds, 150 years apart. With the advantage of hindsight, Bhutan decided that modernisation was not westernisation while Japan then opted to westernise, but some similarities are amazing.\nIn recent years Japanese intellectuals and journalists have taken an interest in Gross National Happiness as a higher goal for human development. This can be an educative exchange because Japanese society, as interpreted by its critics, has wealth and a fair share of unhappiness. Bhutan is financially poor but claims to be happy. It will be an interesting human experiment if the two societies seek a meeting point. Will the two countries become rich and happy or end up being poor and unhappy?\nThe Aura Of The Monarchy\nBut the most fascinating aspect of Bhutan-Japan relations is the institutions of Monarchy that carry a sacred aura in both countries. In recent memory there has been a fairly regular exchange of Royal visits by members of the Royal family of Bhutan and the Imperial Family of Japan.\nHis Majesty King Jigme Singye Wanghcuck, attended the funeral ceremony of Emperor Hirohito and was warmly welcomed again for the Enthronement of Emperor Akihito in 1989. More recently, vivid images of His Majesty The King and Her Majesty The Gyaltsuen dominated the social media as the first State Guests following the Great Eastern Earthquake in 2011. Their Majesties won the hearts of the Japanese people as they met with his Imperial Highness Prince Naruhito and Her Imperial Highness the Empress of Japan, and visited Fukushima in the aftermath of the earthquake. His Majesty the King addressed the Diet.\nIn 1987, His Imperial Highness Prince Naruhito charmed the Bhutanese people with the first ever Imperial visit to Bhutan. Their Imperial Highnesses Prince Akishino and Princess Kiko Akishino also visited Bhutan in 1997 as special guests of Bhutan’s Royal Family. The interaction between the Imperial visitors and members of Bhutan’s Royal Family remain the foundation of the extremely strong bonds between the Royal Families and, therefore, the two countries.\nThe stories of Royalty throughout history have been important chapters in the evolution of nations and societies. They have been recorded and told with drama and sensation that far outweigh fiction. The most extraordinary moments of history unfolded during the reigns of Kings and Emperors, moments that transformed the story of mankind itself.\nIn that sense, Monarchies carry the heavy nostalgia of history. It was the decisions of Monarchs that led to eras of turbulent wars as well as of profound peace and of great cultural and spiritual movements. The charisma of the Institution and moral authority of Kings and Emperors sustained Royal lineages as the only true national institutions.\nEven as Monarchs are recognized as bastions of ancient cultures and traditions, they are also at the forefront of modernization and the future. Philosophy and political theories have enthralled many thinkers with the relevance of Monarchy in modern times. As Kings intersect politics and ceremony, some argue that this Institution is becoming more important with time.\nThe relationship between Monarchy and Democracy, for example, is a topic of profound discourse and political theory. The German philosopher, Hegel, famously noted that Constitutional Monarchy today represents institutional continuity, symbolizes national culture, embodies unity of the state, and most important, enables human freedom.\nYes, there are critics of absolute authority, with good reason. That is why the role of Monarchy has been redefined in the context of democratization. In the transition from absolute Monarchy to democracy – against the tide of political, economic, cultural social transformation – the Monarch provides the stability. Only a sovereign Monarch can represent the state, being above all forms of politics and is best able to steer a country through crises\nIn the context of Constitutional construction, when societies go through massive turbulence, it is the figure of a Monarch and not an elected President who personifies stability and security. The world will not forget the imagery of Gen Suchinda Kraprayoon and Chamlong Srimuang of Thailand crawling to His Majesty King Bhumibol Adulyadej in 1992 and kneeling before the Monarch to be publicly admonished for their political rivalry. It ended a violent era of Thai political history.\nAcademics, philosophers, political analysts all agree on the potency of the Monarch’s soft power. There are many examples of countries where potentially fatal turmoil was calmed by the leadership and guidance of a Monarch. Hegel argued that Monarchy, as a long-standing institution, is the most rational form of government, a safety net, and the best guarantee of public freedom “for the Monarch symbolises the universal will of the sovereign state against the particular will of individuals”. He argued that Monarchy is particularly well equipped to embody national character and Constitutional continuity as it is linked to the country’s history for generations.\nA Discourse That Continues\nBut this is, by no means, the end of political thought and discourse on Monarchy. Recent news that Her Imperial Highness Princess Mako will become engaged to 25-year old Kei Komuro, a former college friend, has re-opened an age old debate that is vibrant, not just in Japan, but on other Monarchies around the world, both East and West. Her Imperial Highness gives up her Imperial status by marrying a “commoner”, meaning someone who is not Royal.\nThe political leadership in Japan, in both the ruling and opposition parties, has agreed to urge the government to consider establishing new branches within the Imperial Family system for female members. Currently, the Imperial House Law only allows male members to establish their own branches and makes it impossible for female members to ascend to the Chrysanthemum Throne. The concern is not just a gender debate but the continuity of the Institutions.\nThe tradition of an unbroken male bloodline has been found to have its own problems and countries are considering the issues to secure stable Royal succession by female succession to the Throne and, thereby, stabilizing the sacred Institution of Monarchy where real sovereignty lies.\nContributed by Dasho Kinley Dorji,\nFormer Secretary of MoIC', 'Royal Reduction: The Postwar Downsizing of Japan’s Imperial FamilyImperial Family\nEnding the War\nOn August 15, 1945, Emperor Shōwa announced Japan’s surrender to Allied forces, and Prime Minister Suzuki Kantarō’s cabinet resigned, but this did not guarantee that World War II was over. With discontent continuing among the military, and particularly the army, the emperor aimed to pick a successor to the premiership who would ensure an end to the war.\nHis choice, Prince Naruhiko, from one of the family’s collateral branches, had been critical of the war, although he was also opposed to the imperial family becoming involved in politics and initially refused the position of prime minister. Apparently, Emperor Shōwa’s worn-out appearance persuaded Naruhiko to take on the job.\nHe wrote in his diary, “I saw it as the highest responsibility as a Japanese citizen and a member of the imperial family, who had always enjoyed favorable treatment, to do my utmost to overcome this unprecedented crisis.”\nEmperor Shōwa decided to deploy imperial family members to convey the message that the war was over. He sent princes serving in the military to spread the word to Japan’s overseas forces.\nAs prime minister, Prince Naruhiko called for penitence among all of Japan’s citizens. When the General Headquarters of the Supreme Commander for the Allied Powers ordered the dismissal of the home minister and top police officials, however, he submitted his resignation in protest against what he saw as interference in domestic affairs. He was in office for just 54 days.\nPurging and Taxing\nIn December of the same year, GHQ ordered the arrest of suspected war criminals, including Prince Morimasa, who had served as a field marshal in the army. Morimasa was held in Sugamo Prison for class A crimes. This was seen by some as a threat against the imperial family, although he was released without charge after four months.\nGHQ launched a purge of public services in January 1946, targeting militarists and supporters of the war, among others. News that Emperor Shōwa’s three brothers and Prince Naruhiko were among 15 imperial family members named in this purge shocked the nation. This was due to their serving in the military, although they were not ultimately purged at this time.\nThe imperial family finances also came under the spotlight. GHQ ordered an inquiry into the total assets of each member, to be subject to the highest tax rate, which had been hiked to 90%. Emperor Shōwa held total assets of ¥3.7 billion (equivalent to more than 100 times the amount in present value) according to calculations, and 90% of this went to the state via property tax. The Fushimi-no-miya branch had ¥7.9 million in assets, which was subject to 85% taxation after the subtraction of debts.\nThe new Constitution was promulgated in November 1946, and Emperor Shōwa assembled the imperial family members in collateral branches (only distantly related to him) to tell them that he would be forced to remove their imperial status. Article 88 of the Constitution was that “All property of the Imperial Household shall belong to the State,” so the imperial family no longer had the financial resources to support 11 collateral branches.\nA Reduced Imperial Family\nThe formal decision to remove 51 imperial family members in 11 collateral branches, including 26 men in line to the throne, came at a meeting in October 1947. Prime Minister Katayama Tetsu noted that there was no need for concern about the succession at that time. After the reduction, there would still be Emperor Shōwa’s two sons, including the future Emperor Akihito, his three younger brothers, and one nephew.\nOf the 51 people who left the family, 40 received one-off payments, excluding those with military records. The total outlay amounted to ¥47.5 million.\nAt a meal with those leaving the family some days later, Emperor Shōwa expressed his wish for ongoing warm ties, as nothing had changed in their relationships.\nSelling to Live\nGreat changes awaited the former imperial family members, though. Fushimi Hiroaki, who was 15 when his imperial status was removed in 1947 and is now 90, recalled in a book he wrote:\n“We had to pay a huge, unprecedented amount of tax, and the one-off sum we received was decided unilaterally, and soon disappeared. So, the former imperial family members had to sell to live. My branch’s land became the Hotel New Otani, and that belonging to the Takeda-no-miya became the Grand Prince Hotel Takanawa. But some branches had lived on Ministry of the Imperial Household land. For example, the Kaya branch really struggled to get by.”\nHe also noted that some of the newly ordinary citizens lost their assets through their inexperience in business, or by being cheated by criminals.\nThe Higashikuni branch also underwent a stormy postwar period. Immediately after Prince Naruhiko, the former prime minister, became Higashikuni Naruhiko, an ordinary citizen, he was purged from public life, along with 10 other former imperial family members who had served in the military. He tried selling food and antiques, and ran a café in Shinjuku’s black market for a time, but all these ventures failed. He even founded his own religion, called Higashikunikyō.\nHis eldest son Morihiro failed the entrance exam for the University of Tokyo at the age of 30 in 1946. This result was a sign of the changing times. Like his father, he was purged after leaving the imperial family due to his military connections. As a commoner, he attended lectures at the university and started work at a company.\nMorihiro had married Princess Shigeko, the eldest daughter of Emperor Shōwa, in 1943. There are stories that she propped up the family finances at times by doing piecework and lining up at local stores’ sales. Her account in a magazine that these hard times allowed her to find human happiness for the first time was also much talked about. However, whether related to these changes in lifestyle or not, she died of cancer at the young age of 35, leaving five children. The deep grief of her parents, the imperial couple, was such that they broke with tradition by attending the funeral of a “common citizen.”\nRenewed Imperial Status?\nEven after leaving the imperial family the former collateral branches maintain some connection. A group called the Kikuei Shinbokukai was established to promote friendship, with gatherings arranged from time to time. For example, the members got together in 2014 at the Akasaka Estate to celebrate the eightieth birthday of Emperor Akihito.\nThe former members also attend events like the emperor’s birthday and New Year celebrations, and they were present at Emperor Shōwa’s funeral. On such occasions, they are ranked behind the imperial family, but ahead of citizen representatives, including the prime minister.\nThere is now discussion of restoring former branches to ensure that the imperial family has enough members to secure the succession and perform official duties. What do the members of these branches think of this idea? At a budget meeting in March 2021, Chief Cabinet Secretary Katō Katsunobu said that he had not asked them and had no plan of doing so. Under these circumstances, it is unclear whether they would welcome renewed imperial status.\nFushimi Hiroaki wrote passionately on the topic in a recent book. “If his imperial majesty ordered me to return to the family, or the state asked me to, I feel I would have to do so.” However, he also said that it would not be possible to suddenly become an imperial prince.\nIt is now 75 years since the collateral branches lost imperial status. While they include some elderly members, the descendants of those subject to the initial 1947 action have been raised as ordinary citizens since their birth. It is asking a lot to expect them to instantly become part of the imperial family. This seems to be what Fushimi was saying.\nNonetheless, the imperial family may face a situation where the pressures of reduced size mean that it must make a decision between restoring the former branches, so as to maintain the limit on succession to male descendants in the male imperial line, or allowing princesses closely related to the emperor to ascend the throne.\n(Originally published in Japanese on April 19, 2022. Banner photo: Prince Naruhiko, at front, with his new cabinet in a commemorative photo from August 17, 1945. © Kyōdō.)']	['<urn:uuid:1b9048fe-e677-40fc-bb86-2e74d9db572d>', '<urn:uuid:4d6a8d87-80f5-485a-b867-69d4f591d6dd>']	open-ended	direct	concise-and-natural	similar-to-document	multi-aspect	novice	2025-05-12T20:46:45.703947	9	116	3217
88	At the Atlanta Botanical Gardens in Gainesville, how do they manage to protect all the existing trees while planting thousands of new plants?	"At the Atlanta Botanical Gardens Gainesville, they protect existing trees through careful planning and specialized techniques. An arborist was consulted to prevent root and canopy damage, trees were carefully limbed up, and all plant holes were dug by hand. When 1½ – 2"" tree roots were encountered, they either moved plant holes or chose smaller plants to avoid root damage. This careful approach aligns with best practices, as damage to the structural root plate can destabilize trees, and the critical root zone typically extends one to two times the reach of branches. Due to competition with existing trees for water, newly installed material was closely monitored with spot-watering to ensure establishment."	"['Location: Gainesville, GA\nProject Scope: Installation of 4,000 tons of subgrade, gravel and topsoil, a 400,000 LF irrigation system, 2,900 shrubs, 19,000 perennials/annuals/groundcover, and 625 trees, stone curb, concrete flatwork, concrete walls, pavers and water features.\nInstalled By: Georgia Landscape Construction Branch\nIndustry Awards Won: National Association of Landscape Professionals (NALP) 2017 Grand Award, Landscape Contractors Association (LCA) 2017 Grand Award\nSite Description: The Atlanta Botanical Gardens, Gainesville is a 168-acre property that was given by philanthropists to preserve the acreage as greenspace. Approximately 12 acres were developed and they include five acres of horticulturally intense and botanically significant display gardens, two miles of walking trails, a 2000-seat amphitheater, a model train garden and a visitor center.\nThe overall design intent of this project was to showcase the woodland regions of the southeast with a richly layered weaving of canopy trees, shrubs, perennials and bulbs. The site was designed to make the space feel larger and more mature than it was. There were many existing trees that had to be planned around and incorporated into the design. Meandering trails were created to showcase impressive existing trees and avoid damaging root systems. Special attention was paid to creating unique, seasonal and more focused experiences throughout the garden—with an eye to crafting areas that could be visited multiple times throughout the year and to building rentable event spaces. Myriad specimen plants combined with countless accent clusters ultimately helped to create the feel of a well-established “secret garden” experience.\nChallenges & Solutions: Accessibility was a challenge—all soil, shrubs, trees, annuals, perennials and groundcover going to designated tree save areas at the property’s rear could only be transported on a few paved areas using bobcats and ball carts to a certain point on the property. Materials were then transferred to wheelbarrows and Georgia buggies or hand carried to specified areas. Additionally, a 60’ remote controlled tele belt was used to transfer subgrade stone to areas efficiently, taking nearly three days to complete.\nAdditional challenges included Georgia clay soils, which turns into mud when it rains, requiring significant focus on soil amendments and erosion control. All planting areas were excavated down 18” and back filled with a mixture of amended top soil. During the initial grading, soil was harvested and screened—eliminating heavy roots, rocks and leaves—to maintain the same nutrient biome. These were then combined with an exact mixture of imported top soil to enable better water absorption, thus reducing soil compaction and erosion issues.\nPhotos & Descriptions:\nApproximately 25% of the time onsite was spent with approximately 20 people hand- carrying and installing six acres (like pictured here) of plant material in designated tree save areas.\nPhoto Credit: Larry Griffeth\nThe visitor’s center is deferential to the landscape (made with wood siding and metal roof) and was placed strategically to act as a gateway to the magnificent woodland.\nStone aggregate, drainage fabric, and drainage pipe tie into the site’s irrigation system, which includes a 30,000 gallon cistern that collects and distributes rainwater back into the site’s many planting beds and lawn areas, like the amphitheater seen here. Seven months before the site opened to the public, a soft opening for investors and donors necessitated redirecting crews’ attention to the event lawn and visitor center, which took them away from their original schedule.\nPhoto Credit: Larry Griffeth\nNearly 30% of the site’s plant material had been grown in the garden’s nursery prior to the awarding of this project. The gardens boast a diverse plant palate with more than 1300 different varieties.\nThere are 300+ varieties of Hydrangea, 150 Magnolia varieties, 88 unusual varieties of plants; 50 Witch Hazel varieties; and 32 Maple species.\nThe site’s design and installation was an intricate partnership between garden staff, the designer and the landscape contractor who worked closely together to generate plant lists, develop color sequences and assure four seasons of floricultural display. The designer likened the process to “an improvised painting that lead to an amazing garden.”\nThere was a record 63 days of rain prior to the start of the landscape portion of this project, delaying the GC’s installation of roads, buildings and artscape. This, in turn, pushed back the start of our work and reducing the time we had to complete it.\nPhoto Credit: Larry Griffeth\nKeeping with the sustainable principles of the site, artist Patrick Dougherty has created this monumental sculpture for an exhibit known as Woven Whimsy. This art piece was created on site, and is made out of sticks which are organic and will break down over time.\nThe landscape is comprised of over 20,000 plants and 600 trees. There are nearly 100 different varieties of shrubs, 150 varieties of perennials and grasses, and 39 varieties of trees all helping to enhance this greenspace. Material was ordered/delivered in waves using Just-In-Time delivery, and we set up some temporary irrigation and had a water truck on site for 8 hours a day on every other day of the project.\nOur project manager on the site described installing a botanical garden as “creating the feel that you’re in a gardener’s back yard but on a commercial scale.” Throughout the site, visitors are encouraged to sit and enjoy the landscape, which looks vastly different depending on the season.\nThe client was heavily involved with the day-to-day process and required daily meetings and walks throughout the project every day.\nTo prevent root and canopy damage of existing trees, an arborist was brought in to consult. Existing trees were carefully limbed up and excavation for all plant material was done by hand. Where 1 ½ – 2” tree roots were encountered, plant holes were moved or smaller plant material chosen to avoid root damage. Due to competition with existing trees for water, newly installed material was closely monitored and spot-watering occurred to ensure root establishment.\nThe project’s design, which includes nearly 1300 varieties of plants, and the attention to detail with its installation, will make this a project enjoyed by visitors for many years to come.', 'Biggest Myth About Tree Roots\nMany people believe that tree roots grow mostly downward and mirror the top of the tree. Not true.\nMost Important Things to Know About Trees\nFact: In our area, the Piedmont region of Georgia, tree roots spread out just under the surface of the soil surrounding the tree.\nFact: Roots survive and grow only where there is adequate water and oxygen. Our soils have high clay content, and deeper soils do not contain sufficient water and oxygen for tree root growth.\nFact: The vast majority of tree roots are in the top 18 inches of soil (some sources say as much as 98% of tree roots).\nFact: Providing adequate room for a tree\'s roots to grow without being disturbed is one of the most important aspects of keeping a tree healthy.\nProtect the Root Zone and Structural Root Plate\nStructural Root Plate: The woody roots close to the trunk, known as the structural root plate, help anchor the tree and provide physical stability. Destroying or disturbing these roots may leave the tree unable to hold itself up. This type of damage can result in disaster. The City of Atlanta Tree Ordinance prohibits damage to the structural root plate of trees.\nCritical Root Zone: The majority of roots reach out just beneath the soil surrounding the tree. The roots usually extend one to two times the reach of the branches. For some types of trees, this critical root zone can be two or three times as wide as the extent of the branches. The City of Atlanta Tree Ordinance defines the critical root zone as a circle having a radius of one foot for each one inch of diameter at breast height of the tree for a free-standing tree with no apparent root restrictions. For trees to be considered ""saved"", no more than 20% of the critical root zone may be damaged.\nWhen it is done correctly, mulching a tree can often be the single most beneficial aspect of tree care. Mulch contributes to soil nutrients levels, helps retain soil moisture during drought, reduces compaction, and can even help control some common root diseases. It also reduces the tree\'s need to compete with other plants and turf for water and nutrients.\nIdeally, mulch should be applied in a circle from near the tree trunk to the edge of the tree canopy, and covers about the same area as the critical root zone. On a practical note, however, sometimes it is not possible to mulch the entire root zone, so mulch should extend as far from the tree as possible within the scope of the site and landscape area.\nMulch should be spread only two to four inches deep, and should not touch the tree trunk or buttress roots, so leave some breathing room at the base of the tree. Never start mulching next to the trunk of the tree. Instead, the mulch should start a foot away from the base of the trunk to prevent damage to the base of the tree. Piling a heap of mulch against the tree trunk is sometimes called \'volcano mulching\' and can cause insect and disease problems, often killing the tree within a few years. Deeper mulch is NOT better and can harm the tree. Covering a wider area is better.\nWood chips, pine straw, shredded cypress, pine bark, and bagged mulch all work quite well. Artificial mulches and stone ground covers (like lava rock, pebbles or crushed granite) may cause changes in soil pH and do not have the same beneficial effects as natural materials that biodegrade over time.\nRoot damage can sometimes result in an immediate effect on a tree. You might see leaves dry or drop during the wrong season or limbs die back. More often, however, the symptoms of root damage appear years later. It can take as many as three to seven years for symptoms of root damage to become evident in the crown of the tree. This damage is often irreversible and results in the death of the tree.\nTree ""murder"" by root suffocation can be the ""perfect crime"" since the injured tree can take several years to die after the offending act. New homeowners often report having moved into a newly constructed home with ""beautiful, perfectly healthy mature trees"" that mysteriously enter a spiral of decline several years later. This is because trees typically store as much as 66% of the energy they make. After tree roots are damaged and the tree is unable to make new energy, it pulls from its ""bank"" of energy for up to seven years before ultimately dying.\nSoil Compaction Irreversible\nSoil compaction should be considered permanent. University of Georgia studies show that under normal forest conditions, soils do not readily ""come back"" from compaction. Recovery time is at least two human generations. Damage to the trees occurs because the roots cannot function properly in compacted soil. Arborists sometimes recommend mechanical means of aerating the compacted soil around trees, but this can be difficult to accomplish without damaging roots.\nReducing the Negative Effect of Soil Compaction\nSometimes root damage cannot be avoided, but it can be minimized by restricting access to the areas close to trees during construction. If the area near the tree must be disturbed by, for example, heavy foot traffic or delivery of construction materials, the roots can be protected by softening and distributing the impact. This can be accomplished by placing heavy mulch, plywood, or metal driving pads under the trees. Physical properties of soil can be improved with the addition of organic matter and aeration to increase pore space. When tree roots must be cut, they should be cut cleanly with a sharp pruning shear or saw. When root damage is severe or occurs to woody roots, the whole tree should be evaluated to determine whether it should be removed. If a damaged tree is to remain, it should be monitored and maintained to promote tree health. Maintenance may include mulching, aeration, and irrigation.\nREFERENCES City of Covington Tree Preservation Ordinance and information, prepared by Connie Head 01/04/2001. International Society of Arboriculture Arborist Certification Study Guide 1010. Soil Compaction Impacts On Tree Roots, Dr. Kim Coder, University of Georgia, July 2000; www.extension.iastate.edu/forestry/publications/for00-008.pdf\n|PLANTING A TREE||PRUNING A TREE||HOW TO HIRE A TREE COMPANY|']"	['<urn:uuid:619304c3-2111-4224-83b2-1fb21fc07b19>', '<urn:uuid:e00d3f65-7162-48d9-a025-146755561543>']	factoid	direct	verbose-and-natural	distant-from-document	three-doc	novice	2025-05-12T20:46:45.703947	23	111	2047
89	What is the recommended sequence for letterpress printing when creating double-sided business cards with a solid color on one side?	When printing a two sided card with a solid color, the solid side should be printed first, followed by the text side. This sequence produces a better sculptural impression on the text-only side, because printing an overall impression on a solid area tends to iron the paper flat and reduces the impression of artwork on the reverse.	['Adam Hudson Photo sent us these cards for letterpress printing. The unique narrow format really make them different in your hand. And the three color options of orange, green and gray on one side is a nice way to add some simple variety to an identity piece. We keep the plate set up on press and just add a couple wash ups to the printing process. A thick 220lb cotton stock takes a beefy impression. When a two sided card is pressed with a solid color, we almost always print the solid side first, then the text. This makes for a better sculptural impression on a text only side. Putting an overall impression on a solid area has the effect of ironing the paper flat and will diminish any impression of artwork on the reverse. Another ink effect we like on this card is the white ink on white paper. We are using a tinted white ink to create a nice subtle detail with just the right amount of contrast to keep it readable. Some times an inkless (blind) impression doesn’t have quite enough visibility to read clearly. We put a little bit of silver in the white ink to give it just the right amount of eye love.\nThe thickness of these cards allows them to also serve as a secret weapon. When your primary audience for business card hand off is art directors and designers, your card had better be memorable. A 55 point coaster board makes them fantastically textural and unstoppable. Clockwork Active Media Systems designed these for photographer rep Jeff Cerise – aka Secret Agent Man. They are hot off the press.\nOne thing to note about Coaster Board – it does not take a sculptured impression like other softer sheets. The surface will break. One of the detail photos below shows the surface cracking that will occur with to much pressure. This is likely especially in interior areas of artwork and must be watched for and adjusted to avoid this breakage. However, the finish of the stock is nice and pulpy – a definate stand-out among more typical paper mill stocks. We also like the “salty” look to the dark ink color on this stock. Gives it an aged / weathered appearance.\nWhen Becky Rosenthal was launching her wedding photography business she asked us to both design and letterpress print her identity system. So many wedding invites are letterpressed, why not appeal to brides with the same tactility?\nHere’s a couple design considerations for letterpress:\nOne of the things we like about letterpress is what happens to the white areas within a design. You can see in the logo below how the interior white spaces have a raised effect because the area around them is being pressed down. Creating those white spaces within a solid area is one of the things we try to include in our design work to show some dimension.\nAnother thing that works well in letterpress design is the inclusion of pattern. Rather than using large solid areas of color (which definitely are not well suited for letterpress) a flood of pattern adds both color and a texture that can be felt.']	['<urn:uuid:148835eb-3691-4003-98d6-7b35a9c4c04c>']	factoid	direct	verbose-and-natural	distant-from-document	single-doc	expert	2025-05-12T20:46:45.703947	20	57	531
90	I'm getting a dog - can Old English Sheepdogs live outside in cold weather?	Yes, Old English Sheepdogs are well-suited for cold weather as they have a weather-resistant coat with a waterproof undercoat pile. Their coat is profuse and of harsh texture, providing good protection from the elements.	['FCI-Standard N° 16\nOLD ENGLISH SHEEPDOG\nORIGIN: Great Britain.\nDATE OF PUBLICATION OF THE OFFICIAL VALID STANDARD: 13.10.2010.\nFCI-CLASSIFICATION:Group 1 Sheepdogs and Cattle Dogs (except Swiss Cattle Dogs).\nSection 1 Sheepdogs. Without working trial.\nBRIEF HISTORICAL SUMMARY: Though the Old English Sheepdogs registry is listed as Britain its actual ancestry is thought to be from the European Shepherd Dogs of the Owtcharka and Bergamasco types bred to sheepdogs of Britain. It is now regarded as a native British breed, often called the Bob-Tail. Strong, compact and profusely coated; his coat is a distinctive feature and is weather-resistant. Basically a country dog, intelligent and friendly; he has a particularly resonant bark sufficient to frighten off any intruder.\nGENERAL APPEARANCE: Strong, square-looking dog of great symmetry and overall soundness. Absolutely free from legginess, profusely coated all over. A thick-set muscular, able-bodied dog with a most intelligent expression. The natural outline should not be artificially changed by scissoring or clipping. Of great stamina, exhibiting a gently rising topline, and a pear-shaped body when viewed from above. The gait has a typical roll when ambling or walking. Bark has a distinctive toned quality.\nIMPORTANT PROPORTIONS: Dog standing lower at withers than loin. Head in proportion to the size of the body. Muzzle measuring approximately half of the total head length.\nBEHAVIOUR AND TEMPERAMENT: A biddable dog of even disposition. Bold, faithful and trustworthy, with no suggestion of nervousness or unprovoked aggression.\nSkull: Capacious, rather square. Well arched above eyes.\nStop: Well defined.\nNose: Large and black. Nostrils wide.\nMuzzle: Strong, square and truncated.\nJaws / Teeth: Teeth strong, large and evenly placed. Scissor bite - jaws strong with a perfect, regular and complete scissor bite, i.e. upper teeth closely overlapping the lower teeth and set square to the jaws. Pincer bite tolerated but undesirable.\nEyes: Set well apart. Dark or wall eyes. Two blue eyes acceptable. Light eyes undesirable. Pigmentation on the eye rims is preferred.\nEars: Small and carried flat to side of head.\nNECK: Fairly long, strong, arched gracefully.\nBODY: Rather short and compact.\nLoin: Very sturdy, broad and gently arched.\nChest: Deep, capacious brisket. Ribs well-sprung.\nTAIL: Previously customarily docked or natural stumpytail.\nDocked: Customarily completely docked.\nUndocked: Natural carriage. Well feathered with abundant hard-textured coat.\nShoulder: Well laid back, being narrower at the point of withers than at the point of shoulder. Loaded shoulders undesirable.\nElbow: Fitting close to brisket.\nForearm: Forelegs perfectly straight, with plenty of bone, holding body well from ground.\nForefeet: Small, round and tight. Toes well arched. Pads thick and hard. Turning neither in nor out.\nGeneral appearance: Quarters well covered, round and muscular.\nStifle (Knee): Well turned, but not exaggerated.\nLower thigh: Long and well developed.\nHock joint: Set low.\nMetatarsus (Rear pastern): Parallel, viewed from behind.\nHind feet: Small, round and tight. Toes well arched. Pads thick and hard. Turning neither in nor out.\nGAIT / MOVEMENT: When walking, exhibits a bear-like roll from the rear. When trotting, shows effortless extension and strong driving rear action, with legs moving straight along line of travel. Very elastic at the gallop. At slow speeds, some dogs may tend to pace. When moving, the head carriage may adopt a naturally lower position.\nHair: Profuse, of good harsh texture, not straight, but shaggy and free from curl.\nUndercoat of waterproof pile. Head and skull well covered with hair, ears moderately coated, neck well coated, forelegs well coated all round, hindquarters more heavily coated than rest of body. Quality and texture to be considered above length and profusion.\nColour: Any shade of grey, grizzle or blue. Body and hindquarters of solid colour with or without white socks. White patches in the solid area to be discouraged. Head, neck, forequarters and under belly to be white with or without markings. Any shade of brown undesirable.\nSIZE AND WEIGHT:\nHeight at the withers: Males 61 cms and upwards.\nFemales 56 cms and upwards.\nType and symmetry of greatest importance, and on no account to be sacrified to size alone.\nFAULTS: Any departure from the foregoing points should be considered a fault and the seriousness with which the fault should be regarded should be in exact proportion to its degree and its effect upon the health and welfare of the dog and on its ability to perform its traditional work.\n·Aggressive or overly shy.\n·Any dog clearly showing physical or behavioural abnormalities shall be disqualified.\nN.B.: Male animals should have two apparently normal testicles fully descended into the scrotum.\nThe latest amendments are in bold characters']	['<urn:uuid:050bf422-6c3e-4999-839b-b73bad555116>']	open-ended	with-premise	concise-and-natural	distant-from-document	single-doc	novice	2025-05-12T20:46:45.703947	14	34	754
91	Why are cheaper metals being tested for emissions control?	Researchers are exploring alternatives to rhodium because of its high cost - it trades at around $2,400 per troy ounce and has previously exceeded $10,000 in 2008. Cobalt has emerged as a promising alternative since it's 900 times less expensive than rhodium and is capable of breaking the N-O bond. It forms a solid solution with platinum at low concentrations and moves to the nanoparticle surface under reaction conditions.	['Materials scientists develop clean nanoparticle-based catalysts that convert nitrogen oxides to nitrogen and water vapor. A research group at Lehigh University led by Charles Lyman, professor of materials science and engineering, has developed catalysts that convert the harmful nitrogen oxides emitted from coal- and gas-fired power plants to nitrogen and water vapor.\nUnlike other methods of converting nitrogen oxides (NOx), the low-cost nanoparticle-based catalysts developed by Lyman’s group do not use ammonia.\nNOx is a generic term for nitrogen oxide (NO) and nitrogen dioxide (NO2). The oxides, which are responsible for smog and acid rain, are produced when nitrogen and oxygen react in the air during combustion, especially at high temperatures.\nThe use of catalysts for pollution control in power plants is not new. A technique called selective catalytic reduction (SCR) can convert 95 percent of NOx to nitrogen and water—but with a catch.\n“Conventional SCR requires another toxic gas, ammonia, to carry out the reduction,” says Rick Herman, a senior research scientist who works with Lyman. “In addition, these catalysts only perform well at high temperatures.”\nFinding the best preparation technique\nLyman and Herman sought first to improve the performance of a platinum-rhodium nanoparticle-based catalyst that uses hydrogen, rather than ammonia, as a reducing agent. This catalyst works at much lower temperatures.\n“Choosing the correct catalyst preparation procedure was critical to ensuring that all these criteria were met,” says Paul Dimick, a Ph.D. candidate in chemical engineering working with Lyman.\nTo obtain the desired microstructure, researchers sequentially impregnated an alumina support with aqueous solutions of platinum and rhodium chlorides. Then they applied a series of thermal treatments to convert the metal chlorides into active bimetallic nanoparticles.\n“This procedure was chosen so that the desired microstructure would be achieved under reaction conditions,” says Dimick.\nWhen the researchers examined the prepared catalysts with Lehigh’s high-resolution aberration-corrected scanning transmission electron microscope, they found nanoparticles with an average diameter of 2 nanometers (nm) well-dispersed on the alumina support. One nm is one billionth of a meter.\nIn-situ Fourier transform infrared (FTIR) spectroscopy revealed that the N-O chemical bond in a catalyst containing 5-percent rhodium was broken as soon as the molecule made contact with the nanoparticle. In a catalyst of 10-percent rhodium, however, the NO molecules tended to adhere to clusters of rhodium atoms present on the surface, which could potentially inhibit the desired reaction. Catalyst performance data also showed that the catalyst containing only 5-percent rhodium exhibited a much higher activity.\nCobalt’s low-cost advantage\nRhodium, however, is rare and expensive. It now trades at around $2,400 per troy ounce after exceeding $10,000 in 2008. To reduce NOx emissions more cheaply, the researchers turned to cobalt, which is 900 times less expensive than rhodium.\n“Cobalt is also capable of breaking the N-O bond,” says Lyman. “It forms a solid solution with platinum at low concentrations and finds its way to the surface of a nanoparticle under reaction conditions.”\nThe preparation of cobalt-platinum catalysts is almost identical to that of rhodium-platinum. The researchers merely substituted metal nitrates for chlorides.\nWhile all catalysts containing less than 5-percent cobalt were found to be capable of reducing NOx to nitrogen and water vapor, the 2-percent cobalt catalyst performed best. “In-situ FTIR spectroscopy revealed that the 2-percent catalyst was the only one capable of immediately breaking the N-O bond,” says Dimick.\nThe same methodology has now been applied to a nickel-platinum catalyst system with equally promising results.']	['<urn:uuid:0a9fe3e7-71ea-4d8c-934a-7ab0e7fd1c06>']	open-ended	with-premise	concise-and-natural	distant-from-document	single-doc	expert	2025-05-12T20:46:45.703947	9	69	569
92	How long does composting take and what affects the speed?	Regular composting takes 3-6 months, while hot composting is faster at 3-6 weeks. The speed depends on temperature, moisture content, particle size of materials, and how often you turn the pile. Hot composting requires daily turning in the first week and 10-12 times in the first two weeks for optimal results.	"['What is Composting and Why is Composting Important?\nComposting is a natural process by which organic materials decompose. Composting is nature\'s way of recycling organic material such as leaves, grass clippings, twigs, fruits, and vegetables into a dark, crumbly, earthy-smelling soil conditioner. By concentrating the activity in one place and balancing food, air, and water, compost happens faster.\nRemember, composting is just another form of recycling. When you compost, you are tapping into the natural nutrient cycle. In nature, organic waste from plants and animals is recycled by decomposition. Composting is controlling that decomposition to speed it up and produce a stable and odorless material for plants to use.\nOrganic waste material, like yard clippings and food scraps, can be given ""new life"" through composting. Finished compost is a wonderful soil amendment that improves texture and adds important nutrients into the soil in your garden, creating healthy, thriving plants. From tomatoes to tulips, compost keeps your garden growing strong! If residents compost their yard and kitchen waste, we can go a long way toward satisfying California\'s challenging goal of a 75% total reduction in waste.\nBenefits of Composting\nOrganic wastes, such as food waste and yard waste, make up 25 to 50% of what people throw away. While you may not be able to compost all of the organic waste you generate, composting can significantly cut down on your overall trash.\nWhen we throw away yard and food waste, it decomposes in a landfill and releases methane gas, a potent greenhouse gas. While most landfills have technology to capture much of this methane, eliminating the gas at its source is even better.\nOther benefits of composting include:\n- Saves you money by replacing store-bought soil conditioners\n- Helps garden and house plants by improving the fertility and health of your soil\n- Saves water by helping the soil hold moisture and reducing water runoff\n- Benefits the environment by recycling valuable organic resources, reducing transport and processing of materials, and reducing waste to our landfills\nWhat can I compost?\nYard waste, such as fallen leaves, grass clippings, weeds, garden plants remnants, and kitchen scraps make excellent compost. However, care must be taken when composting kitchen scraps. Meats, bones, and fatty foods (such as cheese, salad dressing, and leftover cooking oil) do not belong in the bin. Place those items in the garbage.\nWhat Goes In the Compost Pile?\nBrowns are dried or dead organic materials that serve as sources of carbon. Browns are useful for retaining moisture, creating small air pockets, and supporting a more diverse community of decomposers in the pile. Browns include woody materials, dead or dried yard debris, chopped branches and twigs, bark, straw, sawdust, coffee filters, tea bags, shredded paper and paper products.\nGreens are fresh organic materials that serve as sources of nitrogen. Greens are the primary energy source of the active microorganisms, and are useful as a supplementary source of moisture in the pile. Greens include fresh yard trimmings, fresh grass clippings, fresh or moldy fruit and vegetable scraps, coffee grinds, tealeaves, breads, certain types of manure.\nWater helps ensure efficient processing of organics. Ideally, the pile is kept as moist as a wrung out sponge. Too little moisture will inhibit decomposition, but too much water can produce smelly, anaerobic conditions.\nAir is essential for a sweet, earthy-smelling compost pile. Turning your compost pile regularly will help to inhibit the growth of odor-causing anaerobic bacteria, and will result in faster decomposition.\nComposting can be practiced in any home, apartment, or townhouse. Identify a place in your yard that is out of the way but accessible to deposit yard and food waste. Make sure you can reach this location with a garden hose. There are many ways to construct a compost bin starting from the very simple - a pile on the ground to the fancy store bought composting bin with aeration holes and turning mechanism.\nMost people choose to construct a simple box form out of old lumber or pallets with slats spaced apart for adequate ventilation. This is cheap and keeps with the reuse theme.\nNow start creating a pile of vegetation. Try for a 2:1 ratio of brown materials (dried leaves, woody material, etc.) and green materials (food scraps, grass, etc.). Add enough water so that the pile is evenly moistened, like a damp sponge. Turn the pile weekly or when you notice the top layer start to dry out. A good pitchfork is recommended, as it is easier to use than a shovel.\nDepending on variables such a temperature, moisture content, and how often you turn the pile, you could get a rich compost material as soon as a month or as long as a year.\nAvoid the Usual Pitfalls\nThere are times when you may experience an odor emanating from your compost or many flies surrounding it. This is usually due to not immediately covering food waste deposited into the pile. When food waste is added, be sure to turn the pile immediately, or add some fresh leaves or grass on top of it. This will keep the odors and flies away.\nRemember, smaller bits of greenwaste decompose faster than large pieces. If you continually have large quantities of greenwaste to compost, you might consider the purchase of a chipper/shredder to grind your greenwaste into finer particles.\nThere are varieties of problems that you may experience while composting. Follow the guide below to solve your problem!\n- Compost pile smells very bad / offensive\n- Pile isn\'t heating up / getting warm\n- Material isn’t breaking down quickly\n- Pile is attracting ants\n- Excessive flies and/or rodent foraging\n- Anaerobic conditions (not enough air)\n- Pile too small, too dry and/or not enough greens\n- Not enough moisture and/or large material size\n- Pile too dry, food scraps not buried\n- Food scraps exposed, wrong ingredients\n- Turn the pile, add more browns to the pile\n- Add more greens to the pile, add water while turning\n- Add water, chop materials into smaller pieces\n- Add water, bury all food scraps in core of pile\n- Bury food scraps in core, do not add meat, dairy or oils\nAre Worms Ok?\nAfter a while, you may notice that worms are starting to populate your compost pile. Good job! That is an indicator that you have a good compost mix going on and the worms are happy to be there. Redworms are great at recycling decomposing organic matter into rich humus. They generate nutrient rich worm castings, which improve soil fertility and structure. You can jump-start your worm populations by purchasing redworms at most local nurseries, bait shops or over the internet.\nWhat is Grasscycling?\nGrasscycling is the natural recycling of grass by leaving clippings on the lawn when mowing. They decompose quickly and release valuable nutrients back into the soil.\nAdditional Resources and Information\nThe following websites, videos, and books offer additional information that will be helpful in your composting adventures. Many other composting videos and websites are available so do your research so that you can become the ultimate backyard composter.\nCalifornia Department of Resources Recycling and Recovery (CalRecycle)\nCalRecycle brings together the state’s recycling and waste management programs and continues a tradition of environmental stewardship. Please be sure to check out CalRecycle’s tips on household composting, Home Gardening, and composting bins. CalRecycle also has a great guidance document on building your own composting bin.\nEnvironmental Protection Agency (EPA)\nThe EPA’s mission is to protect human health and the environment. They offer a range of helpful tips including composting at home. Be sure to visit the EPA’s Composting at Home website as it provides guidance, information, and troubleshooting on composting.\nWhether you are composting in a composting bin or simply starting a compost pile, many tutorial videos online can assist you. We have provided a few for you here.\nHow to Make Compost: Presented by Lowe\'s (2:24 min)\nComposting 101: Presented by Grow Organic (3:54 min)\nThe Master Composter – http://www.mastercomposter.com/\nGuide to Composting at Home – http://www.improvenet.com/a/guide-to-composting-at-home\nOrganic Gardening Guru – http://www.organicgardeningguru.com/\nCompost Mania – http://compostmania.com/blog/\nSearch in the App Store or Google Play if you want to use an app for composting!\n- Home Composting for Organic Composters\n- Compost Works\nThe Rodale Book of Composting: Easy Methods for Every Gardener – The essential guide to composting for all gardeners and environmentally conscious people. By Grace Gershuny, Deborah L. Martin\nThe Complete Compost Gardening Guide – The authors’ bountiful, compost-rich gardens require less digging, weeding, mulching, and even less planting. By Deborah L. Martin, Barbara Pleasant\nComposting: A Practical Step by Step Guide – The guide explains the benefits of composting – to the environment and to your own garden – and describes exactly how composting works. By Victoria Heywood\nHome Composting Made Easy – Fully illustrated with step-by-step, no-nonsense instructions and state-of-the-art advice by gardening and compost experts. By C. Forrest McDowell, PhD, Tricia Clark-McDowell\nComposting: An Easy Household Guide – A full-color guide for both beginners and experienced composters, with an A–Z reference section. By Nicky Scott', ""Hot Composting is a form of accelerated composting that yields finished compost in 3-6 weeks (longer in winter), instead of the 3-6 months needed for static pile composting.\n© Steve Masley…Click IMAGE to Enlarge\nThis compost is dark, rich, and free of viable weed seeds and plant diseases.\nIn a hot compost pile, the size of the pile—and the particle size and mix of materials that go into it—are optimized to generate high temperatures.\nThis steamy heat kills weed seeds and plant diseases and dramatically accelerates the composting process, but it has to be managed carefully.\nTemperatures in a large, poorly-managed hot compost pile can reach 180° (82° C). On hot days, the dry materials on top of these piles can spontaneously combust!\nApart from the fire hazard, at temperatures above 160° (71° C), all of the oxygen is quickly used up, and the compost pile “goes anaerobic”. That’s when you get problems with odors, nutrient losses, and toxins.\nSmell ammonia wafting from your compost pile? That’s your nitrogen, the element most important for plant growth, venting into the atmosphere.\nSmell a hint of rotten eggs? That’s your sulfur, needed for critical plant enzymes and plant immunity, dissipating as hydrogen sulfide, rotten egg gas.\nSee a ghostly, pale image floating over your compost pile? That’s your phosphorous, needed for root and fruit formation, floating away as phosphene gas—the “swamp gas” that floats over boggy areas and landfills, and sometimes results in UFO reports.\n(Thanks to Dr. Elaine Ingham, Oregon State University, for this paraphrased description of what goes wrong when compost piles go anaerobic.)\nAll of these nutrient losses are due to anaerobic bacteria taking over your compost pile, starting with the hot core.\nThis can happen occasionally in a static compost pile, but it will happen in a hot compost pile, if you don’t turn it when the temperature exceeds 155° (68° C).\nHot Composting has Several Advantages Over Static Pile Composting:\nHot composting avoids this problem. The pile heats up to 150° (65° C) within a day or two, and kitchen scraps dissolve quickly in this heat.\nManaging a hot compost pile takes a lot of attention and effort, especially during the first 2 weeks. You have to check the temperature of the pile with a compost thermometer daily, and turn and mix the pile every time the temperature climbs over 155° (68° C).\nIn practice, this means you’ll probably have to turn the pile every day for at least the first week, and 10-12 times for the first 2 weeks after building a hot compost pile.\nPlan on putting at least 12-18 hours into a hot compost pile—most of it the first week.\nIf you can’t commit to this kind of attention and work, DON’T try hot composting. Use a compost tumbler or build a static compost pile instead.\nWhen there’s enough carbon to fuel the lives of billions of microbes, enough nitrogen and water to build these billions, and enough oxygen to give aerobic bacteria the advantage, magical things happen in compost piles.\nHeat builds in the pile, and thermophilic (heat-loving) bacteria multiply, breaking down all but the most resistant materials around the hot core of the pile, driving off moisture and carbon dioxide in the process.\nEven in January you’ll see narrow columns of steam rising from hot compost piles.\nAs temperature increases in the pile, populations of bacteria rise, dominate, and fall, either going dormant, or becoming fodder for the next wave of bacteria that thrive in hotter temperatures still.\nNutrients in the pile are concentrated and bound into organic molecules, bacterial glues, and the bodies of bacteria, all of which feed the soil food web when the compost is added to the soil.\nYou’ll need some basic composting equipment for hot composting:\nA Compost Thermometer, (a thermometer with a 20”–50cm– probe) for monitoring compost heat. This is a critical piece of equipment for thermal composting.\nA short, sturdy Spading Fork, with a D-handle, preferably one with square—as opposed to rectangular—tines, for turning the pile. I often bounce forkfuls of compost on the tines to break up clumps as I’m turning the pile. Square tines are stronger and have a thinner profile that allows the material to fall through more easily.\nA way to chop or shred compost ingredients. If you have a large yard or garden, a chipper-shredder simplifies processing yard waste. For smaller gardens, you can use a pair of hedge clippers or a machete and a board to chop compost materials.\nA water wand with a shower setting, for even moistening of the layers of the pile.\nA compost bin or wire cylinder to contain the pile. For accelerated composting, you’ll need a bin or cylinder that’s at least 3’ x 3’ x 3’ (1m x 1m x 1m)—larger, if you’re planning on winter composting.\nA compost bin isn’t absolutely necessary for hot composting, but it “stacks” the compost materials, so it heats up faster.\nA compost bin or compost tumbler can cost $40-250, but you can achieve the same result with a hoop of welded wire fencing from the hardware store for $40-50.\nA 14-foot (3.5 m) length will create a 4-foot (1 m +) diameter hoop, with room to overlap the ends. Use 1x1” (2.5 x 2.5 cm) or 1x2” (2.5 x 5 cm) welded wire. The advantage of hoop bins is you can make them narrower when you don’t have a lot of materials, and wider when you have plenty.\nBefore building a hot compost pile, you’ll need to collect—and process—materials.\nThermal composting requires a certain mass to build compost heat, especially if you’re composting in winter.\n(For information on what to compost—and what not to compost—see Compost Ingredients).\nStraw bales, bags of tree leaves, lawn trimmings, wood chips, and bags of manure should be collected at your composting site. If you use stalky materials, chop them into 3-4” (7-10 cm) pieces.\n© Steve Masley…Click IMAGE to Enlarge\nTo “pre-compost”, open the compost hoop out to form a wide cylinder, and bury kitchen scraps between thick layers of brown materials.\nThe pile develops some compost heat, but not enough to require turning. The goal at this point is to start the composting process, and let it “simmer” until you’re ready to turn up the heat.\nWhen the cylinder is almost full, I gather whatever additional compost ingredients I plan to use—tree leaves, straw, stalky materials, wood chips, and manure—then proceed with building a hot compost pile.\nArray the materials in small, separate piles around where you plan to build the pile—much as you array pre-chopped vegetables for throwing into a wok when making a stir-fry.\nThe warm material from the middle of the static pile acts as a compost activator for the rest of the pile.\nInstead of just layering compost materials on whole, or slightly chopped, as you do in static pile composting, hot composting requires that the compost ingredients be chopped or shredded, to increase the surface area in contact with microbes in the pile.\nSmaller pieces also make turning the pile much easier. If you layer whole plants in with manure in a big compost pile, it will heat up to 160° (71° C) in a day or two, and need to be turned.\nMost of the leaves will have dissolved in the middle of the pile, but the stems and stalks will remain. They’ll be softened, but still tough and fibrous, and they’ll be threaded through the pile like strands of rope.\nTurning such a pile is a nightmare. Every forkful is a tug-of-war. You feel like you’ve been in a wrestling match after turning the compost pile.\nThe finer the compost materials are shredded, the faster the pile heats up.\nIf you have access to a chipper-shredder, you can shred green and brown materials at the same time, blending materials as you shred. This creates a fine-textured mix that heats up fast, with high compost heat going almost all the way out to the edges of the bin.\nA chipper-shredder also lets you shred tree leaves, so they’re less likely to mat down in the pile.\nIf you have a large garden and yard, they’re a huge time- and- effort-saver that will help you make better compost, but chipper-shredders, especially good chipper-shredders, don’t come cheap.\nNeighbors or other gardeners may be willing to share the cost, so you can pool resources and share the shredder to save money.\nThey’re a big investment for backyard organic gardeners, one I’ve never been able to justify with my small garden. My solution? A tarp, a board, and a machete.\nSpread a tarp on a flat piece of ground near your compost area, and put a couple of sawhorses on top of the tarp. Lay a 2” x 12” (38x286mm) board across the sawhorses. (I built a squared U-shaped frame with 2 boards bolted to 2 x 4’s (38x89mm's), so it’s one piece and easy to move, but a board on sawhorses works as well.)\nPick up an armload of plants to be composted and set it on the board with the long stems going in the same direction as the board.\nRoll the plants on top of the board, pulling stray ends into the middle to create a cigar-shaped bundle. Whack it into 3-4” (8-10 cm) pieces with the machete, being careful to keep your fingers at least 8” (20 cm) back from where the blade will cut.\nWhack thicker stems into shorter 2-3” (5-8 cm) pieces, and as the stems taper and become thinner, whack longer pieces. To make turning the pile easier, no pieces should be longer than a spading fork is wide, about 10” (25 cm).\nIn addition to processing the materials first, there are two main differences between hot composting and static pile composting:\nin a Hot Compost Pile ©\nSteve Masley…Click IMAGE to Enlarge\nUse thicker layers of green materials as you build the pile, or thin layers of higher-nitrogen materials like manure. Or use a combination of the two.\nWith static pile composting, you add a slightly thicker green layer when the pile is built up one-third of the way, and again at the 2/3 mark. With hot composting, you add a thick green (high-nitrogen) layer when the pile is 2’ (1.3 m) high, then contain the pile inside a compost bin or wire cylinder.\nin a Hot Compost Pile 2\n© Steve Masley…Click IMAGE to Enlarge\nThis is also a good time to sprinkle on some Rock Phosphate or rock dust, (rock milling fines) to increase the phosphorous content of your compost. Greensand is a good source of organic potassium, mined from mineralized sea shells. Dried, ground eggshells added at this time will increase calcium levels.\nSprinkle 1- 1 1/2 lbs (0.7kg) of each or any of these organic soil amendments over the surface of the pile.\nIn compost piles, inorganic nutrients like these are incorporated into living organisms and organic molecules, which are much more readily absorbed by plants and the soil ecosystem when added to the soil.\nWARNING: Do NOT add dolomite lime or large amounts of wood ashes to compost piles (a light dusting of hardwood ashes between layers is okay). Strongly alkaline materials like these increase pH in the pile, and result in nitrogen losses. For a list of materials to avoid in compost piles, click Here.\nClaw the manure or green layer—with or without additional organic soil amendments—into the dry materials below with the tines of a spading fork, then spread the mound in the middle flat across the bin.\nin a Hot Compost Pile 3\n© Steve Masley…Click IMAGE to Enlarge\nWater the layer using a spray wand. And add a layer of dry brown materials on top. Place a stepladder near the side of the hoop, and climb over the wire onto the pile. Walk around on top of the pile to compress the compost ingredients.\nMake sure you walk around the edge of the hoop, not just in the middle. 2 feet (60cm) of loose materials will compress down to about 8” (20 cm) after you’ve walked around on it.\nWater the top of the pile, then continue alternating green and brown layers, adding a little soil or active compost between layers, and watering them as you build the pile.\nVarying the green and brown materials used in each layer increases microbial diversity in the finished compost.\nAdjust the watering time depending on how dry the compost ingredients are—water dry brown layers longer, and moist, green layers less.\nFor hot composting, aim for a 50% moisture level: Materials should be about as wet as a wrung-out sponge, so only a drop or two of water comes out when you squeeze a handful.\nRepeat the process, alternating green and brown layers, adding soil or compost activator, and watering each layer as you build the pile. Every 18” (45 cm) or so, lay down a dry brown layer, and climb on top of the pile to compress it down.\nWhen you’ve used up 2/3 of your compost ingredients, add another high-nitrogen layer, along with another pound or so of soft phosphate, greensand, and/or oyster shell flour, if you’re adding organic soil amendments to the pile.\nMix it in, top with a dry brown layer, walk on the pile, and water it in. Alternate green and brown layers until all of your compost materials are used up.\nA Compost Thermometer (a thermometer with a 20”–50cm– probe) for monitoring compost heat. This is a critical piece of equipment for thermal composting.\nHot composting requires careful monitoring and frequent turning to prevent the pile from going anaerobic.\nScientists use an oxygen probe to check oxygen levels in the middle of compost piles, but oxygen probes are expensive and delicate, and you can get the same result using an inexpensive Compost Thermometer.\nCompost thermometers have a 20” (51 cm) stainless steel probe that allows you to read the temperature at the center of the pile. When the core temperature climbs over 155° (68° C), oxygen will be depleted within a few hours, so it’s time to turn the pile.\nTurning the pile opens and aerates the core before all the oxygen is depleted. It also allows you to adjust the moisture level of the compost, adding water to dry areas, or mixing saturated layers with dryer materials.\nIn a hot compost pile, half the mass will eventually off-gas as carbon dioxide and water vapor. Moisture cooks out of the pile around the hot core, and may have to be replenished when you turn the pile.\nThe photo table below shows changes that occur when hot composting. Each row shows the pile before turning, the temperature before turning, a cross-section of the pile during turning (note the changes in color and texture), the pile after turning, and the temperature after turning the pile.\nClick any photo to enlarge.\n|Pile Before Turning||Temp Before Turning||Pile Cross-Section||Pile End||Temp End|\n|Compost Pile 2010-1\n|(No Photo)||(No Photo)|\n|Compost Pile 2010-1\n|Date||Time||Temp Start||Action||Temp End|\n|04-14-10||4:16pm||158° (70°C)||Turned pile for 1st time.||91° (33°C)|\n|04-15-10||3:44pm||159° (71°C)||Turned pile for 2nd time.||114° (46°C)|\n|04-17-10||1:03pm||164° (73°C)||Missed yesterday, but caught it before the pile went anaerobic. Turned pile for 3rd time.||120° (49°C)|\n|04-18-10||10:13am||160° (71°C)||Turned pile 4th time.||115° (46°C)|\n|04-19-10||2:33pm||160° (71°C)||Turned pile 5th time. White fungi attacking woody materials.||120° (49°C)|\n|04-21-10||4:06pm||162° (72°C)||Missed another day, but caught it before the pile went anaerobic. Turned pile 6th time. Turned it in stages, watered layers & let them cool to drop the pile temperature.||109° (43°C)|\n|04-22-10||1:57pm||140° (60°C)||Pile heating more slowly, no need to turn.|\n|04-23-10||2:18pm||160° (71°C)||Turned plle 7th time.||98° (37°C)<|\n|04-24-10||11:44am||142° (61°C)||Pile heating more slowly, no need to turn.|\n|04-26-10||12:32pm||158° (70°C)||Turned pile 8th time.||102° (39°C)|\n|04-29-10||152° (67°C)||Temperature over 150° (66°C), but holding, not climbing. No need to turn.|\n|04-30-10||145° (63°C)||Pile cooling, no need to turn.|\n|05-16-10||80° (27°C)||Turned pile out for the 9th and last time, to cure.|\nCopyright © 2009-2020, by Steve Masley, Grow-it-Organically.com\nAll rights reserved""]"	['<urn:uuid:1b41579e-a015-49d9-a2a2-ec7776552fcb>', '<urn:uuid:17defb6c-0ed6-46a5-be2d-8f08b1fd1162>']	factoid	direct	concise-and-natural	similar-to-document	multi-aspect	novice	2025-05-12T20:46:45.703947	10	51	4166
93	I'm planning my first sustainable garden and want to implement both natural pest management and proper soil preparation. What's the recommended soil mixture for a productive vegetable garden, and which eco-friendly pest control options would work best with this setup?	For optimal soil preparation, mix 70% soil with 30% well-rotted manure or compost, and add one pound of 5-10-5 fertilizer per twenty feet of vegetable row. The soil should be dug out to about two feet deep for best results. To manage pests eco-friendly, start with the least toxic methods first. Natural pest controls include beneficial insects, which are most effective in a no-spray environment. You can use Bt (Bacillus thuringiensis) for worm larvae, insecticidal soaps for soft-bodied pests like aphids and whiteflies, and neem oil as a broader solution that can be applied as both a foliar spray and soil drench. These methods are less toxic to non-targeted insects and mammals while remaining effective over time. Remember to identify specific pests before treatment and target only the problematic insects to maintain a healthy population of beneficial insects.	"[""Gardening Tips For The Home Gardner\nWhen you first start growing a home garden you'll have the best results if you grow squash, corn, beets, radishes, beans, peas, pumpkins and carrots from seeds. Almost all herbs and spices should also be started from seeds.\nYou Can Buy Or Grow Some Of Your Vegetable Plants\nFor the best results you should start cucumbers, tomatoes, cantaloupes, watermelons, and pepper plants from plants. The beginning gardener will usually buy these plants from their local home and garden center but if you have a small gree nhouse you can start all of these plants from seeds and when the plants are large enough and all the danger of frost has passed you can plant your plants out in the garden.\nTest And Prepare Your Soil Well Before The Time To Plant Your Garden\nUnless you have land with perfect soil for vegetables you'll need to test your soil to see what you need to add to your soil. You can purchase an inexpensive soil test kit at your local garden center.\nPrepare Your Garden Soil\nWhen I start a new vegetable garden I usually measure me out a space say 48 foot X 48 foot and I dig the soil out down to about two feet and I mix the soil I dig out in with very well rotted manure or well rotted compost at a rate of 70 percent soil and 30 percent of either well rotted manure or compost. For every twenty feet of vegetable row I add one pound of 5 - 10 - 5 fertilizer. When I get my soil prepared this way I have perfectly prepared soil for a vegetable garden.\nMake A Plan\nMake a plan of where you are going to plant each vegetable in your vegetable garden. Do you need to purchase tomato stakes or tomato cages for your tomatoes. You may want to put up lines and stakes for your cucumbers to grow on. If you plant your cucumbers so they can grow up on lines or chicken wire you'll end up with a lot more cucumbers.\nDo You Need A Garden Tiller\nDo you need to purchase a garden tiller or can you rent one in your area for when you need to use it. Be sure to reserve a rental garden tiller far enough in advance for when you need it. If your going to be making a garden every year you may want to purchase your own garden tiller. Be sure you check out the garden tiller you are thinking of purchasing carefully. Be sure you know and understand exactly what you are purchasing and what you get with it. Be sure that you understand what kind of maintenance your going to need to do on your garden tiller on a regular basis.\nGrow The Vegetables You And Your Family Like\nWhat's the point of growing a bumper crop of parsnips or turnips if no one in your family eats them. Set down with your family and plan out what vegetables your going to grow in your family vegetable garden. Have a plan drew out and planned on paper for where each vegetable is going to be grown in your garden.\nHome Garden, Garden Tips For You\nHere are 12 Garden Don'ts For You to follow so your garden will grow and produce well. Along with some other great information for the home garden.\n1. Never store left over diluted spray or plant food. Discard it instead as it could grow mold and spread disease if used. When you use your garden sprayers or receptacles you want to be sure that you rinse out all garden sprayers or receptacles well and hang them in full sun so they will dry well.\n2. Never use chemicals or sprays in your garden that are n ot for the garden. Are you having a problem with garden slugs. Just get them drunk and get rid of them. Fill a shallow dish with beer and provide a couple of Popsicle stick ramps up to the rim of the dish for the slugs to climb up and they will get drunk and fall in and drown. Then you can dispose of them.Set out your beer traps at sun down and check them early the next morning.\n3. Read the labels on anything you use in your garden carefully. Make sure that you know exactly what you are doing and follow the directions carefully.\n4. Never allow any weeds to grow in your home garden. Get rid of them quickly. Mulch well around your plants and you will have much less of a garden weed problem.\n5. Don't place fertilizer directly onto plants or seeds. Keep it back 3-4 inches and your plants will do well. As you water the fertilizer will break down and go down into the soil.\n6. Never water your garden or plants when the sun is on them. This of course means you should water just before the sun comes up in the morning or just after the sun goes down in the evening.\n7. Keep in mind how tall your plants are going to get and don't plant your garden plants where they will shade smaller plants. All your taller plants should go to the back of the garden and shorter plants to the front.\n8. Use care when you get near your plants with your garden hoe or spade. Don't dig to near your plants or you can damage your garden plants roots.\n9. Don't plant your rows to close together. Allow room to walk around and work. And be sure to thin your plants when they are very young.\n10. Plant the varieties of Sprinkler System Installation Fort Worth vegetables your local garden supply store recommends especially until you gain a year or two of gardening experience.\n11. Consider setting up rain barrels with tight fitting screens over the tops to keep mosquitoes out. T his will provide you with water for your plants that your not going to have to pay for. Add a spigot at the bottom of each rain barrel to make it easy to get the water out. And this way you will have water for your garden if your town, city, or water district puts out water restrictions that prohibit watering gardens.\n12. Make use of plants like marigolds, rosemary and peppermint to keep pests out of your garden. If you have a real mosquito and insect problem purchase a bat house and set it up 20 ft up a tree or pole above a garden pond or pool. You won't believe how many insects a single bat can eat in one night. And if Sprinkler System Fort Worth the bats are eating the insects you won't have to worry about them.\nHome Garden, Garden Tips For You\nHere are 12 Garden Do's That You Should Do For A Healthy Home Garden that is going to http://www.texasartifici allawns.com/ produce an abundance of garden vegetables for you.\n1. Keep a journal about your gardening activities. Label each one for the year you are in so in the future you can look back and see what you did for a particular garden problem or what was the name of the squash you planted that year that you loved.\n2. You will want to keep your gardening tools all in one place and you will want to make sure that you spray your tools off with water when you are finished with them for the day. You may want to use a wood burning tool to write your family name on your garden tool handles to prevent theft.\n3. Avoid going into your garden when the soil and plants are wet. You can wait until the ground and plants are dry before you work in the garden. You don't want to harvest vegetables when your plants are wet as this will invite disease.\n4. Thin your plants when they are about finger high. Keep those seed packs so you can read the special instructions for y our plants on the backs of those seed packs. If you can't find the information you need there call your local garden supply store or county farm agent and ask your questions.\n5. You will want to water your garden as needed and its always best to water at the base of the plant. Use soaker hoses rather than a sprinkler for the best results. Your water needs to reach a depth of 6 inches.\n6. Use mulch in your garden to reduce the need to water so often and to prevent weeds growing in your garden. Check with your city or county to see if they have a place you can go and get free leaf mulch. If they give you a choice you want the oldest looking mulch you can get.\n7. Do everything you can to see that your garden stays free of insects, disease and weeds. You can plant some marigolds, peppermint in containers and rosemary to keep away a lot of your garden pests. Never plant peppermint directly in the ground as it will spread rapidly.\n8. Examine your garden careful ly each day to see if you see any potential problems that you can catch before they get out of hand. Be sure to take your hoe or garden spade and see how dry your garden is. Don't wait to water until your plants wilt. You will want to keep a special eye on your cucumber and melon plants as they need a lot of water.\n9. Use either compost tea or miracle grow for vegetables on a regular basis. If you need to know how to build a quick and easy Compost Bin then CLICK HERE\n10. Sample your gardens soil and have it tested every two to three years so you will know exactly what your garden soil needs are.\n11. Ask around at your local garden store about what varieties of plants grow best in your area. If everyone else is growing a certain variety of squash it will probably grow well in your garden also.\n12. If you have one bad looking or diseased looking plant in your garden remove it from the garden area and dispose of it so that nothing will spread."", 'Go with Natural Pest Controls First\nOrganic pest control methods are generally less environmentally damaging, and less toxic to non-targeted insects, mammals and aquatic life. Unfortunately, in our time-starved world, many people simply want the most potent, one application product—no matter what the consequences.\nPest control strategies in the eco-friendly garden seek to use the least toxic method first. There are some very effective natural control treatments available. The good news is, with the proper preparation, and cultural practices, rarely if ever will you need to get beyond these measures.\nThe first step in controlling pests, before even applying the most benign treatment is to create the most hospitable growing environment for your plants. A healthy garden is the single best natural pest control there is. Healthy plants are less attractive to pests in the first place, and when they are attacked, the plants are better equipped to defend themselves and recover.\nAn advantage to the “no-spray” method of control is that beneficial insects have the best opportunity to establish populations in your garden and do the work for you. You may need to exercise a little patience and put up with some cosmetic damage initially. But, they are incredibly effective at natural pest control.\nWith any pest control treatment, the first step should be to identify the offending pest, and target a control method that only affects that pest. Don’t apply a non-selective chemical that kills beneficial insects as well! There are a good number of organic options available, and some are more specific to certain pests than others.\nIf there’s a downside to natural pest control methods, most people would say they are not as fast acting, and yet can be every bit as effective as synthetic controls over time. To me, that’s an acceptable tradeoff! Although there are many, here are some of the most common options:\nThese insecticides cause pests to get sick, are very specific to the target pest, and do not harm beneficial insects, nor are they toxic to mammals. One of the most popular choices is Bt (Bacillus thuringiensis). I use this whenever necessary to treat a number of worm larvae from hornworms to cabbage loopers, and cutworms. The bacteria in Bt paralyzes the digestive system of the larvae. They stop eating and within a couple of days, the pests are dead.\nThese soaps utilize the salts and fatty acids within them to target many soft-bodied pests including aphids, whiteflies, mealy bugs, earwigs, thrips, and the early stages of scale. The soaps penetrate the soft outer shell of these and other pest, causing damage to the cell membranes. They then begin to breakdown, resulting ultimately in dehydration, and starvation.\nInsecticidal soaps can be phytotoxic to certain plants, so be sure to test a small area, before applying on a larger scale. The other downside is that soaps are non-selective so they can be toxic to beneficial insects as well. Use them sparingly, as with any pesticide. Insecticidal soaps have not shown to be toxic to humans and other mammals.\nYou can make your own by adding a teaspoon of dish soap (not detergent) and a teaspoon of cooking oil to a one quart spray bottle. Insecticidal soaps are also readily available for purchase at nurseries or in garden centers.\nThese oils work by suffocating the pest. The oil coats them with a petroleum-based, horticultural grade liquid, cutting off their oxygen supply. This control method has been around for a long time. It is primarily used to kill the eggs and immature stages of insects. These products are very effective because they spread so well, and break down quickly. However, these oils can and do affect beneficial insects, but are less toxic to them.\nOils are often used to control aphids, scales, spider mites, mealy bugs, psylla, and some other insects. These oils can harm your plants and trees; primarily leaf damage, so be sure to read the directions that come with the packaging.\nNever spray these oils on a hot day, usually over 85 degrees, and its best to spray a small area of your plants first. After a few days, look for any damage from the oil, before commencing with a larger application. If no damage is observed then continue spraying, coating the top and bottom of all leaf surfaces.\nThis product is the fossilized silica shells of algae. Although these shells are microscopic in size, they’re covered with sharp projections that cut and penetrate the cuticle of an insect. This causes the pest to leak vital body fluids. The result is dehydration and death. The unique aspect of diatomaceous earth is that it is not a poison that causes the damage, but the physical abrasiveness of the dust.\nDE is effective against soft-bodied pests including aphids, trips, whiteflies, caterpillars, root maggots, slugs, and snails. However, DE is non-selective, and will potentially kill beneficial insects as well.\nApply DE to the soil for ground dwelling pests, and to the foliage for other pests. DE adheres best to moist foliage, so application is best early in the morning, when leaves are wet from dew, or after a rain. Be sure to use “natural-grade” vs. “pool-grade” DE which contains additional chemicals that can be harmful to humans and mammals if inhaled. In either case, it’s a good idea to wear a dust mask whenever working with any dusting agent.\nNeem is a broad-spectrum insecticide, acting as a poison, repellent, and deterrent to feeding. It also sterilizes certain insect species, and slows or stops the growth cycle of others. Neem oil is derived from the Neem tree, which is native to India. Neem is applied as a foliar spray, or soil drench. It is used to kill a wide range of pests, including aphids, thrips, loopers, whiteflies, and mealy bugs.\nOne unique aspect to this biological agent is its systemic properties. Plants take up the Neem extracts through plant foliage and roots, where it is present in the plant tissue. As a result, Neem is also effective against leaf minors, which are usually not affected by other non-systemic foliar sprays.\nGenerally, Neem must be ingested to be toxic, and is nearly nontoxic to mammals. Although it breaks down quickly, you should spray Neem only when necessary, and only on plants known to be affected by the pest you are targeting. In this way, you will minimize the damage to beneficial insects.\nBotanical Insecticides: Pyrethrin\nThere are a number of botanical insecticides but we’ll focus on the most popular; pyrethrin. It is the active ingredient extracted from the Pyrethrum daisy. Products containing pyrethrin contain compounds that kill on contact. They are considered broad-spectrum (non-selective) and are used to control many chewing and sucking insects. Do not confuse pyrethrin with the synthetic version called Pyrethroid. It is even more toxic to all insects.\nUse caution when using pyrethrin products as they are toxic to fish and moderately toxic to mammals. They are non-selective and harmful to some beneficials including lady beetles. Insecticides using pyrethrum are of particular concern for mosquito control. Although it is marketed as an organic method of dealing with the problem, the mist used to kill the mosquitos is also lethal to many other insects, including honeybees and other beneficials.']"	['<urn:uuid:fe808675-5049-4e2c-92a6-027a4a2cee9d>', '<urn:uuid:f95e7600-886a-4b5e-ba33-21f53c5ebcf7>']	open-ended	with-premise	verbose-and-natural	similar-to-document	multi-aspect	expert	2025-05-12T20:46:45.703947	40	138	2928
94	can person refuse answer questions police officer asks street	Yes, in most instances, you have the right to refuse to answer questions when approached by law enforcement officers. Politely telling an officer that you do not wish to answer any questions should end the encounter. However, if you are being detained, while you still have the right not to answer questions, the officer can insist that you identify yourself.	['14 Sep What Are Your Rights When Police Want To Question You?\nThe vast majority of encounters between police and the public ends quickly and without incident. It is a part of their job as protectors of the community for law enforcement officers to have the power to interact with civilians and, when necessary, to question them in order to gather information to aid them in performing their sworn duty.\nFor someone approached by the police either on the street, at home or at work, such encounters can be intimidating. It is, therefore, essential for you to know your rights when questioned by the police and to have a clear understanding of how to protect those rights.\nYour Rights and the Constitution\nThe U.S. Constitution and California laws give you the right to move about without being unreasonably interfered with by police and other law enforcement officers from state or federal agencies. While law enforcement officers have the authority and the power to approach you and ask you questions, you have the right, in most instances, to refuse to answer them. Politely telling an officer that you do not wish to answer any questions should end the encounter.\nThe so-called “stop and frisk” you might have seen on TV is usually not an accurate portrayal of what the law and the Constitution actually permit police to do. Police, based upon their training and experience see situations differently than most people. For this reason, the U.S. Supreme Court decided a police officer observing suspicious behavior without having evidence to support his or her suspicion may approach and question the individual to determine if criminal activity might be present.\nAn investigative stop is not, however, the equivalent of an arrest, but if a police officer tells you that you are not free to go, you are being detained. While you continue to retain the right not to answer questions, the officer can insist that you identify yourself.\nQuestioning Following an Arrest\nAlthough you have the right not to answer police questions prior to being arrested, if police take you into custody on a criminal charge, they must advise you of your rights under the Fifth Amendment as follows:\n- You have the right to remain silent.\n- Anything you say can be used against you in court.\n- You have the right to speak to an attorney and to have the attorney present during any questioning.\n- If you cannot afford an attorney, one will be appointed to represent you.\nWhether police read you these rights or not, you should politely refuse to answer any questions not having to do with your name, address, date of birth and other identifying information before speaking to an attorney.\nOnce you tell the police you wish to speak to an attorney, all questioning must stop until your attorney arrives. Just because the police stop questioning you does not mean you can let your guard down. Anything you say, even if it is not in response to a question, can be used against you in court. For example, someone placed in a holding cell to await the arrival of an attorney begins a conversation with another person also being held by the police. If anything of an incriminating nature is said by one prisoner to the other, it can be used by police.\nConsult with a Criminal Defense Lawyer\nIf you suspect you are under investigation by law enforcement or if you have already been arrested, you should consult with a California criminal defense attorney before speaking to the police. Police officers and detectives are highly trained and skilled in the interrogation and questioning of suspects, so you want an experienced criminal defense attorney looking out for your rights and wellbeing.']	['<urn:uuid:6959cd68-e570-44f0-8ae7-b7df8073add7>']	factoid	direct	long-search-query	distant-from-document	single-doc	novice	2025-05-12T20:46:45.703947	9	60	625
95	power system stability control compensation techniques electrical network performance improve	Power system stability can be improved through various compensation techniques and control methods. The NAP software provides stability calculations through its STABIL module, which simulates dynamic behavior during system disturbances like short-circuits or generator loss. Additionally, robust compensating functions using H-Infinity synthesis have been proven effective for power system stabilization, achieving zero overshoot, 1.04 seconds settling time, and strong stability margins (21.7dB gain margin, 79.6 degrees phase margin). These techniques help maintain system performance even during parameter changes and disturbances in the power line.	"['NAP - Network Analysis and Planning\nNAP allows to analyze and plan the electric transmission network by load flow and stability calculations\n- Dimensioning of MV/HV networks (lines, compensators, generators ...)\n- Analysis of existing networks and identification of weaknesses\n- Planning of new distribution and transmission networks\n- Planning for reinforcement of existing distribution and transmission networks\n- Incident management forecast (criterion N-1)\n- Dynamic stability analysis\nEconomic and financial optimization:\n- Loss reduction study\n- Minimization of operating costs\n- Optimization of production plans and dispatching\nNAP is a complete software for the analysis and planning of electrical networks. A single graphical interface groups together several calculation modules:\n- Load Flow (ILF)\n- Load Flow with constraints (CPF)\n- Optimized Load flow (OPF)\n- Short circuit (SCC)\n- Incident simulation (OUTSIM)\n- Stability calculations (STABIL)\nNAP is the result of more than 30 years of experience in charge flow modeling, recent mathematical research, and object-oriented programming. Our main goals have always been to develop software:\n- Simple to use: No training is required to use the NAP interface because it is based on Windows standards.\n- Specific for planning: definition of scenarios, years of switching on and off, as well as load growth.\n- Based on reliable, flexible and powerful algorithms.\nGraphic representation of the networks\nThe networks are presented in the form of single-line diagrams. The user can choose the data and results to display on these diagrams. The thickness and color of network elements may depend on the value of any given data or result. Several schemas can be simultaneously displayed on the screen, making it easier to compare different network alternatives. Standard functions such as zooms are of course available.\nDrawing a network element (node, transformer, line or DC line) is enough to create it. It is also possible to move it with the mouse. By clicking on an element of the network, a form is opened, presenting the results and making it possible to modify the data. The data are defined in physical units (MW, km,? ...); forgotten conversions to ""per unit"".\nThe scenario manager\nScenarios are organized on the principle of inheritance. This means that if the value of a data item is not defined in a scenario, it is equal to that defined in the parent scenario, which can also inherit from its own parent scenario and so on. It is thus possible to define variants by avoiding the copying of data. In planning, scenarios may differ in the value of their technical data or in different investment strategies.\nNetwork Calculation Modules\nThe optimized load flow is to solve a cost minimization problem whose variables must satisfy the following constraints:\n- Equations of charge flows.\n- Inequations: operating limits of controllable variables.\nThe calculation is divided into 3 steps: ILF, CPF and OPF.\nILF - Flow of initial charges\nThis first step consists of solving a standard power flow problem ignoring the inequality constraints, using the Newton-Raphson mathematical technique.\nCPF - Flow with constraints\nWhen the ILF solution does not satisfy the inequalities, the CPF searches in the solution space for a solution that satisfies all the inequalities on the voltages, the generation, the currents and the steps of the transformers. If such a solution does not exist, the violated constraints are highlighted.\nOPF - optimized charge flow\nThis third step consists of finding the power flow solution, which minimises the generators operating cost while not violating the inequality constraints. Due to this powerful approach, the OPF also calculates the marginal costs of real and reactive demand at each node. This information is a powerful help when designing new systems or upgrading existing ones.\nSCC - Short circuit\nThe objective of the Short Circuit simulation is to estimate the status of the generation and transmission system a few voltage cycles after the occurrence of a fault. Transient or sub-transient responses can be analysed. The program simulates symmetrical faults (3 phases to ground) and non-symmetrical faults (1 phase to ground, 2 phases to ground, or phase to phase). The OPF solution is used as the pre-fault status of the system.\nOUTSIM - Contingency Analysis\nOUTSIM is a N-1 incident simulator, i.e. it simulates lines, transformers and generators outages one by one. Starting from the OPF solution, a full AC power flow simulation is performed in order to predict the impact of outages on branch loading, voltages limits and generation capacity.\nSTABIL - Transient Stability calculation\nSTABIL simulates the dynamic behaviour of an electric power system during the first few seconds following a major disturbance of that system (e.g. a short-circuit, the loss of a generator or a major branch opening). The main result is a set of curves describing the evolution of the generator rotor angles during the simulation period (generator swing curves). Additional phenomena can also be studied such as the action of the voltage and speed regulators.\nThis module allows to analyze the evolution of any result according to the variation of the value of any data within limits fixed by the user\n- Report Builder: Create your own reports by filtering, selecting, and sorting results.\n- Import: XML, IEEE, PSS / E, ASCII, clipboard.\n- Backup configurations: reports, drawings ...\n- Export reports, graphs, and schemas to other software such as Microsoft® Office.\n- Multiple selections of network elements.\n- Scripting tools and DDE server.', 'Robust Compensating Function Scheme for Adequate Electrical Power System Stabilization\nJournal of Scientific Research and Reports,\nThis work centers on robust compensating function scheme for adequate electrical power system stabilization. There has been high level of disturbances in the power line and lack of adequate compensation technique to cancel the effects of the resultant instability which has caused power failures. The problem was addressed by the consideration of disturbances in the power line during the design of the compensating function for the improvement of the power system performance and stability. H-Infinity synthesis robust compensating function design method was used to design an adequate compensator that can improve the performance and stability of the power system. From the results, the H-infinity Synthesis Controlled Generating Plant (HCGP) recorded an overshoot of 0%, settling time of 1.04 seconds, tracking error of 0dB, gain margin of 21.7dB and phase margin of 79.6 degrees. The simulation was repeated by varying the value to k to -0.3, and the generating plant produced same results. This shows that the system can maintain performance and stability equilibrium even when there is change in its parameters. Since the HCGP satisfied the performance and stability robustness, therefore it was concluded that power system robust compensating function scheme for improved performance and stability robustness was achieved using H-Infinity synthesis method.\n- Power system stability\n- compensating function\n- h-infinity synthesis\n- tracking error\n- robust control\nHow to Cite\nManuaba IBG, Abdillah M, Priyadi A, Purnomo M.H. Coordination Tuning PID-PSS and TCSC Based Model of Single Machine Infinite-Bus Using Combination Bacteria Foraging-Particle Swam Optimization Method, Control and Intelligent Systems; 2015.\nOonsivilai A, Marungsri B. Stability enhancement for multi-machine power system by optimal PID tuning of power system stabilizer using particle swarm optimization, WSEAS Transactions on Power Systems. 2008;3:465– 474.\nAgbaraji CE. Robustness Analysis of a Closed-loop Controller for a Robot Manipulator in Real Environments, Physical Science International Journal. 2015;8(3):1-11.\nSmith M. Electric Power System Modeling & Simulation; 2010.\nAnderson PM, Fouad AA. Power System Control and Stability, Iowa State University Press, Ames, Iowa; 1977.\nAdepoju GA, Sanusi MA, Tijani MA. Application Of SSSC to the 330KV Nigerian Transmission Network for Voltage Control, Nigerian Journal of Technology (NIJOTECH). 2017;36(4):1258 – 1264.\nVakula P. Study the Power Flow Control of a Power System with Unified Power Flow Controller, Unpublish Master Thesis Submitted to California State University, Sacramento; 2010.\nBada ASA. Transmission Evacuation and Constraints, at National Power Sector Retreat Abuja, Nigeria; 2012.\nAmbafi JG, Nwohu MN, Ohize HO, Tola OJ. Performance Evaluation of PSS and STATCOM on Oscillation Damping of a North-Central Power Network of Nigeria Grid System, International Journal of Engineering and Technology. 2012;2(2):209-219.\nKundur P. Power System Stability and Control, McGraw Hill, New York. 1994;817-822.\nAbido MA. Power System Stability Enhancement Using Facts Controllers: A Review, the Arabian Journal for Science and Engineering. 2009;34(1B):153-171.\nEriksson R. Security-Centred coordinated Control in AC/DC Transmission Systems, Master thesis, TRITA-EE. 2008;034.\nMosaad AM, Attia MA, Abdelaziz AY. Whale Optimization Algorithm to Tune PID and PIDA Controllers on AVR System, Ain Shams Engineering Journal. 2019;10:755-767.\nLakshmi ASVV, Manyala RR, Mangipudi SK. Design of a robust PID-PSS for an Uncertain Power System with Simplified Stability Conditions, Protection and Control of Modern Power Systems. 2020;5(20):1-16.\nBevrani H. Robust Power System Frequency Control, Springer Science+Business Media, LLC; 2009.\nNagsarkar TK, Sukhija MS. Power System analysis, New Delhi: Oxford University Press; 2007.\nAgbaraji CE, Udeani UH, Inyiama HC, Okezie CC. Robust Control for a 3DOF Articulated Robotic Manipulator Joint Torque under Uncertainties, Journal of Engineering Research and Reports. 2020;9(4):1-13.\nAbstract View: 36 times\nPDF Download: 32 times']"	['<urn:uuid:afd999e4-2b64-40e7-ae4f-e4d4ce6ae7e0>', '<urn:uuid:a2981d66-f06d-4b13-b307-132ab0d52097>']	open-ended	direct	long-search-query	similar-to-document	three-doc	novice	2025-05-12T20:46:45.703947	10	84	1492
96	How can businesses verify their suppliers' security standards?	Businesses can perform security assessments and validations, such as Service Organization Control examinations and ISO/IEC 27001 certifications, which is a great way to build trust in the supply chain. Supply chain managers must ensure they're using reputable, industry-tested suppliers.	"['Defining your company\'s security requirements and having a cyber-risk management program to evaluate third-party services can reduce the risk of attacks on supply chain software.\nIncreasingly, hackers are using more sophisticated methods to attack companies\' supply chain management software, ultimately disrupting operations and wreaking havoc on their networks.\nAlthough there are steps organizations can take to minimize the damage caused by supply chain attacks, as well as to shore up defenses after attacks, the smartest option is to prevent these breaches from ever happening.\nCybersecurity best practices\n""Today\'s enterprise must focus on these relationships and ensure that vendors and suppliers are taking security seriously and using the appropriate measures to instill trust in their business relationship.""\nThere are a number of cybersecurity best practices enterprises can follow to reduce their chances of falling victim to supply chain attacks.\nOne thing supply chain managers must do is ensure that they\'re using reputable, industry-tested suppliers, said Justin Bateh, supply chain expert and professor of business at Florida State College at Jacksonville.\nWhen using third-party service providers that have virtual access to information systems, supply chain managers and vendors must have a certain level of trust, as well as transparency about what data is available, who has access to the data and how it will be used, he said.\nJason Rhoades, a principal at Schellman & Co. LLC, a provider of attestation and compliance services in Tampa, Fla., agreed.\nToday\'s enterprise must focus on these relationships and ensure that vendors and suppliers are taking security seriously and using the appropriate measures to instill trust in their business relationship, he said. Performing security assessments and validations, such as [Service Organization Control] examinations and ISO/IEC 27001 certifications, is a great way to build trust in the supply chain.\n""However, trust isn\'t enough, and supply chain managers must ensure that there are hierarchical levels of access, compliance training is present, [and] auditing and evaluation mechanisms are utilized,"" Bateh said.\nSupply chain management involves different processes within a business that are managed in different silos, but that are able to communicate with one another, said Alex Hsiung, a manager at Schellman.\n""From end to end, when you\'re creating a new product, you want each piece of the supply chain to ultimately have the same minimum security requirements throughout,"" Hsiung said. ""You have to ensure that there\'s consistent application of those security controls to mitigate the risks.""\nConsequently, organizations need some kind of internal cyber-risk management program in place, said Sean Peasley, a partner and leader in cyber-risk services at Deloitte & Touche.\n""The program should include the types of risks they\'re trying to alleviate, [as well as] the various leading practices or standards or regulatory mandates that they\'re considering to manage those risks,"" Peasley said.\nJason Rhoades is a Principal at Schellman & Company. Jason is a SOC practice leader and assists with methodology and service delivery across all service lines including SOC, PCI-DSS, ISO, FISMA and HIPAA services. Jason also helps assist large and complex customers who have multiple compliances needs helping them strategically align their efforts to maximize cost and efficiencies. Prior to joining Schellman, Jason served as a project manager with a Fortune 500 company where he was responsible for design, implementation and security of critical applications supporting various business functions.\nAlex Hsiung is a Manager with Schellman & Company, LLC based in Los Angeles, CA. Prior to joining Schellman & Company, LLC in 2015, Alex worked as an Associate at KPMG, specializing in Sarbanes-Oxley compliance audits and IT advisory engagements. Alex also led and supported various other projects, including business process and information technology readiness assessments, internal audit services and regulatory compliance engagements. Over 2 years of experience comprised of serving clients in various industries, including financial services, healthcare and manufacturing. Alex is a dedicated member of the ISO Service Team.\nAbout the AuthorMore Content by Schellman & Company']"	['<urn:uuid:218ce02d-1798-4cc6-a8b7-71c4c2d8ffc4>']	open-ended	with-premise	concise-and-natural	distant-from-document	single-doc	expert	2025-05-12T20:46:45.703947	8	39	646
97	Why are Caribbean countries struggling to help disaster victims?	Several bottlenecks limit the effectiveness of social protection systems in Caribbean countries: some countries have very low social protection coverage, limiting rapid response to those most in need; benefits and programs struggle to address reliance on climate-sensitive sectors; delivery systems in several countries are too rudimentary or rigid to respond quickly when shocks occur; and programs struggle to effectively build resilience and improve sustainable inclusion of the most vulnerable.	['Authors: Nick Grainger, Asha Williams, Kathryn Milliken and Francesca Ciardi\nThe increased intensity of hydro meteorological events in Caribbean countries in recent years has altered landscapes, damaged infrastructure, debilitated economies, and threatens to undermine development and growth. In the midst of these impacts are Caribbean people, whose livelihoods, income, homes, well-being, and ultimately, survival, are directly threatened by climate and disaster risks.\nTake for instance, Hurricane Maria’s impact on Dominica in September 2017. The event caused damages and losses amounting to 226% of the country’s GDP; damaged 90% of the country’s housing stock; and affected approximately 77% of the population. At the center of the disaster, were families like Josiah Stewart, who along with his mother, grandmother, and sisters lost their home and all their belongings during the hurricane.\nIts stories like Josiah’s that highlights the urgency of ensuring that Caribbean populations are (i) more resilient to increased climate shocks and (ii) that Governments are better prepared to quickly and effectively respond to the needs of those affected. Without addressing these two parallel priorities, the poor and vulnerable in the region could fall further into poverty and non-poor households could become poor, all from a single event.\nDespite the threat that climate and disaster risks pose for well-being in the region, there is also much opportunity. Social Protection (SP) programs and systems in the Caribbean are a valuable mechanism for protecting the poor, improving equity, and building human capital and resilience, particularly among poor and vulnerable households. Most Caribbean countries have a solid base which can be leveraged to provide post-disaster support to households, and in some circumstances even predictive support. This base includes cash and in-kind transfers (usually targeted to the poor and vulnerable); public works programs; social security schemes to provide protection in old age; and active labor market programs, including skills training, entrepreneurship programs, and labor intermediation services. As the intensity and severity of climate risks threaten the region, leveraging SP programs and systems to help address these risks is critical.\nThere is global recognition of the significant linkages between the use of SP programmes to respond to the impacts of disasters and a changing climate. Governments, humanitarian agencies, international organizations and NGOs have been increasing efforts to ensure that SP programs and systems are more responsive and better prepared to address these challenges. The motivation is simple, as social protection, by its own objectives is focused on helping households manage risk. SP programs also interface more regularly with vulnerable populations. However, there still remains several bottlenecks which limit the full potential of SP systems in the region from effectively addressing these risks, including:\n- Some countries still demonstrate very low SP coverage, which limits rapid response to those most in need;\n- SP benefits and programs still struggle to address the reliance on climate sensitive sectors and exposure to different shocks;\n- Delivery systems in several countries are either too rudimentary and/or rigid to respond quickly and effectively when shocks occur;\n- SP programs struggle to effectively build resilience and improve sustainable inclusion of the most vulnerable.\nA massive effort is underway by governments and in partnership with development partners to improve the responsiveness of SP systems to address these risks in the region. The United Nations World Food Programme (WFP) and the World Bank have been investing in adaptive and shock responsive social protection (SRSP) programmes in the Caribbean, to strengthen countries capabilities to build resilience and to respond to emergencies with greater efficiency, efficacy and equity. These efforts include several core elements:\n1. Assessing the readiness of SP systems in Caribbean countries to address climate and disaster risks.\na. The World Bank and WFP have both done in-depth assessments to assess the readiness of SP systems in Caribbean countries to address climate and disaster risks, in Belize (ongoing), Dominica, British Virgin Islands, the Dominican Republic, Grenada, Jamaica, St Lucia (ongoing), Guyana (ongoing) and St Vincent and the Grenadines. These assessments have provided Governments with clear analysis of what bottlenecks exist to effectively ensuring that SP programs address disaster risk management objectives.\n2. Understanding the social impacts and needs from climate risks:\na. In St Vincent and the Grenadines, the World Bank developed an index that estimated the social protection budget support needed to reduce hazard exposure of the poor using a model that combined household construction type (roof and wall characteristics); hazard exposure; and poverty.\n3. Supporting improvements in design of SP programs, services and delivery systems.\na. World Bank technical assistance to Jamaica supported the improvement of the household damage assessment instrument – the main tool used to assess post-disaster household needs and conditions, and to determine, safety net response after disasters. The new Jamaica Household Damage Impact and Needs Assessment (JHDINA) instrument is available in multiple formats to enable quick decision making and is a unified instrument to be used by multiple agencies engaged in disaster response to households.\nb. World Bank operations in the Dominican Republic, Jamaica; St Maarten; and St Vincent and the Grenadines are supporting the improvements of SP programs, services and systems, including improved social registries, graduation and productive inclusion of the poor, and more effective payment delivery mechanisms.\nc. WFP is supporting the development and strengthening of Management Information Systems in Dominica and the British Virgin Islands for improved data management and targeting of national social protection programmes.\n4. Helping countries to respond to shock impacts on the population:\na. Joint Emergency Cash Transfer Programs in Dominica: Following the devastating impact of Hurricane Maria in 2017, WFP supported the Ministry of Social Services, Family and Gender Affairs to establish a cash-transfer programme through the national Public Assistance Programme (PAP). Top-up funds were disbursed to 25,000 beneficiaries (approximately 40% of the population) including recipients of PAP and those who had not been enrolled in the programme but who had been severely affected by the passage of the storm. The value of the cash transfer was calculated based on the average size of a family, the monthly cost of a minimum food basket and with support from UNICEF, the cost of providing children with enough food, clothing, education and any other basic need. To address continued food assistance needs, WFP also issued an additional Food Security Cash Transfer in July 2018 reaching a total of 22,750 beneficiaries.\nb. Emergency Income and Training Program in St Maartin: Financed by the Sint Maarten Recovery, Reconstruction and Resilience Trust Fund funded by the government of the Netherlands and the World Bank acting as fund manager, this project provides temporary income support and skills training in selected occupational areas to persons who became unemployed and under-employed following Hurricane Irma’s impact on the country. Since early 2018, approximately 1000 persons have been trained.\nc. Technical advisory services, workshops and advocacy to strengthen the linkages between SP and DRM for improved disaster preparedness and response: Under a multi-country Caribbean Emergency Preparedness and Response strategy funded by ECHO, WFP is supporting CDEMA’s Participating States to increase knowledge, improve coordination and boost cooperation for a more cohesive and integrated approach to disaster risk management, social protection, international humanitarian assistance and climate change resilience, in alignment with small island developing states’ priorities and national sustainable development plans.\nThese efforts have identified critical gaps to improving resilience and effective response, and stepped-up efforts to help Governments improve the performance of their SP systems. Nevertheless, there remains much work to be done, including ensuring effective financing mechanisms for scaled-up SP response; utilizing SP programs like public works to simultaneously mitigate climate risk and boost resilience of the vulnerable; completing improvements to SP delivery mechanisms; and expanding social protection coverage. Addressing these remaining priorities will require a multi-sector, multi-pronged approach, led by Government (disaster risk management agencies, social protection ministries etc.), development partners, NGOs, academia, financing institutions and humanitarian agencies.\nThe Understanding Risk Caribbean Conference will provide spaces for knowledge sharing and collaboration through two sessions jointly organized by the World Bank and World Food Programme. Participants are invited to attend to learn more on how Adaptive and Shock Responsive Social Protection is putting people at the center of resilience and response.\n Caribbean Disaster Emergency Management Agency (CDEMA). 2017. (56,890 persons)\nPanel Discussion: 29 May, 2019 | 14:00-15:15 |EBCCI GL Pedagogical Centre\nEffective Risk Management with a Social Lens: The importance of Social Information Systems, Household Vulnerability Analysis and Adaptive Finance\nTraining Session: 31 May, 2019 | 13:30-17:00 |EBCCI GL Pedagogical Center\nThe Nuts and Bolts of Adaptive/Shock-Responsive Social Protection – Integrating Social Protection into Disaster Risk Management']	['<urn:uuid:344f08da-017d-4402-990d-1b80d779fdec>']	open-ended	direct	concise-and-natural	distant-from-document	single-doc	novice	2025-05-12T20:46:45.703947	9	69	1417
98	When did rail service first reach Crystal Lake area?	The first train station was built in 1856, being pre-fabricated and shipped from Chicago on a flatcar.	"['How to Become an Electrician in Crystal Lake Illinois\nThe initial step to becoming an electrical tradesman or contractor is finding an electrician trade school near Crystal Lake IL. But with so many vocational schools to pick from, just how do you tackle making sure that you enroll in the right one? Especially since there are a number of variables to examine. For instance, some students will start by searching for schools that are close to their residence. Once they have located several that are within driving distance, they will pick the one with the most affordable tuition. While location and cost are important, they are not the sole factors that must be considered. Also critical are the reputations of the schools, their accreditation, along with their graduation and job placement rates. These and other qualifications should help mold your final decision when choosing an electrician school. We will address that checklist in more detail later in this article. But first, let’s review a little bit about becoming an electrician and the instructional options that are offered.\nElectrician Certificate, Diploma and Degree Programs near Crystal Lake IL\nThere are multiple ways to obtain electrician training in a trade or technical school near Crystal Lake IL. You may choose a diploma or certificate program, or obtain an Associate Degree. Bachelor’s Degrees are offered at some schools, but are not as prevalent as the other three alternatives. Frequently these programs are made available together with an apprenticeship program, which are required by most states in order to be licensed or if you wish to earn certification. Following are brief descriptions of the three most common programs offered.\n- Diploma and Certificate Programs are generally offered by Illinois vocational and trade schools and take about a year to finish. They furnish a solid foundation and are aimed towards individuals who would like to get into an apprenticeship faster as a journeyman electrician.\n- Associate Degree Programs require two years to complete and are offered by Illinois junior or community colleges, typically as an Associate Degree in Electrical Technology. They provide a more well-rounded education while supplying the foundation that prepares students to begin their apprenticeship program.\nAs previously stated, Bachelor’s Degrees are offered at some Illinois institutions, but are less favored at 4 years than the other shorter programs. Most states require that an apprenticeship of no less than 2 years and more typically four years be performed before licensing. Because of that, the majority of students are eager to start their paid apprenticeship, especially if it’s not a component of their educational program.\nElectrician License and Certification Requirements\nElectricians in Crystal Lake IL can undertake a wide range of duties, such as testing, installing and replacing electrical systems, and ensuring that the wiring in buildings and homes comply with code standards. After completing an apprenticeship, journeyman electricians are mandated to become licensed in most states or municipalities. The duration of apprenticeship varies by state, but generally about four to five years of prior experience is needed in order to take the licensing examination. The exams commonly assess electrical theory and general knowledge, in addition to knowledge of the National Electrical Code (NEC). Receiving certification is also a voluntary method for an electrician to differentiate him or herself as a skilled and experienced professional. The certifications offered vary by state and may be earned in numerous specializations, such as cable splicing as an example. The certification process in most cases involves three levels of competency:\n- An experience requirement\n- Passing a written exam\n- Passing a practical exam\nExamples of certifying organizations include the National Joint Apprenticeship and Training Committee (NJATC) as well as the National Institute for Certification in Engineering Technologies (NICET). It’s imperative that the electrician tech school that you select not only furnishes a strong educational foundation, but also helps prep you for passing any certification and licensing exams that you might be required to take in the future.\nEnrolling in Electrician Online Programs in Crystal Lake IL\nAn option that you may have contemplated is enrolling in an electrician online school to earn a degree or a certificate. Although online training programs have become more prevalent as a way of attending class without the need for travel, in this instance they are not entirely internet based. Just about all electrician schools require partial attendance on-campus to obtain practical hands-on training. But since the balance of the classes can be accessed online, distance learning can be a more practical alternative for individuals that have limited time for schooling. And as an added benefit many online training programs have a lower tuition cost compared to their on campus counterparts. Travelling expenses from Crystal Lake IL are also reduced and some of the study materials can be accessed on line as well. Each of these benefits can make online electrician trade schools more economical and accessible. And many are fully accredited, which we will deal with in our questions to ask checklist.\nThings to Ask Electrician Vocational Schools\nOnce you have made a decision to earn a certificate, diploma or degree, you can start to refine your training options. Because there are so many electrician trade and vocational schools in the Crystal Lake IL area, it’s important to have a checklist of criteria that each school must meet. The initial 2 that we talked about were location and tuition expense. If you have an interest in earning an online degree, then that needs to be an option that your final school offers. And even though all three qualifiers may be critical when making your selection, there are additional factors that need to be considered also. Below is a checklist of those added qualifications that you will need to assess prior to choosing an electrical vocational school.\nAccreditation. Many electrician vocational schools have attained either a regional or a national accreditation. They may receive Institutional Accreditation, which involves the school’s programs overall, or Programmatic Accreditation, which relates to an individual program, for instance electrical technology. Make sure that the Crystal Lake IL program and school are accredited by a U.S. Department of Education acknowledged accrediting organization, which includes the Accreditation Board for Engineering and Technology. In addition to helping guarantee that you acquire an excellent education, it can assist in acquiring financial aid or student loans, which are often not available for non-accredited schools. Furthermore, a number of states mandate that the electrician training course be accredited in order to be approved for licensing.\nHigh Completion and Placement Rates. Ask the electrician schools you are looking at what their completion rates are. The completion rate is the portion or percentage of students who enroll in and finish the course. A lower completion rate could indicate that students were unhappy with the program and quit. It could also mean that the instructors were not qualified to instruct the students. It’s also imperative that the schools have higher job placement rates. Older and/or more reputable schools may have a more extensive list of graduates, which may produce more contacts for the school to use for their apprenticeship and job placement programs. A high job placement rate will not only confirm that the school has an excellent reputation within the industry, but additionally that it has the network of contacts to help Crystal Lake IL graduates obtain apprenticeships or jobs.\nApprenticeship Programs. Most electrician training programs are taught together with an apprenticeship or an internship program. Those participating trade and vocational schools will help place you in an apprenticeship program within their network of electrician businesses or labor unions. Find out if the schools you are reviewing have working relationships with Crystal Lake IL area electricians or electrical contractors. An apprenticeship not only provides a valuable experience by providing practical training, but it also supplies employment opportunities and helps to form relationships in the regional electrician professional community.\nModern Facilities. Confirm that the campus facilities and the equipment that you will be instructed on are up-to-date and what you will be using on the job. If you are presently in an internship or an apprenticeship, consult with the electrical tech you are working with concerning what you should be looking for. If not, ask a local Crystal Lake IL electrical company if they can provide some suggestions. Additionally bear in mind that unless you are able to move, the school must be within driving distance of your Crystal Lake home. Remember that if you decide to enroll in an out-of-state school, besides the added relocation costs there can be higher tuition fees compared to in-state residents.\nSmaller Classes. It’s important that you get as much personalized instruction as possible, which can be challenging in bigger classes. Ask if you can monitor a couple of the classes so that you can observe how big they are and experience the interaction between teachers and students. Talk with several of the students and get their comments relating to class sizes and instruction. Last, speak with a few of the instructors and find out what their level of experience is and what degrees or certifications they have earned.\nFlexible Scheduling. Verify that the class schedules for the schools you are reviewing are flexible enough to handle your needs. If you can only attend classes in the evening or on weekends near Crystal Lake IL, verify that the schools you are considering provide those choices. If you can only attend part-time, be sure that the school you select offers part-time enrollment. Also, check out what the protocol is to make-up classes should you miss any because of work, illness or family issues.\nConsidering Attending an Electrician School near Crystal Lake IL?\nFollowing is a little bit of background information about the Crystal Lake IL area.\nCrystal Lake, Illinois\nCrystal Lake is a city in McHenry County in the U.S. state of Illinois. Named after a lake 1.6 miles (2.6 km) southwest of the city\'s downtown, Crystal Lake is 45 miles northwest of Chicago. The population was 40,743 in 2010. Crystal Lake is the largest city in McHenry County, part of the Chicago metropolitan area.\nThe City of Crystal Lake traces its origin to two separate communities which were established in the 1800s. Those communities were generally known as Nunda and Crystal Lake. In 1835, Ziba S. Beardsley had come to the shores of the lake and commented that the ""waters were as clear as crystal"", thereby giving the lake its name. Ziba Beardsley continued south to Naperville. In February 1836, the first white settlers, Beman and Polly Crandall and six of their ten children, came from New York State traveling to Crystal Lake in a covered wagon. Their original cabin was built in the vicinity of today\'s intersection of Virginia Street and Van Buren Street. Four of the Crandall children were born there. Najah Beardsley\'s family was the second to settle in the area; his grandson, William Beardsley, was the first white child born on the Crystal Lake prairie, on May 7, 1837.\nThe town was first known as Crystal Ville. It was changed to Crystal Lake sometime before 1840. The area known today as downtown Crystal Lake was first called Dearborn and later, Nunda, from an area in New York where many settlers originated. The village of Dearborn was founded in the 1850s after an extension of railroads through the area. The first train station was built in 1856, although it was pre-fabricated and shipped from Chicago on a flatcar. At that time, the main business district for the village of Crystal Lake was located on Virginia Street, about one mile (1.6 km) southwest of the railroad station. The railroad served to connect both the people and industries of Crystal Lake and Dearborn to Chicago and the rest of the country. Dearborn grew quickly due to this new rail connection.\nFind the Right Electrician Vocational School near Crystal Lake IL\nChoosing the ideal electrician training program will probably be the most critical decision you will make to begin your new trade. As we have discussed in this article, there are many things that you will need to examine and compare between the schools you are considering. It’s a prerequisite that any electrician training that you are reviewing includes a considerable amount of hands-on instruction. Classes should be small in size and every student should have their personal equipment to train with. Classroom teaching should offer a real-world context, and the training program should be up-to-date and in-line with industry standards. Training programs vary in length and the type of credential offered, so you will have to ascertain what length of program and credential will best fulfill your needs. Every training program offers unique possibilities for certification also. Probably The ideal approach to research your short list of schools is to visit each campus and speak with the students and instructors. Invest some time to sit in on some classes. Inspect the campus and facilities. Make certain that you are confident that the school you pick is the best one for you. With the proper training, hard work and commitment, the final outcome will be a new career as a professional electrician in Crystal Lake IL.\nOther Cool Cities in Illinois\nBusiness Results 1 - 10 of 257']"	['<urn:uuid:b69b5b6a-821c-41b2-98ba-71dbc52fe298>']	factoid	direct	concise-and-natural	distant-from-document	single-doc	expert	2025-05-12T20:46:45.703947	9	17	2208
99	tobacco addiction coping mechanisms mindfulness comparison	Tobacco use and mindfulness represent contrasting approaches to coping with stress. While tobacco provides immediate but temporary relief from stress and anxiety, costing as little as 10 rupees per packet in India, it is highly addictive and causes serious health problems including various types of cancer. In contrast, mindfulness offers a healthier approach - studies show that higher levels of mindfulness are associated with improved mood, lower stress levels, and reduced smoking urges, leading to better success rates in smoking cessation without the harmful health effects of tobacco use.	['Kamal Sonavane knew she’d pass out if she chewed smokeless tobacco one more time. It was a scorching April afternoon in the middle of another of India’s brutal heat waves, and with no job to go to, the farmworker had already chewed tobacco five times that day. “Even an addicted person avoids doing this in extreme heat because there’s a risk of fainting,” she says.\nYet Sonavane repeated the familiar ritual: adding the slaked lime to the tobacco leaves, then putting the mixture in her mouth. “I would have anyway collapsed, either because of the heat waves or the mounting stress,” she says, sitting in her two-room brick house in Bhadole in the Indian state of Maharashtra. Anxious about money, her lack of work, and the extreme heat, she turned to the tobacco once again.\nClimate change is making farming in Maharashtra harder. This in turn impacts day laborers, who are hired when agricultural help is needed. “Every few months, farmers report losses caused by heat waves or floods,” says community health worker Shubhangi Patil, who serves the Kolhapur district where Sonavane lives. When crops fail, earnings become more precarious, and farm laborers “resort to substance use to forget their problems,” says Patil. It’s a prevailing issue across the region, Patil says.\nIt’s also a phenomenon that isn’t limited to India—or to countries with predominantly low- and middle-income wages. Research from other regions has found groups responding to the pressures of climate change by increasing their consumption of alcohol and other substances, with potentially deleterious effects on their health.\nA landless farmworker in her mid-60s, Sonavane has been toiling in the fields of Kolhapur for over 25 years. A decade earlier, she says, she didn’t chew smokeless tobacco. “I despised it,” she says. “Today I can’t stay even a few hours without it.”\nThe weather, she says, started to get bad in western Maharashtra in 2019. “This region has seen two floods, unbearable heat, incessant rainfall, hail storms, and a drought,” all in the past three years, Sonavane says. Farmers have faced tremendous losses: 36 million hectares of sugarcane, onions, rice, and other crops lost over the past five years, according to Maharashtra’s department of agriculture. Farm workers are currently finding it difficult to get even eight days of work a month because crop damage is so common, Sonavane says.\nWith no resources for dealing with the stress of being out of work, Sonavane stumbled across the solution of soothing her anxiety with smokeless tobacco, which costs just 10 rupees ($0.12) a packet. Like cigarettes and vapes, chewing tobacco contains nicotine, a central nervous system stimulant. Users say it elevates their mood; improves concentration; and relieves anger, tension, and stress. “They desensitize grief, sadness, and negativity for a while,” says Kolhapur-based clinical psychologist Shalmali Ranmale Kakade, referring to tobacco and other commonly abused substances, such as alcohol.\nBut nicotine is also highly addictive, and in heavy tobacco users, those positive effects may simply be the consequences of staving off withdrawal. Repeatedly chewing tobacco also causes many types of cancer—including those of the mouth, oesophagus, stomach, and bladder.', 'Improve Smoking Abstinence with Mindfulness\nBy John M. de Castro, Ph.D.\n“mindfulness training (MT) may decouple the association between craving and smoking, thus facilitating smoking cessation.” – J. Kim Pemberthy\n“Tobacco use remains the single largest preventable cause of death and disease in the United States.” (Centers for Disease Control and Prevention). So, treating nicotine addiction and producing smoking cessation could greatly improve health. But smoking has proved devilishly difficult to treat. There are a wide variety of methods and strategies to quit smoking which are to only a very limited extent effective. According to the National Institutes of Health, about 40% of smokers who want to quit make a serious attempt to do so each year, but fewer than 5% actually succeed. Most people require three or four failed attempts before being successful.\nOne problem is that nicotine is one of the most addictive substances known and withdrawal from nicotine is very stressful, producing many physical and psychological problems, including negative emotional states and depression. In essence, the addict feels miserable without the nicotine. This promotes relapse to relieve the discomfort. Better methods to quit which can not only promote quitting but also prevent relapse are badly needed. Mindfulness practices have been found to be helpful in treating addictions, including nicotine addiction, and reducing the risk of relapse.\nIn today’s Research News article “Mechanisms linking mindfulness and early smoking abstinence: An ecological momentary assessment study.” (See summary below or view the full text of the study at: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6483850/), Spears and colleagues recruited adult smokers desiring to quit and provided them with “six smoking cessation counseling sessions (10–20 minutes each).” They completed questionnaires on demographics and mindfulness. They were also asked to complete assessments at baseline and for 4 weeks after cessation of smoking on a smartphone of positive and negative emotions, smoking urges, and expectancy of regulating emotions whenever they had an urge to smoke, smoked a cigarette, and 4 randomly selected times per day prompted on their smartphone.\nOf the participants 63% achieved smoking abstinence after treatment and 41% remained abstinent 7 days later. They found that on the day of smoking cessation and 7 days later, the higher the levels of mindfulness the lower the levels of negative emotions, stress, smoking urges, and expectancies that smoking would improve mood and the higher the levels of positive emotions. They also found that lower smoking urges were associated with higher levels of abstinence when the participants were low in mindfulness but not when they were high in mindfulness. In addition, the higher the levels of mindfulness, the higher the levels of positive emotions and the lower the levels of negative emotions which were in turn associated with higher levels of smoking abstinence.\nEven though the smoking cessation therapy did not include a mindfulness component, the participant’s level of mindfulness was an important contributor to successful smoking abstinence. Mindfulness appeared to be associated with improved mood and lower stress levels and urges to smoke which were in turn associated with improved outcomes. Hence, mood and stress appear to mediate the association of mindfulness with successfully quitting smoking. Mindful people appear to have more positive moods which works to improve the likelihood of quitting smoking.\nSo, improve smoking abstinence with mindfulness.\n“stress is caused by craving. If you can let go of that craving, then your stress will dissolve, and practicing mindfulness is the way to do that.” – Judson Brewer\nCMCS – Center for Mindfulness and Contemplative Studies\nThis and other Contemplative Studies posts are also available on Google+ https://plus.google.com/106784388191201299496/posts and on Twitter @MindfulResearch\nSpears, C. A., Li, L., Wu, C., Vinci, C., Heppner, W. L., Hoover, D. S., Lam, C., & Wetter, D. W. (2019). Mechanisms linking mindfulness and early smoking abstinence: An ecological momentary assessment study. Psychology of addictive behaviors : journal of the Society of Psychologists in Addictive Behaviors, 33(3), 197–207. https://doi.org/10.1037/adb0000451\nResearch suggests that individuals with greater dispositional mindfulness (i.e., non-judgmental, present-focused attention) are more likely to quit smoking, but the underlying mechanisms are unclear. This study investigated mechanisms linking mindfulness and early smoking abstinence using ecological momentary assessment (EMA). Participants were 355 smokers (33% Caucasian, 33% African American, 32% Latino; 55% female) receiving smoking cessation treatment. Mindfulness was assessed at baseline and on the quit date. For 4 days pre-quit and 1 week post-quit, participants completed up to 4 EMAs per day indicating levels of negative affect (NA), positive affect (PA), smoking urges, and affect regulation expectancies. Mean, slope, and volatility were calculated for each pre-quit and post-quit EMA variable. Associations among mindfulness, EMA parameters, and abstinence on the quit day and 7 days post-quit, as well as indirect effects of mindfulness on abstinence through EMA parameters were examined. Mindfulness predicted higher odds of abstinence in unadjusted but not covariate-adjusted models. Mindfulness predicted lower NA, higher PA, and lower affective volatility. Lower stress mediated the association between mindfulness and quit-day abstinence. Higher ratings of happy and relaxed, and lower ratings of bored, sad and angry, mediated the association between mindfulness and post-quit abstinence. Mindfulness appeared to weaken the association between craving and post-quit abstinence. This study elucidates real-time, real-life mechanisms underlying dispositional mindfulness and smoking abstinence. During the early process of quitting smoking, more mindful individuals appear to have more favorable emotional profiles, which predicts higher likelihood of achieving abstinence 1 week after the quit date.']	['<urn:uuid:69355e7a-62e4-4d4a-9f24-6e60ca93ce38>', '<urn:uuid:2ee718fa-b424-4229-9481-bbc8619758be>']	open-ended	direct	short-search-query	distant-from-document	comparison	novice	2025-05-12T20:46:45.703947	6	89	1408
100	What are some common pesticides used in gardens for pest control, and what are the health risks they pose to children and pets?	Several pesticides are commonly used in gardens: Pyrethrin from chrysanthemums provides quick but short-lasting control, Permethrin is useful for orchards, Carbaryl is effective against many insects, Malathion controls a wide range of insects, and Spinosad is derived from soil bacteria. However, these pesticides can pose serious health risks. Children are particularly vulnerable due to their developing bodies and nervous systems - in 2009, 45% of pesticide-related incidents involved children under six. Pets are also at risk, as they can pick up pesticide residues on their paws and fur, potentially ingesting them through licking. Some products, like slug bait containing metaldehyde, can be particularly dangerous to dogs and may even be fatal.	['Do you know what you get when you throw two peas? Black-eyed peas.\nEven if plants can’t fight, there is a lot of stress from insects and diseases around the yard and in the garden. In this hot and dry year, the insects are completely gone. Diseases are more common during years of high humidity.\nWhen bugs chew holes in leaves, and sparks threaten to take our plants down, most of us need medicine. Standing in the hallway in the garden or at a hardware store can be confusing, as most shelves are full of many products. Which to choose?\nFortunately, if we know a few products, most of them can be used to treat pests and diseases. Having multiple products in hand makes early treatment easier.\nThe following are some of the products that control most garden and garden plants and diseases.\n- Most diseases around the yard and garden are caused by fungi. It is the most common active ingredient in antifungal drugs Chlorotalone, Can be found in fine print on labels. Many of the products listed for the control of vegetables, flowers, and fruits contain the substance, and some mix it with pesticides and spray it as rose petals or fruit trees.\n- Copper, One of the oldest fungicides, is often used where organic matter is needed.\n- It is important to note that fungicides are very effective as a preventative, and any of these products should be applied early to prevent the spread of the disease to healthy leaves. Always read and follow account instructions.\nCavities caused by cabbage worms can be protected by pesticides. David Samson / The Forum\n- Pyrethrin: One of the first insects is a chrysanthemum. Although it hits fast insects, pyrethrin breaks down quickly, leaving little or no control. It is one of the most common organic insects.\n- Permethrin Sold by eight brands, this synthetic pesticide is composed of a chemical structure similar to pyrethrin, but has a longer shelf life. It is especially useful for orchards because it controls most major pests and has a short time between application and safe harvest.\n- Carbilil: Although new types of sesame seeds contain synthetic pyrite, this is an active ingredient in some products such as sesame seeds. It is effective against many garden and garden insects.\n- Malathion: First introduced in the 1950s, it controls a wide range of insects on a wide variety of plants. It is important to rotate the berry and fruit crops because the time difference between the application and the safe harvest is only one to three days depending on the crop.\n- Spinosad: A relatively new pesticide that is very successful in controlling many insects. It is derived from the bacteria found in the soil near the abandoned Roman factory and has been cleaned for many organic uses. It is a good choice for berry and fruit crops because of the short difference between application and safe harvest.\n- Bacillus touring; BT, BT, these bactericidal pesticides are an effective organic way to control caterpillars such as cabbage, broccoli and cauliflower. Caterpillars should be applied at a young age as they have little effect on large insects.\n- Pesticide soap, neem oil, vegetable oil – Relatively safe for healthy insects, these products can control soft body pests such as aphids, bedbugs, ticks, white flies, termites and scales. Because these products work when insects come in contact, effective application should cover both the upper and lower leaf surfaces and stems. Frequent applications are often necessary.\n- When using any of the pesticides listed above, always read and follow the label instructions to determine the type of insect that is controlled and the appropriate dose and method of application.\nLifelong gardener Don Kinzler is a horticulturalist with North Dakota State University Extension in Cass County. Readers can be reached at firstname.lastname@example.org.', 'The Grow Smart, Grow Safe® consumer guide to lawn and garden products is a project of Thurston County Environmental Health in Olympia, Washington. The information below comes directly from the Grow Smart, Grow Safe website.\nHow can we reduce the danger of pesticides?\nIncreasing the demand for safer products will encourage manufacturers to make products that are safer and more environmentally sound. You can have an attractive landscape without using hazardous pesticides. Use effective non-chemical methods and less-hazardous products instead.\nLearn about pesticide ratings, how to choose safer products, and ways to care for your lawn and garden naturally at growsmartgrowsafe.org.\nPesticides, human health and the environment\nPesticides include some of the most hazardous chemicals commonly used in and around the home. Products that kill insects, weeds, bacteria and fungi may be hazardous to children, pets, birds, fish, other wildlife, and also to beneficial insects like bees and lady bugs. Pesticides used in the yard or at the workplace can be carried inside on shoes and work clothes and mix with house dust. Young children, who crawl on the ground and put objects in their mouths, can then ingest the chemicals. Rain and irrigation wash pesticides off of yards and carry them to streams, sometimes in amounts that can harm salmon or the aquatic organisms that are their food.\nPesticides and children\nIn 2009, United States poison centers had over 90,000 incidents related to acute exposures to pesticides - about 45% involved children under the age of six. Children are vulnerable to repeated, very small, unintended exposure to pesticides. Per pound of body weight, pesticides have a greater effect on children’s growing bodies and developing nervous and hormonal systems and organs.\nDanger to pets\nMany pesticide products are toxic to dogs, cats and other pets. The risks can be similar to the human health risks. Pets with access to treated landscapes may pick up pesticide residues on their paws and fur, licking it or tracking it into the house. Slug bait containing metaldehyde poses a special risk because dogs are attracted to it and may eat enough to be seriously injured or even die.\nToxicity to birds and bees\nMost insecticides are toxic to beneficial insects, bees and other pollinators, and some insecticides are toxic to birds. Insecticides can kill bees directly when they eat or even just land on treated plants. Foraging bees can carry pesticides back to their hives, threatening the entire colony. Some researchers believe that pesticide use may be implicated in honey bee colony collapse disorder.\nMany insecticides and herbicides are toxic to fish, amphibians and other aquatic organisms. Aquatic organisms come into contact with pesticides and fertilizers through irrigation or stormwater runoff from yards and other landscapes. Pesticide ingredients that don’t bind well to soil and are persistent in the environment pose the greatest risk to water pollution. Monitoring studies in the Puget Sound region have found common pesticides and fertilizer nutrients in many of our waterways.\nThe longer a pesticide remains in the environment, the more likely it is to do damage. Older products, like DDT, are still in the environment and in our bodies almost 40 years after their last uses were banned. Many modern pesticides breakdown faster, however most pesticides are not gone after a month or two, and may not be harmless just because the treated area has dried. Some pesticide residues may remain in soil, sediment or water from weeks to over a year or more, and sometimes they breakdown to chemicals that are toxic and persistent.\nHow do I know which products are safer?\nLearn how to read a pesticide label, which chemicals are lowest risk, how to handle and dispose of pesticides, and how to avoid the need for pesticides altogether at www.growsmartgrowsafe.org.']	['<urn:uuid:12591cc1-8b01-4333-a01b-2b8b3ac03816>', '<urn:uuid:ecd62e19-7b14-4024-93f6-ba091430d7c5>']	open-ended	direct	verbose-and-natural	similar-to-document	multi-aspect	novice	2025-05-12T20:46:45.703947	23	111	1268
