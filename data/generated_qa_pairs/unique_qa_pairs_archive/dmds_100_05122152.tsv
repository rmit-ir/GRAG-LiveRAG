qid	question	answer	context	document_ids	question_factuality	question_premise	question_phrasing	question_linguistic_variation	question_multi-doc	user_expertise-categorization	generation_timestamp	question_length	answer_length	context_length
1	empathy evolutionary purpose compassion development connection	Both empathy and compassion have evolved as crucial mechanisms for human and mammalian survival, serving complementary but distinct purposes. Empathy developed as a way to connect and understand others' experiences through multiple elements including cognitive, emotional, and kinesthetic components. Meanwhile, compassion evolved as an innate aptitude in mammalian brains specifically to facilitate survival-oriented connections with others. This evolutionary development is particularly evident in how compassion motivates care for vulnerable members of a group or society, beginning with infant care that establishes trust and mutuality in relationships. While empathy enables understanding and connection, compassion's evolutionary purpose extends to actively promoting helping behaviors and survival-oriented actions.	"['An Overview of Integrative Empathy\nEmpathy consists of at least five elements, with corresponding practices.\nPosted March 25, 2022 | Reviewed by Gary Drevitch\n- Empathy has many definitions and even within one field of study they’re far from consistent.\n- We consider empathy as holding space for others as they are.\n- We identify five distinct but interconnected elements of empathy: self-, kinesthetic, reflective, imaginative empathy, and empathic creativity.\nWhen we ask people what empathy is, a common response is “to put yourself into someone’s shoes."" We also hear that it involves knowing about the thoughts and feelings of others. However, in business, we detect an aversion to emotions. Understanding others: great; feeling with others: meh…\nIn short, empathy is holding space for another person to express themselves, as they are. It allows us to connect, feel, and understand the experiences of others. However, empathy has many definitions and even within one field of study they’re far from consistent. With increasing interest, empathy has become an umbrella term for many processes of shared experiences.\nScience now distinguishes three forms of empathy: cognitive empathy, emotional empathy, and empathic concern. Cognitive empathy is understanding another’s point of view. Emotional empathy is the ability to feel what someone else is feeling. Empathic concern is a response to the distress of another person which is consistent with the distress you perceive them to be in. These definitions, however, are influenced by current trends and limits in psychology and social neuroscience. Empathy is explored in other disciplines too.\nIn our practice, we combine knowledge from different disciplines into an integrated definition of empathy. Our claim that empathy is holding a space for others, as they are, distinguishes it from empathy as intervening with others, no matter how well intended. Holding space is not about interjecting, nor bringing good intentions. Rather, it brings a neutral quality to your relationships. Integrating research and practice on empathy in psychology, philosophy, the arts, primatology, evolutionary biology, medicine, neuroscience, the humanities, and anthropology, we identify five distinct, but interconnected elements of empathy:\n- Kinesthetic empathy\n- Reflective empathy\n- Imaginative empathy\n- Empathic Creativity\nBe present with self-empathy\nUnderstanding others requires that we first understand ourselves. In self-empathy we notice and recognise what is happening in us and work with it in order to be present for others. We’re not talking about feeling sorry for, or caring for, ourselves. Self-empathy starts with observing ourselves in an empathic manner, becoming aware of our conscious and hidden agendas.\nIt helps us to take control of how we are in a situation, taking a moment to sense and make sense of what we feel and think, with an attitude of openness and suspended judgment. If we don’t do this, we are likely to succumb to various pitfalls. We might confuse empathy with sympathy or emotional contagion or project ourselves on others. Biases and preconceptions cause us to confuse our own experience with that of others.\nSelf-empathy is the ability to bring awareness to your own state in the moment and thus to distinguish your own experience from the experience of others\nThrough self-empathy we develop a sense of agency, the awareness of ourselves as the initiator of actions, desires, thoughts and feelings. It enables us to differentiate our own experience from the experience of someone else and prepares us to empathize with them.\nWorking with the ‘unsaid’ with kinesthetic empathy\nKinesthesia is one of our subtle senses. We sense our body’s position and movement with proprioceptors in the muscles and joints. In kinesthetic empathy, we connect with others by coordinating and synchronizing with their bodily expressions. We intuit someone\'s’ inner experiences through their outer bodily expressions.\nKinesthetic empathy is the ability to sense into somebody’s movement, or their sensory experience of movement via direct motor representation.\nKinesthetic empathy has two main purposes: First, to become aware of and explore how we influence each other’s physical space. It establishes embodied connection and synchronization through experience rather than cognition. Second, we modify our own bodily expressions to hold space for another person. How do you express the space you hold for someone? How will they notice?\nPresence with reflective empathy\nHave you noticed, when listening to someone, how quickly you are inclined to share your own story or insights? Or have you felt yourself cut off by someone giving you unwanted advice? This happens because your listener listened to react. In empathic listening we don’t listen to react, but to reflect.\nIn reflective empathy we apply empathic listening skills to stay present and ensure we understand the person we listen to. Empathic listening is an iterative process of listening and reflecting back what we hear. The speaker hears an echo of his expressions, offered in a way that invites them to amend or reject what is reflected. If the reflection is a faithful representation of what the speaker had in mind, then mutual understanding is achieved.\nReflective empathy is the ability to actively listen and reflect back what you hear, including factual information and felt meaning.\nIn empathic listening, we listen to the content and the ‘felt meaning’ of a speaker. The content is expressed in words and the felt meaning, or subjective experience, is communicated through intonation, body language, and what is not being said.\nValuing perspectives with imaginative empathy\nImaginative empathy uses as-if acting and embodied imagination to gain insight into another’s experience. Using self-empathy, active inquiry, and wonder, we embody another person’s perspective in order to gain an experience of their mental life. This ‘as-if’ experience is not their experience, as we cannot inherently know the experience of others. Yet, the practice of imaginative empathy guides us to understand and value other perspectives.\nImaginative Empathy is the ability to gain insight into the experiences of others through embodied imagination, stepping in and out of perspectives.\nWhen we identify with what we believe, thinking we\'re right, we miss out on understanding other perspectives. Cultivating detached observation in the midst of our actions and interactions is vitally important. We have to drop the way we see things to really try to see it the way others do so that we open up to novel perspectives and experiences.\nFrom inspiration to action with empathic creativity\nPractising empathy sparks empathic creativity. It is not a practice of empathy itself but a creative consequence of empathy practices. Empathic creativity can be an insight, a discovery, or a creative outburst. It can show itself as inspiration to try new behaviours or to find solutions to problems. It can also be expressed in generosity: offering financial support, ideas or time; or as a generosity of heart: offering protection, emotional support and love to others.\nEmpathic creativity starts with an intention or plan to take action. The art of empathic creativity is to notice when creativity is sparked, take note of emerging intentions and translate them into actionable outcomes.\nEmpathic Creativity is the ability to notice significant change events and to convert them to actionable outcomes\nResearch shows that empathy training can be highly effective. Yet in order to master it, we need practices. Empathy is not learned through ‘understanding’ what it means but by practising specific skills, preferably in real life. And once you’ve acquired the skills, it’s up to you to maintain them. Empathy, after all, is not something that we are but something we do.\nHall, J. A., & Schwartz, R. (2019). Empathy present and future. Journal of Social Psychology, 159, 225-243.\nMoore, J. W. (2016). What is the sense of agency and why does it matter?. Frontiers in psychology, 7, 1272.\nTeding van Berkhout, E., & Malouff, J. M. (2016). The efficacy of empathy training: A meta-analysis of randomized controlled trials. Journal of counseling psychology, 63(1), 32.', 'by Ann Quinlan-Colwell, PhD, RNC, AHNBC, DAAPM\nTwenty years ago, when she read my holistic tee shirt with the numbers “2 + 2 = 5” the young store clerk smiled and confided “I wasn’t good at math either.” That tee shirt exemplified the holistic concept: the whole is greater than the sum of the parts. That holistic concept is particularly true when working with people with pain management. Historically, the interactions between health care providers and patients involved a therapeutic component that enhanced the beneficial effect of the medications and treatments which were provided (Decety & Fotopoulou, 2015). Today, when they sense empathy and compassion from clinicians, patients continue to report greater satisfaction, less anxiety, and feeling safer, with improved outcomes demonstrated (Dempsey, 2018; Quinlan-Colwell, 2009; Vogus & McClelland, 2016).\nThe term and concept of compassion are discussed frequently in health care. Although no definition for the word compassion is provided in the 10th Edition of the Mosby Medical Dictionary, in that resource, compassion is listed as a component of “ego strength”, “emotional support”, “nursing assessment”, being “sympathetic”, and “waking imagined analgesia” (envisioning and focusing on an enjoyable past experience). Nursing theorist Delores Krieger teaches that compassion is not only essential for working with someone using the energetic modality of Therapeutic Touch®, but that without compassion Therapeutic Touch® is not possible.\nAlthough compassion clearly is important in health care, it is often poorly understood or even misunderstood. To effectively discuss compassion, it is essential to understand what it is and what it is not. As with pain, compassion is multifaceted with various components and supported by a number of theoretical positions (Goetz, Keltner, & Simon-Thomas, 2010). The Merriam Webster Dictionary defines compassion as “sympathetic consciousness of others’ distress together with a desire to alleviate it” (N.A., 2018). Merriam Webster also defines sympathy as “feelings of pity and sorrow for someone else’s misfortune,” and defines empathy as “the ability to understand and share the feelings of another.” With all due respect to Merriam Webster, considering those definitions, I suggest that compassion is not “sympathetic consciousness of other’s distress,” but rather it is conscious awareness of another’s distress together with concern for the other and a desire to alleviate the distress. This definition is essentially consistent with Goetz and colleagues definition of “compassion as the feeling that arises in witnessing another’s suffering and motivates a subsequent desire to help“ (2010, p. 352).\nThus, with compassion there is not only a conscious awareness of the distress of another, but there is also caring about the pain and suffering of another combined with communication of that concern, with an impetus to ease the suffering and improve the status of the other. It is that desire and stimulus to help the other which especially distinguishes compassion from empathy (Chierchia & Singer, 2017). Thus, it is manifesting the emotion or experience of compassion into a therapeutic response/action which distinguishes compassion as a particularly unique trait or emotion different from other emotions.\nEven though it can be associated with and related to love, anguish or sadness, from an evolutionary perspective, compassion is a particular emotion or state unlike love, anguish or sadness (Goetz, Keltner, & Simon-Thomas, 2010; Kagan, 2014). It is an innate aptitude which has been refined in the brains of mammals to facilitate a particular connection with others for the purpose of survival (Decety & Fotopoulou, 2015; Goetz, Keltner, & Simon-Thomas, 2010; Kagan, 2014; Wright & Pendry, 2016). Compassion is not a quality unique to humans, which is well known by humans who live with animals. It is most likely the very rare person who lives with animal pets who has not experienced his or her pet acting with compassion when the owner was ill, injured, grieving or emotionally distressed. Compassion is the source and reason why the healthy and informed members of a group or society care for those who are vulnerable. Compassion then is first experienced with the care received as infants, and that compassion conveys senses of trust and mutuality which support meaningful relationships (Goetz, Keltner, & Simon-Thomas, 2010). In health care compassion not only facilitates, but is essential for healing, and was described by Delores Krieger as being the quality “that powers the engine of a healing relationship” (Hanley, Coppa, & Shields, 2017, p. 375).\nToday, that aged holistic tee shirt is a reminder that good health care and effective pain management are more than just writing a prescription or administering a pill or administering a therapy, or providing education. Compassion is a basic premise of health care and the reason for working with people living with pain, it is how 2 + 2 becomes 5. Too often patients living with pain feel undertreated, frustrated, labelled, and judged, while health care providers feel frustrated and burned out, and while health care system leaders feel they need to acquiesce to electronic medical record documentation, time constraints, government regulations and accreditation agencies. As a result, too often the whole seems even less than the sum of the parts (2 + 2 = 3) with medications and treatments not working as well as expected.\nAlthough the majority of patients and health care providers believe compassion is a critical component of health care, many believe it is often missing in U.S. health care today which led to the identification of a current compassion crisis (Trzeciak, Roberts, & Mazzarelli, 2017). It is curious to ponder whether it is just an interesting coincidence that this compassion crisis is co-occurring at the same time as the opioid crisis or if rather there is a correlation between the two crises. Today, compassion often seems to be the ghost of pain management begging to be re-actualized to enhance pain management and make the whole analgesic effect greater than the sum of the parts.\nThis is the first of a series of explorations of compassion in pan management. The next segment will be a discussion of compassion fatigue followed by developing self compassion and finally the importance of restoring compassion in the work of pain management.\nBurnell, L. (2009). Compassionate care: A concept analysis. Home Health Care Management & Practice, 21(5), 319-324.\nChierchia, G., & Singer, T. (2017). The Neuroscience of Compassion and Empathy and Their Link to Prosocial Motivation and Behavior. In Decision Neuroscience (pp. 247-257).\nDecety, J., & Cowell, J. M. (2014). The complex relation between morality and empathy. Trends in Cognitive Sciences, 18(7), 337-339.\nDecety, J., & Fotopoulou, A. (2015). Why empathy has a beneficial impact on others in medicine: unifying theories. Frontiers in Behavioral Neuroscience, 8, 457.\nDempsey, C. (2018). The Antidote to Suffering: How Compassionate and Connected Care Can Improve Safety, Quality, and Experience. NY, NY: McGraw Hill.\nGoetz, J. L., Keltner, D., & Simon-Thomas, E. (2010). Compassion: an evolutionary analysis and empirical review. Psychological bulletin, 136(3), 351-374\nHanley, M. A., Coppa, D., & Shields, D. (2017). A Practice-Based Theory of Healing Through Therapeutic Touch: Advancing Holistic Nursing Practice. Journal of Holistic Nursing, 35(4), 369-381.\nKagan, S. H. (2014). Compassion. Geriatric Nursing, 35(1), 69-70.\nNo Author (N.A.). (2018). Compassion. Merriam-Webster Dictionary. https://www.merriam-webster.com/dictionary/compassion. Retrieved\nQuinlan-Colwell, A. D. (2009). Understanding the paradox of patient pain and patient satisfaction. Journal of Holistic Nursing, 27(3), 177-182.\nTrzeciak, S., Roberts, B. W., & Mazzarelli, A. J. (2017). Compassionomics: Hypothesis and experimental approach. Medical hypotheses, 107, 92-97.\nVogus, T. J., & McClelland, L. E. (2016). When the customer is the patient: Lessons from healthcare research on patient satisfaction and service quality ratings. Human Resource Management Review, 26(1), 37-49.\nWright, V., & Pendry, B. (2016). Compassion and its role in the clinical encounter–An argument for compassion training. Journal of Herbal Medicine, 6(4), 198-203.']"	['<urn:uuid:8864dfae-95c9-4e8f-ab59-e9beb3588fd2>', '<urn:uuid:8d09b922-7218-43f2-8a8b-b4acd1410c27>']	open-ended	with-premise	short-search-query	similar-to-document	multi-aspect	expert	2025-05-12T21:52:51.670698	6	104	2562
2	As a field manager, I'm curious how Foamstream and Roundup timings differ?	Foamstream and Roundup have different timing requirements and effects. Foamstream uses hot water and biodegradable foam, with weeds dying away in 2-3 weeks after application. Roundup, on the other hand, shows initial results within 2-6 hours as plants begin to yellow and wilt, though complete root system death takes up to 2 weeks. Roundup must be applied when temperatures are between 60-85°F, early in the day, with no rain expected for 6 hours.	"[""- How to report overgrown weeds on the highway\n- 14 June: Trial of chemical-free weed treatment\n- Who controls weeds?\n- Where weeds are controlled\n- When weeds are controlled\n- What is a weed?\n- Why we control weeds\n- How we control weeds\n- Weeds and legislation\nHow to report overgrown weeds on the highway\nTo report an issue with weeds in Tandridge or Mole Valley, please use our online form (also for trees and hedges):\nFor weed control issues in all other areas, please contact your local district or borough council using the links found below:\n14 June: Trial of chemical-free weed treatment\nA week-long trial of a non-toxic, chemical-free weed control solution will start on Monday 14 June 2021. The trial is being carried out along the kerbs of roads in Dormansland, Tandridge.\nThe treatment is called Foamstream and it kills unwanted vegetation by using a precise application of hot water insulated by a specially formulated biodegradable and organic foam. The foam dissipates within a short time period and the weeds, once coated, should die away in around 2 to 3 weeks, as with traditional chemical-based treatments. Weedingtech's website has more information on Foamstream.\nThe works are being carried out by Wealdens, one of Surrey's approved Countryside contractors, under the supervision of the Local Area Highway Office.\nWho controls weeds on the highway?\nIn most areas across Surrey (except Tandridge and Mole Valley), the district or borough council treats the weeds on our behalf.\nTo find out more information about weed treatment in your area, or to make any enquiries, please contact your local district or borough council using the links found in the section at the top of this page.\nWhere weeds are controlled\nAll pavements and kerbs in residential roads are treated to control weeds. The edges of paths immediately adjacent to walls or buildings are also treated when necessary.\nWhen weeds are controlled\nMost residential roads in Surrey are treated once a year but there may be local variation. The sprays are timed to coincide with the weed growth for maximum control.\nWhat is a weed?\nA weed is a plant growing in a location where it is not wanted. On our highways, any plants growing in pavements and kerbs or around drains and street furniture, are weeds.\nWhy we control weeds\nWeeds are controlled for the following reasons:\n- Safety - weed growth can interfere with visibility for road users and obscure traffic signs. Weeds in kerbs or around drains can prevent or slow down drainage. Their growth on pavements may damage the surface causing broken and uneven slabs.\n- Structure - weed growth can destroy paving surfaces, force kerbs apart and crack walls, causing safety issues and greatly increasing our maintenance costs.\nHow we control weeds\nWeeds are controlled using environmentally friendly and effective herbicides. When the herbicide is applied to a weed, usually by spraying, it works its way through the plant killing it completely. On contact with soil the herbicide breaks down into harmless substances.\nThe herbicides used in Surrey have a very low toxicity to humans, animals and insects and can be used in areas open to the public and their pets. In areas close to water courses and reservoirs, herbicides are not used.\nWe are committed to exploring alternatives methods of weed control over the coming years. We regularly consult with independent experts for advice on weed control and related issues, to ensure that we are fully up-to-date with changes in legislation, herbicide recommendations and commercial practice.\nWeeds and legislation\nThere are five weeds listed in the Weeds Act 1959, Spear Thistle, Creeping or Field Thistle, Curled Dock, Broad Leaved Dock and Common ragwort. Surrey County Council will remove ragwort on highway verges where there is, in our view, a high risk of ragwort spreading to land used for the grazing of horses, other grazing animals, or for the production of animal feed.\nUnder section 14 of the Wildlife and Countryside Act 1981, it can be an offence to plant or grow certain specified plants in the wild, including Giant Hogweed and Japanese Knotweed. There is no statutory requirement for landowners to remove these plants from their property but it is an offence to allow them to spread to adjacent land."", 'When you use a weed killer spray, you want results. However, when taking out hardy dandelions and resilient ground ivy, it can be difficult to know if the weed killer you’ve used is working. How can you tell if it’s time to re-apply weed killers? How long should you wait for results after spraying for weeds? How long does it take for weed killer to work?\nTo choose the best weed killer for your lawn, it’s important to know about the different weed killers on the market and their effects. In this article, we’ll discuss different types of weed killers, the best time to spray for weeds, and how long it will take before you see results.\nTable of Contents\nHow Does Weed Killer Work?\nIf your lawn is all weeds, you may need to use more than one type of weed killer in order to kill all the different weed species you’re dealing with. The important questions that help determine how to use weed killers are What, When, and Where:\n- What types of weeds are you killing? Are you battling broadleaf weeds or grassy weeds? Annuals or perennials?\n- When are you beginning your weed-killing efforts? Season matters in the weed-killing game. For instance, pre-emergent weed-killers are only effective if they are applied before seeds sprout.\n- Where are the weeds you’re killing located? Are you tackling weeds in your lawn, your garden, or pesky weeds coming up through seams in a concrete driveway? You may choose different products based on these locations.\nTypes of Weed Killers\nRoundup and other Contact Weed Killers\nRoundup makes some of the strongest and fastest-acting weed killers on the market. They are often non-selective weed killers that kill emergent weeds.\nAdditionally, some long-term weed control Roundup formulations contain pre-emergent weed control measures. These specialized formulations even prevent plants from sprouting in the area of application for up to one year.\nPre-Emergent Weed Killers\nPre-emergent weed killers are designed to stop weeds in their infancy. They work by killing seedlings, which prevents new weeds and grasses from sprouting up in your yard. Pre-emergent herbicides won’t kill established or mature plants.\nPre-emergent weed killers can be applied anywhere in your lawn and garden, but they don’t discriminate. These weed killers will stop the growth of any grass seeds or other plants. Don’t apply to any areas with any desirable seedlings or any areas that you plan to seed within the next 3 weeks.\nEmergent Weed Killers\nEmergent weed killers target established weeds, killing mature plants. They will shrivel up dandelions and kill clover. Keep in mind, weed seeds in the soil won’t be affected by an emergent weed killer.\nNote: The types of weeds targeted by spraying weeds with emergent weed killer depend on the active ingredients included in the formula. Be sure to read the product description to find out if you are using a non-selective or selective weed killer, as well as how to use weed killer.\nNon-Selective Weed Killers\nTypically, the fastest weed killers are non-selective weed killers. These weed killers will kill broadleaf weeds, woody plants, and grassy weeds, along with desirable grasses. So, while these will take out your enemies, they’ll also kill any other plants they touch.\nApply non-selective weed killers only in areas that will not impact desirable plants or grasses. These weed killers are best for dandelions, ivy, or isolated tufts of pest grass. They are not viable solutions when weeds are mixed in with grass, such as with some clover infestations.\nSelective Weed Killers\nSelective weed killers are designed to kill weeds and leave turf grass unharmed. Sounds pretty perfect, doesn’t it? This way, you get to kill the pests and keep what you want.\nSelective weed killers are most commonly found as broadleaf weed killers. These will target dandelions, chickweed, and clover while leaving grass unharmed. Even better, there are selective weed-killing formulations that kill broadleaf weeds AND other pest grasses, such as crabgrass. There are also selective weed killers designed specifically to kill sedge and nutgrass.\nNote: Check all product information to verify the weed killer you use is safe for your type of grass.\nWhen to Apply Weed Killers\nTiming is everything when it comes to killing weeds. You can have the right product and tools, but if you apply the weed killer at the wrong time it can have a diminished effect or no effect at all. For instance, excessively cold or hot weather can hinder the effectiveness of weed killer.\nWhen to apply weed killer to a lawn depends on the product you’re using and on the climate you live in. If you want to see results, follow these simple guides when you spray for weeds in your lawn.\n- Roundup and other Contact Weed Killers: Contact weed killers are any sprays designed to kill weeds quickly. There are some great tips for how to use Roundup and similar weed killers.\n- Spray Roundup when temperatures are between 60°and 85° Fahrenheit to ensure maximum effectiveness.\n- Apply Roundup early in the day so they are absorbed by the leaves during daytime photosynthesis.\n- Do not apply Roundup if rain is expected within the next 6 hours. Rain can wash away the active ingredients before they completely infiltrate the plant’s system.\n- Don’t apply Roundup on an excessively windy day. Wind can cause Roundup to dry too quickly, which hinders it from entering the plant’s leaves.\n- Pre-Emergent Weed Killer: Apply in spring (typically March/April) when soil temperatures reach 50° Fahrenheit for 3–5 days. This is when many seeds begin to sprout and is the effective window for pre-emergent weed killers.\n- Emergent Weed Killer: If you’re applying an emergent weed killer to an entire lawn, there are two times of year that provide the best results. The first time is late-May through late-June. The second is between mid-September and early-November.\nHow Long Does it Take for Weed Killer to Work?\nOnce you have chosen the correct weed killer for your lawn and decided how to use your weed killer, the good news is you should begin to see results soon. Some herbicides work extremely quickly, even causing weeds to wilt within a few hours after application. In contrast, some special formulations designed to control specific weeds may require a bit more patience.\nRoundup Weed Killers\nIf you’re asking, “how long does it take for Roundup to work?” you won’t have to wait long for an answer. Once standard Roundup has been sprayed on weeds, you will often see results within 2–6 hours. The plants you’ve sprayed will begin to yellow and wilt.\nKeep in mind that although the weed is already dying, it may take up to 2 weeks for Roundup to kill the weed’s entire root system.Also, it’s important to note that Roundup variations designed for long-term weed control act more slowly. These may not show results for up to 12 hours.\nBroadleaf Weed Killers\nIf you want to kill broadleaf weeds in your grass, you will usually end up using a selective weed killer designed to kill broadleaf weeds, or you may choose one of the best weed and feed products on the market. Weed and feed formulas often combine broadleaf herbicide with grass fertilizer, to get rid of weeds and improve grass health.\nWhether you’re using a weed and feed product or a herbicide-only solution, patience is necessary. After applying your weed control solution, you will typically begin to see weeds begin to die within 5–7 days. Most weeds should be completely eliminated 2–4 weeks after application.\nGrassy Weed Killers\nIf you’ve chosen a grassy weed killer with Glyphosate, a popular and powerful weed-killer used in Roundup and other products, you will see fast results in killing grasses—wilting within hours and complete death in 1–3 weeks. Fast results, but you have to be careful. Glyphosate is a non-selective herbicide that will kill any plant it is applied to.\nIf you choose to use a selective grassy weed killer, your results will vary based on the product and the type of grass you’re combatting. Different products work best to kill different grasses.\n- Crabgrass killers containing quinclorac kill weeds and grasses within 2–4 weeks.\n- Selective sedge and nutgrass killers such as Sedgehammer require 2–4 weeks to kill weeds completely.\n- If you are using Turflon Ester to control and kill bermudagrass, it may require repeat applications every 30 days throughout the growing season. It may take several months to see full results, as bermudagrass is very hardy.\nPre-Emergent Weed Killers\nPre-emergent weed killers work quickly. However, because they kill seeds just as they begin to sprout, this process is often invisible. Don’t worry though, pre-emergent solutions can begin killing weed seedlings mere hours after application.\nWhen late spring rolls around and you see no new tufts of crabgrass or sprouting chickweed, the pre-emergent weed killer has done its job. If you do see annual weeds returning to your lawn, this may be because the pre-emergent weed killer was applied too late or washed away by excessive rain.\nHow Long Does Weed Killer Stay in the Soil?\nIf you’re considering using one of the most popular weed removal solutions on the market, you may find yourself asking, “How long does Roundup last in the soil?” The answer depends on the specific formulation, but here are some general guidelines:\n- Weed killers containing Glyphosate (the active ingredient in standard Roundup) typically break down within 1–6 weeks after application.\n- Long-Term weed control options (such as Roundup’s Max Control 365) are specifically designed to stop weed and plant growth for up to a year. They remain in the soil for up to 12 months.\n- Selective weed killers such as broadleaf herbicides containing Quinclorac, MCPP, Dicamba, or 2,4 D typically remain in the soil for 2-4 weeks.\n- Pre-emergent weed killers remain active for 3 weeks.\nThere are other factors that determine how long weed killer remains in the soil. Climate and environmental factors play a large role. If you water frequently or if your climate experiences frequent rain, weed killers will be flushed from the soil more quickly. Conversely, in dry areas weed killer remains in the soil for a much longer period of time.\nWorking with Weed Killer for Fast Results\nWhen you’re using a weed killer, you want to see weeds die fast. There are several products that deliver quick results to kill weeds. Whether you choose a contact weed killer like Roundup to kill weeds today or a pre-emergent weed killer to prevent weeds from appearing in the future, you will see the greatest impact if the weed killer is applied at the right time and under the correct conditions.\nAlways make sure to read all product labels and warnings to avoid damage to desired plants or turf grasses. When you spray for weeds, test the spray first in a grassless area to make sure the spray pattern won’t reach any plants that should not be exposed to the weed killer.\nWhen you choose the right weed killer, you can truly see the results of your hard work. Sometimes, in just a few hours! Plus, it only gets better with time. After you’ve applied weed killer to your lawn, you’ll be amazed at how different your yard looks in just a few weeks.']"	['<urn:uuid:3ecd99c8-d167-4697-b3f2-cce345a0390e>', '<urn:uuid:3fa8deb7-d631-4b1c-8a04-5118f8e31597>']	open-ended	with-premise	concise-and-natural	distant-from-document	comparison	expert	2025-05-12T21:52:51.670698	12	73	2602
3	art education system africa how works	In Africa, the art education system operates differently from the European model. Rather than being centered in buildings or formal institutions, art education is based on activities and production. Learning happens through a system where some people teach and others learn, integrated into society. While some countries may have formal schools with different methods (French, Spanish), there is often a deficit of conventional art schools, but the teaching and learning continue through this activity-based system.	['We tracked down Abdellah Karroum in Paris, and asked him some questions about the Biennale Regard Benin 2012, of which he is the artistic director (more on that in yesterday’s post). Born in 1970 in Morocco, he is artistic director of L’appartement 22, an experimental space for exhibitions and residencies in Rabat, which he founded in 2002. He was associate curator for Dak’Art in 2006, a member of the Golden Lion Jury at the Venice Biennale in 2007, co-curator at the Gwangju Biennale in 2008, and curator of the main exhibition at the 2009 Marrakech Biennale (see images here). In 2011 he curated Working for Change, a project in the Rif Mountains and at the Venice Biennale. Earlier this year he was an associate curator at La Triennale in Paris.\nThe premise of the biennial, “Inventing the World: the Artist as Citizen” is very interesting to us. There are a number of different models of citizenship, according to neighbourhood, state continent, world. How does the biennial address these?\nThe notion of artist as citizen is inspired by the specific situation when the artist has to also create the context of visibility of the work. That means the artist works in the studio or just in his or her head. In the context where there’s a deficit of investment from the politician or the institution, the artist has to create the space for visibility of the work in order to share a project.\nThe notion of involvement or engagement of the artist in society is to also act as a citizen, to create this space beyond the artistic project as writing, or formulating an artistic vocabulary. It’s an idea, a form and also a container. The artist becomes a citizen when he or she has to also create this container, making it possible to share the work. In Benin, for example, we have met this example and situation where the artist has to create the conditions to share the work. It can be public space, it can be private space turned into public. The artist meets the context and not only defines the space, they transform existing spaces for encounters. It’s how they engage in citizenship, but it’s also how they can have this awareness of their responsibility as an artist. So the responsibility of the artist, or this awareness, makes them decide to engage some other forms of activity or activism, more than just formulating an artistic project … the artist has to also respond to existent powers of the institution.\nYou curated the Marrakech Biennale in 2009, can you talk about the difference between curating a biennial in your home city and, now, in Benin?\nThe two situations are completely different. The Biennale Benin is based on the artist’s initiative and campaigns by certain institutions.\nIn Marrakech, it was a business initiative. It wasn’t an institution. It’s private and it’s more connected to tourism. The content becomes a commission unlike in Benin, where the content is coming more from the ground and from what artists initiate already.\nThe approach I suggested to the curatorial delegation was this: to take what is already there [in Benin] and to amplify it. To take the artist-as-citizen initiative as the departure point and to make these initiatives more visible — to make the work not only presented in an exhibition space, but to make the activity of the exhibition fit into the existing work. There was no existing space before the artist’s initiative. When the work appears, it creates the space at the same time.\nAt the biennial in Benin this year, we transformed a supermarket as one of the venues and exhibition spaces. The continuity is in the artistic approach: the artist created the work, and showed and activated them in public spaces: sometimes at a crossroads, or in a market space. We transformed the supermarket into an exhibition. But these exhibition spaces are led by the artists, the activity defines the space.\nIn Marrakech it was completely different. You had a private initiative financing private production and showing them in existing spaces, even if it wasn’t a museum. It could have been a palace, a theatre. But still, it was an already existing space for culture.\nIn Benin, it’s based on transforming living spaces into encounter and exhibition spaces. For the activity of the work.\nAre there any tensions — difficult or productive — around the fact that some of the artists in the biennial work in the diaspora and some on the continent? Does the biennial programme address this difference?\nI didn’t see tension between artists or between projects. I think that when an artist works, they are usually responses to suggestions. They are not reactions to other works. It can be an artistic work in response to many realities, to history, to society. To political situations. Artists engage and invest. It can be formulating a different vocabulary.\nBut the tension is not between local and global. It’s not between the home or the diaspora. The tension is between the artist and society, between an investment of an artist and the deficit of investment from the state. It is between the openness of circulation or the open possibilities of people: for artists and non-artists to circulate.\nIn Africa, for example, if you look at Emeka Okereke and his group Invisible Borders, the tension they have is not with the other artists. It’s not between diaspora and local. It takes a critical political position with the borders.\nIs there an issue of legitimacy over who can assume the position of an African artist?\nLegitimacy today is a globalized concept. Every artist working on an issue concerning the world has legitimacy to be critical of the issue, to formulate a project. An artist from Venezuela working on the issue of global warming at the same time an artist from Ethiopia working on the same issue as concerns those in Canada or Benin. Even if the issues addressed are very localized to African countries, the issues addressed also concern America and Mexico.\nAre there responsibilities specific to the curator? Are they the same as those of the artist-as-citizen?\nIn Benin there was no given form for an exhibition — my responsibility was to highlight what was there already, as artistic projects for the artists, and to contribute to making the artwork visible. This is my responsibility, which means also to avoid the confusion between art-works and a new space, which is the editorial space.\nThe editorial space is the space which is able to keep the work active. Even after the studio and even after the street. Even after the production. The post-production cannot cancel the artistic project. That is my responsibility. That means the curator’s responsibility is to keep the initial character of the work. There is no curatorial form, there’s no traveling imagery. But there is a curatorial position that can be open enough to accompany artistic projects. So this what we tried to do in Benin. This is why we had this curatorial delegation I was the director, and my four colleagues worked on special projects based on production with local associations and artists with their encounters based on intellectual exchange and observing the local context.\nThe entire project of the curatorial delegation and the position of the curators in this case would give us the possibility of negotiation with this context. The goal was to really respect, to keep the artistic project intact.\nHow does your curation approach the question of representation — of the continent, of the diaspora?\nWhich Diaspora? Do you mean the Diaspora of my Portuguese artist who was in residency in Benin, working for six months in Brazil as a Portuguese Diaspora artist? Producing artwork in Latin America, addressing the impact of colonialism in Africa and producing work in Africa about ecology, about the relationship to the landscape, about the impact of the machines, the industry of the landscape — the work the artist produces concerns people in Benin or in Angola, but they are artists originally from Portugal. So they are of the Portuguese Diaspora working in Africa. Can we consider them members of the Portuguese Diaspora?\nAt the same time, you have artists like Meschac Gaba working in Amsterdam addressing an issue like travel, or voyage, with flags from different countries around the world. Do we consider him as part of the Benin Diaspora in the Netherlands, when he is addressing work to the audience in the Netherlands about a global issue? With this issue of Diaspora we have make it very relative, be careful using the term to apply it globally and fairly.\nDo you sense, within artists working on the continent, a North-South divide? Does he feel that certain regions are under-represented? If so, what are the reasons for this.\nThese two artists [Françoise Vincent and Elohim Feria] who did a project called “Qui va au nord, va au sud” who goes north who goes south, this is also about teaching the art work. Especially today when an artist is informed by what’s happening in every part of the world. Art offers issues that concern the entire world so this project means the artist is addressing or proposing ideas and work for all of the continent and countries. In Benin, we did not have this problem because the curators were from all continents. We had the curator from Europe, two from Africa including myself. And the curator from the Caribbean and Olivier Marboeuf coming from Guadeloupe and France. And we were in dialogue with professionals from different continents. From India too. So the thinking about the large scale exhibition, Biennale Benin, was thinking globally, but not in a non-space. From a specific position, localized positions from other continents.\nIs the biennial intended to revitalise art in the country, or to recognise what is already happening?\nI think Benin is a country where certain artists are very present in the international level. For example, Meshec Gaba, Dominique Zinkpe, who have been present in the art world for a long time, both in the market and in non-commercial spaces, like biennials. The Benin Biennale is a biennial made by the artists. More than 60 artists [around a quarter of all those involved] are from Benin. Benin is a country with hundreds of artists.\nHow does the relationship between pedagogy and pleasure exist for the artist-as-citizen? How do you map an ethical role onto an aesthetic idea?\nI don’t know if I can answer the question directly, but in the biennial we wanted to deal with the issue of education and we had an exhibition which dealt with encounters. One of the encounters, the subject of a three day discussion, was recovering an art school. To think about the conflict between the artist with a project and the artist as a teacher. Most of the artists we worked with especially in Benin had this dimension of pedagogy, but in the sense of not only sharing or proposing an aesthetic form, but also sharing a position as actual teachers.\nArt is a space for learning and teaching. In Africa, where there is a deficit of art schools — like in Benin there is no art school — it’s not important because the art school was there even before the art school in Europe. But the position of the artist as a teacher is not addressed in the same way as it is in Europe. The art school is not something framed in a global way. The schema is not the colonial one.\nFor example, in some countries in North Africa, you have some art schools in French, some in Spanish, and they have different methods of learning. In Africa, even in Sub-Saharan Africa, you have different schools. The art school is not a building, it is about activities and production. The continuity of this production depends on this system on what you call pedagogy. It’s inclusive of the people who are able to learn and others who are able to teach and understand and to continue. It’s there, it’s part of the society.\n* Questions by Shamira Muhammad and Orlando Reade.']	['<urn:uuid:095b5b39-5e40-4d5b-9da5-ad7213c7e938>']	open-ended	with-premise	short-search-query	similar-to-document	single-doc	novice	2025-05-12T21:52:51.670698	6	75	2026
4	ways greet celebrate lunar new year	In Vietnamese culture, people use several greeting phrases during Tết, such as 'Chúc mừng năm mới' (Happy New Year) and 'An khang thịnh vượng' (good health and prosperity). The Chinese have similar customs, with common greetings like '恭喜发财 Gong Xi Fa Cai' (Congratulations and be prosperous) and '新年快乐 Xin Nian Kuai Le' (Happy New Year). Both cultures emphasize visiting family and friends to exchange these greetings. In China, people specifically wake up on New Year's Day to extend greetings first to parents, then to grandparents and other relatives, neighbors, and friends.	['Happy New Year! And no, I’m not late in sending my wishes but right on time for Lunar New Year/ Chinese New Year or as we call it in Vietnam, Tết Nguyên Đán or just Tết for short. The last time I celebrated Tết in Vietnam was in 2008 (that’s 8 years ago!) Although the Vietnamese community in Canada celebrates Tết, the atmosphere just isn’t the same as it is back home, so I thought I could share with you how I remember Tết as a kid.\nThere are usually a few indicators signaling that Tết is approaching one or two months prior: the streets get filled with vendors selling pink peach blossoms and kumquat trees; the cold, humid and windy weather in Hanoi; and stores selling new year decorations. Just like malls in Canada put up lights and decorations in red and green, stores and malls in Vietnam will decorate in bright yellow, gold, and red colors: lanterns, new year wishes, chữ nôm calligraphy which is the old Vietnamese language, red envelopes and drawings of the animal for that year (2017 is the year of the Rooster – Đinh Dậu). You will mostly find these decorations in the Old Quarter of Hanoi, on Hàng Mã street. Here are a few tips I have for pedestrians in Hanoi.\nDecorations with new year wishes\nIn Hanoi, every household will rush to buy pink peach blossoms and/or kumquat trees to decorate their house. Some people would spend days hunting for the perfect pink peach blossom, with the proper shaping, green leaves, colour and amount of flowers blooming. The ideal peach blossom has both fully bloomed flowers and smaller buds, a deeper pink, a crooked trunk zigzagging towards the top with as many branches as possible, forming a round-shaped canopy: overall, it has to look like it is growing and expanding, symbolizing growth and prosperity. You don’t want a peach blossom where the top is small as it will look like your fortune recedes.\nThis is what an ideal pink peach blossom looks like, colour and shape. A tree like this can cost a few dozen millions of VND.\nColder and rainier winters will ruin peach blossoms as the flowers get damaged by the rain while warmer winters will make the flowers bloom faster and wilt by the time Tết arrives.\nThe pink peach blossom from my last Tết in Vietnam\nFor choosing a kumquat tree, people would look at the following aspects: its shape, the trunk, fruits, flowers, and leaves. The tree should have a natural shape with a trunk more or less straight, it has to have some green and some ripe kumquats but they can’t be too big as they will sag the branches, a few flowers or buds would be good and leaves must be green, big and glossy.\nSome ideal kumquat trees\nIn Southern Vietnam, since the climate is warmer, people don’t buy pink peach blossoms but rather yellow apricot blossoms. Kumquat trees are also available but not many people would have both in their house. I don’t blame them, it is quite a lot of work to search and maintain both festive trees at the same time.\nYellow apricot blossoms are the symbol of Tết for Southern Vietnamese\nThe holidays wouldn’t be the same without overplayed holidays music, wouldn’t it? Here are a few lovable songs that guarantee you will hate after the holidays or at least, that’s the case for me.\nFor some reason, ABBA is extremely popular in Vietnam so who doesn’t love combining ABBA and the holidays? The lyrics of the band’s ‘Happy New Year’ have nothing to do with Tết but when the song starts playing in stores, in malls, hotels, and on TV, you know the holidays are coming soon. And they will play it on repeat, every single day until you hate it. It’s as if the song has become the standard reminder to every Vietnamese to get ready for the holidays and its overuse is the equivalent of your neighbours/colleagues/families constantly asking you “Have you done your Christmas shopping?” Fun challenge: try to make it through the holidays season in Vietnam without coming across this song at least once a day.\nThere are also Vietnamese songs about the new year and there will be new music videos on TV made especially for the holidays. They might be different music videos but they actually all look the same: adorable children, all dressed up in traditional clothing, running around gardens or visiting their grandparents and singing about the new year; young men and/or women dressed in traditional clothing, walking gracefully through pink peach blossom gardens and singing about spring and sending their best wishes to everyone in the new year. Here is an example from ages ago. The general theme of holidays songs is about the beginning of spring and arrival of warmer weather, the new year bringing joy and fortune to families.\nAn old tradition: new year couplets\nMuch of Vietnamese vocabulary and expressions are Sino-Vietnamese (words or phrases borrowed from Chinese) and would often be used to form câu đối or couplets (poetry). One of the old traditions for Hanoians is to go to the Temple of Literature and ask for couplets for the new year to serve as decorations.\nAn example of a New Year couplet: “new year, new happiness, new wealth; more gift, more fortune, more peace”\nIn the old days of the former Vietnamese language (chữ nôm which resembles Chinese characters), literacy was considered a privilege for the rich and educated, and everyone had high regards for scholars. The Temple of Literature in Hanoi, the first university in Vietnam, is considered a symbol of knowledge and education and where you can find these ông đồ (or thầy đồ): old men with white beards, sitting at tables with a brush, an inkstone, writing couplets on red papers and giving them out to people. They were highly respected not only for their knowledge but also for the fact that they were teachers and were making education more accessible to commoners.\nThầy đồ traditionally have white beards, are dressed in black and write New Year couplets on red papers\nNowadays, the official Vietnamese language follows an alphabet system and the old Vietnamese language is considered an art. Thầy đồ are considered artists and you can find them at the Temple of Literature closer to the new year.\nFor those travelling in Vietnam during the holidays season, here are a few common wishes/phrases you can use to interact with Vietnamese people:\nChơi Tết = preparing for Tết or Sắm Tết = Tết shopping. The two phrases are interchangeable.\nChúc mừng năm mới = Happy new year. “Chúc” in Vietnamese means “wish you”. So you can literally say “wish you” and start listing all your good wishes to people. Vietnamese grammar is just simple like that 🙂\nAn khang thịnh vượng = Literally “good health” (an khang) and prosperity (thịnh vượng).\nVạn sự như ý = May all your wishes come true. “Vạn” means “a thousand”, “sự” means happenings/events, “như ý” means “as you want”.\nSức khoẻ dồi dào = Wishing you good health. “Sức khoẻ” means “health” and “dồi dào” means “plentiful”.\nTiền vào như nước = May the new year bring you good fortune. A literal translation would be “money coming in like water/flood”.\nHay ăn chóng lớn = eat lots and grow fast (for kids). Yes, we associate eating with growing up and not really growing fat.\nSống lâu trăm tuổi = literally translates to “may you live up to 100”, a general wish for longevity, usually kids addressing to their grandparents in the family. In Vietnam, you turn a year older on Tết and not on your birthday. We also count your lunar age which includes the 9 months you are in the womb. So all babies are one-year-old on the day they are born.\nApart from trees and other decorations like flowers, people would be going to the banks to get new bills of money. Instead of gift giving, we put new non-crumpled bills of 10k, 20k or 50k VND, then put them in bright red envelopes and give them to kids. We call this lì xì (lucky money) or the act of giving the money is called mừng tuổi, which translates to good wishes with the new age (since everyone ages in the new year).\nSome red glossy envelopes for lì xì\nAnd of course, we also prepare plenty of snacks and dishes for the holidays but I will elaborate on these in the next post since there’s so much to say about food.\nThe entire preparation leading up to Tết reminds me much of the atmosphere in November/December before Christmas: lights, decorations, overplayed holidays songs, buying trees and plants to decorate the house, etc. But more than objects and activities, it is the warm thought of seeing friends and family, and catching up with loved ones in the new year that make the holidays’ atmosphere the way it is, no matter which culture you are from 🙂', 'Top 10 Chinese Spring Festival Activities\nThe Spring Festival is the oldest and most eagerly anticipated festival for Chinese people. The festival traditionally begins on the 1st day of the first month in the Chinese lunar calendar and ends on the 15th day. It is a grand festival filled with a variety of special customs including setting off fireworks, giving lucky money, dragon and lion dances and so many other activities. Below are the top 10 most popular customs during Spring Festival.\n1. CCTV Spring Festival Gala\nThe CCTV New Year’s Gala is enjoyed at get-togethers on the eve of Chinese lunar New Year throughout the country. It has been the country’s most watched variety show since its first launch in 1983. Today watching the show has become traditional activity for many Chinese families. The gala gather the top performers in the country and features musical, dance, comedy, and drama performances.\n2. Staying up the Whole Night on New Year’s Eve\nThe custom of staying up all night on New Year’s Eve dates back to the Northern and Southern Dynasties (420-289). Legend has it that, long ago, there was an ugly and furious monster called “Nian” that would come down from the mountains to hunt people on New Year’s Eve. So people got together on this day, staying up and chatting, hoping for peaceful passage of the time. The custom symbolizes the warding off of all diseases and disasters, wishing for good luck in the New Year.\n3. Eating Jiao Zi (or dumplings)\nIn China, there is one saying goes: No dumplings, no Spring Festival. From this, we can see the significance of dumplings to this festival. During the first day of the New Year, Chinese people, especially in Northern China, usually eat Jiao Zi (or dumplings). The Chinese pronunciation of Jiao Zi means midnight or the end and the beginning of time. Also, the shape of the dumpling resembles gold ingot from ancient China. So people eat them and wish for money and treasures.\n4. Stick Chun Lian (or Spring Festival couplets)\nChun Lian is a special type of couplet used only during the Chinese New Year as part of its celebration. Chinese people usually use Chun Lian to express their good wishes. During the Spring Festival, every household will choose a couplet to affix to the door of the house to add the festival atmosphere. Chun Lian comprises of a couplet written on vertical strips of red paper in the best calligraphic style one can muster. The first (called upper) line is posted on the right side of the front door. The second (called lower) line on the left. Usually a third horizontal piece may be posted across and on top the door.\n5. Setting off firecrackers\nFirecrackers are a specialty of China, its early origin can trace back to 2000 years ago. In the evening of the Spring Festival Eve, Chinese people set off firecrackers, hoping to cast away any bad luck and welcome the better life in the next year.\n6. Giving New Year’s greetings\nWaking up on the first day of the Chinese New Year, people dress up. The first thing to do is to extend greetings to their parents and then to their grandparents and other relatives, neighbors and friends. The two most common greetings during the Chinese New Year are: “恭喜发财 Gong Xi Fa Cai (Congratulations and be prosperous) and“新年快乐 Xin Nian Kuai Le” (Happy New Year).\n7. Giving Lucky Money\nLuky Money is prepared for younger generations by elder generations. Usually the money is tucked into red envelope. During the New Year, red envelopes are typically given to the children and unmarried youths. The amount of money is usually a single note to avoid judging the amount inside before opening.\n8. Eating Niangao\nSouthern Chinese eat Niangao (a kind of cake made of glutinous rice flour) during the Spring Festival. It is considered good luck to eat Niangao during this time, because “Nian Gao” is a homonym for “higher year.”\n9. Dragon dance and lion dance\nDragon dance and lion Dance are traditional Chinese folk activities which are very popular around China during the Spring Festival. In China, people believe that Dragon is a symbol of fortune, nobility, bravery and especially power, and lion represents auspiciousness. All across the county, you’ll see dragon and lion dances everywhere – lending a cheery, festive atmosphere to the occasion.\n10. Pasting paper-cuts\nPaper-cuts are handicrafts made by cutting paper with scissors into different patterns. During the Spring Festival, Chinese people like to paste paper-cuts on windows, which not only help enhance the festive atmosphere, but also bring enjoyment to the people.']	['<urn:uuid:2666b21f-01ea-4e97-bf10-4809235ed3f3>', '<urn:uuid:1b7e344b-813f-48d1-b4c9-b79e598b2e8e>']	open-ended	direct	short-search-query	distant-from-document	three-doc	novice	2025-05-12T21:52:51.670698	6	91	2288
5	authentic native american carved figures manufacturing location authenticity check	Authentic kachina dolls are made only by Hopi artists, while authentic Inuit sculptures are made in the Canadian Arctic. To verify authenticity of Hopi kachinas, each artist should be identified by name and tribal affiliation. For Inuit sculptures, authentic pieces come with an official Igloo tag containing information about the artist, location, and year of creation.	"['American Indian Art --> American Indian Sculpture --> Hopi Kachina Dolls\nNative American Languages \xa0 Native American Tribes \xa0 What\'s new on our site today!\nThese uniquely Hopi artworks are called ""dolls,"" but that is a bit of a misnomer.\nKachinas (or katsinas) are actually stylized religious icons, meticulously carved from\ncottonwood root and painted to represent figures from Hopi mythology. For generations, these figures have been used to teach children about their religion;\nno Hopi child has ever teethed on a katsina or taken one to bed, and given their price, I doubt any non-Hopi child has either.\nIf you are looking for native-made dolls for children or as cultural collectibles, please visit our\nNative American dolls page.\nAuthentic kachina dolls are made only by Hopi artists. There are plenty of other tribes with their own figurine-carving traditions, but the kachina tradition is unique to the Hopi. When you see Navajo kachinas or Mexican kachinas, those are imitations of the real Hopi kachinas, just as much so as kachinas made by white people or imported from Korea are. Genuine kachina dolls are made by only a small number of Hopi carvers who have dedicated their lives to this art--it takes years of practice and religious study to master kachina carving. If you are looking to buy a katsina doll that was actually made by Hopi Indians--either because it\'s important to you to have the real thing or because you want to support the Hopis with your purchase--then here is our list of kachina-makers whose work is available online. If you have a website of Indian sculpture to add to this list, let us know. We gladly advertise any individual native artist or native-owned art store here free of charge, provided that all artworks were made by tribally recognized American Indian, Inuit, or First Nations artists.\nThis Hopi-owned art store offers a selection of kachina dolls from famous and lesser-known carvers.\nIntricate katsina dolls by Hopi carver Milton Howard. This store also sells Hopi jewelry and pottery.\nAmerican Indian Kachinas and Dolls|\nThis craft shop carries Hopi katsinas as well as dolls from various tribes.\n|Traditional Hopi Kachinas: A New Generation of Carvers||Hopi Kachina Dolls: With a Key to Their Identification||A Guide to Hopi Katsina Dolls||Kachinas: Spirit Beings of the Hopi|\n|A great introduction to kachina dolls and the artists who make them. Color photos.||Photographic encyclopedia of the different Hopi kachinas and their mythological representations.||Photographs and descriptions of contemporary and historic Hopi katsinas.||This book shows traditional kachina dance costumes, not the figurines. Good overview of Hopi spirituality and kachina dancing.|\nAbout us: This website belongs to Native Languages of the Americas, a non-profit organization dedicated to preserving and promoting endangered Native American languages. We are not artists ourselves, so if you are interested in buying some of the carvings featured on this page, please contact the artists directly. Though we have featured only authentic Hopi katsina dolls whose artists are identified by name, we haven\'t called the tribal offices to check up on any of them, and we only know a few of them personally. We also don\'t guarantee any of their products. This is not an exhaustive list of Hopi kachina makers--if you would like us to add your kachina carving site to this page, please contact us with your URL and tribal affiliation. We advertise any individual native artist or native-owned art business here free of charge. We do not link to kachinas which are not made by tribally recognized Hopi artists, so please do not ask us to. And finally, websites do occasionally expire and change hands, so use your common sense and this general rule of thumb: if the creator of each individual artwork is not identified by name and tribal affiliation, creator of each individual artwork is not identified by name and specific tribe, you are probably not looking at a genuine Native American kachina doll.\nBack to Southwest Indian Art\nBack to Native American Culture and History\nLanguage of the day: Micmac\nNative Americans Native American Names Tribal Tattoos\nWould you like to help support our organization\'s work with endangered American Indian languages?', ""Tips on The Best Ways To Purchase and Buy Authentic Canadian Inuit Art (Eskimo Art) Sculptures\nLots of visitors to Canada will be exposed to Inuit art (Eskimo art) sculptures while exploring the country. Because Inuit art has been getting more and more international exposure, individuals may be seeing this Canadian fine art form at galleries and museums located outside Canada too. Presuming that the intent is to obtain an authentic piece of Inuit art rather than a low-cost tourist replica, the question arises on how does one tell apart the real thing from the phonies?\nIt would be pretty frustrating to bring home a piece just to find out later on that it isn't authentic or perhaps made in Canada. If one is lucky enough to be traveling in the Canadian Arctic where the Inuit live and make their fantastic art work, then it can be safely presumed that any Inuit art piece bought from a regional northern store or directly from an Inuit carver would be genuine. One would have to be more careful in other places in Canada, especially in tourist areas where all sorts of other Canadian souvenirs such as t-shirts, hockey jerseys, postcards, essential chains, maple syrup, and other Native Canadian arts are offered.\nThe most safe places to shop for Inuit sculptures to make sure credibility are always the reliable galleries that specialize in Canadian Inuit art and Eskimo art. A few of these galleries have advertisements in the city tourist guides found in hotels.\nRespectable Inuit art galleries are also listed in Inuit Art Quarterly magazine which is dedicated completely to Inuit art. When one strolls into these galleries, one will see that there will be only Inuit art and perhaps Native art however none of the other normal tourist mementos such as postcards or tee shirts . The Inuit sculpture may be signed by the carver either in English or Inuit syllabics but not all genuine pieces are signed.\nSome of these Inuit art galleries also have websites so you might go shopping and purchase authentic Inuit art sculpture from house anywhere in the world. In addition to these street retail specialty galleries, there are now credible online galleries that likewise specialize in genuine Inuit art.\nSome traveler stores do carry authentic Inuit art as well as the other touristy keepsakes in order to cater to all types of travelers. When shopping at these kinds of stores, it is possible to differentiate the real pieces from the reproductions. Authentic Inuit sculpture is carved from stone and therefore must have some weight or mass to it. Stone is also cold to the touch. A reproduction made of plastic or resin from a mold will be much lighter in weight and will not be cold to the touch. A reproduction will sometimes have a company name on it such as Wolf Originals or Boma and will never feature an artist's signature. An genuine Inuit sculpture is a one of a kind piece of artwork and nothing else on the shop shelves will look exactly like it. If there are duplicates of a specific piece with exact details, the piece is not genuine. If a piece looks too best in detail with outright straight bottoms or sides, it is most likely not real. Naturally, if a piece includes a sticker indicating that is was made in an Asian nation, then it is clearly a phony. There will likewise be a substantial cost difference between genuine pieces and the imitations.\nThis can be a real gray area to those unfamiliar with genuine Inuit art. If a seller claims that such as piece is genuine, ask to see the official Igloo tag that comes with it which will have information on the artist, location where it was made and the year it was sculpted. The authentic pieces with the accompanying authorities Igloo tags will constantly be the highest priced and are normally kept in a separate (perhaps even locked) shelf within the store.\nSince Inuit art has actually been getting more and more worldwide exposure, people may be seeing this Canadian great art type at museums and galleries situated outside Canada too. If one is fortunate enough to be traveling in the Canadian Arctic where the Inuit live and make their fantastic art work, then it can be safely assumed that any Inuit art piece purchased from a local northern shop or straight from an Inuit carver would be genuine. Respectable Inuit art galleries are also noted in Inuit Art Quarterly magazine which is devoted completely to Inuit art. The Inuit sculpture Kurt Criter might be signed by the carver either in English or Inuit syllabics but not all genuine pieces are signed. Some of these Inuit view art galleries likewise have websites so you could go shopping and buy authentic Inuit art sculpture from home anywhere in the world.""]"	['<urn:uuid:41a08c91-2f54-4b6d-8a0c-10ace7321ef1>', '<urn:uuid:380337c6-3957-4c63-9583-ba486d4994c1>']	factoid	direct	long-search-query	distant-from-document	multi-aspect	novice	2025-05-12T21:52:51.670698	9	56	1500
6	knowledge transfer problems key obstacles what are main difficulties translating arabic metaphors and why cant electric planes work yet	Regarding Arabic translation difficulties, metaphors often don't work directly in German (and by extension other languages), requiring creative recasting. Additionally, religious and cultural references that are merely alluded to in Arabic need to be elaborated for non-Arabic readers to understand them. As for electric planes, the main obstacle is that current batteries don't store enough energy to power an aircraft over useful distances. While electric motors are more efficient than jet engines and can be better integrated into the fuselage, battery limitations in energy storage density make them too heavy for practical aircraft use. NASA is working on this problem with their X-57 prototype, but significant battery improvements are still needed.	"[""Interview with Günther Orth “Arabic Metaphors don't work in German”\nMediator between worlds: Günther Orth translates Arabic literature into German. An interview about the challenges of language and the subtleties of the phrase “I want”.\nMr Orth, you interpret for politicians, scientists and artists from the Arab world. The German version of Fawwaz Haddad’s novel “Gottes blutiger Himmel” (Soldiers of God) is also by you. Currently you’re working on the translation of “Der Messias von Darfur” (The Messiah of Dafur) by Abdelaziz Baraka Sakin. How does a boy from Franconia come to the Arab world?\nI have no Arab ancestors and no orientalists in the family. The first thing I ever heard about the Middle East was the Arab oil embargo against the West in 1974. I was then a small boy; what I picked up was that the Arabs want to make life difficult for us. Later two things worked together. I hadn’t seen much of the world; as a large family from Franconia our holiday radius was limited. Maybe that’s why I was attracted to the world with all its exotic cultures. Then I found learning languages easy and knew a student in Erlangen who travelled regularly to China on scholarships. I found that inspiring. So after taking my school-leaving certificate I looked into studying Sinology. On the same floor of the university was the Oriental and Islamic Studies Department. Since Sinology seemed to be full up, I decided to try Islamic Studies. The pace was very brisk, and after a week there were only 20 students; at the end of the semester only five.\nEven many Arabs find it difficult to learn Modern Standard Arabic, particularly the grammar. How was the beginning for you?\nDifficult. Everything was unfamiliar and hard to compare to European languages. But if right at the start you labour through the classical method and always learn the right endings -un, -in, -an, you can build on this later. As the joke goes: the first five years are difficult; after that it’s not too bad.\n“An erosion of classical Arabic”You learned Modern Standard Arabic, but in Arab countries the inhabitants speak various dialects. Was that disconcerting on your first trips there?\nTo learn Arabic is about as hard as learning four or five European languages. It’s very confusing when simple things such as the phrase “I want” can mean, depending on the dialect, ayiz, beddi, brid, dayir, ashti, bghiti, bhibb, abi…. In Modern Standard Arabic it means “uridu”, but that’s rather out of place in everyday life. A truly frustrating experience for someone who has laboriously learned classical Arabic and then travels to an Arab country and has to ask for directions! When I was at university, many Arabs still had a command of correct Modern Standard Arabic. For about two decades now it’s been apparent that this has become a rare expertise. Even writers make grammatical errors that I notice, because at the beginning of our studies and at the Language Institute of Damascus we were tormented with the fine points of syntax and declensions. In Arab schools that’s evidently no longer the case. The result is a real erosion of classical Arabic.\nYou work as an interpreter, but also translate Arab literature. What are your criteria for selecting a book to translate?\nWhen it’s up to me, then my decision is based on what I like and whether the book will remain a strong text in German translation. When I receive a commission for a translation, the first thing I do is make sure I consider the work to be valuable. The public should be given texts that testify to the vitality and creativity of Arab literature. Literature shouldn’t be translated for purely financial reasons. But it happens all too rarely that you’re commissioned to translate Arab literature. Usually it’s the case that I discover something interesting and would like to translate it, but can’t find a publisher. There isn’t much curiosity here about the work of Arab writers.\n“Arabic metaphors don’t work in German”What difficulties does Arab literature harbour for the translator?\nEssentially the same ones as translators from other languages have to do with. You must be creative to render literary devices as elegantly and precisely as possible, without it sounding forced or outlandish in the target language. Many metaphors don’t work in German, so they have to be recast. It’s a process of endless decision-making. Only with practice does this agony become a pleasurable affair and you realize where you should soften or intensify, smooth, stretch or restructure; in short, how you “say more or less the same thing, but in other words”, as Umberto Eco has described it. Difficult about Arab literature is that much that concerns religion and aspects of everyday cultural life is only alluded to. In German this has to be elaborated so that everyone understands it."", '- American Physical Society Sites\n- Meetings & Events\n- Policy & Advocacy\n- Careers In Physics\n- About APS\n- Become a Member\nBy Fred Schlachter\nAs I sat in my car in total frustration at the horrific traffic jam on my way to a battery conference, my mind wandered to a fantasy from my youth: flying cars. The Dick Tracy two-way wristwatch-radio became reality with the first Apple Watch. Why not flying cars?\nThere have been many attempts. One 1947 flying car looked like a Cessna glued to a Ford Pinto: the Convair Model 118. There is little interest at the moment in a car that flies, and, as long as it uses lift, it would require an airport runway. But while I was en route from Berkeley to San Jose to attend ""Beyond Lithium Ion (BLI) X"" at IBM Research-Almaden, the thought of leaping over the traffic stuck on I-880 reinforced my dream of flying right over the traffic jam.\nWhen I finally got to BLI-X, I found the mood more upbeat and optimistic than in recent years, as progress will likely result in batteries that have a higher energy-storage density, are less expensive and possibly safer, and will likely allow new applications. I was particularly intrigued by the prospect of a new battery chemistry: lithium/sulfur. But my mind kept returning to the traffic jam I had negotiated and musing about flying over it.\nHelicopters are far too expensive and noisy to be a viable option for most of us. Flying cars might be a pipe dream, but vertical-take-off-and-landing (VTOL) planes could be an alternative. A VTOL craft could take off and land at a small local ""vertiport."" Making the plane electric and powered by batteries will eliminate carbon-dioxide production if electricity from a green grid is used. Of course, the problem of going to and from the vertiport remains.\nThe uber-disruptive company Uber says it has a solution. It has proposed a summon-on-demand service using battery-powered VTOL air taxis, which it intends to deploy as soon as 2020. The company is optimistic, even though several critical components are missing or still in early stages of development.\nA key component is a suitable battery. Batteries that can power such a plane and be quickly recharged do not exist due to insufficient specific energy (energy per unit weight), low charging rate, poor cycle life, and cost. A German company — Lilium — along with other companies are working on a demonstration electric VTOL plane, but it is a long way from certification and production.\nThere are a host of other obstacles that must be overcome: Pilots must be trained, and even without pilots, government approved autonomous flight-control software will eventually be necessary; infrastructure needs for takeoff/landing/recharging/air-traffic control must be met; battery safety in aircraft (certain phones are not allowed on commercial flights today due to the risk of battery fires) must be addressed; and a clear business model must be developed. Convincing customers to board a plane with huge batteries could be a problem. Noise at urban vertiports will likely meet with opposition. And costs could be in the stratosphere.\nThe low energy-storage density of batteries and thus their weight is an issue; however, electric motors are greatly more efficient than internal-combustion or jet engines, and they are more quiet as well. This efficiency is used to good effect in the present generation of electric cars.\nNASA is exploring electric planes. Engineers point out that electric motors are much smaller and lighter than jet engines, allowing them to be integrated into the fuselage rather than having them hang under a wing. The result would be improved propulsion-airframe integration and operational efficiency. Unfortunately, present batteries do not store enough energy to power an electric plane over a useful distance. NASA is developing the X-57, a plane with 14 electric motors turning propellers and set into a uniquely-designed wing.\nA battery-powered VTOL plane illustrates some of the challenges that product designers face in the 21st century. Better, cheaper energy storage is required to make many such products feasible. It is important for expanding the use of wind and solar energy sources, which are of little use when clouds block the sun or the wind is not blowing.\nEnergy storage from wind and solar may not require batteries, since portability is not necessary. Energy can be stored in thermal, kinetic, and potential forms. The most important criteria for utility-scale energy storage are long life, low cost, long-term reliability, and scalability. Advanced batteries may be useful but not always the best option for utility-scale energy storage.\nThe optimistic mood at BLI-X was due in large measure to two recent developments.\n(1) All-solid-state batteries, where the conventional flammable liquid organic electrolytes are replaced with solid materials, will reduce or eliminate the fire hazard for batteries. Ceramics and glasses are contenders, although more development is necessary.\n(2) Batteries with a lithium (Li)-metal anode and a cathode containing primarily sulfur (S) may become feasible. A Li/S battery would have an energy density twice today’s Li-ion batteries with potentially increased safety when a solid electrolyte is employed, and reduced material cost. Lithium metal reacts violently with water, which should be possible only in the event of serious physical damage; or fire could occur if a cell shorts due to growth of a dendrite.\nLi/S batteries still have some challenges in cycle life and high-rate manufacturing but look very promising. If these challenges can be overcome, Li/S would be the first significant new battery chemistry since the invention of the Li-ion battery nearly thirty years ago.\nThere is an example of a Li/S battery in use for flight: Sion Power makes a Li/S battery for UAVs (Unmanned Aerial Vehicles). But it is a long way from a UAV to transporting humans.\nThe two advances listed above could considerably increase the range and lower the cost of electric cars, make batteries more safe, and maybe even make electric VTOL planes a reality, but perhaps not soon. Our battery future is looking brighter than it has in recent times.\nI am still dreaming about being able to fly over those awful freeway jams.\nThe author is a physicist retired from Lawrence Berkeley National Laboratory, a Fellow of the American Physical Society, and an occasional policy analyst and consultant to APS. He is especially interested in the future of renewable-energy storage and the application of electricity to our transportation future. He is also a Commissioner on the Energy Commission of the city of Berkeley, CA: one of his present issues is replacing natural gas with electricity for residential space and water heating. Fred continues his study of the Thai language in his spare time.\nPhoto: Wikimedia commons\nWhen do I get my flying car?\n©1995 - 2019, AMERICAN PHYSICAL SOCIETY\nAPS encourages the redistribution of the materials included in this newspaper provided that attribution to the source is noted and the materials are not truncated or changed.\nEditor: David Voss\nStaff Science Writer: Rachel Gaal\nContributing Correspondent: Alaina G. Levine\nPublication Designer and Production: Nancy Bennett-Karasik']"	['<urn:uuid:9e5ebbd7-3533-4fd1-832f-bacb670329d5>', '<urn:uuid:d8fe7e4e-02f0-4e73-8c80-d3a19aac27b0>']	open-ended	with-premise	long-search-query	distant-from-document	multi-aspect	novice	2025-05-12T21:52:51.670698	19	111	1988
7	Hi, I'm curious about different types of coffee plants. How does the coffee bean called Pink Bourbon compare to regular Bourbon in terms of history and characteristics?	Pink Bourbon is a recent mutation of the traditional Bourbon variety, discovered less than a decade ago in Huila, Colombia. It differs from regular Bourbon mainly in cup quality, producing unique flavors and pink fruit-like acidity. Regular Bourbon, on the other hand, has a much longer history - it was found on Réunion Island (formerly Bourbon Island) by French missionaries who brought it from Yemen in the 18th century. Traditional Bourbon produces a balanced cup with bright fruit flavors and caramel sweetness. At El Fénix farm, Pink Bourbon grows as a 'dwarf' variety, allowing for denser planting with 5,000 trees fitting in space where regular Bourbon would only accommodate 2,500-2,800 trees.	['Finca El Fénix has enjoyed many titles since it started its journey with Raw Material back in 2014. Cupping lab, experimental farm, traveller HQ, and now, in 2020, it achieves its finest title yet – community wet mill.\nThe Story of El Fénix\nEl Fénix’s metamorphosis started as the brainchild of New Zealand coffee pros Matt Graylee and Richard Corney, working closely with Miguel Fajardo who runs Raw Material’s Colombian operations. El Fénix was the first big project ever undertaken by Matt and Richard’s now-global social enterprise, in response to observations that many farmers weren’t able to take advantage of the quality potential of their coffee. In the Quindío region, as in much of the coffee-growing world, this disadvantage is due in large part to unpredictable coffee prices and lack of processing infrastructure. Matt, Richard and Miguel set out to disrupt this cycle by building solutions into their business model, and in the case of El Fénix – in physical form.\n[photo courtesy of Raw Material]\nThe farm sits at 1700-1800 meters above sea level, looking westward toward the Cauca Valley. There was only the most basic amenities present on the farm when it was purchased in 2014, thus the project has included many phases of construction since its inception. The ultimate goal has been to build a community wet mill; a processing facility which would add value back to the local community. Along the way, the team has built a cupping lab, an office, drying beds, accommodation for travellers and more. The construction of the wet mill itself (the most laborious and expensive phase of the project) has been ongoing since the beginning.\nThe idea for the community wet mill, as described by Miguel himself when our Head of Coffee spoke with him recently via IGTV, was to address two major issues faced by the area’s small producers. In the region, a lack of processing capacity at many of the farms means that during harvest, they are not able to process all of their coffee at once when the peak of the harvest comes. Coffee cannot be held in cherry form, because it would (like all fruit) quickly begin to rot in the Colombian heat. Therefore, many farmers lacking any other option, sell their excess cherries onto local middle men at below-market prices.\nThe second issue, one that typifies the reality for many coffee producers the world over, is the unstable price of coffee itself. Farmers cannot count on the market price of coffee covering their cost of production year on year; therefore life on the farm is a constant state of precarity, without assurances that the farm will turn a profit at any given harvest. Raw Material has built into their model a flat-rate pricing system, which uses the local cost of production, combined with quality, to ensure that all farmers they work with receive a fair, stable, and transparent price for their coffee.\nEl Fénix community wet mill prior to construction [photo courtesy of Raw Material]\nIn 2016, to raise money for the mill, Raw Material ran a Kickstarter campaign in which companies or individuals were able to sponsor anywhere from a single tree to an entire lot of coffee at the farm. Caravan sponsored eight, making us El Fénix’s largest single contributor. We are thrilled to have been a part of this ground-breaking project from the beginning.\nEach of our lots was planted with a single varietal of coffee, including well-known varietals such as Geisha, to the more rare and curious sounding ‘Wush-Wush’. This summer was the first full harvest from our sponsored lots, with great weather and healthy trees contributing to a truly stellar crop. This July, we are thrilled to offer the first-ever release from the El Fénix project in the UK. This coffee comes from our Pink Bourbon lot, and shows off this farm’s incredible potential in the cup, with juicy acidity reminiscent of ripe peaches and watermelon.\nPink Bourbon: What’s in a Name\nWithin the coffee growing world, there are hundreds of different ‘varieties’ of the coffee tree (sometimes referred to as cultivars). These range from very old and traditional varieties such as Bourbon and Typica, to newer, purpose-bred ones (such as Marsella and Tekesic) all the way through to next-generation F1 Hybrids. Each variety serves a different purpose, and are often grown in only certain regions where that particular tree’s genetic expression adapts best to the local environment.\nPink Bourbon is a new expression of a traditional variety, being a naturally occurring mutation between the red and yellow varieties of Bourbon. Bourbon has a long and revered history, and is still prized for its superior cup quality around the world. Having come to Reunión Island (formerly Bourbon Island) by way of Yemen in the 18th century, it was grown there by missionaries until the mid-1800s, when it was then introduced to Latin America through Brazil. Cultivation of the variety spread rapidly, and today many areas of the continent continue to grow Bourbon trees which are not at all dissimilar to those brought over in the 19th century.\nPink Bourbon cherries at El Fénix [photo courtesy of Raw Material]\nThe pink mutation was only recently identified, having been discovered first by farmers in the department of Huila, south of Quindío. Being less than a decade old, the variety is not yet considered stable by most coffee breeders, which makes its continued study and cultivation all the more interesting. The differences between pink bourbon and its red and yellow cousins are mainly in cup quality, with unique flavours and surprisingly pink fruit-like acidity. The Pink Bourbon trees grown at El Fénix are what are referred to as a ‘dwarf’ variety, meaning they grow shorter and wider in comparison to others. This smaller stature means that the farm was able to plant 5,000 trees in the same amount of space that other trees would only fit 2,500-2,800.\nThe Future: Serving the Community\nThe El Fénix mill is now ready to start accepting cherries from the local community when the fly crop begins in October. The mill has been designed for modularity, meaning they can process small and large lots, blended lots, and lots separated by farmer and/or variety. The mill also has the capacity to process honey and natural coffees, which impart different, often sought-after flavours compared to the traditional washed method. Miguel and his team oversee the working of the mill, hiring locally to keep the value of the work in the community.\nThe El Fénix community wet mill ready to start accepting cherries [photo courtesy of Raw Material]\nThe coronavirus crisis has hit the coffee industry hard, with many cafes, restaurants, and roasteries taking on major losses. This ripple effect is sure to be felt within the producing community as well, as coffee buyers reduce their spending as they try to weather the storm. In Colombia, an additional effect of the pandemic has been the inability of coffee pickers to travel from region to region as they normally would. Instead, farmers are reliant on family and local labour to work the harvest, which is not always adequate. Luckily for El Fénix, their long relationship with the local community means they are able to secure all their labour needs for this harvest, but all eyes are on what will happen to the national coffee industry should the crisis continue to escalate.\nFrom all of us at Caravan here in London, we are happy to support and to celebrate with our partners at Raw Material, El Fénix, and the community of Santo Domingo Alto. Our thoughts and well wishes are with the team in Colombia and we hope the upcoming harvest and the opening of the community wet mill goes on smoothly, with much more delicious coffee on its way in the future.', 'Whether you prefer an automatic drip coffee maker or the meditation of a perfect...\nWhile there are many types or varieties of coffee out there in the consumer market, there are really only two main species of coffee plant.\nJust a few years ago, it was enough to know whether your coffee was Arabica or Robusta, the two main species of the coffee plant. Arabica, the more delicate of the two plants, produces flavorful coffee with many nuances in the cup. Robusta, on the other hand, tends to be flatter and bitterer. Arabica is more expensive to grow. It is very picky about growing conditions, and it doesn’t produce as heavily as the Robusta plant. Robusta coffee plants are, as the name implies, robust. They resist many of the diseases to which the Arabica crop succumbs, and they’re far less particular about the conditions under which they grow.\nThanks to some heavy marketing by standard supermarket brands, most coffee drinkers know that Arabica coffee is the more prized of the two main species of coffee. Fewer coffee lovers are aware that there are actually hundreds of species of coffee besides Arabica and Robusta, but only a couple of them are grown and harvested for beverage outside those two. In addition, there are dozens of varieties within the Arabica coffee family, most of them either deliberately hybridized for specific qualities or natural hybrids found in the wild and cultivated because they offered something unique.\nThose coffee beans have names like Bourbon, Pacamara, Gesha and SL-32. Some of them are very specific to particular areas and coffee-growing regions – the heirloom varieties of Ethiopian coffee, for example. Others, like Bourbon, are widespread and grow in many different countries and regions of the world.\nEach of these beans has a specific cup profile, though there are wide variances among the actual cup flavor, depending on the climate, soil and care the coffee plant received while it was growing, and the type of processing the coffee bean went through after the coffee cherry was picked.\nToday, specialty coffee lovers want to know far more about the coffees they enjoy than whether the beans are Arabica or Robusta. There are many factors that affect the flavor of the brewed coffee, and coffee consumers want to know more and more about those factors when they buy a pound of coffee beans. Get to know the many varieties of coffee beans and learn a whole new level of appreciation for the not-so-humble beverage that is made from them.\nArabica v. Typica\nMost of today’s coffee beans are hybrids or mutants of Coffea Arabica v. typica, originating in Ethiopia, where many heirloom varieties of Typica are still grown and marketed. In fact, there are more than 1,000 typica varieties grown in Ethiopia, and are similar to the typica varieties that grow in Yemen. Typica trees were probably the first coffee trees to make the migration to the Americas. Many coffee experts view the qualities of Ethiopian coffees as the standard by which to judge other coffees. There is no describing a “typical” cup profile for these heirloom coffees. Depending on the region, the specific variety and the processing, the cup may be light to medium bodied, and may have fruity, citrus, floral or winey flavors intermingling. Ethiopian coffees grown in the Yirgacheffe and Harrar regions are generally regarded as among the best in the world.\nThe Bourbon variety is a natural mutation of typica, that was found on the island of Bourbon, now called Réunion, by French missionariee. They cultivated it and carefully bred it to produce a balanced cup of coffee with bright fruit flavors and an underlying caramel sweetness. Bourbon (pronounced Bor BONN) spread widely throughout Africa and the Americas, largely through the French colonies and holdings. The Bourbon bush produces more coffee than Typica bushes, but is more susceptible to disease. It is also the “mother” to many other varieties of coffee on the market today.\nCatimor is a cross between the Caturra variety and Timor, a hybrid that crossed Arabica beans with Robusta beans in an attempt to breed a disease-resistant coffee plant. Timor is widely regarded as inferior because of the harsher flavors imparted by the robusta. Coffee experts find the same criticisms of Catimor coffee, but there are strains grown in India, El Salvador and Nicaragua that can overcome the sourness and astringency if they are processed properly.\nAnother Caturra hybrid, Catuai was developed in the 1950s-60s in Brazil by crossing Caturra with Mundo Novo, a typica/Bourbon hybrid. It includes variations Red Catuai, Yellow Catuai and Ouru Verde, and is widely grown in Brazil. It exhibits some of the better characteristics of good Brazilian coffee: bright acidity with an underlying sweetness.\nA natural mutant of the Bourbon strain, Caturra was found in Brazil in 1937, but it didn’t do very well in the lower regions of that country. In the higher altitudes of Colombia and other Central American countries, however, it flourishes and produces a cup with light body and bright, citric acidity. It is one of the mother strains for a number of other varieties, including Catuai, Catimor and Maracatu.\nA relative newcomer in the world of coffee varieties, Gesha originates near the Ethiopian town of Gesha, but is best known as a Panamanian coffee. In 2007, a Gesha from Hacienda Esmeralda took the judges of the Panamanian Cup of Excellence competition by surprise with its complex layers of flavors and silky mouth feel. It took top honors in that competition and has consistently scored high in competitions since. Gesha is among the most prized varieties of coffee beans on the market today.\nIcatu is a Catimor hybrid that is widely planted in Brazil. Because it was recrossed with Arabica varieties, it does better in the cup than other Catimors, especially when it is dry-processed, allowing the plum, berry and chocolate flavors to shine.\nJackson is a Bourbon variety that grows in Rwanda and Burundi. The flavor is similar to Bourbon, but it has a delicate acidic edge that Bourbon lacks. In recent years, as production and processing advances are made in those two countries, Burundi and Rwanda are starting to acquire a name for high quality coffee.\nJamaican Blue Mountain\nJamaican Blue Mountain is not just the name of the coffee growing region, it is the name of the coffee variety that grows best there. Jamaican Blue Mountain coffee is one of the first coffees to grow in the New World, and unlike many others that flourish in the Americas, it is a Typica strain. The Jamaican Blue Mountain name can be used for any coffee grown within the region, but not all coffees that bear the name are of the JBM strain. On the other hand, you can buy JBM coffee that is grown in Hawaii. The typical cup profile is light in body, balanced in flavor and mild in acid.\nJember is an Indonesian strain of Typica that is a combination of Kent and S228. It may be referred to as S795. It was developed for hardiness in the 1940s and is widely grown throughout Indonesia. It exhibits the qualities associated with Indonesian coffees: heavy body, buttery richness and maple, brown sugar and caramel sweetness.\nKent is a precursor of Jember, developed on an estate in Mysore, India in the 1920s. Its primary characteristic is disease resistance, particularly to coffee leaf rust. It was adopted and widely planted throughout the region, but was nearly wiped out entirely by a new wave of coffee leaf rust. Some heirloom trees remain and are being cultivated carefully. It features a much lighter body than most Indonesian coffees, with hints of floral and spice in the flavor.\nTypica coffees seem to flourish on the Hawaiian Islands. Kona Typica is descended from Typica trees imported from Brazil originally, and later, from Guatemala. The Guatemalan Typica trees are commonly referred to as Kona Typica coffee, however, many coffee farmers grow Kona Typica, Red Caturra, Jamaican Blue Mountain and other coffee varieties side by side and mix them when harvesting. The coffee is clean, mild and balanced.\nThis cross between Maragogype and Caturra is a large coffee bean that is grown at the higher altitudes in Central America. It features complex acidity and ripe fruit flavors.\nMaragogype (mara go hee pe) is a mutation of the Typica coffee plant that grows in Brazil. It’s primary feature is the enormous size of its coffee beans, which has earned it the nickname Elephant Bean coffee. It is one of the base varieties in a number of other varieties. The cup has a heavy, buttery body and hints of citrus and floral flavors.\nThis mutation of Typica/Bourbon originated near Harrar, Ethiopia, and is grown in Hawaii and Yemen. Its very small bean size distinguishes it from other coffee beans, as does the pronounced chocolate flavor in the cup.\nThis natural hybrid between Typica and Bourbon coffee plants is a heavy producer and resistant to most diseases that affect coffee plants. It has some natural flavor defects that can be overcome with a lot of fertilizer and soil modifications. It is a base for a number of popular varieties. On its own, it is slightly bitter without the sweetness of either of its progenitors.\nPacas and Maragogype combine in the Pacamara coffee bean, which originated in El Salvador in 1958. Most Pacamara still grows in El Salvador, but neighboring Central American countries also supply some Pacamara. In the cup, it is very balanced with sweet acidity and floral notes overlaying citrus flavors.\nA natural Bourbon mutation from El Salvador, it retains many of the qualities that make Bourbon coffee beans so popular. It offers higher yields and done well at higher elevations. It balances sweetness and acidity and has floral and spice notes.\nPache coffee beans grow in Guatemala. The typica mutation has two distinct variations: Pache Comum and Pache Colis. They are heavy producers that offer a smooth, flat flavor profile which makes it popular as a blender coffee.\nRuiru 11 was meant to be the next big thing in Kenyan coffee – a wild Arabica with the resistance of a Robusta. It was developed at Ruiru, Kenya, but its flavors don’t live up to the standards for Kenyan coffees because of the Robusta content in its DNA.\nSL-34 and SL-38\nThe SL stands for Scott Labs, where the famous Kenyan coffee strain was developed. Two SLs – SL-34 and SL-28 – make up nearly 90% of Kenya’s coffee exports. They are the basis for Kenya’s reputation for fine, wild coffees with fruit/wine flavors and long, sweet finishes that are commonly called “blueberry bombs.”\nVilla Sarchi is a Bourbon hybrid developed near the town of Sarchi. It does very well using organic and sustainable farming methods, and features medium body and refined acidity with intense fruit flavors in the cup.\nLike Villa Sarchi, Villalobos is grown in Costa Rica, but is a mutation of Typica rather than Bourbon. The plant does well at high elevations and in poor soil, making it very popular with Costa Rican farmers. In the cup, it offers outstanding sweetness and high, fine acidity.']	['<urn:uuid:c58e7751-cb4a-4c38-8994-cc80c456c7c3>', '<urn:uuid:23168759-a8cc-4f88-92df-d75bc7966c9e>']	factoid	with-premise	verbose-and-natural	distant-from-document	comparison	novice	2025-05-12T21:52:51.670698	27	111	3159
8	protect garden plants freezing winter prevent drying out specific techniques	Plants need several protective measures to survive winter. First, maintain healthy plants throughout summer and fall. Water thoroughly before ground freezes and during any thaws. For evergreens like rhododendrons, apply anti-desiccant products when temperatures are above freezing but below 50°F, ensuring complete coverage including leaf undersides. Physical barriers like burlap screens can be installed after ground freezes, leaving the top open for insulating snow. Additionally, maintain a 2-4 inch layer of mulch using materials like pine bark or pine needles.	['Soil preparations are critical to the successful growing of\nRhododendrons. They require a rich well drained high organic content\nsoil mix. You can provide this by mixing into the existing soil a\nvery large amount of Peat Moss. The Peat Moss should be 50% by volume of\nthe soil. This will produce an acidic soil of about 6.0 to 6.5\npH. Check this estimate with a pH meter. After checking the pH of\nthe soil mix you will want to adjust the pH to 4.5to 5.5 using Garden\nSulphur according to the label directions. Never use Aluminum\nSulphate to adjust the pH of the soil for Rhododendrons because in\nacidic conditions the Aluminum can become toxic to the Rhododendrons\nespecially to younger plants. You should also add some leaf mold\nand coarse sand to the soil mix to encourage better growth.\nWhen you plant Rhododendrons like this Duke of York prepare a hole at\nleast 2 to 3 feet wide and 6 to 8 inches deeper than the height of the\nroot ball. If you think extra drainage will be needed place a 4 to 6 inch layer of\ncoarse gravel in the hole ensuring that you still have the required 6\nto8 inches between it and the bottom of the root ball. Back fill\nwith your soil mix and gently firm the mix around the root ball\nwhile never pushing or standing on the Rhododendrons root ball.\nThe finished grade should be\n2 to 6 inches above the original grade. You should now water in\nyour plants with a transplant fertilizer with a rooting hormone\nin it. A mulch of pine bark, cocoa beans, pine needles or oak\nleaves at least 2 to 4 inches thick can now be applied.\nAfter the flowers are finished you must disbud. This is the\nremoval of the spent flowers and must be done very carefully because the\nnew growth is immediately behind the spent flowers, though some buds\nwill also occur in the leaf axils.\nTheir removal directs the plants energies towards producing new growth\nand the flowers for next year.\nAfter disbudding gently remove the mulch by hand carefully so as not\ninjure their very shallow root system. Now add a layer of\nmulch. Alternatively you can just add the new mulch over the old\nmulch. Do, which ever you feel is best for the Rhododendron.\nFeed your Rhododendrons at this time with an acid based\nDo not feed them after July 1st. Feeding after this time encourages late\nseason growth that may not harden off before the winter sets in and it reduces\nIf winterizing your Rhododendrons is required by the environmental\nconditions i.e. northwest location then just before the ground freezes\npound in some wooden stakes at least 1 foot out from the ends of the\nbranch tips and taller than the plant. Now after the ground is\nfrozen staple a single layer of burlap to the stakes 2 inches above the\nground. The top should be\nopen to allow the snow to enter and insulate the plant. In very cold\nclimates you can carefully stuff leaves in between the burlap and the\nleaves only after the ground is frozen.\nAlways remember never cultivate around the Rhododendrons because their\nnew feeding roots grow up and hence the need for an annual mulch.\nCultivating around them can kill them.\nTruly Rhododendrons are the Kings of my garden!', 'You’ve removed late-autumn weeds, layered on the mulch, pruned appropriately, possibly even covered or wrapped your plants – so why do some still die in the winter, despite all your well-meaning efforts?\nMany plants die during winter because they dry out, or desiccate. As temperatures drop, the ground freezes and plant roots cannot take water from the soil, no matter how much snow may fall. This causes the plant to use stored water from the leaves and stems as part of the transpiration process, during which water exits the plant through the leaves. If the sun is shining or the wind is blowing, transpiration increases and more water exits the leaves. If no water is available and transpiration continues, the plant will soon die. Because evergreen plants do not drop their leaves, they are especially susceptible to this death.\nHow can you help your plants stay well-hydrated through the frozen drought of winter? The first step is to remember healthy plants in the summer survive the hardships of winter far better than sickly or stressed plants. Through the spring, summer and fall, you should always be on the lookout for signs of pests, diseases and damage, and take all necessary steps to keep your plants thriving.\nSecond, be sure to water well even when temperatures begin dropping below freezing. Later, if the ground thaws, water before the ground refreezes. Water slowly to provide a deep drink without waterlogging the roots, however, so they are not damaged by ice.\nThe third step is to use an anti-desiccant, also called an anti-transpirant, to reduce the moisture loss from the leaves and needles. Because broadleaf evergreens such as boxwood, aucuba, holly, rhododendron, many laurels, Japanese skimmia and leucothoe do not drop their leaves, they are especially vulnerable to winter death. Using a product such as Wilt-Pruf to reduce transpiration by protecting the pores will save many broadleaf evergreens.\nWhen using any horticultural product, be sure to check the label and follow all instructions properly. Some conifers such as cedar, cypress, juniper and pine may benefit from these products. However, be sure to read the instructions to prevent burning specific conifers. Also, do not use on “waxy” blue conifers, such as blue spruce, which already have an oily protective film on the nettles.\nHere are a few reminders to get the best protection from an anti-desiccant:\n- Plan to apply when day temperatures begin dropping below 50⁰ Fahrenheit. Apply when temperatures are above freezing on a dry day with no rain or snow anticipated within 24 hours. This allows the product to thoroughly dry. Spraying in freezing temperatures will cause plant damage.\n- Do not spray conifers until thoroughly dormant, generally in late winter. This prevents trapping moisture in the needles which could burst when frozen.\n- Generously apply to dry leaves and needles. Don’t forget the undersides. Spray from several angles to ensure complete coverage.\n- Because the anti-desiccant will break down in light and warmth, reapply in late winter on a dry day when temperatures are above freezing for at least 24 hours.\nBeyond Winter Drought\nOther than protecting your landscape evergreens from winter drought, there are other uses for anti-desiccants. Many gardeners use it to protect newly transplanted shrubs from drying winds and sunshine as they settle in. It also provides protection to tender bulbs going into storage. A quick spray in early winter protects rose canes and hydrangea stems. Spraying onto live or cut Christmas trees and carved pumpkins slows the drying process, making them last longer for greater holiday enjoyment.\nTo answer your questions, or to choose the best product for your landscape plants, come in to discuss anti-desiccants with one of our friendly and knowledgeable staff members. Together, we can reduce the number of plants you lose to the dryness of winter and keep your garden beautiful and healthy.']	['<urn:uuid:26ea3468-181c-4941-83de-250323abba67>', '<urn:uuid:2ad5c424-599b-49ab-b03a-677385a46618>']	factoid	direct	long-search-query	distant-from-document	multi-aspect	novice	2025-05-12T21:52:51.670698	10	80	1208
9	What happened to America's unknown WW1 soldier in France?	The Unknown Soldier's remains were transported from France aboard the USS Olympia on October 25, 1921. Before departure, French officers decorated the hero with the Legion of Honor medal and placed wreaths on the coffin. The ship arrived at the Washington Navy Yard on November 9, 1921, where the fallen soldier was received by high-ranking officials including Secretary of War Weeks, Secretary of Navy Denby, General Pershing, and Major General Lejeune. The flag-draped casket was then transported by horse-drawn caisson to the U.S. Capitol in Washington DC.	"['The background of World War II. German Hitler Youth children march with rifles, flags and a band beneath a camp entrance named Nordmark Lager 1935. Flag with Nazi symbol. Japanese flag and Japanese youth march in military exercises, followed by teenage Japanese forces. Italian fascist troops in uniform march in Torino Italy led by a band. Views of various German and Nazi forces parade and goose step in various cities. Units include brownshirts, Storm Troopers, Wehrmacht. Civilians give a salute. Mostly sad looking men women and children watch the parades. The world globe. U.S. Capitol Building in Washington DC. French, Japanese and United States dignitaries attend the Washington Disarmament Conference or Washington Naval Conference at Memorial Continental Hall in Washington DC in 1921. View of the Washington Naval Treaty naval fleet reduction treaty book with signatures on it. Officials sign the Nine-Power Treaty that guarantees the integrity of China (per the John Hay ""Open Door Policy""). Japanese delegation is shown at signing ceremony for the Nine-Power Treaty. Next scene shows 1928 signing ceremony in Paris of the Kellogg-Briand Pact renouncing war as a means of settling international disputes. French Foreign Minister Aristide Briand is seen speaking, and American Secretary of State Frank Kellogg is also seen at the treaty signing which was signed by 47 nations including Germany and Japan. In a 1930 scene, U.S. Secretary of State, Henry Stimson hands the London Naval Treaty to President Herbert Hoover. Scene of a U.S. Navy ship being blown up and scrapped in accordance with the treaty terms. View of American citizens in small town parades with floats marching as anti-war and isolationist groups.\nThe White House past and the present. Aerial view of The White House in Washington DC in the late 1950s. Scene with 1950s cars parked and moving in parking areas and roads in front of the White House. Visitors walking on the White House grounds near the North Portico. View of a Birch tree planted by wife of President Calvin Coolidge and Magnolia trees by President Andrew Jackson. View of the south portico entrance and the north entrance of the White House. Plans and sketches of of The White House. Portraits of President John Adams and his wife Abigail Smith. Portrait of President Thomas Jefferson. Plan of the east and the west wing of The White House. Illustration of British attack on The White House. Portraits of President James Madison and his wife Alley. Portrait of President George Washington. Portrait of President James Monroe. Picture of President Abraham Lincoln. Footage of President Lincoln\'s study room, and his bedroom with its custom bed over 8 feet in length. Sketch of President Benjamin Harrison taking oath. Still images of ornate furnishings in the White House during Harrison\'s tenure. Still images of more simplified furnishings in the White House under Theodore Roosevelt. Picture of President Theodore Roosevelt with sons. Aerial view of the new west wing area enlarged for White House office use. Still photo of Theodore Roosevelt writing. Footage of President Woodrow Wilson signing a bill at his desk, with a crowd of officials standing by. Footage of United States Military officers and French General Ferdinand Foch and other French military officers entering the White House to meet with President Harding on October 29, 1921. (Foch was touring the U.S. and being officially thanked for his leadership in World War I.)\nCabinet members and prominent persons of the administration under President Warren G Harding. United States Secretary of State, Charles Evans Hughes and others walk down the stairs of a building in Washington DC, United States in 1921. Hughes along with AA Adee, HP Fletcher and RW Bliss outside a building.\nThe American Unknown soldier of World War I before burial in the Tomb of the Unknowns at Arlington National Cemetery. Remains of America\'s unknown hero sails aboard the USS Olympia (C-6) for America. A caisson carriage arrives at a harbor in France bearing the casket on October 25, 1921. Large crowd of soldiers and dignitaries present. France decorates the Unknown Hero as French officer reads citation for Legion of Honor medal. Officers crowd around the coffin. The ship USS Olympia (C-6) in the background. Sailors carry the coffin of the Unknown Soldier aboard the ship. French soldiers lay a wreath on the coffin. USS Olympia leaves the harbor. Next scene shows USS Olympia arriving in the United States at the Washington Navy Yard on November 9, 1921. Soldiers carry the casket off the ship. Unknown Soldier being received by U.S. Secretary of War John W. Weeks, U.S. Secretary of Navy Edwin Denby, U.S. Army General John J. Pershing, Commander of the U.S. Marines Major General John A. Lejeune. They salute as sailors carry the body past the military band. The flag draped casket is borne to the U.S. Capitol in Washington DC. A horse drawn caisson takes the body as officers and soldiers follow.\nThe Washington\'s Monument in Washington DC. The Equestrian Statue in the foreground. Tourists stand at the base of the monument. They enter an elevator at the base of the monument.\nBritish writer Herbert George Wells (H G Wells) poses aboard ship on visit to United States, in 1921, where he will attend Washington Disarmament Conference. He removes his hat.\nGerman composer Richard Strauss, on second visit to America, in 1921. (His first visit was in 1904.)']"	['<urn:uuid:9abd14c6-8265-49f6-978b-9d05ffa4cdde>']	open-ended	with-premise	concise-and-natural	distant-from-document	single-doc	novice	2025-05-12T21:52:51.670698	9	87	891
10	promising candidates species listed european union invasive regulation north america asia	The promising candidates include Pallas's squirrel from Asia, the North American eastern grey squirrel, fox squirrel, ruddy duck, American bullfrog, painted turtle, and the red-eared slider.	"['The first Europe-wide legislation to combat alien species with far-reaching consequences\nA new regulation governing the control of invasive alien species became effective in all EU states on 1 January 2015. The European Union hopes that it will actively combat one of the greatest threats to biodiversity and thus the functioning of ecosystems. The new regulation has far-reaching implications for authorities as well as for trade in animals and plants. The legislation also incorporates the findings of the EU\'s DAISIE research project and thus the findings of scientists at the Helmholtz Centre for Environmental Research (UFZ).\nThe Oregon grape (Mahonia aquifolium) is from western North America and came to Europe as an ornamental shrub. Since then it spreads like invasion.\nPhoto: André Künzelmann/UFZ\nThere are around 12,000 alien species currently in the EU and its neighbouring European countries, of which an estimated 10 to 15 percent are considered to be invasive. That means that these invasive species displace native species, impair the functioning of ecosystems or cause economic damage.\nPoliticians are having to set priorities as there is now a very large number of invasive alien species. What is being termed a ""Union List"" of these prioritized species is thus to be produced by 2 January 2016 and will list species that pose transnational threats.\nClassification of the respective species as alien in at least three EU countries is the prerequisite for inclusion in this list. The EU DAISIE project undertook important preliminary work in this field. Between 2005 and 2008 researchers created a database, which now contains detailed information on 12,122 species and 2,440 experts on biological invasions in Europe. It also produced a list of the 100 most problematic species - including their occurrences and an assessment of the risk they pose.\nThe EU and its member states are now faced with the challenge of deciding which of them should be included in the Union List. However, it is not intended that this will stop at pure risk assessment but rather will also include the benefits of species and regional considerations.\n""A species that could be a problem in Norway, for example, is not necessarily a problem in the south of Italy. It will not be easy to agree on a Europe-wide common denominator and yet still take into account regional interests when assessing the health, economic and ecological hazards,"" estimates the biologist Dr. Stefan Klotz of the Helmholtz Centre for Environmental Research. However, he regards the European regulation as being a major step in the right direction, as many countries are very small and, as is known, species do not stop at borders.\nConsequences extending to flower beds and home aquaria\nUnlike a regulation that only provides the framework and first has to be translated into national law, the new regulation has been in effect in all Member States since the start of the year. The direct legal implications will only become apparent when the Union List has been defined. ""The Union List is the key element of the regulation. Once you have the list, comprehensive bans on ownership and sale will apply to the listed species.\nThere will also be obligations on the part of the member states to enforce these bans and to take ongoing management and elimination measures where this is possible with reasonable effort and expenditure,"" states the environmental lawyer Prof. Wolfgang Köck of the Helmholtz Centre for Environmental Research, stressing the consequences that the legislation will have, above all, for commercial trade.\nThere is a special transitional rule for non-commercial owners: owners of companion animals shall be allowed to keep invasive alien species included on the union list until the end of the animals\' natural life, if they make sure that reproduction or escape of these animals are not possible.\nThe involvement of science plays a key role\n""The participation of the scientific community is important to provide an adequate knowledge base to address the problems raised by invasive alien species,"" acknowledges the new regulation explicitly, providing for the establishment of a scientific forum to involve researchers, among other things to produce and update what is being called a ""Union List"" for risk assessment and emergency measures. A first meeting on this subject was held in Seville, Spain, in January.\nAdvice from the scientific sector has already paid off: ""It was originally planned to limit the list of species to be actively combated to 50,"" reports Professor of Biology Ingolf Kühn from the Helmholtz Centre for Environmental Research. ""These plans were then dropped again after protests from the scientific sector. It simply does not make sense to impose an arbitrary number before the Member States have provided their data. Moreover, ecosystems can develop very dynamically and we therefore need to be able to respond flexibly to the latest developments in future. ""\nPrevention is cheaper than cure\nThe new legislation is not purely intended as a regulation to combat invasive species, but it also has a strong preventative character, as it is hard to combat species that have just become established or only to do so at great expense. Therefore only species, which do not yet occur in the EU or are at an early invasion stage but pose a major potential danger, should be given priority on the list. Experts therefore do not believe that the infamous giant hogweed (Heracleum mantegazzianum) or the allergy-inducing ragweed (Ambrosia artemisiifolia) will end up on the ""Union List"", as the cost of eliminating them would be too high.\n""Promising candidates"" are to a greater extent the species included in an earlier nature conservation regulation and will therefore be explicitly named in the new regulation. These species include Pallas\'s squirrel (Callosciurus erythraeus) from Asia, the North American eastern grey squirrel (Sciurus carolinensis), fox squirrel (Sciurus niger), ruddy duck (Oxyura jamaicensis), American bullfrog (Rana catesbeiana), painted turtle (Chrysemys picta) and the red-eared slider (Trachemys scripta elegans).\nNature conservation authorities will be faced with a lot of work: within 18 months they have to determine the routes along which the reviled species are introduced into the EU, prepare action plans and establish a monitoring system. At the same time, it is hoped that this work will improve the exchange of information and establish a kind of ""early warning system"" so that unaffected regions can react in a timely manner.\nThe new EU regulation thus represents a core element of the EU strategy for the conservation of biodiversity passed in 2011. The regulation will have far-reaching implications in the coming years for authorities, retailers and even consumers, as it sets ambitious targets for solving the problems caused for nature by increasing globalisation.\nRegulation (EU) No 1143/2014 of the European Parliament and of the Council of 22 October 2014 on the prevention and management of the introduction and spread of invasive alien species\nHelmholtz Centre for Environmental Research (UFZ)\nDr. Stefan Klotz, Prof. Dr. Ingolf Kühn\nDepartment Community Ecology\nPhone: +49 (0)345-558-5302, -5311\nProf. Dr. Wolfgang Köck\nDepartment of Environmental and Planning Law\nPhone: +49 (0)341-235-1232\nTilo Arnhold, Susanne Hufe (UFZ press office)\nPhone: +49 (0)341-235-1635, -1630\nEU about Invasive Alien Species\nDatabase of the EU research project DAISIE (Delivering Alien Invasive Species Inventories for Europe):\nEuropean Alien Species Information Network\nresearch project INVASION - Evolutionary, ecological and social consequences of Biological Invasions\nGlobalisation burdens future generations with biological invasions (Press release from December 20, 2010)\nConsequences of being rich: wealth and population are key drivers of invasive species in Europe (Press release from June 16, 2010)\nEurope\'s flora is becoming impoverished (Press release from December, 11th 2009)\nEcologists Put Price Tag on Invasive Species (Press release from April 22nd, 2009)\nSignificant increase in alien plants in Europe (Press release September 17, 2008)\nVideo ""Mink invasion"" by UFZ\nIn the Helmholtz Centre for Environmental Research (UFZ), scientists conduct research into the causes and consequences of far-reaching environmental changes. Their areas of study cover water resources, biodiversity, the consequences of climate change and possible adaptation strategies, environmental technologies and biotechnologies, bio-energy, the effects of chemicals in the environment and the way they influence health, modelling and social-scientific issues. Its guiding principle: Our research contributes to the sustainable use of natural resources and helps to provide long-term protection for these vital assets in the face of global change. The UFZ employs more than 1,100 staff at its sites in Leipzig, Halle and Magdeburg. It is funded by the federal government, Saxony and Saxony-Anhalt. http://www.ufz.de/\nThe Helmholtz Association contributes to solving major and urgent issues in society, science and industry through scientific excellence in six research areas: Energy, earth and environment, health, key technologies, structure of matter as well as aviation, aerospace and transportation. The Helmholtz Association is the largest scientific organisation in Germany, with 35,000 employees in 18 research centres and an annual budget of around €3.8 billion. Its work is carried out in the tradition of the great natural scientist Hermann von Helmholtz (1821-1894). http://www.helmholtz.de/\nTilo Arnhold | UFZ News\nDispersal of Fish Eggs by Water Birds – Just a Myth?\n19.02.2018 | Universität Basel\nRemoving fossil fuel subsidies will not reduce CO2 emissions as much as hoped\n08.02.2018 | International Institute for Applied Systems Analysis (IIASA)\nA newly developed laser technology has enabled physicists in the Laboratory for Attosecond Physics (jointly run by LMU Munich and the Max Planck Institute of Quantum Optics) to generate attosecond bursts of high-energy photons of unprecedented intensity. This has made it possible to observe the interaction of multiple photons in a single such pulse with electrons in the inner orbital shell of an atom.\nIn order to observe the ultrafast electron motion in the inner shells of atoms with short light pulses, the pulses must not only be ultrashort, but very...\nA group of researchers led by Andrea Cavalleri at the Max Planck Institute for Structure and Dynamics of Matter (MPSD) in Hamburg has demonstrated a new method enabling precise measurements of the interatomic forces that hold crystalline solids together. The paper Probing the Interatomic Potential of Solids by Strong-Field Nonlinear Phononics, published online in Nature, explains how a terahertz-frequency laser pulse can drive very large deformations of the crystal.\nBy measuring the highly unusual atomic trajectories under extreme electromagnetic transients, the MPSD group could reconstruct how rigid the atomic bonds are...\nQuantum computers may one day solve algorithmic problems which even the biggest supercomputers today can’t manage. But how do you test a quantum computer to...\nFor the first time, a team of researchers at the Max-Planck Institute (MPI) for Polymer Research in Mainz, Germany, has succeeded in making an integrated circuit (IC) from just a monolayer of a semiconducting polymer via a bottom-up, self-assembly approach.\nIn the self-assembly process, the semiconducting polymer arranges itself into an ordered monolayer in a transistor. The transistors are binary switches used...\nBreakthrough provides a new concept of the design of molecular motors, sensors and electricity generators at nanoscale\nResearchers from the Institute of Organic Chemistry and Biochemistry of the CAS (IOCB Prague), Institute of Physics of the CAS (IP CAS) and Palacký University...\n15.02.2018 | Event News\n13.02.2018 | Event News\n12.02.2018 | Event News\n23.02.2018 | Physics and Astronomy\n23.02.2018 | Health and Medicine\n23.02.2018 | Physics and Astronomy']"	['<urn:uuid:3ffd4540-dd45-4297-b00e-410970488af0>']	factoid	direct	long-search-query	similar-to-document	single-doc	expert	2025-05-12T21:52:51.670698	11	26	1864
11	sea creatures found near thai coastal waters that die eating trash	According to the Thai Marine and Coastal Resources Department, at least 300 sea animals on average die each year from eating plastic fishing gear and trash, with 60 percent of these deaths being whales and dolphins. Additionally, filter feeding invertebrates tested off the coast of Chonburi Province showed high levels of microplastics, raising concerns about health risks when people consume these contaminated marine organisms, particularly shellfish.	"['|Gulf of Thailand|\n|Primary inflows||South China Sea|\n|Surface area||320,000 km2 (120,000 sq mi)|\n|Average depth||58 m (190 ft)|\n|Max. depth||85 m (279 ft)|\nThe Gulf of Thailand, also known as the Gulf of Siam, is a shallow inlet in the southwestern South China Sea, bounded between the southwestern shores of the Indochinese Peninsula and the northern half of the Malay Peninsula. It is around 800 km (500 mi) in length and up to 560 km (350 mi) in width, and has a surface area of 320,000 km2 (120,000 sq mi). The gulf is surrounded on the north, west and southwest by the coastlines of Thailand (hence the name), on the northeast by Cambodia and the Mekong Delta region of Vietnam, and opens to the South China Sea in the southeast.\nThe modern Thai name of the gulf is Ao Thai (Thai: อ่าวไทย, [ʔàːw tʰāj] (listen), \'Thai Gulf\') and ""Gulf of Thailand"" has been adopted as the official name of the body by the International Hydrographic Organization.[when?] Its name in Malay is he ""Gulf of Siam"", Teluk Siam, and in Khmer: ឈូងសមុទ្រសៀម, Chhoung Samut Siem. In Thai, the gulf is historically known as Ao Sayam (Thai: อ่าวสยาม). In Vietnamese it is known as Vịnh Thái Lan.\nIt is generally identified with the Great Gulf (Latin: Magnus Sinus) known to Greek, Roman, Arab, Persian, and Renaissance cartographers before the influx of Portuguese explorers removed the phantom Dragon Tail peninsula from European world maps in the 16th century.\nThe Gulf of Thailand is bordered by Cambodia, Thailand, Malaysia, and Vietnam. It occupies a seabed area of 304,000 km2 from 6° N to 13°30\' N latitude and 99°E to 104° E longitude.: 250 The northern tip of the gulf is the Bay of Bangkok at the mouth of the Chao Phraya River. The southern boundary of the gulf is defined by a line from Cape Bai Bung in southern Vietnam (just south of the mouth of the Mekong River) to the town of Tumpat and Pengkalan Chepa on the Malaysian coast.\nThe gulf is relatively shallow: its mean depth is 58 metres (190 ft) and the maximum depth is only 85 metres (279 ft).: 250 This makes water exchange slow, and the strong water inflow from the rivers reduce the level of salinity in the gulf (3.05–3.25 percent) and enriches the sediments. Only at greater depths does water with a higher salinity (3.4 percent) flow into the gulf from the South China Sea. It fills the central depression below a depth of 50 metres (160 ft). The main rivers which empty into the gulf are the Chao Phraya, including its distributary Tha Chin River, the Mae Klong, and Bang Pakong rivers at the Bay of Bangkok, and to a lesser degree the Tapi River flowing into Bandon Bay in the southwest of the gulf.\nThe International Hydrographic Organization defines the southern limit of the gulf as ""[a] line running from the Western extreme of Cambodia or Camau Point (8°36\'N) to the Northern extreme of the point on the East side of the estuary of the Kelantan River ()"".\nThe seabed morphology in the central depression of the gulf is characterised by the presence of elongated mounds and ridges arranged parallel to the axis of the basin. This morphology, widespread within the gulf in water depths exceeding 50 m, covers an area of tens of thousands of square kilometres.\nIt reflects an interaction between sediment dewatering and the erosional activity of the present-day bottom currents. The sediment dewatering and fluid seepage result in the formation of numerous small pits and pockmarks. The long-term erosion imposed by currents of stable orientation modifies pockmarks into long runnels and depressions, and ultimately leads to the formation of the large fields of elongated mounds and ridges, as well as the residual outliers of un-eroded mud and clay sheets.\nThe larger islands in the gulf include:\nThere are 75,590 rai of coral reef in the gulf, of which five percent are considered to be in fertile condition. In 2010 severe coral bleaching occurred at most reef sites in the country. Bleaching of reefs in the Andaman Sea was more severe and extensive than that in the Gulf of Thailand. In 2016, coral bleaching was detected at Ko Thalu and Ko Lueam in Prachuap Khiri Khan Province for the first time. Scientists have determined that bleaching starts when seawater temperature rises beyond 30 °C for more than three weeks. Given the prolonged period of temperatures up to 32 °C at Ko Thalu in Prachuap Khiri Khan, five to ten percent of corals in the area are already bleached.\nCoastal water monitoring results in 2015 from 202 sampling locations, collected twice annually, indicate that no Thai coastal waters were found to be in excellent condition. Sixteen percent of coastal water was of good quality, 72 percent was of fair quality, 9 percent was of poor quality and 3 percent was of very poor quality. The quality of all coastal waters exhibited similar percentages — most were of fair quality — except for the Inner Gulf of Thailand, where the coastal water was poor to very poor. In comparison to coastal water quality as measured in 2014, water quality has deteriorated.: 52 Some gulf waters off Chachoengsao Province, Samut Sakhon Province, Samut Prakan Province, Bangkok, Rayong Province, Chonburi Province, Phetchaburi Province, Prachuap Khiri Khan Province, and Surat Thani Province were judged to have coastal waters in ""poor"" or ""very poor"" condition.: 54 Songkhla was the only province on the gulf with coastal water rated ""good"" quality.: 56\nOf Thailand\'s total marine catch, 41 percent is caught in the Gulf of Thailand and 19 percent in the Andaman Sea. Forty percent is caught in waters outside Thailand\'s EEZ.\nThailand has 1,660 kilometres of coastline bordering the gulf. ""Severe erosion"", more than five metres of coastline loss per year, afflicts 670 kilometres of that total. At least some of the erosion is attributable to the clearing of mangrove forests to make way for shrimp farms.\nIn February 2017, a 10 kilometer-long patch of plastic refuse was found floating off Chumphon Province. Thailand is among the world\'s worst plastic polluters. More than half of ""land-based plastic waste leakage"" into the sea originates from just five countries: China, Indonesia, the Philippines, Thailand, and Vietnam.\nThe Thai Marine and Coastal Resources Department has noted that at least 300 sea animals on average—60 per cent of which are whales and dolphins—die from eating plastic fishing gear and trash each year. Filter feeding invertebrates tested off the coast of Chonburi Province showed high levels of microplastics, leading the authors to warn that, ""Health risks are possible when people consume these contaminated marine organisms, particularly shellfish.""\nThailand\'s Pollution Control Department (PCD) estimates that plastic waste in the country is increasing at an annual rate of 12 percent, or around two million tonnes per year.\nIn 2013, a pipeline leak resulted in an oil slick that went on to coat a beach on the nearby Ka Samet island.\nIn late January 2022, a leak in the pipeline operated by the Star Petroleum Refining Public Company Ltd caused a spill of 20 to 50 tonnes across 47 sq km of water, with some oil reaching the coast of Rayong province 20 km away.\nThe gulf\'s many coral reefs have made it attractive to divers. The tropical warmth of the water attracts many tourists. Some of the most important tourist destinations in the Gulf of Thailand are the islands of Ko Samui and Ko Pha Ngan in Surat Thani Province, Pattaya in Chonburi Province, Cha-am in Phetchaburi Province, Hua Hin in Prachuap Khiri Khan Province, and Ko Samet in Rayong Province.\nIn recent years, the bay has become known for its whale watching activities, targeting the endemic, critically endangered populations of cetaceans (Eden\'s whales, newly described Omura\'s whales, Chinese white dolphins, and Irrawaddy dolphins showing unique feeding behaviors), and dugongs. It was first classified by Müller in 1776 as Trichechus dugon. Five species of the sea turtles have been found in the Gulf of Thailand and the Andaman sea coast, including olive ridley turtles, green turtles, hawksbill turtles, loggerhead turtles, and leatherback turtles.\nThe area between Malaysia, Thailand, Cambodia, and Vietnam is subject to several territorial disputes. Malaysia and Thailand have chosen to jointly develop the disputed areas, which include the islands of Ko Kra and Ko Losin. A long-standing dispute between Cambodia and Vietnam in the Gulf of Thailand concerns mainly the island of Phú Quốc or Koh Tral in Khmer, which is off the Cambodian coast. Cambodia also claims 48,000 square kilometres (19,000 sq mi) of shelf area.']"	['<urn:uuid:3c7b70a7-9ffb-4a0c-9e6a-e9764f763ebc>']	open-ended	direct	long-search-query	distant-from-document	single-doc	novice	2025-05-12T21:52:51.670698	11	66	1436
12	problems magnet treatment heal pain	According to proponents of magnetotherapy, magnets can be used to treat pain through local application. They claim that the north pole of the magnet should be used against pain and inflammatory processes, while the south pole provides strength and energy. For example, for shoulder pain, they recommend putting the north pole of one magnet on the front of the shoulder and the south pole of another on the back. They claim this method can help with numerous conditions including abscess, acne, allergy, anaemia, arthritis, asthma, bronchitis, cellulitis, sciatic pain, diabetes, and many other conditions. However, it's important to note that these are unproven claims.	"[""Biomagnetic Pseudoscience and Nonsense Claims\nThe following is a short excerpt from an updated version of a paper titled “Magnetotherapy, the Latest Magic Touch” presented at the Ninth European Skeptics Conference, in La Coruna, Spain, September 4-7, 1997.\nOne of the main ways magnetotherapy is applied is through the use of permanent magnets. According to proponents, you can get their benefits as follows:\n- Local application: Effects depend drastically on which pole you apply. North pole: against pain, inflammatory and infectious processes. South pole: provides strength and energy. Does your shoulder hurt? No problem. Put the north of one of the magnets on the front of your shoulder and the south of the other one on the back of it (like a sandwich).\n- General application: The aim of this method is to provide a general magnetic flow inside the body. This helps to regulate the disorders of the organism. A correct application of the general method is said to involve a deep knowledge of the polarity of the human body and its affinity with therapeutic magnets.\nProponents contend that the north pole has negative electric potential and south, positive (nonsense x 1010) and that it has been established (by whom?) that the front and the right side of the human body are positives, and back and left side, negatives. So on the right side you must apply north and on the left the south. Pay attention to the extraordinary therapeutic: if your illness is above the waist, you must correctly apply the magnets on the palms of your hands. But if your sickness is below the waist then magnets must be placed below your feet. Isn't it great?\nTheir claims. — The following collection of nonsense claims about magnetotherapy has been compiled from booklets and radio programs. In parenthesis are my comments, perhaps cynical but . . . who could help it?\n- The Earth’s magnetic field has fallen fifty percent in the last centuries and five percent in the last hundred years. This falling has provoked an increase in common diseases. This is called Magnetic Field Deficiency Syndrome. Such effect has increased because of modern life, immersed in a world of iron, steel and concrete. The modern world deprives us of the healthy influence of the natural (of course!) magnetic field necessary to maintain our bioelectric cellular equilibrium. (??)\n- Cells work better in the presence of magnetic fields. Experiments with plants prove it. (Which ones?)\n- Cells live on magnetic energy, and most of this energy is provided by water. (It is true. I feel better when I eat a juicy magnetic steak)\n- In big cities the magnetic field doesn't exist or it has been strongly modified. (So you can not use a compass in a city, I suppose)\n- Spring water is magnetized. But if you bottle it, after five days it loses its power.\n- Water has a magnetic potential (?) that we can help to recover. (This sounds Aristotelian, doesn't it?)\n- The natural phase of water is gaseous, but the Earth’s magnetic field and its uncommon properties make it liquid. (No comments. The ways of illiteracy are endless).\n- Magnetic water loses its curative properties in contact with metals. (Who doubts it?)\n- Illness is caused by losing energetic equilibrium, and magnetotherapy helps to recover it. (I think you had better plug in. Everything works better if it is plugged in)\n- Eighty percent of common diseases have their origin on Magnetic Field Deficiency Syndrome. (Why are you laughing?)\n- Magnetotherapy was known by Chinese, Egyptians, and Indians 3,500 years ago. (Neither Egyptians nor Chinese knew this therapeutic use of magnets. Egyptians probably didn't even know the existence of magnets.)\n- Magnetic water is based on the principle of electromagnetic induction discovered by Faraday. How? In their words, we have a water flow (doubtless it is a current) and a magnet, so . . .\n- Louis Pasteur studied the effect of magnets on plant growth and on fruit enzymes. (Sure?)\n- The magnetic field modifies water structure, changing some of its physical properties.\n- In the body, the magnetic field increases the dilution of oxygen in plasma, transporting more oxygen together with the hemoglobin mechanism. (Give them the Nobel Prize for Medicine!)\n- Osteoporosis is caused by the absence of the Earth’s magnetic field. Space agencies use magnetotherapy to help their astronauts recover. (Absolutely false. Osteoporosis occurs during space flight because of weightlessness. There is no more stress on the weight-bearing bones of the body when in orbit, and so calcium tends to be absorbed in the bloodstream. The same thing happens on Earth to patients who are in bed for long periods of time, and to people who have to keep one leg suspended or non-weightbearing due to disease or fractures. The “cure” for this problem seems to be to provide exercises that will periodically stress these bones, such as running on a treadmill or doing squatting exercises. Some investigators have tried electrical stimulation on legs to promote bone growth, but it has not worked so far.)\n- In 1777, two French medical doctors, Androy and Thouret, gave their approval to magnetotherapy in a session of the Royal Society of Medicine. (I don't know if it was actually passed; it seems so. In the eighteenth century, therapies included unguents, purges, and bleedings. As with homeopathy, such a harmless and inoffensive treatment was better than real medicine!)\nMedical applications. — After this wonderful exhibition of deep scientific knowledge we could expect specific treatments for different diseases. It’s amazing (well, not much from what have seen so far), that only one treatement is needed to help nearly eighty common diseases:\n- Apply the therapeutic magnets following the general method;\n- Apply the therapeutic magnets in the zone of the disease. North is for pain; south is to recover lost energy and vitality;\nDrink magnetic water. It’s so easy . . . By this method you can get cured of: abscess, acne, allergy, anaemia, arthritis, asthma, bronchitis, cellulitis, sciatic, diabetes, diarrhea, dyspepsia, stringent, frigidity, impotency, flu, herpes, headaches, zoster and a lot more!\nSome of the supposed ‘cures’ are quite obvious. For example, for stringent they recommend drinking two or three litres of magnetic water daily (if you don't succeed after drinking three litres, what else can you do?). Or to gargle with magnetic water and lemon juice if you suffer pharyngitis (my grandmother said that it is better to use lemon juice and honey . . .) Finally, it is quite impressive to learn that to cure for all child diseases we must only apply low power magnets and make them drink magnetic water.""]"	['<urn:uuid:e36a5d75-cd6a-4aa6-9435-a0260a8a8274>']	open-ended	with-premise	short-search-query	distant-from-document	single-doc	novice	2025-05-12T21:52:51.670698	5	104	1112
13	What activities were Russians and British doing in Persia in 1917?	The Russians were withdrawing from Persia while the British were fighting tribes around Shiraz and securing the border with Persia from Mesopotamia. The British were also engaged in negotiations with the Bakhtiari khans, including a payment of £2,500 made to them through the Anglo-Persian Oil Company.	"['File 469/1917 Pt 1 \'Persia: Bakhtiari affairs\' [137r] (283/535)\nThe record is made up of 1 volume (260 folios). It was created in 10 May 1915-9 Jul 1920. It was written in English. The original is part of the British Library: India Office The department of the British Government to which the Government of India reported between 1858 and 1947. The successor to the Court of Directors. Records and Private Papers.\nThis transcription is created automatically. It may contain errors.\nrThi s Document is the Property of His Britannic Majesty’s Government,\nan d should be returned to the Foreign Office when done with unless\nrequired for official ase.\nSir C, Marling\nD. 8,5 aorrio Septemher 2ncU1916,\nHo 11.20 a*m, September 3rd*1916.\nGerman plan appears to have comprised comoined\noperation between band under Kardof and force of\nChiragh AXi Khan noted brigand who was in German pay\nlast year and a contingent supplied by sisuer of\nSamsan~es~Sultaneh but plan seems to have failed for\nthe present owing to destruction of Chiragh s band\nby Russians at place 5 or 6 miles outside Ispahan and\nto Captain Noe^s successful influence with Ilkhani.\nBut considerable forces are reported to be moving in\nthe neighbourhood of Ispahan.\nNoel mistrusts Ilbegi and I think as soon as\n1 situation clears sufficiently Amir IviUjahid must be\ni replaced by Amir Jang Sirdar Assad\'s second son.\nI have some reason to suspect that party of\nZil~es~Sultan in Ispahan is endeavouring to implicate\nKhans in Teheran with hostile movement but with possiole\nexception of Sams an-es-Suitaneh who is capable of any\nfoolish vagary I think Khans here are too cautious to\nAbout this item\nThe file contains papers, mainly correspondence and India Office The department of the British Government to which the Government of India reported between 1858 and 1947. The successor to the Court of Directors. minute papers, mostly relating to the situation in the Bakhtiari [Baḵtīārī] territory in Persia [Iran], during the First World War. It includes papers concerning British negotiations with the Bakhtiari khans, and the agreement of February 1917 signed by Charles Murray Marling, HM Minister to Iran, and the principal Bakhtiari khans.\nThe file also includes papers which relate to relations between the Bakhtiaris and the Russians in Persia, and the payment of £2,500 made to the Bakhtiari khans in December 1915 by Dr Young of the Anglo-Persian Oil Company, in connection with the agreement concluded by Young with the Bakhtiari Sardars.\nThe main correspondents include: the India Office The department of the British Government to which the Government of India reported between 1858 and 1947. The successor to the Court of Directors. ; the Foreign Office; the Chief Political Officer, Basra (Sir Percy Zachariah Cox); the Government of India Foreign and Political Department; HM Minister, Tehran; HM Consul, Kerman; HM Consul, Sistan and Kain; and the Political Resident A senior ranking political representative (equivalent to a Consul General) from the diplomatic corps of the Government of India or one of its subordinate provincial governments, in charge of a Political Residency. in the Persian Gulf Historically used by the British to refer to the sea area between the Arabian Peninsula and Iran. Often referred to as The Gulf or the Arabian Gulf. .\nThe file includes a divider which gives the subject number, the year the subject file was opened, the subject heading, and a list of correspondence references by year. This is placed at the back of the correspondence.\n- Extent and format\n- 1 volume (260 folios)\nThe papers are arranged in approximate chronological order from the rear to the front of the volume.\nThe subject 469 (Persia: Bakhtiari affairs) consists of two volumes, IOR/L/PS/10/652-653. The volumes are divided into two parts, with each part comprising one volume.\n- Physical characteristics\nFoliation: the foliation sequence (used for referencing) commences at the first folio with 1 and terminates at the last folio with 260; these numbers are written in pencil, are circled, and are located in the top right corner of the recto The front of a sheet of paper or leaf, often abbreviated to \'r\'. side of each folio.\nA previous foliation sequence, which is also circled, has been superseded and therefore crossed out.\n- Written in\n- English in Latin script View the complete information for this record\nUse and share this item\n- Share this item\nFile 469/1917 Pt 1 \'Persia: Bakhtiari affairs\' [137r] (283/535), British Library: India Office Records and Private Papers, IOR/L/PS/10/652, in Qatar Digital Library <https://www.qdl.qa/archive/81055/vdc_100054148780.0x000054> [accessed 20 February 2020]\nCopy and paste the code below into your web page where you would like to embed the image.\n<meta charset=""utf-8""><a href=""https://www.qdl.qa/en/archive/81055/vdc_100054148780.0x000054"">File 469/1917 Pt 1 \'Persia: Bakhtiari affairs\' [\u200e137r] (283/535)</a> <a href=""https://www.qdl.qa/en/archive/81055/vdc_100054148780.0x000054""> <img src=""https://images.qdl.qa/iiif/images/81055/vdc_100000000419.0x000223/IOR_L_PS_10_652_0283.jp2/full/!280,240/0/default.jpg"" alt="""" /> </a>\nCopyright: How to use this content\n- File 469/1917 Pt 1 \'Persia: Bakhtiari affairs\'\n- front, back, spine, edge, head, tail, front-i, i-r:i-v, 1r:4v, 7r:15v, 16ar, 16r:71v, 76r:161r, 163r:164v, 166r:204v, 205ar:205av, 205r:260v, ii-r:ii-v, back-i\n- East India Company, the Board of Control, the India Office, or other British Government Department\n- Usage terms\n- Open Government Licence', ""The Fall of the Ottoman Empire\nSouthern Asia 1917.1107\nMiddle Eastern Theatre of World War I, Russian Revolution, end of the Ottoman Empire (7 November 1917)\nHistorical Map of South & Southwest Asia\nUnfortunately for the Allies, the new Russian Republic which had arisen after the February Revolution was weak and unstable. On November 7 (October 25 in the old-style calendar), Vladimir Lenin's revolutionary Bolshevik party seized power in Petrograd, the capital, and overthrew the government. Further armed insurrections took place across Russia.\nChanges to the map 06 July 1917 - 07 November 1917\nIn Mesopotamia, the British have advanced up the Tigris, destroying Ottoman positions at Tikrit although not permanently occupying that town. In the east, they have begun securing the border with Persia.\nIn Persia, the Russians are continuing to withdraw. The British are still fighting tribes around Shiraz.\nIn Palestine, the British have resumed their advance with Arab support in the Jordan valley.\nIn the Arabian peninsula, the Arab rebels have almost completely expelled the Ottomans. Medina still holds out, supported at the cost of sending supplies down the long and vulnerable Hejaz Railway.\nSometime between 1916 and 1917, the Aidids of Upper Asir begin to assert their independence. However Ottoman forces remain in Abha, their capital, until after the end of the War.\nIn Ethiopia, Lij Iyasu's revolt has been defeated although Iyasu himself has escaped. In British East Africa, British forces have brought an end to the revolt of Juba tribes.\nBritish Protectorates in the Persian Gulf\nThe British Residency of the Persian Gulf maintains British India influence in a number of Gulf states. These states are nominally independent - and shown as such in most atlases from the period - but have all signed treaties guaranteeing British control over their foreign affairs.\nThe Sultanate of Muscat and Oman is the only one of these states with significant international relations, having obtained trade agreements with the US and France before it signed its treaty with Britain. Maps of the time often show Trucial Oman and even Qatar as regions of Oman.\nTrucial Oman is the region to the west of Oman which collectively signed treaties with Britain. The sheikhdoms of this region are often called the Trucial States, and will become the United Arab Emirates. However at this time they have little unity, with no regional council until 1952.\nThe British Indian Empire, also known as the British Raj, is comprised of a complex of presidencies, provinces, protectorates, and agencies. Only the top level subdivisions are shown here.\nThe area under direct British rule is known as British India and made up of presidencies and provinces - a presidency simply being the name for an older province.\nOutside British India, but often included within the sphere of the presidencies/provinces, are the hundreds of protectorates or 'princely states'. These are indirectly ruled states, the largest being Hyderabad, Kashmir, and Mysore. The others are either collected into agencies - which may in turn contain other smaller agencies - or fall under the sway of the provinces.\nEntry of China into World War I\nThe Republic of China, under Premier Duan Qirui, declares war on Germany, entering World War I on the side of the Allies.\nBritish Foreign Secretary Arthur James Balfour sends a letter confirming British support for the establishment of a Jewish homeland in Palestine to Walter Rothschild, 2nd Baron Rothschild, for transmission to the Zionist Federation of Great Britain and Ireland. The letter, published on 9 November, is in contrast to British promises to the Arab independence movement.\nOn October 25 (Old Style), Bolshevik Red Guards, led by Leon Trotsky, mount an armed insurrection in Petrograd, capital of the Russian Republic, capturing several government buildings. The following day, they seize the Winter Palace, the seat of Alexander Kerensky's Provisional Government. The Bolsheviks declare a new government, the Council of People's Commissars, with Vladimir Lenin as its head. Simultaneously and over the following days, other Bolshevik uprisings take place in towns and cities across Russia.""]"	['<urn:uuid:7865184e-177b-4857-8a13-28529d94e01a>', '<urn:uuid:db4f9447-8ee7-4c7c-ae41-d0ccf55b8c7b>']	factoid	direct	concise-and-natural	distant-from-document	three-doc	expert	2025-05-12T21:52:51.670698	11	46	1516
14	What's new in genetic crop engineering, and how do farmers time harvests?	Modern genetic engineering uses CRISPR technology to create crops that are more disease-resistant and better adapted to harsh conditions. For harvest timing, farmers must carefully monitor crop maturity - for instance, cutting grass seed crops at the wrong moisture content can result in 20-30% losses, which is why precise moisture testing and following recommended moisture levels is crucial.	"['- About Extension\n- Get Involved\n- Statewide Locations\nNew Ideas Crop up on Amity Farm\nBy Mary Stewart, OSU Extension Regional Communications and Marketing Coordinator\nHelle and Bruce Ruddenklau collect a ryegrass seed sample on their farm near Amity. Rotational crops have added economic vigor.\nAbout one cup of grass seed is gathered by hand, then tested for moisture content in a lab on their farm.\nBruce adds grass seed to the Koster moisture testing equipment. Harvesting too soon, or too late, will reduce income.\nResearch and information from OSU Extension online publications guides decisions on crop management and harvest.\nAMITY, Ore.—The sun-bronzed hands of farmer Bruce Ruddenklau work furiously as he rubs a bunch of perennial ryegrass stems and seedheads until the slender grass seeds fall into a small white bucket. His wife, farmer Helle Ruddenklau, follows suit and they both continue to hand thresh and strip the heads until a cup of seeds are collected.\nThe seed sample is taken back to a simple lab in the Ruddenklau’s shop and weighed, heated for a brief time in a Koster moisture tester, then weighed again as part of the test to determine seed moisture content. The seed moisture information will help Bruce and Helle determine the best time for swathing their grass seed crop, ensuring the seed will be harvested at the optimum time. “We will need to cut this field soon,” says Bruce as he reads the dials on the equipment and consults the OSU Extension Seed Moisture guide which recommends the best swathing moisture levels by variety of seed crop. The recommended moisture level for perennial ryegrass seed is 35-43 percent.\nTIMING IS EVERYTHING\n“The information we get from Oregon State University Extension is phenomenal,” says Bruce, who is raising three varieties of grass seed and six other crops on his 1000-acre farm. Bruce and Helle have owned the productive farmland for 22 years. “If we get behind and don’t cut at the right time, we stand to lose 20 percent of the crop from loss, or, if we cut it too early we will lose 20-30 percent from seed being light and not good,” he explains. The Ruddenklaus look to the OSU Extension Seed Moisture guide to help them determine optimum seed moisture content for swathing various seed crops.\n“Measuring seed moisture content is the most reliable method to determine seed maturity and predict harvest timing in grass seed crops,” says Nicole Anderson, OSU Extension field crops faculty. Other methods, including visual observation, are not always precise enough to maximize seed yield and minimize seed losses in the field.\nOn the Ruddenklau Farm, grass seed is harvested in a two-step system. First, the standing crop is cut with a swather at the recommended seed moisture content and dried in windrows until ready for combining, which can be several days to two weeks depending on weather conditions. Second, the swathed crop is threshed when dry by using combines with belt pickup attachments.\nDEVELOPING NEW WHEAT VARIETIES AND BEST MANAGEMENT PRACTICES\nWillamette Valley farmers depend on new wheat varieties to stay on the leading edge of the agriculture product market, and to find varieties that have high yield potential, can tolerate a high rainfall environment, and have genetic resistance to fungal diseases and insect pests. “To develop new varieties, OSU Extension personnel work closely with both public and private wheat breeding programs to conduct variety trials across the region, both at OSU research stations and on commercial fields under real life conditions,” says Nicole.\nOngoing education is another important piece in farming success. “Without question we learn important information from OSU Extension’s crop tours and field days,” says Bruce. “You have to make the effort and be there.” A recent field day in the North Willamette Valley taught growers best practices to use when applying fertilizer to farmland in order to reduce the loss of nitrogen through volatilization.\n“Extension agents are a sounding board,” says Helle. “They keep us informed with quick updates and news alerts. They provide excellent information that gives us the independence to make our own informed farm management decisions.”\nROTATIONAL CROPS SHOW PROMISE\nThe Ruddenklaus have benefitted by rotating diverse crops – growing and harvesting broadleaf crops in a field followed by grass crops. “This has really been an advantage,” says Helle. Instead of relying on just grass seed crops, they are raising crimson clover and other broadleaf crops such as radish and peas for the Japanese sprout market and as cover crops for Midwest farms.\nThey also produce sugar beet seed, meadowfoam, and wheat, spreading the workload across most of the growing season and making efficient use of the land, labor and equipment.\n“Some of the rotational crops have become main stream,” says Helle. ""We needed some viable crop alternatives when the grass seed industry went down. The Midwest cover crop program has come along at a good time for Willamette Valley growers.""\nThe new choices and success of the rotational crops have infused an element of enjoyment into the annual process of deciding what to grow. “Now it’s a fun thing,” says Helle. “It used to be that you had to raise that one crop, and it wasn’t going to make us much money but it was investing in the future. Now, the rotational crops are good crops in themselves and it is exciting!” she adds.\n###Photos by Mary Stewart\nLearn More:Using Seed Moisture as a Harvest Management Tool\nThe OSU Extension Service provides a variety of information on agriculture production for farms of all sizes. Resources include tips and advisories, faculty consultations, videos, podcasts, crop tours and field days, how-to publications, and newsletters.', 'Are you keen on understanding the modern twists revolutionizing traditional farming practices? From precision agriculture to vertical farming, innovative farming methods are reshaping the agricultural sector globally. This article will explore 10 modern farming techniques that are pushing the boundaries and creating sustainable, productive, and efficient agriculture.\n1. Precision Agriculture – The Tech-Driven Farming Revolution\nPrecision agriculture is a game-changer in the farming industry. By leveraging technologies such as GPS, remote sensing, and the Internet of Things (IoT), farmers can now analyze and measure variations in crop yield and soil properties accurately. The result? Enhanced efficiency, maximized yield, and minimized environmental impact.\n2. Vertical Farming – Reimagining Urban Agriculture\nOne of the significant agricultural breakthroughs of the 21st century, vertical farming, allows crop production in vertically stacked layers within controlled environments. This innovative farming method makes urban farming possible, significantly reduces the need for pesticides, and guarantees year-round crop production.\n3. Hydroponics – The Power of Soil-less Farming\nHydroponics is a revolutionary soil-less cultivation technique, growing plants in nutrient-rich water solutions. With its incredible water efficiency and the ability to farm all year round, hydroponics is setting a new standard in sustainable agriculture.\n4. Aeroponics – Elevating Agriculture to New Heights\nSimilar to hydroponics but with even greater water efficiency, aeroponics grows plants in air or mist environments without soil. With roots misted with nutrients, water, and oxygen, this cutting-edge technique delivers impressive yields in limited spaces.\n5. Aquaponics – Where Aquaculture Meets Hydroponics\nBy integrating conventional aquaculture with hydroponics, aquaponics creates a closed-loop, sustainable farming system. Fish waste serves as an organic nutrient source for plants, while the plants naturally filter the water for the fish.\n6. Genetic Engineering – Crafting The Perfect Crop\nGenetic engineering opens the door to unprecedented crop improvements. By modifying plant DNA, scientists can breed crops that are more nutritious, resistant to pests and diseases, and adaptable to varying environmental conditions.\n7. Drone Technology – The Sky’s The Limit in Agriculture\nDrone technology is taking agriculture to new heights. From monitoring crop health to applying fertilizers and pesticides and even planting seeds, drones are increasing agricultural efficiency and reducing manual labor.\n8. Robotics and AI – Automating The Field of Agriculture\nRobots are now shouldering many of the burdens of farm work, performing tasks from planting to harvesting and weeding. Combined with AI’s ability to analyze data from various sources, this tech combo is setting new productivity standards in farming.\n9. Regenerative Agriculture – Going Beyond Sustainability\nRegenerative agriculture isn’t just about maintaining the status quo – it’s about improving it. This modern farming technique emphasizes soil regeneration, biodiversity, and holistic livestock management, resulting in resilient farms and healthier ecosystems.\n10. CRISPR Technology – Precision Gene Editing in Agriculture\nCRISPR technology is one of the most groundbreaking advancements in genetic engineering. This gene-editing tool has immense potential to create crops that are more disease-resistant, better adapted to harsh weather conditions, and nutritionally superior.\nIn conclusion, modern farming methods are radically transforming the agricultural sector. While they present challenges and raise critical discussions, these technologies and practices hold the promise of a more efficient, sustainable, and productive future for farming. As we continue to explore and refine these innovative techniques, the future of agriculture looks brighter than ever.']"	['<urn:uuid:92eb5565-2957-41c7-9a22-d3970b1718d4>', '<urn:uuid:6152aa95-ec6b-49c3-b8f1-1bc6a79f4b9e>']	factoid	with-premise	concise-and-natural	similar-to-document	multi-aspect	novice	2025-05-12T21:52:51.670698	12	58	1486
15	How do I make iced tea at home, and what health benefits can I get from it?	For iced tea, you can either infuse tea directly in cold water in the fridge for 5 hours (for black tea) or overnight (for green and fruit teas). This cold brewing method prevents bitterness from tannins. You can then sweeten and add fruits or mint as desired. As for health benefits, studies have shown that increasing tea consumption by 3 cups per day is associated with reduced risks of coronary heart disease, cardiac death, stroke, and overall mortality.	"['To enhance the flavour and overall experience of your tea, the correct brewing method must be used. Please check below:\nBLACK TEA: Use one teaspoon per cup, generally a flat teaspoon for a BOP (broken leaf) or a medium to heaped teaspoon for a larger leaf. Brew for 3-5 minutes at 100ºC. Milk can be added to most black teas, and you can adjust your brew time depending on desired strength.\nFor iced teas, allow to infuse directly in cold water in the fridge only for approximately 5 hours (pop it in the fridge in the morning and it will be ready for the afternoon). This will limit the amount of tannin in your tea, and will prevent a bitter taste from occurring in your iced tea. Sweeten with sugar syrup as needed, and add desired fruits, mint etc.\nGREEN TEA: Use a half to flat teaspoon per cup. Green teas should be brewed around 70-80ºC for 2-3 minutes. If you do not have a kettle with temperature adjustments, you can easily create the desire temperature by adding approximately 1cm of cold water to your cup/pot, flushing the cold water over the tea leaves, before adding hot water. This will prevent any bitterness or tannin from ruining the taste of your tea.\nAllow to brew for 2-3 minutes before removing leaves and setting aside. When you are ready for another cup, return those same leaves to your cup/pot and add hot water directly on top for your second cup. Green teas can be brewed multiple times. So after the initial brew at a lower temperature, you can continue to re brew your leaves up to 4-5 times with hot water.\nIf you find your green tea is tasting too bitter, either drop the temperature lower or decrease the brewing time. Green teas should not be allowed to ""stew"".\nFor iced teas, infuse in cold water overnight in the fridge, remove tea leaves in the morning and sweeten and garnish as desired.\nWHITE TEA: Use one heaped teaspoon per cup. Brew for 3-5 minutes at 80ºC for your first brew. Like green teas, white teas can then be brewed multiple times using boiling water for the subsequent brews.\nROOIBOS: Use one teaspoon per cup. Brew for 3-5 minutes at 100ºC. Milk can be added if desired.\nHERBAL BLENDS: Use one teaspoon per cup. Brew for 3-5 minutes at 100ºC. You can re brew some herbals a second or third time with boiling water, depending on the blend.\nFRUIT INFUSIONS: Use one teaspoon per cup. Brew for 3-5 minutes at 100ºC. Lovely when sweetened with honey if desired.\nTo create an iced tea, simply brew in cold water in the fridge overnight.\nCHAI: there are a couple of ways that a thick creamy chai can be made.\nIf making on the stove top, the quickest way is to use a small saucepan, use a couple scoops of tea with a small amount of water (about an inch or so of water in the pot) bring the water up to boil, add the tea and turn down and allow to simmer for a few minutes. Then add your milk (one or two cups), and any honey, sugar or even condensed milk that you\'d like to sweeten to taste. Simmer for a couple minutes while stirring and then strain into your cup!\nIf that way is a bit too strong or releases too much tannin/bitterness (depending on what tea base you are using) you can put milk, tea and a small amount of water all together at once in a pot, bring up to a medium heat, then turn down and continue to stir for a few minutes. Add your sweetener and then strain into cup. (This way takes a bit longer, as the tea doesn\'t draw as easily in milk, but it will reduce tannin/bitterness).\nThe final way, if you have a coffee machine/milk frother, boil the jug and add a small amount of hot water and a teaspoon of tea to a metal milk jug. Give it a couple of minutes to brew in the water, then add milk and sweetener, and steam or froth the blend all together. Strain into your cup, and dress with a shake of ground cinnamon or nutmeg on top!\nAt the end of the day it comes down to personal taste and having a little play to get it just right for you.', 'Studies that investigated the association between tea consumption and the risk of major cardiovascular events have reported inconsistent results. We conducted a meta-analysis of prospective observational studies in order to summarize the evidence regarding the association between tea consumption and major cardiovascular outcomes or total mortality. In July 2014, we performed electronic searches in PubMed, EmBase, and the Cochrane Library, followed by manual searches of reference lists from the resulting articles to identify other relevant studies. Prospective observational studies that reported effect estimates, with 95 % confidence intervals (CIs), for coronary heart disease (CHD), stroke, cardiac death, stroke death, or total mortality for more than two dosages of tea consumption were included. A random-effects meta-analysis was performed to determine the risk of major cardiovascular outcomes associated with an increase in tea consumption by 3 cups per day. Of the 736 citations identified from database searches, we included 22 prospective studies from 24 articles reporting data on 856,206 individuals, and including 8,459 cases of CHD, 10,572 of stroke, 5,798 cardiac deaths, 2,350 stroke deaths, and 13,722 total deaths. Overall, an increase in tea consumption by 3 cups per day was associated with a reduced risk of CHD (relative risk [RR], 0.73; 95 % CI: 0.53–0.99; P = 0.045), cardiac death (RR, 0.74; 95 % CI: 0.63–0.86; P < 0.001), stroke (RR, 0.82; 95 % CI: 0.73–0.92; P = 0.001), total mortality (RR, 0.76; 95 % CI: 0.63–0.91; P = 0.003), cerebral infarction (RR, 0.84; 95 % CI: 0.72–0.98; P = 0.023), and intracerebral hemorrhage (RR, 0.79; 95 % CI: 0.72–0.87; P < 0.001), but had little or no effect on stroke mortality (RR, 0.93; 95 % CI: 0.83–1.05; P = 0.260). The findings from this meta-analysis indicate that increased tea consumption is associated with a reduced risk of CHD, cardiac death, stroke, cerebral infarction, and intracerebral hemorrhage, as well as total mortality.\nThis is a preview of subscription content, log in to check access.\nBuy single article\nInstant access to the full article PDF.\nPrice includes VAT for USA\nSubscribe to journal\nImmediate online access to all issues from 2019. Subscription will auto renew annually.\nThis is the net price. Taxes to be calculated in checkout.\nRimm ER, Katan MB, Ascherio A, Stampfer MJ, Willett WC. Relation between intake of flavonoids and risk for coronary heart disease in male health professionals. Ann Intern Med. 1996;125:384–9.\nSone T, Kuriyama S, Nakaya N, Hozawa A, Shimazu T, Nomura K, Rikimaru S, Tsuji I. Randomized controlled trial for an effect of catechin-enriched green tea consumption on adiponectin and cardiovascular disease risk factors. Food Nutr Res 2011;55. doi: 10.3402.\nFood and Agriculture Organization of the United Nations. Food balance sheets. 1996;1994–6.\nMink PJ, Scrafford CG, Barraj LM, Harnack L, Hong CP, Nettleton JA, Jacobs DR Jr. Flavonoid intake and cardiovascular disease mortality: a prospective study in postmenopausal women. Am J Clin Nutr. 2007;85:895–909.\nWang X, Ouyang YY, Liu J, Zhao G. Flavonoid intake and risk of CVD: a systematic review and meta-analysis of prospective cohort studies. Br J Nutr. 2014;111:1–11.\nde Koning Gans JM, Uiterwaal CS, van der Schouw YT, Boer JM, Grobbee DE, Beulens JW, Verschuren WM. Tea and coffee consumption and cardiovascular morbidity and mortality. Arterioscler Thromb Vasc Biol. 2010;30:1665–71.\nSuzuki E, Yorifuji T, Takao S, Komatsu H, Sugiyama M, Ohta T, Ishikawa-Takata K, Doi H. Green tea consumption and mortality among Japanese elderly people: the prospective Shizuoka elderly Cohort. Ann Epidemiol. 2009;19:732–9.\nMukamal KJ, Maclure M, Muller JE, Sherwood JB, Mittleman MA. Tea consumption and mortality after acute myocardial infarction. Circulation. 2002;105:2476–81.\nGeleijnse JM, Launer LJ, Van der Kuip DA, Hofman A, Witteman JC. Inverse association of tea and flavonoid intakes with incident myocardial infarction: the Rotterdam Study. Am J Clin Nutr. 2002;75:880–6.\nWoodward M, Tunstall-Pedoe H. Coffee and tea consumption in the Scottish Heart Health Study follow up: conflicting relations with coronary risk factors, coronary disease, and all cause mortality. J Epidemiol Community Health. 1999;53:481–7.\nHertog MG, Sweetnam PM, Fehily AM, Elwood PC, Kromhout D. Antioxidant flavonols and ischemic heart disease in a Welsh population of men: the Caerphilly Study. Am J Clin Nutr. 1997;65:1489–94.\nSato Y, Nakatsuka H, Watanabe T, Jousilahti P, Antikainen R, Tuomilehto J. Possible contribution of green tea drinking habits to the prevention of stroke. Tohoku J Exp Med. 1989;157:337–43.\nKeli SO, Hertog MG, Feskens EJ, Kromhout D. Dietary flavonoids, antioxidant vitamins, and incidence of stroke: the Zutphen study. Arch Intern Med. 1996;154:637–42.\nHertog MG, Feskens EJ, Hollman PC, Katan MB, Kromhout D. Dietary antioxidant flavonoids and risk of coronary heart disease: the Zutphen Elderly Study. Lancet. 1993;342:1007–11.\nArab L, Liu W, Elashoff D. Green and Black Tea Consumption and Risk of Stroke: a meta-analysis. Stroke. 2009;40:1786–92.\nPeters U, Poole C, Arab L. Does tea affect cardiovascular disease? A meta-analysis. Am J Epidemiol. 2001;154:495–503.\nMoher D, Liberati A, Tetzlaff J, Altman DG. PRISMA group. Preferred Reporting Items for systematic reviews and meta-analyses: the PRISMA statement. Plos Med. 2009;6:e1000097.\nWells G, Shea B, O’Connell D. The Newcastle-Ottawa Scale (NOS) for assessing the quality of nonrandomised studies in meta-analyses. Ottawa (ON): Ottawa Hospital Research Institute. 2009. http://www.ohri.ca/programs/clinical_epidemiology/oxford.htm.\nHiggins JP, Green S. Cochrane handbook for systematic reviews of interventions, version 220.127.116.111. www.cochrane-handbook.org.\nDerSimonian R, Laird N. Meta-analysis in clinical trials. Control Clin Trials. 1986;7:177–88.\nAdes AE, Lu G, Higgins JP. The interpretation of random-effects metaanalysis in decision models. Med Decis Mak. 2005;25:646–54.\nOrsini N, Bellocco R, Greenland S. Generalized least squares for trend estimation of summarized dose–response data. Stata J. 2006;6:40–57.\nGreenland S, Longnecker MP. Methods for trend estimation from summarized dose–response data, with applications to meta-analysis. Am J Epidemiol. 1992;135:1301–9.\nDeeks JJ, Higgins JPT, Altman DG. Analyzing data and undertaking meta-analyses. In: Higgins J, Green S, editors. Cochrane handbook for systematic reviews of interventions 5.0.1. Oxford: The Cochrane Collaboration; 2008. chap 9:9.5.4.\nHiggins JPT, Thompson SG, Deeks JJ, Altman DG. Measuring inconsistency in meta-analyses. BMJ. 2003;327:557–60.\nHiggins JP, Thompson SG. Quantifying heterogeneity in a meta-analysis. Stat Med. 2002;21:1539–58.\nTobias A. Assessing the influence of a single study in meta-analysis. Stata Tech Bull. 1999;47:15–7.\nEgger M, Davey Smith G, Schneider M, Minder C. Bias in meta-analysis detected by a simple, graphical test. BMJ. 1997;315:629–34.\nBegg CB, Mazumdar M. Operating characteristics of a rank correlation test for publication bias. Biometrics. 1994;50:1088–101.\nKokubo Y, Iso H, Saito I, Yamagishi K, Yatsuya H, Ishihara J, Inoue M, Tsugane S. The impact of green tea and coffee consumption on the reduced risk of stroke incidence in Japanese population: the Japan public health center-based study cohort. Stroke. 2013;44:1369–74.\nLarsson SC, Virtamo J, Wolk A. Black tea consumption and risk of stroke in women and men. Ann Epidemiol. 2013;23:157–60.\nLeurs LJ, Schouten LJ, Goldbohm RA, van den Brandt PA. Total fluid and specific beverage intake and mortality due to IHD and stroke in the Netherlands Cohort Study. Br J Nutr. 2010;104:1212–21.\nMineharu Y, Koizumi A, Wada Y, Iso H, Watanabe Y, Date C, Yamamoto A, Kikuchi S, Inaba Y, Toyoshima H, Kondo T, Tamakoshi A. JACC study group: coffee, green tea, black tea and oolong tea consumption and risk of mortality from cardiovascular disease in Japanese men and women. J Epidemiol Community Health. 2011;65:230–40.\nLopez-Garcia E, Rodriguez-Artalejo F, Rexrode KM, Logroscino G, Hu FB, van Dam RM. Coffee consumption and risk of stroke in women. Circulation. 2009;119:1116–23.\nTanabe N, Suzuki H, Aizawa Y, Seki N. Consumption of green and roasted teas and the risk of stroke incidence: results from the Tokamachi-Nakasato cohort study in Japan. Int J Epidemiol. 2008;37:1030–40.\nLarsson SC, Männistö S, Virtanen MJ, Kontto J, Albanes D, Virtamo J. Coffee and tea consumption and risk of stroke subtypes in male smokers. Stroke. 2008;39:1681–7.\nKuriyama S, Shimazu T, Ohmori K, Kikuchi N, Nakaya N, Nishino Y, Tsubono Y, Tsuji I. Green tea consumption and mortality due to cardiovascular disease, cancer, and all causes in Japan: the Ohsaki Study. JAMA. 2006;296:1255–65.\nSesso HD, Paffenbarger RS Jr, Oguma Y, Lee IM. Lack of association between tea and cardiovascular disease in college alumni. Int J Epidemiol. 2003;32:527–33.\nKlatsky A, Armsterong MA, Frienman G. Coffee, tea, and mortality. Ann Epidemiol. 1993;3:375–81.\nLopez-Garcia E, van Dam RM, Willett WC, Rimm EB, Manson JE, Stampfer MJ, Rexrode KM, Hu FB. Coffee consumption and coronary heart disease in men and women: a prospective cohort study. Circulation. 2006;113:2045–53.\nSesso HD, Gaziano JM, Liu S, Buring JE. Flavonoid intake and the risk of cardiovascular disease in women. Am J Clin Nutr. 2003;77:1400–8.\nYochum L, Kushi LH, Meyer K, Folsom AR. Dietary flavonoid intake and risk of cardiovascular disease in postmenopausal women. Am J Epidemiol. 1999;149:943–9.\nBidel S, Hu G, Qiao Q, Jousilahti P, Antikainen R, Tuomilehto J. Coffee consumption and risk of total and cardiovascular mortality among patients with type 2 diabetes. Diabetologia. 2006;49:2618–26.\nDuvall S, Tweedie R. A nonparametric ‘‘trim and fill’’ method for assessing publication bias in meta-analysis. J Am Stat Assoc. 2000;95:89–98.\nSchlesselman JJ. Case-control studies. Design, conduct, analysis. New York: Oxford University Press; 1982.\nde Whalley CV, Rankin SM, Hoult JR, Jessup W, Leake DS. Flavonoids inhibit the oxidative modification of low density lipoproteins by macrophages. Biochem Pharmacol. 1990;39:1743–50.\nBasu A, Sanchez K, Leyva MJ, Wu M, Betts NM, Aston CE, Lyons TJ. Green tea supplementation affects body weight, lipids, and lipid peroxidation in obese subjects with metabolic syndrome. J Am Coll Nutr. 2010;29:31–40.\nSalonen JT, Yla-Herttuala S, Yamamoto R, Butler S, Korpela H, Salonen R, Nyyssönen K, Palinski W, Witztum JL. Autoantibody against oxidised LDL and progression of carotid atherosclerosis. Lancet. 1992;339:883–7.\nBergmark C, Wu R, de Faire U, Lefvert AK, Swedenborg J. Patients with early-onset peripheral vascular disease have increased levels of autoantibodies against oxidized LDL. Arterioscler Thromb Vasc Biol. 1995;15:441–5.\nThis study was funded by The Ministry of Science and Technology of China (2008ZX10002-007, 2008ZX10002-018, 2008ZX10002-025), The Leading Talents of Science in Shanghai 2010 (022), The Key Discipline Construction of Evidence-Based Public Health in Shanghai (12GWZX0602), The National Science Foundation of China (81373105). The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.\nConflict of interest\nNone of the author has a conflict of interest.\nChi Zhang, Ying-Yi Qin, Xin Wei and Fei-Fei Yu have contributed equally to this article.\nAbout this article\nCite this article\nZhang, C., Qin, Y., Wei, X. et al. Tea consumption and risk of cardiovascular outcomes and total mortality: a systematic review and meta-analysis of prospective observational studies. Eur J Epidemiol 30, 103–113 (2015). https://doi.org/10.1007/s10654-014-9960-x\n- Cardiovascular disease\n- Total mortality']"	['<urn:uuid:bdca0b03-9645-4000-8291-21512c6ee4c5>', '<urn:uuid:8e1405a8-1fb7-41ff-81b8-38d7dc13f964>']	factoid	with-premise	concise-and-natural	distant-from-document	multi-aspect	novice	2025-05-12T21:52:51.670698	17	78	2476
16	system quality improvement method origin benefits	The DMAIC methodology originated at Motorola in 1986 to compete with Japanese quality standards, and was later adopted by GE in the 1990s. It offers concrete benefits by providing a structured approach to solving business problems through specific tools at each phase (like Process Flowcharts, Pareto Charts, and Control Charts), while reducing defects and improving quality - as measured by Six Sigma levels, where higher sigma values indicate fewer defects per million opportunities.	['It’s not an exaggeration to say that DMAIC is a method for solving almost any kind of business problem. This may seem like hyperbole, until you sees exactly what’s involved in this exacting methodology.\nThe name itself is, as one could guess, an acronym: D is for Define; M is for Measure; A is for Analyze; I is for Improve: and C is for Control. Thus, each letter stands for a phase you take in trying to solve a problem. Each phase, incidentally, is broken down into different steps.\nThis way of solving problems is far from academic. It has been tested long enough in the real world to be shown to be highly effective in resolving a wide range of initially bewildering business problems. However, with that being said, the dmaic process is most suitable for following types of business problems:\n- · One that is an obvious problem, rather than one where there is some disagreement that things need to be improved.\n- · One that is worth solving, rather than focusing on fixing things that aren’t broken. A problem is considered worthy of time and attention if the solution will potentially increase revenues, slash costs, or improve efficiency.\n- · One where data can be collected; a quantifiable problem rather than a philosophical or qualitative problem.\nAlthough DMAIC appears linear and sequential in theory, it’s not exactly a step-by-step approach in practice. Since problem-solving is often a process of discovery and insights, iteration is usually necessary for the problem to be resolved.\nSuppose, for example, you are trying to keep your cloud services secure. When you get to the Analysis phase, you might discover that you don’t have enough information on all the biggest threats that could affect security. You then iterate back to the earlier stage, Measure, where you identify and collect the data you forgot or that you didn’t realize you needed.\nLet’s take a look at each DMAIC phase and the tools you could use for that stage:\nD or Define Phase: This is the stage where you get clear on the nature of your project and your customers. What are your project goals? Who are your internal and external customers? What are your deliverables? At the Define Phase, your choices of tools include: Project Charter, Process Flowchart, SIPOC Diagram, Stakeholder Analysis, DMAIC Work Breakdown Structure, CTQ Definitions, and Voice of the Customer Gathering.\nM or Measure Phase: Here is where you will quantify the problem. You need to measure current performance so that you can measure the process. At the Measure Phase, your choices of tools include: Process Flowchart, Data Collection Plan/Example, Benchmarking, Measurement System Analysis/Gage R&R, Voice of the Customer Gathering, and Process Sigma Calculation.\nA or Analyze Phase: You now analyze and figure out the root cause or causes of the problem or defects. At the Analyze Phase, your choices of tools include: a Histogram, Pareto Chart, Time Series/Run Chart, Scatter Plot, Regression Analysis, Cause and Effect/Fishbone Diagram, 5 Whys, Process Map Review and Analysis, Statistical Analysis, Hypothesis Testing (Continuous and Discrete), and Non-Normal Data Analysis. It’s a lot of tools, but remember, you don’t have to use them all; you will probably only need a few tools to analyze the particular problem you’re working on resolving.\nI or Improve Phase: You are now ready to see how you can improve the process by eliminating the issues that are causing the setbacks in the business issue. At the Improve Phase, your choices of tools include: Brainstorming, Mistake Proofing, Design of Experiments, Pugh Matrix, QFD/House of Quality, Failure Modes and Effects Analysis (FMEA), and Simulation Software.\nC or Control Phase: Finally, you are now at the point where you work out how to control future processes. At the Control Phase, your choices of tools include: Process Sigma Calculation, Control Charts (Variable and Attribute), Cost Savings Calculations, and Control Plan.\nAfter reviewing what goes on at each stage, you’re now in a position to appreciate the reason why the DMAIC methodology works as well as it does. Since it is so structured and rigorous, there are different degrees of Six Sigma expertise measured by belts, from white belt to black belt and then beyond to master black belt. Besides understanding all the steps to be taken at each phase, each tool has to be understood clearly enough to put to good use when needed.', 'Lean Six Sigma and a sample application\nLean Six Sigma is a concept that aims to improve process performance by minimizing waste and reducing variations. It is a method that combines Lean Manufacturing, Lean Enterprise and Six Sigma principles to eliminate waste and improve quality.\nThe origins of Lean Six Sigma can be traced back to 1986 when Motorola came up with strategies to compete with higher quality Japanese products. Japan used the Kaizen approach (continuous improvement) in product development to produce world-class products of high quality.\nIn the 1990’s, an American businessman called Larry Bossidy introduced Six Sigma in Manufacturing and soon after he was engaged to introduce the concept in GE.\nIn early 2000’s the two concepts of Lean Manufacturing (Reduction of waste) and Six Sigma (higher process quality leading to reduced variability) came together as a single concept called Lean Six Sigma. The concept then found acceptance in other industries such as Healthcare, Finance, Retail and Supply chain etc.\nLean focusses on eight kinds of waste (Muda is Japanese word for waste) inherent in processes;\nSix Sigma focuses on improving the quality of process outputs by identifying and removing the causes of defects and minimising variability in processes.\nLean Six Sigma aims to achieve continuous flow of quality outcomes, by exposing constraints between process steps and reducing variability between and within the process steps through a cycle of iterative improvements. Lean Six Sigma uses the DMAIC (Define, Measure, Analyse, Improve and Control) phases similar to Six Sigma.\nBasic Concepts of Six Sigma\nSix Sigma quality is a statistical term used to indicate how well a process is controlled in terms of its variability from the mean. It is a fundamental nature of any process that over time and scale, variations will creep in due to a variety of reasons or factors. The aim of Six Sigma is to keep the process running within acceptable limits from a mean (or arithmetic average of a process data set).\nThe word Sigma ( σ ) is the standard deviation or the spread around the mean or central tendency. In simple terms, Six Sigma quality performance means 3.4 defects per million opportunities. It is important to note that not all processes, products or systems need to function at Six Sigma quality level. Other than for critical processes involving high safety requirements, such as healthcare, pharmaceuticals, airplanes, manufacturing, etc. it is enough for most processes to function at 3 Sigma or 4 Sigma. The trade-off between achieving Six Sigma or lower levels of Sigma is simply cost and often it is not practical or cost-effective to aim for a high level of Sigma. The table below illustrates the number of defects per million opportunities (DMPO) at various levels of Sigma. It is easily evident as to how efficient processes need to be at Six Sigma level.\nSigma Level DMPO\n2 σ 308,537\n3 σ 66,807\n4 σ 6,210\n5 σ 233\n6 σ 3.4\nLean Six Sigma Case Study\nThe objective of this case study is to illustrate how to apply Six Sigma thinking and concepts to organizational problems and processes.\nImagine a retail organization that uses disparate core systems such as CRM (Customer relationship management), ERP (Enterprise Resource Planning), Analytics and Financial Accounting to run its business. This organization has 100,000 unique Customer master records that are regularly referenced in sales, order management, delivery, invoicing and accounts receivable transactions. Each Customer master record has 10 attributes associated with it as shown below;\nCustomer Master Records (100,000 unique records)\nCustomer Pricing Code\nThis structure implies that there are 1 million elements associated with Customer master data.\nThese Customer master records are created, referenced and updated separately by different individuals, in different departments and business units, depending on their role and function. For example, the Finance Department may manage elements of Customer master data relating to invoicing and accounts receivable. The Sales team may manage elements relating to Customer orders. As is typical in many organizations and situations, disparate systems and independent work functions cause the following issues with Master data;\nDuplicated master data across systems that are out of sync\nWasted effort in data maintenance\nErrors that get accumulated over time because of data changes made in multiple systems\nBusiness risk arising from poor governance, etc.\nDMAIC Approach to improve the process of data management\nThe DMAIC (Define, Measure, Analyse, Improve and Control) concept can be applied to the above case problem, as follows;\nDefine (The Problem)\nMaster Data, maintained separately in multiple business systems, has been observed to contain an unacceptable level of errors (defects) causing unnecessary manual intervention (extra processing) that is costing the organization money, business responsiveness and customer satisfaction.\nMeasure (The Process parameters and Sigma)\nAn important step in Six Sigma Analysis is to measure the key operating parameters of the process in consideration to understand current levels of Sigma (Standard Deviation from mean). The table below shows the impact of errors (Defects per million elements) in terms of cost and time. An error can be broadly defined as any situation relating to any Customer master data element that requires changes to data arising from any non-business driven reason. Sigma (standard deviation) can be determined by sampling in a specific section or department or the entire organization at data element levels or entire Customer Master record levels.\nIt has been assumed that it takes an average of 10 minutes to fix an error at a cost of $50/hour. These assumptions can be validated in the Analysis stage by end users or by Six Sigma experts.\nLet us assume that sampling shows there are likely to be 200,000 errors (20% of the total volume of Customer Master data elements). From the table, we can see that the current process of managing customer master data reveals a Sigma between 2 and 3, implying it is costing the organization at least $556,725 in remediating these process errors. It is useful to note that data volume may also grow at 20% year on year and with growth in master data volume there is also an increase in DMPO.\nAnalysis (Why are there errors in the process?)\nThe errors in the process may be occurring due to one or a combination of the following reasons;\nAsynchronous editing of Customer master data elements in different systems\nLack of a concept of Master data and downstream systems for data leading to uncontrolled changes\nLack of formal data governance policies and procedures defining how Master data is created and edited\nImprove (How can the process be improved?)\nThe process for managing Customer master data and its elements can be improved through several options;\nNominating one of the systems (CRM or ERP or Financial or Analytics systems) as the Master system for Customer data and setting up integrations between that designated Master system to propagate changes\nImplementing a centralized, organization-wide Master Data Management (MDM) system to manage and control all Customer master data. This MDM system will then propagate all changes to master data to all other systems referencing that data through integrations (This may be a costlier option but improve process accuracy the most and help achieve 4 σ or 5 σ efficiencies)\nSetting up a manual governance process to manage changes to Master data in all systems in a coordinated way (This may be the cheapest option but may not improve process accuracy significantly and in a sustained manner)\nControl (How can the process be controlled?)\nRegardless of which option is chosen to improve the process efficiencies, it is important to ensure that process parameters are regularly measured and action taken to remediate. This is done by implementing good data governance initiatives.\nLean Six Sigma is a concept that aims to improve process performance and efficiencies by reducing waste and eliminating errors. It relies on collaboration amongst team members to achieve that. Lean Six Sigma uses the DMAIC (Define, Measure, Analyse, Improve and Control) phases to reduce errors. As such it is a evidence-based, data driven approach to improvement that focuses on defect prevention. Lean Six Sigma initiatives improve customer satisfaction and profitability by reducing variation, waste and cycle time while promoting work standardisation and flow.\nIt is recommended that organizations new to the Lean Six Sigma begin by implementing the Lean approach to make the workplace as effective and efficient as possible, reducing waste and using value stream mapping to improve throughput. When process problems persist, the more technical Six Sigma statistical tools can be applied by a team of specialists in various areas of the overall process.']	['<urn:uuid:e3ef5fe8-8799-4dee-ac0b-ecbcd848d566>', '<urn:uuid:02bb5903-0b04-4b5d-bf7f-7f6c52131372>']	factoid	direct	short-search-query	distant-from-document	multi-aspect	novice	2025-05-12T21:52:51.670698	6	73	2155
17	how rna splicing works in normal cells and what happens when splicing goes wrong in cancer	In normal cells, RNA splicing is a process where introns (non-coding sequences) are removed from messenger RNA after a gene is transcribed, leaving only the coding sequences (exons) that will be used to make proteins. Through alternative splicing, cells can create different protein variations from the same gene, which is essential for producing our full range of proteins. However, when splicing goes wrong in cancer, several problems can occur. Abnormal patterns of alternative splicing can lead to the production of incorrect proportions of proteins, as seen in metastatic breast cancer where a variant of the protein plectin is overproduced. Additionally, a problem called intron retention can occur, where introns that should be removed are accidentally left in place. This is particularly problematic when it affects tumor suppressor genes, as it can prevent these genes from properly regulating cell growth, leading to uncontrolled cell division and cancer development.	['What kills most people who die from cancer is not the initial tumor. It’s the intolerable disease burden on the body that arises when tumor cells continually expand their numbers after spreading to different organs.\nIn comparison to what is already known about specific mutations that drive early-stage cancer growth—several now targeted by treatments—researchers understand less about the continual accumulation of new abnormalities within tumors and how they eventually generate cancerous cells capable of spreading to other areas of the body through a development called metastasis.\nIn recent years, scientists have made an increasing number of discoveries about metastasis, raising hopes that better treatments might be developed to save the lives of those with advanced cancers. It turns out that one type of abnormality that often pops up in metastatic tumor cells affects a process known as alternative splicing.\nRNA Structure Drives Aberrant Splicing\nAlternative splicing determines which protein will be produced by a specific gene once the DNA of that gene has been transcribed into RNA, the coding template that factories within cells use to make proteins. Humans have more than 20,000 genes, relatively few in comparison to the number of proteins required for normal development and function. Cells can splice and recombine the RNA template created from a given gene in a variety of ways, resulting in a set of alternative templates that each produce a unique protein. We rely on this process to make all the genetic “recipes” required to produce our entire repertoire of proteins.\nScientists have discovered that abnormal patterns of alternative splicing occur in cancer and result in the production of abnormal proportions of these related, yet distinct proteins.\nIn a new study published online in Science on May 14, researchers led by Hani Goodarzi, Ph.D., an assistant professor in the Department of Biochemistry and Biophysics and a member of the UCSF Helen Diller Family Comprehensive Cancer Center, determined that a specific variant of a protein called plectin is overproduced as a result of abnormal alternative splicing in many cases of metastatic breast cancer. In healthy tissue, plectin plays a role in determining elasticity and mechanical integrity.\nFirst, the UCSF scientists used computational techniques they developed to analyze RNA sequences and structures from both highly metastatic and less metastatic breast cancer cells. They then identified specific RNA structural characteristics that defined a subset of alternative splicing sites associated with metastatic cancer.\n“We found a previously unknown RNA structure that acts to enhance splicing, and determined that this structural element drives aberrant alternative splicing in highly metastatic breast cancer,” Goodarzi said.\nStopping Metastasis by Blocking the Site of Deviant RNA Splicing\nTo treat metastatic breast cancer, the study authors suggested, it may be possible to target the RNA splice site that generates the invasion-promoting variant of plectin with small bits of complementary RNA that bind to the splice site sequence and block splicing.\nIn the team’s experiments, this approach worked to lower the rate of metastasis in a mouse model of breast cancer. A similar strategy has already been used to treat spinal muscular dystrophy, in which abnormal alternative splicing also plays a role.\nThe mechanism for enhancing splicing at this RNA site also was previously unknown. The researchers discovered that a protein called SNRPA1, which plays a role in normal alternative splicing as part of a coordinated ensemble of molecules called a “spliceosome,” also has a side hustle—enabling splicing at the distinctive plectin-associated RNA motif the researchers identified.\n“We discovered that independently from its well-known function, SNRPA1 is moonlighting, all on its own,” Goodarzi said. “In a mouse model of breast cancer we found that by binding this specific splicing enhancer in RNA, SNRPA1 acts as a promoter of breast cancer metastasis.”\nThe researchers also analyzed clinical data from more than 1,700 breast cancer cases and determined that higher levels of SNRPA1 were associated with worse outcomes for breast cancer patients.\n“Basically, this is a biochemical pathway that drives metastasis that we didn’t know existed,” Goodarzi said.', 'Tumor suppressor genes are normally busy keeping your cell’s growth cycle regulated, and when in good working order, prevent cells becoming malignant. A new study finds that many cancers are caused by mutations that block the tumor suppressor gene’s effect, through a process called Intron Retention. Introns are normally removed after a gene is transcribed into RNA, but in intron retention, one is accidentally left in place. The result can be disastrous, leading to cancer and possibly other disease.\nIntrons are found in complex cells, like those of animals and plants, but not in simple cells like bacteria. Introns are non-coding sequences, meaning that even though the intron is part of the gene, it’s DNA is not used in the gene’s instruction for making protein. Within the gene, the introns are placed between exons – the sequences of DNA that are the actual code for protein. A single gene may contain many introns and exons. When a gene becomes activated, the cell transcribes the DNA into messanger-RNA (mRNA), which then leaves the cell nucleus to be translated into a protein by ribosomes in the cytoplasm. Under normal conditions, both intron and exon DNA on the gene get transcribed into mRNA, but the intron is then cut out of the mRNA prior to leaving the cell nucleus, and so never goes on to the ribosome. Therefore, the mature mRNA comes only from the DNA of the gene’s exons – all unwanted introns having been removed. The removal of the introns from the mRNA is called splicing, and is carried out by complex cellular machinery, composed of both protein and RNA, called the Splicosome.\nSplicing is an important invention of complex cells, leading to greater variation due to Alternative Splicing – different combinations of the exons used to code for the final protein. Alternative splicing creates the potential for multiple protein product from a single gene. For example, some proteins might use the code from all the gene’s exons, but others might use only a few exons, the others having been spliced out, leading to very different proteins with different functions.\nGetting back to Intron Retention, this is the situation when an intron escapes being spliced out, and erroneously remains in the mature mRNA. It can happen due to a mutations at a splice site – sequence of DNA that marks where to splice – and therefore, makes the intron invisible to the Splicosome.\nThe study, published in the November 2015 journal of Nature Genetics, by Hyunchul Jung at the National Research Center in Gyeonggi-do, South Korea, describes the analysis of DNA samples from 1812 patients, with a variety of different types of cancer, including breast, colon, lung, kidney, ovarian, and uterine. After computational analysis of the tumor DNA, Jung found that 31.6% had mutations disrupting normal splicing, with one of the most common type of splicing error being Intron Retention.\nJung also showed that the mutation disrupting the splice site doesn’t need to be one that would change the amino acid sequence of the future protein – a so called Synonymous Mutation- so might be easily overlooked as being disease causing. In many cases, the intron contains a warning signal for the cell to destroy the mRNA before making it into protein, so that gene never gets expressed. In a significant number of the cancers studied, Intron Retention was found to have occurred more frequently in Tumor Suppressor Genes (TSG). TSGs are like the brakes on the cell cycle, and Intron Retention is like having bad brakes, so the cell cycle speeds out of control, leading to a malignant cell that divides uncontrollably. As a suggestion for future research the paper states, “…intronic splice sites should be carefully considered for their potential as disease-causing variants, regardless of whether an amino acid change occurs.” Knowledge of this mechanism of gene disruption may lead to a much better understanding of the causes of certain cancers and other diseases.\n- Intron retention is a widespread mechanism of tumor-suppressor inactivation. Jung H, Lee D, Lee J, Park D, Kim YJ, Park WY, Hong D, Park PJ, Lee E. Nat Genet. 2015 Nov;47(11):1242-8. doi: 10.1038/ng.3414. Epub 2015 Oct 5. PMID: 26437032']	['<urn:uuid:5d5786a5-3fb8-4ed9-8680-4f4dc07d9ee1>', '<urn:uuid:c46c6f88-6893-4ff7-97e6-48b0f0d290a5>']	open-ended	direct	long-search-query	similar-to-document	multi-aspect	novice	2025-05-12T21:52:51.670698	16	147	1358
18	what measurements do i need get perfect fit dog harness size	To properly measure a dog for a harness, you need three key measurements: 1) Chest girth - measure around the widest part of the torso behind the front legs near the bottom of the rib cage, 2) Neck measurement - measure just below the narrow part of the neck where a collar would normally sit, right above the shoulders, and 3) Dog's weight - this is important as harness straps and buckles are designed with specific strength ratings based on the dog's weight. These measurements ensure the harness will fit properly and prevent mobility issues or discomfort.	['Dog ownership entails purchasing the necessary equipment to work with your dog and train your dog. One area dog owners can agree on is they all desire a well mannered healthy pup.\nCertain harnesses can help us with teaching our dogs good manners. Before you can purchase dog a harness, you need the correct size that is suitable for your dog. It’s important you know how to measure a dog for a harness before ordering one online or making a purchase in a pet store.\nIn this article, How to Measure a Dog for a Harness, we’ll look at dog harness origins, the many reasons people chose a harness for their dog, the different styles of harnesses, factors to consider when choosing the right harness and how to measure a dog for a harness.\nThe Modern Dog Harness Evolved\nOriginally dog harnesses were designed for dogs to pull sleds or heavy wagons and carts. In the years during and after World War I, dogs wore harnesses to carry rescue equipment and send important messages.\nIn the 1930’s a remarkable women named Dorothy Harrison Eustis, brought dogs trained to lead a visually impaired person, from Europe to America. She started the first school for the blind called The Seeing Eye. The visually impaired students used a rigid leather dog harness with a handle to assist them.\nDuring the 1990’s dog harnesses gained popularity due to campaigning by animal right activists. The biggest concern was while walking your dog, strictly using a flat buckle collar, was causing stress on the dog’s neck and throat region.\nThe modern dog harness has evolved in the years since that initial health concern in the 1990’s.\nBefore you chose a dog harness and learn how to measure a dog for a harness, we’ll look at several reasons dog owners choose to buy a harness.\n- How to Potty Train an Older Dog in an Apartment\n- How to Measure a Dog for a Harness\n- A Review of the 6 Best Dog Harnesses for Hiking\n10 Common Reasons to Use a Harness\nDog owners from every walk of life chose the reasons listed below to purchase a dog harness and learn how to measure a dog for a harness:\n- Short nose or flat-nosed breeds. Dogs with short muzzles often already have breathing issues and respiratory difficulties. Harness use reduces this strain for breathing issues.\n- Health Issues. Harnesses are appropriate for dogs with health issues such as neck injuries and respiratory issues.\n- Collar escape artists. Dogs with heads and necks that are approximately the same size can slip their collar. Harnesses are especially useful during training and travel for these dogs.\n- Reduces neck and back strain. A harness disperses pressure over a larger area of the body rather than just the neck area.\n- Easier for older dogs. A harness with a top handle makes it easier for humans to assist their senior dogs. Senior dog issues could include lack of balance, instability walking on slippery surfaces, and navigating stairs as they age.\n- Discourages pulling. The chest clip harnesses were designed with this in mind. Dog trainers often recommend the harness while owners are first learning to train their dog to walk nicely on a leash. No pull designs will aid you with daily training.\n- How to Train a Dog to Run Next to a Bicycle\n- How to Teach an Older Dog New Tricks\n- How to Train a Deaf Dog\n- The Top 12 Fun and Easy Dog Tricks\n- How to Train a Dog\n- Better control. People have found that while walking their dog in busy neighborhoods and parks that the harness gives them better control. While your dog is still working on heeling nicely, it can relieve some strain on the humans back.\n- Hiking with your dog. Harnesses allow for dogs to be assisted over large boulders or slippery hiking surfaces. Trail conditions can be unknown, and the harness can give the humans an extra area to assist their dogs.\n- Useful for working dogs. Specially designed harnesses are used for the visually impaired person. Dogs that pull sled and carts wear a full body harness designed for this type of work.\n- Dogs in rehabilitation. Harnesses are useful for assisting a dog after an injury or surgery.\n- How to Obedience Train Your Dog\n- How to Leash Train Your Dog\n- How to Get Rid of Dog Warts at Home\n- How to Housebreak or Potty Train Your Puppy\n- How to Clicker Train Your Dog\nHow to Measure a Dog for a Harness: Three Basic Types of Harnesses\nWe’re almost ready to learn how to measure a dog for a harness. First, it’s important to choose the style of harness that fits your need and lifestyle.\nThe Y Harness. This type of harness was derived from the horse harness. The strap surrounding the neck allows the dog to exert themselves pulling from the chest and back. This design is used by dogs who pull sleds and heavy carts.\nChest Harness. This design was made to assist with teaching your dog not to pull while on a leash. The straps go over the dog’s head and around the chest and clip between their shoulders.\nPack Harness. This is a heavy duty harness with pockets on either side and wide straps that come across the chest and under the belly. Pack harnesses are made for dogs who carry light loads while hiking and camping.\nDog harnesses can vary in style to fit the lifestyle of their owner. All styles require a properly fitted harness.\n- Ultimate Guide: How To Take Care Of A Puppy\n- Ultimate Guide: How To Take Care Of A Dog\n- Ultimate Guide: How To Take Care Of A Senior Dog\n- The Benefits of Owning a Dog\n- Top 12 Tips for First Time Dog Owners\n- How to Train a Dog\nHow to Measure a Dog for a Harness: Styles of Dog Harnesses\nDog harnesses come in many different styles. Some are designed to aid the handler with a specific task and others might allow the handler to have better control of their dog.\n- Nylon dog harnesses come with front clips, back clips and dual clips (front and back clips). No pull or easy walk nylon harnesses are designed with the front clip centered in the dog’s chest.\n- Soft mesh harnesses are lightweight and best designed for toy breeds and small sized dogs.\n- Tactical harnesses are made with heavy duty military quality fabric that have extra pockets, handles, and attachments. They are designed to protect your dog while they are carrying a load.\n- Fashion harnesses with extra frills are preferred over a collar by some owners.\n- A Review of the 5 Best Waterless Shampoos for Dogs\n- A Review Of The Top 5 Best Halloween Costumes for Dogs\n- How to Stop a Dog from Pooping in the House at Night\n- How to Potty Train an Older Dog in an Apartment\n- How to Grind Your Dog’s Nails\nHow to Measure a Dog for a Harness and Considerations for Different Body Types\nBefore you begin to learn how to measure a dog for a harness, take into consideration your dog’s body type.\n- Short bodies dogs, like the Pug, will require a harness with wide girth straps and the leash attachment on the front third of their body.\n- Dogs over 40 pounds, should consider getting a harness with straps that are 1.5”-2” wide.\n- Wide chest straps allow for even weight distribution. The narrower the strap, the higher load on the dog’s body.\n- Long bodied dogs, like daschunds require a different ring position. The harness needs to allow the leash to sit on the middle third of the dog’s body.\n- Older dogs do best with wide straps, a padded chest strap and a handle on top of the harness.\nIt’s important to learn how to measure a dog for a harness, a poorly fit harness can cause issues with mobility and uneven weight distribution. One bad scenario of an ill-fitting harness is your dog will try to escape.\nAnother issue with a poorly fit harness is the dog will feel like they are choking, or breathing will be impeded while exercising.\nNow we are ready to begin the steps of how to measure a dog for a harness.\n- A Review Of The Top 5 Best Salmon Oils for Dogs\n- The Best Canned Dog Food for Dogs with Sensitive Stomachs\n- How to Cook Liver for Dogs\n- A Review of the 5 Best High Fiber Dog Foods For Anal Gland Problems\nHow to Measure a Dog for a Harness\nThe main piece of equipment you’ll need for this job is a flexible measuring tape. Often the kind of measuring tape that is used by seamstresses. If your dog has a difficult time standing still, ask for assistance from a friend or family member.\nAn alternative to a flexible measuring tape is a wide piece of string that you can measure.\nThis method of measuring a dog can be used on all breed types and sizes.\nHow to Measure a Dog for a Harness with 3 Easy Steps:\n- Chest Girth. Wrap your measuring tape around the widest part of the torso, behind the dog’s front legs towards the bottom of their rib cage. It should be fitted by not tight. Write down the measurement.\n- Neck Measurement. Take your measuring tape and wrap it just below the narrow part of the neck where the dog normally wears a collar. Record that measurement. The neck measurement is taken right above the shoulders of the dog.\n- Dog’s Weight. It’s important to know how much your dog weighs. For some harnesses, the harness straps and buckles are designed with certain strength and durability based on the weight of your dog.\nCheck your manufacturers suggestions to find if weight is a necessary measurement.\nOnce you have your dog’s chest girth, their neck measurement and weight, you’ll be able to purchase the right harness for your dog. Some harness manufacturers will even send the harness pre-adjusted based on the measurements you sent them.\n- A Review Of The Top 5 Best Dog Whistles\n- A Review of the 5 Best Dog Muzzles\n- A Review Of The Top 5 Best Dog Clickers\n- How to Build an Outdoor Dog Potty Area on Concrete\nHow to Measure a Dog for a Harness with Confidence\nIt’s amazing to see how the dog harness has evolved in the last century. One reason dog owners chose the harness for walks and travel is because they like the control it gives them.\nAn area to emphasis when learning how to measure a dog for a harness, is that all harnesses are different. Follow the measurement and weight guidelines given by the harness company.\nAs with any dog equipment, you will need to introduce the harness to your dog slowly and you will be rewarded with the time and training you put into your canine companion!\n- A Review Of The Top 5 Best Carpet Cleaner Solutions for Dog Urine\n- A Review Of The Top Best Silent Dog Whistles\n- A Review Of the Top 5 Best Artificial Grasses For Dog Potty\n- A Review of the Top 5 Best Dog Carrier Backpacks\n- A Review of the 5 Best Dog Frisbees\n- A Review of the Top 5 Best Dog Bones For Aggressive Chewers\n- How to Get a Picky Dog to Eat\n- A Review Of The Top 5 Best Bone Broths for Dogs\nPlease read our Legal Disclaimer']	['<urn:uuid:d1c58076-29f3-4c07-87d1-89ea78e6c427>']	open-ended	direct	long-search-query	distant-from-document	single-doc	novice	2025-05-12T21:52:51.670698	11	97	1955
19	looking for organizations helping communities im curious how giving kitchen and bayview opera house support local people	Giving Kitchen and Bayview Opera House both serve their communities in different ways. Giving Kitchen provides emergency financial assistance to food service workers in Georgia during crises, offering grants for issues like illness, injury, disaster, or death of family members. Since 2014, they've helped over 1,300 restaurant workers. For instance, they're currently offering disaster relief grants to Olde Pink House employees affected by a recent fire. Meanwhile, Bayview Opera House serves as a cultural center in the Bayview Hunters Point community, providing accessible arts education, cultural programs, and community events. They focus on building social cohesion through arts-based events that reflect the community's cultural heritage and provide opportunities for local artists to perform. Both organizations aim to create stability in their respective communities - Giving Kitchen through financial support for food service workers, and Bayview Opera House through arts and cultural engagement.	"['Olde Pink House employees offered financial help\nOlde Pink House employees can apply for emergency help thanks to Giving Kitchen, a nonprofit food service assistance group.\nGiving Kitchen is offering grants to the employees who are out of work because of the Dec. 27 fire that caused the popular restaurant to close for repairs. The fire was contained in the restaurant\'s second-story ballroom.\n“It’s an honor to stand alongside the citizens of Savannah in support of Old Pink House employees,"" said Bryan Schroeder, GK executive director. ""Being a food service worker shouldn’t mean that you have to work without a safety net — stability is something we should all strive to provide to this deserving community.”\nGiving Kitchen is offering disaster relief grants for those who need financial assistance due to job loss from the fire by applying at bit.ly/pinkhouseapply by Jan. 31. Funds raised at participating events as well as restricted donations from the public will be equally distributed to Olde Pink House employees who apply by the deadline. GK will in turn match up to $100 of that amount for each applicant.\nFood service workers in the Coastal Georgia area are encouraged to join the GK | Food Service Workers Coastal GA group on Facebook, a private group monitored by GK to share job openings and other resources to help workers find stability in the immediate future. Businesses willing to offer direct support to affected workers may also request to join the group. Anyone requesting to join should answer the group questions to ensure approval.\nIf you would like to contribute to GK’s Olde Pink House disaster relief fund, donate at bit.ly/give2pinkhouse. You may also mail a check (write “Pink House” on the memo line) to Giving Kitchen, Attn: Pink House Disaster Relief, 513 Edgewood Ave., Suite 100, Atlanta GA, 30312.\nDonations made after Jan. 31 will go to Giving Kitchen’s general fund to assist food service workers in the Savannah area.\nIf you would like to host a fundraiser to support Olde Pink House employees, email Abbey Freund at firstname.lastname@example.org or call 404-254-1227.\nWhile GK is here to support Olde Pink House employees affected by this unexpected emergency, their direct and matching grants are open to food service workers in Georgia who need financial assistance because of an illness, injury, disaster such as house fire or flood, or death of an immediate family member that has happened within the past six months. Note that this is a separate application process, where supporting documentation will be required to show that financial need has been caused by a qualifying crisis. To apply for a GK grant, start at bit.ly/apply2gk.\nFor more about how Giving Kitchen helps food service workers in crisis, go to givingkitchen.org and follow on social media @givingkitchen.\nFor questions about the grant application process, email email@example.com or call 404-254-1227. Note that GK has a Spanish speaker on staff.\nAbout Giving Kitchen\nGiving Kitchen is a nonprofit organization that provides emergency assistance to food service workers through financial support and a network of community resources in order to create a restaurant community where crisis is met with compassion and care. GK’s promise to the food service community is stability. Since its 2014 inception, GK has given crisis grants to more than 1,300 restaurant workers in Georgia.', 'About Bayview Opera House, Inc.\nBayview Opera House, Inc. (BVOH) was founded in 1989 to run programs at our namesake building. The San Francisco Arts Commission had purchased and renovated the facility in the 1970\'s for use as a community cultural center along with four other centers in disadvantaged neighborhoods. By the end of 80\'s the commission came to the conclusion that it was best to have each center run by a local group to provide programming instead of the commission itself running programs. BVOH received its 501(c)3 status in 1992.\nBVOH provides opportunities to engage in the arts for a community with a long history of disinvestment, creates a sense of belonging and community ownership of the only significant cultural resource in the neighborhood, maintains inclusiveness of the traditional population in the light of gentrification, and provides educational and employment opportunities.\nOur mission is to serve as the focal point for Art and Culture in the Bayview Hunters Point community by providing accessible, diverse, and high-quality arts education, cultural programs and community events in a safe environment and to employ the arts to support aspirational values, hope and passion, and to inspire the self-confidence and self-esteem needed to be successful, especially for our youth.\nBVOH anchors BV/HP by providing a central location where the community comes together to enjoy culturally relevant art exhibits and performances, community festivals, and local artists get their first opportunities to perform.\nBVOH builds social cohesion through arts-based events that are reflective of the rich cultural heritage of the Bayview community that appeal to multiple generations and both traditional and new residents.\nBVOH provides arts education to young people where the arts have been largely eliminated from the school curriculum, building the capacity for arts appreciation and building artistic skills for self-expression.\nCity of San Francisco Cultural Centers\nThe Bayview Opera House Ruth Williams Memorial Theatre is one of six city-owned cultural centers operated by nonprofit arts organizations providing cultural and arts programs. The centers are venues for performances, festivals, and gallery exhibits and offer free or low cost classes. They are funded, at least in part, by the San Francisco Arts Commission.\nBoard of Directors\nThe Bayview Opera House Board of Directors is a group of dedicated individuals who volunteer their time for the benefit of our organization, provide valuable vision for our programs and financial sustainability. They exercise financial oversight, help fundraise, make a meaningful annual donation to the organization and contribute their expertise.\nBVOH is accepting applications for new board members. Board members are expected to actively participate in board meetings, join committees and task forces as needed, volunteer for and attend Opera House events and programs, donate to the organization and participate in fundraising efforts. To apply, please send your resume to secretary Micah Fobbs along with a letter stating your interest in the Bayview Opera House.\nBarbara Ockel, Executive Director. Barbara has been leading the Bayview Opera House since 2009. She has a strong arts and business background that has enabled her to develop the Bayview Opera House into the strong and sustainable organization it is today. She has been able to attract major artistic collaborators, such as A.C.T., the Museum of the African Diaspora, the Asian Pacific Islander Cultural Center and others. She has also greatly expanded and diversified funding sources.\nTheo Ellington, President. Director of Public Affairs for the Golden State Warrriors. He holds a Bachelor of Arts in Political Science and a Master’s degree in Urban Affairs from the University of San Francisco. Theo has served on the Redevelopment Successor Agency Commission for the Office of Community Investment and Infrastructure, following his service on the Youth and Southeast Facilities Commission and the Youth Commission.\nThor Kaslofsky, Vice President. Principle, Ground Urban Development. Formerly Hunters Point Shipyard Project Manager with the Office of Community Investment and Infrastructure. He received his MBA from New York’s Baruch College business school.\nMicah J. Fobbs, Secretary. Owner of multiple businesses and community advocate. His company M.J.F. & Associates Consulting currently contracts with the City of San Francisco to assist the Mayor’s Citizens Advisory Committee with the largest redevelopment project in San Francisco, the Hunters Point Shipyard.\nJacqueline ""Jackie"" Cohen, Co-Treasurer. Co-director 100% College Prep Institute\'s Parent Academy. She was recognized by San Francisco Foundation’s Koshland Civic Unity Program, providing her an opportunity to join seven other Fellows to invest $300,000 to improve the quality of life for Bayview residents.\nAaron J. Pischke, Co-Treasurer. Branch Manager at First Bank. Aaron has been with First Bank since 2007, and previously worked in the financial services sector as a financial analyst. He lives in Bayview and enjoys making a difference in his community.\nJerald Joseph, Member. Senior VP of Sales & Property Mgt at Haight Street Realty/Finance Management (HSM). Jerald holds a degree in Business Administration with an emphasis in Marketing and Finance. Elected as BVOH President in 2010, he is a longtime Bayview resident and the President of the Board of LaSalle Heights Homeowners Association.\nVernon C. Grigg III, Member. Adjunct professor of law at Golden Gate University. Vernon is an independent law practitioner specializing in labor and employment. He is an active member of the State Bar, licensed in California since 1993. Vernon received his J.D. from Yale University.\nClinton Sockwell II, Member. Youth and community activist. Mr. Sockwell is a regular school facilitator/consultant with the Community-Intentions Program, which delivers culturally responsive instruction to increase student achievement for historically underserved populations.']"	['<urn:uuid:e877ca64-1d62-47b7-a3be-51594a3ace60>', '<urn:uuid:198e35d2-7f88-49f3-a1d3-c7829dbae7ef>']	open-ended	with-premise	long-search-query	similar-to-document	comparison	novice	2025-05-12T21:52:51.670698	17	142	1457
20	environment friendly building ontario mohawk college design features power system	The Joyce Centre at Mohawk College features several environmental design elements. For energy, it uses 653 kW DC solar power through nearly 500 solar panels, producing 550,000 kilowatts of clean energy annually. The building employs 28 geothermal wells drilled 600 feet deep for heating and cooling. The structure has high-efficiency features including triple-glazed curtainwall system (R-8), insulated precast panels (R-30), and R-40 roofing. It uses no natural gas, operating entirely on electricity. The building also incorporates water conservation through ultra-low flush urinals, low flow faucets, and rainwater harvesting. The facility is certified as a Zero Carbon Building by the Canada Green Building Council, demonstrating both environmental sustainability and innovative design.	['(photo courtesy Mohawk College)\n(published in the May 2018 issue of Canadian Consulting Engineer)\nIn the fall of 2016 Mohawk College in Hamilton, Ont. began breaking ground on the new Joyce Centre for Partnership & Innovation, a 96,000-sq.-ft. five-storey net zero building project that was secured with $20 million from the federal government’s post-secondary institutions Strategic Investment Fund, a fund established to modernize and improve the environmental sustainability of college and university campuses across the country.\n“We had a very ambitious project,” explains Tony Cupido, P.Eng., the chief building and facilities officer at the College and the lead representative for the client on this project.\nCupido previously led nearby McMaster University through several LEED projects including the first LEED Gold facility of any university or college in Ontario, back in 2009.\n“When you look at the energy intensity of that building at McMaster, it’s five times what this one will be,” remarks Cupido. “Reaching this intensity is a combination of improved design, improved materials and improved performance of materials.”\nThe net zero Joyce Centre, slated to achieve substantial completion by June, will consume as much energy annually as the photovoltaic (PV) panels located on it and on the rooftops of adjacent sites will generate.\nEnergy Intensity Target\nPlanning for the net zero project began in early 2016 led by architects from McCallum Sather and B+H Architects. The team of engineers assembled to work on the project included RDH (energy modeling), Mantecon Partners (structural), TMP – The Mitchell Partnership (mechanical), and Mulvey & Banani (electrical, security). “There was a high focus on net zero by the entire team,” says Cupido, “and that really hadn’t been done on a large scale like this before in this area.”\nThe team produced a Basis of Design document, outlining the overall concept of the building and all of the specific features. “It’s an important document, because it lays out a roadmap for us as an owner,” says Cupido. “It tells us what all of these people are going to do and how they are going to do it. Their approach and methodology is all laid out.”\nEarly in the process the team established an annual energy use intensity (EUI) target, ultimately ending up at around 73 equivalent kWh/square-metre/year for the building, and RDH then began the energy modeling to determine solutions for heating and cooling the building that would achieve the target.\nTo keep energy demand low the building envelope needed to be well designed. The facility uses a triple-glazed curtainwall system with an R-value around R-8 and an insulated precast sandwich panel system with an R-value approaching R-30. The ratio of glazing to panels sits around 35% : 65%, providing an overall combined window/wall R-value of at least R-10.\nThe roofs is designed to an R-40 performance.\nThe Mohawk campus uses centralized heating for its various buildings, but the new Joyce Centre sits independent. The building will use variable refrigerant flow (VRF) heat pumps coupled to a geoexchange well field. The 28 geothermal wells required drilling down 600 feet through shale, each one taking a day to complete.\nThe geothermal system will extract heat from the building in the summer, and in the colder months extract heat from the earth as a heating source.\nThe heat pumps, operating in zones, will help with heating or cooling different parts of the building.\nA dedicated outdoor air system with heat recovery separates the ventilation from heating and cooling.\nThere is also a solar thermal hot water system providing domestic hot water and heating for the building.\nPotable water use reduction also includes the use of ultra-low flush urinals, low flow faucets, and also rooftop rainwater harvesting for toilet/urinal flushing and irrigation requirements.\nThe solar farm feeding the facility is located on wing-shaped structures\nabove the building and on adjacent building roofs. (photo courtesy Mohawk College)\nThe solar farm feeding the building will provide 653 kW DC power, translating to 500 kW AC through inverters, says Cupido. That should be enough run the Centre.\n“One of the biggest challenges we had was a roof capacity issue,” says Cupido. Fortunately, some of the existing flat roofs on campus adjacent to the new Centre required replacing, so the College installed a lightweight insulated concrete system on the roofs freeing up capacity for the PV systems.\nThe building superstructure is structural steel, and the solar farm spans across the building on wing-shaped structures supported by a combination of structural steel and supports from the solar panel fabricator.\nEllisDon was the general contractor on the project, and Walters Group provided design assist on the structural steel elements. Walters became involved from the preliminary stages.\n“They had hardly figured out what it would look like and how it would work before we got to work on it,” says Walters’ project manager Kevin McElhone.\n“We became involved in the design/assist process to help our customer get what they needed as quickly as possible.”\nTo restrict energy usage modern controls and system automation systems are in place. It’s what Cupido calls, “Brutally efficient.”\nThe HVAC is demand responsive, with CO2 sensors adjusting ventilation when the facility is occupied.\nLighting controls reduce output when daylight is available, and occupancy sensors will turn off lights in vacant rooms. “When you leave a room the lights won’t go off in five minutes, they will go off in five seconds or less,” says Cupido, adding that it has to be that highly reactive to keep the energy use to a minimum.\nTony Cupido, P.Eng., chief building and facilities officer Mohawk College, with one of many project awards. (photo courtesy Mohawk College)\nZero Carbon Project\nThe College is participating in the Canada Green Building Council Zero Carbon Building pilot program, with the Joyce Centre being one of 16 projects across the country. “The program will help us understand what the carbon impact of a new buildings like this is, what issues need to be resolved, what types of standards should be met, and what type of designation should ultimately be achieved.\n“And also, from a learning perspective, how does one go about this and what is the protocol to obtain some type of a designation under the Zero Carbon Building initiative.”\nAlready the project was named the sustainable project of the year by the Ontario Sustainable Energy Assoc.\n“We believe this has been a successful project. The proof will come out of the final installation details, training, commissioning all of the components, and then how we manage it as an organization,” says Cupido.\n“No one has built a building like this, and no one has operated a building like this, yet. So we have our own challenges of getting the right team here. We believe we have that foundation ready, but there’s still some work to do.”\nJoyce Centre for Partnership & Innovation, Mohawk College, Hamilton, Ont.\n- Owner: Mohawk College\n- Architects: B+H Architects/McCallum Sather\n- Contractor: EllisDon Corp.\n- Energy consultant: RDH Building Science\n- Structural consultant: Mantecon Partners\n- Mechanical consultant: The Mitchell Partnership (TMP)\n- Electrical, security: Mulvey & Banani\n- Structural Steel: Walters Group', 'Media Release: Mohawk Helps Set New Standard for Green Building Innovation\nJoyce Centre for Partnership & Innovation Canada’s First Institutional Building to Receive Zero Carbon Building – Design Certification\nHamilton, ON – Mohawk College has been awarded Zero Carbon Building – Design certification from the Canada Green Building Council (CaGBC) for the Joyce Centre for Partnership & Innovation. The Joyce Centre is the second building to receive the certification. The first was a multi-tenant office building in Waterloo, Ontario.\nIn 2017, the Joyce Centre became one of 16 projects from across Canada to be chosen as a pilot project for CaGBC’s new Zero Carbon Building Standard. New construction projects earn Zero Carbon Building – Design certification by modeling a zero carbon balance, highly efficient envelope and ventilation systems to meet a defined threshold for thermal energy demand intensity, and onsite renewable energy systems capable of providing a minimum of 5% of building energy consumption. Project teams are required to evaluate energy use holistically, including impacts on peak electricity, and determine the GHG emissions associated with structural and envelope materials. Both new and existing buildings can obtain the standard.\n“We’re honoured to receive the Zero Carbon Building – Design certification. The Joyce Centre for Partnership & Innovation will not only be a showcase for green construction but also a living lab that will teach future generations how to build better, more sustainable buildings.”\n“Congratulations to Mohawk College on earning Canada’s second Zero Carbon Building – Design certification for their impressive Joyce Centre for Partnership and Innovation. With this project, Mohawk has proven that they are on the leading edge of innovation going well beyond the status quo. This project shows that these buildings are technically feasible and financially viable, and sets a strong example for students about the low-carbon economy.”\n“Having founded our firm on the principals of sustainable design in Hamilton, we are thrilled that The Joyce Centre for Partnership & Innovation at Mohawk College is awarded the Zero Carbon Building –Design Certification in our city. Achieving this standard has been a huge team effort. In addition to generating all of the building energy needs through renewable solar photovoltaic panels, we worked diligently to enhance sustainable features using natural materials, maximizing natural light and views, applying finishes strategically, integrating water conservation measures and green roofs. We believe this facility will serve as inspiration for what can be achieved through a unified team process to integrate innovative design solutions and proven building system technologies to achieve Net Zero energy and zero carbon facilities.”\n“The Joyce Centre represents a new archetype for learning environments in Canada that recognize and encourage innovation. It’s not just a lab building. In many ways, it’s also a teaching tool for students, training them to deliver a more sustainable future. Its environment explores the symbiotic relationship between all stakeholders and underscores the importance of a cultural shift in how we interact with our built environment—from unrestricted consumption to personal accountability.”\n- The Joyce Centre for Partnership & Innovation will be home to new and existing programs offered through Mohawk’s School of Engineering Technology.\n- The Centre will house high-tech labs and classrooms specializing in fields such as clean and renewable energy, sustainable design, technology automation, cyber security and materials manufacturing among others.\n- Contributing to the zero carbon certification was Mohawk’s curtain wall technology which offers much better insulation value than typical curtain walls.\n- The Centre also uses no natural gas onsite – the all-electric set up allows for the easiest route to low carbon for most buildings.\n- There will be nearly 500 solar panels mounted to canopies on the roof of the Joyce Centre. Campus-wide the college is installing nearly 2,000 solar panels as part of a clean energy retrofit.\n- Mohawk’s solar panel system will produce 550,000 kilowatts of clean energy per year which is enough to power 45 Canadian homes for one year.\n- The Joyce Centre also features 28 geothermal wells that draw energy from more than 600 feet below the building.\n- Mohawk has been named one of Canada’s Greenest Employers for the past five consecutive years in recognition of its leadership in environmental sustainability\n- 30 -\nFor more information about Mohawk College and the Joyce Centre for Partnership & Innovation contact:\nDirector of Communications\n905-575-1212, ext. 3965\nFor more information about CaGBC’s Zero Carbon Building Standard contact:\nCaGBC Media Relations\nDeveloped to be broadly applicable, the Standard can be used by new and existing commercial, institutional, and multi-family buildings. The Standard is unique in that it is the first in North America to use carbon instead of energy efficiency to measure a building’s impact. Recognizing the choice of energy sources and the carbon intensity of regional electrical grids is critical to accurately assessing environmental impacts and guiding investments. The Zero Carbon Building program reinforces the importance of energy efficiency while also driving careful choices about the types of energy used and encouraging more renewable energy generation both on the building site and offsite.\nCaGBC is the leading national organization dedicated to advancing green building and sustainable community development practices. As the voice of green building in Canada, we work closely with our national and chapter members in an effort to make every building greener. The CaGBC reduces environmental impacts from the built environment through project certification, advocacy and research. CaGBC established the LEED® green building rating system in Canada and developed the country’s first Zero Carbon Building Standard. For more information cagbc.org.\nAbout Mohawk College\nMohawk College educates and serves more than 31,700 full-time, part-time and apprenticeship students at three campuses in Hamilton, Ontario. Mohawk is among the leading colleges in Canada for applied research and is ranked number one for student and graduate satisfaction among colleges in the Greater Toronto and Hamilton Area.']	['<urn:uuid:217b61fd-e6c4-450f-9a97-a98a83fb8674>', '<urn:uuid:816c314b-3f7e-4d03-88b4-5a41979f5f9b>']	open-ended	with-premise	long-search-query	distant-from-document	multi-aspect	novice	2025-05-12T21:52:51.670698	10	110	2146
21	my baby has yellow skin after 24 hours of birth how dangerous is it and what treatments are available	Any baby with jaundice in the first 24 hours of life should be urgently assessed in hospital, as very high bilirubin levels could cause permanent brain damage if untreated. The main treatment is phototherapy, which uses light waves to break down bilirubin. This can be delivered through conventional devices (using fluorescent or LED lamps) or fiber optic devices. For most babies, jaundice is not harmful and resolves within 10-14 days without causing problems.	"[""Blood type and Rh determination in mother and infant. This is because there's a small risk the bilirubin could pass into the brain and cause brain damage. Ensure that routine metabolic screening (including screening for congenital hypothyroidism) has been performed. But only around 1 in 20 babies has a blood bilirubin level high enough to need treatment. What causes jaundice? Jaundice is 1 of the most common conditions that can affect newborn babies. Approximately 60% of term and 80% of preterm babies develop jaundice. Is it safe to delay your period for your holiday? If a baby with very high levels of bilirubin is not treated, there's a risk they could develop permanent brain damage. I. Upgrade to Patient Pro Medical Professional? Neonatal jaundice What is jaundice? Further investigation is essential for any baby who is also unwell, presents in the first 24 hours or has prolonged (after 10 days) jaundice. Reducing substance in urine: screening test for galactosaemia (provided the infant has received sufficient quantities of milk). Check SBR (note that a direct SBR very rarely indicated within the first 5 days of life). jaundice beyond 14 days of life) you must refer to the Prolonged Neonatal Jaundice - Management in the Community protocol. This information explains the advice about jaundice in newborn babies that is set out in NICE guideline CG98. Do not measure bilirubin levels routinely in babies who are not visibly jaundiced. Follow expert advice about care for babies with a conjugated bilirubin level greater than 25 µmol/L because this may indicate serious liver disease. haemolytic disease of the newborn (rhesus). Symptoms of newborn jaundice. Jaundice is the name given to the yellow appearance of the skin and the whites of the eyes. Any baby presenting with jaundice in the first 24 hours of life should therefore be seen urgently for assessment in hospital. This guidance is changing frequently. What should the junior doctor know? Neurological signs - eg, changes in muscle tone, seizures, or altered crying - require immediate attention to avoid kernicterus. Physiological jaundice is by far the most common cause and is easily recognisable, typically appearing two to three days after birth in an otherwise well infant and normally resolving by 2 weeks of age. By the time a baby is about 2 weeks old, their liver is more effective at processing bilirubin, so jaundice often corrects itself by this age without causing any harm. This often requires inpatient hospital treatment with phototherapy. They are written by UK doctors and based on research evidence, UK and European Guidelines. There were 7 hospital admissions for kernicterus in England in 2015-16. He is now nearly 2... Assess your symptoms online with our free symptom checker. NICE has issued rapid update guidelines in relation to many of these. Learn about Neonatal Jaundice from our professional reference. Jaundice reaches its peak at about four days of life and This is known as kernicterus. What is jaundice? 1 It is usually clinically detected when the serum bilirubin levels are greater than 40-50µmol/L (when the jaundice is observable). Each of the Neonatal Unit leaflets are detailed below, select the heading of the one you would like to view and the content will expand with an option for you to download the PDF version. Neonatal Unit Jaundice in New-born Babies Information Leaflet Jaundice is common in new-born babies and is when their skin and whites of their eyes (sclera) look yellow. Adv Neonatal Care. Add filter for Diabetes UK (13) ... Add filter for Patient (261 ... Base the diagnosis of neonatal jaundice on clinical observation at every contact, particularly within the first 72 hours. … Care and support of people growing older with learning disabilities Cataracts Dementia Coronavirus: how quickly do COVID-19 symptoms develop and how long do they last? For most babies, jaundice is physiological and not harmful. Biliary atresia requires surgery within the first two months of life for a better prognosis. Hepatosplenomegaly, petechiae and microcephaly are associated with haemolytic anaemia, sepsis and congenital infections. Treatment for newborn jaundice is not usually needed because the symptoms normally pass within 10 to 14 days, although they can occasionally last longer. Neonatal jaundice describes a condition in which an infant’s skin appears yellow within the first few days of life. How to treat constipation and hard-to-pass stools. Disclaimer: This article is for information only and should not be used for the diagnosis or treatment of medical conditions. As Walls (2004) says, with appropriate training and enthusiastic community support, treating neonatal jaundice at home appears to be feasible, safe, and well accepted by families and medical staff alike. from the best health experts in the business, Neonatal jaundice, NICE Quality Standards (Mar 2014), Jaundice in the newborn; NICE CKS, November 2015 (UK access only). Patient aims to help the world proactively manage its healthcare, supplying evidence-based information on a wide range of medical and health topics to patients and health professionals. Background: Neonates with jaundice are usually managed according to their serum bilirubin despite an unclear overall correlation between bilirubin levels and patient-important outcomes (PIOs) such as kernicterus spectrum disorder (KSD). Usually, a total serum bilirubin level is the only testing required in a moderately jaundiced infant who presents on the second or third day of life and is otherwise well. The bilirubin level does not usually rise above 200 μmol/L and the baby remains well. Blanching reveals the underlying colour. Neonatal jaundice is one of the most common conditions needing medical attention in newborn babies. Neonatal jaundice refers to yellow colouration of the skin and the sclera (whites of the eyes) of newborn babies that results from accumulation of bilirubin in the skin and mucous membranes. Cochrane Database Syst Rev. Jaundice is one of the most common conditions affecting newborn babies with approximately 60% babies (80% of premature babies) developing jaundice in the first week of life. Patients who present with jaundice in the third trimester may require delivery. Quality standard - Jaundice in newborn babies under 28 days. Jaundice is common in newborn babies because babies have a high number of red blood cells in their blood, which are broken down and replaced frequently. 2013 Apr2(2):61-9. doi: 10.4103/2249-4847.116402. Neonatal jaundice can often be physiological due to increased break down of premature erythrocytes and insufficient Glucuronyl Transferase in the newborn liver but jaundice … Pregnancy and neonatal jaundice. This is known as prolonged jaundice.\nMarge Vs Itchy And Scratchy Tv Tropes,\nFour Poster Wood Bed,\n974 Bus Route Dtc Delhi,\nOne Piece Season 22 Release Date,\nLady With Lemon Painting Meaning,"", 'Jaundice is one of the common conditions in full term and premature babies and appears usually in the first week of birth. Jaundice is caused due to the increased levels of bilirubin in the blood, where processing of the compound is slow as the infant’s liver is immature in the first week of birth. Jaundice in infants can be classified into physiological jaundice, breast-feeding jaundice, breast milk jaundice, and jaundice caused as a result of blood group incompatibility with the mother. Physiological jaundice is the most common form of jaundice in infants and is observed in more than 50% of the newborns. Phototherapy is a common mode of treatment in infants affected with jaundice. The treatment involves a process of using light waves which are absorbed by the skin and blood, resulting in the breakdown of bilirubin. The breakdown products of bilirubin after phototherapy do not cause any effects on the central nervous system. They are rapidly eliminated through the liver and kidneys.\nThe global infant phototherapy lamp market is anticipated to grow during the forecast period owing to the common presence of jaundice in neonates. In addition, low cost and ease of the treatment contribute to the growth of the market. However, alternate modes of treatment such as biliblanket, IVIG treatment, and exchange transfusion are projected to hinder the growth of the market. Furthermore, the phototherapy lamp treatment is anticipated to be associated with long-term effects such as testicular effects, transient DNA damage, and melanoma. This factor might drag the market growth. In addition, lack of research that accurately proves the negative effects of the phototherapy treatment is estimated to negatively impact the infant phototherapy lamp market.\nThe global infant phototherapy lamp market can be segmented based on product type, modality, end-user, and geography. On the basis of product type, the market can be segmented into conventional phototherapy devices and fiber optic phototherapy devices. Conventional phototherapy devices can be further sub-segmented into fluorescent lamps and light-emitting diode (LED) lamps. On the basis of modality, the market can be segmented into mobile phototherapy unit and fixed phototherapy unit. Based on the end-user, the market can be segmented into hospitals and neonate clinics.\nA fiber optic phototherapy device comprises a pad of optic fibers that include a fiber optic bundle through which the light passes. This pad is placed close to the trunk of a neonate and hence does not require shielding of the eye. As a result, retinal injury and other eye complications can be prevented, making it the preferred choice of product for the treatment of jaundice in infants. Among the conventional phototherapy devices, fluorescent bulb is a commonly used device for the treatment. In terms of end-user, the neonate clinics segment of the market is anticipated to record a high growth during the forecast period.\nOn the basis of geography, the market can be segmented into North America, Latin America, Europe, Asia Pacific, and Middle East & Africa. North America is anticipated to be a high revenue generating region of the infant phototherapy lamp market in the near future. However, the Asia Pacific market is anticipated to expand at a high CAGR during the forecast period owing to increase in birth rate, focus on enhancing the health care facilities by governments, and growth in awareness among the population.\nThe leading global players operating in the infant phototherapy lamp market include Natus Medical Incorporated, GE Healthcare, Philips Lighting Holding B.V., Ibis Medical Equipment & Systems Pvt. Ltd., Dixion, Ardo, Drägerwerk AG & Co. KGaA, Beijing Julongsanyou Technology Co., Ltd., NOVOS Medical Systems, and Zhengzhou Dison Instrument And Meter Co., Ltd.\nThe report offers a comprehensive evaluation of the market. It does so via in-depth qualitative insights, historical data, and verifiable projections about market size. The projections featured in the report have been derived using proven research methodologies and assumptions. By doing so, the research report serves as a repository of analysis and information for every facet of the market, including but not limited to: Regional markets, technology, types, and applications.\nThe study is a source of reliable data on:\n- Market segments and sub-segments\n- Market trends and dynamics\n- Supply and demand\n- Market size\n- Current trends/opportunities/challenges\n- Competitive landscape\n- Technological breakthroughs\n- Value chain and stakeholder analysis\nThe regional analysis covers:\n- North America (U.S. and Canada)\n- Latin America (Mexico, Brazil, Peru, Chile, and others)\n- Western Europe (Germany, U.K., France, Spain, Italy, Nordic countries, Belgium, Netherlands, and Luxembourg)\n- Eastern Europe (Poland and Russia)\n- Asia Pacific (China, India, Japan, ASEAN, Australia, and New Zealand)\n- Middle East and Africa (GCC, Southern Africa, and North Africa)\nThe report has been compiled through extensive primary research (through interviews, surveys, and observations of seasoned analysts) and secondary research (which entails reputable paid sources, trade journals, and industry body databases). The report also features a complete qualitative and quantitative assessment by analyzing data gathered from industry analysts and market participants across key points in the industry’s value chain.\nA separate analysis of prevailing trends in the parent market, macro- and micro-economic indicators, and regulations and mandates is included under the purview of the study. By doing so, the report projects the attractiveness of each major segment over the forecast period.\nHighlights of the report:\n- A complete backdrop analysis, which includes an assessment of the parent market\n- Important changes in market dynamics\n- Market segmentation up to the second or third level\n- Historical, current, and projected size of the market from the standpoint of both value and volume\n- Reporting and evaluation of recent industry developments\n- Market shares and strategies of key players\n- Emerging niche segments and regional markets\n- An objective assessment of the trajectory of the market\n- Recommendations to companies for strengthening their foothold in the market\nNote: Although care has been taken to maintain the highest levels of accuracy in TMR’s reports, recent market/vendor-specific changes may take time to reflect in the analysis.\nNote : All statements of fact, opinion, or analysis expressed in reports are those of the respective analysts. They do not necessarily reflect formal positions or views of Transparency Market Research.']"	['<urn:uuid:04a5cafe-1419-477f-932c-bc23d3f91b21>', '<urn:uuid:07d2ca63-b2ac-445e-b42d-e92754d5ee9c>']	factoid	with-premise	long-search-query	similar-to-document	three-doc	novice	2025-05-12T21:52:51.670698	19	73	2118
22	uses furniture vs construction rosewood ipe	Rosewood and ipê serve different primary commercial purposes. Rosewood is predominantly used in China for manufacturing luxurious ornamental furniture in the Ming and Qing dynasty style, valued for its dark red hue. In contrast, ipê is primarily used in construction, particularly in coastal environments, due to its hardness and resilience. It is commonly used for boardwalks, home decks, marina docks, benches and railings, and notably has a Class A fire rating equivalent to concrete and steel. The wood is also known for its resistance to fungi and insects.	"['We are inclined to think that trees are a renewable natural resource. Yet precious hardwood trees have already been almost completely logged out from many countries across the tropics. Myanmar is the latest country to experience the insatiable demand for its precious rosewood.\nRosewood, also known as bois de rose, is an umbrella term for a whole group of tropical timber species, mostly from the genus Dalbergia, Pterocarpus, Diospyros, and Milletia, which all have a dark red hue and high quality timber in common. The vast majority of rosewood is imported to China where it’s fashioned into luxurious, highly-priced ornamental furniture in the Ming and Quing dynasty style.\nMyanmar, one of the most important biodiversity hotspots in Asia, has also several species of rosewood highly prized by the Chinese furniture trade. Even though Myanmar’s forest and hardwood stocks have been diminishing for several decades already (less than 10% of the land is now forested, the rosewood logging and smuggling has increased to an unprecedented level in the last three years.\nIn 2013 alone, Myanmar exported 237,000m3 of rosewood to China, triple the volume of the previous year. This amounts to one thirteenth of the estimated remaining rosewood stock of Myanmar – at current logging rates, Myanmar’s forests will have been stripped of rosewood in just 13 years.\nAs Chinese hunger for the luxuriant, dark red timber grows and spreads across the greater Mekong region, rosewood species might face not only commercial extinction, but also final, biological extinction.\nIt is hardly just the loss of a few species that is at stake. Forest overexploited for timber is likely to lose many species of animals, its ability to absorb carbon dioxide from the atmosphere deteriorates, and it is more likely to experience fires. Logging also brings about more hunting and increases the chances of complete deforestation.\nIn Myanmar illegal logging also brings with it a raft of socioeconomic problems. Loggers undertake long and dangerous scouting expeditions into the forest, or take the risk of timber smuggling in conflict-ridden border regions, such as Kachin at the border with Yunnan province, China – one of the main rosewood smuggling routes. Not every logger returns from these expeditions. Besides the fact that logging in the tropics is rated as one of the most dangerous jobs, there is in Myanmar an added danger of being shot in a timber-related conflict. Moreover, loggers are often rewarded by various stimulating drugs.\nSo why isn’t Myanmar establishing commercial rosewood plantations? Some tropical timber can indeed be mass-produced in plantations, especially faster growing species such as rubberwood, eucalyptus, or teak. But the extremely slow growing, high density rosewood trees take many decades to grow to a commercially viable size, requiring several generations of tree planters to wait for the profit. Such long-term investment is commendable, but unlikely in a conflict-ridden, poor country like Myanmar, with unstable land tenure and an explosive political climate.\nAct now or lose it\nIt is in Myanmar’s interest to completely stop the illegal logging and export of rosewood to China. As almost all processing of Burmese rosewood is done in China, no value is added in Myanmar. Worse still, almost no tax is generated: Myanmar lost an estimated US$6 billion through illegal logging between 2013 and 2014. Instead of the desperately needed cash for healthcare, education, and environmental protection, laundered rosewood money goes to corrupt officials and government cronies.\nIf Myanmar wants to escape its rosewood crisis with at least some viable rosewood populations left, it should take lessons from other countries that have already undergone the “rosewood massacre”. On April 1 this year, the Myanmar government put in place a ban on raw timber export, but without enforcement this cannot be effective. Myanmar has to show its dedication to a permanent, non-negotiable, exception-free rosewood export ban. In Madagascar, we have an example of how temporary and unclear bans only lead to a more dynamic and thriving rosewood black market. During periods of temporary bans, illegal rosewood logging continues, and traders simply accumulate rosewood stockpiles. Meanwhile, rosewood prices go up, stimulating even bigger bouts of logging when the ban is lifted.\nHowever, even an effective national ban on rosewood export might not be enough to stop the rosewood crisis in Myanmar. In some cases, a national export ban caused China’s rosewood appetite to shift to a new country. In other cases, for example Vietnam, China simply grabbed the opportunity of cheaper labour and moved its basic rosewood processing to Vietnam, effectively circumventing the raw timber export ban. This may bring some economic benefit to Vietnam, but does nothing to alleviate the pressure on the forests.\nOf the 33 species that pass China’s strict hongmu quality standards for rosewood, more than a third is already deemed vulnerable by the IUCN Red List of Threatened species and six are listed by the Convention on International Trade in Endangered Species (CITES). The convention binds signatory countries to regulate or stop trade in the listed species, depending on the degree of protection.\nWhereas China offers high levels of support to protect its growing rosewood industry, for customers and businesses, it appears to have a complete lack of interest in regulating the industry’s environmental impact or improving its sustainability. Europe, the US and Australia all tightened their regulations regarding rosewood import in recent years. But with Chinese domestic demand growing significantly since 2011, only stricter regulations in China can save Myanmar’s rosewood forests.', ""Ipê (Tabebuia spp.)\nThe term ipê refers to seven different tree species within the genus Tabebuia, each found in the tropical forests of Central and South America. These massive trees can reach heights of 150 feet, and are supported by a surprisingly narrow trunk. Ipê is not an evergreen, and drops its leaves before producing large clusters of beautiful flowers in colors including white, golden yellow, lavender, bright pink and red.\nIpê is found in the tropical forests of Argentina, Bolivia, Brazil, Colombia, French Guiana, Guyana, Paraguay, Peru, Suriname, Trinidad and Tobago, and Venezuela. This type of tree is often found growing on river banks, in marshes and atop ridges.\nIpê is a hard and resilient wood, often used in coastal environments. Its wood is also known for its resistance to attack by both fungi and insects, and holds a Class A fire rating -- the same rating given to concrete and steel. It is commonly laid down for boardwalks and home decks, marina docks, benches and railings. The hardest ipê comes from the Tabebuia serratifolia species, which is today primarily harvested and exported from Bolivia, after having been overharvested in Brazil. Several species of Tabebuia are also known for Pau D’Arco, a special herb extracted from their leaves and bark. For centuries, the extract has been sold as a tea and used by indigenous communities to treat a range of ailments. While few scientific studies have been conducted, Pau D’Arco is believed to treat illnesses such as leukemia, allergies, diabetes, dysentery and malaria.\nThroughout much of the world, ipê has been overharvested -- its wood used for outdoor building projects and construction in coastal environments. Many trees have also been stripped of their bark, renowned for its medicinal properties, leading to the eventual death of these trees.\nThe Rainforest Alliance’s SmartWood Program has been working with community forestry operations that harvest ipê and other timber species to ensure that they follow a strict sustainable forest management plan. Located in Bolivia -- where ipê is generally sustainably harvested and abundant -- the Angel Sandoval Concession was one of the country’s first and largest businesses to become Forest Stewardship Council (FSC) certified by the Rainforest Alliance. During the 1990s, Bolivia's deforestation rate reached nearly 0.5 percent annually and the country lost 6.5 percent of its forest cover in a fifteen year period (1990 to 2005). Forest loss rates continue to increase: as of 2010, over 741,316 acres (300,000 hectares) of forest are lost annually, according to Bolivia's national forestry agency. In the face of such rampant deforestation, it’s notable that, over the past decade, Bolivia has become a leader in tropical forest certification. More than 3 million acres (1.5 million hectares) of its forests are FSC-certified, making it the third largest FSC certifier in Latin America and the Caribbean. The success of FSC certification in Bolivia promises to help improve forestry practices and conserve the country's globally significant biodiversity.\n- Jukofsky, Diane. Encyclopedia of Rainforests. Connecticut: Oryx Press, 2002.\n- Rainforest Relief: What to Avoid and Alternatives\n- Finca Leola -- Costa Rica Reforestation: http://www.fincaleola.com/Ipe.html\n- Angel Sandoval Concession: http://rainforest-alliance.org/multimedia/angel-sandoval-concession""]"	['<urn:uuid:93d8df16-aaa5-49ba-91ab-e73c5a25e81e>', '<urn:uuid:b0596648-38d9-4ac8-9a2e-1279e9abcc05>']	open-ended	with-premise	short-search-query	similar-to-document	comparison	novice	2025-05-12T21:52:51.670698	6	88	1422
23	I study cell biology and I'm wondering if both the Newcastle disease virus in egg products and its HN protein alone can kill cells - what's the difference in how they work?	Both the complete Newcastle disease virus (NDV) and its HN (hemagglutinin-neuraminidase) protein can affect cells, but through different mechanisms. The complete virus can survive in egg products and organ tissues, requiring thermal treatment for inactivation - specifically temperatures between 55-61°C for varying times to be killed. The HN protein alone, on the other hand, can directly induce cell death (apoptosis) through multiple pathways - it triggers the SAPK/JNK pathway and can downregulate NF-κB expression in cells. The HN protein accomplishes this through its multiple functions including receptor binding, neuraminidase activity, and fusion promotion activities.	"['Submitted to: Journal of Food Protection\nPublication Type: Peer Reviewed Journal\nPublication Acceptance Date: 3/3/2011\nPublication Date: 7/1/2011\nCitation: Chmielewski, R.A., Beck, J.R, Swayne, D.E. 2011. Thermal inactivation of avian influenza virus and Newcastle disease virus in a fat-free egg product. Journal of Food Protection. 74(7):1161-1168.\nInterpretive Summary: The United States produces and exports internationally a large amount of egg product. Although the U.S. is normally free of avian influenza and Newcastle disease viruses, concern about contamination of egg product with these viruses has in the past resulted in restrictions on trade. Because these egg products are normally pasteurized, this study was performed to see if normal pasteurization times and temperatures would be effective at inactivating viruses in fat-free liquid egg product at various times and temperatures of heat processing. Fat free egg product could be categorized as either imitation egg product or liquid egg white as it contained at least 99 percent egg white. The USDA pasteurization standard for imitation egg products is 56.7 degrees C with a minimum holding time of 4.6 min while the pasteurization standard for egg white with unadjusted pH is 57.7 degrees C with a minimum holding time of 6.3 min. These pasteurization standards are based on a 5 log reduction of Salmonella. Both of these standard pasteurization processes were used to inactivate two avian influenza (AI) viruses and two Newcastle disease viruses (NDV) to determine if these pasteurization processes would adequately inactivate 100,000 virus/ ml of fat free egg product. Usage of this data in developing egg pasteurization standards for AI and NDV infected countries should allow safe trade in liquid egg products.\nTechnical Abstract: Avian influenza (AI) and Avian Paramyxovirus Type-1 (AMPV-1) viruses can survive on the carcasses, in organ tissue of infected birds, on fomites, and have the potential for egg transmission and egg product contamination. With the increase in global trade, there are concerns that egg products could potentially present biosecurity problems and affect international trade in liquid and dried egg products. Therefore, the generation of inactivation curves to determine thermal death times (D-value) and heat resistance of the viruses (Z-value) within fat-free egg product (FFEP) would provide valuable information in the development of risk assessment strategies. Thermal inactivation studies using A/chicken/Pennsylvania/1370/83 (H5N2) high pathogenicity AI (HPAI) virus, artificially inoculated into FFEP at 6.25 log10 mean embryo infectious doses (TID50)/ml was heat treated for 0, 1, 2, 3, 4, 6, 8, 12, 15 and sometimes up to 40 min. The resulting thermal death times D55, D56, D56.7, D57, D58, D59 values were 18.6, 8.5, 3.6, 2.5, 0.4, 0.4 min, respectively. The Z-value was 4.4 degree C. For low pathogenicity AI (LPAI) virus A/chicken/New York/13142/94 (H7N2) inoculated at 6.9 log10 TID50/ ml had D55, D56.7, D57, D58, D59, D60 values of 2.9, 1.4, 0.8, 0.7, 0.7, and 0.5 min, respectively, and a Z-value of 0.4 degree C. For virulent Newcastle disease virus (vNDV) AMPV-1/chicken/California/212676/2002 inoculated at 8.75 log10 TID50/ ml resulted in D55, D56, D56.7, D57, D58, D59 values of 12.4, 9.3, 6.2, 5, 3.7, and 1.7 min respectively. The Z-value was 4.7 degree C. While for lentogenic NDV AMPV-1/chicken/United States/B1/1948 inoculated at 8.4 log10 TID50/ ml resulted in D53, D55, D57, D58, D59, and D61 values of 5.3, 2.4, 2.3, 0.62, 0.19, and 0.17 min, respectively and a Z-value of 1.6 degree C. Usage of this data in developing egg pasteurization standards for AI and NDV infected countries should allow safe trade in liquid egg products.', ""Genomes and Genes\nGene Symbol: HN\nSpecies: Newcastle disease virus B1\n- A Y526Q mutation in the Newcastle disease virus HN protein reduces its functional activities and attenuates virus replication and pathogenicitySunil K Khattar\nVirginia Maryland Regional College of Veterinary Medicine, University of Maryland, College Park, MD 20742, USA\nJ Virol 83:7779-82. 2009The hemagglutinin-neuraminidase (HN) protein of Newcastle disease virus (NDV) is a multifunctional protein that plays a crucial role in virus infectivity...\n- HN protein of Newcastle disease virus sensitizes HeLa cells to TNF-α-induced apoptosis by downregulating NF-κB expressionR S Rajmani\nMolecular Biology Lab, Division of Veterinary Biotechnology, Indian Veterinary Research Institute, Izatnagar, Bareilly, 243122, India\nArch Virol 161:2395-405. 2016Hemagglutinin neuraminidase (HN) is a membrane protein of Newcastle disease virus (NDV) with the ability to induce apoptosis in many transformed cell lines...\n- HN Protein of Newcastle Disease Virus Induces Apoptosis Through SAPK/JNK PathwayR S Rajmani\nMolecular Biology Laboratory, Division of Veterinary Biotechnology, Indian Veterinary Research Institute, Izatnagar, Bareilly, UP, 243122, India\nAppl Biochem Biotechnol 177:940-56. 2015..Hemagglutinin neuraminidase (HN), a multifunctional protein of Newcastle disease virus (NDV), is one of such proteins...\n- Roles of the highly conserved amino acids in the globular head and stalk region of the Newcastle disease virus HN protein in the membrane fusion processChengxi Sun\nDepartment of Virology, School of Public Health, Shandong University\nBiosci Trends 9:56-64. 2015..The hemagglutinin-neuraminidase (HN) protein of NDV has multiple functions including mediating hemadsorption (HAD), neuraminidase (NA), and fusion ..\n- Fusion activation through attachment protein stalk domains indicates a conserved core mechanism of paramyxovirus entry into cellsSayantan Bose\nDepartment of Molecular Biosciences, Northwestern University, Evanston, Illinois, USA\nJ Virol 88:3925-41. 2014..Two viral integral membrane glycoproteins (fusion [F] and attachment [HN, H, or G]) mediate a concerted process of host receptor recognition, followed by the fusion of viral and cellular ..\n- Role of C596 in the C-terminal extension of the haemagglutinin-neuraminidase protein in replication and pathogenicity of a highly virulent Indonesian strain of Newcastle disease virusShin Hee Kim\nVirginia Maryland Regional College of Veterinary Medicine, University of Maryland, College Park, MD, USA\nJ Gen Virol 95:331-6. 2014We modified the haemagglutinin-neuraminidase (HN) glycoprotein of the virulent Newcastle disease virus (NDV) strain Banjarmasin/010/10 (Ban/010) by adding C-terminal extensions similar to those found in certain avirulent NDV strains...\n- Measles virus fusion machinery activated by sialic acid binding globular domainAparna Talekar\nDepartments of Pediatrics\nJ Virol 87:13619-27. 2013..to proteinaceous receptors on host cells, while the receptor binding protein of NDV (hemagglutinin-neuraminidase [HN]) interacts with sialic acid-containing receptors...\n- Effects of the HN gene C-terminal extensions on the Newcastle disease virus virulenceWei Zhao\nSoutheast Poultry Research Laboratory, Agricultural Research Services, United States Department of Agriculture, 934 College Station Road, Athens, GA, 30605, USA\nVirus Genes 47:498-504. 2013The hemagglutinin-neuraminidase (HN) of Newcastle disease virus (NDV) is a multifunctional protein that has receptor recognition, neuraminidase, and fusion promotion activities...\n- HN gene C-terminal extension of Newcastle disease virus is not the determinant of the enteric tropismWei Zhao\nCollege of Veterinary Medicine, Northwest A and F University, Yangling 712100, Shaanxi, China\nVirus Genes 47:27-33. 2013The hemagglutinin-neuraminidase (HN) protein of Newcastle disease virus (NDV) plays an important role in virus pathogenicity and tissue tropism...\n- The second receptor binding site of the globular head of the Newcastle disease virus hemagglutinin-neuraminidase activates the stalk of multiple paramyxovirus receptor binding proteins to trigger fusionMatteo Porotto\nDepartments of Pediatrics and of Microbiology and Immunology, Weill Medical College of Cornell University, New York, New York, USA\nJ Virol 86:5730-41. 2012The hemagglutinin-neuraminidase (HN) protein of paramyxoviruses carries out three distinct activities contributing to the ability of HN to promote viral fusion and entry: receptor binding, receptor cleavage (neuraminidase), and activation ..\n- A mutation in the stalk of the newcastle disease virus hemagglutinin-neuraminidase (HN) protein prevents triggering of the F protein despite allowing efficient HN-F complex formationAnne M Mirza\nDepartment of Microbiology and Physiological Systems, Immunology and Microbiology Program, University of Massachusetts Medical School, Worcester, Massachusetts, USA\nJ Virol 87:8813-5. 2013..disease virus (NDV)-induced membrane fusion requires formation of a complex between the hemagglutinin-neuraminidase (HN) and fusion (F) proteins...\n- Newcastle disease virus fusion and haemagglutinin-neuraminidase proteins contribute to its macrophage host rangeIngrid Cornax\nSoutheast Poultry Research Laboratory, Agricultural Research Service, United States Department of Agriculture, Athens, GA 30605, USA\nJ Gen Virol 94:1189-94. 2013The fusion (F) and haemagglutinin-neuraminidase (HN) proteins of Newcastle disease virus (NDV) are multifunctional proteins that play critical roles during infection...\n- 'a'-Position-mutated and G4-mutated hemagglutinin-neuraminidase proteins of Newcastle disease virus impair fusion and hemagglutinin-neuraminidase-fusion interaction by different mechanismsFu Lu Chu\nDepartment of Virology, School of Public Health, Shandong University, Jinan 250012, China\nIntervirology 56:27-36. 2013To determine the effects of heptad repeat regions (HRs) and N-linked carbohydrate sites of the Newcastle disease virus hemagglutinin-neuraminidase (HN) protein on fusion of HN and fusion (F) proteins and HN-F interaction.\n- [Induction of apoptosis in human hepatoma cell line SMMC7721 by Newcastle disease virus HN gene]Ying chun Sun\nGenetic Engineering Labotory of PLA, Academy of Military Medical Sciences, Changchun 130062, China\nZhonghua Zhong Liu Za Zhi 27:279-82. 2005To investigate the mechanisms of apoptosis induced in human hepatoma cell line SMMC7721 by plasmid pVHN constructed with Newcastle disease virus (NDV) HN gene.\n- Role of the two sialic acid binding sites on the newcastle disease virus HN protein in triggering the interaction with the F protein required for the promotion of fusionPaul J Mahon\nDepartment of Microbiology and Physiological Systems, University of Massachusetts Medical School, 55 Lake Avenue North, Worcester, MA 01655 0002, USA\nJ Virol 85:12079-82. 2011..Newcastle disease virus (NDV)-induced membrane fusion requires an interaction between the hemagglutinin-neuraminidase (HN) attachment and the fusion (F) proteins, triggered by HN's binding to receptors...\n- Positive selection in the hemagglutinin-neuraminidase gene of Newcastle disease virus and its effect on vaccine efficacyMin Gu\nAnimal Infectious Disease Laboratory, School of Veterinary Medicine, Yangzhou University, Yangzhou 225009, PR China\nVirol J 8:150. 2011..the relationship between the selective pressure and the sequence variation of the hemagglutinin-neuraminidase (HN) protein, we performed the positive selection analysis by estimating the ratio of non-synonymous to synonymous ..\n- The anti-tumor effect of Newcastle disease virus HN protein is influenced by differential subcellular targetingHong Sui\nMedical Department, The Tumor Hospital Affiliated Harbin Medical University, Harbin, Heilongjiang, China\nCancer Immunol Immunother 59:989-99. 2010..A DNA vaccine expressing the major immunogenic hemagglutinin-neuraminidase (HN) protein of this virus has shown promising results as an immunotherapeutic agent.\n- Genetic comparisons between lentogenic Newcastle disease virus isolated from waterfowl and velogenic variantsRyota Tsunekuni\nAvian Influenza Research Centre, Kyoto Sangyo University, Kamigamo Motoyama, Kyoto 603 8555, Japan\nVirus Genes 40:252-5. 2010..Seven amino acid substitutions were detected in the M, F, and HN proteins...\n- Activation of natural killer cells by newcastle disease virus hemagglutinin-neuraminidaseMostafa Jarahian\nGerman Cancer Research Center, Heidelberg, Germany\nJ Virol 83:8108-21. 2009..Hemagglutinin-neuraminidase (HN) of the virus serves as a ligand structure for NKp44 and NKp46, as indicated by the blockade of binding to NDV-..\n- [Evaluation of Newcastle disease virus with derivated hemagglutinin-neuraminidase gene of mesogenic strain]Yong Wang\nNational Key Laboratory of Veterinary Biotechnology, Harbin Veterinary Research Institute, Chinese Academy of Agricultural Sciences, Harbin 150001, China\nWei Sheng Wu Xue Bao 48:638-43. 2008The hemagglutinin-neuraminidase (HN) and fusion protein of Newcastle disease virus (NDV) plays a crucial role in the process of budding and infection...\n- [Correlation of the neutralization index in chicken embryo with the homologies of F and HN gene of different Newcastle-disease isolates]Zhuoming Qin\nInstitute of Poultry Science, Shandong Academy of Agricultural Sciences, Jinan 250100, China\nWei Sheng Wu Xue Bao 48:226-33. 2008..7%-84.1%. In addition, these NDV isolates all shared 95.2%-100% nt homology with the hemagglutinin-neuraminidase (HN) gene, whereas only 79.1%-84.3% compared these viruses with La Sota...\n- HN protein of Newcastle disease virus causes apoptosis in chicken embryo fibroblast cellsP V Ravindra\nDivision of Animal Biotechnology, Indian Veterinary Research Institute, Izatnagar, 243122, India\nArch Virol 153:749-54. 2008..In the present investigation, the ability of haemagglutinin-neuraminidase (HN) protein of NDV to cause apoptosis in CEF cells was examined...\n- Incorporation of functional HN-F glycoprotein-containing complexes into newcastle disease virus is dependent on cholesterol and membrane lipid raft integrityJason P Laliberte\nDepartment of Molecular Genetics and Microbiology, Room S5 250, University of Massachusetts Medical School, 55 Lake Avenue North, Worcester, MA 01655, USA\nJ Virol 81:10636-48. 2007..reduced fusion capacity of particles released from cholesterol-extracted cells correlated with significant loss of HN-F glycoprotein-containing complexes detected in the virion envelopes of these particles and with detection of cell-..\n- [Newcastle disease virus heredity mutation and correlation of HN and F gene]Zhuo ming Qin\nCollege of Animal Science and Technology, Shandong Agriculture University, Tai an 271018, China\nWei Sheng Wu Xue Bao 46:227-32. 2006..The fusion protein (F) gene and hemagglutinin-neuraminidase (HN) gene of these isolated viruses were cloned and sequenced...\n- Inhibition of receptor binding stabilizes Newcastle disease virus HN and F protein-containing complexesL W McGinnes\nDepartment of Molecular Genetics and Microbiology, University of Massachusetts Medical School, 55 Lake Ave North, Worcester, Massachusetts 01655, USA\nJ Virol 80:2894-903. 2006..To explore the relationships between Newcastle disease virus (NDV) HN and F protein interactions and HN protein attachment to sialic acid receptors, HN and F protein-containing complexes ..""]"	['<urn:uuid:c497fd74-0790-4c9c-b835-ae633691a815>', '<urn:uuid:3fb8b0fc-ad21-4632-9db5-3a00c9c4db87>']	open-ended	with-premise	verbose-and-natural	distant-from-document	comparison	novice	2025-05-12T21:52:51.670698	32	94	2092
24	need info cleaning maintenance regular dental bridges compared maryland bridges reliability long term based experience	For traditional dental bridges, maintenance involves using special bridge threaders for cleaning under the pontic (replacement tooth), as regular flossing isn't possible. Bridge threaders are readily available in dental stores and pharmacies. Regarding long-term reliability, conventional Maryland bridges have shown poor durability with a 15% annual debond rate based on an international survey of over 7,000 cases. In contrast, improved inlay adhesive bridges (an enhanced version of Maryland bridges) have demonstrated exceptional reliability, with a clinical track record showing no maintenance requirements over 12 years of function in over 400 cases. After an initial prototype phase where there were 7 failures in the first 20 placements, subsequent placements have maintained a near-perfect success rate, performing as well as or better than implants.	"['Inlay Adhesive Bridges\nOne of the techniques taught in my study groups is an anterior inlay adhesive bridge. While resembling the ""Maryland"" retainer design, viz., a thin extracoronal flange with accessory external retention and resistance features, this design is utterly different.\nThe flange component is retained, but an intracoronal inlay is used to index the retainer to the tooth definitively, to confer anti-rotation features, and secure resistance to displacement in the incisal or gingival direction. This has proven a breakthrough in reliability. Over a period of ten years, over 400 retainers have been placed, primarily in un-ideal situations.Following an initial prototype phase of development of the design, every unit placed is still in function after 12 years.\nCosmetics are better than conventional Maryland bridges: there is no darkening of the abutment,provided the correct opaque cement is used. As well,there is no facial show of metal or greying at the line angles.\nAll the multi-tooth splints made with this method in my practice over the last 12 years are still functioning.I am speaking of 500 successful abutments. There has been no clinical maintenance required on any of these placements. This is not a sketchy or provisional prosthesis. This is reliability as good or better than implants.\nIn the prototype stage of the first 20 placements, there were 7 failures: 2 debonds, 2 carious failures, and three teeth lost to terminal periodontal collapse.\nHow much better is this than conventional Maryland Bridges?\nTo those familiar with the clinical practice and literature of conventional Maryland bridges, these survival rates utterly defy comparison. Typically, according to an international survey of over 7,000 cases of Maryland Bridges,(Wyatt et al) a 15% annual de-bond rate is normal in conventional Maryland splint method.\nA typical periodontal splint\nA typical periodontal splint is seen in the following three pictures. This patient presented with mobility, diastemas due to flaring of the lower incisors, and a crown/root ratio of 3:1. Her chief complaint was that the teeth felt loose and painful and she could no longer eat apples. This splint has been unchanging and successful and comfortable for the last 12 years. The main clinical obstacle was that diastemas existed between the teeth. The metal framework would show through to the facial as unsightly black interproximals if something was not done. The clinical solution was to opaque the metal framework after cementation as follows: the metal framework and the adjacent proximal tooth surfaces were sandblasted with 37 grit aluminum oxide for ten seconds. The teeth were acid etched with 37% phosphoric acid, and a bonding agent was placed and cured. (Photobond, a MDP-based adhesive, Kuraray Dental Corp). The metal was opaqued (Kerr Kolor A3 opaque) and the diastemas filled with an opaque flowable (Starfill 2B, Danville Dental).\nThe result has been cosmetically stable for the past 12 years.\nAs can be seen by the photos, clearly, her plaque scores remain high. However, the periodontal collapse cycle has been interrupted. The forward feedback loop in the syndrome of mobility/widened PDM/osteoclasia has been broken by stabilization, osteoclasia has ceased or at least greatly attenuates.Bone level on radiographs has stabilized in this patient\'s case.\nThis is the typical result seen when periodontally splinting with this method. In some cases, there continues to be a decline, but always at a much slower rate, particularly if the etiological factor has been smoking and the patient has successfully quit.\nIs mobility a contraindication?\nIn conventional Maryland method, mobility of the abutments contraindicates placement. In my method, quite to the contrary, periodontal mobility is an indication for placement. This opens a whole new stream of treatment and practice income. It is hard to qualitatively appreciate the satisfaction for patients otherwise facing options of removal of the lower 6 anterior teeth and replacement with two implants and a fixed bridge,at a cost of half a new car. Or else a Kennedy Class IV RPD with attendant discomfort, aesthetic compromise, and anterior-posterior rocking and instability.\nWhy would this not be a preferred option? At much the same cost as RPLD,and usually, as an insured service,this is superior treatment. Emotionally, compared to the invasiveness and cost ( 600% higher) of implants and traumatic loss of one\'s identity, it is a much more agreeable choice for the patient.Also, the range of treatment relative to implants is greater, in that bone quality and quantity do not always allow implant treatment to proceed.\nVariations based on this method\nUsing this retainer design, many variations have been explored: as an alternative to lab-fabricated porcealin pontics, composite resin pontics can be fabricated chairside.Long spans that include the bicuspids when the path of insertion prmits, of up to 10-units. Metal-to-metal joints over pre-existing and periodontally-failing ceramco crowns. Single tooth cantilever pontics. Porcelain pontics placed into grafted immediate extraction sockets. Precision partial denture attachments such as ERA or Preci-Vertix?, fixed to a multiple tooth splint or single tooth retainers to optimize partial denture fabrication.\nTreatment for apnea patients\nThis device has also helped to stabilize mobile lower anterior segments loosened by MADs (sleep apnea Mandibular Advancement Devices), a newly recognized and prevalent sequel to this treatment, which the literature is recording as collateral to as high as 10% of all mandibular advancement apnea treatment.\nSplints are fabricated on a refractory model, primarily in non-precious cast-chrome alloy. After casting,these are not electrolytic-etched but rather sandblasted chairside immediately prior to placement. A typical laboratory fee of $200to $300 (2017 lab fees) is commensurate with the lack of difficulty or complexity in fabrication; a partial denture framework is far more complex for the lab to make.\nPublications about this method\nIn terms of clinical ease, a simple protocol has been developed and is described in the CARDP Journal. ""A New Prosthetic Retainer for Splinting and Tooth Replacement"", Journal of the Canadian Academy of Restorative Dentistry and Prosthodontics, Spring 2012, Vol 5-1, pgs. 24-38. http://cardp.ca/wp-content/blogs.dir/1/files/CARDP_V5N1-Spring12.pdfexternal link\nThis article covers the Inlay Maryland concept and technique and various applications for its use, and details a highly cosmetic case replacing a lateral incisor, cantilevering from a cuspid.Preparation,instrumentation, temporization,and cementation are described.\nAdvantages compared to resin/fiber splints in failing mouths\nCompared to direct splinting with wire or fibers, this is a more effective restoration. It is less bulky, has higher strength by several orders of magnitude, and it provides continuity for anterior sextants as a whole even when the root of a vulnerable tooth is lost to endodontic or periodontal disease.The diseased root is amputated in situ and the visible crown retained on the splint, as a ""natural tooth pontic"". The amputated gingival surface is obviously a cosmetic defect for the patient, so two or three weeks later, or whenever the soft tissue has healed after extraction, the tooth is lengthened to a normal gingival extent as an ovate pontic using cosmetically appropriate flowable composite resin. The protocol involves tapering the enamel, sandblasting, acid etch,adhesive, and flowable resin. The viscosity of the flowable is chosen to naturally form into a teardrop below the amputated tooth. Of course it is shaded correctly and an appropriate opacity is chosen as well. The tooth does not have to reach the gingival tissue; in fact, it is preferable from the hygiene standpoint if it doesn\'t. The important factor is to close the perception of a black space. This often may require lengthening the proximal contact with the adjacent teeth. The protocol for this is to sandblast, bond and place flowable.\nSingle-tooth cantilever treatment\nIn the above picture, tooth #12 is a porcelain pontic, cantilevered from #13. The retainer design is a shallow, guitar-shaped inlay, surrounded by a Maryland-type flange. The inlay portion delivers retention and accuracy in seating without facial show of metal to darken the abutment and spoil the case. The path of insertion is slightly inciso-lingual rather than lingual.\nTo complement the resistance form of the inlay, the flange delivers huge bonding area; the luting sequence, Panavia F2.0 Opaque, acid-etched, delivers over 25MPA with a degradation cycle measured in decades. Enamel provides the most adhesive and most durable bonding substrate we have in dentistry, does it not?\nTo provide a parallel context, we have the same bonding area and substrate as porcelain laminate veneers, supplemented with a highly retentive inlay, all in a metal without the fragility of porcelain. The flange adhesion is complemented by the inlay, which mechanically keys the restoration to place regardless of tooth mobility, and provides resistance form against gingival or incisal displacement. As well, the inlay accepts any twisting forces that one might expect in use, and spares the flange from a gradual accumulative fatigue that one would expect form mobile teeth. Should work, shouldn\'t it?\nWhat luting protocol is used? The specific luting agent used in this method is Panavia F2.0, Opaque.Do not use the tooth-colored or transparent formulation. A dentin bonding protocol is followed which bypasses the proprietary Panavia S/E DBA which unfortunately only deliver under 10 MPa. (See Reality Research for their bench assessments of adhesive outcomes between S/E and total etch protocols.)\nThe dentin is instead bonded as follows: after a 15-second etch with liquid 37% phosphoric acid, it is rinsed with distilled water, and lightly air dried until all standing water is gone. Then it is primed with Microprime B,(Danville Dental), air dried thoroughly and bonded with Kuraray Photobond, a thin-film Generation V dual-cure bonding agent, which is also air-dried thoroughly. It can be light-cured or not as desired, as it is DC.\nThe literature reports about 20MPa for Photobond\'s dentin bonding.\nWhy does this make sense? Photobond is Panavia without filler. The adhsive monomer is MDP, by now proven to be one of the best if not the best monomer. In this writer;s experience,this has proven to be durable,versatile, stain-free,and without sensitivity as a bonding method. It also has succeeded in the hands of many study club members.\nAn important feature is the effect of primers-either benzethonium or gluteraldhedye primers in resisting degradation of the hybridized layer by MMPs. (Metallic Proteases).\nThe esthetics of the cantilever pontic in this case is enhanced by relieving the tissue surface of the stone model 2mm in the laboratory phase, and fabricating the pontic tissue surface to a bullet-form or fully ovate pontic surface. Bone sounding at the preparation appointment revealed that the bone depth below the tissue surface would require relief at insertion, so some bone was removed to ensure adequate ""biological width"", meaning adequate width for healthy bone and soft tissue under the pontic.\nSurgical Placement of pontics\nAt try-in, a small H-shaped incision is made in tissue to the depth of bone, respecting and leaving undistrubed at least 1mm of the papilla as a collar remaining around the teeth. The flap is raised where the pontic inserts subgingivally. Sufficient bone is removed using an irrigated dentate oral surgery bur (e.g.OSFG 1558) to allow seating of the inlay.Once seating is confirmed, more bone is taken if needed to create ""biological clearance"" from bone and to allow the normal tissue apparatus of connective tissue and epithelium to reform in the healing stages.\nOptimizing enamel bonding\nObviously the next step is to regain isolation so that impeccable bonding can be achieved. The first step is to place retraction cord in the sulci of the abutment and adjacent teeth. The cord chosen should contain epinephrine. Two products that have been successful are Gingibraid 0e, which contains 8% and aluminum potassium sulphate 7%, and Pascal Siltrax EPI #8 or #9. which contain epinephrine alone.\nThe lingual surface of the abutment should have been sandblasted for 10 seconds with 37 grit aluminum Oxide with an intraoral sandblaster such as the Danville Microetcher prior to the surgery.This removes the etch-resistant hyperfluoridated outer 10 microns of amorphous remineralized enamel and exposes the structurally cohesive underlying enamel rod ends for optimum and reliable adhesion. This step must not be overlooked , as enamel bonding is not optimized without it. I feel certain that it accounts for my very high success rate. A tidy and rapid method is to place a dental dam with a single hole over the abutment This contains the dust and makes it more comfortable for the patient.\nEtching in a surgically contaminated field\nClotting takes about 7 minutes to develop following the flap and surgical phase. Essentially a platelet plug is forming. The abutment(s) should be isolated under rubber dam and etched with 37% liquid phosphoric acid. In the final seconds of the 15 second etch, the etchant is titrated with Hemodent (Pulpdent) (buffered aluminum chloride) or a unbuffered saturated solution of aluminum chloride, prepared by a local pharmacy at lower cost. The site is then rinsed gently with distilled water delivered with a Monoject 412 15cc plastic syringe. Do not use a triple syringe as not only is the water stream contaminated with waterline disinfectants, but it is difficult to deliver an irrigating stream that is gentle enough to not disturb the platelet plug.\nRemove the rubber dam The buccal flap is approximated over the facial surface of the pontic, without suturing, healing by secondary intention and thus allowing an increased bulk of keratinized gingiva to cover the pontic, i.e., acting as a soft tissue augmentation procedure.\nThe final result\nThe pontic appears to emerge from beneath the tissue, The papillae are full and correct height, although on close examination one can see that the papilla lack facial bulk.\nComparison to an implant; cost,hygiene, difficulty of execution\nThis simple but essentially sophisticated restoration provides the patient with a flossable, attractive and conservative tooth replacement at about 10 to 15% of the cost of implant therapy. The pontic is flossed in much the same way as an implant, by forming a crossed loop around it like a tourniquet and sliding it subgingivally down the neck of the pontic.\nIt would have been difficult to achieve an equivalent result aesthetically in this case with implant therapy, given the proximity of the adjacent roots, rotation of the clinical crown, thin tissues, and deficient volume of the labial ridge. In other lateral incisor replacement cases, where root convergence has not been eliminated by orthodontic treatment,this restoration can treat the edentulous area without compromise or risk.\nWhat\'s not to like? Exposure of dentin?\nClinicians familiar with the unreliability of the conventional Maryland Bridge might, at first glance, reject the penetration into dentin of this method. Maryland retainers, at a 15% annual debond rate should not be allowed to penetrate into dentin due to the risk of secondary caries. In other words, when your Maryland bridge is going to debond, and you expect that from the outset, it is better that it be placed solely in enamel, which won\'t decay as readily as dentin.\nHowever, with the Inlay/Flange method, which has virtually no failure rate, the penetration into dentin with this retainer is quite a different proposition. If your retainer is not going to fall off, then what is the problem with dentin penetration? If you don\'t agree, ask yourself, do we not accept full crown reduction into dentin? Is this not standard practice 24/7 around the globe? If we can strip crowns wholesale of dentin, what\'s wrong with a little inlay? There is a significant record with full coverage crowns un-luting, my friend,lest we live in illusions.\nPeriodontal and Endodontic Morbidity?\nPeriodontally, this supra-gingival retainer design is innocuous.\nEndododntic morbidity is absent, unless the preparation is clumsy and incautiously deep.\nCourse of Healing\nThe 3 pictures below shows the appearance at one month,at three months and at five years. At one month, healing is not yet complete, as seen from the residual inflammation surrounding the pontic emergence. At 3 months, when granulation tissue has been replaced with keratinized tissue, the problem has corrected itself. Long-term stability is seen in the 5 year photo.\nNote that in this case the aesthetics of the smile was balanced by mimicking the rotation and coronal inclination of contra-lateral lateral incosor. This enabled the pontic to be its appropriate mesio-distal width, so that the patient\'s smile has credible left/right symmetry. Initially the lab fabricated a conventional pontic,of too narrow a facial dimension, despite instructions to make the pontic in labioversion. The faulty pontic was corrected by a mock-up in flowable resin an sent back for revision. This provided a blueprint to the lab for an improved final.The shading of the cervical is not quite correct but the patient was satisfied and a low lip line hides this imperfection.\nThe interplay of tissue emergence, inconspicuous retainer design, opaque cementation,and attention to tooth form create an unobtrusive result in this high cosmetics case. It is top value for the patient,taking less than 1.5 hours of chair time between the two appointments.The lab fee was slightly more than a single PFM crown. Consider how that compares to a single tooth implant with grafting and soft tissue management in cost, value, comfort, and expediency.\nThe dentin in the inlay was treated with Microprime B and a soft resin inlay (Ez-Temp Onlay, Cosmedent)was placed between appointnemts, taking minimum chair time. No temporary was fabricated for the flange.\nThree Week Appearance\nThree month Appearance\nFive Year Appearance', 'What is a dental bridge?\nA dental bridge is a fusion of two or more fabricated dental crowns that function to restore chewing and fill the space of a missing tooth or multiple teeth.\nA bridge is comprised of two components, abutments and one or more pontics.\nAbutments are best described as the teeth with roots (gray color in illustration below) that anchor or support the entire dental bridge.\nPontics are replacement teeth (crown portion only, no tooth root) that fill the space of missing teeth between abutment teeth.\nThe bridge abutments and pontic crowns are fused together and are permanently cemented over dentist prepared teeth. Dental bridges are a permanent tooth replacement option.\nHow many teeth can a dental bridge replace?\nA bridge can replace multiple teeth if necessary. How many teeth are missing along with the health of the abutment teeth will determine the number of anchor and replacement teeth necessary in order to restore optimal chewing function and fill the missing tooth space.\nWhy should I consider replacing my missing tooth with a dental bridge?\n- Restore the visible appearance of you smile\n- Restore existing tooth space with a replacement tooth\n- Restore the ability to effectively chew in the area of missing teeth\n- Prevent premature deterioration of existing teeth due to increased occlusion (biting) forces to compensate for missing tooth/teeth\n- Prevent bite collapse. Mother Nature does not like a void. The adjacent and opposing teeth will shift to fill the space of the missing tooth or teeth over time.\n- Prevent remaining teeth from drifting out of their current position known as hypereruption\n- To maintain facial definition. Our jaw and teeth define many of our outward appearance facial features\nWhat types of dental bridges are available?\nThere are four main types of bridges available.\n- Traditional dental bridge\n- Cantilever dental bridge\n- Maryland dental bridge\n- Implant supported dental bridge\nUpon completion of a dental examination, your dentist will discuss the best available treatment options so that you can make an informed decision regarding proposed dental treatment.\nTraditional dental bridge\n- Replaces a single missing tooth\n- Dental bridge is comprised of two abutment teeth and one pontic\nCantilever dental bridge\n- Replaces a single missing tooth\n- Cantilever bridge is comprised of only one abutment teeth and one pontic\nMaryland dental bridge\n- Replaces a single missing tooth\n- Maryland dental bridge is comprised of two abutment “wings” and one pontic\n- Abutment wings are resin bonded to the backside of the two abutment teeth\nImplant supported dental bridge\n- Bridge supported by dental implants instead of natural teeth\n- Implant supported dental bridge comprised of two or more dental implant abutment teeth and one or more pontics\nAesthetic dental bridge options\nPorcelain dental bridge\n- Most popular\n- Tooth colored porcelain fabricated to made shade of existing teeth\nPorcelain fused to metal dental bridge\n- Tooth colored porcelain to match shade of existing teeth with a metal core (inside)\nGold dental bridge\n- Gold colored\nAcrylic temporary bridge\n- Tooth colored acrylic shade to match existing teeth\n- Fabricated and temporarily cemented following your bridge preparation appointment\n- Fabricated and temporarily cemented immediately following a tooth removal in a highly visible part of the mouth. An acrylic temporary bridge serves as an interim bridge while your tooth extraction site heals, typically 10-12 weeks. Once healed, your dentist can then prepare abutment teeth for a permanent dental bridge.\nDental bridge procedure\nThe bridge procedure is pain free and typically requires two dental appointments.\n- 1st Appointment (2 hours): bridge preparation, impressions, fabrication and placement of a temporary dental bridge.\n- 2nd Appointment (40 minutes): bridge cementation, try in permanent bridge and ensure a custom fit prior to final cementation.\nDental Bridge Preparation Appointment: 2 hours\n- Digital x rays and a photograph of the teeth are taken prior to preparation of teeth.\n- A shade will be chosen that closely matches your existing teeth.\n- A preliminary dental impression is taken and is used to fabricate a custom temporary bridge after preparation procedures are complete. This temporary bridge will be temporarily cemented in place while the dental lab fabricates your permanent dental bridge.\n- Pain free anesthesia is administered.\n- Once your teeth are completely numb feeling, the dentist will prepare your teeth by reducing the size of the abutment teeth to custom fit a permanent bridge.\n- After teeth are prepared, the dentist will take a series of dental impressions of the teeth and of your bite. These impressions are then sent to the dental lab and used to fabricate your permanent bridge.\n- Finally, your dental assistant will temporarily cement your temporary bridge in place, review post appointment instructions, and schedule a future appointment to permanently cement your fabricated dental bridge.\n- Note:Additional amenities including nitrous oxide (aka laughing gas), dentist noise cancelling headphones, and cable tv/music are available\nDental Bridge Cementation Appointment: 40 minutes\n- Dental assistant will remove temporary dental bridge\n- Dentist will place permanent bridge and perform a series of tests to ensure that fabricated bridge fits your teeth and fills the missing tooth space perfectly.\n- Once the bridge is checked and fits perfectly, the dentist will permanently cement your bridge in place and will review post appointment instructions.\n- Note:The dentist will recheck your bridge at every future preventive visit.\nCall (763) 586-9988 to schedule an appointment\nOr click the button below to request an appointment online.\nFrequently Asked Questions regarding Dental Bridges\nQ. What is the difference between a dental bridge and a dental implant?\nA. A dental implant is a single, entire tooth replacement (root and dental crown) used to restore chewing and fill the space of a missing or previously extracted tooth. There are no abutments or pontic.\nQ. Is it better for me to replace my missing tooth with a dental bridge or with a dental implant?\nA. It depends. Dental bridges and dental implants are both viable, permanent tooth restoration and replacement options to consider. There are often advantages of electing on option over another. Your dentist will discuss your options and explain the pros and cons of each option so you can make an informed decision about which option is best for you.\nQ. Is the dental bridge procedure painful?\nA. No, the dentist will make sure your tooth is comfortably numb prior to beginning tooth preparation procedures. Additional amenities including nitrous oxide (aka laughing gas) are available.\nQ. How do I care for my temporary bridge?\nA. You’ll want to brush and floss. A bridge threader is required to clean the space under the pontic. Should you accidentally pull your temporary bridge off, just call us at (763) 586-9988 and we’ll recement it back into place.\nQ. How do you floss under a dental bridge?\nA. You won’t be able to floss your bridge the same way as you floss your other teeth. A bridge threader is used to clean under your dental bridge.\nBridge threaders can be purchased in the dental aisle of your favorite store or pharmacy. There are many styles available. Your dental assistant will dispense an initial supply of bridge threaders and will show you ho to floss under your new bridge.\nHow to use a bridge threader\n- Dispense a 8-12 inch piece of dental floss.\n- Insert the dental floss through the eye of a dental bridge threader.\n- Insert the end of the dental threader at the base between your bridge pontic and abutment. Push threader through till visible on the inside of the mouth.\n- Hold your floss in one hand and with the other hand remove the bridge threader.\n- Floss the side of abutment tooth. Slide the floss under the pontic to the other abutment tooth. Floss the side of the abutment tooth.\n- Remove floss strand from underneath bridge by pulling floss out backwards to exit mouth.\nBelow is a short Youtube video demonstrates how to use a dental bridge threader. Click here to watch video on Youtube.\nQ. Are there any foods that I should avoid in the interim till my bridge is permanently cemented?\nA. It is best to avoid sticky foods like candy and chewing gum as well as any extra crunchy foods like nuts.\nQ. Will my teeth be sensitive afterwards?\nA. It is not uncommon for the teeth to be extreme temperature sensitive initially (2-3 days) following your bridge appointment. Using a desensitizing toothpaste in the interim can help.\nQ. What happens if my temporary bridge becomes loose?\nA. Don’t panic. Remove the temporary bridge to avoid accidentally swallowing it and call us as soon as you can. We’ll temporarily recement your temporary bridge back into place.\nIf you are unable to come in, most drug stores sell temporary dental cement. Just be sure to follow the package instructions.\nNote: It is very important to have your temporary bridge in place the entire time while your permanent one is being fabricated. If the temporary bridge falls out and is not recemented in a timely fashion, the adjacent teeth will begin to shift and potentially impact the fit of the permanent dental bridge.\nQ. Are dental bridges covered by dental insurance?\nA. Most comprehensive dental insurance plans provide some level of dental bridge coverage. This varies among dental insurance plans. Prior to scheduling your bridge appointment, one of our patient care coordinators will review your specific dental insurance coverage with you and inform you of any plan deductibles and/or estimated out of pocket expenses.\n- Some dental insurance companies have a missing tooth clause.\n- A missing tooth clause is hen a dental insurance company declines insurance coverage to replace a missing tooth that was extracted prior to existing dental insurance coverage contract.\nQ. What if I don’t have dental bridge coverage?\nA. Dentistry for the Entire Family offers a cash discount for patients without dental insurance coverage. Need flexible payment options? Click here to learn more about available financing options through Care Credit and Wells Fargo.\nQ. What is the difference between a traditional and a cosmetic dental bridge?\nA. An individual may elect to have a cosmetic bridge placed to improve or enhance the visible aesthetics of their smile. Cosmetic bridges are not typically covered by dental insurance plans.\n- People who are born without a permanent tooth to replace an exfoliated primary (baby) tooth.\n- People who have excessive crowding of teeth who do not wish to have teeth straightened by orthodontic therapy\nNeed to schedule an appointment?\nCall (763) 586-9988 to schedule an appointment or click green button below to request an appointment online.']"	['<urn:uuid:c9e34ed5-e873-455c-b750-a3b904d4c7d7>', '<urn:uuid:a3c30c5c-127b-40f5-9040-0ddcbb346372>']	open-ended	with-premise	long-search-query	distant-from-document	multi-aspect	novice	2025-05-12T21:52:51.670698	15	122	4619
25	My child was just born with dwarfism and I'm worried about school - what challenges should I expect when they start education?	Based on past experiences, schools may have concerns about cognitive and learning impacts of physical disabilities. Practical challenges include the need to adjust physical accommodations like the height of coat hooks, tables, and chairs. It's also important to prepare children with appropriate language to respond to questions about their differences.	['Good Kids: How to Empower Your Kids to Love Their Differences with Sinéad Burke\nGood Kids transcript — How to Empower Your Kids to Love Their Differences with Sinéad Burke\n[00:47] Hi, I’m Sinéad Burke and this is Good Kids.\n[01:02] I’m not a parent. I don’t have any children. It’s not an ambition of mine just yet, though I would like to be a mother. Who knows when. And I think I would like to be exactly as my parents. They are extraordinary people, and it’s only now that I’m an adult that I realize how unique and powerful their parenting was. You see, I’m disabled, I have dwarfism, I have a achondroplasia, which is the most common form of dwarfism. I stand at the height of three feet, five inches tall. And in terms of identity and language, I describe myself as a little person. My dad is also a little person. My mom is average height, like many of you listening to this. And my siblings, I have four of them, they’re all average height, too. Having my dad in my life every day — and somebody who looked like me, genetically — transformed my whole existence. There’s that phrase in advocacy circles that if you can see it, you can be it. And it was never explained to me that I was different, or that I would have challenges. I was just like my dad. And I knew innately that everything would be OK because my dad had achieved and succeeded and thrived and survived. So why wouldn’t I?\n[02:30] But it wasn’t the fact that my dad was a little person that I think makes me a good kid, or somebody who at least tried to be. What made the difference is the fact that I’m a loved child. It’s tangible and has been for my entire existence. And that love manifested in so many different ways. In care, in security and safety, in nourishment, in encouragement. But more than anything, it was my parents extraordinary belief that I could do anything. In a way in which the world disagreed with every turn, but that social conditioning was never, ever transferred to me. I started school on the day of my fourth birthday, the 19th of September 1994. And I started on that day because my principal worried that my physical disability could have cognitive and learning impacts, and that I needed to start on the day of my fourth birthday, because if academically I didn’t have the same educational attainment as everybody else, I needed to be young in case I had to repeat a year. My parents constantly told the school that I would be OK. That I had reached all of the milestones that a child at four was supposed to reach. But it was a different era. There wasn’t an understanding, or an openness, to what inclusion could look like. There was still segregation in terms of education for those with disabilities, and there still is. But my parents were the advocates I needed when I couldn’t advocate for myself.\n[04:09] Simple things such as the height of the coat hook — when I went to school and was taking off my coat, and where would I put it? The height of the tables and chairs that I would sit at. The language that the teachers needed to have to make sure that I was included. The language that I needed to have that in the playground, if a child said, why are you so small? From the age of four, I knew that the automatic response had to be, ‘I was born like this.’ Because I was. This wasn’t something I could control. And yet on my first day of school, when I came home and told my parents that I wanted to be a teacher, they had this fundamental belief that I could do it. But I reflect now on that moment as somebody who’s 28. And if I had a child, I hope that that would be my automatic response. And I’m embarrassed to admit that I don’t know if it would be because we live in a world where we are constantly told that only certain types of people can achieve and do and be successful.\n[05:09] And yet my parents had this overwhelming sense of self that, why not try? When I went through school and got my place in college, I received this form from the Department of Education, which was called the Fitness to Teach Form. There was two questions on it that I distinctly remember. One, how long can you run for? And two, do you get tired easily? I sat with my mother and I said, what do we do? Will we answer these questions truthfully and risk not getting my place in college, even though I’ve worked hard and I’ve met all of the criteria and I deserve to be there? Or will I lie and show up on the first day like tah-dah! To everybody’s surprise. And she said no, we’ll tell the truth. And if you don’t get it, we’re gonna fight it in court. Because this document has been designed by people who do not look and think like you.\n[06:10] The way in which they view teachers is stereotypical and archaic. It’s only right that you exist in those spaces. And in terms of how to raise a good kid, what my parents did that I would like to imbue and mirror should I ever have a child is making sure that empathy, kindness, curiosity and ambition are things to be embraced and celebrated and encouraged. But not to be overprotective in a way that exploration and mistakes can’t happen. But to encourage difficult conversations — I will never forget, I was walking down the street in Dublin and a car pulled over. And there was four people in the car. They pulled over right where I was standing. They wound down the windows. They put their phones at the window. They took photographs and videos of me just walking. And a minute they had captured the content. They drove off.\n[07:12] I felt violated and so vulnerable that somebody would choose to act in that way and couldn’t figure out what conversation was had before they decided to pull over. My immediate instinct was to ring my mom and tell her what happened. And I was so upset. Her response was, well, ‘do you think it’s because maybe they just liked your dress?’ We both knew that that wasn’t the reason. But it helped, you know? And later that evening, she sat with me and she said, if somebody needs to make you feel small, to make them feel big or feel better, then they’ve just giving you a gift because you immediately understand the fact that they are not people that you want to surround yourself with. That, you know, nobody chooses how, why, or when, or to how much money, or to what religion, or to what culture they’re born into. But we do get to choose how we behave.\n[09:46] When I was 11 I considered limb lengthening surgery, which is the deliberate fracturing of your limbs, and over the course of a year your leg bones are stretched apart, and a new bone grows. It has to be done between the ages of 10, 11, 12 — just before puberty so that you get the optimum and maximum growth. When I considered the surgery, the doctors told me I would get anywhere between two and six inches in height, which at most would have made me four foot tall.\n[10:16] I told my parents that I wanted to think about it. We had meetings with the doctors. An appointment was booked, a surgery date was listed and given to me. My parents told me that I had to make the decision, that they couldn’t make it for me, because if they did, I’d be resentful of them as I got older, if it wasn’t the right choice. That at 11 I had to decide whether or not I wanted to transform myself. Maybe it would have helped me reach things. Maybe I could have reached the light switch, or the shower, or the lock on the hall door without thinking about it. But it was only six inches. And as I sat with it, I realized that, you know, the only reason I’d be undertaking this surgery was to make other people more comfortable with me. Because this surgery would somehow make me look less like I had dwarfism. It would teasure me closer to the world’s definition of normality. Whatever that is. And maybe it would help me make friends. Maybe it would make the world be kinder to me because I would look less different. And I realized that if people liked me more, or only liked me, because I was six inches taller, because I had transformed myself to fit with their ideals. Then I wanted different friends. I wanted to be around different people.\n[11:42] I didn’t want to change myself — something which I didn’t consent to and had no control over. I don’t want to change myself for them. And the power of being a loved child meant that I had this ability to love myself. And as biblical or as trite as that sounds, I loved myself enough to be proud of who I was physically, emotionally, cognitively, socially, that I wasn’t willing to sacrifice something that transformed me as a person to fit with others. When I told my mom and dad that it wasn’t something I was willing to do, and they said, great, if you want to talk about this again at an another time, if you feel differently and decide later that you want to have that surgery, then we’ll do it. But can you imagine how my dad felt? I don’t know if he was offered that surgery at any time. But his 11-year-old daughter was currently going through this mental gymnastics of whether or not she was comfortable and happy looking like him. Whether or not it was good enough for her or the world, all the while knowing that it was his genetics that I inherited that made me who I was. I didn’t think about that, of course, at eleven. I’ve probably just considered it now for the first time.\n[13:06] But how do you raise a good kid? You have to love them deeply. But in a way that gives them space to figure out who they are. You have to be proud of them beyond belief and continuously remind yourself and them of it because the inner monologue that we all have and are conditioned to narrate within ourselves is not always positive. And don’t shelter them from the complexities of the world. Revel in it, and embrace it, and have those difficult conversations. And don’t belittle children because they’re children. Respect them and honor and celebrate their opinions and their perspectives on the world because they’re valuable.\n[14:03] Thanks for listening to Good Kids. You can find me, Sinéad, online @thesineadburke or listen to my podcast, As Me with Sinéad.\n[14:13] Good Kids is produced and edited by Samantha Gattsek. Our executive producer is Stephanie Wittels Wachs. Our music is by Dan Milad. Ad sales and distribution is by Westwood One. You can find more about us at LemonadaMedia.com or on all the social platforms at @LemonadaMedia. If you like what you heard, share the gospel with everyone you know and rate and review us on Apple, Stitcher, Spotify or wherever else you listen to podcasts.']	['<urn:uuid:ea04e223-e263-4083-8daf-a2a2ef9d6024>']	factoid	with-premise	verbose-and-natural	distant-from-document	single-doc	novice	2025-05-12T21:52:51.670698	22	50	1962
26	how does government match presidential candidate campaign donations requirements small contributions	Under the Federal Election Campaign Act of 1971, presidential candidates can receive matching funds in the primaries if they raise lots of small contributions in at least 20 states. The process is supervised by the Federal Election Commission and must be transparent, with all contributions disclosed. This funding system only applies to presidential elections, not congressional ones.	"[""Federal Election Campaign Act 1971\nIntorduces federal funding for presidential (but not congressional) elections, with matching funds provided for candidates in the primaries who raise lots of small contributions in at least 20 states.\nAll contributions to candidates must be disclosed and supervised by the Federal Election Commission - the process is open and transparent\nLimits are placed on all direct contributions to candidates (hard money - intended to prevent the buying of elections)\nThe Bipartisan Campaign Reform Act 2002\n(McCain v Feingold)\nThe main loophole in the funding of campaigns was in the form of soft money. This is unregulated money usually given to parties for general political activites.\nThrough soft money, money could also be given to the campaign to allow individuals to spend money on behalf of candidates.\nThe BCRA banned all soft money contributions to candidates op parties and increased the upper limit for hard money doinations from individuals to $2500\nThis meant candidates make internet appeals to large numbers of small donors, making greater use of grass-root funding.\nThe BCRA led to the growth of 527.\nNamed after a setion of the US tax code which allows organisations to raise and spend an unlimiter amount of money for general political activities such as tv adverts.\n527s meant candidates don't always have to rely on PACs\nEg) The swift boat veteran for truth group- funded an advert attacking Kerry's war record on bravery in order to neutralise his advantage over Bush who was an alleged draft dodger (2004)\nAn independant organisation that intrest groups, officeholders and political candidates can establish for the sole purpose of contributing money to the campaigns of candidates who symapthise with it's aim\nThey are a result of federal laws that prohibit most interest groups from donating money to political campaigns.\nThey are the finiancial arms of pressure groups, set up to raise campaign fund and channel them to support or oppose candidates in electoral contests.\nThey are limited by the amount of hard money they can donate to candidates ($5000)- but they can 'bundle' these contributions to increase the sum given\nFEC v Citizens United\n2010 Supreme Court Ruiling.\nAllows corporations, unions and individuals to make unlimited donations to partisan groups, now known as Super PACs.\nThese then campaign for or against electoral candidates (but can not directly donate any hard money)\n- A PAC that is allowed to raise and spend unlimited amounts of money from corporations, unions, individuals and associations. Some non-profit groups are allowed to contribute to super PACs without disclosing where their money came from.\n- Super PACs have no limitations on who contributes or how much they contribute. They can raise as much money from corporations, unions and associations as they please and spend unlimited amounts on advocating for the election or defeat of the candidates of their choice.\n- There are restrictions prohibiting super PACs from working in conjunction with the candidates they're supporting. They can not co-ordinate their spending with either parties or candidate, but can spend on behalf of them using 'independant expenditure' for 'issue advocacy'\n- In 2010, the US Chamber of Commerce spent $75 million to target for defeat congressmen who had voted for healthcare reform (eg spending money on attacking advertising)\n- In 2010 mid-terms, the new super PAC American Crossroads spent $70 million supporting Republican candidates.\nFederal Regulation of Lobbying Act 1946\nRequired lobbyists to regsiter with the clerk of the HofR and the secretary of the Senate if they raised money to be used to help or hinder legislation.\nHowever this has been largely ignored.\n1990s restrictions on PG\nCongress passed further regulatory legislation by expanding the definition of what qualified as a pressure group - more groups therefore had to register.\nSignificant restrictions on lobbyists; eg) by banning of gifts to members of Congress\n2007 Honest Leadership and Open Government Act\n- Passed by the Democrats, but had bipartisan support.\n- Prohibiting gifts by lobbyists\n- Closing the revolving door\n- Senators have to wait two years before they can lobby congress. Cabinet offices can not lobby the department they working in for 2 years. Senior Senate and House staff can not lobby contacts with the entire Senate for 1 year.\n- Full public disclosure of lobbying activity\n- disclosure filings to be filled quarterly rather than biannually. Increases civil penalty for knowing/wilful violations of the Lobby Disclosure Act, Increased to $200,000. Criminal penalty up to 5 years. Gov accountability office will audit annually lobbyists compliance\n- Congress Pensions Liability\n- anyone convicted of bribery or conspiracy will lose retirement benefits\n- Prohibited use of Private Aircraft\n- Candidates, other than those running for the House, to pay fair market rate for air fares when using non-commercial jets + house members have to use commercial aircraft\nThey are significant:\n- impact on the electoral process can be vital in a close race, some people argue congressmen will not speak out against powerful interests because they are afriad of being targeted for defeat.\n- they bolster the rise of candidate-centered campaigns- they support candidates, not parties. Candidates feel more beholden to them than their party and this can affect their voting behaviour in Congress.\n- They encourage influencing peddling in Congress where is is alleged you have to 'pay to play' and they can deter challengers to incumbent members of Congress\nThey aren't significant:\n- Existance of 527s and no concrete proof of correlation between PAC funding and the way Congressmen vote on issues\n- The transparency of much PAC funding may also lessen their impact\n- PAC funding is much less signigicant for candidates who fund their own campaigns or receive money from other sources.""]"	['<urn:uuid:87387836-d354-4ab9-9271-f67e583818c0>']	open-ended	direct	long-search-query	distant-from-document	single-doc	novice	2025-05-12T21:52:51.670698	11	57	948
27	How does neglect during the first few years of life affect a child's development and attachment patterns?	Neglect is particularly damaging in the first two to three years of life, which is a crucial period for social, emotional, and neurobiological development. Neglected infants and toddlers show a dramatic decline in their overall developmental scores between 9 and 24 months of age, and their cognitive function continues to decline throughout pre-school years. In terms of attachment, neglected children commonly develop disorganized attachment styles and may withdraw from relationships as their need for nurturing goes ignored. They often show delayed cognitive development, poor language skills, poor social skills, and limited coping abilities. These children may become dependent and unhappy, display pathological behaviors, feel a greater sense of failure, and might even blame themselves for the neglect they experience.	['The DfE published chapter and verse research on the impact of neglect in 2014 (Brandon et al – Childhood Wellbeing Centre). Of particular interest to adopters and special guardians are the long term consequences of childhood neglect as it is us who must live with them. Neglect has far reaching consequences, which must be factored into support for us by policymakers and legislators, who need to ensure we receive it. Instead of support however it can seem that children’s services suffer with ‘organisational amnesia’ and ‘silo approaches’ are taken where information is not shared and used throughout the organisation. It is a shame, and perhaps indicative of the skewed priorities of government and the DfE, that no consideration is given in this DfE report as to what sort of support will actually be needed for those parenting and caring for previously looked after children – where the problems arising from neglect may manifest, sometimes years later in adolesence, as attachment disorder, PTSD, anxiety, depression, ADHD or child to parent violence.\nThe system is failing us and we need the Adoption and Special Guardianship Leadership Board to let us in, so we can tell them about the problems we are experiencing and the gaps we fall through. Enough of political hyperbole and the rhetoric of ‘positivity’, we need formal policy evaluation that service users can feed into, which asks the right questions. We need to be able to learn from where the system is not working in order to improve it.\nOf course the system may fail children who go into care too. With no one to look out for a child – who will fight to ensure they receive the help and therapy they need? As Dame Christine Lenehan pointed out in the recent SCIE project on Mental Health of Children in Care, which she co-chaired, there is often no commissioning for children in care because no one commissions into a vacuum. So there can be very serious corporate neglect.\nThe way to resolve a silo organisational culture is with better transparency and the development of shared goals. Therein lies the problem. The DfE and various Regional Boards offer no transparency at all and it seems their priorities may not be focused on supporting families in the years that follow the Adoption Order and on into the child’s adulthood. Match funding from LAs with the Adoption Support Fund is far too low to suggest any real commitment to post adoption support. Just one child re- entering care, or going into secure accommodation, may involve expenditure that is greater than all adopted children have received throughout England. This makes no sense.\nAn Adoption Order is permanent but the Special Guardian Order ends when the child is 18, which is when children’s services cease being involved. The Adoption Support Fund finishes at 21, or 25, if there are special educational needs/ an EHC Plan. There is a gap here where support services are not in synch. What does this mean for the child and family? We are extremely concerned also that special guardians seem to get very little attention – none in this report on neglect – and are so often an afterthought or ‘add on’.\nWith no requirement for Local Authorities to do anything other than assess us, when we ask for help, we can end up in big trouble as families – as we are so often on the verge of crisis by the time support is requested, and an assessment may take several months or more to complete. Assessment is not support and may not lead to it either, especially if there is a ‘silo mindset’ and a solely ‘child focused’ approach, instead of the organisation properly getting behind us and providing whole family support. We suggest that a more holistic approach is needed. We would like to see the DfE and children’s services becoming more long term and more family centred in their focus.\nWhen considering the impact of neglect, it is not sufficient to simply ignore the past and expect adopters and special guardians to battle for support, be unable to access respite without our children having to re-enter care (with the destabilisation and added trauma this brings), worry about being blamed and overly critical approaches, and struggle to be heard at all by the Adoption and Special Guardian leadership and Regionalisation Boards – not when there is so much knowledge about the harm neglect does. Here is an excerpt from pages 7-9 of the Brandon Report commissioned by the DfE:\n“Neglect is considered to be at least as damaging, if not more so, than other forms of maltreatment because its impact is the most far-reaching and difficult to overcome (Gilbert et al., 2009). Neglect in the early years may be the most damaging from the point of view of long-term mental health or social functioning.\nImpact of neglect on children’s development\nNeglect is rarely life threatening but has the potential to compromise a child’s development significantly, across multiple domains. Because neglect frequently coexists with other forms of maltreatment, it can be difficult to disentangle its unique consequences. Nevertheless, there is now a relatively robust consensus based on a range of empirical evidence that demonstrates its adverse impact on all the seven dimensions of development identified in the Assessment Framework: health, education, identity, emotional and behavioural development, family and social relationships, social presentation and self-care skills (see Tanner and Turney, 2003; Norman et al., 2012; Hildyard and Wolfe, 2002; Manly et al., 2001).\nWhile neglect is thought to be particularly damaging in the first two to three years of life – a formative period for social, emotional and neurobiological development – it can compromise development throughout childhood and adolescence. There is good evidence from animal studies and increasing evidence from human studies that exposure to neglect, often alongside other forms of maltreatment, is associated with alterations in the development of the hypothalamic-pituitary-adrenal (HPA) axis stress response and differences in brain structure and function. Importantly it is thought that such changes serve to biologically embed vulnerability that may only later manifest in mental health problems (see for instance McCrory et al., 2010; McCrory et al, 2012).\nPersistent, severe neglect indicates a breakdown or a failure in the relationship between parent and child. This may be reflected in maladaptive attachment patterns; for example, neglected children are as likely as children maltreated in other ways to develop disorganised attachment styles (Barnett, Ganiban and Cicchetti, 1999). However, they differ from other maltreated children in that they show more evidence of delayed cognitive development, poor language skills, and poor social skills and coping abilities (Hildyard and Wolfe, 2002). They may also present as dependant and unhappy, and display a range of pathological behaviours (see Egeland et al, 1983; Ward, Brown and Westlake, 2012). Children who are neglected from early infancy may find that as their need for nurturing or responsive relationships goes ignored, they withdraw from relationships, feel a greater sense of failure and may even blame themselves for the neglect they experience (Manly et al, 2001).\nThere is some evidence that children who experience neglect without other forms of maltreatment have worse outcomes than those who are both neglected and physically abused (Egeland and Sroufe, 1981). Later in this paper we explore the observable indicators which signal impairment in pre-school aged children who are experiencing neglect.\nThe impact of neglect is not only widespread, affecting a wide range of developmental domains, it is also cumulative. Neglected infants and toddlers show a dramatic decline in overall developmental scores between the ages of 9 and 24 months (Egeland and Sroufe, 1981; Naughton et al, 2013). They also show a progressive decline in their cognitive function throughout the pre-school years (Strathearn et al, 2001). Neglected infants who initially display secure attachments increasingly develop insecure and disorganised attachment styles as they grow older (Howe, 2005). These findings imply that the longer pre-school children are exposed to neglect, the greater will be the harm. Longer duration of neglect has also been associated with an increased pattern of neural reactivity to social threat (McCrory et al., 2012).\nHindley and colleagues’ systematic review of risk factors for the recurrence of maltreatment highlights the cumulative nature of neglect since it is the most likely form of maltreatment for a child to re-experience (Hindley et al, 2006).\nFinally, neglect in the early years has long-standing consequences that can endure throughout childhood, adolescence and well into adulthood (Stein et al, 2009; Rees et al, 2011). Neglected babies and toddlers may develop both internalising (depressed, withdrawn, lacking in self-confidence) and externalising (acting out, aggressive, impulsive) behavioural problems in later childhood (English et al., 2005). Kotch and colleagues (2008) for example, have noted that ‘child neglect in the first two years of life may be a more important precursor of childhood aggression than later neglect or physical abuse at any age’ (p.725). Childhood aggression in turn is closely associated with adolescent violence and delinquency (Broidy et al., 2003). Anti-social behaviour and ADHD symptoms displayed by neglected and physically abused children may endure into adulthood, and both groups are significantly more likely to be arrested for violent offences than their peers (Widom, 1989).\nNeglect in childhood is also associated with a range of mental health problems in adulthood. As adults, neglected children are more likely to develop major depressive disorders (Widom et al., 2007) and post-traumatic stress disorder (Widom et al., 1999). Physical neglect in childhood is associated with greater functional and social impairment in adults with schizophrenia (Gil et al., 2009). Robust evidence has also been found for relationships between childhood neglect and anxiety disorders, suicide attempts, substance misuse, risky sexual behaviours and sexually transmitted infections in adulthood (see Norman et al., 2012 for further details). There are also long term physical consequences of neglect in adulthood which can include an increased risk of hypertension, and chronic pain syndromes (Anda et al., 2006).']	['<urn:uuid:19955bdc-44b1-4022-88a7-4832748c5b6b>']	open-ended	direct	verbose-and-natural	similar-to-document	single-doc	novice	2025-05-12T21:52:51.670698	17	119	1650
28	What's the difference between who can attend proceedings in youth courts versus coroner's inquests when it comes to public access?	In youth courts, members of the public are not allowed unless they have express permission, and parents/guardians must accompany youths under 16 or those aged 16-17 with court orders. In contrast, inquests are public hearings that can attract media interest, with participation rights extended to a wide range of interested persons including family members, personal representatives, and those who may have contributed to the death.	['What is the youth court?\nThe youth court is part of the Magistrates’ Court and deals with criminal cases involving young people aged between 10 and 18 years old. The youth court is less formal than the magistrates’ court, as the primary aim of the youth justice system is to prevent reoffending.\nThere is the presumption that all young people are dealt with in the youth court, however there are a number of situations when a youth will appear in the Crown Court. In these situations, the youth will first appear before the youth court.\nDistrict judges and magistrates sitting in the Youth Court have specialist training and there is no jury in a Youth Court trial. A parent or guardian must accompany a youth to court if they are aged under 16 and if the youth is aged 16 – 17 and given a court order. Members of the public are not allowed into the court unless they have express permission. Young people are called by their first name in court. The Youth Court allows participation by the young person and their family.\nOffences in the youth court\nWe are able to represent young people when:\n- They appear in court for the first time charged with a criminal offence\n- Not guilty pleas have previously been entered and they are due to stand trial\n- They are appearing in court in relation to a breach of a previously imposed court order\n- They are applying for bail or variations of bail conditions already imposed\n- They are appearing in court in relation to non payment of fines\n- They are awaiting sentence\nOur solicitors are sensitive and appreciate that appearing before the youth court can be a very stressful time for both the young person and their family. Our specialist solicitors aim to offer support to the youth and their families throughout the youth court proceedings.\nIn certain situations, specialist reports can be obtained when a young person has health vulnerabilities such as mental health or learning difficulties. These reports can be very useful and beneficial to the youth’s case.\nOur solicitors can liaise with various agencies in relation to the young person.\nYouth court sentences\nWhen a youth pleads guilty or is found guilty before the youth court, there are a number of sentences available.\nDepending on the nature of the case, the young person’s circumstances and whether they pleaded guilty to the criminal offence, youths aged under 18 on the date of conviction can be sentenced to:\n- An absolute discharge\n- Conditional discharge\n- A fine\n- A compensation order\n- A referral order\n- A reparation order\n- A youth rehabilitation order\n- A detention and training order\nIf the youth was aged under 18 at the time of conviction, the court cannot suspend a custodial sentence.\nThere are a number of sentences available to the youth court when the youth has turned 18 prior to the date of conviction, including:\n- An absolute discharge\n- A conditional discharge\n- A fine\n- A community order\n- Detention in a young offenders’ institution\n- A suspended sentence order\nNb. This guide is intended to give general information only and not intended to be used as the basis upon which advice is given, nor should it be relied upon as giving advice specific to a case or individual. Lawtons do not accept liability for anyone using this guide. Should you require specific advice in connection with a real case or situation, please contact us immediately so that we can provide specific advice.', 'What exactly is an inquest?\nAn inquest looks at the way in which a person died; examining the direct circumstances surrounding the death but does not decide guilt in the same way as a criminal trial or public inquiry.\nThe scope of an inquest is outlined in the Coroners and Justice Act 2009 and is usually confined to examining how, when and where the death occurred. This differs if Article 2 of the European Convention on Human Rights is engaged (the Right to Life).\nAn Article 2 inquest will take place if an ‘agent of the state’ (such as the Police or Army) has been implicated in the person’s death. This type of inquest has a broader approach, looking more widely at the circumstances involved (such as inquests into deaths in Police or Prison custody.)\nWhat are the available verdicts at an inquest?\nAt the conclusion of the inquest, the coroner will formally record the conclusion as to the cause of death. A range of short form outcomes are available, including unlawful killing, misadventure, suicide, accident, an open verdict if the evidence proves inconclusive, or a narrative conclusion setting out factors which contributed to the death.\nCan I get Legal Aid for an inquest?\nThere is no automatic right to Legal Aid, but an application for ‘exceptional funding’ can be made. Inquests which engage Article 2 (as above) will usually result in Legal Aid being granted.\nHow do I go about obtaining medical records belonging to a deceased person?\nYou’re solicitor on your behalf will make formal requests to hospitals, care homes, private companies or prisons. They will also liaise with coroner’s office for disclosure of records.\nWhen does the coroner investigate the circumstances of a death?\nIf there is reasonable suspicion that a person died in certain circumstances, it is the coroner’s duty to investigate. Circumstances include:\n- Dying a violent or unnatural death\n- The cause of death is unknown\n- Deceased died whilst in custody or another state detention.\nWho is involved in a Coroner’s investigation?\nIf you fall within the definition of an interested person, you are entitled to participate:\n- Family: a spouse, civil partner, partner, parent, child, brother, sister, grandparent, grandchild, child of a brother or sister, stepparents, half-siblings\n- Personal representative of the deceased\n- A person who may by any act or omission have caused or contributed to the death of the deceased, or whose employee or agent may have done so\nThe criteria for an interested person are wide, meaning that those close to the deceased have participation into the investigation.\nWhen can a coroner hold an inquest into a death as part of the investigation?\nIf a post mortem examination or any other preliminary investigation establishes there is no reason to suspect that any circumstances apply from the Justice Act and a Coroner can reach a conclusion of how the deceased died without further enquiry, the investigation will be discontinued an therefore no inquest held. If a coroner is unable to establish circumstances of a death and suspects that the death was violent or unnatural, an inquest will be held.\nWill a jury be called at an inquest?\nA Senior Coroner is able to empanel a jury. Only in specific circumstances will this happen:\n- If death occurred in custody and there is reason to suspect the death was “violent” or “unnatural” or the cause of the death is unknown.\n- If death is caused by a “notifiable accident, poisoning or disease” and needs reported to the Health and Safety Executive. Such accidents include deaths caused by accidents in the workplace or death in a healthcare setting where a patient has committed suicide or killed by another patient.\n- If death occurred in circumstances which are prejudicial to the health and safety of the public.\nHow do solicitors help?\nWe represent you during the inquest proceedings, obtaining any relevant records or documents belonging to the deceased person, making legal submissions to the coroner, and instructing a barrister to represent clients at court hearings. Inquests are public hearings and depending on the case can sometimes attract media interest. We can assist you with managing such press interest in a sensitive manner.\nWhat is the process, what happens at an inquest?\nAn inquest gives those close to the deceased the opportunity to raise direct queries relating to the death of the loved one. Evidence can include any governance systems to be scrutinised, as well as local authorities and health and social care professionals. You will be asked to make a statement, with all relevant documentation disclosed to the coroner.\nA Coroner heads up an inquest and will usually sit alone, unless the case involves a death in custody or state detention, and the death was violent or unnatural, or of unknown cause; where the death resulted from an act or omission of a police officer or member of a service police force in the purported execution of their duties; or where the death was caused by an accident, poisoning or disease which must be reported to a government department or inspector, eg certain deaths at work. In these cases the Coroner will sit with a Jury which will return factual conclusions (as seen in the Hillsborough inquests.) Witnesses can be called to attend and be questioned, but they do not have to answer a question if it could incriminate them.\nWhat is the outcome of a Coroner’s investigation?\nAt the end of the investigation, the coroner will issue a record of an inquest. In this document, will be determinations relating to the deceased and how they died. This record will include a conclusion of the death, in short form explaining whether the death was caused by neglect or natural causes. It can also include narrative of circumstances leading up to the death.']	['<urn:uuid:a5f6289d-ed7e-48da-8960-d0979aee039a>', '<urn:uuid:2774f960-abe6-443a-9c53-f259afa5a234>']	factoid	direct	verbose-and-natural	similar-to-document	comparison	expert	2025-05-12T21:52:51.670698	20	65	1567
29	I'm opening a catering business and need to move both hot and cold food to events. What's the difference between using insulated food carriers and refrigerated trucks for keeping food at the right temperature?	Insulated food carriers and refrigerated trucks serve different purposes for temperature control. Insulated food carriers are portable containers made of heavy-duty plastic that can maintain food temperatures for extended periods and are available in various sizes to hold hotel pans or sheet pans. They're often used with dollies for easier movement and are suitable for smaller-scale catering operations. In contrast, refrigerated (reefer) trucks have powered cooling systems that can maintain specific temperatures continuously, offer better temperature and humidity control, and can be monitored throughout transport. Reefer trucks are more suitable for larger quantities of food and longer distances, but require pre-cooling of both the truck and the food before loading. They also must comply with strict food safety regulations including proper sanitization and temperature monitoring records.	['For many restaurant start ups, it can be a difficult process to sort out what kinds of equipment are essential when first getting started. Storage and transport equipment are incredibly part of the equation when talking about a fully functional foodservice operation, however it is often one of the last things business owners think about when making their initial shopping lists. Though storage and transport equipment should be purchased around the same time as or just after equipment, it is still an incredibly important part of both BOH and FOH operations and plays an integral part in the flow of a restaurant. First we will look at the most common / important storage and transport equipment, and then we will look at some factors to look at when deciding at which will fit your business best.\nShelving is a necessity in every foodservice environment, however there is a tremendous variety of shelving that may or may not be needed in your specific operation. To start, it is a good rule of thumb to remember that every item in a restaurant will need a place to be stored if when it is not in use.\n- Wire Shelving & Dry Storage – Wire shelving is the most common kind of storage in the BOH of a restaurant or other foodservice operation. Perfect for dry food storage, storing chemicals or even storing clean dishes to accelerate drying, your business will, most likely, require some wire shelving or something comparable.\n- Wall Shelving / Pot Racks – Wall shelving provides storage space in areas that otherwise may not accommodate fully assembled shelves. It is a great way to keep items on hand and accessible, and, often times, wall shelves have attached pot / utensils storage hooks. It is important to have pot / utensil racks (and even knife racks) accessible because these are some of the most frequently used items in the kitchen, and it isn’t very logical to store them in areas that are not easy for employees to get to.\n- Walk-In Shelving – Walk-in coolers and freezers need storage shelving, though the excess moisture in the environment is not optimal for standard wire shelving units. To avoid shelf corrosion, heavy duty plastic shelving or epoxy coated wire shelving is commonplace in walk-in coolers.\n- Dunnage Racks – Dunnage racks are intended to store large or heavier items. There are a number of regulations that specify certain items (like food products) cannot be placed on store on the ground, and close to the ground dunnage racks are the easiest way to store those heavy items without having to move them to heights that could result in injury. These racks often have weight capacities that far exceed that of traditional wire shelving.\n- Bakery Racks – Bakery racks are necessary not only in bakeries, but in many foodservice operations. Bakery racks are the best way to transport full sheet pans, and store them while cooling or waiting to go into an oven.\n- Specialty Storage, Wine Racks, & Security Cages – Some items, such as expensive seafood, meats, or liquor are often kept under lock and key in a security cage. These items account for a larger portion of food / beverage costs, and many restaurateurs prefer to keep track of them a bit more stringently. Wine is also one of these higher cost items, and there are some security cages specifically designed to house wine bottles. Standard wine racks, along with backbar shelving is a must in the FOH, and other FOH storage such as point of sale shelving and silverware storage will be necessary as well.\nFood & Ingredient Storage\nThere are some storage needs that are necessary specifically for storing food and ingredients.\n- Refrigeration – Refrigerated storage space (including freezer space) is absolutely needed in a commercial foodservice environment. Items such as like walk-ins, reach-ins, undercounter reach-ins, and refrigerated prep tables are necessary to keep food at the proper temperature while it is waiting to be prepared.\n- Hotel Pans – Both metal and plastic hotel pans are usually necessary for storing items on a kitchen line. Hotel pans are standard fits for refrigerated prep tables and refrigerated drawers, and metal hotel pans can also be used to bake the items they are storing.\n- Sealable Storage Containers – Most foods need to be covered whether they are being stored in a refrigerated or dry storage setting. Sealable storage containers are absolutely necessary in every foodservice setting for this reason.\n- Vacuum Sealers – Vacuum sealers are becoming more and more popular in BOH operations. They help extend the life of food product, and are a great way to extend the life of foods that are being refrigerated or frozen.\n- Ingredient Bins – When dealing with large quantities of ingredients such as flour or sugar, ingredient bins are traditionally the best way to go. They provide a mobile covered storage area, provide easy access, and reduce the mess that can be created when scooping ingredients out of a large bag.\n- Bakery Rack Covers & Dough Boxes – Bakery rack covers allow foods to rest undisturbed on a bakery rack, and allow cooks to store entire racks of product in walk-in coolers. They are a necessary tool in most bakeries, along with other items such as dough boxes to store pizza or bread dough.\n- Heated Storage Space – There are a number of ways to adequately store hot food. For more information on this, take a look at our Buyer’s Guide on Heating and Warming Equipment.\nHeated Food & Beverage Transport\nCaterers, banquet facilities, and even large restaurants often times have the need to move quantities of hot (or cold) food over distance without sacrificing temperature. There are several ways they get this done.\n- Insulated Food Carriers – Operations such as traveling catering services need to move heated hotel or sheet pans over large distances while still maintaining proper serving temperatures. Heavy duty plastic insulated food carriers are available in a variety of shapes and sizes to suit the needs of businesses, and there are a number of dollies to help ease the movement of these occasionally obtuse containers. Additionally, there are a number of soft insulated food carriers (such as pizza delivery bags) that are more suited to smaller delivery operations, but still keep food temperatures for extended periods of time.\n- Beverage & Liquid Transport – Traveling caterers often need to transport beverages or other liquids, such as soup, to catered events with the same expectations as with food. There are a number of heavily insulated beverage and liquid transport carriers that can keep liquids either hot or cold, and some that come with built-in dispensers.\n- Banquet Carriers – Large banquet facilities and other businesses often have to move pre-plated dishes rather than hotel or sheet pans, making the banquet carrier essential. These heated mobile cabinets can facilitate either covered or uncovered plates, and keep the food inside warm until it is served to patrons.\n- Ice Caddies – Ice caddies are the best (and easiest way) to transport large volumes of ice. They have tremendous volume capacities, are mobile, and many are actually made to fit underneath and ice machine head; eliminating the need for an ice bin.\nThings To Note While Shopping\nThere are some important factors to examine when shopping for the right storage and transport equipment.\nSpace is important in every commercial foodservice environment. Cooking equipment takes up a great deal of space, so it is important that storage equipment fit in around the kitchen’s biggest space hog. Items including wall shelving, under shelves on prep tables, and even add-on shelving to ranges are great ways to get the most out of your space.\nIt is important to know where you will be placing your storage equipment. As previously discussed, standard wire shelving do not do well in refrigerated areas, and likewise, it wouldn’t make much sense to place plastic shelving on or around deck or convection ovens. Know what storage equipment should be where, and recognize that products stored on it should be able to withstand those same conditions\nWhen it comes to storage containers, ingredient bins, and transport equipment, it is important to have a ballpark figure as to how much product will need to be stored or transported at any given time. Oversized storage containers could take up too much space in a walk-in and it isn’t prudent to get several large insulated food carriers when a soft transport bag will suffice.\nWhen transporting food, it is important to take into account how far prepared food will be traveling to its final destination. Food needs to remain at proper serving temperatures up to and including the time it is being served, so it is important to get transport equipment that will adequately hold food temperatures the entire distance it is traveling.', 'Guide to Trucking Perishable GoodsAugust 21, 2018\nWhether you are shipping produce, floral arrangements or pharmaceuticals, maintaining a cold chain from production to delivery is essential. If the temperature is not adequately controlled at all stages of production, your perishable goods could spoil or even make your customers sick. But for a small company, delivering perishable goods safely can be an intimidating task. You want to make sure your products arrive fresh and safe for consumption, but you may not know much about logistics management or shipping regulations.\nDepending on the size of your business and the goods you need to deliver, you may opt to ship your perishable goods in the mail or use a reefer truck for ground deliveries. No matter which method you choose, you must comply with federal regulations and maintain best practices for delivering perishable goods. Use this guide to perishable goods shipping to find the best delivery method for your business.\nHow Do I Ship Perishable Food and Goods?\nFor companies sending smaller deliveries directly to customers, shipping perishable goods in the mail may be the best option. Shipping cold or frozen products in the mail requires proper planning and care to make sure everything arrives safely to your customers. Follow these step-by-step instructions on how to ship perishable goods.\n1. Know Your Products\nBefore you begin shipping goods, make sure you know the temperature at which you must keep each product, so it stays fresh. You should know the shelf life of each product to determine how quickly it will need to be shipped. These factors will help you decide which type of packaging is best for your products.\n2. Choose a Container\nA variety of shipping containers are available for shipping perishable goods. These special packages provide insulation to keep out heat. You can purchase styrofoam boxes with thicker or thinner walls to keep out more or less heat, and they’re generally the most effective at maintaining temperatures. Typically, Styrofoam boxes are shipped inside of cardboard boxes for better durability during transport.\nYou can also purchase Styrofoam sheets, insulated liners or insulated pads to place inside of a cardboard box. Sheets and pads are often more cost-efficient than Styrofoam boxes, as you can purchase them in bulk and cut them to the right size for each package. You can also wrap insulated liners around your product for extra protection.\n3. Select a Cooling Agent\nOnce your product is packaged, you will need to add a cooling agent to keep your product cold during shipping. Ice packs are typically a good option for refrigerated goods, while dry ice is best for frozen products. Dry ice is considered a hazardous material, so you’ll need to clearly label any your packages containing dry ice and should check for additional requirements from your shipping provider.\nWhen determining how much dry ice or how many ice packs to use, consider the size of the product, what temperature you need to maintain, how long it will be in transit and the climate of the area it’s shipping to. Shipping trucks can reach very high heats in the summer, so be sure to take this into account when adding your cooling agent.\n4. Get It There Fast\nChoose a shipping partner that can provide guaranteed shipping times, so you know your product will be delivered on time. You should also balance speed with cost, as overnight or one-day shipping can be very expensive. If you want to provide a reasonable shipping fee for your customers, choose the longest shipping time that will still keep your product fresh and safe.\nChallenges of Shipping Perishable Goods in the Mail\nThe cost to ship frozen food can be expensive, as you will need to send it quickly and ensure that proper temperatures are maintained. Ice packs and dry ice also take up extra space, meaning you may need to use larger shipping containers, which can add additional cost.\nAim to use the smallest possible container for your product and choose the longest shipping time that will still keep your product safe. No matter what shipping method you select, make sure to have your shipping policies readily available on your company’s website so customers can understand the associated costs.\nPerishable Goods Delivery With Reefer Trucks\nWhen transporting large quantities of perishable goods, a reefer truck may be the best option. Reefer trucks have internal cooling systems that can maintain a specific temperature inside the trailer. These trucks can be set at a range of temperatures from freezing to room temperature.\nReefer trucks were invented in the 1930s as a solution to the problem of transporting cold and frozen goods safely. Manufacturers and producers wrestled with this question for decades, employing the first attempt at a cold shipment method on trains in the mid-19th century. Railroad companies filled train cars with large blocks of ice in an effort to keep products cold as they traveled across the country.\nAfter the invention of automobiles, drivers carried this method over to trucks and also used insulation to keep out heat. Fans in the vehicles also helped to move cold air around the container. This method was not sufficient, however, because drivers would still have to stop periodically to refresh the ice.\nIn the 1930s, inventor Fred Jones created a portable cooling unit that relied on the truck’s motor for power, and drivers could add it to the outside of shipping containers. These first refrigerated trucks allowed critical perishable items — such as food, blood and medicine — to be shipped to soldiers during WWII.\nRefrigeration technology for transporting goods has continued to develop, and reefer trucks are now available with a wide variety of capabilities. Modern refrigerated trucks are powered by diesel and can be adjusted to exact temperature specifications. For smaller shipping needs, refrigerated vans are also available.\nIn many reefer trucks, internal technology allows drivers to continuously monitor the temperature inside the truck to ensure products are stored properly during the entire shipping process. If temperature monitoring is not already built into a reefer truck, additional devices must be used for accurate temperature maintenance and recording.\nWhen to Use a Cold or Frozen Truck\nUsing a reefer truck can be an affordable, efficient way to transport perishable goods. Reefer trucks are valued for their ability to control both temperature and humidity, making them perfect for ensuring freshness of a variety of products.\nFrozen food shipping companies use reefer trucks to keep products frozen during transport, while fresh produce is shipped in refrigerated trucks so it does not spoil or become contaminated. While cold and frozen trucks are most often used in the food and beverage industry, they have many applications in other industries, as well.\nBelow are examples of perishable goods that many companies often transport using reefer trucks:\n- Frozen foods\n- Produce, dairy or meats\n- Cakes or baked goods\n- Vaccines and medications\n- Chemicals or engineered materials\n- Fine art or antiques that require humidity control\n- Perfumes and cosmetics\n- Tobacco products\n- Floral arrangements\nBenefits of Reefer Trucks for Perishable Goods Delivery\nReefer trucks offer many benefits for perishable goods delivery that are not available using traditional shipping methods. Below are a few reasons to use a cold or frozen truck for your perishable products:\n- Cost-Efficient: Reefer trucks allow you to ship more products at once while cutting back on packaging costs.\n- Great for Shorter Transports: You can complete local deliveries more quickly with a reefer truck than via mail, and your customers will be pleased with your fast delivery services.\n- Perfect for Large Shipments: Reefer trucks have the capacity for large shipments that would be costly to send via mail. If you have high-volume clients, they will appreciate receiving their deliveries quickly and at the same time rather than in several different packages.\n- Better Temperature Control: When using a reefer truck to deliver perishable goods, you can continuously monitor the temperature to ensure high-quality. When shipping products in the mail, you cannot control the temperature of the product once it leaves your business.\nPerishable Goods Delivery Regulations\nDepending on what type of perishable goods your company is transporting, there may be specific regulations that your logistics team must stay in compliance with to avoid liability. If you are carrying floral arrangements or other perishable products not meant for consumption, your primary concern will be maintaining quality rather than ensuring safety. If you are delivering food or pharmaceutical products, however, you must be aware of the applicable federal regulations.\nThe Food and Drug Administration (FDA) released the Food Safety Modernization Act (FSMA) in 2011 in an attempt to prevent food contamination that has become a growing problem in the U.S. As part of this regulation, the FDA specifies safe practices for transporting human and animal food. The rule on Sanitary Transportation of Human and Animal Food sets guidelines for maintenance and design of transportation vehicles, transportation operations, training procedures and record-keeping.\nAll vehicles transporting human or animal food must be made of materials that are able to be adequately cleaned and sanitized, such as stainless steel. Refrigerated shipping vehicles must be able to maintain the appropriate temperatures for food transport and have functioning temperature controls. During transportation, reefer truck temperatures must be maintained and monitored. Companies must keep accurate records of food temperatures at each stage of shipping, including loading, transport and unloading. Then, they must fully disclose these records when requested and retain them for up to 12 months.\nBelow are some other best practices to keep in mind when shipping food or beverages in reefer trucks:\n- Always pre-cool the truck before loading the product.\n- Always pre-cool the product before loading it into the vehicle. Reefer trucks are designed to maintain temperatures, not to lower the product’s temperature. This is particularly important with produce that has a high rate of respiration.\n- Verify that the reefer truck is at the correct set point required for the product you’re shipping.\n- Set the temperature mode to continuous, so the reefer truck actively maintains the set temperature. A continuous setting will have a set defrost cycle, so ice and frost do not build up on the truck’s cooling mechanisms. If the truck will be making a lot of stops and the door will frequently be open, you may need to adjust the defrost cycle, so the vehicle defrosts more regularly.\nWhen delivering pharmaceutical products, you must comply with the FDA’s Code of Federal Regulations. Pharmaceutical products must not be handled or stored in a way that could adversely affect their quality, including proper temperature and humidity control during processing and transport. FDA regulations also require accurate record-keeping throughout the transportation and distribution process.\nOutsourcing Your Transportation for Perishable Goods\nAs your company grows, you may need to develop a more advanced cold chain system for delivering your perishable goods. You may have been able to rely on shipping products in the mail before, but as your capacity grows, you will need a more efficient and cost-effective delivery system.\nManaging your own logistics system will mean hiring staff members and buying or renting reefer trucks. If you prefer to focus on creating your product rather than worrying about shipping, consider using a third-party logistics (3PL) partner to manage your perishable goods delivery. Outsourcing your perishable goods delivery system has many benefits that will save you time and money while allowing you to focus on what you do best — running your business.\nBelow are a few benefits of outsourcing your perishable goods transportation:\n- Less Stress: Your 3PL partner will be knowledgeable about regulations and policies that affect perishable goods delivery. They will handle all the shipping paperwork, so you can be confident your goods arrive at their destinations safely and within regulations.\n- More Flexibility: Working with a 3PL company means you’ll have access to different sizes of trucks and shipping vehicles to tackle any job. Rather than sending a truck that is only half-full, your shipping partner can supply the right-sized vehicle for the job. An excellent 3PL partner will be able to adapt to your changing needs on a day-to-day basis.\n- Save Money: When your 3PL partner handles your shipping paperwork and transportation management, you do not have to hire your own staff for the job, saving you time and money. The 3PL market has grown in recent years, making it an accessible and affordable option for small and large companies alike.\nOutsourcing your perishable goods transportation will allow you to focus on growing your business, rather than worrying about how your products will get to your customers. Save time, money and stress by outsourcing your perishable goods delivery.\nTrust Mitchell’sNY Logistic for Your Perishable Goods Delivery\nMitchell’sNY Logistic is a 3PL company serving New York City and the tri-state area since 1946. With decades of experience behind us, we know the tri-state area better than anyone else and have an unbeatable record of on-time performance. Our local expertise means we can supply fast, accurate perishable goods delivery in Manhattan and beyond, with consistency you can trust. We even offer two-hour deliveries anywhere in Manhattan, so you can fulfill orders faster and keep your customers happy.\nWhen you choose Mitchell’sNY Logistic for your refrigerated truck delivery service, our expert team will ensure your perishable goods always arrive at the proper temperature and ready for your customers to enjoy. We deliver to businesses, hospitals and residential addresses to meet all of your delivery needs. Whether you’re an individual seeking fresh grocery delivery or a hospital receiving pharmaceutical supplies, Mitchell’sNY Logistic can provide perishable goods delivery for you. Our reefer truck delivery offers temperature control up to 65°F, so you can ship anything from frozen fruit to fresh flowers.\nWhen your business works around the clock, Mitchell’sNY Logistic can offer early morning, late evening and weekend deliveries. We’ll help you meet deadlines and impress your customers with prompt delivery of refrigerated or frozen goods anywhere on the east coast. Plus, our reefer trucks have GPS tracking, so you will know exactly when your products have arrived.\nRest easy when you partner with Mitchell’sNY Logistic as your perishable goods delivery partner. We pride ourselves in handling each delivery with special care and a personal touch you won’t find at a larger company. Our team wants to provide the best perishable goods delivery service to you, so you can offer the best products to your customers. To hear more about our frozen and refrigerated truck services, contact us today for rates and services.']	['<urn:uuid:f34cd2a5-98a4-4afd-bbe9-2135dd433d3f>', '<urn:uuid:92a4300d-0bb3-4c16-97be-8e9a44887e1e>']	open-ended	with-premise	verbose-and-natural	distant-from-document	comparison	novice	2025-05-12T21:52:51.670698	34	126	3915
30	pediatrician explain physical features make newborn babies appear endearing large eyes reason	Adults are evolutionarily programmed to find babies cute, particularly due to their eyes. Research shows adults prefer faces with larger eyes, and babies have disproportionately large eyes for their head - by 3 months, babies' eyes have reached their full adult width. Other universally perceived cute traits include a large, symmetrical head and a small nose and mouth. This is nature's way of ensuring adults care for babies through challenging times like sleepless nights and colic.	"['Why Are Babies So Cute? (And Other Burning Newborn Questions!)\nFrom the moment they enter the world -- with their pickled skin, blinking eyes, and pay-attention-to-me cries -- babies mesmerize and mystify us. We watch, amazed, as they grow from helpless newborns, driven by instinct and reflexes, into determined toddlers with impressive wills and skills. This inevitable, incredible metamorphosis, both awesome and curious to witness, can have parents regularly scratching their heads and wondering, Is that normal? The answer, almost always, is yes. Listen in as three top pediatricians explain some of the puzzling things babies do.\nWhy Do Babies Look So Cute?\nScientists say that adults are evolutionarily programmed to find babies cute -- nature\'s way of ensuring that we care for them through sleepless nights and bouts of colic. But just what do we find so attractive? It\'s the eyes, mainly. Research has shown that adults prefer faces with larger eyes over faces with smaller eyes. And babies have disproportionately large eyes for their head: in fact, by 3 months, babies\' eyes have reached their full adult width -- and may look huge until the rest of their features catch up. Other traits almost universally perceived as cute include a large, symmetrical head and a small nose and mouth. It\'s what makes us ""ooh"" and ""aah"" over all young creatures with improbably big peepers peering out from big round faces, whether they\'re puppies, kittens, or, especially, babies.\nWhy Do Babies Smile So Much?\nMost parents won\'t witness their baby\'s first smile for 8 to 12 weeks. But usually by the third month, most babies will flash their first grin -- and melt their parents\' hearts. These earliest smiles are probably unintentional facial movements, but our gushing responses to them virtually ensure we\'ll see more of the same. For the preverbal infant, smiling provides a means to communicate: when your baby smiles and you smile back, you\'re practicing the same type of back-and-forth exchange that happens later with language. From about 4 to 6 months, babies will smile at nearly everyone they encounter. But after that, many babies begin to develop stranger anxiety and will smile more discriminately, reserving their biggest crinkle-eyed, raised-cheek smiles for their parents and other loved ones, while greeting strangers with a more tentative, low-key smile, if any. But the wonderful thing about all babies\' smiles -- big or small -- is that they\'re invariably genuine. Whereas adults may smile to be polite, when babies smile, they\'re happy.\nAn Eyes & Nose Thing\nWhy Do Babies\' Eyes Change Color?\nEye color is determined by the amount of melanin, or pigment, in the iris; brown eyes have the most melanin, blue the least, explains Louis Borgenicht, MD, a Salt Lake City pediatrician and coauthor of The Baby Owner\'s Manual: Operating Instructions, Trouble-Shooting Tips, and Advice on First-Year Maintenance (Quirk Books). Most Caucasian babies are born with very little melanin in their irises and so have blue or gray eyes for their first six to nine months. But as babies\' melanin gradually increases, their eyes may darken to green, hazel, or brown, usually arriving at their permanent color between 9 months and a year. African-American, Asian, and Hispanic babies typically arrive with brown eyes, since they have more melanin at birth.\nWhy Do Babies Sneeze So Much?\nNewborns breathe exclusively through the nose for the first couple of months -- nature\'s way of making sure their mouth is free to suckle, explains Vincent Iannelli, MD, associate professor of pediatrics at the University of Texas Southwestern Medical Center. Consequently, their nasal passages must be kept clear, and that\'s where all that sneezing comes in. During the first days of life, babies sneeze to clear their lungs and nasal passages of residual amniotic fluid. After that, they sneeze to clear their nose of dust, or milk or formula. ""And all babies sneeze in the winter, because the heat\'s on and the humidity is low,"" which can leave baby\'s nasal passages dry and irritated, notes Dr. Iannelli. While the sneezing can make your baby sound like she has a chronic cold, absent a runny nose or other symptoms it\'s just her body\'s way of ensuring she breathes easy.\nWhy Do Babies Cry Without Producing Tears?\nNewborns have underdeveloped tear glands, which produce just enough tears to keep their eyes moist but not enough to spill over their lids when they\'re upset, explains Dr. Iannelli, who is also the author of The Everything Father\'s First Year Book (Adams Media). You won\'t see tears running down the cheeks of most infants until they\'re about 7 or 8 months old, he adds. The exception: babies with a blocked tear duct will regularly shed tears from the affected eye, even when they\'re not crying, starting as early as 2 or 3 weeks of age. Blocked tear ducts, which prevent the normal nasal drainage of tears, are relatively common -- some 5 percent of babies are born with at least one -- but more than 90 percent unclog spontaneously within the first year. You may be able to move things along by massaging the tear ducts -- ask your doctor how to do it. In some cases, the tear ducts may need to be opened with a tiny probe, a simple surgical procedure performed by ophthalmologists.\nDrool, Spit, and More\nWhy Do Babies Hiccup So Much?\nInfants are prone to hiccups because of a common and temporary malfunction of their diaphragm, explains Dr. Borgenicht. The diaphragm is that large muscle that sits between your chest and abdomen. Normally, it expands on the inhale and contracts on the exhale, but in some newborns, just the opposite happens -- the diaphragm contracts as they breathe in and expands as they breathe out, a glitch that can trigger hiccups but is otherwise harmless. As babies mature physiologically, hiccupping episodes become less frequent. If baby seems bothered by her hiccups, says Dr. Borgenicht, you can try stopping them by blowing on her face or taking her out into the cold air; either may cause her to gasp, and the sudden inhale may correct the movement of her diaphragm. Feeding baby may also reset the diaphragm.\nWhy Do Babies Drool?\nThough it\'s commonly blamed on teething, drooling usually starts around the second month, a good five months or more before most babies cut their first tooth, notes Jennifer Shu, MD, coauthor of Heading Home with Your Newborn: From Birth to Reality (American Academy of Pediatrics). This increased saliva production has less to do with baby\'s teeth than with what he\'ll soon be chewing. The drool dripping down his chin is full of digestive enzymes, which will help break down the solid foods he\'ll start eating between 4 and 6 months. Incidentally, babies don\'t produce more saliva than adults. But they tend to let the saliva run out rather than swallow it, observes Dr. Shu. This is especially true when baby is teething -- and gnawing on things to relieve the pressure in his gums -- which may be why we link the two.\nWhy Do Babies Spit Up So Much?\nIn many babies, the esophagus is a two-way street, at least for the first few months, until the sphincter muscle at the top of the stomach strengthens sufficiently to keep food down, explains Dr. Shu. Picture your baby\'s stomach as a water balloon with the top untied, she suggests. If you fill it up and then lay baby down or squeeze him, or if an air bubble gets caught beneath the milk or formula and rises up, some liquid is bound to come out, says Dr. Shu. To minimize spit-up, hold baby at an upright angle, with his head higher than his stomach, during feedings and for about 20 minutes after, she recommends.\nThe good news is that most babies stop spitting up (or do it a lot less) as their stomach muscles tighten, they spend more time sitting up, and they begin to eat solid foods. In the meantime, as long as baby continues to gain weight and seems generally content, there\'s no need to worry, even if he returns a portion of most or all meals. If, however, your baby spits up frequently, and is failing to gain weight or seems to be in pain or distressed following feedings, he may have acid reflux; talk to your pediatrician, who may prescribe an acid-reducing medication. But in most cases, assures Dr. Shu, spitting up is a laundry problem, not a medical one.\nHair and Skin Issues\nWhy Do Babies Lose their Hair?\nMost babies shed the hair they are born with in the first six months of life. This type of near-total hair loss, called telogen effluvium, is triggered by the drop in maternal hormones in baby\'s body in the weeks and months following birth. (Hormonal shifts may similarly cause moms to shed excess hair postpartum -- a temporary loss that\'s usually restored within a year.) The hair that grows back on baby\'s head may differ in color and texture from the crop she arrived with (so don\'t get attached to those golden ringlets or jet-black spikes just yet!). Babies may lose hair in patches as well, if, for example, their head repeatedly rubs in the same spot against the crib mattress. ""Tight barrettes or headbands can also cause this sort of hair loss,"" says Dr. Shu.\nWhy Do Some Babies Get Acne?\nMaternal hormones lingering in baby\'s body for the first few months after birth are the culprit, says Dr. Shu. Some 20 percent of babies develop acne -- with red pimples or whiteheads erupting on their cheeks, forehead, or chin -- at around 3 to 4 weeks. Fortunately, most babies\' skin will clear by about the fourth month, requiring only gentle cleansing with plain water or a mild baby wash, says Dr. Shu. Avoid vigorous scrubbing, as well as lotions and perfumed soaps, which may make acne worse. In severe cases, doctors may recommend an over-the-counter or prescription medication, or an antibiotic if pimples become infected. But typically the acne will disappear as Mom\'s hormones leave baby\'s system.\nFascinating Baby Behaviors\nWhy Do Babies Love Mirrors?\nDo they know they\'re seeing themselves? When babies look in the mirror, initially they\'re unaware that the face in the glass is theirs, but they\'re still captivated. Babies are inherently interested in human faces -- and seem particularly drawn to those that are close in size to their own, says Dr. Shu. Studies suggest babies begin to recognize themselves about midway through their second year. In one famous British experiment, researchers asked mothers to play with their babies, age 9 to 24 months, in front of a mirror. The mothers then pretended to wipe dirt off their child\'s face but instead dabbed rouge on their nose. The babies younger than 15 months made no attempt to touch their own red nose (though some swatted at their nose\'s reflection), while nearly all of those 21 months and older touched the rouge.\nWhy Do Babies Love to Be Rocked?\nMost babies love to be rocked, swayed, and danced in circles -- in a way that would make many adults motion sick! Like their fondness for white noise, this could be a holdover from the days in utero, suggests Dr. Shu, where for nine months babies float in a fluid environment. ""Think of how weird it feels after you\'ve been on a boat and you step off onto solid ground,"" she says, adding it may feel similarly strange for infants who find themselves suddenly lying in a static bassinet. Rocking or swaying while holding your baby may bring back for her the comforting rhythm of the womb.\nWhy Do Some Babies Bang Their Head?\nWhile this behavior is understandably disconcerting for parents, some 20 percent of babies and toddlers bang their head. For reasons unknown, it\'s three times more common in boys than girls. Some babies may bang their head out of frustration and an inability to express themselves, says Dr. Shu. For these children, it may help to try to put their emotions into words for them: ""I know you\'re mad because you want that cookie now and don\'t want to wait until after dinner."" Hearing that you understand may defuse the tantrum, suggests Dr. Shu.\nOther babies seem to do this as a self-soothing technique, rhythmically rocking and banging as they try to calm down or fall asleep. They may do it more frequently when they\'re teething or have an ear infection, notes Dr. Shu, perhaps to distract themselves from their discomfort. Still others may bang their head to elicit a reaction. ""It gets them attention, even if it\'s not good attention,"" says Dr. Shu. For this reason, it\'s usually best to ignore head-banging, since any response will only reinforce the behavior, she says. Just be sure your child is in a safe place where he can\'t bang his head against sharp, breakable, or unstable surfaces. Though head-banging looks dangerous, your baby generally won\'t knock his noggin hard enough to hurt himself, assures Dr. Shu.\nWhy Party All Night and Sleep All Day?\nThe most common theory is that this pattern is established in the womb, when Mom\'s daytime comings and goings rocked and lulled baby to sleep, leaving him plenty of energy for midnight somersaulting. This routine may continue after birth, with baby catching plenty of shut-eye in the car or stroller, then clamoring for playtime at 2 a.m. Since your baby has no inherent way of discerning day from night, provide plenty of cues, says Dr. Borgenicht. When the sun\'s up, open the shades, turn on the lights and music, and try to keep baby actively engaged for substantial stretches of time, he advises. In the evening, dim the lights and limit stimulating sights and sounds. During middle-of-the-night feedings and diaper changes, keep the room dark and the chatter to a minimum. If you\'re consistent, your baby will soon get the message that daytime is for playing and nighttime is for sleeping, says Dr. Borgenicht.\nOriginally published in American Baby magazine, December 2006.\nAll content here, including advice from doctors and other health professionals, should be considered as opinion only. Always seek the direct advice of your own doctor in connection with any questions or issues you may have regarding your own health or the health of others.']"	['<urn:uuid:997beedc-4568-4a56-a347-11c8bc57bed0>']	open-ended	with-premise	long-search-query	distant-from-document	single-doc	expert	2025-05-12T21:52:51.670698	12	76	2388
31	As a lawn care specialist looking to advise clients on winter lawn solutions, how do perennial and annual ryegrass compare in terms of their effectiveness for overseeding dormant winter lawns?	Both perennial and annual ryegrass can effectively provide green winter lawn coverage when overseeded on dormant warm-season grasses. Perennial ryegrass, which germinates in 10 days or less, is commonly used for overseeding winter-dormant lawns and provides a permanent turf solution with a rich green color. Annual ryegrass, emerging within five to 10 days, can also be used to overseed warm-season grasses in fall, winter, and early spring, providing a quick burst of color. However, annual ryegrass is more suited for temporary seasonal use as it turns yellow and dies in late spring to early summer, while perennial ryegrass can maintain its color year-round in moderate temperatures.	"[""If you want fast-growing grasses suitable for northern temperate climates, choose cool-season varieties. These actively germinate and grow during periods of less intense heat in the spring and fall when soil temperatures are between 50 and 65 degrees Fahrenheit. The grasses sprout quickly, in as little as three to five days for some varieties under ideal conditions. Once germination occurs, cool-season grasses grow quickly to establish a fine lawn.\nAs the quickest germinating variety -- sprouting in 10 days or less -- perennial ryegrass adapts well to coastal regions that have moderate temperatures throughout the year. The fine-textured grass has a rich green color with a bunchgrass growth habit. Perennial rye is often used for overseeding winter-dormant lawns. Because it emerges rapidly, it is often used in turfgrass seed mixes and is ideal for suppressing weeds.\nAnnual ryegrass, an upright and coarse, shiny lime-green grass with shallow roots, emerges within five to 10 days. Because is has low drought tolerance and only moderately tolerates wear, it is normally used to provide cover for soil stabilization and not as a permanent turf solution. It can be used to overseed warm-season grasses in the fall, winter and early spring, resulting in a quick burst of color. Annual ryegrass turns yellow and dies in late spring to early summer.\nBentgrasses are often used in areas where cool-season grasses grow well, although they're not considered one of the major turfgrasses. Colonial and creeping bentgrass germinate within 10 to 14 days of seeding. Creeping bentgrass is more commonly used for seeding purposes on golf course greens and lawn tennis facilities. The fine-textured bright green grass has a shallow root system that requires frequent mowing, watering, aerating and dethatching, along with high fertilizer levels. High maintenance costs make it impractical for residential usage.\nTall and fine fescues germinate in seven to 14 days and are drought-tolerant, eco-friendly grasses that develop deep root systems. Both varieties are shade tolerant and stay green all year. Fine fescues are better suited to borderline cold climates in the northern United States. Tall fescues are well-suited to high-traffic areas and residential lawns. Seeding mixtures often include each type as an ingredient, thanks to overall hardiness. Fescues become dormant when weather turns too hot or too cold.\nOnce established, Kentucky bluegrass grows quickly during fall, winter and spring. Because germination takes two to four weeks, this grass is often mixed with faster germinating varieties, such as perennial ryegrass, to prevent weed infestation in new turfgrass areas. It is not well suited for temperate areas that experience extended periods of warm to hot temperatures in summer. When stressed by heat, this species goes dormant and can become prone to disease.\n- University of California-Davis: Perennial Ryegrass -- Lolium Perenne\n- University of California-Davis: Colonial Bentgrass Agrostis Capillaris\n- University of California-Davis: Creeping Bentgrass -- Agrostis Stolonifera\n- University of California-Davis: Annual Ryegrass -- Lolium Multiflorum\n- University of California-Davis: Kentucky Bluegrass -- Poa Pratensis\n- The Midwest Sod Council: Turfgrass Establishment\n- University of California-Davis: Grass Seed Germination Rates\n- Fescue.com: Facts About Fescue Grass\n- American Lawns: Tall Fescue Grasses\n- Thinkstock Images/Comstock/Getty Images"", 'Both Perennial and Annual Rye grass can provide a green winter lawn overseeded on warm season grasses that go dormant in the fall/winter. Ryegrass planted\non newly seeded lawns of cool and warm season grasses ryegrass acts as an erosion barrier while the permanent lawn develops. Ryegrass also provides any area with an almost ""instant"" green\ncoverage. This green coverage is the chief purpose for planting annual ryegrass. Ryegrass clippings mown and left on the developing lawn as it grows, providing extra nutrients as the\nBright green, fast growing, and prolific; the ryegrasses do much for lawns by adding contrasting color to early spring flowers, trees and shrubs as well as\nnutrients. Annual Ryegrass Information\nAnnual Rye grass is one of the most popular seeds sold for lawn and pasture purposes and is planted all over the world on it\'s own and in numerous grass seed mixtures. The\nadaptability of this cool season grass to many soils and climates coupled with fast germination and prolific growing rates make annual ryegrass an important grass type in establishing\nthousands of lawns and pastures in all but the hottest of planting zones.\nAnnual ryegrass is exceptionally useful as a fill-in grass when planted with slower growing permanent lawns. Ideally suited for short term seasonal use in\nareas reaching below the transition zone, annual ryegrass is a great fill-in to create green grass on new grass areas. In warmer regions, ryegrass lawns may require watering, fertilization\nand more frequent mowing.\nAnnual Ryegrass is a cool season grass generally used as a temporary planting for overseeding warm or cool season grasses. Its uses are many, including\ntemporary pastures, green manure, temporary coverage for erosion control, short term use on reclamation sites, parks, new lawns, and along highway shoulders. Annual ryegrass\nmay be seeded with other grass mixtures, legumes, clovers and small grains. Annual Ryegrass is fast germinating, quick growing and requires low to medium fertilization. Annual Rye Grass Is Eco-Friendly\nAnnual Ryegrass is a compatible growing medium when planted with any vegetation where needed.\nAnnual ryegrass is very beneficial to areas that are infected with nematodes which are almost impossible to get rid of except by heavy chemical control. A natural and inexpensive\nresolution for this infestation problem, annual ryegrass is ecologically friendly. Yearly planting of this ryegrass seed ahead, behind, or with crops known to be a nematode favorite would\ncertainly be cost effective.\nAnnual ryegrass is easily removed from any area through heavy grazing, mowing or turning under before the seeding stage.\nnon-aggressive, vigorous grower, furnishes nutrients back to the soil, forms an erosion soil basis for stability, highly digestible, beautiful color, dies off when you need it to without\nchemicals - makes annual ryegrass a winner!\nRyegrass is one of the few ""throw and grow"" seeds that can be sown without the hassle of tilling, scarifying, or digging into the soil and destroying any of the\npermanent ground covers already in place. Plus if the weather helps out by raining just before sowing then you practically have it made. Otherwise watering the yard and applying\nfertilizer before or after sowing are the main two steps for quickest establishment. Note that the trick to sowing any seed is to get an even coverage in conjunction to sowing the correct\namount to achieve an even lawn appearance as the grass fills in. And to get the seeds to make ""soil contact"" for germination to occur.\nOne of the best ways to get rid of those winter ""\n"" is a spot of annual ryegrass "" blues green"" and some\nearly spring bulbs. Beautiful mental picture with very little costs! Perennial Rye Grass Information\nPerennial Ryegrass lawns are one of the best lower maintenance lawns of the cool season grasses. This is one of the toughest and most traffic tolerant turf\ncovers that can be grown. Ryegrass is noted for quick germination, shiny green color, fine texture (newer turf type varieties) and dense forming sod. High disease and insect resistance\nhelps to make perennial ryegrass one of the leading choices for lawn and athletic covers in the cooler regions of North America. Perennial ryegrass is planted as permanent turf or lawn\ngrass and to overseed warm season grasses while they are in winter dormancy.\nPerennial ryegrass seed is also mixed with other cool season grasses to provide more density to the dominate grass in the mixture. Perennial ryegrass seed\nis often added to Kentucky bluegrass seed to add strength and give bluegrass time to repair while the perennial ryegrass is in full growth. These two grass species can be mown at the same\nheight and retain a sod density for which both are well known. Although bluegrass may require more fertilizer, the perennial ryegrass will respond to additional amounts without any damage. Turf Quality Perennial Ryegrass Seed Blends\nPerennial Ryegrass seed blends\nsuch as the Champion GQ, Clubhouse GQ or Pennington\'s Perennial Rye Grass Blend\nare widely planted and are excellent choices for lawns in the cooler\nregions of the USA. They are also popular with Turf Agronomists for overseeding of quality Bermuda lawns, golf areas and sports fields in more Southern state locations during the winter\nmonths. Most Golf Course greens today are overseeded with Perennial Ryegrass for primary winter cover. Note: Centipede lawns should NOT be overseeded with Perennial Ryegrass, but\ninstead use Annual Ryegrass. GRASS\nSEED DISCOUNT PRICES!\nFarmers, Ranchers, New home lawn construction? Save\n$$$ by buying in large volume.\nAnnual Ryegrass For Pasture & Farm Use\nAnnual ryegrass certainly must be considered a staple\nin the farming and ranching business. Ryegrass is used in so many useful ways to\nbenefit pastures or farm lands -- providing quick growing short season forage, hay,\ngreen chop, green manures, erosion control, weed control, and so much more.\nPASSEREL PLUS ANNUAL RYEGRASS FOR PASTURE\n- Highly rated Forage Ryegrass! Read about\nPasserel Plus Pasture Ryegrass along with Data\nReports. Passerel Plus annual ryegrass is inexpensive, easily seeded, established and harvested, this grass is a number one seller\nAnother popular annual ryegrass,\nAnnual, is planted as \'row crops\' in agricultural fields from berry fields to corn fields as well as orchards. Annual ryegrass is row cropped\nwith all kinds of crops to keep down weeds and provide early coverage, shade for some\ncrops, green fertilizer when mown and returned to the soil, wildlife crops, and when\nleft to maturity it will partially reseed for the next season -- although not as dense as the first year of planting. Do Not Plant Ryegrasses Developed For Lawn Use In Pastures!\nDue to the potential for Ryegrasses in pastures\nto have the possibility of producing higher than desirable Endophyte production of toxins, you should only plant ryegrasses that have a known history of safe use for forages such as\nPasserel ryegrass seed or Gulf Annual ryegrass seed. Some of the newer\nimproved perennial ryegrass varieties for lawns often contain\nEndophytes and are NOT suitable or recommended for pasture\nuse. Plant only forage type ryegrass varieties for pastures. Perennial Ryegrasses For Pastures\n. A valuable winter and spring grazing crop that can be overseeded\nonto additional forages or used alone lengthening the pasture season. A fast growing\nmulti-use forage crop requiring minimal tillage and competitive enough to make a place for\nitself. A compact growing habit reinforces the ground and gives a perfect erosion base for\npastures with heavy traffic or in the case of heavy rains or snow meltdown. Ryegrass helps\nkeep the livestock out of the mud as much as possible. PERENNIAL RYEGRASS PASTURES are highly digestible and a\nfavorite of all grazing animals\nPerennial ryegrass is fast germinating, grows prolifically, and can be grazed heavily. Many varieties have been developed and are in ""prescription"" mixes or in stand alone crops for complete\npasture and hay production. The ryegrasses are well known for being companion grasses in\nmixes of fescue, clovers, timothy, orchard grass and many other pasture crops. The\nryegrass is usually first to germinate and grow in while the others are developing.\nPerennial rye is used to overseed many warmer season pastures that may go dormant or are\ngrowing slower due to cooler temperatures. This is one of the most diversified grasses\nused in the forage industry for permanent and semi-permanent pastures. How To Overseed With Ryegrass\nOVERSEEDING - Since annual and perennial ryegrasses are\nused in overseeding many areas the best stands achieved are still the ones with a small amount of\ninput. Rake or remove all debris (this helps scratch the soil for better soil contact),\nfertilize as suggested (slow release is actually the best and easiest), water lightly (if possible) before seeding evenly at the heaviest rate recommended, rake again if\npossible and lightly mist to keep seeds in place. Do not use excessive force as this will move seeds\ninto uneven clumps. Watering helps increase germinating process.\nSEED ONLINE FROM SEEDLAND\na beautiful tomorrow!®']"	['<urn:uuid:eec722e1-db15-489a-bf9d-26cc30e08813>', '<urn:uuid:04b34d3d-6ca3-4112-aec1-3ee136a32aa2>']	open-ended	with-premise	verbose-and-natural	distant-from-document	comparison	expert	2025-05-12T21:52:51.670698	30	106	2005
32	What approaches help protect against poor crop yields?	To manage production risks and protect against lower than expected yields, you should track recommended production methods, diversify by planting different crop varieties and new crops, implement intensive growing practices, and purchase crop insurance coverage to stabilize income during times of loss. Production risks mainly arise from adverse weather conditions like excessive rainfall, drought, or freeze during harvesting or planting.	['Certain agricultural risks need to be addressed for cultivators and small farms to stabilize income: human resources, legal, production, financial, and marketing risks. Different strategies and tools can be utilized to counter each of these risks.\nHuman Resource Management Risks\nRisks about human resources are linked with people and relationships. The relationship can be of any kind. It can include family members, farm employees, and consumers. The main sources of risks related to human resources come up due to any reason such as death, divorce, or disability.\nAny of these proceedings can have a devastating impact on the small farm and its cultivators. Other human resource risks can involve harmful influences rising from poor communication and employee management skills.\nWhat strategies can be used to manage human resource risks?\n- Develop and implement good “people skills,” not just with your family members but also with your employees\n- Assess other sources of labor\n- Deliver adequate training to your employees\n- Converse with your family members and employees\n- Identify and give incentives for good performance\n- Evaluate your powers of attorney, wills, and trusts\n- Consider life insurance and health needs\nLegal risks refer to accomplishing and achieving business contracts and agreements. If you are unable to meet these agreements, it will result in you paying a high cost for it. Another main foundation of legal risk relates to tort liability.\nWhat strategies can be used to manage legal risks?\n- Evaluate insurance policies\n- Carry adequate liability coverage\n- Select a different legal structure for your business. The sole proprietorship is not always considered the best\n- Comprehend business agreements and contracts. Wherever you feel the need, ask questions if unsure.\n- Make good interactions with your neighbors and speak their concerns\n- Utilize good agricultural methods to restrict environmental risks\n- Identify and follow all Federal and State regulations linked to your farming process\nProduction risk relates to the likelihood that your output levels or yields will be lesser than projected. The chief causes of production risks rise from hostile weather conditions such as excessive rainfall, drought, or freeze when harvesting or planting.\nWhat strategies can be used to manage production risks?\n- Track recommended production methods\n- Expand enterprise by planting different varieties of crops and entirely new crops\n- Increase production by implementing intensive growing practices\n- Buy crop insurance coverage so that you can stabilize your income at times of loss\nFinancial risks refer to not having enough cash to fulfill the expected obligations. You may be generating lower profits than expected, or you might be losing your equity invested in the farm. Foundations of financial risk normally come from marketing and production risks. That’s why bookkeeping is encouraged so that farmers can easily keep tabs on their financial position.\nWhat strategies can be used to manage financial risks?\n- Create a calculated business plan\n- Keep tabs on your enterprise benchmarks and financial ratios\n- Regulate key farm expenditures\n- Follow a trend analysis so that you can assess changes in your farm’s owner’s equity and profits over time\nMarketing risks are related to the likelihood that you might lose your market for your production items. It can also be that you receive the price for less than expected.\nWhat strategies can be used to manage marketing risks?\n- Create a marketing plan that is realistic in terms of target prices and sales forecasts\n- Make or be a part of a marketing cooperative so that you can enhance your prices and assure your market']	['<urn:uuid:99677eb5-e3b4-4c02-9c31-4d6a4bf2734a>']	open-ended	with-premise	concise-and-natural	distant-from-document	single-doc	expert	2025-05-12T21:52:51.670698	8	60	592
33	What's the correct way of moving my hands when I want to make a fast shot in table soccer?	When taking a foosball shot, you should turn your wrist quickly so the rod spins about 180 degrees. Turning your wrist only a quarter of a turn will not provide enough speed on your shot.	['Foosball Handle Grip\nHow to Grip the Foosball Handle\nKnowing the right way to grip the foosball handle is extremely important to your shot speed, accuracy, and overall performance. Most beginners will start out playing the game by gripping the foosball handle too tight. This is most likely a bad habit carried over by other sports like baseball or tennis. Most golfers understand that it takes a little more finesse, and they tend to catch on to the proper method more quickly. Foosball is a game of finesse and your grip should be too. Keeping a loose grip on the handle will allow for maximum movement and speed. This will limit your resistance on the handle and allows your rod to move freely and fluidly. There are 2 primary grips that are used in foosball that are explained in more detail below.\nTraditional Shot Grip\nThis grip applies to all the standard shots like pull shots, push shots, spray shots, and tic-tac shots. You will know that you are holding the handle right if there is a small gap between the skin that connects your thumb and index finger and the handle. If this skin is completely wrapped around the handle, you know you are gripping it too hard for shooting. Having white knuckles is also a really good indicator for gripping the handle too hard. Remember, a foosball handle is not a tennis racket or baseball bat that you have to swing! Another foosball tip when shooting is to turn your wrist quickly and the rod should spin about 180 degrees when taking the shot. Turning your wrist a quarter of a turn will not provide enough speed on your shot. You can also shoot an open-handed shot where you actually roll the handle on your wrist in order to increase the speed of your shot.\nOpen Handed Foosball Grip\nAn open-handed grip is used when shooting a rollover shot, also known as a snake shot. Setting up this shot requires the man to be tilted forward where the foosball man’s toe is pinning the middle of the ball against the table. Once the foosball ball is in this position, players switch between the traditional grip, as explained above, to an open-handed grip. Your hand should be in an open position with the bottom part of your hand and beginning of your wrist touching the handle. This will allow you to slide your hand in an upward motion so your man swings almost a full rotation before making contact with the ball again. Many beginners think this is an illegal spin of the rod the first time they see a snake shot but the man actually makes contact before a full 360 degree turn is made, making it a legal shot.\nImproving your Foosball Grip\nTo have the best grip on the handle, it is important to wrap the handles with wraps or tubes. There are many different styles of tackiness, stickiness, and thickness, so we recommend trying a variety of them until you find what works best for you.']	['<urn:uuid:b0fa94e8-3a1f-4f63-b652-1306ae5f13c5>']	factoid	direct	verbose-and-natural	distant-from-document	single-doc	novice	2025-05-12T21:52:51.670698	19	35	511
34	How do performance tests work and what scoring systems exist?	Performance tests involve observing users completing tasks in environments matching real usage conditions, providing them with the same tools and resources as actual users. For scoring, there are established systems like the Nielsen's Attributes of Usability Questionnaire (NAU) which uses a 1-7 scale across five dimensions (Learnability, Efficiency, Memorability, Errors, Satisfaction) with a maximum score of 35, and the System Usability Scale (SUS) which uses a more complex 0-100 scoring system based on 10 questions with 5-point scales, where scores above 80 are considered excellent and below 50 are deficient.	"['Web Brutalism Research Dataset\nThis dataset contains results from two usability tests: the Nielsen\'s Attributes of Usability Questionnaire (NAU) and the Brooke\'s System Usability Scale (SUS) test. Nielsen\'s Attributes of Usability Questionnaire (NAU): This test had five questions and eight options for each response: from one (bad) to seven (good), and a null value in case users decide not to provide an answer. The questions included in the NAU test evaluated: Learnability (How easy was it to learn to use the system?), Efficiency (How efficient was the system performing set tasks?), Memorability (How easy was to remember the features of the system?), Errors (How easy was to correct errors done in the navigation process?), and Satisfaction (How interesting, exciting or pleasant was the system to use?). The highest possible score for each question of the NAU was seven. That means thirty-five was the highest mark a system could obtain. With that in mind, we established five tiers to qualify systems: A - Excellent (35), B - Good (between 26 and 34), C - Average (25), D - Poor (between 15 and 24), and E - Deficient (below 15). Brooke\'s System Usability Scale (SUS): This evaluation is comprised of ten questions and each one has associated a five-point scale that range from ""Strongly disagree"" to ""Strongly agree."" Items could therefore obtain a minimum of one point and a maximum of five. These are the questions of the SUS test: (1) I think that I would like to use this system frequently; (2) I found the system unnecessarily complex; (3) I thought the system was easy to use; (4) I think that I would need the support of a technical person to be able to use this system; (5) I found the various functions in this system were well integrated; (6) I thought there was too much inconsistency in this system; (7) I would imagine that most people would learn to use this system very quickly; (8) I found the system very cumbersome to use; (9) I felt very confident using the system; and (10) I needed to learn a lot of things before I could get going with this system. The formula to obtain a score from this test was more complex. According to Brooke (1995), ""to calculate the SUS score, first sum the score contributions from each item. Each item\'s score contribution will range from 0 to 4. For items 1, 3, 5, 7, and 9 the score contribution is the scale position minus 1. For items 2, 4, 6, 8, and 10, the contribution is 5 minus the scale position. Multiply the sum of the scores by 2.5 to obtain the overall value of SU. SUS scores have a range of 0 to 100."" Given the highest possible mark was one hundred, the five categories established to qualify systems according to their results were: A - Excellent (above 80), B - Good (between 66 and 80), C - Average (65), D - Poor (between 50 and 64), and E - Deficient (below 50).\nSteps to reproduce\nAfter opening the Brutalist site of Pieter Paul Pothoven (https://www.pieterpaulpothoven.com), participants must carry out six tasks within that website: (1) Navigate to the ""In Abstentia\'\' exhibition; (2) Recover the opening date of the ""Territorial Drift\'\' exhibition; (3) Navigate to the ""Limen\'\' exhibition; (4) Navigate to the ""Laguard\'\' exhibition; (5) Return to the homepage; and (6) Find the artist\'s e-mail address. After that, participants should complete both the NAU and the SUS test.', ""Usability Testing of Technical CommunicationsEdit\nWhat is Usability Testing?\nUsability testing is a tool used to evaluate a set of instructions or product. It tests to see if these instructions or product do what it is designed to do. Usability tests are a necessary part of the process of writing instructions or developing a product.\nWhat is the Purpose of Usability Testing?\nUsability testing involves test readers who read and/or use a draft in a similar situation as the intended audience would be in. After the test, you gather important information by observing and questioning the test reader. This allows you to get a good sense of the overall response of your draft so you can revise areas that need improvement. Usability testing can be simple or complex depending on the situation and importance.\nWhy is usability testing such a big deal and who uses it?\nYou may think your writing is easy to understand but more importantly, does your reader? Does your draft work the way it is intended to? The only accurate way to answer this question is to have a test subject read and evaluate your draft. Primary draft testing is the best method to increase the quality of a final draft that will be read and studied by thousands of people. Test materials range from drafts of websites, to print instructions, to informational documents.\nEverybody from fortune 500 companies to college students use usability testing in order to improve the efficiency of their final draft. Big corporations treat testing as such a big deal that they build special buildings for the sole purpose of usability testing. These special facilities can be equipped with video cameras, monitored computers, as well as one-way mirrors so testing specialist employees can observe customers' responses to drafts. College students can use this strategy with a roommate or classmates. Whatever the situation, usability testing is proven to be a valuable tool in the writing world.\nPreparing for Usability TestingEdit\nEstablish your test objectives Create questions or concerns that you want answered as a result of your usability testing. Be prepared however, often times questions arise during the testing process. Pay close attention to what your tester is telling you, this is valuable information to help improve your instructions. Make sure to annotate these questions/concerns in your observations, because this should be red flag that issues exist in a particular section that must be addressed. Once you realize what you want to learn, you can customize the test so it evaluates and gives you the specific information you are looking to obtain. The two most general questions usability testing seeks to answer are:\n- How can my draft be improved?\n- Is my communication sufficient?\nOther examples of more specific question are:\n- How well is the page formatted?\n- Does the draft influence the test readers’ attitude the way I intended it to?\n- Are my tasks easy to follow?\nTo help evaluate this criterion you may create measurable criteria. Measurable criteria can involve having a test reader read an informative draft and having him answer questions. Make sure the criteria is highly specific to your objectives. Criteria that is too broad will prove to be invaluable when assessing the problem later on. If he gets a certain percentage correct, the draft is adequate. It can also be time related. For instance, can the test reader navigate to the website link I provided within twenty seconds?\nPick suitable target readers that fit with the testing situation This is a very important part of the testing because if a test reader is not a part of the target audience, then they may influence you to make ineffective adjustments to your paper. A vital rule for most cases is to pick a dentist for dentistry or an electrician for electrical instructions. Furthermore, the knowledge levels of your target readers on the subject you want to test them on should varied according to the the objective of your testing. For example, you are constructing an origami instruction for individuals with intermediate origami level, and you want to test the clearly of your instruction with a usability testing. If you select test readers with no origami experience, your instructions will seen overly complicated to the readers and the outcome of the usability testing will send you a message that additional clarification is needed where in actuality your the amount of information/explanation in your instruction might just be sufficient for your target audience. Similarly, testing the clearly of your instruction with test readers in advanced origami can lead you to over simplify your instruction. This is to say that you have to make sure the test reader is not too familiar with the subject so that he already knows what to expect. If the reader is already familiar with the project, he may not have any difficulty understanding it, but your target audiences may be perplexed by how complex the writing is. The number of test readers used can range and play a role in determining the usability of your final draft. The number of test readers used depends on the type of draft and the availability of test subjects. For example, sometimes ten or more test readers are needed in testing the overall comprehension of a draft. The purpose of needing more test readers is that each test reader have a unique perspective, and thus can contribute different opinions and ideas. On the other hand, in case of a more structured drafts such as the step-by-step instructions, fewer testers are needed because all you need from them is for them to test for you the clearly of your instruction. The key here is to make sure that your test readers use your draft the same way your users will.\nMajor categories for readers' tasks Generally speaking, the tasks done by the test readers can be categorized into three categories.\n- Perform a procedure (i.e. performance test), such as going through a step-by-step instruction.\n- Locate information (i.e. location test), such as finding information in a Reference Manual.\n- Understand and remember content (i.e. understandability test), such as learning something through reading.\nIn case of testing for a newly designed website, all three categories are needed to ensure accurate results.\nThere are three main types of usability tests:\nA performance draft is also known as instructions. It is writing in order to help the reader perform an action. The way to test a performance draft is to give the draft to tester in an environment that has the same tools, information, and references as your target audience would have. As they read the instructions, observe them without interference in order to see if they can perform the process as smooth as possible without any complications.\nHere are the four major elements of performance tests:\n- Tasks: Ask testers to perform the same tasks your readers will perform.\n- Location: Use the same setting for testing that your readers would use.\n- Resources: Provide testers with the same tools, equipment, and other resources that your readers will have but not additional ones.\n- Information Gathering: Gather information in ways that will enable you to observe the details of the testers' experiences without interfering in their testing of your draft.\nLocation tests ask readers to locate information in reference manuals or websites. One way to test how valuable the layout of your writing is is to ask the test readers questions and have them locate information in the text as fast as they can. This type of test is very valuable in terms of how good your headings, topic sentences, front matter, back matter, and links are at guiding a reader through the text or website.\nUnderstandability tests are extremely important just for the fact that if your reader does not understand or comprehend what you are communicating then the writing is useless to them. Understandability testing should be utilized in every writing test. The main way to conduct understandability tests is to have the test reader read the information and then answer strategic questions in order to get responses that will help guide your editing process.\n- Note: Conducting many small usability test drafts is far more efficient than conducting one large test after the finished draft is complete. If you are testing a web page, sketch the design on paper to be tested so you get a sense of what is needed before starting the website creation. Testing drafts more frequently will allow for many small easier revisions instead of one big overhaul of the draft.\nAfter Usability TestingEdit\nAfter the test subject reads and uses your draft, the first step is to interview the person. This is used to gain more crucial information in addition to the information you observed during the testing. The questions vary according to the type of draft and the criterion of the usability test, but here are some general questions that could be asked:\n- What was the biggest problem you encountered in the draft?\n- What do you suggest?\n- What did you like about the draft?\n- Was the format user friendly?\nAfter usability testing, the second step would be to analyze the data you collected. A very helpful way to analyze data is to input your raw data into an application such as Microsoft Excel from which graphs and tables can be constructed. From the test and the interview, you should be able to determine specific areas of your draft that need improvement or revamping. Examples could be anything from simply rewording to make more sense or possibly even a complete revamping of your draft. For this reason, usability testing is a great tool for determining the effectiveness of your draft.\nThe third step would be to implement the necessary changes to your draft. After the changes are made, further testing is often carried out to see whether or not the changes worked. It is important to realize that your first draft of instructions are usually not perfect and multiple revisions and testings are often necessary.\nHow to Make Usability Testing More EffectiveEdit\nUsability testing can often be a lengthy process. For this reason you want it to be as effective as possible. Listed here are some ideas to make your usability testing more effective.\n- Make sure you are prepared to take notes on your testing. Have documents with charts that can be filled with text during the testing process to make data more easy to analyze\n- Ensure testers are part of your target audience.\n- ex. If you have a set of instructions designed for adolescent men, do not perform usability testing on women or men outside the specified age group\n- Have all the materials for the tester be within reach, or easily available\n- Since you will be testing multiple people on a given day, bring enough testing material so you can perform all the tests you wanted to do\n- Be sure be descriptive about what criteria you will be recording. You can make a number system to rank experience or possibly overall clarity of whatever is being tested.\n- Also leave room for a short descriptive interview at the end of the usability sheet to get the users thoughts and concerns. This will be valuable when you are revising your document to ensure it is of the highest quality and free of complications.\nAvoiding Bias in the Testing ProcessEdit\nAvoiding bias is a big thing that can help save a lot of time spent revising. Test readers' responses need to be natural and must be similar to how the target audience would see the draft. Five simple things can minimize biased test results are:\n- Downplay the fact that the writing is yours. This will allow the test readers to be more honest because they will not be afraid of offending or criticizing you. Do not show expression when the tester is critiquing the draft because the tester may not be as honest if they see that the work is yours.\n- Pay attention to how you phrase questions. Make the questions so the reader has an open path to answer either way. If the format of your questions creates pressure to pick the ideal answer for the test reader, your results could be considered biased.\n- Only intervene into the testing phase if it is absolutely necessary. Letting the test reader struggle through issues in the draft himself will allow for pure results that are unbiased. Of course, intervention should be used if the test reader is in danger of injury.\n- Make sure the environment is as distraction-free as possible for the test reader. Keep your distance! You do not want to impede in the testing process distracting the subject.\n- Ideally, if possible, watch the test from a video camera, one-way mirror, or afar while somebody else conducts the test. However, if this person conducts the interview also, he probably will not summarize the answers the same way you would.\nCross Cultural TestingEdit\nIf you are going to have your writing read by people around the world or in different cultures, you are going to have some extra issues to consider. This is because every culture has different beliefs and norms. Writing in one culture that is clear and simple can be complex and inappropriate in another culture, unless special measures are taken when conducting usability testing. For example, when Breathe Right(R) nasal strips were going global, their box contained dark coloring because the product was suppose to be used to clear up the nasal passage during the night time. However, this dark package theme did not work well in Europe. They saw it as dark and gloomy and they wanted the package to be light and clear. The key to obtaining global technical communication is to have usability testers from that culture.\nEthics in Usability TestingEdit\nDue to the fact that technical communication is lawsuit prone, appropriate measures should be taken even when usability testing is used. Technical writing testing has the same loose guidelines as research in medicine and psychology:\n- The volunteer must be informed and fully aware of the process.\n- The volunteer must have agreed and given approval to participate in the testing.\nFor some types of tests, this is only an ethical issue. However, there are situations where there are legal restrictions for the researcher to inform the volunteer of the situation in writing.""]"	['<urn:uuid:dd4f9fc2-d225-4e8a-83b3-8c156b4b0d68>', '<urn:uuid:c5c95bef-a526-45b6-be71-ff5773763d25>']	open-ended	with-premise	concise-and-natural	distant-from-document	multi-aspect	expert	2025-05-12T21:52:51.670698	10	90	2991
35	How do the licensing models of qMRLab and Bio-Formats differ in terms of their requirements for software distribution and commercial use?	qMRLab and Bio-Formats have different licensing models. qMRLab uses the MIT License, which allows users to freely use, modify, and distribute the software without restrictions, including for commercial purposes, as long as they include the original copyright notice. In contrast, Bio-Formats is released under the GPLv3 license, which requires that any software linking to Bio-Formats must also make its source code available under GPL terms. However, Bio-Formats does offer some components for open file formats under a more permissive BSD-2 license, and commercial licenses are available through Glencoe Software for non-GPL software developers.	"['qMRLab is an open-source software for quantitative MR image analysis.\nThe main goal is to provide the community with an intuitive tool for data fitting, plotting, simulation and protocol optimization for a myriad of different quantitative models. The modularity of the implementation makes it easy to add any additional modules and we encourage everyone to contribute their favorite recipe for qMR!\nFor documentation, visit the Documentation website.\nIf you are a developer, please visit the Wiki page.\nPlease report any bug or suggestions in GitHub.\nqMRLab is a fork from the initial project qMTLab.\n- Karakuzu et al. The qMRLab workflow: From acquisition to publication. ISMRM 2019\n- Duval et al. Quantitative MRI made easy with qMRLab. ISMRM 2018\n- Cabana et al. Quantitative magnetization transfer imaging made easy with qMTLab: Software for data simulation, analysis, and visualization. Concepts in Magn Reson 2015\n- Soustelle et al. Correlations of quantitative MRI metrics with myelin basic protein (MBP) staining in a murine model of demyelination. NMR in Biomed 2019\n- Kim et al. Rapid framework for quantitative magnetization transfer imaging with interslice magnetization transfer and dictionary‐driven fitting approaches. Mag Res Med 2019\n- Boudreau et al. All you need is a browser: eliminating barriers to MRI education with open-source interactive tutorials. Junior Fellows Symposium: Africa Challenge, ISMRM 2019\n- Romero and Sinha. Magnetization Transfer Saturation Imaging of Human Calf Muscle: Reproducibility and Sensitivity to Regional and Sex Differences. J Magn Reson Imaging 2019\n- Michálek et al. Fast and accurate compensation of signal offset for T2 mapping. Magn Reson Mater Phy 2019\n- Barbieri et al. Circumventing the Curse of Dimensionality in Magnetic Resonance Fingerprinting through a Deep Learning Approach. arXiv:1811.11477\n- Varma et al. Low duty-cycle pulsed irradiation reduces magnetization transfer and increases the inhomogeneous magnetization transfer effect. J of Mag Res 2018\n- Campbell et al. Promise and pitfalls of g-ratio estimation with MRI. NeuroImage 2018\n- Boudreau and Pike. Sensitivity regularization of the Cramér‐Rao lower bound to minimize B1 nonuniformity effects in quantitative magnetization transfer imaging. Mag Res Med 2018\n- Boudreau M. Relaxometry Series: MP2RAGE T1 Mapping. qMRLab.org 2019\n- Boudreau M. Relaxometry Series: Variable Flip Angle T1 Mapping. qMRLab.org 2018\n- Boudreau M. Relaxometry Series: Inversion Recovery T1 Mapping. qMRLab.org 2018\n- Karakuzu et al. Quantitative MR Study Group Competition, second place, ISMRM 2019\n- Boudreau et al. Junior Fellows Symposium Challenge, Africa challenge winner, ISMRM 2019\nIf you use qMRLab in you work, please cite:\nCabana, J.-F., Gu, Y., Boudreau, M., Levesque, I. R., Atchia, Y., Sled, J. G., Narayanan, S., Arnold, D. L., Pike, G. B., Cohen-Adad, J., Duval, T., Vuong, M.-T. and Stikov, N. (2016), Quantitative magnetization transfer imaging made easy with qMTLab: Software for data simulation, analysis, and visualization. Concepts Magn. Reson.. doi: 10.1002/cmr.a.21357\nThe MIT License (MIT)\nCopyright (c) 2016 NeuroPoly, Ecole Polytechnique, Universite de Montreal\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the ""Software""), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\nThe above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\nTHE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.', 'OME (Open Microscopy Environment) is an open-source initiative aiming to produce open-source software and format standards for microscopy data. The project is started by researchers from the University of Dundee, later it gathered the attention and support from researchers, developers, & scientists from all over the world, from universities, institutes, laboratories, companies, & open-source developers.\nOME\'s team had released 3 projects that have a great impact on digital pathology. Here we will provide an overview for all of them:\nOMERO is an open-source client-server solution that provides a platform for managing, visualizing, & analyzing microscopy imaging. It\'s a complete solution to manage whole-slide images with powerful features to build applications over it or use it with an existing application.\nOMERO is built on a solid architecture, It has a developer friendly-API, multiple clients for different operating systems, It also supports multiple programming languages.\nOMERO is used as an optional microscopy imaging server in Orbit (Whole-slide image analysis platform with BigData and Machine learning/ Deep learning support).\nOrbit is an open-source virtual slide/ whole-slide image analysis that supports machine learning, deep learning, & big data (Apache Spark). It\'s built for researchers and AI software specialists. Read our snap review about it here: Orbit: Digital Pathology meets AI (Machine learning & Deep learning) & Big Data with Open-source flavour.\n- Clients (Linux, Windows, & macOS) / Server/ Plugins (Fiji, Matlab)\n- Java, Python, REST API\n- The OMERO API allows clients to be written in Java, Python, C++ or MATLA\n- The OMERO platform includes a Java client OMERO.insight, a Python-based web client OMERO.web, a Command Line Interface which uses Python, and a Java ImageJ plugin.\n- Work with your data in Python, Java, MATLAB, and C++.\n- Server scripting with Python\n- Analysis support by OMERO server using database tables\nBio-Format is a community-driven Java library for reading, & writing microscopy image files, & metadata. It uses standardized, open formats.\nBio-Format is used OMERO as well as some powerful medical image analysis programs like ImageJ, CellProfiler, & Icy. It can be used with Matlab as well.\nBio-Formats 5 improves support for High Content Screening, time-lapse imaging, digital pathology, and other complex multidimensional image formats, reading over 150 file formats and their metadata to the OME data model standard. [src:OME]\nBio-Formats Java API, which supports reading of over 140 file formats and writing of over 15 file formats. [src:OME]\n3- OEM files OME-TIFF\nOEM files are similar to BioFormats but it\'s focused on specific file type: OME-TIFF. It reads and writes OME-TIFF files with support of OEM data model, which eases the visualization and analysis for OEM-TIFF files.\nOEM files is available for download for Linux (Ubuntu), Unix (FreeBSD), macOS (10.12 and later), CentOS (7.3, & 6.9), & Windows (32-bit, & 64-bit). OEM also offers the source code.\nThe source code for OMERO, & BioFormats are released under GPLv3, However, there is a commercial license from\nGlencoe Software, a company that formed by members of the OEM\'s core team, It provides commercial-based solutions and custom solutions based on OME products.\nUnder the terms of the GNU public ""copyleft"" license, any software package linking to Bio-Formats, either directly or indirectly, cannot be distributed unless its source code is also made available under the terms of the GPL. Some components which provide reader and writer implementations for open file formats are released under a more permissive BSD-2 license which enables non-GPL third-party software. For a complete list of which file formats are included in the BSD license, see the BSD column of the supported formats table. Developers of non-GPL software wishing to leverage Bio-Formats components not covered by the BSD license may purchase a commercial license from Glencoe Software.\nOEM files is released under the BSD-2 license.\nOME provides a solid infrastructure for Digital Pathology, for research, building clinical diagnosis applications, & education. It provides developers with open source software packages that allow them to build production-ready digital pathology applications with powerful features. It supports multiple-platform, and it\'s battle-tested with several productions. OEM is maintained and continuously developed & improved by groups of researchers include biologists, software engineers, software developers, & bio-informatics specialists, which it\'s unlikely to be abandoned or discontinued in the near future.\nWhole-slide Image Analysis\nOpen source Free Whole-slide Image Viewers and Analysis Software']"	['<urn:uuid:c02f2121-763f-4421-8592-6bdd50b141fc>', '<urn:uuid:6b91d92e-5992-460f-a8c4-da8b581b9c7e>']	open-ended	direct	verbose-and-natural	similar-to-document	comparison	expert	2025-05-12T21:52:51.670698	21	93	1350
36	What makes modern campuses better at handling crises?	Modern campuses are better equipped to handle crises through two key developments. First, they have advanced microgrid systems that can operate independently during emergencies, providing uninterrupted power to critical facilities. These systems use sophisticated software controls and smart building management technologies for optimal operation. Second, campuses have improved their ability to support students' psychological wellbeing through better understanding of mental health factors. Research shows that fostering positive self-concept and social adaptation capabilities helps students cope with various pressures, as about 20-30% of college students experience mental health challenges. This combination of infrastructure resilience and mental health support makes campuses more capable of managing both physical and psychological crises.	['In recent years, with the increasing enrollment of universities throughout the country, the number of college students is increasing year by year. At the same time, with the complexity and diversity of the current society, college students are facing more and more psychological pressure, such as employment, emotional and interpersonal relationship. The mental health of college students has increasingly become an issue and hotspot which is noticed and researched by society. Currently, College students in all factors of Symptom Check-List90 (SCL-90) were significantly higher than the norms in 1986   . The number of college students with mental disorder arrived at 20% - 30%, which is much higher than other social groups  . Therefore, the aim of the study was to explore the internal impact factors and the mechanism of action of the psychological health among college students, and to provide an important theoretical and practical significance for the prevention and intervention of college students’ psychological health.\nSelf-concept is the individual’s cognition and evaluation of the self and social environment which are formed during the process of socialization, which is the sum of the individual’s self-cognition   . The formation and development of self-concept is a dynamic and changing process. College day is the period that self-concept goes through fastest development and has the most problems. College Students’ understanding and evaluation of self will not only affect their own behavior, but also affect their mental health. It is found that there is a significant positive correlation between self-concept and mental health, which has a strong predictive effect on mental health     . Self-concept can prevent the social dysfunction  , but also can prevent mental health problems  . That is to say, college students with good self-concept can make a more objective evaluation of themselves, and actively accept the self, thus regulating and maintaining their own mental health. However, how does self-concept affect the mental health of college students?\nSocial adaptation refers to the individual in the interaction with the social environment, and constantly learns or modifies the behavior and life-style, and ultimately to maintain a harmonious and balanced state with the social environment  . Does social adaptation play an intermediary role in the relationship between self-concept and mental health of college students? At present, there is no relevant literature to explore this issue. It is found that there is a significant positive correlation between self-concept and social adaptation of college students, that is, the lower the self-concept of college students is, the lower the level of their social adaptation is  . Relative to the individual with high self-concept, the individual with low self-concept in the fact of social adaptation showed negative features  . In addition, social adaptation is one of the crucial factors affecting mental health  . For example, researches have found that social adaptation can effectively predict mental health, individuals with poor social adaptation; their mental health status will be also relatively poor    . Therefore, it is reasonable to infer that the social adaptation is a mediating variable between self- concept and mental health of college students.\nOverall, self-concept, social adaptation and mental health are closely linked, and social adaptation may act as a link and bridge. Therefore, this study takes college students as the research object, and for three main purposes, one is to explore the relationship among self-concept, social adaptation and mental health; two is to examine the mediating role of social adaptation in self-concept and mental health; three is to reveal the internal mechanism that self-concept affects mental health.\nThis study adopts the method of random cluster sampling, taking the class as a unit. Questionnaire survey was conducted among the undergraduates in a university; subjects filled the questionnaire under unified instructions from staff. Questionnaires were in total 580, and valid questionnaires were 505. Of the participants, 47.1% were males (n = 238); 52.9% were females (n = 270). The ages of the subject range from 17 to 24 (Mean = 21.41, SD = 1.35).\n2.1. China College Student Adjustment Scale\nThis study uses the China College Student Adjustment Scales (CCSA), which has been used as china college student adjustment in the former research  . The scale has 7 dimensions that interpersonal adjustment, learning adjustment, campus life adjustment, job selection adjustment, emotion adjustment, self adjustment, and life satisfaction, total 60 items. It is a 5-point rating scale (1-2-3-4-5), respectively, from the “completely disagree” to “completely agree”. The higher the score, the better the ability of social adaptation. The coefficient of internal consistency of this scale in the present study was 0.883.\n2.2. College Student Self-Concept Questionnaire\nThis study uses the College Student Self-Concept Questionnaire (CSSQ), which has been used as college student self-concept in the former research  . The questionnaire has 23 items, α = 0.716, the retest reliability was 0.785. The questionnaire has good content validity and constructs validity. It is a 5-point rating scale (1-2-3-4-5), respectively, from the “completely disagree” to “completely agree”. The higher the scores, the students’ self-concept is more positive, the level of self-development is more higher. The coefficient of internal consistency of this scale in the present study was 0.883.\n2.3. Development of Chinese College Student Mental Health Scale\nThis study uses the Development of Chinese College Student Mental Health Scale (DCCSMH), which has been used as college student mental health in the former research  . The scale has 12 dimensions and 96 items, measures of somatization, anxiety, depression, low self-esteem, withdrawal, attack, paranoia, compulsion, dependence, impulsivity, psychological disorders, psychotic tendencies and other mental health problems. The reliability of the subscales is between 0.764 - 0.893, which indicates that the scale has good reliability. It is a 5-point rating scale (1-2-3-4-5), respectively, from the “completely disagree” to “completely agree”. The higher the score, the more the mental health problems were severe. The coefficient of internal consistency of this scale in the present study was 0.955.\nTo test the model, data were collected from one university, which adopts the method of random cluster sampling. Before the data collection, all participants was signed informed consent.\n3. Data Analyses\nThe data were analyzed statistically by SPSS 18.0 and AMOS 22.0. The methods of statistical analysis including descriptive statistics, correlation analysis, Bootstrap test.\n3.1. Statistical Test for Common Method Biases\nAccording to previous research  , the study adopts Harman’s single way factor analysis to test common method biases; the result shows that when the research data is not rotating, a total of 47 factor, could explain variance of 68.43%. The first factor explains 17.91% variance, under 40% of estimate criteria. It is shown that the study has not existed serious common method biases.\n3.2. The Correlation Analysis of College Students’ Self-Concept, Social Adaptation and Mental Health\nFrom the correlation analysis as shown in Table 1, except the self-concept and mental health of three factors (anxiety, dependence and impulse) are no significant correlation. self-concept, social adaptation and mental health are significant positive correlation among various factors.\n3.3. The Test of Mediating Effect\nThis research uses Bootstrap test for each mediating effect   . First, using repeat random sampling method (N = 505) extracted from the original data of 1000 samples of the Bootstrap, then according to the samples of fitting mediating effect model (Figure 1). Mediating effect index of each path and the mediation effect of 95% confidence interval (Table 2). If the path coefficient of the 95% confidence interval does not include zero, indicates that the mediation effect is significant. From the Table 2, the relationship\nTable 1. The result of correlation analysis, mean and standard deviation.\nps: 1 = somatization, 2 = anxiety, 3 = depression, 4 = Inferiority, 5 = flinch, 6 = attack, 7 = bigotry, 8 = constrain, 9 = dependency, 10 = impulsion, 11 = psychosexual disorder, 12 = psychotic tendencies, 13 = social adaption, 14 = self-concept; *p < 0.05, **p < 0.01.\nFigure 1. Social adaptation in mediating effect model between self-concept and mental health.\nTable 2. The bootstrap analysis of mediating effect significance test.\n*p < 0.05, **p < 0.01.\nbetween self-concept and mental health of three factors (anxiety, dependence and impulse) are “covered” by social adaptation, which belongs to cover effect   . Others factors, the 95% confidence interval of the path does not contain zero, which social adaptation is verified in the partial mediation effect of self-concept and mental health. The proportion of mediation effect respectively that somatization = 12%, depression = 16%, Inferiority = 14%, flinch = 20%, attack = 8%, bigotry = 11%, constrain = 10%, dependency = 16%, psychosexual disorder = 14%, psychotic tendencies = 9%.\nFrom the correlation analysis, we can see that there is a significant positive correlation between the factors of self-concept, social adaptation and mental health, in addition to self-concept and anxiety, dependence and impulse. The result is consistent with the conclusions of previous studies     , this shows there is a close relationship among self-concept, social adaptation and mental health. It can be seen from the test results of mediating effect that self-concept can not only influence the mental health directly, but also affect the mental health indirectly through the social adaptation. But the relationship between self-concept and the three factors (anxiety, dependence, impulse) are “covered” by social adaptation. This is consistent with the results of the correlation analysis, which shows the influence of self-concept on the mental health of three health factors (anxiety, dependence and impulse) are restricted by social adaptation.\nIn this study, social adaptation plays a mediating role between self-concept and mental health of college students. This shows that self-concept can not only affect mental health directly, but also affect the mental health indirectly through social adaptation. The time of university is the period of development, change and maturity of self-concept. Besides, having a good self-concept and a positive self-acceptance will help students to actively regulate and maintain their own mental health. On the contrary, the negative self-concept will cause a lot of bad social behavior, such as aggression, infraction, crime and psychological problem (depression and neurosis, etc.).   . Self-concept, of course, is not an independent factor that affects mental health. It also needs to through the interaction between social adaptation and environment, and then influence the mental health. Self-concept through a good social adaptation can provide the power and source for mental health, and poor social adaptation will lead to the hidden trouble about the more psychological problems.\nIn short, self-concept as an internal factor, social adaptation as an external factor, the two play crucial role in mental health. Among them, self-concept not only has a directly impact on mental health, but also indirectly impacts on mental health through the social adaptation, namely, the mediating effect, such as inferiority, aggression and paranoia, etc. In addition, social adaptation, in turn, suppresses self-concept, namely, the cover effect, such as anxiety, dependence, and impulse. Therefore, the conclusion of this study has two implications for the prevention and intervention of college students’ mental health: First, self-concept and social adaptation should be meanwhile considered, especially in the college students’ mental health prevention and intervention. Second, enhance and even strengthen the social adaptation of college students, functions of social adaptation as bridge should be fully exerted.\nNaturally, there are still limitations in this study that can be improved in future research. First, this study adopts the cross-sectional study design which could not get the conclusion of causality. Second, this study adopts self-report measurement way, so the social desirability may influence the result. Third, longitudinal studies or experiments could be considered in future research.\nThis work was supported by the Science foundation for Young Scholars of Jiangxi Province (20151BAB215033); the postdoctoral Science Foundation of Jiangxi Province (2014KY52, 2014RC11); the graduate student innovation project of Jiangxi Province (YC2016-S136); and the MOE (Ministry of Education in China) Project of Humanities and Social Sciences (16YJCZH105).\n Byrne, B.M. (1986) Self-Concept/Academic Achievement Relations: An Investigation of Dimensionality, Stability, and Causality. Canadian Journal of Behavioural Science, 18, 173-186.\n Nie, Y.G., Lin, C.D., Peng, Y.S., Ding, L. and Gan, X.Y. (2008) The Development Characteristic of Adolescents’ Social Adaptive Behavior. Acta Psychologica Sinica, 40, 1013-1020.\n Chen, F.X. and Zhang, F.J. (2010) The Characteristics and Relationships of Peer Attachment, Self-Concept and Loneliness of Students in Reform School. Psychological Development & Education, 26, 73-80.\n Li, Y.M. and Li, Y.X. (2015) Adolescents’ Interpersonal Competence, Social Inferiority and Mental Health: The Mediating Role of Social Adaptiveness. Journal of Psychological Science, 38, 109-115.\n Richardson, H.A., Simmering, M.J. and Sturman, M.C. (2009) A Tale of Three Perspectives: Examining Post Hoc Statistical Techniques for Detection and Correction of Common Method Variance. Organizational Research Methods, 12, 762-800.\n Shrout, P.E. and Bolger, N. (2002) Mediation in Experimental and Nonexperimental Studies: New Procedures and Recommendations. Psychological Methods, 7, 422-445.\n Wen, Z., Marsh, H.W. and Hau, K.-T. (2010) Structural Equation Models of Latent Interactions: An Appropriate Standardized Solution and Its Scale-Free Properties. Structural Equation Modeling: A Multidisciplinary Journal, 17, 1-22.\n Marsh, H.W., Parada, R.H. and Ayotte, V. (2004) A Multidimensional Perspective of Relations between Self-Concept (Self Description Questionnaire II) and Adolescent Mental Health (Youth Self-Report). Psychological Assessment, 16, 27-41.\n Kuzucu, Y., Bontempo, D.E., Hofer, S.M., Stallings, M.C. and Piccinin, A.M. (2014) Developmental Change and Time-Specific Variation in Global and Specific Aspects of Self-Concept in Adolescence and Association with Depressive Symptoms. Journal of Early Adolescence, 34, 638-666.', 'TODAY’S STUDY: A Microgrids Primer\nMicrogrids: Expanding Applications, Implementations, and Business Structures\nNadav Enbar, Dean Weng, Ryan Edge, and John Sterling, December 2016 (Smart Electric Power Alliance and Electric Power Research Institute)\nHistorically, microgrids have been employed to provide an additional layer of electricity supply reliability for customers in remote locations with limited access to the grid, or for large institutions managing campus-style energy systems. However, new interest in these systems is now emerging, driven by the changing energy landscape, specifically: nEfforts to modernize the electricity system to more effectively leverage rising penetrations of interconnected distributed energy resources (DERs)\nnDesire to accommodate increased customer choice\nnNeed to provide critical or emergency services, and enable greater grid resiliency in response to more frequent extreme weather events.\nThis report characterizes the latest developments in microgrid deployment, the expanding capabilities of these systems, the business models being used in their deployment, and the obstacles and opportunities that lie ahead.\nBecause the term “microgrid” is often used to refer to a range of distributed energy systems, for this report we use the U.S. Department of Energy (DOE) definition, which is considered the standard for the industry.\nThe DOE defines a microgrid as: “. . . a group of interconnected loads and distributed energy resources (DERs) within clearly defined electrical boundaries that act as a single controllable entity with respect to the grid, and that can connect and disconnect from the grid to enable it to operate in both grid-connected and ‘island’ mode.”\nThe microgrid concept is not new; it is simply a reformulation of local power systems as they were originally designed. Like the traditional, centralized electric grid, microgrids generate, distribute, and regulate the supply of electricity to customers, but do so locally and on a much smaller scale. However, the systems themselves and interest in them have evolved.\nnSuperstorm Sandy and other extreme weather events in recent years have underlined the need for enhanced resiliency and reliability for critical public services—ranging from hospitals to police stations to military bases. Microgrids offer the capability to “island,” or to operate disconnected from the grid, for long periods of time and provide uninterrupted power to mission critical entities or remote rural communities.\nnThe falling cost of solar power and battery storage make these technologies a compelling economic choice for microgrid installations. Additionally, they do not introduce the risk of fuel cost volatility.\nnAdvances in software control systems and the desire to integrate “smart” loads—such as networked thermostats, lighting and occupancy-sensing controls, and building energy management systems—have resulted in improving microgrid control systems. These systems can collectively optimize interconnected DERs to provide a range of energy and grid services.\nMicrogrid Implementation Types and Business Models\nnMicrogrids are customized to their purpose and location—ranging from commercial or industrial, to military, to remote rural systems—thus, the details of their design and construction vary significantly.\nnWhile the different use cases for microgrid installations are multiplying, and individual technologies are not exclusive to any one installation. Most are built to increase reliability and resiliency.\nnUntil recently, a large portion of microgrids have been third-party installations serving a single customer. However, utility-owned microgrids are also being developed, primarily due to statelevel policies and directives, in tandem with technology maturity and expanding applications for these systems. A small number of hybrid, “unbundled” microgrids are also emerging.\nMicrogrid Barriers And Challenges\nAlthough the technology and equipment necessary for creating microgrids are available today, off-the shelf commercial solutions are rare. A number of technical, economic, and regulatory issues must be addressed to unlock the full potential of microgrids. For example:\nnTechnical: Considerable technical challenges exist when toggling a microgrid between gridconnected and islanded modes. For example, during transition to island mode, phase and frequency drift is highly likely, which could cause loads and DERs to trip. Without a finely calibrated synchronization process, grid reconnection could damage generators and loads within the microgrid and in surrounding systems.\nnEconomic and regulatory: Determining standardized methods for valuing microgrids—from either the customer or utility perspective—is difficult due to an intersecting and fluid set of economic and regulatory issues. For example, regulatory and market uncertainties affect the upfront costs and life-cycle economics of microgrids and associated DER technologies.\nA second challenge is that a microgrid’s costs and benefits can be difficult to monetize, or nonmonetizable, thus complicating value stream calculations.\nnStandards:Current technical standards offer guidance for microgrid development, but do not address more nuanced issues germane to system design. For example, further definition is required for protocols governing advanced protection coordination, multilayer-device communications and controls, microgrid-to-grid interactions, and grid resynchronization.\nMicrogrids can be justified across a wide variety of use cases based on a specific set of major drivers.\nBehind these use cases, the ownership and control of the component technologies range along a continuum between the extremes of customer and utility control. Since no two markets or utilities are alike, microgrids will continue to proliferate based on unique served loads, targeted drivers, and deployed technologies.']	['<urn:uuid:829a16ac-0dc4-4e60-b767-c8f81032e1c5>', '<urn:uuid:352824a0-bd81-474e-a97d-4832c6c3a6fb>']	open-ended	direct	concise-and-natural	distant-from-document	multi-aspect	novice	2025-05-12T21:52:51.670698	8	108	3027
37	I'm researching modern military social traditions and I'm curious about how Marines make their formal dinners more entertaining nowadays compared to the past?	Modern mess nights include 'fining' where Marines can accuse other Marines of outlandish and arbitrary crimes ranging from bad fashion to strange habits. These fines vary in severity and punishment, depending on the creativity of the staff, president, and Mr. Vice. This practice offers a more light-hearted atmosphere for attendees to laugh among each other and bond as a band of brothers and sisters.	"['Results 1 to 5 of 5\n12-03-06, 02:02 PM #1\nHas Anyone Attended A Mess Night?Mess Nights: more than just good manners night\nLance Cpl. Nicole A. LaVine\nThe Marine Corps is notorious for its proficiency on the battlefield as well as its dignified manner in formal environments. In the introduction of the 9th Communication Battalion’s mess night manual titled “The History of Mess Night,” such professionalism is addressed:\n“Gallantry and comradeship in arms have enabled our small corps to build a name for itself that is known throughout the world, feared by our enemies and respected by every military service in existence. This name was not easily won, and we of the Corps should not allow ourselves to forget how it was accomplished, nor should we allow others to forget. By that we do not mean we should advertise our war record nor should we even mention it, but to maintain our name we must conduct ourselves at all times in a manner befitting one who is part of such a tradition. The maintenance of traditional discipline, gallantry and love of the Corps is our duty. We must see that the same high standard of respect and prestige is not only maintained but further strengthened. It must be passed on to the future officers who will take our place among the ranks of the Corps and look back with pride on what we have accomplished. The mess night is an element in fostering such standards.”\nThe seemingly-ridged foundation of mess nights is due to its orientation dating back to the Royal British Marines, and even as far back as King Arthur and his Knights of the Round Table. Despite the long history of mess nights, they still hold the same concept of celebration and camaraderie among warriors.\nThe first notable formal U.S. Marine mess nights took place in Washington, D.C., during the 1930’s and 1940’s. It was in this time period that the 20th Commandant of the Marine Corps, Gen. Lemuel C. Shepherd, ordered the publication of the book “The Marine Officer’s Guide,” the first published document of mess etiquette.\nRay V. Wilburn, a retired Sergeant Major, recalls the mess nights he attended while in the Corps.\n“We did everything completely by the book,” said Wilburn, referring to the execution of mess night affairs. “If you had to go to the bathroom, no matter if you were enlisted or an officer, you had to raise your hand and ask the president permission before you could go. Then, you would have to ask permission again before you could re-enter the room.”\nWilburn went on to describe the differences he sees in the mess nights now. Now, there’s been kind of a drifting away from the regulations they had in earlier mess nights. Now, it’s much more of a fun thing for Marines. It’s more relaxed than the controlled environment they had at first.”\nSomething mess nights have now that wasn’t always there is the “fining”. That is where Marines can accuse other Marines at the mess of outlandish and arbitrary crimes ranging from bad fashion to strange habits. These fines vary in severity and punishment, depending on the creativity of the staff, president, and Mr. Vice.\nIt may seem as though the fines are meant to make the attendees uncomfortable, but they are not intended to single-out a Marine. Instead, it offers a more light-hearted atmosphere for attendees to laugh among each other and bond as a band of brothers and sisters.\nMess Nights are a traditional way of giving honor to past Marines and victorious battles which changed the course of our history.\nNo better friend/No worse enemy\n12-03-06, 05:20 PM #2\nOddly enough, in the Marine Corps, none. In the Air National Guard, one dining-in mess night (the super formal kind) and three dining-outs (less formal rules with non-military guests allowed). Plus I attended a mess night at the Warrant Officers and Sergeants Mess for the Windsor Regiment and Essex and Kent Scottish Regiment in Canada (also very formal). Much fun. Big hangovers.\nI had to sing the Marines Hymn at the formal dining-in as a fine for being ""A jarhead infiltrating an Air Force event."" Guilty as charged. But they regretted it as I am a horrible singer.\n12-03-06, 07:57 PM #3\nHe have had three in H&S Company at 8th & I. All three were NCO-only mess nights. Of the three the only one I didn\'t goto was the one during corporal\'s course. They really are fun cause you can talk smack about each other without the Lcpls and below getting any ideas.\n12-04-06, 07:26 AM #4\nSounds gay to me.\n12-04-06, 08:39 AM #5\nI had occasion to perform at many mess nights. There is a certain amount of traditional martial music that is part of the festivities. We would sound ""Attention."" Play ""Ruffles and Flourishes"", and play ""The Roast Beef of Old England"" as the chef wheeled in the food cart, with the meat on it, etc. Usually our portion of the ceremonies would end with ""The Marine\'s Hymn.""\nOnce in a while, the musicians (who are of course enlisted) were invited to stay and party...once the ""smoking lamp is lighted"" and the invitation ""Gentlemen, will you join me at the bar?"" were announced.\nFrom my perspective, they were always a lot of fun.\nUsers Browsing this Thread\nThere are currently 1 users browsing this thread. (0 members and 1 guests)']"	['<urn:uuid:e02abcf9-e931-47b4-bbc0-3e50fe2e86ae>']	factoid	with-premise	verbose-and-natural	distant-from-document	single-doc	expert	2025-05-12T21:52:51.670698	23	64	907
38	night shift workers sleep problems effects	Night shift workers commonly experience serious sleep-related issues. They typically get 5 to 10 hours less sleep per week than other workers, and the sleep they do get is often fitful - a condition known as shift work sleep disorder. This lack of proper sleep makes them especially prone to stress and various health problems, including depression, heart disease, high blood pressure, stomach problems, weakened immune systems, and infertility.	"['Anybody who has ever tossed and turned after a rough day at work knows that stress can make it hard to get a good night\'s sleep. But the connection between sleep and stress is a two-way street. Just as surely as stress interrupts sleep, lack of sleep can be an uncomfortably large source of stress.\nPeople working night shifts could be Exhibit A in the trial of sleep vs. stress. Humans aren\'t meant to be nocturnal, and people who work at night struggle to get enough sleep. As reported in the book Principles and Practice of Sleep Medicine (Elsevier/Saunders, 2005), night workers average five to 10 hours less sleep per week than other workers. The sleep they do get is often fitful. This problem is common enough to have a formal name: shift work sleep disorder.\nWhether you\'re a long-haul trucker driving through the night or a student pulling an all-nighter to finish a paper, the shortfall of deep, restful sleep exacts a heavy toll, both emotionally and physically. As reported in Medscape Neurology and Neurosurgery, people who work night shifts are especially prone to stress, both on and off the job. Not surprisingly, they\'re also vulnerable to a wide range of stress-related conditions, including depression, heart disease, high blood pressure, stomach problems, weakened immune systems, and infertility.\nStress and sleep: The inside story\nNo matter what your age or occupation, a lack of sleep will throw your system off balance. To fully understand the consequences, it helps to take a quick look at the inside story of stress and sleep.\nJust as the Pony Express relied on a series of riders to deliver mail across the Old West, the body uses a chain of chemicals to send messages of stress. First, the brain releases a hormone called CRH. This hormone prompts the pituitary gland to produce ACTH -- the next link in the stress chain reaction. ACTH is the messenger that tells the adrenal gland to release adrenaline and other stress hormones. These are the hormones that make a person feel ""stressed out."" They\'re also the hormones that, over time, can set the stage for stress-related illnesses.\nA good night\'s sleep seems to block this chain reaction. As Stanford stress expert Robert Sapolsky describes in his book Why Zebras Don\'t Get Ulcers (Henry Holt, 2004), there\'s ""decent evidence"" that the same brain chemical that brings on deep sleep also tells the pituitary gland to slow down the production of ACTH. As a result, the adrenal gland never gets the signal to pump out stress hormones, and the body gets a chance to truly rest.\nInterestingly, the part of the brain that houses the stress signal becomes more active during the dreaming (REM) stage of sleep, presumably to help fuel the startling visions that go on in our heads.\nThe toll of tossing and turning\nIf you don\'t get enough deep sleep, you\'re missing a chance to take a break from stress. Losing sleep might even send your levels of stress hormones in the wrong direction. Although research has been mixed so far, one study published in 1997 in the journal Sleep found that sleep deprivation boosted stress hormones the next evening.\nIt doesn\'t take a research study to show that missing sleep can make a person miserable. We\'ve all felt the consequences: drowsiness, irritability, and a fog that just won\'t lift. Stress hormones block storage of short-term memories, which may explain why sleep-deprived people notoriously have trouble holding onto thoughts, much to the chagrin of students who cram all night for a test.\nMissing one night\'s sleep is one thing; struggling with sleeplessness for weeks or months will really turn up the volume on stress. A study of insomniacs published in the Journal of Clinical Endocrinology & Metabolism found that those with the worst sleep produced especially large amounts of ACTH and stress hormones throughout the day and night. The levels of stress hormones -- and, thus, levels of stress -- were highest from afternoon until early night, a time when most people get to wind down.\nPeople with insomnia feel more stressed out than people who have no trouble sleeping at night. As reported in the Journal of Clinical Endocrinology & Metabolism, people who sleep soundly enjoy a daily reprieve from stress hormones. For insomniacs, however, hormone levels stay high all day long.\nWith stress messengers literally coursing through their veins, its no surprise that insomniacs often feel frazzled. A study of 772 men and women of all ages published in the journal Sleep found that people who suffered from insomnia were 17 times more likely than sound sleepers to have anxiety problems.\nSleeping away stress\nIf you\'re feeling stressed out, getting enough sleep should be one of your top priorities. Try to get at least eight hours of sleep every night, even if you feel like you can ""get by"" on less. Of course, putting sleep on your schedule is only the first step. The National Sleep Foundation recommends ""winding down"" for an hour or two before bed -- no catching up on work, no phone calls. Also, you should reserve your bedroom for sleep or sex, not ""waking"" activities such as working or watching TV. Don\'t go to bed unless you actually feel tired. And if you find yourself wide awake between the sheets, get up and do something else until you feel sleepy again.\nFor shift workers, sleep can be especially elusive. The National Sleep Foundation offers special sleep tips for the graveyard shift: Sleep in a dark room (or wear a mask or dark glasses), use earplugs to block out sound, go to bed at the same time every day, and avoid caffeine or alcohol before bedtime.\nGood sleep is worth the effort. Anyway you look at it, rest is better than stress.\nSapolsky, R.M. Why Zebras Dont Get Ulcers. Third Edition. Henry Holt and Company, New York. 2004.\nVgontzas, A.N. et al. Chronic insomnia is associated with nyctohemeral activation of the hypothalamic-pituatary-adrenal axis: clinical implications. Journal of Clinical Endocrinology & Metabolismm. August 2001. 86(8): 3787-3794.\nJohnston, S.L. Societal and workplace consequences of insomnia, sleepiness, and fatigue. Medscape Neurology & Neurosurgery. September, 2005.\nLeproult, R. et al. Sleep loss results in an elevation of cortisol levels the next evening. Sleep. 1997. 20: 865-870.\nTaylor, D.J. et al. Epidemiology of insomnia, depression, and anxiety. Sleep. November 1, 2005. 28(11): 1457-1464.\nNational Sleep Foundation. Sleep and stress.\nMayo Clinic. Insomnia. January 8, 2009.\nMayo Clinic. Insomnia: Lifestyle and home remedies. January 8, 2009. .\nNational Sleep Foundation. Shift work: coping. April 2005.\nNational Sleep Foundation. ABCs of ZZZZ when you can\'t sleep.\nCaltech Health Education. Got sleep?']"	['<urn:uuid:dc0c4492-bcd3-4ba2-9a4f-05b639005b87>']	open-ended	direct	short-search-query	similar-to-document	single-doc	novice	2025-05-12T21:52:51.670698	6	69	1111
39	how cosmetics harm marine animals evidence	There is substantial evidence that cosmetics containing microbeads harm marine animals in multiple ways. Studies have shown that zooplankton and small fish mistake these particles for food due to their size and movement. When consumed, microbeads can affect marine species' mortality, development, reproduction, behavior, cellular response, lifespan, metabolism, and gene expression. They also interfere with photosynthesis in phytoplankton species. Additionally, these plastic particles contain harmful additives like phthalates, heavy metals, and plasticizers that are released into organisms. The problem is so significant that it has led many countries and companies, including L'Oréal, Unilever, and Johnson & Johnson, to ban or phase out microbeads in their products.	"['Could your face wash end up in your sushi? Is your whiter-than-white smile damaging the Great Lakes?\nThe answer to those seemingly bizarre questions is ""yes.""\nA number of personal care products, such as shower gel, face cream and toothpaste, contain tiny plastic particles known as ""microbeads.""\nThe tiny synthetic spheres offer attractive abrasive qualities that enable ""age-defying"" facial scrubs, polished teeth or pretty nails. But when they are flushed down the drain, they pass straight through water treatment systems and into waterways, where they create environmental damage -- or are gobbled up by fish who mistake them for food.\nBeauty products aren\'t the only source of microbeads, which the National Oceanic and Atmospheric Administration defines as any particle 5 millimeters or smaller. They also are created as larger plastic items, such as bottles or containers that found their way into the environment, break down.\nBut eliminating the use of the particles from such products removes one source of the problem, according to NOAA.\n""When microbeads enter the marine environment or Great Lakes, they are considered a form of ""microplastic"" marine debris,"" it said in a recent blog post. ""Plastics never really go away when they\'re in rivers, oceans, or lakes. Instead, they can last decades.""\nAwareness of the problem is increasing, with legislation banning the use of microbeads in personal care products being passed or proposed in Europe, Canada and at least 18 U.S. states. Michigan this week became the latest state to consider such a ban.\n""Plastics have come to play a crucial role in modern life, but the environmental impacts of the way we use them cannot be ignored,"" says Achim Steiner, under-secretary-general of the United Nations Environment Program, citing a study that estimates plastic waste causes $13 billion a year in environmental damage. ""Scientists have recently found tiny pieces of plastic trapped in sea ice. These contaminated particles eventually become a source of chemicals in our food. The key course of action is to prevent plastic debris from entering the environment in the first place.""\nThe personal care industry is searching for solutions. L\'Oréal, Unilever, Colgate-Palmolive, Procter & Gamble, Johnson & Johnson, and Crest have eliminated or have promised to get rid of microbeads in their products. But some companies and concerns have pushed back on proposed legislation, arguing that anti-microbead laws would ""stifle innovation.""\nMany smaller natural or organic personal care companies use alternatives to microbeads.\n""There are already so many naturally great ways to exfoliate -- we use elements like sea salt or lava for our soaks and scrubs,"" said Stephanie Lawrence, co-founder of Mer-Sea & Co., a home fragrance and body care company.\nPolly Fox, European brand manager for Siam Botanicals, which specializes in plant-based beauty products, said there also are health concerns about microbeads.\n""Do the chemicals on these plastic beads get absorbed into the bloodstream and tissue of the animals that eat them? And does this have implications for seafood-eating humans?"" she said. ""Scientists are investigating, but we don\'t yet have the answers. If you feel strongly about this issue, stop using your marine-life-polluting products - and don\'t forget to let the manufacturer know that you\'ve done so. There are lots of natural, biodegradable alternatives out there, such as jojoba wax beads.""\nTo find out if your favorite product contains plastic microbeads, you can download the Beat the Microbead app and scan the barcode.', 'Discussing the effects of microplastics on our environment it is important to understand that microplastic particles from rinse-off and leave-on cosmetics usually end up in sewage, microplastics manage to escape from the wastewater treatment systems due to their small size. The separation of microplastics from other particles is difficult and limited. Mainly in underdeveloped and developing countries, according to (European Chemicals Agency, 2019) more than around 87% of the microplastics end up in biologically active sludge and aquatic environments (European Chemicals Agency, 2019), (Yurtsever, 2019b, 2019a), (Hidayaturrahman and Lee, 2019), (Li et al., 2020).\nThere are a lot of studies conducted which demonstrate the presence of microplastic particles literally everywhere: in the air we breathe, in bottled and tap water, in foodstuff like salt, in home dust and almost everywhere in the aquatic environment (surface, mildwater, benthos), in freshwater systems. Furthermore, it was detected that there are microplastic particles in all forms of marine species: from tiny organisms to predators.\nAccording to (Haegerbaeumer et al., 2019) and (Xu et al., 2020), there is proven effect of microplastics on our environment and life processes like mortality, development, reproduction, behaviour, cellular response, lifespan, egestion, regeneration, air and food consumption, even on metabolism, gene expression, neurotoxicity and carcinogenicity of aquatic species.\nDid you know that resembling the movement and the size of food, microplastics can be consumed by mistakes by zooplankton and small fish. Microplastics can be absorbed by phytoplankton and filter feeders; and by mistake by larger organisms ending up in the intestine. There are issues with photosynthesis function in phytoplankton species. Thus, microplastics are putting the life of microorganisms at a high risk which leads to the problems of the trophic chain.\nIndirect effects of microplastics should be also mentioned here, as they may contain additional harmful additives like phthalates, pigments or heavy metals, plasticizers, alkylphenols for functionality and conservation purposes. However, afterwards, these additives are released to the environment. Also, there are studies on the interaction of microplastics with inorganic pollutants (Xu et al., 2020). These pollutants and harmful additives may be released into the organisms, it happens more easily with small plastic nanoparticles. Nevertheless, detailed research on toxicity, and effects of plastic nanoparticles on the environment and organisms is needed in order to make specific conclusions.\nMoreover, in terms of humans themselves, further studies are needed in order to ensure toxicity of microplastics on the human organism. However, recent findings have shown the presence of microplastics in the human faces (Yan et al., 2020; Zhang et al., 2020). We inhale, swallow and consume microplastics in the direct way through air, water, and seafood.\nThere are significant gaps in analysis and scientific knowledge on the effects of micro- and nanoplastics on human organisms. A lot of researchers are worried about possible harmful effects of nanoplastics, as they are more likely to be absorbed and distributed in our organisms.\nActions against plastic microbeads\nHaving identified possible effects of microplastics on the environment, it can be said that microplastics may pose a greater risk than macroplastics. Therefore, the policies on exclusion of intentionally added microplastics from products (e.g. microbeads from cosmetic products) started by particular countries. In 2012, there started a global pressure against microbeads in cosmetics by NGOs, scientific findings, and intergovernmental organizations. The topic was thoroughly analyzed by environmental consultants and scientific institutes indicating that microbeads are an unnecessary source of plastic pollution.\nThus, in 2014 the state of Illinois was the first to ban non-biodegradable microbeads from personal care products, then 15 States followed this ban. Therefore, “Microbeads Free – Water Act” was introduced by the U.S. prohibiting manufacture (from July 2017) and sales (from July 2018) of intentionally added non-biodegradable plastic microbeads in rinse-off personal care products. Other countries also took actions, for instance, Canada followed the ban prohibiting the manufacture, import, or sale of toiletries containing microbeads.\nIn October 2019, China also introduced a ban to stop the production of household chemical products which contain plastic microbeads as of the 31th of December 2020, the full-scale ban will be implemented in December 2022.\nNon-European countries like New Zealand, South Korea and the Taiwan province have already banned microbeads usage in rinse-off cosmetics. Iceland joined the ban and signed a commitment. India is considering future restrictions. In Brazil the law was introduced in 2016 and approved in 2019, but has not been accepted as a public law yet.\nIn 2015 two out of six states of Australia signed an agreement to phase out microbeads.\nSouth Africa’s government is currently in discussion and consultation with the cosmetic industry for a microbeads’ phase-out in cosmetic products.\nRegarding the European Union, in 2013 Denmark was the first country to propose a European-wide ban on microbeads. A new directive 2014/893/EU published by the European Commission in 2014 prohibited rinse-off cosmetics that contain plastic microbeads to have the Ecolable sign. Therefore, the Netherlands, Austria, Luxemburg, Belgium, and Sweden joined the European ban on microbeads in personal care products. However. The European ban was not the straightforward policy, as it is in the USA and Canada due to the different EU’s structure.\nNordic countries were the pioneers of promoting the microplastics ban in cosmetics. In 2017, a full ban of microplastics in cosmetics was proposed by the Nordic council. In 2018, Sweden added solid plastic particles in cosmetics in the list of prohibited particles. Finnish government had to exclude microbeads in rinse-off cosmetics by June 2020. Norway being a niot full EU member, has prohibited personal care products containing microplastics to bare the Swan label (eco-friendly Nordic product), and the country is considering a full ban. Belgium signed a phase-out agreement with cosmetic industries which had to be implemented by the end of 2019.\nFrance aims to ensure water quality justifying the prohibition of microbeads by using the European Marine Strategy Framework Directive. In 2018, the national law banning microbeads in rinse-off cosmetics was introduced in France. Starting as of 2020, Italy joined the same restriction of microbeads usage in exfoliating and cleansing rinse-off cosmetics.\nThe UK has preferred to join a European-wide ban introducing the ban into the national legislation, UK’s prohibition is one of the strongest and most comprehensive.\nHowever, the microbeads (from 1 nm to 5 mm) phase-out in exfoliating and cleansing rinse-off cosmetics does not seem to be enough and does not lead to a risk reduction. The European Chemical Agency proposed a restriction of all microplastics (other than microbeads) that are added to rinse-off cosmetics to be initiated 4 years after the ban becomes effective and for leave-on cosmetics 6 years respectively.\nTaking everything into consideration, alternative solutions should be developed. BIOWEG is aiming to fully replace plastic microbeads in cosmetic and personal care products offering biodegradable, 100% bio-based microbeads to cosmetic industries.\nKeep following us on social media channels to know more.\nSource: (Anagnosti et al, 2021).\nSource: Anagnosti, L., Varvaresou, A., Pavlou, P., Protopapa, E., & Carayanni, V. (2021). Worldwide actions against plastic pollution from microbeads and microplastics in cosmetics focusing on European policies. Has the issue been handled effectively?. Marine Pollution Bulletin, 162, 111883. doi: 10.1016/j.marpolbul.2020.111883']"	['<urn:uuid:a7153b33-fd95-4f72-bc2d-ba6b7746a271>', '<urn:uuid:2f4165b1-af6c-40c1-8aed-386b48b0aa61>']	open-ended	direct	short-search-query	distant-from-document	three-doc	novice	2025-05-12T21:52:51.670698	6	106	1743
40	how big pompeii city conservation efforts	Pompeii was a thriving city with a population of 11,000-15,000 people, featuring water systems, an amphitheater, gymnasium, and a valuable port. Today, its preservation poses significant challenges - the exposed walls with lavish decorations are visibly disintegrating. Researchers are now implementing conservation measures including drainage systems, protective structures, and preventive restoration using simple, traditional materials, avoiding modern materials like concrete.	['The city of Pompeii was once a bustling city on a hill just across from the modern city of Naples. Along with Herculaneum and many small villages in the surrounding area, Pompeii was destroyed by Mt Vesuvius on the 24th of August in 79AD. Buried under 13-20 feet (4-6m) of volcanic ash and pumice, Pompeii was so quickly covered that it was incredibly well preserved. The eruption destroyed the city, killing thousands of its inhabitants and burying it under tons of ash. For almost 1,500 years Pompeii was undiscovered and because there was.\nBelieved to have been founded in the late sixth or early seventh century BC by the Oscans, Greeks settled in the area in the eighth century BC, and the region attracted wealthy vacationers interested in soaking up the sun and seeing the sights. Pompeii did not become a domination of Rome till the fourth century BC, and it was not till 80 BC that it became a Roman colony. After a short 160 years, with a population of between 11,000-15,000 it was a thriving city with water systems, an amphitheater, gymnasium, and a highly valuable port.\nRediscovered in 1748 by Spanish engineer Rocque Juaquin de Alcubierre, the objects covered by ash were remarkably well preserved. Thanks to the lack of air and moisture, these artifacts have been able to provide detailed insights into the life of an ancient city. During the excavation, plaster was used to fill voids in the ash layers that once held human bodies. These plaster casts have allowed archaeologists to see the exact position people were in when the perished.\nWhile excavation of the site has given the world innumerable value, weathering began almost as soon as the site was uncovered. Subject to natural and man-made forces, Pompeii is rapidly weathering and it is thought that further protection may have to be instituted by UNESCO to protect the site for future generations.\nA popular tourist destination for more than 250 years and one of the most popular sights in all of Italy, Pompeii was declared a World Heritage Site by UNESCO in 1997. Vesuvius National Park includes Herculaneum, Pompeii, and many smaller sites covered by the ash of Mt Vesuvius.\n- Of an estimated population of 15,000 people, almost 2,000 died during the eruption of Mt Vesuvius.\n- The people of Pompeii didn’t even know Mt Vesuvius was a volcano since it hadn’t erupted in almost 2,000 years.\n- The rich volcanic soil of previous eruptions of Mt Vesuvius ironically made the site perfect for farming.\n- Mount Vesuvius is the only active volcano on mainland Europe and is considered one of the most dangerous volcanoes in the world.\n- Mt Vesuvius erupted just after noon on the 24th of august in 79 AD, completely destroying the city.\n- The eruption of Mt Vesuvius that destroyed Pompeii is estimated to have lasted more than 24 hours.\n- The remains of the city were not discovered till 1748.\n- The Catastrophic eruption on the 24th, August 79 AD was said to have lasted more than 24 hours.\n- The ruins of Pompeii are visited by more than 2.5 million people each year.\n- The Amphitheatre of Pompeii, built in 80 BC is the oldest stone building of its kind known to mankind.\n- Because of deterioration, Pompeii was included in the 1996 World Monuments Watch by the World Monuments Fund.\n- Pompeii was a place where mostly wealthy people lived and Nero, the Roman Emperor had a holiday home near Pompeii. Nero’s wife Poppaea Sabina was a native on Pompeii.\n- Use a licensed guide. There will be lots of people claiming to be the best just about everywhere you look. The licensed guides will have the best and most accurate information, and they will not be as likely to overcharge for the experience.\n- Use the restroom before you go. There are not many (if any) restrooms within the ruins of Pompeii. There is a chance that you will cut your tour short if you drink too much water.\n- Drink lots of water. Even though it may cut your tour early (for lack of a restroom), it can get hot in the summer and dehydration is a serious vacation destroyer.\n- Take comfortable shoes. Pompeii is most likely going to be bigger than you think it is. Spending 3-4 hours walking around will still leave you with lots haven’t seen.\n- Assume anything you see costs money. There are maps and information packets in the visitors center and they almost always cost.\n- Take the train. There are taxi cabs that will offer to get you there faster and cheaper than the train. While it may be faster in the morning, it will not be in the afternoon or evening, and it will never be cheaper. The train is also extraordinarily easy to buy tickets for and use.\n- If you see someone dressed in period costumes it will cost a little money to take your picture with them. For that matter, it will cost a little money to take a picture of them.', 'Modern buildings are designed to have a lifespan of around 50 years. But in historical terms, that is a mere blink of an eye. We would like archeological sites like Pompeii, for example, to stand the test of time immemorial. Preserving sites such as this with the most basic materials represents a huge scientific challenge. As part of the “Pompeii Sustainable Preservation Project”, researchers from Technische Universität München (TUM), Fraunhofer-Gesellschaft and ICCROM will spend the next ten years investigating long-term solutions to prevent the UNESCO world heritage site of Pompeii from falling further into ruin.\nAlmost two thousand years ago, the city of Pompeii was buried under a shroud of ash and lava ejected by nearby Vesuvius. The eruption in 79 AD essentially froze the ancient city, preserving it for centuries. Large-scale excavations did not begin until the 18th century. Bit by bit, the city began to reveal its secrets about life in ancient times.\nAs one of the largest self-contained sites surviving from antiquity, Pompeii is a treasure trove. Each new excavation yields new knowledge, and is greeted with huge interest by the public and research community. All too often however, a lot less interest has been shown in the sustainable preservation of this unique site.\nMany of the finds, most notably Pompeii’s frescos, have been moved to museums, to protect them from the wind and weathering. But because of inadequate conservation measures, the exposed walls of the city with their lavish decorations are now visibly disintegrating.\nPompeii as a center of world-class research\nThe researchers participating in the Pompeii Sustainable Preservation Project intend to concentrate on one of Pompeii’s apartment buildings, known as an insula. From 2014, they will embark on an ambitious conservation program, taking in everything from elaborate murals to the smallest wall. “The first step will be drainage, followed by new types of protective structures. But that is just the start,” explains Professor Erwin Emmerling of TUM’s Chair of Restoration.\nAn important new approach is preventive restoration. “To date, this has not been undertaken on an adequate scale. We want to find out more about ongoing restoration,” continues Emmerling. The researchers will only use simple, traditional materials. In any case, large equipment like cranes would be of no use in the narrow streets of Pompeii. They will also have to make do without concrete because it was not used in those days. Instead, the restoration team will use lime and other traditional building materials.\nModern technology for ancient monuments\nBut the researchers will not be foregoing all high-tech aids. They will use nanotechnology to make the lime more fluid, thus stabilizing the frescos through backfilling. The experts intend to conserve the topmost layer of the paintings using lime and silicon compounds.\nResearchers from various disciplines will be working alongside restoration experts and archeologists in the Pompeii Sustainable Preservation Project. The ancient city will be accurately surveyed both on the ground and through aerial photographs. Seismic measurements will provide information on how the monument will be impacted by future seismic activity, which will help to ensure that the conserved structures will later withstand these tremors. Construction and structural engineers will be supporting activities in this area.\nLast but not least, suitable sites within Pompeii are to be re-landscaped, and the project as a whole will be a training site for conservationists from around the world.\nThe key partners in the Pompeii Sustainable Preservation Project are Technische Universität München (Chair of Restoration, Art Technology and Science of Conservation), Fraunhofer-Gesellschaft (Fraunhofer Institute for Building Physics), and the International Center for the Study of the Preservation and Restoration of Cultural Property (ICCROM), which is attached to UNESCO. These institutions will be assisted by the Soprintendenza Speciale per i Beni Archeologici di Napoli e Pompei and the Istituto Superiore per la Conservazione ed il Restauro, which is a body of the Italian Ministry of Cultural Heritage. The University of Oxford’s School of Geography and the Environment, the Department of Ancient History at Ludwig Maximilian University in Munich, the German Archeological Institute (DAI) in Rome, the University of Pisa and the Istituto per i Beni Archeologici e Monumentali of the Consiglio Nazionale delle Ricerche (CNR) are supporting the project as research partners.\nProf. Erwin Emmerling\nTechnische Universität München\nLehrstuhl für Restaurierung, Kunsttechnologie und Konservierungswissenschaft']	['<urn:uuid:1ac1a6ac-5ede-44f6-8458-dd0d78470886>', '<urn:uuid:8ce8cd42-3a55-4275-a07a-660d16098ec2>']	factoid	with-premise	short-search-query	distant-from-document	multi-aspect	novice	2025-05-12T21:52:51.670698	6	60	1565
41	flour enzyme process texture relation	The relationship between flour enzymes and bread texture manifests differently across bread types. In sprouted wheat flour, the enhanced enzyme activity from sprouting breaks down complex carbs into simpler sugars, creating better flavor but requiring careful management to avoid gummy texture - hence the need to skip pre-ferments and avoid lengthy fermentation. For Brazilian pão francês, enzymes are deliberately added to standardize flour performance, including amylases, xylanases, hemicellulases along with emulsifiers like polysorbate. Too much enzymatic activity can cause dark crusts and soft crumbs, while too little can result in pale crusts and insufficient crispiness. The proper balance of enzyme activity is crucial for achieving the desired texture characteristics in both bread types.	['How to Bake With Sprouted Wheat Flour\nOne of our goals this year is to bake with healthier flours. Happily, this carries none of the self-deprivation, lackluster flavor or increased effort that often accompany other dietary reforms, because sprouted whole wheat flour tastes better than standard whole wheat and is easier and faster to work with..\nSprouted Whole Wheat Flour is Super Healthy\nThis flour is made from wheat kernels that have been allowed to sprout, beginning the process of a seed growing into a plant. After sprouting, the kernels are dried and ground into a lovely, soft whole grain flour, which is often organic. Why is sprouting desirable? Sprouting releases vitamins, activates enzymes and makes grains more digestible, particularly for those with mild wheat sensitivities.\nThe sprouting process reduces phytic acid, a naturally-occurring component of the bran and outer layers of many grains. Phytic acid is problematic because it prevents phosphorous and minerals like calcium, magnesium, zinc and iron from being absorbed. Sprouting is a way to increase the bio-availability of whole wheat’s nutrition, and to reduce the amount of minerals used by the body to process whole grains\nSprouted Wheat Flour is Super Delicious\nAn added benefit of sprouted flour’s enhanced enzyme activity is better flavor. Enzymes facilitate the breakdown of some of wheat’s complex carbs into simpler, sweeter sugars. As a result, the flour tastes sweet and mild, with none of the bitterness of traditional whole wheat. It’s the perfect choice for our healthy remake of classic Brioche, below.\nSprouted whole wheat flour is available in the U.S. from Arrowhead Mills and King Arthur Flour.\nGeneral Tips for Baking Bread with Sprouted Wheat Flour\n1. Skip the pre-ferment. Because sprouted wheat flour has rich enzyme content and sweet flavor, no pre-ferment or lengthy autolyse is necessary. Eliminating these steps simplifies bread making and helps avoid a gummy texture that can sometimes result from too much enzyme activity.\n2. Avoid lengthy fermentation. This is partly due to the need to avoid too much enzyme activity, and also because of the delicate nature of the gluten structure in whole wheat. We have had good luck rising sprouted wheat flour breads to 1.75x to 2x volume, being careful not to allow the dough to go past that. Similarly, the final shaped proof often goes quickly. We start checking the bread after 60 minutes at 85F/ 29C.\n3. Add enough water. Whole grain flours are thirsty, and often benefit from a soft, slightly sticky dough consistency. This helps ensure adequate hydration of the grain and also promotes a moist, light texture.\n4. Knead a few extra minutes. Bran particles in whole wheat flours interfere with gluten bonding, so additional kneading/folding and incorporating some strong white flour can help boost structure.\n6. Use a warm rise. Long, cold ferments may allow enzymes to degrade the structure or create a gummy texture. We like to use a warm rise of about 85F/ 29C.\nSprouted Wheat Flour Recipes:\nOrange-Apricot Better Brioche\nWalnut Wheat Couronne Bordelaise', 'Brazilian cuisine is a reflection of the country’s multicultural population. Depending on the region, eating habits have been shaped by indigenous, European, African or Creole influences. But different as culinary tastes may be among the 212 million inhabitants, there is surprising agreement when it comes to a particular bakery product: Brazilians love pão francês — a light-colored, yeast-raised wheat roll characterized by a tender, flaky crust and a fluffy crumb.\nThese cylindrical rolls are a central feature of meals, especially breakfast. Most Brazilians start the day with cafézino, a strong, sweet espresso, and a crispy pão francês that they spread just with butter or margarine. Those who prefer a more substantial meal fry the halved rolls in a pan with butter to make “pão na chapa.”\nFor midday lunch, the roll is served with soups, stews or salads. And in the evening, with a circle of friends in a restaurant, the Brazilians like to dip into the breadbasket and soak up the sauces from the lavish meat and fish dishes with pão francês.\nMost important sales driver\nBecause freshness has top priority in the purchase of rolls, private households and restaurant owners mostly buy what they need from a total of some 64,000 small- and medium-sized artisan bakeries that are a familiar part of the street scene in Brazil. For these businesses, pão francês is generally the main sales driver and the most important item in their product range.\nCustomers’ expectations in respect of their favorite rolls are clearly defined: the pão francês must have a light, fluffy crumb, a tender, flaky, wafer-thin crust with a pale brown color, a mild taste and a wide split. The weight is between 50 and 60 grams.\nSince the cylindrical rolls are baked close together in the oven, their ends meet as the dough rises. After cooling, these “snakes” of bread are broken apart at the joints to reveal the white, fine-textured crumb. This specific appearance is another feature consumers expect.\nQuality reference for pão francês\nMany bakeries advertise their French bread as the hallmark of their trade. Nevertheless, problems with quality and standardization often occur, sometimes because of a shortage of qualified workers.\nTo offer the industry practical help in improving its production methods, the Brazilian standardization committee ABNT (Associação Brasileira de Normas Técnicas) has developed recommendations for the production of pão francês. Since 2013, the technical standard “ABNT NBR 16170” has specified criteria for evaluating and classifying French bread. Shape, size, properties of the crust, color, crumb structure, shred, odor, flavor… every detail of a perfect pão francês is described precisely in these guidelines. Although the standard is not binding, the initiators hope it will help to ensure that “French rolls” are of uniform, defined quality in all corners of the country.\nPolysorbate as an emulsifier\nIf the rolls are to be light and fluffy inside and have a thin, crisp crust, the flour used must guarantee suitable baking performance. To reconcile quality with economy in the grinding process, the mills generally combine cheaper domestic wheat lots with higher-performance imported wheat. In the past it was the Mercosur countries Argentina, Uruguay and Paraguay that played the most important role, but countries like Russia, Ukraine and other Black Sea countries also have become established trading partners.\nTo standardize flours made up of very different wheat varieties and regions of origin, it is established practice in Brazil to treat the flour with enzymes and additives. The products used include oxidizing agents, amylases, xylanases, hemicellulases and emulsifying additives like DATEM and SSL. A peculiarity of Brazilian food law is the approval of polysorbate, a high-performance emulsifier that has a particularly strong effect on the gas retention capacity and stability of a dough. The additive azodicarbonamide (ADA) is also a much-used improver in Brazil. However, in comparison with other South American countries that permit a dosage of up to 45 ppm, the maximum quantity allowed in Brazil is lower, at 40 ppm.\nEnsuring firm doughs\nBesides a suitable flour, the making-up process for the dough is an important factor for the quality of pão francês. Even at the smallest artisan bakeries it is usual to have a special mechanical roll shaper to replace laborious manual rounding and molding.\nAfter mixing and kneading, the dough is left to ferment only for a short time before processing continues mechanically. On a feeder conveyor belt, the dough pieces are pressed automatically and pass through various roller systems with decreasing gap widths. This results in thin, portioned sheets of dough that are rolled up tightly by several deflectors.\nThe bakers only have to intervene to give the rolled dough portions a finishing touch to their appearance. The loaded proofing trays are then placed in the proofing chamber at a temperature of about 30°C and a relative humidity of 80%.\nThe “French mini-loaves” are baked for about 20 minutes at 200°C to 220°C after being scored lengthwise to create the split.\nFor many Brazilian consumers, the price and freshness of the rolls are still the most important criteria for buying. But generally speaking, the bakeries are noticing increasing expectations on the part of their customers. Fewer consumers are willing to tolerate fluctuations in appearance or taste. On the contrary, there is a demand for high and consistent product quality. Since it is sometimes only necessary to make small adjustments to the recipe or production method to enhance the appearance and sensory properties of pão francês, here is an overview of the most common mistakes made in production, with suggestions for avoiding them:\nProblem: Low bread volume.\nPossible causes: Fermentation time too short; too little yeast; unsuitable flour with too little enzymatic activity.\nSolutions: Prolong the fermentation time of the dough; use more yeast; use suitable flour; increase or adjust flour treatment (Alphamalt VC 5000, Alphamalt HCC 2).\nProblem: Inadequate shred/bloom.\nPossible causes: Fermentation time too long; dough temperature too high; flour too weak; too little enzyme activity.\nSolutions: Shorten the fermentation time; use cold water; use more steam in the oven; increase humidity in the fermentation chamber; use more oxidizing agent (OXEM, Elco P-100); add more enzyme improvers (Alphamalt EFX Mega, Alphamalt H 19480).\nProblem: Dark crust, soft crumb.\nPossible causes: Enzymatic activity too high; oven too hot; baking time too long.\nSolutions: Use wheat flour with lower enzymatic activity; add less alpha-amylase (Alphamalt VC 5000) to the flour; set the oven to a lower temperature; shorten the baking time.\nProblem: Holes in the crumb.\nPossible causes: Flour too weak; doughs too warm; fermentation chamber too warm; too much yeast; mixing and resting times too long.\nSolutions: Use stronger flour; use cold water; set a lower temperature in the fermentation chamber; use less yeast, shorten processing times; adjust flour treatment (use Mulgaprime 10, Alphamalt Gloxy TGO, Alphamalt EFX Mega).\nProblem: Not enough crispiness.\nPossible causes: Dough too firm; flour too weak; baking temperature too low; doughs too dry and bucky; too little enzymatic activity.\nSolutions: Use stronger flour; set the oven to a higher temperature; increase the amount of liquid in the recipe; adjust the enzymatic activity (Increase the flour treatment (Alphamalt GA 23750, Deltamalt FN, Mulgaprime 10).\nProblem: Pale, white crust.\nPossible causes: Fermentation chamber too dry; fermentation time too long; oven temperature too low; baking time too short; too little enzymatic activity.\nSolutions: Increase humidity in the fermentation chamber; reduce the fermentation time; set the oven to a higher temperature; prolong the baking time; add more alpha-amylase to the flour (Alphamalt VC 5000, Alphamalt GA 23750, Deltamalt FN).\nProblem: Blistered crust.\nPossible causes: Flour too weak; incorrect machine setting; fermentation time too long; doughs too soft; fermentation chamber too warm.\nSolutions: Use stronger flour; increase the pressure of the dough molding machine; shorten the fermentation time; reduce the amount of liquid in the recipe; reduce the temperature in the fermentation chamber.\nA Brazilian product with a hint of France\nNo Brazilian consumer expects a traditional French baguette when buying pão francês. The misleading name, which means “French bread,” is thought to have originated in the early 20th century, when the upper classes of Brazil took pleasure in visiting far-off destinations in Europe, especially France. Upon their return, the travelers gave their bakers rapturous descriptions of the delicious bread they had enjoyed in Paris. The typical loaves in France at that time were short and cylindrical, with a firm crumb and a golden-brown crust. They were forerunners of the baguette, which did not acquire its characteristic elongated shape until later. Following these descriptions, Brazilian bakers tried to imitate this European speciality. What emerged was a new bakery product in its own right; it was destined to become an unprecedented bestseller, known as “pão de sal” (salt bread), “cacetinho” (stick bread), “pão careca“ (bare bread) or “filão” (snake bread), according to the region.\nSven Mattutat is a product manager with Mühlenchemie. He may be contacted at firstname.lastname@example.org.']	['<urn:uuid:8d4af816-1e35-438f-b87a-e4cf80a1d23a>', '<urn:uuid:308dd7d3-f7a4-4e07-ad09-f6e9a6a06d4d>']	open-ended	direct	short-search-query	distant-from-document	multi-aspect	expert	2025-05-12T21:52:51.670698	5	113	1978
42	importance of conservation areas for wildlife preservation and main dangers to reef ecosystem	Conservation areas like Important Bird Areas (IBAs) are vital for saving up to 90% of African bird species affected by climate change. These protected areas provide new habitats and help species adapt as temperatures and rainfall change. As for reef ecosystems, the most pervasive local threat is unsustainable fishing, affecting over 55% of world's reefs through overfishing and destructive fishing methods. Additionally, both systems face threats from human development - IBAs lack legal protection in over 40% of areas, while coastal development linked to human settlements affects reefs through direct damage and increased pollution.	"['Nature parks can save species as climate changes\nRetaining a network of wildlife conservation areas is vital in helping to save up to 90 per cent of bird species in Africa affected by climate change, according to scientists.\nThe research team - led by Durham University - including BirdLife International and the RSPB (BirdLife in the UK) looked at the effects of climate change on 815 bird species of conservation concern in sub-Saharan Africa and on the network of sites designated for them (termed Important Bird Areas).\nPublished in the journal Ecology Letters, the research - funded by the RSPB - demonstrates that a network of wildlife areas will be a crucial tool to help biodiversity survive future climate change. The findings suggest an urgent need for legislators to protect eco-systems and key wildlife areas in Africa. They show that, over the next 75 years, the biodiversity of some regions will suffer more than others as a result of climate change. They also underline the importance of providing \'green corridors\' to help wildlife to move to find new climatically-suitable areas.\nThe team led by Dr Stephen Willis and Dr David Hole from the School of Biological and Biomedical Sciences at Durham University, used simulation models to see how climate change might affect birds in Important Bird Areas, in the coming decades under a scenario of moderate climate change.\nThe researchers looked at a network of 863 IBA sites across 42 countries and territories covering around 2,079,306 square kms (1,292,020 square miles) or 7 per cent of the African continent. The sites are identified as being critical for the conservation of birds, in particular, species that are globally threatened, restricted in range or restricted to particular biomes. Together, African IBAs are home to 875 of these species.\nClimate change is not the only issue affecting wildlife in Africa. More than 40 per cent of African IBAs lack any form of legal protection under national or international law. Agricultural development, logging, invasive alien species, and unsustainable hunting and trapping are the main threats to bird species and IBAs across the African continent.\nDr Stephen Willis said: ""We looked at bird species across the whole network of protected areas in Africa and the results show that wildlife conservation areas will be essential for the future survival of many species of birds.\n""Important Bird Areas will provide new habitats for birds that are forced to move as temperatures and rainfall change and food sources become scarce in the areas where they currently occur. Protected areas are a vital conservation tool to help birds adapt to climate change in the 21st century.""\nThe findings show that the biodiversity of particular areas is likely to change significantly. The turnover of species in some sites could be as high as 50 per cent, as established species leave to find more suitable climes or new food supplies, and new species move in to an area. The adaptability of birds will be an important factor, the experts say.\nDr Stephen Willis said: ""The results show that 90 per cent of priority species in Africa will find suitable climate somewhere in the network of protected areas in future. However, one in ten birds will have to find new places to live and breed so new sites will have to be added to the IBA network.\n""The central regions of Africa should maintain many of their current species as long as the protected areas remain intact. By contrast, areas of the Afrotropical Highlands, which occur in countries such as Cameroon, South Africa and Ethiopia, will see enormous change with more than 40 per cent of species leaving.""\nThe findings also show that some species are likely to struggle, and may even become extinct unless new populations can be established. A priority species might be lost from a particular IBA, but there may be other climatically suitable sites in the network for the species to move to. Many species will only survive if they adapt by moving across Africa to seek out new, climatically-suitable areas to inhabit.\nDr Stuart Butchart, Global Research Coordinator at BirdLife International, said: ""The survival of much of the planet\'s biodiversity under climate change will depend upon adequate protection for biodiverse ecosystems, the IBAs within them, and support for the people who depend on them - so that local communities can participate actively in making their environment more resilient. It is essential that policy leads to adequate protection of IBAs and takes account of the critical role that ecosystems play in helping wildlife and people adapt.""\nRuth Davis, head of climate change at the RSPB, said: ""Looking after IBAs is vital for the future of our wildlife. Protecting the natural resources and services provided by these ecosystems is vital for people too. Healthy ecosystems are the first line of defence against the impacts of climate change for many of the world\'s poorest people.""\nOne example, is the Gola Forest Transboundary Peace Park, on the borders of Sierra Leone and Liberia, uniting existing protected IBAs and encompassing additional forest to provide corridors for the movement of wildlife between them. It protects one of the largest remaining blocks of intact forest in the Upper Guinea Area of West Africa.\nSource: Durham University', ""Beyond threats associated with climate and ocean change, coral reefs are also affected by various local and regional threats. These threats may occur alone or synergistically with climate change adding to the risks to coral reef systems.\nOverfishing and Destructive Fishing\nUnsustainable fishing has been identified as the most pervasive of all local threats to coral reefs. ref Over 55% of the world’s reefs are threatened by overfishing and/or destructive fishing. Overfishing (i.e., catching more fish than the system can support) leads to declines in fish populations, ecosystem-wide impacts, and impacts on dependent human communities. Destructive fishing is associated with some types of fishing methods including dynamite, gill nets, and beach seines. These harm coral reefs not just through physical impacts but also through by-catch and mortality of non-target species including juveniles. Read more about threats and management strategies in the Reef Fisheries Toolkit.\nTraditionally, impacts from wastewater pollution have been associated with human health, but the detrimental effects of wastewater pollution on marine life – and the indirect impacts they have on people – cannot be overlooked. Wastewater transports pathogens, nutrients, contaminants, and solids into the ocean that can cause coral bleaching and disease and mortality for coral, fish, and shellfish. Wastewater pollution can also alter ocean temperature, pH, salinity, and oxygen levels disrupting biological processes and physical environments essential to marine life.\nOther sources of pollution to coral reef waters include land-based pollution associated with human activities such as agriculture, mining and coastal development leading to the discharge or leaching of harmful sediments, pollutants, and nutrients. Marine-based pollution associated with commercial, recreational, and passenger vessels can also threaten reefs by discharging contaminated bilge water, fuel, raw sewage, and solid waste, and by spreading invasive species. Learn more in the Wastewater Pollution Toolkit or in the Wastewater Pollution Online Course.\nMore than 2.5 billion people (40% of the world’s population) live within 100 km of the coast, ref adding increased pressure to coastal ecosystems. Coastal development linked to human settlements, industry, aquaculture, and infrastructure can cause severe impacts on nearshore ecosystems, particularly coral reefs. Coastal development impacts may be direct (e.g., land filling, dredging, and coral and sand mining for construction) or indirect (e.g., increased runoff of sediment, sewage, and pollutants).\nTourism and Recreational Impacts\nRecreational activities can harm coral reefs through:\n- Breakage of coral colonies and tissue damage with direct contact such as walking, touching, kicking, standing, or gear contact that often happen with SCUBA, snorkelling, and trampling\n- Breakage or overturning of coral colonies and tissue damage from negligent boat anchoring\n- Changes in marine life behavior from feeding or harassment by humans\n- Water pollution by tour boats through the discharge of fuel, human waste, and grey water\n- Invasive species which can be spread through transportation of ballast water, hull fouling of cruise ships, and fouling from recreational boating\n- Trash and debris deposited in the marine environment\nCoral disease is a naturally occurring process on reefs, but certain factors can exacerbate disease and cause outbreaks. Coral disease outbreaks can lead to an overall reduction in live coral cover and reduced colony density. In extreme cases, disease outbreaks can initiate community phase-shifts from coral- to algal-dominated communities. Coral diseases can also result in a restructuring of coral populations.\nDisease involves an interaction between the coral host, a pathogen, and the reef environment. Scientists are learning more about the causes of coral disease, especially in terms of identifying the pathogens involved. To date, the most infectious coral diseases are caused by bacteria. Transmission of coral diseases can be facilitated in areas of high coral cover ref as well as through coral predation, as predators can act as vectors by oral or fecal transmission of pathogens. ref\nThe causes of coral disease outbreaks are complex and not well understood, although research suggests that important drivers of coral disease include climate warming, land-based pollution, sedimentation, overfishing, and physical damage from recreational activities. ref\nOn coral reefs, marine invasive species include some algae, invertebrates, and fishes. Invasive species are species that are not native to a region. However, not all non-native species are invasive. Species become invasive if they cause ecological and/or economic harm by colonizing and becoming dominant in an ecosystem, due to the loss of natural controls on their populations (e.g., predators).\nPathways of introduction of marine invasive species include:\n- Ship traffic, such as ballast water and hull fouling\n- Aquaculture operations (shellfish aquaculture is responsible for the spread of marine invasive species through global transport of oyster shells or other shellfish for consumption)\n- Fishing gear and SCUBA gear (through transport when moving from place to place)\n- Accidental discharge from aquaria through pipes or intentional release\nSargassum are a type of brown, fleshy macroalgae that can have detrimental ecological and economic impacts on coral reefs when overabundant.\nIn the Indo-Pacific, high percent cover of Sargassum is common on degraded coral reefs and often represents a phase-shift from a coral to algae-dominated reef system. ref Their reproductive biology and morphology make them excellent colonizers of free space and particularly resilient to disturbances such as tropical storms. ref When overabundant, they can negatively impact the reef by shading, limiting space available for coral larvae to recruit, and transmitting pathogens. ref\nIn the Atlantic, two species of floating sargassum, S. natans and S. fluitans, are responsible for causing large mats of algae blooms which are particularly harmful and prevalent on the Caribbean and West African coastlines. ref Floating algae mats are naturally prevalent in the Northern Atlantic and provide many ecological benefits such as habitat, food, and nursery grounds to many species of fish, crustaceans and even sea turtles. ref However, in the last ten years, a shift in oceanic currents has led to an algae invasion in coral reef areas, causing reduced sunlight required by corals and anoxic and hypoxic conditions on reefs, as well as poor conditions on beaches that are detrimental to the tourism industry. ref\nCoral predators (or 'corallivores') are naturally occurring organisms that feed on corals for their polyps, tissue, mucus, or a combination of the above. Such predators typically include echinoderms (starfish, sea urchins), mollusks (snails), and some fish.\nCorallivory is a common process that, under normal conditions, allows for natural turnover in the ecosystem. However, when these predators are overly abundant (e.g., outbreak conditions), they can cause significant declines in coral cover.\nCommon coral predators include:\n- Crown-of-Thorns starfish (COTS), which are found throughout the Indo-Pacific region, occurring from the Red Sea and coast of East Africa, across the Pacific and Indian Oceans, to the west coast of Central America. COTS can be a major driver of coral loss in the Indo-Pacific, particularly under outbreak conditions.\n- Drupella snails, which are commonly found living on corals in reefs throughout the Indo-Pacific and Western Indian Ocean.\n- Coralliophila snails, which are often more problematic for Caribbean reefs, although some species are prevalent in the Pacific.""]"	['<urn:uuid:5c25f3f9-83cc-451d-a5f9-031c83361919>', '<urn:uuid:6dd87468-19aa-450b-b9cc-46d264a1ee33>']	factoid	direct	long-search-query	distant-from-document	multi-aspect	novice	2025-05-12T21:52:51.670698	13	94	2027
43	As a welder working with thick metals, how does penetration compare?	FCAW (flux core welding) generally provides better metal penetration than MIG welding. However, the actual penetration depth depends on factors like electrical voltage, current applied to the electrode, and the thickness of the metal being welded.	['Professional welders often argue on which type of welding is the most convenient for a certain application. It is the case of flux core welding versus MIG.\nBoth welding types have some similarities and differences, which argue in favor and against each of these welding types.\nThe truth is that, in general terms, no method can be considered absolutely better than the other.\nYou have to take into account some specific characteristics of the work that you’ll perform. What will you weld? Where? What finish do you intend to get after welding?\nHence, the most important thing is to learn about the conditions under which each type of welding will provide better results.\nIn this article, we aim to inform you about the essential criteria to determine which type of welding will be more suitable for your particular case.\nWhat Is Flux Core Welding?\nFormally known as flux-cored arc welding or FCAW, this technique uses a wire as an electrode. It is connected to an electricity source with the help of a special cable.\nAn electric arc heats the wire, which is deposited in the welding. The wire is fed to the weld using a special welding machine. To shield the weld from contaminants, you can use two possible methods.\nThe first one implies using shielding gas. Typically, it can be CO2, oxygen, or an inert gas such as argon or helium. This technique is pretty much similar to MIG welding, which we will explain below.\nWhat is MIG Welding?\nThis welding technique, formally known as metal inert gas or MIG, implies using a wire as well. Just like in FCAW, it serves as an electrode. Also, a shielding gas prevents the weld from being contaminated.\nHowever, in this type of welding, an external source of gas is used. The gas is fed using a hose. A special welding gun holds the electrode and allows a steady flow of the shielding gas.\nMIG is an effective welding technique to weld metals such as aluminum, stainless steel, and other nonferrous metals. This feature has made it very popular in car manufacturing and the construction industry.\nWelding Quality: Flux Core Welding vs. MIG\nThe quality of the weld is a determining factor to decide which technique to use. It is generally accepted that MIG provides better quality. It may be the result of the use of shielding gas, which is more effective at preventing contamination of the weld.\nBut what exactly will we refer to as “quality”?\nIf by quality, we mean the strength of the resulting weld, both FCAW and MIG are at the same level.\nHowever, if we refer to the look of the weld as quality, then MIG is the winner. The resulting weld using the MIG technique has a more pleasant texture and appearance.\nDoes Flux Core Offer Better Metal Penetration than MIG?\nThere is a consensus that FCAW penetrates metal better than MIG does. However, we have to understand which factors determine metal penetration. Hence, this characteristic may change depending on these parameters.\nFor starters, the electrical voltage and the current that is applied to the electrode determine how deep the welding penetrates the metal.\nBut the thickness of the metal also plays a role. Depending on it, the welding may or may not penetrate much in the metal.\nWhich Welding is Cheaper?\nAt first sight, one may think that MIG is much cheaper. Many welders come to this conclusion considering the type of welding wire used for each technique. MIG requires a solid wire, whereas FCAW uses a flux core wire.\nManufacturing the flux core wire incurs higher production costs. And true, this type of welding wire is more expensive.\nWhere Can I Weld with Each of these Techniques?\nOne of the important factors to decide which welding technique to use is the location where you will weld. MIG has to be used necessarily indoors or in enclosed environments.\nHence, you must have an enclosed workshop where you can do this type of welding. It is necessary because the wind could alter the effectiveness of the shielding gas. If you must use MIG in an open space, then you need to use a windshield.\nOn the other hand, FCAW doesn’t use a shielding gas. Hence, this welding technique is suitable for open spaces.\nPros and Cons of Both Welding Techniques\nAs seen, both techniques have their advantages and disadvantages. Let’s summarize them below.\n- It works in open environments and under different weather conditions\n- It is more effective to weld thick metal\n- Less equipment needed for welding\n- A gas tank is not needed\n- The overall cost is lower than that of MIG\n- Not recommended for thin metal\n- It creates a better-looking weld\n- Maintenance is not complicated\n- It is easy-to-learn and suitable for beginners\n- Slag and spatter are easy-to-clean\n- Not recommended for welding thick metal\n- It needs more equipment, which makes the initial cost higher than that of FCAW\nAs stated at the beginning of this article, none of these two techniques can be considered better than the other. Both FCAW and MIG have their advantages for some specific scenarios.\nHence, before you choose a welding technique, consider your particular needs. Where will you weld? What will you weld? What do you want the result to look like? The answers to these questions will help you to choose the right option.']	['<urn:uuid:48ba2631-ad01-41a6-9168-c28b7e7b7e63>']	open-ended	with-premise	concise-and-natural	similar-to-document	single-doc	expert	2025-05-12T21:52:51.670698	11	36	905
44	how many kalamazoo bats cards were known 1956	In 1956, only 39 Kalamazoo Bats cards were known.	['For its combination of popularity, scarcity, beauty, and utter unfathomability, no 19th-century set – maybe no other card set ever – exceeds Kalamazoo Bats, which appears to have been a two-year issue produced by Charles Gross & Co. of Philadelphia in 1886 and 1887. Extraordinarily, 12 of the 62 known player cards have been discovered since 1980, one was uncovered in 2005, another early in 2006. Some of the long-known cards are so scarce as to exist in mere handfuls; some of the recent discoveries are comparatively common.\nThe chronology of the issue may be indecipherable. The 20 cards depicting the New York Mets and Giants would seemingly date to 1886 (several players appeared with those teams in ’86 and not ’87) – except that Dude Esterbrook spent 1886 with the Giants and 1887 with the Mets – but is known only with the latter team. The 40 Philadelphia Athletics and Phillies all seem to be from 1887.\nThe New York and Philadelphia cards vary in design or execution in four significant ways: only the Philadelphia cards are copyrighted (and dated, 1887); only the Philadelphia cards offer two-player poses; only the New York cards feature studio portraits (the Philadelphia players are depicted outdoors in their respective ballparks: National League Park for the Phillies and Jefferson Street Grounds for the A’s); and only the Philadelphia cards ever appear with printed backs. These offer premiums like a silk umbrella or a Meerschaum cigar tube, in exchange for remarkably large numbers of cards (400 cards were needed to get the umbrella). The wording on the offer (“for the return of these Pictures in GOOD ORDER…”) suggests Gross & Co. may have actually recycled the cards they got in exchange for the premiums, and put them back in the cigarette packs.\nThere are also six team cards, clearly dated to 1887, which have always been considered and catalogued as a separate issue. But in the late ’90s, one of them turned up with the same premium offer that appears on many of the backs of the Philly cards. This suggests – though not with any certainty – that despite the exotic selection of teams (Detroit but not the Mets, Pittsburg but not the Giants) these cards were probably issued concurrent with the player cards, at least in Philadelphia. The other rationale for listing the teams as a separate set was their extraordinary scarcity. But as more and more unique (or nearly unique) player cards have turned up, the team cards have become relatively less scarce, and should probably be considered part of the main set or sets of 1886-87 (and are considered such in the checklist on the second page).\nThere’s another complicating factor which might be contributing to our collective confusion. Gross & Co. issued many other cards in the identical 21/4-by-4-inch sepia format on thick cardboard, featuring actresses and other non-sports figures. It’s very possible that the baseball players are a mere subset of one larger, amorphous, added-to-as-needed running issue of celebrities throughout 1886 and 1887.\nThe bulk of the Giants and Mets may have been published in 1886 with Esterbrook and a few others (or maybe just Esterbrook) added in ’87. The New York players might have been available only in New York, and the Philadelphia players only in Philadelphia. Esterbrook, a remarkably popular player in his day (given his seemingly common-place statistics), could have been an aberration that does not preclude any of our theories on the timing or geography of the sets.\nThere are also cabinet cards, on distinctive black cardboard mounts. Only the team cards and some of the Philadelphia players are known in this format, suggesting another link between the regular-issue team cards and the player cards.\nAs late as 1956, only 39 Kalamazoo Bats were known. Lew Lipset wrote that three more (Esterbrook, Dave Orr, and the joint card of Jim Fogarty and Deacon McGuire), had been added to the list by 1965. By 1974, the number was up to 48. Lipset’s 1983 checklist shows 50 cards, and identifies the ones discovered after 1965 as Dan Casey, Jim Devlin, Bobby Mathews, Harry Stovey (Hands on Hips), and the two-player cards of Charlie Bastian and Harry Lyons, and Wilbert Robinson and Fred Mann, and George Townsend and Jocko Milligan – with Pete Gillespie of the Giants as the then-most-recent discovery (1980). Charlie Mason, only briefly the manager of the A’s in 1887, turned up not long after publication of Lipset’s milestone Encyclopedia of Baseball Cards: 20th Century Tobacco Cards, 1909-1932 (Paperback), and then the card started to turn up fairly regularly.\nBut a century after the cards were produced, a remarkable collection of 41 – nearly half of them in pristine condition – was sold to Lipset and Ron Oser. Among them were previously unknown cards of Buck Ewing (which auctioned for the then-astounding price of $2,920), Tim Keefe ($2,460), and John Ward ($2,655). Also, previously unreported cards in the collection included Pat Deasley and Tom Poorman. By this time Gillespie and Mason had become so common that the former, in EX-MT condition, sold for just $775, and the later, auctioned in EX, fetched just $495.\nThe six most recently discovered cards are Chippy McGarr (misidentified as “McGan” – a second copy turned up on eBay in 2005), Ed Seward, and Gus Weyhing of the A’s, Tom McLaughlin of the Mets (1999), then in 2005 Denny Lyons of the A’s (also on eBay) and, earlier this year, Bill Holbert of the Mets.\nThe great 1987 discovery may have increased the checklist by nearly 10 percent, but it did nothing to unravel the enigma of “K-Bats.” Among the 41 cards were the rare, in some cases, previously-unknown Giants, plus Phillies and A’s – but not a single Met.\nThere is also some confusion over player identities. The card showing the Phillies’ trainer, Taylor, working on the arm of a player identified as Harry Lyons, is a wonderful vignette of baseball as they lived it in Philadelphia in 1887. In fact, the player is Jim Devlin. The confusion is understandable: Lyons played one game for the Phils that year; Devlin, only seven. Adding to the befuddling nature of the series is that despite their brief tenures with the Phillies in 1887, the cards of Devlin and Lyons are among the most frequently found in the series.\nStill lost to history is the identity of a player identified as “Gallagher,” shown being tagged out by A’s second baseman Lou Bierbauer on the action-pose card they share. No such-named player ever appeared for the A’s – although it could easily be Jim Gallagher, who had played one game for the Senators the year before. Similarly, there is the irony of Whitey Gibson, who played just one major league game – a year after these cards were produced. Locals (Gibson was from Lancaster, Pa.) who hung around but rarely, or never, played, were much more common in the 19th century than in the years since. Gibson, shown with the Phillies, is a common Kalamazoo Bat. But he appears in N172 Old Judge with the A’s – and only two copies of that card are known to exist, with the first having not turned up until 1986.\nAre other Kalamazoo Bats still out there awaiting discovery? When Lipset wrote in 1983, there were Cabinet versions of McGarr and Seward, but no regular cards of them. The A’s team card showed Manager Mason, Poorman, and Weyhing – and none of them were known in the regular set, either. As noted, all five have since turned up – Mason in sufficient quantities to make his virtually a common card.\nWith the discovery of Denny Lyons, the list of likely candidates for “new” Kalamazoo Bats is reduced to Charlie Bastian of the Phillies, and Jocko Milligan and the unknown “Gallagher” of the A’s – the only players on any of the nine multiple-player cards who don’t have their own individual cards. Countless ’86 Giants and Mets are missing (Hall of Famer Mickey Welch, as an example). Of the Philadelphia players there are many longshots: Al Atkinson, Frank Bancroft, Bill Casey, Ed Flanagan, Ed Greer, Bill Hart, Chief Roseman (who is already in the set as an ’86 Met), Jim Roxburgh, Bill Taylor, and Cannonball Titcomb of the Athletics, and Tony Cusick, Ed Daily, Tommy McCarthy, and Barney McLaughlin of the Phillies. There’s one more possibility. Another cabinet was discovered in the ’90s for which no regular card is known. It shows Bastian, Casey, and the Phils’ trainer in the series’ only three-shot.\nNearly all write-ups of Kalamazoo Bats miss the exceptional similarities in layout and font style to the Old Judge set (the newly discovered Holbert has the same photo as his card in the Old Judge “Dotted Ties” subset), or the obviously shared photography with the Four Base Hit series (the photos of John Montgomery Ward, though not identical, were clearly taken at the same sitting at the same photography studio). Most of the cards of the prolific 1886-90 era are seemingly interconnected in design or photography, hinting at a story of production or design at which we can only guess.']	['<urn:uuid:82ff4bcb-ceaa-4a25-8d06-f6fafeea7cf4>']	factoid	direct	short-search-query	similar-to-document	single-doc	novice	2025-05-12T21:52:51.670698	8	9	1526
45	asian vs western tea steeping differences	Asian-style steeping uses a small teapot (10 ounces or less) with a large quantity of tea and small quantity of water, with very short steeping times of 30 seconds to 1 minute. The tea is re-steeped multiple times for rich, full-bodied flavor. In contrast, Western-style uses larger teapots (18-32 ounces) with more water and less tea, steeping for 2-5 minutes. Western-style rarely allows for successful re-steeping as the taste becomes thin and light.	['TIPS FOR STEEPING ABSOLUTELY DELICIOUS TEA\nAre you ready for extraordinary flavor?\nTea comes in a variety of leaf sizes, shapes and six classifications: black, green, oolong, heicha (‘dark tea’, including Pu-erh), white, and yellow. How to proceed when steeping a new tea can be puzzling, but with our guidelines you will be able to steep delicious tea – easily – every time. Each tea that we sell on our website has a detail page with specific steeping instructions for that tea.\nOur tea-steeping instructions are based on the guidance we have been given by the artisans who make our teas and the Tea Masters we have visited during our tea sourcing trips to Asia\nOur methods are theirs, and the variations we suggest come at their encouragement. Tea preparation is both collaborative and evolutionary – there is no right or wrong – just recommendations based on the collective wisdom of seasoned tea drinkers over time. Our guidelines will help you understand how to proceed when steeping a new tea or how to tweak your technique to improve the taste of your favorite tea.\nA Great Tasting Cup of Tea is the Result of Many Factors\nSeasoned tea drinkers understand that steeping a great cup of tea is not something that ‘ just happens’ – it is the result of knowing how to proceed with the type of tea you will be steeping and paying attention to the essentials of careful tea preparation.\n1. Each type of tea will taste best when steeped in a certain way.\nFor example, green tea does not like to be steeped in the same manner as black tea; and white tea methods are different again. Understanding these differences is the first step in encouraging delicious results from your various teas.\n2. Steep each Tea Trekker tea according to our recommended steeping method.\nOur whole-leaf teas are selected by us from fresh seasonal batches of premium quality leaf, and from the two highest grades of that tea available each season. Because we know how different our teas are from teas sold elsewhere with the ‘same’ name, and how much fresher our teas are, we ask that you steep our teas using our tea steeping recommendations, in order to enjoy the best flavor. Once you are familiar with the flavor attributes of a particular season’s leaf, feel free to make personal adjustments to suit individual taste.\n3. Use enough leaf\nA delicious cup of tea begins with using enough leaf. Using less leaf than our recommended amount will give you a weak-tasting cup.\n4. Be mindful of the temperature of the water that you are using.\nDifferent teas have optimal temperature requirements and will not yield their best flavor when the water is too hot (or too cool).\n5. Longer steeping time does not yield tastier tea\nEach tea and each steeping method has a preferred steeping time. Some teas respond well to only a specific time in the water – other teas have more flexibility for longer (or shorter) steep times.\n6. Re-steeping tea\nAsian-style tea steeping is the practice of steeping certain teas multiple times in a small teapot or tea vessel. Western-style steeping uses a larger teapot and the tea gives up most of its flavor in one steeping ( with some reasonable exceptions). The differences in taste, body and overall depth of flavor that is shown by tea stepped by these two steeping methods is significant – try both ways and see which you prefer, or hopefully you will enjoy them both. (see the section below for more info on these two popular methods)\nWhen you purchase loose-leaf tea you will notice that the appearance and the volume of the tea leaves differs from tea to tea. Some teas are light and bulky, while other teas are heavy and dense. For light and bulky teas you will need to use more leaf (by volume) than you will with heavy and dense teas. Our recommended measurements of tea take the density and bulky-ness of each tea into account. For true, consistent accuracy, a small scale is invaluable, so that we can all measure by weight.\nIn general for Western-style tea-steeping we recommend this proportion of leaf to water:\n2 to 3 grams of leaf for every 6-ounces of water in your teapot or tea-steeping vessel.\nThis measure will vary in volume depending on the size and heft of the tea leaf. A 2 to 3 gram measure can be any of the following:\n1, 2 or 3 Tablespoons\nIf you wish to experiment with enjoying many different teas, an easy and inexpensive method for accurately measuring leaf tea is to use an inexpensive electronic kitchen gram scale. These cost approximately $16.00-$35.00 and will take the guesswork right out of measuring your tea.\nFor every type of tea there is a preferred water temperature that will coax the best flavor from the tea leaves. Water that has cooled from the boil for a minute or two will extract different nuances of flavor and aroma from certain teas than water that is either hotter or cooler in temperature. Cooler water diminishes astringency; hotter water encourages it.\nAfter water is brought to a boil, it will drop approximately 10°F every minute it stands off the heat in a covered kettle.\nHere are our suggested water temperatures for the major types of tea:\nPu-erh & other Heicha:\n200°F – 210°F\nThat’s water that is just at the boil\n195°F – 205°F\nThat’s water that is just under a full boil\nOolong Tea ( strip-style):\n195°F – 205°F\nThat’s water that is just under a full boil\nOolong Tea ( semiball rolled-style):\n180°F – 190°F\nThat’s water that has cooled slightly\n‘Main Harvest’ China Green Tea & China Jasmine Tea:\n175°F – 185°F\nThat’s water that has cooled a bit more\nChina Spring Green Tea & Japan Ichibancha Spring Tea:\nThat’s water that is cooler yet\nChina Yellow & White Tea:\n160°F – 170°F\nThat’s the coolest water of all\nTea that is either under-steeped or over-steeped will likely be disappointing. Each type of tea has a preferred steeping time that brings out the best taste in the tea.\nThe eventual addition of milk and/or sugar will make a difference as to how a tea is steeped. Adding milk or sugar will cut the astringency of a longer steep time, so it may be appropriate to steep certain black teas for the longer 5 minutes if milk will be added. The same tea drunk plain will likely be tastier if steeped for 3 minutes.\nAlso, bear in mind the place of origin of the tea that you are steeping. All tea is manufactured for a particular assumed or historic audience of tea drinkers, so the characteristics of a certain tea will be better suited for an addition of milk or not, depending on the expected market.\nHow to tell?\nChinese, Japanese, Korean and Taiwanese teas, of all classifications, are historically assumed to be drunk plain, with nothing added. African, Ceylon and Indian teas (with a few exceptions) are primarily made for a Western export market and are manufactured to accommodate (or encourage) the addition of milk and/or sugar.\nASIAN-STYLE STEEPING & WESTERN-STYLE STEEPING:\nFor teas that can be steeped either Asian-style or Western-style such as oolong and Pu-erh, we provide information that will familiarize you with the differences between these different cultural tea-steeping methods. And we will explain how to use the tea vessels that are important in Asian tea cultures to steep these teas, so that you will obtain the best flavor when using this method.\n- uses a small teapot (10 ounces or less) or a gaiwan (3-6 ounces)\n- uses a small quantity of water and a large quantity of tea\n- time in the water for the tea is very short – steep times are 30 seconds to 1 minute\n- tea is re-steeped multiple times to reveal a different facet of flavor each time. Flavor will be rich in dimension and very full-bodied due to the ratio of tea to water\n- uses an ‘English-style’ teapot (18-32 ounces)\n- uses a large quantity of water and a small quantity of tea\n- time in the water for the tea is long – steep times of 2-5 minutes depending on the tea\n- a few teas can be re-steeped successfully, but the taste will be thin and light due to the large amount of water and long steeping time used in this method\nMore Tea 101 Tutorials:']	['<urn:uuid:17cb20db-8a2f-48ff-898b-0482eba2185a>']	open-ended	direct	short-search-query	similar-to-document	single-doc	novice	2025-05-12T21:52:51.670698	6	73	1423
46	How can someone efficiently manage their time and keep track of tasks when working on a big research project or important assignment?	To effectively manage time and tasks during a big project, you should treat it like a part-time job and implement several key strategies. First, identify and reduce your top time-wasters, and set small, achievable goals such as writing a specific number of pages per week. Create a dedicated workspace and carry a notebook to write down thoughts as they occur. For task tracking, you can use a combination of digital and analog methods - either printing layouts and calendars, using digital stickers for organization, or employing project management tools. It's also important to make regular appointments with advisors, establish progress benchmarks, and ask for help when needed. You can use waiting time productively for reading or note-taking, and utilize bibliographic software to manage research sources efficiently.	['But if you enjoy the feel of writing down your daily tasks, you could use these digital hacks to make to keep up with your bullet journaling habits.\n1. Use Printable Layouts and Calendars\nOne of the greatest things about the bullet journal system is the ability to create your own layouts. But it’s also one of the most time-consuming.\nWhile many people clearly enjoy the process of hand drawing their layouts, decorating their journals, and more, you can choose to take a more utilitarian approach and just use printed layouts and calendars.\nWhile it’s preferable that you design your own layout to ensure that you get the most out of your bullet journal, you could start out with someone else’s printables.\nEither way, whether you create them yourself or use someone else’s layouts, you can print out daily, weekly, and monthly spreads, as well as any other pages you choose to include in your journal such as goals, budgets, reading lists, meal planners, and more.\nCreate Your Own Layouts and Dotted Paper\nThere are plenty of digital tools you can use to create your own layouts:\n- If you’re on a computer, you can use any desktop design program like Adobe Illustrator or InDesign. You could also use Microsoft Word to draw tables.\n- If you have an iPad Pro, consider an app like Procreate.\n- If you don’t want to pay for an Adobe program or don’t have a tablet, opt for online alternatives to these programs—like Gravit or Canva.\nCheck out the video below for a simple planner page created in Illustrator. You can use the tips and tricks in the video and apply it to your own design.\nIf you need more video tutorials, check out the examples below:\nDo you prefer PDF format? You can make them fillable and add items to your agenda or to-do list on your computer and then print it out.\nAnd finally, if you want to stay true to the most favored style among bullet journal enthusiasts, you’ll probably want to create your own dotted grid paper which you can do with a free service like Gridzzly. The site allows you to choose the spacing and opacity of the dots.\nSo if you still feel like creating these layouts, there are plenty of free and paid options available. A quick search on Pinterest for journals and habit trackers will yield a ton of both.\nYou’ll find that most printables are available as PDFs.\nHere are a few free examples to get you started:\n- 2018 Calendar (You get a PDF with a spread for each month, but will have to provide your email to get the free printable.)\n- Dot grid paper printable (Available in Letter and A5 size PDFs.)\n- Weekly template (PDF)\nHow to Use Your Printables\nSo you have your prints. Now what? The easiest approach is to use a smaller binder or Filofax rather than having to stick the printed pages into an existing journal.\nYou could also go for something even simpler like a clipboard if you want to keep things as minimal as possible.\nBut if you’ve already picked out a journal that works for you, you’ll want to buy full sheets of sticker paper. You can print two separate pages for an A5 journal onto one sheet of letter sticker paper and cut it down the middle. If you don’t want to do any extra trimming, then you’ll want to buy A4 sticker paper rather than the letter size.\n2. Make It Pretty With Digital Stickers or Stamps\nDoing a quick search for bullet journals on Instagram or Pinterest will reveal a lot of really beautifully decorated planners. Again, you might not have time to dedicate to doodling in the margins of your bullet journal.\nInstead of trying to draw these things yourself, you can jazz up your bullet journal with digital stickers that you can print out at home.\nIf you have a die cutting machine like the Silhouette Cameo or Portrait, you have the added bonus of not having to cut your stickers out by hand.\nYou can either create the stickers yourself in the Silhouette software or just use the machine to cut out stickers you’ve purchased or downloaded.\nThere are plenty of places you can purchase stickers to use in your bullet journal, but, just like the layouts, with a bit of creativity, you can make your own using an online service like PicMonkey.\nBut be warned, PicMonkey made a significant change to its free service—you can’t save your images unless you have a paid account.\nIn PicMonkey, create a blank canvas and use the Overlay and Text features and you can create all sorts of stickers. Either pick a theme that appeals to you, or check out the Scrapbooky section of the overlays with banners, labels, tape, and much more that could fit right into your bullet journal aesthetic.\nIf you don’t want to upgrade to a paid PicMonkey account, Canva is a great alternative. Create a new canvas using the US letter size and you can create stickers using text and elements. Search for the same items listed above or any keywords for stickers that are relevant to your tasks, chores, pages, and more.\nTo get started, here are a few places you can download free stickers:\n- Banners, Dividers, Doodles, and Letters\n- Printable Planner Stickers (PDF, JPG, PNG and Silhouette files, but you will have to provide your email to get them)\n- Printable Label Stickers (PDF, JPG, and PNG files)\nOnce you have your stickers ready to print, you can print them on regular sticker paper, or in some cases, transparent sticker paper might look better.\n3. Combine With Your Digital Life\nAs satisfying as it is to handwrite your to-do list, and check items off as they’re completed—there are a lot of advantages to a digitized planner or calendar. So why not have the best of both worlds?\nSearch, reminders, recurring tasks, and seamless sharing are just a few of the features you might want to hang on to with a semi-digital, semi-analog system. Not to mention the ability to take a digital planner with you everywhere, provided your phone battery doesn’t die.\nYou can take the most important parts of your journal with you by snapping photos. If you simply need to remember your to-do list, just use your phone’s camera app.\n- Search: If you need something a little more robust, among the many robust features in Evernote is the ability to snap pictures of text which will be fully searchable in the app.\n- Seamless Sharing: Google Calendar is a great tool for all your scheduling needs, and can be shared with colleagues and family. As you’re planning for your week, keep Google Calendar (or your preferred digital calendar) open, and you can log important events, meetings, and dates in digital format as you write them into your planner, and vice versa.\n- Reminders: You can also use the same approach with reminders. Use something as simple as the Reminders app on your iOS phone or opt for something multi-platform like Google Now.\n4. Use a Digitized Notebook or Pen\nThe final option will require a little more of an investment on your part but will make it easier to digitize your paper planner.\n- Smart Pens: Use a tool like Livescribe’s smartpens to convert lists, and dates in your bullet journal to digital notes.\n- Digital notebooks: A traditional journal with a modern twist, like Moleskine’s Paper Tablet paired with the Smart Writing Set Pen can also take your notes from the page to the screen.\nThese methods will cost you anywhere from $30 to $180, so it might be worth trying your hand at the first three options before committing to something a little more pricey.\nThe Tools Don’t Matter\nAt the end of the day, the bullet journal system is all about keeping you organized and staying on task. It started out as a simple system created to fill a gap in the planner system.\nWhile it’s really easy to get bogged down in the details, all that matters is the ability to keep track of your tasks. Rather than obsess over layouts and doodles, remember that the most important thing is the system of logging and migrating tasks and events, and everything is else is just gravy.\nAnd if you are short of ideas, go to these places to find inspiration for your next bullet journal.', 'When writing an academic paper, and in particular a long paper such as a senior thesis, it helps to be organized. There is no single solution for everybody. All of us have our own organizational styles. That said, it can be helpful to review what others have found useful.\nadapted from Erica Chenoweth\nMaking time for thesis work\n- Identify your top five time-wasters; try to cut back on at least one\n- Identify your best time of day for reading and writing\n- Set small and achievable goals (i.e. “I will write five pages this week” or “I will write 750 words each day”).\n- Do mundane tasks first\n- Work in the same place\n- Avoid distractions and use them as rewards\n- Carry a notebook with you to write down thoughts as they come up\n- Use waiting time to read or write down your thougths\n- Treat your project as a part-time job\nMaking the most of your reading\n- Use flags in library books and highlight articles\n- Add the book or article into a bibliographic document or program, along with the relevant quotes or ideas from the book\n- Cut yourself off from reading in December\n- Make regular appointments with your advisor\n- Establish benchmarks for progress\n- Exchange completed work with other thesis-writers\n- Ask for help when you need it!\n- Mortimer J. Adler’s (1940) How to Read a Book is not very short but it is valuable. One of the things he does, for instance, is distinguish between “three distinct readings”. Structural, or analytic, reading proceeds from the whole to the parts. Interpretive, or synthetic, reading, proceeds from the parts to the whole. Critical, or evaluative, reading involves judgments by the reader. There is also a newer version of this book, co-authored with Charles Van Doren, which outlines four reading styles\n- Hampshire College: “How to Read a Scientific Research Paper”\n- Matthew Cornell: “How to read a lot of books in a short time”\nProject Management and Tasks\nSoftware and Technology\nTwo other great resources for information about software and technology relevant to our academic work are Lifehacker and Profhacker. A search of their blog posts can reveal some great resources, many of which are free.\nBibliographic Software. There are a lot of options out there, especially if you want to pay money. Zotero is a free, easy-to-use Firefox extension to help you collect, manage, cite, and share your research sources. EndNote is a citation management system designed to keep track of references in your research. It offers many different reference types, including journal articles, books, conference proceedings, maps, and theses. You can import references directly from many online databases, copy and paste from online citations, or manually type in the reference. Other reference software. There are quite a few options, many of which are specific to certain operating systems. Wikipedia has a decent reference software comparision. Most of these are not free. Two great Mac programs are Bookends (used by this writer) and Sente.\nNote-Taking. It is a great idea to figure out a note-taking system for yourself. There are a lot of different tools for establishing such a system, from the ever-popular pen-and-paper to software solutions such as EverNote. Lifehacker has a number of posts about note-taking options, including: Best Note Taking Apps for Students.\nData Analysis. Among political scientists, STATA, SPSS, and R are the most popular software packages for statistics. R takes a little more effort to learn, but has the advantage of being free.\nProject and Task Management. Completing a major research project is not a simple task. There are many different steps to getting there and it is useful to put a system in place to manage your project. Lifehacker: “Five Best Personal Project Management Tools”\nupdated July 12, 2017 – MN']	['<urn:uuid:9aef4da4-81dc-4c1f-8676-856450e64c6b>', '<urn:uuid:b3c78c9a-c30a-4308-b927-b699d730240c>']	open-ended	direct	verbose-and-natural	distant-from-document	three-doc	novice	2025-05-12T21:52:51.670698	22	126	2059
47	complete melting greenland west antarctic ice sheets how much sea level rise and what consequences for human civilization	Complete melting of the Greenland and West Antarctic ice sheets would lead to around 7m and 5m of sea-level rise respectively. This would have severe consequences for human civilization, particularly affecting the lower parts of the world's major rivers (Po, Rhine, Nile, Ganges, Indus, Mekong, Yellow, Mississippi, Amazon) where over 3 billion people live and where most agriculture and industry are located, as these areas sit only a few meters above sea level. This would result in widespread inundation of low-lying areas and reconfiguration of coastlines worldwide.	['18.104.22.168 Deglaciation of West Antarctic and Greenland ice sheets\nThe potential for partial or near-total deglaciation of the Greenland and the West Antarctic ice sheets (WAIS) and associated sea-level rise (Jansen et al., 2007 Sections 22.214.171.124 and 126.96.36.199; Meehl et al., 2007 Sections 10.6.4, 10.7.4.3 and 10.7.4.4; Alley et al., 2005; Vaughan, 2007), is a key impact that creates a key vulnerability due to its magnitude and irreversibility, in combination with limited adaptive capacity and, if substantial deglaciation occurred, high levels of confidence in associated impacts. Ice sheets have been discussed specifically in the context of Article 2 (O’Neill and Oppenheimer 2002; Hansen, 2005; Keller et al., 2005; Oppenheimer and Alley, 2005). Near-total deglaciation would eventually lead to a sea-level rise of around 7 m and 5 m (***) from Greenland and the WAIS, respectively, with wide-ranging consequences including a reconfiguration of coastlines worldwide and inundation of low-lying areas, particularly river deltas (Schneider and Chen, 1980; Revelle, 1983; Tol et al., 2006; Vaughan, 2007). Widespread deglaciation would not be reversible except on very long time-scales, if at all (Meehl et al., 2007 Sections 10.7.4.3 and 10.7.4.4). The Amundsen Sea sector of the WAIS, already experiencing ice acceleration and rapid ground-line retreat (Lemke et al., 2007 Section 188.8.131.52), on its own includes ice equivalent to about 1.5 m sea-level rise (Meehl et al., 2007 Section 10.7.4.4; Vaughan, 2007). The ability to adapt would depend crucially on the rate of deglaciation (**). Estimates of this rate and the corresponding time-scale for either ice sheet range from more rapid (several centuries for several metres of sea-level rise, up to 1 m/century) to slower (i.e., a few millennia; Meehl et al., 2007 Section 10.7.4.4; Vaughan and Spouge, 2002), so that deglaciation is very likely to be completed long after it is first triggered.\nFor Greenland, the threshold for near-total deglaciation is estimated at 3.2-6.2°C local warming (1.9-4.6°C global warming) relative to pre-industrial temperatures using current models (Meehl et al., 2007 Section 10.7.4.3). Such models also indicate that warming would initially cause the Antarctic ice sheet as a whole to gain mass owing to an increased accumulation of snowfall (*; some recent studies find no significant continent-wide trends in accumulation over the past several decades; Lemke et al., 2007 Section 184.108.40.206). Scenarios of deglaciation (Meehl et al., 2007 Section 10.7.4.4) assume that any such increase would be outweighed by accelerated discharge of ice following weakening or collapse of an ice shelf due to melting at its surface or its base (*). Mean summer temperatures over the major West Antarctic ice shelves are about as likely as not to pass the melting point if global warming exceeds 5°C (Meehl et al., 2007 Section 10.7.4.4). Some studies suggest that disintegration of ice shelves would occur at lower temperatures due to basal or episodic surface melting (Meehl et al., 2007 Sections 10.6.4.2 and 10.7.4.4; Wild et al., 2003). Recent observations of unpredicted, local acceleration and consequent loss of mass from both ice sheets (Alley et al., 2005) underscores the inadequacy of existing ice-sheet models, leaving no generally agreed basis for projection, particularly for WAIS (Lemke et al., 2007 Section 220.127.116.11; Meehl et al., 2007 Sections 10.6.4.2 and 10.7.4.4; Vieli and Payne, 2005). However, palaeoclimatic evidence (Denman et al., 2007 Sections 18.104.22.168 and 22.214.171.124; Overpeck et al., 2006; Otto-Bliesner et al., 2006) suggests that Greenland and possibly the WAIS contributed to a sea-level rise of 4-6 m during the last interglacial, when polar temperatures were 3-5°C warmer, and the global mean was not notably warmer, than at present (Meehl et al., 2007 Sections 10.7.4.3 and 10.7.4.4). Accordingly, there is medium confidence that at least partial deglaciation of the Greenland ice sheet, and possibly the WAIS, would occur over a period of time ranging from centuries to millennia for a global average temperature increase of 1-4°C (relative to 1990-2000), causing a contribution to sea-level rise of 4-6 m or more (Meehl et al., 2007 Sections 10.7.4.3 and 10.7.4.4; Oppenheimer and Alley, 2004, 2005; Hansen, 2005).\nCurrent limitations of ice-sheet modelling also increase uncertainty in the projections of 21st-century sea-level rise (Meehl et al., 2007 Section 10.6.4.2) used to assess coastal impacts in this report. An illustrative estimate by WGI of the contribution of processes not represented by models yielded an increase of 0.1-0.2 m in the upper ranges of projected sea-level rise for 2100 (Meehl et al., 2007 Section 10.6.4.2). Other approximation methods would yield larger or smaller adjustments, including zero.', 'The remote location of the Antarctic and Greenland polar ice sheets may leave us with the impression that developments in these regions have little effect on the climate and life in the temperate zones of the Earth, where most of us live. We may therefore be forgiven for asking why should we care when these changes are projected to unfold over tens to hundreds of years.\nHowever, the stability of the polar regions is critical for maintaining a planet with the conditions that allowed the emergence of humans, agriculture and civilisation, as well as many other species. The polar ice sheets serve as “thermostats” of global temperatures from which cold air and cold ocean currents emanate, moderating the effects of solar radiation. The ice sheets regulate sea levels, store volumes of ice whose melting would raise sea level by up to 61 metres.\nUnfortunately, what’s happening with the polar ice sheets now ought to warn humanity of what is to come.\nIf greenhouse gas emissions continue unchecked, the world may warm by 8–10℃ by 2300. Such a temperature rise could raise sea levels by tens of meters over hundreds of years.\nThe recent paper only looked at sea level rise from melting Antarctic ice sheets and does not take into account sea level rise contributions from the Greenland ice sheet (currently about 280 billion tonnes per year), which would more than double the Antarctic contribution.\nAntarctic warming: Red represents areas where temperatures have increased the most during the last 50 years, particularly in West Antarctica. NASA\nPeering Into The Past To See The Future\nMuch of the discussion in the paper and related papers appears to assume linear global warming – that is, little change to the rate of warming over time.\nLittle mention is made of feedbacks which could increase the rate of warming. Such feedbacks could arise from reducing albedo, where solar radiation usually strongly reflected by ice is replaced by strong absorption by water.\nOther feedback processes associated with warming include methane release from permafrost and bogs; loss of vegetation; and fires.\nIn a recent article, former NASA climate scientist James Hansen and a large group of climate scientists point to observations arising from detailed studies of the recent history of the atmosphere-ocean-ice sheet system.\nThe climate records of the past — specifically, the Holocene (from about 10,000 years ago) and the Eemian interglacial period (about 115,000 to 130,000 years ago) — are closely relevant to future climate projections. These records include evidence for rapid disintegration of ice sheets in contact with the oceans as a result of feedback processes resulting in sea level rise to 5-9 m above current levels. All this during a period when mean global temperatures were near to only 1℃ above pre-industrial temperatures.\nSea levels reflect the overall global temperature and thus of global climate conditions. As shown by the position of the circles in the chart below, the ratio of sea level rise (SL) to temperature rise (TR) during the glacial-interglacial cycles was approximately between 10-15 metres per 1℃.\nPlots of Temperature rise (relative to the pre-industrial age) vs relative sea level rise in (meters).\nBy contrast from around 1800 to the present sea level rose by an approximate ratio of 0.2-0.3 m per 1℃. This suggests significant further rise towards an equilibrium state between sea level and temperature. Thus, the points in the right-hand circle represent long-term temprature-sea level equilibria in the past while points in the left-hand circle represent where we’re at now, namely at an incipient stage moving toward future temprature-sea level equilibrium.\nWhy Should Long Term Climate Change Matter?\nDue to the extreme rate of CO₂ and temperature rise during the 20th century relative to earlier events and the non-linearity of climate change trends the timing of sea level rise may be difficult to estimate.\nEven on conservative estimates, current global warming is bound to have major consequences for human civilisation and for nature, as follows:\nFurther melting of the ice sheets will destroy the climate conditions which allowed agriculture and the rise of civilisation in the first place.\nThe lower parts of the world’s great rivers (Po, Rhine, Nile, Ganges, Indus, Mekong, Yellow, Mississippi, Amazon), where more than 3 billion people live and the bulk of agriculture and industry are located, sit no more than a few metres above sea level.\nFurther melting of the Antarctic and Greenland ice sheets can only result in sea level rises on the scale of tens of metres, changing the continent-ocean map of Earth.\nGlobal temperatures have already risen 0.9℃ and continental temperatures 1.5℃ degrees above pre-industrial levels. If we account for the cooling effect of sulphur aerosols from industrial pollution, greenhouse gases have already contributed 2℃ of global warming. The current rate of global warming, faster than any observed in the geological record, is already having a major effect in many parts of the world in terms of droughts, fires, and storms.\nAccording to James Hansen burning all the fossil fuels on Earth would result in warming of 20℃ over land areas and a staggering 30℃ at the poles, making “most of the planet uninhabitable by humans”.\nIn 2009 Joachim Hans Schellnhuber, Director of the Potsdam Climate Impacts Institute and Climate Advisor to the German Government, stated: “We’re simply talking about the very life support system of this planet”, constituting one of the most critical warnings science has ever issued to our species.\nMitigation plans proposed by governments would slow down the rate of carbon emissions but continuing emissions as well as feedbacks from ice melt, warming oceans, methane release and fires would continue to push temperatures upwards.\nAn effective technology required for global cooling efforts, if technically possible, would require investment on a scale not less than the trillions of dollars currently poured into armaments and war in the name of defence (more than $1.6 trillion in 2014).\nWhich planet do current decision makers think we are living on?']	['<urn:uuid:0963e46b-7b86-4460-9bfe-ac565f4bafd3>', '<urn:uuid:908e8572-5519-4971-a7d9-49332bd17e1d>']	factoid	direct	long-search-query	similar-to-document	multi-aspect	novice	2025-05-12T21:52:51.670698	18	87	1732
48	What security options exist for executing PostgreSQL stored procedures?	PostgreSQL procedures have two security options: SECURITY INVOKER and SECURITY DEFINER. These determine which user privileges should be used when executing the procedure.	['In this article, we will learn how to work with procedures in Azure Database for PostgreSQL.\nData is typically stored in tables in any database management system. Data is queried typically using the query language supported by the database. Applications that consume data from these tables can typically execute ad-hoc queries on the database to read and write data. Ad-hoc queries are a reasonable approach when analyzing data, but when the queries are expected to be executed in an industrialized fashion, compiled routines of query logic are employed as the query plan is optimized for execution. Additionally, it becomes a standard façade for decoupling actual data sources from the consumer application. These routines are called procedures in the database parlance. Azure Database for PostgreSQL is the Postgres database offering on the Azure cloud.\nProcedures in Azure Database for PostgreSQL\nIt is assumed that one already has an Azure account with an instance of Azure Database for PostgreSQL setup on it. To operate in this instance, we typically need a management tool. pgAdmin is one of the most popular tools that is typically used with PostgreSQL. It is assumed that one has already installed this tool and connected it to the instance of Azure Database for PostgreSQL, as we would be using it for working with procedures.\nThe first thing to understand before starting to develop and define procedures in Azure Database for PostgreSQL is the syntax of procedures. CREATE PROCEDURE is the SQL statement that is used to create procedures in Azure Database for PostgreSQL, and the syntax for the same is as mentioned below.\nLet us understand different parts of this syntax before proceeding with an example of implementing the procedure.\n- name – This parameter represents the name of the procedure.\n- argmode – This parameter represents whether mode of parameter, whether it is an input or input and output parameter.\n- argname – This parameter represents the name of the parameters that would be passed to the procedure while invoking it.\n- argtype – This parameter represents the data type of the parameter.\n- default_expr – This parameter represents the expression for the default value of the specified parameter.\n- lang_name – This parameter represents the language name that the procedure is implementing.\n- TRANSFORM – This part lists and custom or user-defined transformations to handle any custom data type that the user may have defined.\n- SECURITY INVOKER | SECURITY DEFINER – These parts represent the privileges of the user using which the procedure should be executed.\n- SET – This section and the options in this section represent the syntax to set to the value of the variables as well as system parameters.\nNow that we understand the syntax of creating procedures, let’s proceed with an example of implementing a procedure in the Azure Database for PostgreSQL. We need to have at least one table with some records in it so that we can use the stored procedure to perform some action on this table. As shown below we have created one such table named employee and have inserted a few records in it.\nAssuming pgAdmin is open and we are using it for developing procedures in Azure Database for PostgreSQL, navigate to the procedure section in the browser pane of pgAdmin, right-click and select the menu option to create a new procedure. Here we are using the graphical interface provided by pgAdmin to create a new procedure instead of coding it from scratch. The procedure creation wizard would look as shown below. The first step is to provide a name for the procedure. We intend to use this procedure to insert data into the employee table, so we have named it InsertEmpData. By default, the owner would be the user id using which one has connected to the database instance and the default schema would be public. Configure these options as desired, and then click on the Definition tab.\nIn the Definition tab, we can define the parameters that would be used by the procedure for input and output. Let’s say that we intend to pass two parameters – employee id and employee name. Click on the plus button to add parameters, select the data type of the parameter, provide an appropriate name, select the type of the parameter – input / input-output and optionally and default values for it. Once done, click on the Code section.\nThis section contains the body of the procedure where we define the actual logic that should be executed when the procedure is invoked. We intend to insert a record in the employee table using the input parameter employee id and name. We will use the INSERT command to insert data into the table, and the input parameter names would be passed in the value part of the command. Some query languages use the “@” prefix with parameters. In this case, in PostgreSQL, we do not need this prefix. So, key in the SQL statement as shown below and then click on the options tab.\nIn this section, we can configure a variety of options like whether to enforce strict definition and casing, whether to use the security of the user who is defining the procedure, and other such options. This is an optional step, and we can continue with the default values here.\nIn the next tab i.e., the Parameters tab, we can optionally select the system parameters and set their value. When we click on the button to add a new system parameter, we can select from the list of system parameters and set the desired values. As this is an optional step, we will skip configuring any system parameters and will continue with the next tab.\nIn the Security tab, we can grant privileges to different users and roles. If we expand the grantee drop-down, we will be able to view the list of users and roles as shown below. The privileges section allows setting the type of privilege to be granted to the selected user or role. For now, we will skip configuring options in this tab and navigate to the SQL tab.\nAfter all these configurations, the SQL code to generate the procedure would look as shown below. Click on the Save button to execute this code and create a new procedure as shown below.\nOnce the procedure is created, it would be listed in the browser pane under the procedures section as shown below. Now that the procedure is in place, we can execute this procedure to test whether it works as expected. Right-click on the procedure and select the execute script option to generate the code to execute this procedure. It would generate the code as shown below. We must provide the parameter values that we want to use with the procedure. Change the values of the input parameters as shown below and then click on the Execute button to invoke the procedure.\nOnce the procedure is executed successfully, we can query the actual table to verify whether the procedure resulted in the addition of a new record in the employee table. If everything works as expected, then we should be able to find the new record in the table as shown below.\nIn this way, we can use the graphical interface of pgAdmin to create and maintain procedures in a structured manner.\nIn this article, we started with a setup of Azure Database for PostgreSQL instance, connected to it using pgAdmin, and created a basic table with few records. We learned the syntax of creating new procedures and then developed a parameterized procedure to insert records in a new table.\n- Introduction to the SQL Standard Deviation function - April 21, 2023\n- A quick overview of MySQL foreign key with examples - February 7, 2023\n- Overview of the SQL Median function - January 4, 2023']	['<urn:uuid:9b3d3268-dece-41d8-97ef-2c7edebfc2fe>']	factoid	with-premise	concise-and-natural	distant-from-document	single-doc	expert	2025-05-12T21:52:51.670698	9	23	1307
49	How does the cell membrane control chemical signals from outside the cell, and what types of clinical trials would test treatments targeting these processes?	Cell membranes are dynamic systems that control chemical signals through a process where external signals (ligands) bind to surface receptors, activating the PI3KC2A enzyme which then enables the uptake of these signals into the cell interior. New treatments targeting these processes would be tested through a series of clinical trials: Phase I trials to determine safe dosing, Phase II trials to assess if the treatment works, Phase III trials to compare it with standard treatments, and Phase IV trials to monitor long-term safety after approval.	['The uncovering of the molecular acrobatics of a key cell enzyme could lead to new treatments for cancer and metabolic diseases such as obesity and diabetes.\nThe cell enzyme is called PI3KC2A, and though scientists knew that it controlled many crucial cell functions, they remained unsure of the detailed structural mechanisms.\nOne thing they did know was that the enzyme controls what occurs at cell membranes when they receive external signals.\nThey also knew that it controls how the signals affect vital processes inside the cell.\nThese processes regulate, among other things, how cells grow, divide, and differentiate.\nNow, a new paper that features in the journal Molecular Cell describes for the first time how the cell enzyme changes from an inactive state inside the cell to an active state in the cell membrane.\nThe researchers, from the Leibniz-Forschungsinstitut für Molekulare Pharmakologie (FMP) in Berlin, Germany, together with colleagues at the University of Geneva in Switzerland, have been investigating PI3KC2A for some time.\nTheir new work reveals previously unknown facts about a crucial cell mechanism called “receptor uptake.” Disruptions of processes involving this mechanism are implicated in diseases such as cancer, diabetes, and other metabolic disorders.\nOne of the senior study authors, Prof. Volker Haucke, of the FMP, says that their findings “may provide a direct target for therapies.”\nCell membranes do a lot more than hold cell contents together. If that was all they did, they would be no more than inert skins; but a closer look reveals that they are dynamic systems that tightly control the passage of chemicals in and out of the cell.\nThe structure of a cell membrane has been described as a “\nLipids, which are fat-like molecules, are also active in the permeability process. They function as “molecular switches” for cascades of chemical signals that get switched on inside cells. Many of these cascades control essential functions such as cell growth, division, and differentiation.\nEnzymes such as PI3KC2A have a role to play in the production of the lipids that act as molecular switches. Therefore, finding ways to target them could lead to drugs that can intervene in these processes.\nCell differentiation, for instance, is crucial for the formation of new blood vessels, or angiogenesis, which is a key step in tumor growth.\nIn previous work, the scientists had already discovered much about the structural and cell biology of the processes involving PI3KC2A, including its role in receptor uptake.\nThey had established, for instance, that ligands, or external chemical signals, from outside the cell stimulate the enzyme by binding to surface proteins called receptors. Such ligands include insulin and growth factors that trigger signaling cascades inside cells.\nOnce activated, PI3KC2A enables a process called endocytosis in which little bags, or vesicles, carry the “ligand-bound receptors” into the cell interior.\nOnce inside the cell, the ligand-bound receptors trigger the signaling cascades that control crucial cell functions.\nThe new study is significant because it reveals the detailed changes that PI3KC2A undergoes at each step of this process.\nProf. Haucke explains that one of the things that they discovered is that when the cell enzyme, or kinase, is inactive and resting inside the cell, it appears “rolled up looking as if it had wrapped its ‘arms’ around itself.”\nHe and his colleagues also found that the enzyme only becomes active when two components of the cell membrane are in the same place at the same time.\n“When this happens,” he says, “the kinase unfolds its ‘arms,’ and each ‘arm’ binds to one of the two components.”\nA few seconds after this, the process will kick off. The enzyme starts making lots of lipid signaling molecules that then trigger “uptake of activated signaling receptors” into the cell’s interior. In turn, they set off the cascades that regulate cell growth, division, and differentiation.\nThe team now plans to identify candidate molecules for drug developers to take further.\n“For the first time, we have a handle on a mechanism, which may eventually enable us to alter PI3KC2A lipid kinase activity.”\nProf. Volker Haucke', 'Clinical Trials for Cancer\nWhat are clinical trials?\n“The treatments we use today were discovered, tested and first made available to patients in clinical trials—and the drugs that are the future of cancer treatment are in trials today. I want to emphasize that being in a clinical trial is how you get access to the next generation of cancer treatment.”\n—David Carbone, MD, Ohio State University\nClinical trials for cancer are research studies that compare the most effective known treatment for a specific type or stage of cancer with a new approach. This can be a new drug, or combination of drugs or a different way of using established therapies. There are trials that involve new approaches to surgery and radiation therapy. There are clinical trials for every type of cancer. While many trials focus on late stage disease, there are also trials to prevent cancer, improve early diagnosis, stop the cancer from coming back, reduce side effects or improve quality of life.\nCancer clinical trials are:\n- The engine that drives progress in cancer treatment\n- The only way to find out if a new drug or treatment is better than the existing standard of care\n- In many instances, the only way for a patient to get a new or experimental approach before it is approved for general use\n- A proven way for people facing cancer to get high quality treatment and care\n- A chance to benefit from a new therapy or approach\n- An opportunity to help future cancer patients and contribute to research\n- A way to be involved in the frontlines of research to advance cancer care\nClinical trials for cancer—phases\nPHASE I: Once a drug is approved for human studies, it is tested in a small trial to determine the optimal safe dose. Phase I studies often involve patients with different kinds of cancer, or more recently, a single genetic change.\nPHASE II: If a drug can be given safely to people on the Phase I trial, it is tested in a Phase II study. These are larger studies, usually for one or more specific types and stages of cancer. The goal of Phase II studies is to both determine the optimal dosing and provide an early assessment of whether the drug works.\nPHASE III: These trials take place after a drug has shown good results in earlier studies. They are large studies, often involving hundreds, or even thousands of patients, in multiple centers in the United States and/or abroad. Patients on Phase III trials have specific types and stages of cancer. Many Phase III trials are randomized—meaning that patients are randomly assigned to receive either the new treatment or the established standard of care. These trials are designed to provide definitive evidence to support FDA approval of the drug or agent for use in the public.\nPHASE IV: These trials take place after a drug is approved and are often called post-marketing trials. The goal is to make sure that no safety or other concerns come up after a drug is approved that may not have been seen in the pre-approval trials. It is important to follow patients for a number of years to determine if there are any long-term side effects or other issues that affect the way the treatment is used.\nWhen should I think about a clinical trial?\n“I was diagnosed with a rare and pretty nasty sarcoma. My doctor suggested that I go on a trial to see if chemotherapy before the surgery and radiation helped shrink the tumor and reduce the risk it would come back. I knew that meant I would have more aggressive treatment and more side effects—with no guarantee that it would make any difference. But I also knew the odds if the cancer came back. I chose the trial.”\n—Doug, sarcoma patient\nA cancer clinical trial may be a treatment option for anyone who has received a cancer diagnosis. The best way to learn more about your specific options is to talk to your doctor, nurse or other members on your health care team. If your doctor is not involved in doing clinical research or does not explain your options for a clinical trial, consider getting a second opinion.\nA clinical trial can be particularly important if:\n- You are diagnosed with cancer that has spread or your cancer recurs or comes back after your primary treatment\n- You are interested in getting access to the newest cancer treatments\n- You have a rare or difficult to treat cancer\n- Your treatment that was working has stopped working\n- Your cancer requires that you remain on treatment for a long period of time or the rest of your life\n- You are interested in helping other cancer patients today and tomorrow by advancing cancer treatment\n- Your treatment involves significant side effects and the trial is designed to reduce the impact of these on your life\nInformation provided by Cancer Support Community.']	['<urn:uuid:063811ec-bf27-49a4-9188-ca956c3d6501>', '<urn:uuid:29924fa1-8432-4f3e-95a5-b64f29191794>']	factoid	direct	verbose-and-natural	similar-to-document	multi-aspect	novice	2025-05-12T21:52:51.670698	24	85	1504
50	types meditation practices managing depression symptoms loving kindness transcendental yoga	Several types of meditation can help manage depression: Loving-Kindness Meditation (Metta) involves directing positive thoughts and compassion towards oneself and others; Transcendental Meditation (TM) uses a mantra to quiet the mind and has been found to reduce depression symptoms; Yoga combines physical postures, breath control, and meditation to promote relaxation and improve mood.	['Depression is a mental health disorder characterized by persistent feelings of sadness, hopelessness, and a lack of interest or pleasure in activities. It goes beyond temporary feelings of sadness that everyone experiences from time to time. Depression affects how a person thinks, feels, and behaves, and it can lead to a variety of emotional and physical problems.\nThe symptoms of depression can vary from person to person, but some common signs include:\nPersistent sadness or emptiness\nLoss of interest or pleasure in activities once enjoyed\nChanges in appetite and weight (either significant weight loss or weight gain)\nDifficulty sleeping or sleeping too much\nRestlessness or irritability\nFatigue or loss of energy\nFeelings of worthlessness, guilt, or hopelessness\nDifficulty concentrating or making decisions\nThoughts of death or suicide\nDepression can be caused by a combination of genetic, biological, environmental, and psychological factors. It is a complex condition, and the exact causes can vary from person to person. Some common risk factors for depression include a family history of the disorder, certain medical conditions, traumatic life events, chronic stress, and certain medications.\nIt is important to note that depression is a treatable condition. Treatment options may include psychotherapy (such as cognitive-behavioral therapy), medication (such as antidepressant medications), or a combination of both. Lifestyle changes, such as regular exercise, maintaining a healthy diet, and getting enough sleep, can also be beneficial in managing depression.\nMeditation can be a helpful tool in managing and reducing symptoms of depression. Here are some of the benefits that meditation can offer:\nStress reduction: Meditation has been shown to reduce stress levels by activating the body’s relaxation response. This can help alleviate the physical and emotional symptoms of stress that often accompany depression.\nImproved emotional well-being: Regular meditation practice can enhance emotional well-being by increasing positive emotions and reducing negative emotions. It can promote feelings of calmness, inner peace, and contentment, which can be particularly beneficial for individuals struggling with depression.\nIncreased self-awareness: Meditation cultivates a sense of self-awareness, allowing individuals to observe their thoughts and emotions without judgment. This heightened self-awareness can help identify negative thought patterns and behaviors that contribute to depression, enabling individuals to develop healthier coping mechanisms.\nEnhanced focus and concentration: Depression often affects concentration and cognitive function. Meditation practices, such as mindfulness meditation, can improve focus and concentration by training the mind to stay present and redirect attention to the present moment. This can help counteract the cognitive difficulties associated with depression.\nRegulation of mood: Regular meditation practice has been shown to positively impact mood regulation. It can help individuals develop skills to navigate and regulate their emotions, reducing the intensity and duration of depressive episodes.\nImproved sleep: Depression can disrupt sleep patterns, leading to insomnia or excessive sleep. Meditation techniques, such as progressive muscle relaxation or guided imagery, can promote relaxation and better sleep quality, which can indirectly alleviate depressive symptoms.\nIncreased self-compassion: Depression often involves self-critical thoughts and feelings of low self-worth. Meditation practices, such as loving-kindness meditation, can foster self-compassion and self-acceptance. This can counteract the negative self-perception common in depression and promote a more positive relationship with oneself.\nIt’s important to note that while meditation can be a helpful adjunct to treatment for depression, it is not a substitute for professional help. If you’re experiencing symptoms of depression, it’s essential to consult with a mental health professional for a comprehensive evaluation and appropriate treatment plan.\nIn addition to mindfulness meditation, which is commonly associated with depression, there are several other types of meditation that can be beneficial for managing depression. Here are a few examples:\nLoving-Kindness Meditation (Metta): Loving-kindness meditation involves directing positive thoughts, well-wishes, and compassion towards oneself and others. It can help cultivate feelings of love, kindness, and empathy, which can counteract the negative self-perception and social isolation often experienced in depression.\nTranscendental Meditation (TM): TM is a form of meditation that involves the use of a mantra, a specific word or sound, repeated silently to oneself. It aims to quiet the mind and access deeper levels of consciousness. TM has been found to reduce symptoms of depression and anxiety and improve overall well-being.\nYoga and Yoga Nidra: Yoga combines physical postures, breath control, and meditation to promote relaxation, flexibility, and mental clarity. It can help alleviate symptoms of depression by reducing stress, improving mood, and enhancing self-awareness. Yoga Nidra, also known as “yogic sleep,” is a guided meditation technique that promotes deep relaxation and can be particularly helpful for individuals with depression-related sleep disturbances.\nBreath-focused Meditation: This type of meditation involves focusing on the breath as a point of concentration. It can help bring the mind to the present moment, reduce rumination on negative thoughts, and induce a state of calmness and relaxation.\nBody Scan Meditation: Body scan meditation involves systematically directing attention to different parts of the body, noticing physical sensations without judgment. It can promote relaxation, body awareness, and release tension, which can be beneficial for individuals with depression-related physical symptoms or somatic complaints.\nWalking Meditation: Walking meditation involves bringing mindfulness and awareness to the experience of walking. It can be done indoors or outdoors and can help promote relaxation, grounding, and a sense of connection with the body and the environment. Walking meditation can be particularly useful for individuals who find it challenging to sit still for traditional meditation practices.\nIt’s important to remember that different types of meditation work for different individuals, and it may be helpful to explore and experiment with different techniques to find what resonates best with you. It is also recommended to learn meditation techniques from qualified instructors or through guided meditation resources such as books, apps, or online programs.']	['<urn:uuid:76f248bd-2697-43e4-b96e-77d227b79446>']	factoid	direct	long-search-query	similar-to-document	single-doc	expert	2025-05-12T21:52:51.670698	10	53	942
51	How do residential architectural visualizations differ in 2D versus 3D?	In 2D visualizations, residential designs require drawing key poses and scenes with puppet pins for bendable points in the character's composition, allowing for more flexibility in adjustments. For 3D residential visualization, the process involves modeling and rigging to create a complete skeletal structure, enabling developers to show technical aspects like light effects, engineering implications, and cozy living spaces with subtle details. While 2D work is more evenly distributed throughout the process, 3D requires more upfront work but allows easier repurposing for future visualizations.	['5 Types Of Architectural Renderings You Can Use For Your Projects\nArchitectural rendering has come a long way over the years. Like almost everything else, there has been incredible progress in the quality and creativity that goes into the practice.\nConsequently, with the different architectural rendering types that are now available, there has also been a scramble on the market to use the technology as a differentiating factor and gain a competitive edge.\nReal estate developers want to get clients to buy into projects, quite literally. They need a compelling argument to communicate the potential of undertakings that aren’t quite fully done. Realtors need to somehow demonstrate the quality of their buildings, answer any questions potential clients might have, and put any doubts to rest.\nWhatever the goal, architectural renderings offer advantages that are very beneficial. So what are the various architectural rendering types? Let’s take a look at the rendering options available and what variation would best suit your project.\nTypes of Architectural Renderings\nDepending on the nature of your project and your objectives, you will have to decide between exterior 3D visualization, interior rendering, or a combination of the two. While the names are pretty self-explanatory, it is important to understand what role they play, what specific benefits they bring, and what characteristics make them stand out. With a good understanding of the nature of these types of architectural renderings, you are more likely to pick the right options for different aspects of your projects, and get the best out of those options.\n#1 Exterior 3D Visualization\nExterior 3D visualization, as the name would suggest, provides a view of your building or project from the outside. You can use this type of architectural renderings from various perspectives to convey different impressions of your structures.\nFor example, a bird’s eye view is a great way to show your building in its entirety. It allows you to display your building in its context and demonstrate how different elements might interact with your designs. You can show public infrastructure that might be relevant to your project, or just show the aesthetics of your design within its surroundings. You can also use street views, the worm’s eye view to show the building dominating your image, or close-ups of important details you may want to show.\nWe can split exterior 3D visualization further down into two branches: residential and non-residential.\n- Residential Exterior CGI\nResidential exterior renderings are usually used to provide a first impression to the client. So you will most likely see them on posters, brochures, project websites, and different promotional materials.\nHowever, the value of exterior CGI goes beyond just persuasion. Developers and architects can see certain technical aspects of the project beforehand. For example, the engineering implications of the terrain, or how light will affect the building – an extremely important consideration for tenants.\n- Non-Residential Exterior CGI\nThe same is true for commercial projects. Exterior CGI can produce the wow factor that impresses clients and convinces them that your project is worth investing in. And on the flipside, this architectural rendering type can also be used to manage expectations, to clearly communicate the vision with its advantages and technical limitations, to all the stakeholders involved.\nColors, materials, juxtaposition with other buildings, scale, and other aspects of construction projects that are hard to mentally visualize, can all be shown at a glance to all the parties involved so that everyone has a clear picture of what the end goal is.\n#2 Interior Rendering\nWhile external visualization shows off the structure with the architect’s vision, interior rendering shows the personality and character that can be given to the interior spaces of the structure. It is a great way to show a client what is possible, or to agree on an interior design with them. Like exterior CGI, it can also be used to streamline communication and ensure that everyone agrees on one established design before any work commences.\n- Residential Interior 3D Visualization\nFloor plans and construction drawings don’t do a great job convincing clients that this really is their next home, or those in the hospitality industry that their project, once complete, will be attractive to guests. That level of nuance, the emotional response that comes from a cozy living space, can only be provided by well-crafted 3D visualization. Great visualizations enable you to depict living spaces with subtle details like light, with all its reflections and interactions with different objects in the rooms, and even furniture with different textures, designs and elements.\n- Non-Residential Interior 3D Visualization\nFor commercial projects, 3D visualization is crucial to the marketing process in different phases of the project. Such architectural renderings can be used to portray measurements, provide scale and proportions, it can show lighting, different facilities in the building and help to influence the final purchase decision.\n#3 CG 360 Panoramas\nComputer Graphics (CG) panoramas bring all the benefits of architectural rendering but also an additional dimension of immersion. A client can experience – not just see – the building in a 3D world, not just two-dimensional pictures. CG 360 Panoramas offer the opportunity to pan around a scene and zoom into specific areas of interest.\n#4 3D Animation\n3D animation is used to create a lifelike video of a project. Sometimes real footage is combined with computer-generated graphics to create truly stunning scenes. For instance, a 3D model of a building can be grafted into aerial footage of a city to show what a building will look like in context, once complete. Or the reverse can be done. Real videos of people can be inserted into computer-generated scenes to create a real-life atmosphere and show what interior spaces might be like.\n#5 Virtual Tours\nYou can swap the monitor or screen for a VR headset and jump into the scene yourself, explore buildings, walk up to elements of interest and investigate the finer details. While animation is guided, showing the user what the architects want them to see, how, and in what order, virtual tours hand control to the viewer and allow them the freedom to literally explore the structure in a virtual space.\nHow To Choose The Right Architectural Rendering Type\nWith the technology available today, we are spoiled for choice when it comes to options for architectural rendering. What you choose will come down to your specific goals, the size of your project, and the resources available to you, and this includes time. Animations and virtual tours take a lot longer to plan and actualize. They consequently also have a higher price tag. But then again, they provide an experience that 2D renders simply cannot match. Your choice will inevitably be a balancing act between your resources, your needs and how far you are willing to go to impress your clients.\nabout the author', '2D and 3D Computer animation: Know the Distinctions\nIf you think you’ll require unique glasses to check out 3D computer animation, this post is for you.\nKidding (kind of).\nYet typically, individuals are puzzled by the distinctions between 2D as well as 3D animation, and specifically the processes involved in developing each kind. There isn’t a much better type of computer animation, but it is very important to recognize how they each job to establish which is a much better suitable for your timeline and also objectives.\nEach can serve a wide variety of objectives as well as styles, all depending upon the vision as well as goals of the project. While 2D computer animation appears flatter, it could still result in a tidy, advanced look. Likewise, it could also be developed with an one-of-a-kind, detailed look that can work well for several jobs and also brand names. 3D animation gives you a complete series of activity once a personality is rigged– which we’ll clarify in a moment. You can in fact see around the last character, which offers it that crisp, life-like look.\nWe developed the adhering to 2D computer animation for the New york city Council on Trouble Gaming.\nJeff Fugelsang, an activity graphics designer on Overit’s Movement group, compares the comparison between 2D and also 3D animation to the distinction in between illustration as well as sculpting. Either type can be personalized to fit a variety of designs and also functions, so choosing a strategy actually comes down to the client’s preferences, goals, timeline and also budget.\nJust how 2D as well as 3D Computer animations Are Produced\nThe procedure for developing both 2D and 3D animations typically start the same way, then the course forks.\nRegardless of the animation style, animators will initially learn more about the vision and objectives that the customer has going into the job. They will create principles then showed storyboards to bring these ideas to life. When a storyboard is accepted, it gets in the animatic phase, relocating the pictures right into the suitable computer animation software– which will be different for 2D as well as 3D. The animatic is where sound clips and voiceover are added as well as motion is introduced to the characters. This is where things get vastly various.\nWhen a 2D personality is drawn, the animator has to highlight the key presents and also scenes, consisting of a drawing in every structure, with 24 frameworks per second being the requirement. Depending upon the speed of the computer animation, a personality drawing can continue to be still for a few frameworks each time. The same is hardly ever real for 3D computer animations, which have to remain in near-constant movement to keep their realistic appearance. It could be subtle, yet living points blink, take a breath as well as shift their pose, so reasonable 3D animations must do the very same. After the animatic is developed, the animator after that functions to version and also gear the character. Modeling is essentially the process in which the character is constructed, and afterwards the rigging develops the personality’s skeletal structure beneath that permits animators to regulate the motions of the personality. It’s a little bit like creating a creature.\nTime Investments for Developing 2D as well as 3D Animations\nTiming will be totally dependent upon the task, and either could be produced quicker– or gradually- based upon numerous imaginative factors and customer requests. Producing a 3D animation is usually more time-intensive in advance, with the need to gear the personality. It’s a highly-detailed procedure, once it’s finished, the animators could re-use the character and change its poses over as well as over once again– which is a benefit for personalities that will have a long life-span with their brand (i.e. The Geico gecko) or for us, the Catseye personalities we created at Overit. This is also why 3D computer animations are much more time-efficient and affordable for animated attributes from business such as Pixar.\n2D computer animations could be created quicker for a certain video, and also the work of the task is usually extra evenly dispersed throughout the procedure. A different variation of personality rigging happens here, by using the ‘puppet pin’ tool to earn bendable factors in the personality’s composition (elbows and knees, for instance). A set up 2D character can be received stages throughout the procedure, allowing for even more minor adjustments and flexibility (make him elevate both arms, not simply one!).\nDeciding Between 2D as well as 3D Computer animation.\nProducing a 3D computer animation requires a solid understanding of the task in between the customer as well as animators, as a great deal of job will certainly take place in the first rigging process. The procedure where the computer system assembles all the computer animation work into video clip layout usually takes much longer, too. However, once it’s done, it could be repurposed for future animations reasonably conveniently.\nIn order to produce a new video clip with a 2D animation, you may have to draw brand-new angles and placements for the personality gears, which will certainly take even more time if the needed angles as well as creature pins haven’t been created in advance. If you could intend ahead from the beginning of your first project, you could connect these prospective requirements for the future with your computer animation team. They could after that work to create the proper angles as well as puppet pins ahead of time– for this project as well as future projects, so adjustments are easier to apply for future animations.\nJust how do you decide? A lot depends upon your visual preferences, yet likewise take into consideration questions such as:.\nDo we plan on re-using this computer animated character in future videos or advertisements?\nAre we searching for a (potentially) irreversible mascot or icon for our brand?\nDo we intend to experiment with different characters in the future to target other segments?\nExactly what’s our timeline?\nWhat’s our budget?\nWhere will these animations be displayed?\nShould our computer animation be character-based, or information/process-based?\nIt is essential to have a great vision of the final video clip you desire to create, the target market you intend to reach with it and also the goals you desire to accomplish, as they will greatly influence the choice to produce 2D and/or 3D computer animations. Once you as well as your animation group both recognize the designated objective and objectives for placing the computer animation to make use of, it’ll come to be easier to choose which techniques to embrace.']	['<urn:uuid:f9dcc060-af21-4991-ac44-5c06c1922f63>', '<urn:uuid:f08c2866-1f29-4239-8d68-0e73617f131f>']	open-ended	direct	concise-and-natural	distant-from-document	multi-aspect	expert	2025-05-12T21:52:51.670698	10	83	2233
52	What are the key differences between the cognitive symptoms experienced in schizophrenia compared to the mental state changes that occur during bipolar medication-induced mania?	In schizophrenia, cognitive symptoms include problems with executive functioning (difficulty understanding information and making decisions), trouble focusing, and issues with working memory. In contrast, medication-induced mania from antidepressants in bipolar disorder is characterized by abnormally elevated mood, increased energy and activity, decreased need for sleep, unusual talkativeness, and racing thoughts. While schizophrenia's cognitive symptoms are often subtle and difficult to recognize, manic episodes triggered by antidepressants typically present as more obvious behavioral changes and mood elevation.	['What Is Schizophrenia?\nSchizophrenia is a chronic, severe, and disabling brain disorder that has affected people throughout history.\nPeople with the disorder may hear voices other people don’t hear. They may believe other people are reading their minds, controlling their thoughts, or plotting to harm them. This can terrify people with the illness and make them withdrawn or extremely agitated.\nPeople with schizophrenia may not make sense when they talk. They may sit for hours without moving or talking. Sometimes people with schizophrenia seem perfectly fine until they talk about what they are really thinking.\nFamilies and society are affected by schizophrenia too. Many people with schizophrenia have difficulty holding a job or caring for themselves, so they rely on others for help.\nTreatment helps relieve many symptoms of schizophrenia, but most people who have the disorder cope with symptoms throughout their lives. However, many people with schizophrenia can lead rewarding and meaningful lives in their communities. Researchers are developing more effective medications and using new research tools to understand the causes of schizophrenia. In the years to come, this work may help prevent and better treat the illness.\nExperts think schizophrenia is caused by several factors.\nGenes and environment. Scientists have long known that schizophrenia runs in families. The illness occurs in 1 percent of the general population, but it occurs in 10 percent of people who have a first-degree relative with the disorder, such as a parent, brother, or sister. People who have second-degree relatives (aunts, uncles, grandparents, or cousins) with the disease also develop schizophrenia more often than the general population. The risk is highest for an identical twin of a person with schizophrenia. He or she has a 40 to 65 percent chance of developing the disorder.\nWe inherit our genes from both parents. Scientists believe several genes are associated with an increased risk of schizophrenia, but that no gene causes the disease by itself. In fact, recent research has found that people with schizophrenia tend to have higher rates of rare genetic mutations. These genetic differences involve hundreds of different genes and probably disrupt brain development.\nIn addition, it probably takes more than genes to cause the disorder. Scientists think interactions between genes and the environment are necessary for schizophrenia to develop. Many environmental factors may be involved, such as exposure to viruses or malnutrition before birth, problems during birth, and other not yet known psychosocial factors.\nDifferent brain chemistry and structure. Scientists think that an imbalance in the complex, interrelated chemical reactions of the brain involving the neurotransmitters dopamine and glutamate, and possibly others, plays a role in schizophrenia. Neurotransmitters are substances that allow brain cells to communicate with each other. Scientists are learning more about brain chemistry and its link to schizophrenia.\nStudies of brain tissue after death also have revealed differences in the brains of people with schizophrenia. Scientists found small changes in the distribution or characteristics of brain cells that likely occurred before birth. Some experts think problems during brain development before birth may lead to faulty connections. The problem may not show up in a person until puberty. The brain undergoes major changes during puberty, and these changes could trigger psychotic symptoms. Scientists have learned a lot about schizophrenia, but more research is needed to help explain how it develops.\nWho is at risk?\nAbout 1% of Americans have this illness.\nSchizophrenia affects men and women equally. It occurs at similar rates in all ethnic groups around the world. Symptoms such as hallucinations and delusions usually start between ages 16 and 30. Men tend to experience symptoms a little earlier than women. Most of the time, people do not get schizophrenia after age 45. Schizophrenia rarely occurs in children, but awareness of childhood-onset schizophrenia is increasing.\nIt can be difficult to diagnose schizophrenia in teens. This is because the first signs can include a change of friends, a drop in grades, sleep problems, and irritability—behaviors that are common among teens. A combination of factors can predict schizophrenia in up to 80% of youth who are at high risk of developing the illness. These factors include isolating oneself and withdrawing from others, an increase in unusual thoughts and suspicions, and a family history of psychosis. In young people who develop the disease, this stage of the disorder is called the “prodromal” period.\nSigns & Symptoms\nThe symptoms of schizophrenia fall into three broad categories: positive symptoms, negative symptoms, and cognitive symptoms.\nPositive symptoms are psychotic behaviors not seen in healthy people. People with positive symptoms often “lose touch” with reality. These symptoms can come and go. Sometimes they are severe and at other times hardly noticeable, depending on whether the individual is receiving treatment. They include the following:\nHallucinations are things a person sees, hears, smells, or feels that no one else can see, hear, smell, or feel. “Voices” are the most common type of hallucination in schizophrenia. Many people with the disorder hear voices. The voices may talk to the person about his or her behavior, order the person to do things, or warn the person of danger. Sometimes the voices talk to each other. People with schizophrenia may hear voices for a long time before family and friends notice the problem.\nOther types of hallucinations include seeing people or objects that are not there, smelling odors that no one else detects, and feeling things like invisible fingers touching their bodies when no one is near.\nDelusions are false beliefs that are not part of the person’s culture and do not change. The person believes delusions even after other people prove that the beliefs are not true or logical. People with schizophrenia can have delusions that seem bizarre, such as believing that neighbors can control their behavior with magnetic waves. They may also believe that people on television are directing special messages to them, or that radio stations are broadcasting their thoughts aloud to others. Sometimes they believe they are someone else, such as a famous historical figure. They may have paranoid delusions and believe that others are trying to harm them, such as by cheating, harassing, poisoning, spying on, or plotting against them or the people they care about. These beliefs are called “delusions of persecution.”\nThought disorders are unusual or dysfunctional ways of thinking. One form of thought disorder is called “disorganized thinking.” This is when a person has trouble organizing his or her thoughts or connecting them logically. They may talk in a garbled way that is hard to understand. Another form is called “thought blocking.” This is when a person stops speaking abruptly in the middle of a thought. When asked why he or she stopped talking, the person may say that it felt as if the thought had been taken out of his or her head. Finally, a person with a thought disorder might make up meaningless words, or “neologisms.”\nMovement disorders may appear as agitated body movements. A person with a movement disorder may repeat certain motions over and over. In the other extreme, a person may become catatonic. Catatonia is a state in which a person does not move and does not respond to others. Catatonia is rare today, but it was more common when treatment for schizophrenia was not available.\nNegative symptoms are associated with disruptions to normal emotions and behaviors. These symptoms are harder to recognize as part of the disorder and can be mistaken for depression or other conditions. These symptoms include the following:\n- “Flat affect” (a person’s face does not move or he or she talks in a dull or monotonous voice)\n- Lack of pleasure in everyday life\n- Lack of ability to begin and sustain planned activities\n- Speaking little, even when forced to interact.\nPeople with negative symptoms need help with everyday tasks. They often neglect basic personal hygiene. This may make them seem lazy or unwilling to help themselves, but the problems are symptoms caused by the schizophrenia.\nCognitive symptoms are subtle. Like negative symptoms, cognitive symptoms may be difficult to recognize as part of the disorder. Often, they are detected only when other tests are performed. Cognitive symptoms include the following:\n- Poor “executive functioning” (the ability to understand information and use it to make decisions)\n- Trouble focusing or paying attention\n- Problems with “working memory” (the ability to use information immediately after learning it)\nClick here to learn about our current schizophrenia studies.', 'Antidepressants can trigger mania in people with bipolar disorder. If antidepressants are used at all, they should be combined with a mood stabilizer such as lithium or valproic acid. Taking an antidepressant without a mood stabilizer is likely to trigger a manic episode. Antidepressants can increase mood cycling.\nAre antidepressants bad for bipolar disorder?\nUsing antidepressant medication alone to treat a depressive episode is not recommended in people with bipolar I disorder. The drugs may flip a person, particularly a person with bipolar I disorder, into a manic or hypomanic episode. Hypomania is a more subdued version of mania.\nHow do antidepressants affect people with bipolar?\nAlso using antidepressants alone may trigger a manic or hypomanic episode in some people with bipolar disorder. Antidepressants alone also may lead to or prolong rapid cycling. In rapid cycling, a person may recover more quickly from depression but then experience mania followed by another episode of depression.\nWhich antidepressant is best for bipolar depression?\nTreating bipolar depression with antidepressants remains a popular option in clinical practice and published guidelines. Most clinicians choose the drug or class of drugs, usually selective serotonin reuptake inhibitors and bupropion, that is most effective and best tolerated.\nCan antidepressants send you into mania?\nAll antidepressants were associated with an increased incidence of mania/bipolar disorder (unadjusted HR>1.0 for all antidepressants) with incidence rates ranging from 13.1 (TCAs) to 19.1 (trazodone) per 1000 person-years.\nWhat is the best mood stabilizer for bipolar?\nLamotrigine (Lamictal) may be the most effective mood stabilizer for depression in bipolar disorder, but is not as helpful for mania. The starting dose of lamotrigine should be very low and increased very slowly over four weeks or more.\nWhat is the safest medication for bipolar disorder?\nLurasidone (Latuda) and Lamotrigine (Lamictal)\nBut lamotrigine is the better tolerated option, with few of the adverse effects that matter most to patients: weight gain, fatigue, sexual dysfunction, and long-term medical risks. Lamotrigine is better at preventing depression than it is at treating it.\nCan you be bipolar and not take medication?\n“Around half of people with bipolar disorder don’t take their medication which can lead to a relapse of symptoms. And this can have a knock-on impact with problems at work, strained relationships with family and friends, hospitalisation, and an increased risk of suicide.\nWhat happens if you take bipolar medication without being bipolar?\nTaking an antidepressant without a mood stabilizer is likely to trigger a manic episode. Antidepressants can increase mood cycling. Many experts believe that over time, antidepressant use in people with bipolar disorder has a mood destabilizing effect, increasing the frequency of manic and depressive episodes.\nCan you have a bipolar episode while on medication?\nYou may not be able to totally prevent bouts of mania or depression. Even people who always take their medication and take care of their health can still have mood swings from time to time.\nDoes Bipolar get worse as you age?\nBipolar may worsen with age or over time if this condition is left untreated. As time goes on, a person may experience episodes that are more severe and more frequent than when symptoms first appeared.\nHow can I fix my Bipolar without medication?\n10 Alternative Treatments for Bipolar Disorder\n- Fish oil.\n- Rhodiola rosea.\n- St. John’s wort.\n- Calming techniques.\nHow do you beat bipolar depression?\n10 Tips for Overcoming Bipolar Disorder\n- Be An Active Participant In Your Treatment. …\n- Go To Therapy. …\n- Closely Observe Your Mood and Symptoms. …\n- Don’t Isolate Yourself. …\n- Develop a Routine. …\n- Focus on Diet and Exercise. …\n- Reduce Your Stress. …\n- Avoid Drugs and Alcohol.\nCan you have a manic episode and not be bipolar?\nWhat are mania and hypomania? Mania and hypomania are symptoms that can occur with bipolar disorder. They can also occur in people who don’t have bipolar disorder.\nHow do you tell if you’re in a manic episode?\nBoth a manic and a hypomanic episode include three or more of these symptoms:\n- Abnormally upbeat, jumpy or wired.\n- Increased activity, energy or agitation.\n- Exaggerated sense of well-being and self-confidence (euphoria)\n- Decreased need for sleep.\n- Unusual talkativeness.\n- Racing thoughts.\nHow long does bipolar mania last?\nUntreated, an episode of mania can last anywhere from a few days to several months. Most commonly, symptoms continue for a few weeks to a few months. Depression may follow shortly after, or not appear for weeks or months. Many people with bipolar I disorder experience long periods without symptoms in between episodes.']	['<urn:uuid:64c705b8-d877-4e19-a924-2e28c9cf866f>', '<urn:uuid:a51aed1d-f01b-44e9-b82e-ee71635daf7f>']	open-ended	direct	verbose-and-natural	distant-from-document	comparison	expert	2025-05-12T21:52:51.670698	24	76	2166
53	why do some people get anxious so easily	From an evolutionary perspective, some people have a lower 'signal detection threshold,' meaning their brain's amygdala activates the fight or flight response more easily. While this can lead to anxiety disorders, it actually provided a survival advantage to our ancestors - similar to how a sensitive smoke detector may give false alarms but better protects against real threats. Those with more active anxiety responses were likely better prepared for dangers like predators or adversaries, leading this trait to be passed down through generations despite its negative effects.	['As many of you know, and experience regularly, the downsides of mental illness can be exhausting and extremely detrimental to well-being. Many of you have regularly experienced the impossible tasks of arising from bed on a dreary morning, having no escape from an overwhelmingly anxious situation, or containing a dangerous manic state. Obviously, these are not easy occurrences to handle or control. Obviously, mental health issues have plagued enough people where they are worthy of careful observation.\nTo gain more knowledge of mental health and its discrepancies, we must ask two questions- “How?” and “Why?”. When we look into modern psychological pathology, the former question seems to dominate the latter. Chemicals, and a lack thereof, have been most abundantly accepted as the main reason why mental health issues physically exist. When we have an abundance or scarcity of certain chemicals in our system, we experience maladaptive mental symptoms. However, when we ignore the “Why?”, we avoid some of the most important information behind mental health.\nI was first introduced to this concept in my studies on evolutionary psychology. This field is rapidly becoming entangled within other psychological fields, such as cognitive, developmental, and social. This is simply because the concept of evolution does play a role in everything we experience, as it has essentially shaped us into everything that we are.\nThis is relevant because unlike many modern treatments to mental health problems, evolutionary approaches answer the long-term, overhanging “Why?” question. Medications and therapies can help change chemicals and cognitive distortions, but why do these problems exist in the first place? Even though the research is still not abundantly available in this rapidly developing field, there are hypotheses that exist for most mental health problems.\nLet’s start with anxiety.\nAnxiety-related disorders, ranging anywhere from generalized anxiety disorder to very specific phobias, are some of the most common mental health issues that affect us today. Almost 30% of the American population will experience symptoms associated with anxiety at some point in their lives. Anxiety has been proven to stem from a lack of the chemical GABA within us, environmental stressors anywhere from a procrastinated assignment to divorce or bankruptcy, substance abuse, genetics, and more. There are myriad ways in which anxiety can begin and can present itself. But why did this develop in us if it is so detrimental to our sense of well-being?\nMany of you have probably heard of a phenomenon within us and other species referred to as the “fight or flight” response. This is a catchy way phrasing of one of the main functions of our sympathetic nervous system, which is to determine the level of threat presented to us. Is the stimulus we are faced with threatening? If so, is it something worth standing up against, or running from? When this system is activated, we prepare for possible danger; our heart rates increase, our cortisol levels increase, we sweat more, and we experience more anxious symptoms.\nThis response is activated by a really tiny part of our brain called the amygdala. Once activated, all of the aforementioned systems will react in sort of a domino fashion. When the amygdala detects a “threat”, it will quickly flick the first domino and set off a profound chain reaction. This naturally brings us back to our original question of “Why?”, or to be more specific, what constitutes a threat, and an activation of fight or flight?\nThe answer is not surprisingly different for everyone. Everyone has a different “signal detection threshold”, a term stemming from the evolutionary concept of signal detection theory. To summarize a theory that has a bit of length to it, this concept essentially tells us that everyone has a different threshold in which the amygdala will activate our fight or flight response. Those with lower thresholds will experience more sympathetic nervous system activations, and thus, will be on guard more frequently. Those with these lower thresholds will theoretically be at a much higher risk for maladaptive anxiety, often in the form of pathological disorders.\nThose familiar with evolution know that traits adapt over time because certain genes are spread more frequently through generations due to higher reproductive success. By this principle, we have to conclude that despite the negative consequences of high anxiety levels, there must be some benefit that caused individuals with lower thresholds to reproduce more over time.\nCue Randolph Nesse’s “Smoke Detector Principle”.\nAll of us have been the victim of false alarms via our smoke detectors. Whether it be a fire drill at school or a roommate innocently boiling Ramen, there has almost certainly been a situation in all of our lives where the smoke detector went off when there was no plausible threat of a lethal fire. The result? Our ears hurting, almost inevitably followed by some swearing or complaining. The immediate sensory effects are annoying and perceptually negative. However, when we think of the ultimate purpose of a smoke detector, it makes sense to have some false alarms rather than the other end of the spectrum. What if there was an actual fire and the alarm was not triggered? We’d have a lot higher probability of becoming ash.\nNow think of our amygdala as a smoke detector. Those with overactive sympathetic nervous systems will certainly be the victims of more frequent false alarms, but what about situations in which there is an actual threat? We have to assume that our ancestors who were a bit more anxious were more prepared for a predator or an adversary fighting for food or land. This, of course, is extremely valuable in terms of survival. All of the anxiety caused by false alarms seems to be worth it for the situations in which we need the somatic effects of our sympathetic nervous systems to stay safe and in some cases, stay alive.\nTo wrap this all together, anxiety is a trait present within us for some reason. Many of us struggling with anxiety disorders often wonder why something so invasive and often crippling exists. The evolutionary answer is that this hypersensitivity to possible threats has actually kept our ancestors more safe over time.\nThe next time you find your palms drenching with sweat, your breath shortening, and the room quickly closing in on you, remember that you theoretically have a higher chance of adequately preparing yourself for a threat as opposed to a friend who has a much higher signal detection threshold. This isn’t exactly something that would prevent a panic attack for many, including myself, but when you think about it, it’s a pretty sweet silver lining, isn’t it?\nBateson, M., Brilote, B., & Nettle, D. (2011). Anxiety: An Evolutionary Approach. Can J Psychiatry Vol 56 (12), 707–715.\nHesse, R (2001). Smoke Detector Principle. Annal’s New York Academy of Sciences. Retrieved from: http://www.personal.umich.edu/~nesse/Articles/SmokeDetectorPrin-Nesse-NYAS-2000.pdf']	['<urn:uuid:f03660e1-65d5-4113-9704-7edeecc1643d>']	open-ended	direct	short-search-query	distant-from-document	single-doc	novice	2025-05-12T21:52:51.670698	8	87	1131
54	What roles did mountains play in ancient Greek and Roman culture?	Mountains in ancient Greek and Roman culture served multiple functions. They were seen as places of memory and had military functions. They were also viewed as objects of contemplation, serving as sources of inspiration while maintaining an enigmatic quality. Ancient writers like Pausanias documented varied and often contested views of mountains, as evidenced by both archaeological and topographic findings.	"['The School of Classics\' podcast series introduces recent research within the School, across a wide range of subject areas. Many podcasts are tied to recent or forthcoming book publications. Some are connected with ongoing research projects.\nOros: Mountains in Ancient Greek and Roman Culture\nProfessor Jason König\nOn 4th November 2015, Professor Jason König presented his Inaugural lecture entitled Oros: Mountains in Ancient Greek and Roman Culture to a packed audience. Deputy Principal Professor Garry Taylor introduced the lecture with some remarks on Jason’s world-leading scholarship to date and his exemplary leadership as the School’s Director of Research in the period of the last REF. Jason treated the audience to a thought-provoking examination of the role and representation of Greek and Roman mountains in ancient writers (such as Pausanias) and 19th century travellers (such as Edward Dodwell). These writers reveal that views of mountains in the ancient and modern imagination are varied and often contested. Archaeological and topographic evidence was also canvassed to reveal that Greek and Roman mountains were seen as places of memory, as having military functions, as objects of contemplation, as inspirational and as enigmatic. The lecture was beautifully illustrated and the event was a happy celebration of Professor König’s work.\nMorality, Politics and Religion in Euripidean Tragedy\nIn this lecture Jon Hesk argues that Euripides’ plays are not, as is sometimes thought, radical critiques of religious practice and belief. Rather, they stage the difficulty of moral, social and political decision-making in a world where external forces are ineluctable (and yet often hard to detect), stakes are high (and yet not always easily perceived as such), and humans have the capacity to reason between courses of action (and yet find themselves in dilemmas fuelled by emotion and conflicting moral imperatives). At the end of the lecture, the audience asked Jon lots of excellent questions and you are likely to learn as much from that discussion as from the lecture itself!\nPolitics and gender conflict in Greek drama\nDr Jon Hesk\nIn this recording of a lecture, Jon Hesk discusses Aristophanes’ Lysistrata, Euripides’ Medea and Sophocles’ Antigone. He shows how these plays’ representations of their female protagonists provided Athenian men with important food for thought concerning their own roles and responsibilities within the city and the household. In the case of Aristophanes, however, we have to distinguish between some of Lysistrata’s more serious remarks and the tenor of the play as a whole.\nPlato and Conversation\nDr Alex Long\nAlex Long talks about Plato\'s dialogues and their representation of philosophy. In Plato\'s dialogues Socrates engages his interlocutors (sophists, philosophers and his young admirers) in exchanges of short questions and answers. Plato considers why this interpersonal exchange is advantageous in inquiry and in teaching, and how it can be adapted in order to persuade others, but he also introduces an alternative to it: self-questioning or ‘internal’ dialogue. Alex Long discusses what this shows about the importance of conversation in Platonic philosophy, a question explored further in Conversation and Self-Sufficiency in Plato (forthcoming with Oxford University Press).\nSymposium literature in the Roman Empire\nDr Jason König\nJason König discusses his new book Saints and Symposiasts: The Literature of Food and the Symposium in Greco-Roman and Early Christian Culture (Cambridge University Press, 2012). He talks about a series of imperial Greek and early Christian texts which reshape the tradition of intellectual conversation in drinking-party settings—a tradition which stretches back to classical texts like Plato\'s Symposium—focusing especially on Plutarch\'s Sympotic Questions. He also discusses the changing uses of scenes of grotesque eating and drinking in both Greco-Roman and early Christian narrative.\nFrontinus: Roman author and statesman\nDr Alice König\nAlice König introduces the book she is writing on the Roman author and statesman Sextus Julius Frontinus – with a little help from the crime novelist Lindsey Davis. Her book, ""Frontinus’ \'Technical\' Treatises in Close-up and in Context"" is due to be published by CUP, and her research is being supported by a Leverhulme Trust Research Fellowship.\nSome of her other publications on Frontinus include: A. König (2007), ‘Knowledge and power in Frontinus’ On Aqueducts’, in J. König and T. Whitmarsh (ed.), Ordering Knowledge in the Roman Empire (Cambridge): 177-205; A. König (2013) ‘Frontinus’ Cameo Role in Tacitus’ Agricola’, Classical Quarterly (forthcoming); A. König (forthcoming), ‘Authorial presence and absence in Frontinus’ Strategemata’ (to be published in J. König and G. Woolf (ed.) Expertise and Authority in the Roman World).\nOlympics in the Roman Empire\nDr Jason König\nJason König discusses the ancient Olympics in the wider context of the Greek athletics of the Roman Empire. For more, see Dr König\'s blog \'Ancient and Modern Olympics\' (updated monthly); also his books Athletics and Literature in the Roman Empire (Cambridge University Press, 2005) and Greek Athletics (Edinburgh University Press, 2010; in paperback from 2013).\nTrajan\'s column: Introduction\nDr Jon Coulston\nJon Coulston presents the first in a series of podcasts featuring his research on Trajan’s Column in Rome (dedicated in AD 113). This monument’s remarkable state of preservation may be ascribed to its architectural design, its placement on solid ground (rather than on the alluvium of the Flavian Amphitheatre or the Column of Marcus Aurelius), the splendour of its sculptures, and the value of its viewing balcony as a money-making asset in the pilgrim/tourist trade. The Column pedestal bore reliefs of 600 items of captured barbarian equipment, and the dedicatory inscription which is accounted the finest manifestation of Roman epigraphic art. Sculptures depicting Trajan’s Dacian Wars on a helical frieze running up the Column shaft include 2650 human figures in fantastically minute detail, down to belt-buckles and finger-nails. These features have been recorded and spatially plotted to reveal the practicalities of planning, composition and execution. Together, the architectural design and triumphal sculptural display mark Trajan’s Column as one of the most magnificent monuments to survive from Antiquity, one which has cast a long shadow across the modern world. The podcasts introduce the subject in conjunction with a forthcoming monograph publication and the soon to be launched Trajan’s Column Online Project.\nLiterary interactions under Nerva, Trajan and Hadrian\nDr Alice König\nAlice König outlines her British Academy/Leverhulme-funded research project on Literary Interactions under Nerva, Trajan and Hadrian . She sets out the various authors and issues that the project will examine, and some of the ways in which different contributors will get involved. She also gives a flavour of what the project is designed to achieve by sketching some literary interactions between the Roman author Frontinus and some of his contemporary writers. The research project will result in at least one edited volume and a series of Working Papers will also be published regularly on the project’s website. For more on Frontinus specifically, see A. König (2013), Frontinus’ Cameo Role in Tacitus’ Agricola, Classical Quarterly and A. König (forthcoming), Frontinus’ ‘Technical’ Treatises in Close-up and in Context.\nLies and Broken Promises in Classical Athens and Modern Democracies\nDr Jon Hesk\nJon Hesk discusses the ways in which the Athenian democracy was able to bring its politicians and officials to account for telling lies or going back on their promises. It turns out that procedure and practice were two very different things and the question of whether we should apply Athenian procedures and sanctions to modern democratic politics is more complex than is often assumed. For more, see Dr Hesk\'s book Deception and Democracy in Classical Athens (Cambridge University Press 2000).\nEthnography in the ancient world\nProfessor Greg Woolf\nGreg Woolf discusses his book, Tales of the Barbarians: Ethnography and Empire in the Roman West (Blackwell 2011). He talks about how the encounters between Greeks, Romans and local populations created an entirely new mythology of empire in the Roman Wild West. Travelling scholars teaching Greek to local chiefs or moving in the retinues of Roman generals, bilingual soldiers campaigning in Darkest Europe or the Atlas Mountains, native scholars discovering classical literature for the first time, all played a part. The new myths of origin they created cannibalised stories from the Homeric poems, military reports, eye-witness accounts of strange monuments and bizarre meteorological phenomena.They supported hypotheses that seem bizarre to us from the evidence of place-names and the physical appearances of different peoples, and from complex theories about the relationship between physical environment and national character. Greek science gave these ideas a shape, Roman power presided over their creation: but it was the locals who stood to gain the most from the new identities these tales gave them.\nMembers of the School contribute to many other resources including BBC programmes, external podcast series and publishers\' YouTube channels.\nBBC Radio 4 series: \'In Our Time\'\nHosted by Melvyn Bragg\n- Aristotle\'s Poetics - Stephen Halliwell\n- Cult of Mithras - Greg Woolf\n- Hadrian\'s Wall - Greg Woolf\n- Roman Britain - Greg Woolf\n- Vitruvius and De Architectura - Alice König\n- Pliny the Younger - Alice König\nOxford Academic (OUP)\n- Rome: an empire\'s story - Greg Woolf\n- What sustained the Roman empire - Greg Woolf\n- The latest discoveries about the Roman empire - Greg Woolf\n- What made Greeks laugh? - Stephen Halliwell\n- Greek Gods and the Archaic Aesthetics of Life (Boston University, Institute for Philosophy and Religion, Nov. 7, 2012) - Stephen Halliwell\n- Philostratus\' Gymnasticus and beyond (Hellenic Society, June 2012) - Jason König\n- Oros: Mountains in Ancient Greek and Roman Culture\n- Morality, Politics and Religion in Euripidean Tragedy\n- Politics and gender conflict in Greek drama\n- Plato and Conversation\n- Symposium literature in the Roman Empire\n- Frontinus: Roman author and statesman\n- Olympics in the Roman Empire\n- Trajan\'s column: Introduction\n- Literary interactions under Nerva, Trajan and Hadrian\n- Lies and Broken Promises in Classical Athens and Modern Democracies\n- Ethnography in the ancient world']"	['<urn:uuid:b94825bf-fe4f-4d1c-bb5f-64c7b5a7b6d3>']	open-ended	direct	concise-and-natural	similar-to-document	single-doc	expert	2025-05-12T21:52:51.670698	11	59	1630
55	I'm working on community support initiatives and I'm curious about where the concept of giving comfort toys to children in distressing situations originated. Do you know the background of this practice?	The scheme originated with Australian firefighters. It was later implemented in Dundee by Police Scotland in collaboration with Children 1st, where they introduced Trauma Teddies as handknitted comforters for children who have experienced upsetting incidents.	['Launched in Dundee by Police Scotland, Trauma Teddies, are handknitted comforters for children who have experienced an upsetting incident. The teddies, made and donated by the public are kept in the Police car for when they are needed.\nTayside Police worked with Children 1st on the scheme which originated with Australian firefighters.\nKnitted teddies can be handed into any Police Station for the attention of Longhaugh Community Team or to Children 1st, 47 Blinshall Street, Dundee. DD1 5DF.\nInspector Chris Boath, Longhaugh Locality inspector, said:\n“We have been stunned by the response we have received from the knitters of the area, and can only ask you to please keep them coming, we can never have enough of them.\n“Such has been the response, that in conjunction with Children 1st, we are creating ‘Starter Packs’ for distribution across the country to other Community Teams who have expressed an interest in running schemes of their own.\n“Thank you again to everyone who has contributed, for your generosity and your time.”\nFor further information: e-mail taysidenortheastCPT@scotland.pnn.police.uk\nKnitted/Crochet Trauma Teddy Pattern Instructions\nThese simple guidelines explain the design requirements for hand-made trauma teddies.\nYour interest in supporting this project is greatly appreciated. In order to comply with recommendations provided by Police Scotland Legal Department and to ensure good Health and Safety practice, we request that all donations of teddies adhere to the following guidance:\n- Donations must be hand-knitted and meet the pattern specifications provided.\n- Teddies must not include buttons, zips or anything else which could be accidentally or forcibly removed.\n- Seams must be securely fastened. Should access be gained to any fibrous filling material within the teddy through a seam, this may present a choking risk if swallowed. Seams should be tension-tested to ensure they are as secure as possible.\n- Prior to their distribution, pre-prepared labels will be attached to each donated teddy, advising of the need to take care of the item, given that this has been hand-made. Labels will be provided to Police Scotland by Children 1st and attached with string to the teddy.\nTrauma teddy knitting pattern\nDouble knitting wool 1 pair size 11 knitting needles, head and paws (preferably fawn), scarf (should be bright).\nStuffing – please only stuff teddy with polyester or other healthy padding. Foam rubber is NOT recommended.\nCast on 10 stitches main colour. Knit 10 rows.\nContinue in garter stitch for 30 rows in trouser colour. Make another leg in the same way.\nKnit across all 20 stitches and work 16 rows. Change to jumper colour and knit 24 rows.\nChange to main colour for head and also change to stocking 5 ½ inches.\nChange to jumper colour and knit 24 rows.\nContinue remainder of Teddy in reverse order.\nStitch down the sides of the head. With jumper colour, pick up 8 stitches either side of the neck join (16 in all) and knit 20 rows.\nChange to main colour and knit 10 rows for paws.\nSew up Teddy leaving opening in crotch.\nTurn right side out and sew diagonal top corners for ears before stuffing.\nAfter stuffing, run a thread through the knitting around the neck to draw it in.\nSew a happy face and for mouth use either stem stitch or back stitch.\nScarf : Cast on 75 stitches. Knit 4 rows and cast off. Tie scarf on Teddy the back of the neck, close to the jumper edge. Don’t sew down at the front.\nTrauma Teddy Crochet Pattern\nMade using 8ply acrylic, and a 4mm hook.\nAll dc rows start with a turning chain (ch1) unless otherwise stated.\nIN white, ch 11.\nDc in second ch from hook, dc across (10 stitches)\nDc the next 14 rows, break yarn.\nJoin the colour you are using for your shorts. Dc across for 5 rows, break yarn and finish off.\nDo this again for the other leg, but instead of breaking yarn, dc back across this leg, and dc across the first leg you made to join them. (20 stitches)\ndc across for another 6 rows\nChange to shirt colour.\ndc across for 10 rows.\nDc 20, ch 9\nTurn and dc into the second ch from the hook. Dc into the next 7 ch, dc across your dc’s, and chain 9\nTurn work and dc in second ch from hook. Dc in next 7 chains and dc all the way across.\nDc the next 7 rows.\nDc 28, ch1 turn and dc 20.\nJoin your white yarn, and dc 17 rows, break yarn and finish off.\nMake another of these, sew your face, onto one side, then with the pieces together (face side in) Sew around your teddy, leaving the top of his head open.\nTurn it out the right way and stuff . whipstitch through the front loops along the top of his head for a nice nearly invisible join. Using white yarn, running stitch a line to form triangle ears on the top corners of his head.\nPattern from Cook, Craft, Cope']	['<urn:uuid:7e6b7ce0-a11a-4b15-b148-2d69a3db80fe>']	open-ended	with-premise	verbose-and-natural	distant-from-document	single-doc	expert	2025-05-12T21:52:51.670698	31	35	834
56	thermal imaging infrared electrical inspection capabilities limitations non destructive method temperature measurement	Thermal imaging is a non-destructive inspection method that measures surface temperatures of objects by viewing and recording thermal signatures. While it cannot see through walls like X-rays, it can detect temperature differences that transfer through surfaces. In electrical inspections, it detects heat generated by electrical resistance, which is a symptom of faulty electrical systems. The technology can identify problems in connectors, relays, switches, semiconductors, and circuit boards, helping detect issues before equipment failures occur. This allows for preventative maintenance scheduling and can potentially save thousands in repair costs and prevent fire hazards. Thermal imaging is particularly effective when conducted during periods of high system loads to reveal existing or potential hot spots in electrical connections.	"['What is Thermal Imaging?\nTo keep it simple, Thermal Imaging is the process of viewing and recording actual surface temperatures, signatures, and anomalies observed at surfaces such as walls, ceilings, floors, pipes, ducts, etc. Think of it as a thermometer with a television attached. Instead of mercury rising through the marks, the temperature is translated into a visual image. Using a high resolution thermal imager (or camera), we can observe and interpret surface temperatures to help determine what is or may be occurring in areas we can’t readily view without destructive evaluations. All objects emit energy in the form of heat. Thermal imaging allows us to ‘see’ this energy and interpret our findings or the anomalies observed.\nCan you see through walls?\nSimply put… No. What we can see are the temperature differences that transfer through or occur at the surface of an object. Air movement, evaporation, moisture, heat transfer, and more all have an observable effect on surface temperatures. These create a temperature difference that our camera ‘sees’ and creates an image of the thermal signatures. Although our service sometimes appears to be, it is not x-ray vision.\nI am buying a brand new house. Why do I need a thermal inspection?\nEven new construction has defects. It is of great importance to have a thermal inspection performed since many mistakenly expect all systems to be installed properly and functioning as intended. Quite the opposite can occur unfortunately. I have observed active roof leaks, misaligned and damaged framing, and even missing insulation. Once the drywall is up, it’s impossible to ‘see’ these defects without the assistance of thermal imaging. Often the builders are just as concerned to discover the carelessness of their sub-contractors.\nHow can Thermal Imaging help with my commercial property?\nThermal imaging can help identify numerous issues that can be prevalent in both commercial and residential properties. From roof and moisture scans, to electrical and insulation scans, these issues are very important to help you understand the overall condition. Traditional visual inspections can’t come close to seeing what we can see. Thermal imaging can even help contractors to identify troublesome issues and find hidden systems like buried radiant heating systems and plumbing in walls.\nWhy are Infrared Thermal Inspections so vital?\nThe remarkable view from an infrared camera allows us to see and compare the relative temperature of an object or surface area to that of its surroundings. These comparisons can help identify otherwise invisible problems and concerns which can be caused by issues occurring below the surface of a floor, behind a wall, or above a ceiling. The images seen by the camera can reveal problems with moisture, electrical, and HVAC systems, as well as problems with insulation, foundation, framing, plumbing, and more. Combining Infrared Thermal Inspections with traditional inspection methods provides our clients the very best, state of the art service available.\nAbout Infrared Thermal Inspections\nInspect It Neil LLC offers advantages to you over traditional inspections with our INFRARED TECHNOLOGY. We use a state of the art Flir B 50 and I am a Certified Infrared Thermographer. Very few understand the importance of infrared technology, yet what we discover through using infrared thermal imaging has helped save hundreds, and even thousands of dollars for commercial and residential investors, business owners, homeowners, home buyers, and home sellers. But most important is that Thermal Imagaing is a NON DESTRUCTIVE method to inspect areas that were once could only be inspected with destructive methods.\nWhat once was only thought of as a military or scientific technology, Infrared Imaging is now being applied in commercial and residential applications. Using the same technologies as used by science, industry, and military, Infrared Thermal Imaging technology combined with a traditional inspection provides us with a view of your property like no other. With Infrared Thermal Imaging, we can see beyond the spectrum of “natural light” or what we see with our own 2 eyes.\nBecause everything has a surface temperature, infrared thermal imaging technology allows us to ‘see’ the temperature of those surfaces. The images seen are then interpreted and further investigated. As human beings, we are limited to seeing light only in the visible light spectrum called white light. This is the light that bounces off everyday objects whether the light is being emitted by our sun or an artificial source such as a light bulb. The infrared spectrum is otherwise invisible to the naked eye. Without assistance from technology, we are unable to see the vital information provided by the infrared spectrum and surface temperatures. It is this ability to ‘see’ these surface temperatures that allows us to more accurately identify potential, and immediate, problems in your commercial or residential property that would have otherwise been missed.\nRoof leaks, plumbing leaks, pests and vermin, airflow such as cold air seeping under a door or warm air leaking from central air ducts, all can be detected with an infrared camera. Because these and other issues often thrive or exist in hard to view places (such as behind walls), the use of Infrared technology allows us to pinpoint exactly where a problem area is in your home, office, or building without the need for any invasive investigation damage to the structure.\nIn short, our advanced technology is purposefully designed to provide you with a level of service that increases the accuracy of correctly identifying problems, reduces the time and effort required to fix those problems, helps you to catch small problems before they become expensive or unmanageable problems, and saves you time and money.', 'Using Thermal Images for Electrical Inspections\nHeat: The Symptom of a Faulty Electric System\nAn infrared electrical inspection of your facility can provide you with valuable information about the condition of your electrical system and equipment that can save thousands of repair dollars and hundreds of hours of lost productivity. Even worse, unchecked electrical system failures can result in fires or dangerous explosions. Infrared inspections are designed to identify potential equipment failures before they happen so that appropriate preventative action can be taken. The decreased liability of a thermally inspected electrical system even encourages some insurance providers to offer discounted rates.\nWhen an electrical system begins to have problems, it gives off heat as a byproduct of electrical resistance. Infrared cameras record and visually display this emitted heat to show the exact location of the electrical defect. Thermal imaging simplifies diagnosis and repairs while protecting adjacent electrical components from hot spots. Routine thermal electrical inspections will reduce repair costs and decrease lost production hours that result during unexpected down time.\nA thermal scan of your electrical system increases efficiency, greatly reduces the risk of fires, and allows preventative and predictive maintenance to be scheduled during planned down-times. In business, this alone can mean mean big cost savings.\nMore About Predictive Maintenance\nThermal Imaging has proven to be an ideal inspection method for all types of predictive electrical maintenance. Infrared technology gives thermographers the ability to “see”, measure, and record temperatures on defective components and the normal wear, chemical contamination, corrosion, fatigue and faulty assembly that occurs in many electrical systems. The molecular friction produced by problem electrical components is visible within the infrared spectrum and recordable by infrared cameras. Overheating can occur in any electrical component including generators, transformers, pole top connections, insulators, disconnects, jumpers, shoe connections, fuse connections, switchgear, starters, contactors.\nOhm’s law states that power dissipated in the form of heat exponentially increases as the current increases. This means that the amount of heat generated in a conductor is proportional to its resistance and to the square of the current it carries, while the temperature rise depends on the rate at which the heat is dissipated through convection, radiation, and conduction.\nBy allowing a small amount of current to pass through electrical devices, localized ""hot spots"" can be detected using a thermal camera making thermal imaging a non-destructive method of detecting problems on electrical devices.\nHow Electrical Inspectors Use Thermal Imaging Technology:\n- Connectors, Relays, and Switches: Find poorly secured, corroded, or current overloaded hardware\n- Semiconductors: Find poorly bonded, die attached, open, shorted, or leaky active devices\n- Circuit Boards: Find overstressed components, plated through holes, poor heat sinks, and bad solder joints\n- Discrete Components: Find overstressed transformers, capacitors, and resistors\nStandards for Thermal Electrical Inspections\n7.1.1. Current-carrying capacity of a bus - The current-carrying capacity of a bus is limited by the temperature rise produced by the current and other factors. Buses for generating stations and substations are generally rated on the basis of the temperature rise which can be permitted without danger of overheating equipment terminals, bus connections, and joints. ANSI C37.20C-1974 (IEEE standard 27-1974) permits a hottest spot temperature rise for plain copper buses of 30 ºC (54 ºF) above an ambient temperature of 40 ºC (104 ºF), with a hottest spot total temperature limit of 70 ºC (158 ºF). The standard allows a hot spot temperature rise of 65 ºC (117 ºF) for metal-enclosed applications where silver contact surfaces are used on connections and a hot-spot temperature rise of 45 ºC (81 ºF) for silver-surfaces terminals of outgoing circuits. Aluminum used for bus work ordinarily has a conductivity of 63 percent, as compared to copper at 99 percent. For a given current rating and for equal limiting temperatures, the area of an aluminum bus will be about 133 percent of the area of a copper bus.\n7.1.2. Allowable current density in a bus - Allowable current density in a bus is the amount of current that the bus can carry per square inch or cross-sectional area without exceeding the permissible temperature rise. For both ac and dc buses, densities may vary from values of 9.3. x 105 and 1.09 x 106 A/in2 (600 and 700 A/in2) in heavy current-carrying copper buses to 1.86 x 106 and 2.17 x 106 A/in2 (1,200 and 1,400 A/in2) in light buses under favorable conditions. For aluminum, densities of 75 percent of the above values are usually permitted.\n7.1.3. Current-carrying capacity of conductors - No method has been generally accepted by the industry for the calculation of the current-carrying capacity of conductors for overhead power transmission lines. However, Reclamation designs its ACSR transmission line capacity in accordance with tables, charts, and procedures in the ""Aluminum Electrical Conductor Handbook,"" 1971 edition or specific instructions from the conductor manufacturer.\n7.2. Temperature of Connections - The principal function of an electrical connection is to satisfactorily carry the electrical load over its entire service life. The electrical load can be expected to have daily fluctuations from no load to full load and frequently to very heavy overloads, thus causing wide fluctuations of operating temperature. In addition, the ambient temperature can be expected to fluctuate between daily extremes and between seasonal extremes. The effect of this heat cycling on a poorly designed or improperly installed connection is frequently progressive deterioration and ultimate failure of the connection or associated equipment. Therefore, temperature rise provides an important and quite convenient method of monitoring the condition of electrical connections.\n7.4. Thermographic (IR) Surveys - Sophisticated infrared equipment in vans or helicopters is now widely used throughout the electric power industry for rapid scanning of substation and switchyard bus and equipment as well as transmission lines. Available equipment can survey an average-sized switchyard in about 30 minutes. Experience in the industry has proven the technical worth and economic justification of infrared testing as a preventive maintenance tool.\n7.4.1. Thermal Imaging devices - Thermal imaging devices are useful in permitting a rapid scan of switchyard buses and equipment terminals to detect temperature differences. Infrared signals from photovoltaic detectors are electronically amplified and transmitted to a small television viewing screen producing an image corresponding to the thermal patterns within the viewed scene. Temperature differences are shown in varying shades of gray or in different colors. An accessory instant camera may be used to provide a permanent record of temperature differences.\n7.4.3. Scheduling of infrared surveys - Scheduling of infrared surveys to detect hot spots in electrical connections and equipment is an important facet of an effective infrared testing program. Such surveys should be conducted during periods of high system loads, which circuits in a given station are sufficiently loaded to reveal existing or potential hot spots. Subsequent surveys should be scheduled to test transfer of auxiliary buses under loaded conditions. Each hot spot detected by an infrared survey should be physically inspected and evaluated. Judging the severity of a faulty connection by temperature readings alone can be misleading. A reading of 50 ºC (90 ºF) above ambient in one case can be as serious as one 180 ºC (325 ºF) above ambient, especially if the load is low in the first case. A follow up inspection of suspect and/or repaired connections should be performed with a hand-held infrared thermometer, temperature-sensitive tape, or other suitable means.']"	['<urn:uuid:93d2ac50-98e6-4540-adc8-04b03d8b1e41>', '<urn:uuid:e6f25812-9c79-494b-8563-f3a8a177beca>']	open-ended	direct	long-search-query	similar-to-document	multi-aspect	expert	2025-05-12T21:52:51.670698	12	115	2140
57	how attract spiders garden home	You can provide spider habitat by covering bare dirt with mulch, creating a 'wild' area in your yard, planting dense shrubs and coniferous trees, and providing a water source such as a pond. If you are concerned about spiders in your home, sealing off your living space from the crawlspace, garage, floor voids and unfrequented storage space will help. It's important to note that house spiders and outdoor spiders are not the same species.	"['This aptly-named, vicious-looking bug is about 1/4 to one inch long, with a cone-shaped head and wide curving beak. They may cause a painful bite to a human if captured. Some species squeak if caught. Females lay single eggs in cracks, under rocks or in other sheltered spots in summer, and new adults emerge around the following June. There is only one generation per year.\nAssassin bugs are voracious predators of many garden pests including flies, mosquitoes, beetles and large caterpillars.\nAdult and nymph assassin bugs stab their prey with long, pointed ""beaks"" that are held folded under their bodies while not feeding.\nA related family of smaller predatory insects that wait for their prey on flowers are called ambush bugs.\nThese insects resemble a yellow jacket but are larger, up to 3/4 inch in length. They are mostly black, with a white face and white markings on the tip of the abdomen. They build large paper nests that can measure up to 14 inches in diameter and 24 inches long!\nThey eat many pests including crane flies and other flies. They also eat yellow jackets. They may also act as pollinators of some plants.\nIf the nest is disturbed, these hornets will sting humans.\nOften called ""insect hawks"" because they pounce upon pests.\nLarge and lumbering, black and yellow bumble bees measure up to one inch in length. These fuzzy insects make a loud droning buzz as they fly somewhat awkwardly from flower to flower. Bumble bees nest in soil or leaf litter where a single queen lays 8 to 12 eggs in spring and continues to lay eggs through the summer.\nEmerging workers are able to fly in very cool weather, making them a very valuable pollinator of a variety of plants.\nAs these bees nest in soil and leaf litter, try to leave a section of your yard undisturbed. A little ""wild"" place in your yard can offer a haven for many other beneficial insects that would otherwise be killed by tilling and mowing. Provide native flowers, as they are a primary food source.\nThere may be as many as 40 species of bumble bee in the Western U.S.\nThis long (1/2 to three inches) many-legged creature is light brown to black in color and moves quickly. Centipedes have only one pair of legs per segment. Millipedes, which are important in compost decay, have two pairs per segment. Both prefer moist areas in the garden and compost piles.\nCentipedes prey on pests and insects in the soil including slugs, worms and fly pupae. Centipedes kill their prey with venom, and their bite is moderately painful (although not dangerous) to humans.\nAs with ground beetles, low-till gardening can maintain the populations of centipedes.\nThere are more than 80 species in Washington. They can be identified by their long narrow body, their large compound eyes and the four transparent wings. There is variation in color. Sizes range from one to two inches. The larvae are found in water.\nThey eat mosquitoes, aphids and other pest bugs.\nDragonflies and damselflies have decreased considerably in abundance as the wetland areas where they live have dramatically decreased. Want dragonflies? Don\'t fill in that marshy area. Better yet, enlarge it, or dig your own pond.\nAdult green lacewings have delicate, light green bodies; large clear wings; and bright golden or copper colored eyes. They are 1/2 to 3/4 inch long. The larvae are small, grayish brown, and narrow and they have pincerlike mandibles. Eggs are found on plant stems and foliage; they are laid singly or in small groups on top of fine, silken stalks.\nLacewing larvae and adults feed voraciously upon aphids and other small insects, insect eggs, and spider mites. They also eat leafhopper nymphs, whiteflies and small caterpillars.\nPlant flowers that produce pollen and nectar. Adults are mobile, but lay eggs where they stop to eat.\nSuccess with lacewing eggs or larvae may require practice. Hold eggs at room temperature until the larvae begin hatching, then sprinkle them on plants (about one to five per square foot of garden space).\nWhile shapes and colors may vary widely, they are usually shiny. Some are very ferocious-looking, but they are not known to bite people. Black is a common color, sometimes with a metallic sheen of another color on their wing covers. Most ground beetles feed at night and hide in the soil or under debris during the day. Adult beetles range from 1/8 to one inch long.\nThese very common garden insects feed on many soil-inhabiting pests such as cutworms and root maggots. Some types eat slugs and snails.\nYou probably don\'t need to attract these common beetles. Unless it\'s discouraged with pesticides, the species that is right for your garden will find you. Low-till gardening techniques can save the lives of many soil-dwelling ground beetles. Objects to hide under, such as logs, boards or flat stones, will help keep ground beetles in your garden.\nAdults measure 2/3 inch long and are fuzzy, with gold-and-black striped bodies and transparent wings. Honey bees can often be identified by the balls of yellow pollen they carry on the backs of their legs.\nHoneybees are important pollinators of many plants.\nGrow flowering plants. Encourage wild honey bees. Because the spread of mites has seriously reduced honey bee populations, the wild honey bees that are left are even more important.\nPollinators are estimated to be worth $8 billion to our economy! Many other insects are pollinators as well.\nThe adults have bodies with black and yellow stripes. While they look like bees or wasps, they don\'t sting. They range in size from less than ¼ inch to ½ inch.\nAlthough not all are directly beneficial, many hover fly larvae prey on aphids, mealybugs and other small insects. Adults must feed on nectar before they reproduce, so are good pollinators.\nThese flies are called ""hover flies"" because they hang in one spot like a helicopter when they fly.\nMost people know an adult lady beetle (lady bug), but the larvae are most valuable. The larva is soft-bodied and alligator-shaped with black and orange markings. Each species has a distinct pattern.\nBoth larvae and adults feed on soft-bodied insects such as aphids, mealybugs, scale insects and spider mites as well as insect eggs.\nPlant flowers that produce pollen and nectar (dill, angelica). Allowing weeds (dandelion, wild carrot, yarrow) can help too. You could also spray a combination of whey and yeast on plants as a food source.\nWe don\'t recommend buying lady beetles for pest control: most often they won\'t stay in your garden when released. The best thing is to attract lady beetles, or other insect predators, to your yard.\nSlightly smaller than a honeybee, these gentle, non-aggressive insects resemble house flies more than honey bees. They are deep blue-black in color and have no stripes.\nMason bees pollinate apples, cherries and other tree fruit. They are active between apple blossom and cherry blossom season, then die out by summer.\nProvide them a home. Drill holes 1/4 to 3/8 inch in diameter and 3 to 6 inches deep into wooden boards or blocks. Attach boards to a house or post near where you have seen the bees. Some protection from rain is a good idea. Position boards facing morning sun. Place homes by early spring, preferably by April 1.\nToo small to be noticeable, these miniwasps don\'t sting people or pets. They range in size from the smallest insect known (about 1/50 inch) to about one inch, although most are on the small side. These parasites reproduce by laying their eggs in a pest host (adult or egg). The immature wasp feeds inside and kills its host. A round hole can be often seen where the adult parasite has chewed its way out.\nDifferent species may attack aphids, whiteflies, and butterflies or moths, such as cabbage loopers and hornworms.\nYou need to have some of the prey around to be able to sustain populations of the parasite.\nMany are available for sale to home gardeners. Examples are Encarsia formosa, which attacks greenhouse white flies, and Trichogramma species, which attack many caterpillar pests.\nAdult mites are tiny, about half a millimeter in length, and are beige to reddish tan. They resemble pest mites but are faster moving and have fewer hairs.\nPredatory mites are valuable predators of pest mites such as spider mites.\nThere is no great way to attract mites. Since you probably already have some, don\'t discourage them with pesticides.\nPredatory mites naturally occur in large numbers in the Northwest. Since you probably already have mites in your garden, it may not make sense to buy new ones for outdoor use. However, they may be useful for greenhouses and indoor plants. They are especially effective against spider mites. You can purchase predatory mites at many garden stores.\nThese fascinating insects may resemble a tiny scorpion when they hold the tip of their abdomen up in the air. They are fast moving and measure 1/10 to one inch long.\nDepending upon species, rove beetles prey upon aphids, springtails, mites, nematodes, slugs, snails, fly eggs and maggots. They also eat and help break down decaying organic material.\nCreate a ""wild"" area of your yard where leaves are allowed to pile up,the grass isn\'t mowed, and the soil isn\'t tilled. This can give these and many other insects a haven.\nRove beetles are often seen hanging around dead animals but they\'re not scavengers. They\'re actually waiting for their prey, such as flies and maggots.\nApproximately 1/2 inch in length, the adult soldier beetle has a narrow, black abdomen and bright red head or thorax. The soldier beetle larva is various shades of orange with black markings.\nSoldier beetles prey upon aphids, caterpillars, grasshopper eggs and beetle larvae, among other insects around the garden.\nSince some soldier beetles feed on nectar, you may be able to attract them with flowering plants.\nSoldier beetles are nicknamed leatherwings because of their soft, clothlike wing covers.\nSpiders aren\'t insects at all. They can be identified by their eight legs and two-part body. Although there are hundreds of species of spider in Washington, they all share this trait. Spiders are far more beneficial than they are dangerous. Most spiders are shy and harmless to humans.\nSpiders are the most important predators on insects, killing more than all other predators combined. They feed on a broad variety of pest insects year-round.\nYou probably already have an abundance of spiders. You can provide spider habitat by covering bare dirt with mulch, creating a ""wild"" area in your yard, planting dense shrubs and coniferous trees, and providing a water source such as a pond.\nIf you are concerned about spiders in your home, sealing off your living space from the crawlspace, garage, floor voids and unfrequented storage space will help. House spiders and outdoor spiders are not the same species.\nResembling house flies, tachinid flies are 1/3 to 1/2 inch in length and may be brown, gray or black in color. Some species are very hairy.\nThere are many species of tachinid flies; many are parasites of pest caterpillars including cutworms, codling moths, tent caterpillars, cabbage loopers and gypsy moth larvae.\nAdults feed on nectar, so plant flowers as well as herbs in the Umbelliferae family such as dill, parsley and Queen Anne\'s Lace.\nAdult wasps are 1/2 to 3/4 inch long, with characteristic yellow and black stripes and transparent wings.\nYellow jackets and other wasps are predators of caterpillars, flies and beetle grubs.\nYellow jackets are often feared for their sting, which is a hazard to people who are allergic. Some yellow jacket species are more likely to sting and scavenge at picnics than others. The scent of pears is a sting signal for yellow jackets.\nThink about that nest in your yard: Does it really have to go? If the yellow jackets are not interfering with the lives of people in the area, leave the nest alone and get great pest-control and pollination benefits.\nTo occupy yellow jackets during a picnic, place raw meat or a fish head away from the picnic (over a bucket of soapy water).\n**NOTE: Many insects have a different form when they are young. These younger bugs are also beneficial to your garden. For example, lady beetle larvae (young lady beetle) are thought to eat more aphids than their adult lady beetles.']"	['<urn:uuid:7d44f290-e7a7-4782-8c1d-365cf7e0976b>']	open-ended	direct	short-search-query	distant-from-document	single-doc	novice	2025-05-12T21:52:51.670698	5	74	2063
58	How do churches and art galleries work together nowadays?	According to contemporary thinking, churches and art galleries should maintain distinct roles rather than trying to imitate each other. However, the church's aesthetic framework, particularly its tradition of icons, can inform and shape contemporary art practices. The church's aesthetics and poetics are considered foundational to all aesthetics and poetics, with influence flowing from the church outward toward culture, not the reverse. This perspective suggests that churches shouldn't try to be galleries, nor should galleries attempt to be churches, but there exists a meaningful relationship between religious and artistic spaces.	['In a recent book assessing the state of evangelical scholarship, Mark Noll refers to a boomlet in evangelical art history [that] rests squarely on the work of the Dutch Reformed scholar Hans Rookmaaker. Had Noll seen Daniel Siedells book God in the Gallery , he might have thought differently. Siedell is a long way from Rookmaaker, and his book¯whether or not it can be called evangelical¯is no boomlet. God in the Gallery is an impressive detonation in and of itself.\nThe Christianity-and-art conversation is gridlocked. The stalled traffic includes those who are profoundly suspicious of the art world, and those who are infuriated enough by this unforgivably conservative suspicion that they, in turn, write contemporary artists a theological blank check. A book capable of broaching this impasse has long needed to be written¯but who would have suspected it would be this good? What makes God in the Gallery noteworthy is that it addresses another gridlock as well, that of contemporary art. The traffic in this case involves those liberated by the end of modernity to explore spiritual directions, and those committed to keeping art a staunchly secular enterprise. The art world, insists Siedell, is growing increasingly uncomfortable with its collective unbelief.\nSiedells qualifications enable him to address both these dilemmas. He is a firmly ecclesial Lutheran with deep¯one might say overriding¯sympathies for the Orthodox Church. In addition, Siedell holds a Ph.D. in contemporary art (he studied with noted critic Donald Kuspit), and he is a seasoned curator with a decade of gallery experience.\nSimply put, God in the Gallery succeeds by dividing, that is, by clearly distinguishing the sanctuary from the salon . The author has no interest in churches aping galleries or galleries playing church. But what keeps Siedell from merely erecting a Jeffersonian wall of separation between church and gallery is his unflinching insistence that the churchs aesthetic framework, grounded in the ecumenical warrant for icons, is strong enough to inform, shape, and underwrite the practice of contemporary art. The churchs aesthetics and poetics . . . is the ground of all aesthetics and poetics. And the direction of influence goes from the church outward toward culture, not from culture to a passive, inert, irrelevant church.\nTo buttress his argument, Siedell quotes Christoph Cardinal Schönborn: A Church that in her liturgy, in her very life, draws vitality from the sense of awe in facing the mystery, will provide breathing space for any art whose primary purpose is not a breathless pursuit of outward success. In order for this vision to be realized, however, Protestant approaches are simply not expansive enough for Siedell. Only the economy of the icon can provide an important foundation on which to rethink modern and contemporary art. Byzantine insights from the eighth century are marshaled to fortify the twenty-first.\nAll these are serious claims, and Siedell has done the work necessary to back them up. The book offers a brief survey of modern art, an introduction to the condition of and key players in contemporary art, a summary of the academic¯populist divide in art criticism, a diagnosis of the Christianity-and-art conversation, and a primer on recent theological trends. Of course one book cant do all these completely, but this one does them all surprisingly well.\nMost interesting in my view is Siedells ambitious attempt to solve a major crisis in the discipline of art history¯the status of the term art . For Siedell, the development of art as a Western concept is something we should accept. Not all products of modernity are theologically and spiritually suspect. But, while Siedell accepts arts institutional reality, he does not accept the narrow range of activities the term art currently represents.\nInstead, Siedell borrows from the philosopher Paul Crowther and suggests defining art as common (universal) human practice of making and experiencing whose primary goal, Siedell adds, is to seek communion with God. The most effective example of this has been the Byzantine icon. Nicene Christianity does not merely tolerate images in the church. It requires them. While it certainly needs to be developed further, Siedell has suggested a Copernican revolution in the artistic solar system that makes the Christian icon the governing sun. Christianity, therefore, in all its myriad cultural and material manifestations is never absent from the modern artist.\nSiedell even manages to pull recent, stranger trends of contemporary art into this iconic orbit. In the shift from modern to postmodern art, Siedell sees a transcendence in transition from modernitys disembodied purity to one that is sought in and through embodiment, tradition, cultural practice, and the material world. This resonates with, without being equivalent to, the Christian sacramental tradition. High is the pile of sophisticated recent books on theological aesthetics. God in the Gallery might enable such labor to actually reach where it is needed most¯plunging from the ethereal heights of the seminaries deep into the streets (even the gutters) of contemporary art.\nThe modern artist Mark Rothko (1903“1970) provides an example of how Siedells vision plays out. Initially, the mystical ambition of Rothkos large, figureless fields of paint might be considered direct competition to Christian faith¯vying for the very transcendence that religion has already achieved. Siedell neither gives in to this suspicion, nor does he offer unqualified endorsement. Instead, he secures a complex middle ground: Mark Rothkos brooding paintings function ambivalently as icons; or rather, the content of their iconicity is underdetermined. But this does not mean that they do not participate in some way in the reality of the icon.\nNot all of Siedells interpretations, however, are as successful as his take on Rothko. Siedell is uncritical of Janine Antonis Gnaw , where the sculptor chews on massive cubes of chocolate and lard. This is similar, Antonini says, to receiving the host from the priest in the old-fashioned way. In defense of the sculptor, Siedell claims that the complex relationship between tasting and seeing can be expressed outside the divine liturgy only through the aesthetic complexities of art. (What about restaurants?) Siedell is celebratory of German artist Wolfgang Laib, who considers the attempt to create beauty a futile pursuit. He sees Robert Gobers headless Christ installation as an attempt for an alienated Catholic to create an alternative sacred space. Another artist writes directly on his work that his piece is in fact a meaningless failure. While Siedell suggest we look deeper, some might wish to take the artist at his word. At some points Siedells interpretations were convincing, at others, readers might share my sentiment: I believe, help my unbelief. Nevertheless, Siedell does not claim to offer the Christian explanation of these works of art, and he does not suggest¯as have some¯that they infiltrate the liturgy. Siedell errs on the side of charity because he aims to prompt the skeptic to take another look, and to critique the way Siedell carries out his vision is not to dismiss the vision itself.\nStill, at times Siedell is burdened by unhelpful terminology pulled straight from the paradigms he has done so much to overcome. He refers to the neoconservative captivity of art criticism, and is sharply critical of New Criterion editor Hilton Kramers conservative political agenda. Indeed, politics can affect art and art criticism for the worse. But Siedell breathes not a word of criticism for the leftist agenda that¯it takes willful effort not to see¯has overtaken so much contemporary art and art criticism. Because it is this agenda that Kramer is attempting to counter, one would have at least hoped for a pox on both your houses from Siedell. (His reference, albeit in a footnote, to Christian anarchism without critical distance is unsettling). Like it or not, a healthy art world may depend on a stable free world. There is a reason that Enrique Martínez Celaya, the Cuban-born artist whom Siedell lionizes, is flourishing here and not in, say, Cuba.\nIt is tempting to suggest that Siedell expends so much charity to the art world that he has little left for his Christian colleagues. Though he is never belittling or rude, Siedell hits his co-religionists hard. Gregory Wolfe, editor of the religion and art journal Image , is criticized for not differentiating clearly enough between the institutional framework of the church and the world of art. Cutting edge innovation does not belong in the liturgy, where recognition by the faithful is essential. On the other hand, Siedell is frustrated with those who have tried to establish an alternative, Christian art world through a network of Christian college art departments and organizations. Siedell berates what we might call this Hauerwasian model of Christian art enclaves. He prefers instead what could fairly be called a more Neuhausian engagement of Christians in arts public square.\nThe model for Siedells art criticism is no one less than the apostle Paul at Athens (Acts 17). Art can function as an altar to an unknown God, and the Christian art critic can say, with Paul, that which you worship as unknown, I proclaim to you (Acts 17:23). There is indeed a remarkable parallel between first-century Athenian pagans and the twenty-first-century art world, and Siedell could have perhaps taken it further. Both Athenians and art enthusiasts, for example, spend their time in nothing except telling or hearing something new (Acts 17:21). But there is also, of course, a great divide between then and now. The Athenians knew nothing of Christianity. Contemporary artists do, and remind us of that fact by intentionally mixing Christian imagery, often with deliberate offense, into their work. The point is important because Paul makes much of it. While God has overlooked the times of human ignorance, now he commands all people everywhere to repent (Acts 17:30). Siedell might avoid this criticism by suggesting that the contemporary artists alienation from traditional Christianity by either injury or ignorance functions as a second naiveté.\nBut Pauls vision goes further. The altar to an unknown God was not the only altar in Athens. In fact, we read that this was only one in a city packed with idols which greatly distressed the apostle (Acts 17:16). Engagement with contemporary art sometimes leads to this conclusion, which is equally Pauline. Noting that art has failed as a rival religion does not necessarily indicate neoconservative captivity (and if it does, then count Nietzsche a neocon). Is it necessarily philistine to point out the unfortunate condition of contemporary art? Take for example, the abortion art catastrophe at Yale, or Banks Violettes recent show at the Whitney that meditates on Satanic ritual murders, or Damian Hirsts sadistic stations of the cross. Sadly, there is only more evidence today to support the reformed art historian Hans Rookmaakers 1970 thesis, Modern Art and the Death of a Culture. (A thesis, it is important to note, that Rookmaker complemented with daringly positive reviews of countless avant-garde exhibitions as well.)\nStill, as Siedell explains, the art world is extremely complex, and wholesale dismissals are entirely unwarranted. Many of the artists Siedell has worked with are also frustrated with the art world, and Siedells iconic vision is a needed strategy for that worlds renewal. Christians who consider contemporary art an unpleasant mystery will find this book an effective primer to genuine engagement. God in the Gallery infuses the Protestant art historical tradition with the broader insights of the Seventh Ecumenical Council, and it appropriately segregates liturgical and contemporary art, with the former underwriting the latter. The church is not a religious sphere separated from the realities of the world but reveals the worlds true meaning and significance. Siedells vision of contemporary art inspired by the Christian icon is a compelling one. At least for now, it is a reality in Orthodox cultures much more than in our own.\nMatthew J. Milliner is a Ph.D. candidate in art history at Princeton University. He blogs at http://millinerd.com .\nGod in the Gallery by Daniel Siedell']	['<urn:uuid:998cd821-4776-4a3a-af40-6d15dedebe35>']	open-ended	with-premise	concise-and-natural	distant-from-document	single-doc	novice	2025-05-12T21:52:51.670698	9	89	1971
59	comparing darwin wallace specimen collection methods	While both naturalists collected specimens, their methods differed significantly. Darwin's main specimen collection occurred during his voyage on the HMS Beagle (1831-1836), particularly in South America and the Galapagos Islands where he studied finches and tortoises. Wallace conducted two major collecting expeditions - first to Brazil (ending in disaster when his specimens were lost in a shipwreck), then to Indonesia where he collected over 125,000 specimens and discovered 1000 new species, including the golden birdwing butterfly.	['Evolution and Darwin’s Theory of Evolution by Natural Selection\n- Evolution is the unifying and organizing principle of all Biological sciences.\n- Today, evolution can be studied using the language of Mathematics (quantitative formulations, models, statistical methods, etc.) and from an interdisciplinary perspective.\n- The basic idea behind biological evolution is that all new species derive and diverge from shared ancestors.\n- Darwin defined biological evolution as: “descent with modification”. This means that over time and succeeding generations, those traits that provide reproductive advantage become more common within a population.\n- Evolution is a process that results in heritable changes in a population that spreads over many generations. Inheritable features vary from individual to individual.\n- Populations are subject to evolution, while individuals change over time. Hence, evolution always occurs at the population level, not at the individual level.\n- Natural selection is one of the mechanisms by which evolution operates.\n- Evolution by natural selection acts when heritable variation leads to differential reproductive success (fitness).\n- The main feature or property of evolving systems is their ability to reproduce. Evolution is meaningless without reproduction.\n- Natural selection in nature acts when individuals with certain genotypes are more likely to survive and reproduce than other individuals with diverse genotypes and thus, to pass on their alleles to the next generation. Through this ongoing mechanism, traits that tend to promote reproductive success become more prevalent in time.\n- Individual variations that proved beneficial would be preserved within a population by means of selection, whereas variations that were disadvantageous tend to decrease in frequency or died off over time.\n- According to Darwin’s evolutionary theory, natural selection is a matter of reproductive success. The “fittest” individuals are the ones who have the combination of traits that allow them to survive longer and produce more offspring. Those beneficial traits will be inherited by next generations.\n- The expression: ”the fittest”, does not mean the strongest individual, but the best or sufficiently adapted to the environment.\n- The four basic mechanisms responsible for evolutionary change are: natural selection, genetic drift, mutation and migration (gene flow). These are processes that caused populations to change over time.\n- Reproduction and cooperation are closely related concepts. Certain individuals are considered the fittest, because they cooperate with their peers rather than competing with them ensuring in that way their survival and reproductive success. Environmental resources, food sources and breeding partners in nature are limited and very often scarce. This causes individuals to have to compete for them restricting in that way their possibilities of survival and reproduction.\n- Evolution has organized cooperation in complex cells, multi-cellular organisms and many other amazing cooperative systems.\nTwo stories with a common vision\nOn 1 July 1858, the Theory of evolution of species by natural selection conceived by Charles Darwin (1809-1882) and Alfred Russel Wallace (1823-1913) was read to the audience at the Linnean Society of London. With the birth of this theory, these two men laid the foundations of Modern Biology and established a new way to understand evolution. However, it would have to spend much time until their ideas were finally accepted as an unquestionable fact. For several years, Darwin was a member, as a naturalist, of a scientific and geographical voyage (1831-1836) through South America and around the World on board of the British naval vessel H.M.S. Beagle. When Darwin arrived to Galapagos Islands located in the Pacific Ocean in 1835, he became fascinated with the local fauna and, particularly, with the amazing giant tortoises to which he devoted some of his writings. It was, precisely, in these Islands where Darwin had the opportunity to observe the already famous finches. These birds were different from one island to another and their beaks had slight variations in the size and shape depending on the food sources (seeds, insects) available on the islands they have colonized. The finches had to adapt to different environments (ecological niches) and over many generations those with most favorable traits were able to survive longer in their respective environments and have more offspring. Alfred Russel Wallace was a recognized and exceptional English naturalist, explorer, geographer, social critic, etc., who traveled through different regions of the world (mainly Amazon river and Malay archipelago- a group of islands between Southeast Asia and Australia-) trying to collect specimens of flora and fauna and relevant information on aboriginal population. Thanks to his thorough work and the numerous expeditions in which he was involved, especially throughout Indonesia, he could collect an astounding amount of specimens of great value to science. Wallace was also an expert on the geographical distribution of animal species and he is known as the father of Biogeography. After some years of observations, analysis and findings, Wallace wrote an essay in 1858 in which he proposed that the mechanics of an evolutionary divergence of species was similar to that due to environmental pressures. Both Wallace and Darwin were inspired on the social economic theory of Thomas Malthus and, particularly, in a work written by Malthus in 1803 entitled: “Essay on the Principle of Population”. They applied to the natural world (plants and animals), the fundamental dynamics of the controversy related to the growth of human population and the insufficient amount of available resources to support it. Wallace did not use the term natural selection as such, but, certainly, his findings were based on the role of natural selection as the driving mechanism in the evolution of species. This essay: “On the tendency of varieties to depart indefinitely from the original type” was sent to Darwin, who had been collecting evidences and tenaciously working on the same idea for almost 20 years. The purpose of Wallace was that Darwin reviewed the manuscript and eventually helped him to publish it. Despite they had worked in a parallel and independent way, the ideas of both naturalists remarkably coincided. In fact, Wallace’s manuscript was very similar to a Darwin’s essay written some years before that had never been published. But the main concern was that Darwin’s theory had not yet seen the light, because during many years, he had exclusively devoted to collecting reliable and convincing evidences that would proof his theory. Once Darwin had spoken with Charles Lyell (geologist, lawyer and friend of Darwin), he and the botanist Joseph Dalton Hooker decided to arrange everything to make public the Darwin’s main conclusions on the subject through an extract from an unpublished Darwin’s 1844 Essay consisting of a portion of a chapter and a personal letter sent in September 1857 to the American botanist Asa Gray. In that letter, Darwin had already outlined his basic ideas on natural selection. These two Darwin’s contributions and Wallace’s manuscript were read out together for the first time at the Linnean Society of London on July 1858. Both works were presented under the title: «On the Tendency of Species to form Varieties; and on the Perpetuation of Varieties and Species by Natural Means of Selection”. However, that day, neither Wallace nor Darwin could attend the event. Wallace could not be consulted in advance (he was still in Malay archipelago) and Darwin was in mourning for the death of his son. Although Darwin’s work on evolutionary theory was more comprehensive and detailed, the similar vision of Wallace contributed to crystallize and support the foundations of the emergent theory of natural selection. It is worth mentioning that these two men kept a good relationship for life.\nThe theory of evolution by natural selection had been presented to the world. From that moment, and despite of unbelief and skepticism that accompanied this theory for several years, nothing would ever be the same. One of the most scientific influential and best-selling books of all times: “On the Origin of Species by Means of Natural Selection, or the Preservation of Favored Races in the Struggle for Life”, written by Charles Darwin was published on November 24, 1859. Linnean Society of London: http://www.linnean.org/\nThis is an instructive interacting tour through the Galapagos Islands developed by NOVA, in which you can explore the different islands of the Ecuadorian archipelago, see amazing landscapes, know the native fauna of the region and learn more about Darwin’s experiences and findings. It is a different way to follow the footsteps of the great English naturalist Charles Darwin.\nEvolution: Darwin vs Lamarck\nDarwin’s Theory of Evolution by Natural Selection:\n- Individual variation: there are variations among individuals within species.\n- Some of these variations are heritable features that pass on to offspring (at that time, the mechanisms by which such variations (heritable features) occurred were not known. They were discovered some years later by Gregor Johann Mendel who established the laws of inheritance and laid the foundations of Genetics).\n- In a population, more individuals are born than can survive. The available resources in nature are finite (food, water, space, etc.) and the environment is not able to support unlimited growth of a population. This causes an inevitable struggle for existence among individuals (continual struggle for existence).\n- Survival and reproduction of individuals do not occur at random. Individuals vary in their ability to survive and reproduce. Those with the most favorable traits will survive longer and have more offspring.\n- Life started out simple and became more complex, organisms changing progressively towards a greater complexity.\n- Evolution occurred through simple organisms originating from spontaneous generation.\n- Acquired characteristics: According to Lamarck’s Theory of Inheritance of Acquired Characteristics (soft inheritance), parents pass on to their offspring only those physiological changes that have occurred in their physical make-up during the course of their existence.\n- Change through use and disuse: Organisms evolve over time due to several environmental factors acting on them. All changes occur according to the need of organisms to adapt to their environment. The new emerging needs force organisms to a differential use or disuse of some body parts. Those body parts that are not being used will tend to gradually disappear, while those that do, will have a tendency to develop better and stronger, and will be inherited by next generations. During this process, every generation adapts its organs and body towards ever-greater perfection.\nThe evolution of a pathogen: Mycobacterium tuberculosis\nThe TB case: TB is one of the oldest and deadliest diseases in the world. In 2012, an estimated 8.6 million people developed TB and 1.3 million died from the disease, including 320.000 deaths among HIV-positive people (Global Health Observatory-WHO). This disease has accompanied man since ancient times and is caused by a Gram-positive bacterium, specifically, an acid-fast bacillus known as Mycobacterium tuberculosis. This bacterium was isolated and identified in 1882 by the German Physician and Nobel Prize winner Robert Koch. One of the biggest concerns today is related to the increased number of cases of infected people worldwide by multi-drug resistant tuberculosis (MDR-TB), including some countries of Western and Eastern Europe. According to WHO estimates, more than 500.000 new cases of MDR-TB arise each year around the world and this rate continues to rise. On the first contact with Mycobacterium tuberculosis, the immune system of most individuals generally controls or clears the infection through a complex network of interactions. This mainly depends on the effectiveness or susceptibility of the host defense mechanisms. Eventually, individuals can experience only a mild form of the active disease (primary TB). However, throughout many years of evolution, this pathogen has developed some effective survival strategies or mechanisms like inhibition of phagosome-lysosome fusion, inhibition of phagosome acidification, etc., to evade the macrophage responses and therefore remains inactive for long periods of time (latency). All of this suggests the existence of an equilibrium between the host immune system and the pathogen that is established and maintained by a wide network of interactions and biological scales. However, if for some reason the immune response fails and is unable to control the infection, as well as in the presence of certain favorable conditions of pH, temperature, oxygen pressure, etc, the infection can be reactivated (reactivation) causing an active disease again. Recent studies suggest that specific pathogenicity and persistence of Mycobacterium tuberculosis can be explained via a combination of different genetic mechanisms such as loss of function of certain genes or the acquisition of new ones via horizontal transfer. “Genomic analysis of smooth tubercle bacilli provides insights into ancestry and pathoadaptation of Mycobacterium tuberculosis”, Nature Genetics 45, 172–179 (2013). The latency of Mtb bacterium is a biological condition in which although the pathogen is not completely eradicated, there are no clinical symptoms of the illness in the host. People with latent TB infection do not exhibit any sign of the disease, since the tubercle bacilli are not active in the body (they remain in a non-replicating state). This ability of bateria is part of their evolutionary strategy. In its origins, Mtb bacterium was less virulent because human populations were significantly smaller and the pathogen needed a host or reservoir to ensure its survival and reproductive success. As the population density was increased and humans began to live in larger groups or communities, the bacterium became more aggressive or pathogenic. Obviously, human interactions favor efficient pathogen transmission from one person to another. After having concluded the most complete work to date on the sequencing of MTB genome (I. Comas et al. Nature Genetics, DOI: 10.1038/ng.2744) and analyzed numerous samples from infected patients distributed in different geographical areas of the planet (in total, 259 samples of TB bacteria), it is absolutely clear that the history of this bacterium is linked to the history of mankind. The team in charge of the research determined that the association between TB and humans is dated to about 70,000 years before humans left Africa. After so many, many years, the fight against the pathogen goes on. You can also read: – J. Sanz, J. Navarro, A. Arbues, C. Martin, P. C. Marijuan, and Y. Moreno, “The Transcriptional Regulatory Network of Mycobacterium Tuberculosis”, PLoS ONE 6(7): e22178 (2011). PDF Version. – José A, Cuesta, Mathematics of Evolution, Newsletter of the EMS 76, 28-37 (2010).', 'The great 19th Century naturalist Alfred Russel Wallace was born on January 8, 1823 (died 1913). Today, Wallace is known mostly as a footnote—the other originator of the idea of evolution and natural selection, aside the much more famous Charles Darwin. His prolific career as a naturalist and social commentator, however, made Wallace one of the most famous and honored men of England during his life.\nWallace was born to a working-class family in Gwent, Wales, and was forced to quit school at age 14 to take up work with his brother as a surveyor. As they roamed southern England and Wales surveying, Wallace took an interest in the landscapes they charted. He bought a book about botany and began to identify the plants along his surveying transects. A naturalist was born! When the surveying business went bust, he joined a local school as a drafting teacher. His interest expanded to beetles, and he never looked back.\nWith a fellow teacher, he left for a multiple-year expedition to Brazil in 1848, at the age of 25. There, he collected specimens of all kinds, with the goal of selling them to collectors back home. After four years in Brazil, he boarded a ship along with his treasure of new and exotic specimens and many live animals. A few days out from port, the ship caught fire and sank. Along with it sank Wallace’s specimens and fortune. The survivors floated in leaky lifeboats for ten days before being rescued.\nUndaunted, Wallace started a second major expedition in 1854, this time to what we now call Indonesia. Collecting was again his agenda, but this time he also planned to investigate an idea that had captured his thinking—evolution. He spent a total of eight years exploring the region, collecting and sending home more than 125,000 specimens and in the process discovering 1000 new species. His most spectacular find was the golden birdwing butterfly; he described the moment he found it:\n“None but a naturalist can understand the intense excitement I experienced when I at length captured it. On taking it out of my net and opening the glorious wings, my heart began to beat, violently, the blood rushed to my head, and I felt much more like fainting than I have done when in apprehension of immediate death”\nHis subsequent book, The Malay Archipelago, was considered the foremost scientific travel book of the period and remains in print today.\nWhile in Indonesia, he began writing extensively about his collections and ideas, including sending his thoughts about evolution to leading scientists in England. When Charles Darwin saw Wallace’s work, Darwin rushed to finish his own writing on evolution. Together they published a paper on the subject in August of 1858, thereby rightfully establishing Wallace as the co-creator of the theory of natural selection. Darwin, however, gets the lion’s share of the credit for these ideas, while Wallace’s stature has continued to shrink.\nAt the time, however, Wallace was widely acknowledged for his work and became a more famous figure than Darwin. Wallace wrote more than 1000 papers and 22 books during his life, many about the natural world, but others branching into realms of social reform and politics.\nThe plight of the two figures—Darwin and Wallace—is in evidence today in the London Museum of Natural History. A marble sculpture of Darwin sits atop the grand staircase leading to the upper floors of the museum; Wallace, however, merits only a portrait on the adjacent wall, hung there many decades after Darwin’s statue. In compensation, perhaps, a relatively new bronze statue of Wallace graces a gallery on the second floor. The statue depicts a moment on his collecting expedition to the Malay, and his gaze extends to the ceiling of the cathedral-like central hall of the museum. Follow his gaze, and you will see a golden birdwing butterfly painted on the distant ceiling.\nSmith, Charles H. The Alfred Russel Wallace Page. Western Kentucky University. Available at: http://people.wku.edu/charles.smith/wallace/BIOG.htm. Accessed January 7, 2017.\nToday in Science History. Quotes by Alfred Russel Wallace. Available at: https://todayinsci.com/W/Wallace_Alfred/WallaceAlfred-Quotations.htm. Accessed January 7, 2017.\nWallace Fund. The Alfred Russel Wallace website. Available at: http://wallacefund.info/ Accessed January 7, 2017.']	['<urn:uuid:0fd87a84-0a14-482c-8742-f27bf628eaa4>', '<urn:uuid:8fbc8919-2796-4d5f-ab38-b71c23d4a481>']	factoid	with-premise	short-search-query	similar-to-document	comparison	expert	2025-05-12T21:52:51.670698	6	76	3028
60	What role do deities play in Taoist philosophy?	Unlike Abrahamic religions, Taoism does not have an omnipotent God who created and controls the universe. Instead, the universe springs from the Tao, which impersonally guides things. The religion features a developing pantheon of gods and transcended humans who still inhabit China's religious landscape today. Key deities include the Jade Emperor, the First Principal, the Three Pure Ones, Three Officials, Three Epochs, and the Eight Immortals.	"['For centuries Daoism (Taoism) has played a central role in the development of Chinese thought and civilization, yet to this day only a few of its sacred texts have been translated into English. Now Stephen R. Bokenkamp introduces the reader to ancient scriptures never before published in the West, providing a systematic and easily accessible introduction to early Daoism (c. 2nd-6th C.E.). Representative works from each of the principal Daoist traditions comprise the basic structure of the book, with each chapter accompanied by an introduction that places the material within a historical and cultural context. Included are translations of the earliest Daoist commentary to Laozi\'s Daode jing (Tao Te Ching); historical documents relating the history of the early Daoist church; a petitioning ritual used to free believers from complaints brought against them by the dead; and two complete scriptures, one on individual meditation practice and another designed to rescue humanity from the terrors of hell through recitation of its powerful charms. In addition, Bokenkamp elucidates the connections Daoism holds with other schools of thought, particularly Confucianism and Buddhism.\nThis book provides a much-needed introduction to Daoism for students of religion and is a welcome addition for scholars wishing to explore Daoist sacred literature. It serves as an overview to every aspect of early Daoist tradition and all the seminal practices which have helped shape the religion as it exists today.\nEarly Daoist Scriptures\nAmong the ""three teachings"" of traditional China, Daoism (Taoism) is the least understood, although it has played a central role in the development of Chinese thought and civilization. This book provides a systematic and easily accessible introduction to the origins and early development of the Daoist religion (c. 2nd-6th C.E.). Representative works from each of the principal early Daoist traditions--the Celestial Master, Shangqing, and Lingbao sects--comprise the basic structure of the work, with each chapter headed by an introduction that places the material within a historical and cultural context. Explanatory footnotes aid the reader in understanding difficult terms and concepts within the texts, while endnotes cover textual matters and arguments of more concern to scholars of Daoism. Included are translations of the earliest Daoist commentary to Laozi\'s [Lao-tzu] Daode jing [Tao-te ching], accounts of the early history of the church, and a petitioning ritual used to free believers from complaints brought against them by the dead. In addition, the book features complete translations of two important scriptures, a Shangqing text outlining meditation practices based on Celestial Master sexual rites that foreshadowed the development of Inner Alchemy [neidan or nei-tan] and a Lingbao ritual text designed to rescue humanity from the terrors of hell through recitation of its powerful charms. Through these works, fundamental aspects of Daoism are revealed in the words of those who helped to shape the religion during the period in which it grew from a small sect cut off in an isolated valley to a major religion with temples spread throughout the land, commanding the attention of rulers and commoners alike during one of China\'s most prosperous periods of history. The texts selected here reveal a developing pantheon of gods and transcended humans, many of whom inhabit the religious landscape of China to the present day. Among the topics covered, the reader will find extended discussions of the nature of the Dao [Tao]; Daoist divinities and concepts of immortality; heavens and hells; cosmogony and millennial expectations; alchemy and meditiation; the makeup of the human body and views of sex; ethics and the prospects for human perfectability. Throughout the work, Bokenkamp elucidates the connections between Daoism and other schools of thought, particularly Confucianism and Buddhism, but also popular religion and traditions of divination. He shows some of the ways that Daoism, while influenced by its rivals, served to shape them in its turn through adapting traditional Chinese systems of thought into a compelling and enduring religious system.', 'Is there a god in Taoism?\nTaoism does not have a God in the way that the Abrahamic religions do. There is no omnipotent being beyond the cosmos, who created and controls the universe. In Taoism the universe springs from the Tao, and the Tao impersonally guides things on their way.\nWhat are the 3 main beliefs of Taoism?\nThe ‘Three Jewels of Tao’ (Chinese: 三寶; pinyin: sānbǎo) refer to the three virtues of taoism:\n- compassion, kindness, love.\n- moderation, simplicity, frugality.\n- humility, modesty.\nWhat are Taoists not allowed to do?\nTaoists believe that good actions will mean a better life for their soul so Taoists follow rules and guides for living. They are not allowed to tell lies, steal, commit adultery, commit murder or drink alcohol. They also have a list of good deeds to further guide they way they live.\nDoes Taoism still exist today?\nToday, Taoism is one of five official recognized religions in the People’s Republic of China. The government regulates its activities through the Chinese Taoist Association. However, Taoism is practiced without government involvement in Taiwan, where it claims millions of adherents.\nWhat are the 4 principles of Taoism?\nDaoism, sometimes spelled Taoism, comes from Ancient China and the writings of philosophers like Laozi, who lived during the 5th or 4th century BCE.\nThese 4 Teachings of Daoism Will Help You Navigate Life\n- SIMPLICITY, PATIENCE, COMPASSION. “Simplicity, patience, compassion.\n- GOING WITH THE FLOW.\n- LETTING GO.\nIs Taoism a Buddhist?\nTaoism is originated in China and many believe that it is started in the sixth century B.C. whereas Buddhism is said originated in the 500’s B.C. in India. Both Taoism and Buddhism belief in reincarnation which means the life after death and both have similar ultimate goals.\nHow is Taoism different from Christianity?\nChristianity emphasizes separation of body and mind/spirit. Body is evil and to be suppressed. Taoism emphasizes mind-body integration. Taoist laugh at themselves, Christians burn people who laugh at them.\nWho is older Jesus or Buddha?\nBuddha (Siddhārtha Gautama) insisted he was human and that there is no almighty, benevolent God. He was born in present-day Nepal roughly 500 years before Jesus Christ (Jesus of Nazareth).\nAre Taoists celibate?\nSexual practices. The Daoist spectrum of sexual activities ranged widely across schools. Some stressed strict celibacy, others mystically married celestial partners, and still others practiced communal ritual intercourse.\nCan Taoists eat meat?\nThe modern Taoist diet essentially respects the basic theory of yin-yang and the 5 elements, it relies heavily on unprocessed whole grains, fresh vegetables and very little meat. In general, all red and blue meats, including pork, snails, rabbits and the like should be avoided at all costs.\nHow often should a man release sperm Taoism?\nIn fact, some Taoist teachers recommend that you only ejaculate 20 to 30 percent of the times you have sex. That translates to 2 or 3 times out of every 10 sessions.\nIs Daoism and Taoism the same?\nTaoism (also spelled Daoism) is a religion and a philosophy from ancient China that has influenced folk and national belief.\nWhat is the Taoism symbol?\nThe ubiquitous yin-yang symbol holds its roots in Taoism/Daoism, a Chinese religion and philosophy. The yin, the dark swirl, is associated with shadows, femininity, and the trough of a wave; the yang, the light swirl, represents brightness, passion and growth.\nHow do you practice Taoism?\nBut unlike most religions, modern philosophical Taoism does not require any specific form of belief or initiation. Becoming a Taoist can be as simple as reading Taoist texts to become familiar with Taoist beliefs. Certain practices – like attending temple, observing feng shui, and meditating – are considered Taoist.\nWhat is Taoism similar to?\nTaoism shares similar principles with Confucianism. The teachings of Lao‐tzu stress the importance of meditation and nonviolence as means of reaching higher levels of existence.\nWhat is the oldest religion?\nThe word Hindu is an exonym, and while Hinduism has been called the oldest religion in the world, many practitioners refer to their religion as Sanātana Dharma (Sanskrit: सनातन धर्म, lit.\nWhat are the main gods of Taoism?\nThe Taoist Deities\n- Yu-huang — The Jade Emperor.\n- Yuan-shih T’ien-tsun — The First Principal.\n- San-ch’ing — Three Pure Ones.\n- San-kuan — Three Officials.\n- San-yuan — Three Epochs (or Principals)\n- Pa-hsien — Eight Immortals.\nCan Taoist monks marry?\nAs for a monk or nun, he or she should never marry or practice sexual intercourse with anyone.”\nHow does Taoism view death?\nTaoism places great value in life. It does not focus on life after death, but on health and longevity by living a simple life and having inner peace. It is said that the human body is filled with spirits, gods, or demons. When people die, it is believed that they should do rituals to let the spirits guard the body.\nDo Taoists have monks?\nA daoshi (道士 “master of the Tao”), translated as Taoist priest, Taoist monk, Taoist master or Professional Taoist, is a priest in Taoism. Along with Han Chinese priests, there are also many practicing ethnic minority priests in China.']"	['<urn:uuid:36d3830d-e4a9-4279-b836-00d66a44d631>', '<urn:uuid:8602bf09-4ecf-4b73-8392-52714cf3f0ae>']	factoid	with-premise	concise-and-natural	distant-from-document	three-doc	expert	2025-05-12T21:52:51.670698	8	66	1498
61	How does the organizational structure and membership approach of the Big Easy Wing differ from the operational command structure of Canada's fighter aircraft program in protecting North American airspace?	The Big Easy Wing operates through a volunteer-based membership system with specific roles like Wing Leader, Operations Officer, and Maintenance Officer, requiring members to join both the CAF and complete a Big Easy Wing application with different membership levels including full, military, and cadet memberships. Meanwhile, Canada's CF-18 program operates as part of a formal military structure, working under the bi-national NORAD treaty with the United States to defend North American airspace, maintaining constant alert status and conducting regular intercept operations of unidentified aircraft.	"[""The Big Easy Wing is a newly formed organization officially designated on Friday, April 28, 2017 by the Commemorative Air Force, Head Quarters in Dallas, TX. We are dedicated to acquiring, restoring and preserving WWII Warbirds to flying conditions. We are dedicated to working toward accomplishing that goal. This is our formative year. We are working hard at fundraising so that we can bring a plane to New Orleans Lakefront airport.\ncytotec zur geburtseinleitung aperture viewing a review and a essay caliban character essay for national junior interferon viagra synthroid labs https://themilitaryguide.org/14days/characterization-in-literature-essay-samples/55/ open coursework for stats https://smartfin.org/science/geflschte-viagra-trkei/12/ https://shilohchristian.org/buy/a-lesson-before-dying-analytical-essay-conclusion/54/ comprar viagra online brasil go https://greenechamber.org/blog/teen-pregnancy-essay/74/ internet and human society essay https://mainejournal.umaine.edu/wp-content/uploads/index.php?generic=online-finasteride-5mg source site help i can't focus on my homework cipro stomach upset follow link seroquel stomach acid watch mice men john steinbeck essays https://cadasb.org/pharmacy/cialis-5mg-rezeptfrei-in-deutschland-kaufen/13/ izet viagra vs cialis high dose lipitor side effects define persuasive essay topics phizer viagra overnight representation of women in the media dissertation viagra pode ser tomado pela metade para que sirve la cialis 20 mg children of the holocaust essay cheap personal essay writers sites for phd Membership–Becoming a member of the Big Easy Wing puts you where the action is- close to the aircraft, maintaining the aircraft and educating the public. Various skills are needed. We need YOU! To join our Wing, you must belong to the CAF. Visit their website to join. Also, be sure to complete the Big Easy Wing application. At this time, we have 3 level of membership for 2018: full membership, active military membership, and cadet membership (a student between the age of 13-23.) You can mail the application to the address listed on the application or email it to firstname.lastname@example.org. Also, you can complete the Hold Harmless Form and bring it to a meeting or email it to Robert Claypool– email@example.com\nBecome a member by clicking on JOIN US\nMeet the 2022 Team —Wing Leader Harold Buchler\nMike Scott–Operation Officer Terry Clausing-Maintenance Officer\nDonald Allen--Assistant Operations Officers Jeffery Segner–Finance Officer\nShirley Colomb–Adjutant Officer Jim Buchler–Educational Officer\nShirley Colomb–Development Robert Claypool–Executive Officer\nRichard Wood–Safety Officer\nClick on JOIN US to become a member, to make a donation, to purchase an item.\nOur vision is to acquire, restore, and preserve WWII warbirds to flying conditions. Having warbirds located here in the area will give our community the opportunity to explore, enjoy and learn about the role these historical aircraft served in WWII. This ties to the Commemorative Air Force mission which is: “education, such that generations of Americans will value and support the contributions of military aviation in assuring our nation’s freedom.” By bringing a Wing to New Orleans area, we will expand on the CAF mission.\nThe Big Easy Wing will work to refurbish, educate and acknowledge the service that these warbirds served in the success of preserving our democracy by following the CAF objectives. CAF is a 60 years old organization.\nOur unit leader and his team are diligently working to acquire an aircraft that will be based at Lakefront airport and/or Hammond Airport.\nBe a volunteer, be a sponsor or a contributor\nOr help us raise funds by booking a flight–coming soon."", ""The Role of Canada's CF-18 Fighter Fleet\nNovember 22, 2016 – Ottawa – Government of Canada\nCanada requires a fighter aircraft to contribute to the safety and security of Canadians and protect the sovereignty of one of the largest expanses of airspace in the world. Our fighter aircraft give the Canadian Armed Forces (CAF) the flexibility required to meet the various tasks set by Government, specifically:\nDefence of Canada: The primary role for CF-18s is to protect Canadian sovereignty. Twenty-four hours a day, 365 days a year, the CF-18 fleet maintains a constant state of alert, ready to respond immediately to potential threats along Canada’s 200,000 km of coastline. The Canadian Armed Forces also use the aircraft to provide air policing during significant events in Canada: for example, CF-18s were heard over British Columbia, where they provided around the clock support to the 2010 Olympic Games, and most recently over Ottawa during the Three Amigos meetings in June 2016.\nContinental Defence: Canada has a bi-national obligation under the North American Aerospace Defense (NORAD) treaty with the United States to help defend North American airspace. Since 2001, NORAD aircraft have conducted over 1,800 intercepts of non-military aircraft of concern because they were unresponsive or could not be identified. No other CAF asset, alone or in combination, can substitute for the critical role of an airborne interceptor.\nMissions Abroad: The fighter fleet gives the Government foreign policy options in an increasingly complex global security environment. Moreover, a fighter element is the fastest-responding capability.\nExamples of CF-18 Operations\nCanada's first operational deployment of the CF-18 took place during the Gulf War in the early 1990s, when Canada sent 24 CF-18s to Qatar to participate in the American-led Desert Shield and Desert Storm campaigns. Similarly, Canada deployed 18 CF-18s to Italy to take part in the Kosovo campaign in 1999.\nIn 2011, they played a vital role during Operation MOBILE, as part of a NATO-led effort to enforce an arms embargo and no-fly zone to protect civilians in Libya in support of the United Nations Security Council Resolutions 1970 and 1973. Seven CF-18s made up Task Force Libeccio, conducting 946 missions, 10 percent of NATO strike sorties, and dropping 696 bombs of various types to engage military assets threatening the civilian population. In 2014, CF-18s helped patrol European skies alongside NATO partners, building confidence in Europe’s stability and security through Op REASSURANCE. And from 2014 until February 2016, Canada’s fighter fleet deployed to Kuwait in support of the allied coalition to fight the atrocities of Daesh in Iraq, and also later in Syria as well.\nToday, working alongside American partners, Canada has crews ready to respond on behalf of NORAD to protect North American airspace.\n- 30 -\n- News Release: Canada announces plan to replace fighter jet fleet\n- Backgrounder: Engagement with Industry and Allied Partners\n- Backgrounder: The Procurement Process Defined: Replacing Canada’s CF-18 Fleet\n- Backgrounder: Ensuring Economic Benefits for Canada\nPublic Services and Procurement Canada\nOffice of the Honourable Judy M. Foote\nInnovation, Science and Economic Development Canada\nOffice of the Minister of Innovation, Science and Economic Development\nSearch for related information by keyword\nReport a problem or mistake on this page\n- Date modified:""]"	['<urn:uuid:96705fc1-3c02-4c65-9e20-5cdd351a8398>', '<urn:uuid:b8f4ef86-da93-4589-83f1-75a3b506453f>']	open-ended	direct	verbose-and-natural	distant-from-document	comparison	expert	2025-05-12T21:52:51.670698	29	84	1060
62	As an aquarium enthusiast studying feeding behaviors, I'd like to know about Bala Sharks' dietary preferences in nature versus captivity, and how their eating habits change as they grow from juveniles to adults?	Bala Sharks are omnivores that adapt well to various foods in both natural and captive environments. In their native habitat, they consume both plant matter and small prey, including invertebrates like shrimp and snails. In captivity, they accept a wide variety of foods including flakes, pellets, freeze-dried and frozen foods, live foods (such as daphnia, bloodworms, brine shrimp, and mosquito larvae), as well as vegetables like spinach and peas, and fresh fruits. As they grow from juveniles to adults, their feeding behavior changes somewhat - while juvenile Bala Sharks are generally peaceful community fish, adults may occasionally eat small fish, particularly sleek species like Neon Tetras. They are notably greedy eaters that continuously search for food, though they can become stressed if fed the same foods repeatedly and may resort to eating algae if variety is lacking.	['A school of Bala Shark can make a dramatic addition to a large aquarium tank. These Southeast Asian fish are not true sharks, but their shark-like appearance and size make them quite an intriguing pet. Here are the facts on the species and how to care for them.\nCommon Names: Bala Shark, Hangus, Malaysian Shark, Silver Bala, Silver Shark, Tricolor Shark, Tri-color Shark Minnow\nScientific Name: Balantiocheilus melanopterus\nAdult Size: 13 inches (35 cm)\nLife Expectancy: 10 years\n|Social||Peaceful, but may eat small fish|\n|Tank Level||All levels|\n|Minimum Tank Size||120 gallon|\n|Diet||Omnivore, accepts all foods|\nEgglayer, not bred in home aquaria\n|Care||Easy to intermediate|\n|Hardness||to 10 dGH|\n|Temperature||72–82 F (22–28 C)|\nOrigin and Distribution\nBala Sharks originate from Southeast Asia in medium to large-sized rivers, as well as lakes. At one time they were found in Thailand, Borneo, Sumatra, and the Malayan peninsula. However, they have become rare in many areas that they originally inhabited and are believed to be completely extinct in some regions.\nThe cause of this drastic reduction of Bala Sharks in their native lands is still under debate. Some believe they were overfished for the aquarium industry, others believe damming of the rivers is to blame, while still others believe pollution is the root cause. All these events quite likely factored into the demise of this fish that at one time was quite prolific in the area.\nRegardless of the reason, there is no question that the Bala Shark is rarely found in its original native haunts. In fact, since 1996 it has been on the IUCN Red List of Threatened Species. Currently, this species is commercially farm bred in the Far East, using hormones to promote spawning. Almost all specimens sold in the aquarium trade are captive bred.\nColorings and Markings\nThis species is known by a number of names, all of which have something in common: the word shark. Even though the Bala Shark is not a true shark, it has a large triangular-shaped dorsal fin and torpedo-shaped body that gives it a distinctly shark-like appearance. That’s where the similarity to real sharks ends, as this species is far more peaceful than the average ocean-going shark (and is a freshwater fish while sharks are ocean-dwellers).\nA member of the Cyprinid family, Balantiocheilos melanopterus has a shiny metallic silver body with well-defined scales, large eyes, and a deeply forked yellow-tinged teal. The dorsal caudal, pelvic, and anal fins are all edged in deep black. This tri-color scheme of silver, yellow, and black gives rise to another of its common names, the Tri-Color Shark.\nPerhaps the most important feature of this fish is its adult size. Usually sold as young juveniles in pet shops, they are only a mere three to four inches, giving potential owners the impression they are suitable for most tanks. What isn’t apparent at this point is the fact that this fish can grow to a foot or more in size, making it suitable for only a very large aquarium. This is particularly true because Bala Sharks are schooling fish that must be kept with others of its own kind. Bala Sharks are active fish that startle easily and will leap out of the tank if it does not have a good cover.\nDue to their peaceful nature, juvenile Bala Sharks do well in a community aquarium. As they mature, however, they will eventually outgrow most tanks, and they continue to grow for several years.\nAlso, be aware that many pet shops will not take large fish back, so take that into consideration before bringing one home. Ask the shop if it makes trade-in arrangements. If all else fails, check out public locations, such as medical offices or other businesses that have large aquariums that could use large fish. The one option that should never be used is to dump a Bala Shark, or any nonindigenous fish, into local waterways. Unwanted fish should be euthanized before opting to dump them outdoors.\nJuvenile Bala Sharks can be kept with a wide variety of fish due to their generally peaceful nature. As they grow larger, though, they will sometimes eat small fish, particularly sleek fish, such as the Neon Tetra. They also tend to stress shy or slow-moving fish, due to their constant vigorous activity in the tank. Balas should not be kept with inverts, such as snails and shrimp, as these are part of their normal diet in nature.\nBalas should be kept in schools, preferably four or more. When kept alone, they tend to be quite timid and skittish. If kept with only one or two others of its own kind, a dominant fish may emerge and bully the others. Therefore, it is wise to keep a larger school of Balas. If the tank is large enough, adult Bala Sharks can be kept with other medium- to large-sized robust fish.\nHabitat and Care\nAs previously mentioned, the key factor in the Bala environment is tank size. An aquarium of 125 gallons is needed to keep a school of adult Bala Sharks. Since they are active swimmers, a longer tank is recommended. In warm climates, ponds are also an option for this species, but they should only be kept outdoors in locations where it is warm year-round.\nWater should be soft to medium-hard, slightly acidic, and 72–82 F (22–28 C). They are sensitive to water conditions, particularly low water temperatures, and are susceptible to white spot when temperatures drop too low.\nThe tank should be fitted with a good cover, as this species jumps when startled. Decorate with large robust plants around the periphery of the tank, leaving plenty of open swimming space in the center of the tank. Floating plants are also suitable to deter the fish from jumping out. Provide smooth rocks and driftwood to round out the décor. Filtration should be robust to ensure good water movement and high oxygen levels in the water column.\nThis species is an omnivore that is not fussy about what it eats. Bala Sharks accept flake foods, pellets, and freeze-dried and frozen foods. They also voraciously accept live foods, including Daphnia, bloodworms, brine shrimp, mosquito larvae, and tubifex. Vegetables should be included in their diet, and they readily consume fresh veggies such as spinach and peas, as well as fresh fruits.\nMost of the time, there are no obvious external differences between the sexes. However, during the spawning season the female develops a noticeable rounder underbelly than the male.\nBreeding the Bala Shark\nBala Sharks have not been successfully bred in home aquaria, although occasional undocumented reports surface. The primary issue is tank size, as well as the lack of information regarding the required conditions for breeding. Interestingly enough, this species is bred commercially. In fact, most specimens for sale are commercially bred in the Far East. However, these commercial breeders employ the use of hormones to induce spawning, so natural spawning conditions are still largely undefined.\nMore Pet Fish Breeds and Further Research\nIf you’re interested in similar breeds, check out:\nOtherwise, check out all of our other freshwater pet fish breed profiles.', 'Bala Shark – The Complete Care Guide of This Sparkling Beauty\nWhat about keeping a shark in your freshwater aquarium? Before you get a spine-chilling feel, you should know that it is a shark (precisely a mini-shark) by name only. Bala Shark or Silver Shark is not at all related to those ocean-dwelling Sharks you see in movies. Rather, it got the name just because of the similar structure and appearance. Impressive indeed!\nBut what about caring for the Shark? If you consider that it is going to be extremely difficult, it’s high time to come out of misconception. It is a large yet very friendly and peaceful fish that can transform your boring freshwater aquarium into a pleasing centerpiece.\nKey Specifications of Bala Shark\n|Scientific Name||Balantiocheilos melanopterus|\n|Origin||South-east Asian Countries|\n|Size||Maximum 12” (30.48 cm)|\n|Color||Black and grey with yellow gradients|\n|Temperament||Peaceful but semi-aggressive to small fish|\n|Compatibility||Medium (with peaceful and small fish)|\nThis fish is also known as Hangus, Tricolor Shark, Silver Bala, Silver Shark, Bala Shark Minnow, Malaysian Shark, and Tri-Color Minnows. Its colors give a metallic look that attracts the fish keepers at once. In 1850, Pieter Bleeker first found this species. At present, the number of this one is decreasing fast due to the massive change in their habitat.\nOrigin and Habitat of Bala Shark\nThis fish is originally from South-East Asia; to be more specific, it is available in Thailand, Indonesia, Malaysia, Cambodia, Kalimantan, Borneo, and Sumatra in large numbers. In Chao Phraya basin and Mekong Basin in Thailand, it used to be available in huge quantities. They are found in the lakes and medium to large rivers of these areas.\nRight now, in most of its natural habitats, it has become extinct. However, trade for the aquarium is not mentioned as a reason behind this, but pollution and deforestation are two definite reasons behind this. This fish is an inhabitant of clean water, and it is simply unable to cope up with an increasing level of water and air pollution. Fish farms are now trying to breed this fish in their natural habitat to meet the demand of aquarists.\nAppearance of Bala Shark\nIt looks like the sharks because of the torpedo-shaped body and slanting, triangular-shaped dorsal fin. It is a beautiful looking fish whose scales have a different sickle-like shape. The fluke of this fish is seen to be bifurcated. Overall, it has a slender structure. Females are larger than males, and they don’t have the slender structure like males, rather they are round.\nTheir eyes are larger in comparison to their small heads. These big eyes help them to see the prey clearly and have a better focus at the time of hunting. It has a flattened body from both sides and its body is covered with very large silver scales.\nSize of Bala Shark\nAt the juvenile stage, this fish is 2” (5.08 cm), but adult ones are 10-12” (25.4-30.48 cm). Precisely, in their natural habitat, they are generally 13-14” (33.02-35.56 cm). But in captivity, they can be found 10-12” (25.4-30.48 cm).\nColor of Bala Shark\nDifferent shades of black, grey, and golden can be seen on their bodies. All their fins (dorsal, pelvic caudal, and anal) have different shades of golden yellow, followed by a black edge. Ventral fins are sometimes found to be mono-colored as well. Among the adults, this golden-yellow stretch is seen to turn into white.\nSilver-steel scales of this fish have truly made it a sparkling fish. Towards the fins, the scales are light golden. The density of the scales of this fish makes its appearance even shinier. In some parts of its body, silver with tinges of teal can also be noticed among the juvenile ones. It looks splendid in ample lighting conditions.\nBehavior of Bala Shark\nIt is generally a peaceful fish, and they stay in the mid-level of the aquarium. Being the fish of mid-level, they do not tend to dig a hole in the substrate like the bottom dwellers. On the other side, they prefer to swim, and they need more and more space to swim. They only go towards the bottom to fetch the food in case the food particles sink before eating and at that time, they make sounds.\nThey are definitely unlike the ocean Sharks, and they do not mess with the aquarium decors or plants.\nThis fish is usually active, but sometimes they like to take rest in the caves and logs, and during their resting time, if you tap on the aquarium from outside, they get startled very easily. Also, they take some time to cope up with the environment of the aquarium.\nLifespan of Bala Shark\nIn a properly maintained aquarium, Bala Sharks can live up to ten years. However, their lifespan in the wild is longer than this.\nDiet of Bala Shark\nThis fish is omnivorous, and an aquarist will get several food options for them. It is seen to be stressed due to the same type of foods; therefore, variations are needed.\nWhen it comes to diet, it is a very greedy kind of fish, and right after eating, it starts looking for food at the bottom of the tank. In spite of being greedy, they will refuse the food because of the lack of variations and live on algae.\nAs they are core omnivores, they can eat the plant substance as well as the meat in their natural habitat. When it is in the aquarium, you have to combine the plant materials with the non-veg items in a balanced proportion so that the fish can get all the nutrients from the food. A list of food options is given below to assist you:\n- Dry Seaweed\n- Shelled peas\n- Small pieces of fruits\n- Freeze-dried vegetables\n- Tropical flakes and Algae Wafers\n- Brine shrimps\n- Insect Larvae\n- Small insects\n- Chopped Earthworms\nEnsure that your pet fish is consuming the food in five minutes. Determine the quantity of food according to that. It is also recommended to feed the fish twice or thrice. Make sure you do not overfeed the fish. When you are feeding the fish twice or thrice, make sure your fish is consuming that food within a couple of minutes.\nKeep it in mind that this fish grows quite large. Therefore, the protein is a must in their diet. If you are feeding it twice or thrice a day, one meal among them has to be composed of fleshy food items. In case, you feel that you have overfed the fish, you are free to feed it in less quantity in its next feeding.\nAlong with the store-bought foods, live and frozen foods are required to keep them active and playful. The quantity of herbivorous flakes and pellets should be lesser than non-veg foods as they will eat algae often.\nSkip Bloodworms which are preferred by most of the freshwater species, as Bala Shark may have some digestion problems for eating this.\nTank Requirements of Bala Shark\nIndeed, food is the secret of a healthy fish, but Bala Shark has typical tank requirements. It will fall sick, and they will be stressed due to improper tank conditions. You need to be very careful while making these adjustments. Know about the ideal tank for this fish:\nIn the pet shops, you will get the juvenile ones, which are just ¼ in size, which can mislead you that your small tank will do well for them. But an adult one is very large which means that a spacious tank will be required. In addition to that, this fish is among the shoaling fish which prefer the individuals from own community. It is generally advised to keep four Bala Sharks in a tank for which you will need a minimum 150-gallon tank.\nYou can keep 2-3 pairs of juvenile fish in a 70-80-gallon tank, but as soon as it becomes mature, you have no other options except upgrading into a bigger tank. In case you want to keep a pair or a single one in a community tank, the size should be minimum 150-gallon because it has to be kept with other large fish.\nThey are very fast swimmers, and as soon they will see that there is a shortage of space in the aquarium, they will be stressed. For a single adult Bala Shark, a minimum 45-gallon tank will be required.\nThis fish is among the jumpers, so you cannot keep the tank uncovered. It stays peaceful most of the time, but it gets startled easily and jumps right then. A tight lid is required to keep the fish secured because they can jump out of the aquarium and die.\nAs this fish is too shiny, it will be better to get a substrate of dark pebbles. It is not a bottom dweller or hole-digger, so, it is unnecessary to use sand. Also, it enjoys swimming most of the time, so hiding places like caves will not be required much. 1 cm thick substrate will be good for them.\nThe use of aquatic plants is completely optional, but if you want, you can use the floating plants with thick leaves and strong roots. There is a possibility that the fish will unintentionally damage the plants at the time of swimming. Water Wisteria can be a good choice here. Duckweed, Anubias, Hornwort, Dwarf Water Lettuce and Cabomba can be used in this matter.\nA filter is mandatory as this fish is intolerant with polluted water. In its natural habitat, they are used to the fast-flowing water, which you can replicate with the filter. A tank with Bala Shark should be clean and oxygenated. With the help of a canister filter, you can keep the tank clean and produce low to medium current. Alternatively, you can use the powerheads for current.\nIt prefers natural lighting, and if that is not possible, you can use any mild lighting. Simple aquarium lamps for eight hours will be sufficient for them.\nDriftwood will be a good décor here, along with small rocks. Actually, this fish is more concerned about the space of swimming it gets in the aquarium, and it is least bothered about the decors. If they get space to swim, it will not displace the decors, or they will not hide. Initially, they are stressed, and at this time they tend to hide in the logs. So, small logs and caves will be alright for the first few months. Avoid any décor with sharp edges or light color as it will make the fish pale.\nWater Type for Bala Shark\nFor Bala Shark, water parameters need to be strictly maintained, and it sometimes becomes difficult for the aquarists. Know these in detail:\nThe temperature has to be between 72-82°F (22.2-27.8°C), but 77°F (24°C) is considered as the ideal water temperature for them.\nThe range of pH levels of such tanks will be between 6.5 and 7.8. Higher or lower from this range will make the fish unhealthy, and they will become prone to various diseases.\nMaintain the water hardness within 10-13 dGH, even though Bala Sharks are not very sensitive to water hardness.\nJust like any other aquarium fish, nitrate is extremely harmful to Bala Sharks. Nitrate level should be the least possible, and to do so, food wastes, and fish excretes need to be removed regularly.\nUse a vacuum siphon to clean the tank. It is necessary because Bala Sharks feed on the sinking foods, which may be stuck to the aquarium base. Manual cleaning can also be done to make sure you do not use any chemicals or soap to clean the dirt from the substrates.\nThe natural habitat of Bala Sharks is the origin of the rivers and the bends where water is clearer than the rest of the river basin. Therefore, cleaning the aquarium regularly is mandatory. It will be good to change one-third of water each week.\nCompatibility of Bala Shark\nThis fish is likely to stay within its own community due to its shoaling tendency. You can add 5-6 Bala Sharks in your tank, but make sure your tank is not overcrowded. In a small tank, you can keep a pair as well, but keeping it alone is not a good idea. You can notice their initial stress and aggression are getting reduced if you keep a small group of four. Also, they will soon get used to the aquarium and engage in playful activities if they are kept in the group.\nSuitable Tank Mates of Bala Shark\nIt is a peaceful fish, and you will see them staying in harmony with various large fish. Also, their aggression will be suppressed with these fish. It is recommended to keep the same sized fish and slightly larger ones with this fish. You can keep them with peaceful to semi-aggressive fish too. Following are the suitable tank mates for this fish:\n- Any large Gourami\nBefore adding Bala Sharks to a community tank, put four or more Bala Sharks together because they are stressed for many days initially. Keeping them together will help them to settle in smoothly. They get easily scared, and adding them in the community tank right at the beginning will startle them, which will result in their attempts to jump out of the aquarium.\nUnsuitable Tank Mates of Bala Shark\nIn case you want to raise this fish in a community tank, beware of keeping them with small fish, which is generally a part of their diet in the natural habitat. Extremely large and carnivorous fish are also restricted as they will eat the Bala Sharks. The unsuitable tankmates of Bala Sharks are as follows:\nBreeding of Bala Shark\nCaptive breeding of this fish is not successful because of their large size. Also, their ideal breeding requirements are still unknown. They are egg-scatterers, which makes home aquaria breeding impossible. However, breeding in fish farms in Thailand and some other Asian countries have some successful reports. For this reason, commercial breeders often use hormones to spawn in order to meet the demand of the aquarists.\nCommon Diseases of Bala Shark\nIt is an extremely sensitive fish that generally does not catch diseases if aquarists follow strict guidelines and parameters for this fish. Due to negligence in water change or faulty filters or temperature or feeding issues, this fish can have Dropsy and Ich.\nIt is a bacterial infection which makes the fish fat. In most of the cases, excessive fluid is seen in the fish. Some antibiotics can control this disease. Since the size of this fish is quite big, get experts’ advice to know about the dosage.\nThis disease affects the scales most, and the fish rubs its body against the gravels to heal itself. Along with that, small white spots can be seen in its body. It is a parasite that goes into the body of fish from foods. You can adjust temperature and apply aquarium salt to heal the fish. If these two do not work, you can apply the copper or potassium-based solutions in the water to control the growth of the parasite.\nBala Shark is for experienced fish keepers, and you cannot neglect its requirements. It is an extremely sensitive fish that cannot tolerate a slight mismatch in the tank and water requirements. You have to keep this fixed in your mind before petting this fish.\nWell, difficulties are a part of fish keeping. But are you willing not to pet such a magnificent fish just because of these difficulties? Of course not! You have to put some efforts for this fish, and in return, it will fill you with the joy of keeping a shiny look-alike of shark in the aquarium. Now, that sounds really cool, right?\nInteresting facts about Bala Shark\n- In the natural habitats, Bala Sharks do not look after their young ones. The fries are self-dependent from the beginning, and they find a group and grow up with them.\n- The main reason behind the extinction of this fish is the adults eating almost all their eggs. So, it becomes difficult to collect their eggs from the wild. Presently, fish farms are playing a major role in breeding them.\nSome Other Mini-Sharks for Freshwater Aquarium\nIf you find Bala-Shark interesting, the following species will definitely interest you:\n- Iridescent Shark – Pet a shark that is always active and never sleeps! Also, it is a peaceful Shark that needs the least aquarium adjustments.\n- Red Tail Shark – Get the feeling of ocean-dwelling Sharks with this semi-aggressive species. Know how to handle them.\n- Rainbow Shark – Know about the multi-colored mini-Shark, which becomes aggressive at different times to attack their enemies.']	['<urn:uuid:1e4268c4-a3ef-4192-956c-c60287949533>', '<urn:uuid:4124f429-e19d-4734-bbe4-cd1e91d6d563>']	open-ended	with-premise	verbose-and-natural	distant-from-document	multi-aspect	expert	2025-05-12T21:52:51.670698	33	137	3967
63	As a periodontist, I'm curious about the connection between gum disease and tooth removal. What's the process?	Periodontal disease can necessitate emergency tooth extraction when left untreated. It can cause gum recession, loss of jaw bone structure, and loose teeth. Poor oral care, certain medications, and genetics can contribute to periodontal disease. If gum disease becomes severe, it can cause degeneration of the bones and tissues surrounding a tooth to the point where the tooth either falls out or needs extraction to prevent further damage to remaining teeth. Regular professional cleanings are important to prevent this progression.	['Will an emergency dentist extract a tooth?\nSometimes, a dental problem can only be resolved by extracting the tooth. A number of issues can lead to extraction including impacted teeth, extensive decay, dental disease, or fracture. Wisdom teeth are often extracted to ensure the health of the rest of the mouth. A patient may even need a tooth extracted under emergency circumstances for one of the above reasons or even after an accident. Let’s take a deeper look into emergency dental extraction.\nDecay in a tooth can spread to other healthy teeth so there may be an instance in which decay is too extensive so that too much of the tooth structure is compromised and cannot be properly restored. Your dentist may advise having a tooth extracted to restore the health of your mouth.\nPeriodontal disease can also lead to an emergency extraction. Left untreated, periodontal disease can lead to gum recession, loss of jaw bone structure, and loose teeth. Poor oral care, certain medications, and even genetics can cause a patient to suffer from periodontal disease which is why it is so important to maintain a regular schedule with your dental office for professional cleanings. If a tooth does become loose, it may need to be extracted to prevent further damage to remaining teeth.\nInfection in a tooth is another reason that a tooth may need to be extracted under emergency circumstances to ensure the health of remaining teeth. Most dentists will prescribe a round of antibiotics as a first step to see if the infection can be eradicated without extraction. An infection is indicated by sensitivity to hot and cold foods and beverages, swelling in the area, and even a fever. If you experience any of these, it is important to visit your dentist as quickly as possible so they can examine the area, diagnose the problem, and begin treatment as soon as possible.\nOvercrowding is another reason that teeth may be extracted. This will not usually be an emergency extraction but sometimes, as with wisdom teeth, it may be more dire than in other circumstances such as a precursor to orthodontic treatment. Whether or not they are erupted, wisdom teeth can be more challenging to properly clean and are more susceptible to decay, which can damage neighboring teeth.\nImpacted teeth, which just means that they never erupted through the gum tissues, can develop an infection which can spread to neighboring teeth. Having impacted teeth extracted helps ensure the health and integrity of surrounding teeth.\nAnd finally, teeth may need to be extracted after an accident such as a fall or a car or bike accident. Your dentist is always going to do all that they can to save the tooth but after they evaluate and x-ray the area, it may be determined that the tooth cannot be saved and will need to be extracted. Sometimes, a tooth can fracture and if the fracture is too deep, the tooth may not be restorable.\nMore on Emergency Dentistry : Finding an Emergency Dentist Near Me', 'Tooth extraction is a procedure done to remove a damaged or potentially problematic tooth. Like every dental procedure, the wound created by the extraction process heals over time with proper care. The dentist will provide the necessary aftercare instructions to follow.Aside from using the recommended medications and resting adequately, there are certain things to avoid…\nWhen Is a Tooth Extraction Necesssary?\nWhile dentists will often do everything possible to save a tooth no matter the age of the patient, there are some situations where tooth extraction is necessary. When it comes to removing teeth, there are two different types of extractions a dentist or oral surgeon can use depending on the reason for removal: Simple extraction and surgical extraction.\nA simple extraction can be used when the tooth is already fully emerged past the gum line and can be easily removed using forceps. A surgical extraction involves cutting into the gums to retrieve a tooth, usually because it is impacted. Here are some common reasons a patient might need to have either a simple or surgical extraction performed.\nReasons for a simple tooth extraction\nA dentist might suggest pulling a patient’s tooth because of infection or overcrowding in the mouth. Continue reading to learn more about each of these reasons for a tooth extraction.\nTooth decay or gum disease\nFillings or root canal treatment can often treat minor cases of tooth decay, but severe deterioration can warrant the need for extraction. This situation often occurs if the patient has not had a dental exam in a while and the decay has gone untreated for a long period of time. Eventually, the decay can extend to the pulp of the tooth leading to an infection.\nIf the infection is too severe to be treated by root canal therapy, the extraction of the tooth is necessary to keep the infection from spreading. Gum disease is another type of infection that causes the degeneration of the bones and tissues surrounding a tooth. If not addressed early on, these structures can deteriorate to the point that the tooth either falls out or needs to be pulled.\nIf teeth are severely misaligned or overcrowded, it might be necessary to pull teeth to open up space in the mouth or to make an opening for an impacted tooth. Most often, the removal of the tooth or teeth is combined with orthodontic treatment. Braces are used after the tooth is removed to further expand the jawbone and balance the bite.\nImpacted teeth and surgical extraction\nAn impacted tooth is one that has been blocked from emerging from the gums and is often discovered during a dental X-ray. Some impacted teeth will eventually break through the gums without treatment; also, removing baby teeth or orthodontic treatment can sometimes allow the teeth to come in properly. Other times, tooth extraction is necessary if the teeth are unable to erupt through the gums on their own. Often, surgical extraction is used on wisdom teeth or maxillary canines that cannot be guided to the correct position.\nTypically, a dentist will do everything possible to preserve a tooth if possible before choosing to extract it. However, if the patient is suffering due to an infection of the tooth or gums it might be necessary to quickly remove the tooth to avoid even more harm from occurring. If teeth are overcrowded or impacted, a treatment plan can be made to determine if a tooth extraction is the best line of action.\nCheck out what others are saying about our dental services on Yelp: Tooth Extraction in Irvine, CA.\nAfter a tooth extraction procedure, it is important to follow the aftercare tips provided by the dentist to ensure full healing and prevent complications. The first few days after the treatment are crucial, and a quick recovery will mean that you can resume your normal activities as soon as possible. This article highlights a few…\nTooth extraction is a relatively common procedure as teeth become decayed, which results in a need for removal. Many people who are living with problematic teeth automatically assume that these teeth need to be extracted from the mouth, however, sometimes they can be salvaged by a general dentist. It is good to know when a…\nThe last thing you want to do after a dental emergency develops is to drive longer than necessary to an emergency dentist because you were unable to find a qualified one near you. This guide provides useful tips to help you find an emergency dentist so that you can get the prompt treatment you need…']	['<urn:uuid:f0e8218a-acc9-4c4e-9d0b-781ebb8c3dd9>', '<urn:uuid:9b2d4a48-ad37-41d7-8d95-c816a2efb729>']	open-ended	with-premise	concise-and-natural	distant-from-document	three-doc	expert	2025-05-12T21:52:51.670698	17	80	1266
64	When can businesses claim losses from social disruptions?	Businesses can claim losses in two major scenarios: First, during civil disruptions/riots through commercial property coverage for physical damage, business interruption coverage for lost revenue (after waiting periods), and civil authority coverage when government orders limit access. Second, during pandemics, claims may be possible if the disruption undermines the contract's essence and wasn't an allocated risk, though standard business interruption insurance typically only covers physical damage rather than pandemic-related losses.	['Wednesday, September 23, 2020\nVirtual Symposium Part IX: Hanoch Dagan and Ohad Somech Part I, Failures of Basic Assumptions\nWhen Contract’s Basic Assumptions Fail: From Rose 2d to COVID-19\nWhen we started this project, in May 2019, we set out to develop a liberal account of three contract law doctrines: mutual mistake, frustration, and impracticability. These three doctrines tackle a fundamental problem in contracts: the failure of a basic assumption shared by the parties (or, FBA as we call it). At that time, the FBA doctrines were of theoretical importance, but of little practical use. This is no longer the case. As prior contributions to this Symposium point out (Stephen Wilks; Andrew Schwartz; and Mitu Gulati and Mark Weidemaier), frustration and impracticability are expected to feature heavily in Coronavirus litigation. We thus take the crisis – that is: both the virus and the measures to contain it – as a case study for our FBA theory.\nA Liberal Account of the FBA Doctrines – Part I: What Makes an Assumption Basic? Does the Pandemic Constitute an FBA?\nWe begin with first principles. Contract is an empowering practice that is, and should be, guided by an autonomy-enhancing mission. Contract’s operative doctrines allow people legitimately to recruit others to their projects by committing their own future selves in return, thus advancing a joint plan. This commitment necessarily curtails the self-determination of the parties’ future selves. Therefore, liberal contract law must ensure that by advancing the autonomy of the parties at the time of formation it does not overly compromise the autonomy of their future selves. This maxim is the key to understanding the FBA doctrines.\nThese doctrines allow a party to withdraw from the contract if: (i) a shared basic assumption failed; (ii) the party seeking excuse did not assume the risk; (iii) the failure had a material effect on the exchanged considerations; and (iv) the court does not find it reasonable to allocate the risk to the party seeking to withdraw. These elements help address the two questions the FBA doctrines invoke: (1) what makes an assumption basic; and, (2) when an FBA occurs, what are its legal consequences? In this Part of our contribution we focus on the first question. We will turn to the second question in Part II.\nA focus on the autonomy of the parties’ future selves implies that an assumption is basic if its failure undermines the agreement’s autonomy-enhancing properties. This means that a basic assumption is one which pertains to the essence of the parties’ agreement – the raison d’être of their joint plan.\nDivining the essence of an agreement does not call for a metaphysical inquiry (the pertinent question in Sherwood v. Walker is not “what is the essence of a cow?”). As with other doctrinal contexts, an autonomy-enhancing contract law takes an objective approach, which relies on prevailing conventions. It examines the type of contract at hand, and more particularly the way actors in the relevant market typically understand the main purpose of this contract type. A careful reading of courts’ decisions (e.g., Florida Power & Light v. Westinghouse, Lewanee Bd. of Health v. Messerly, and Dingeman v. Reffitt) confirms that this is, in fact, what they actually do.\nWhat does this finding mean for COVID-19 related litigation? Most parties to contracts formed before the virus began to spread did not consider the possibility of a global and lasting pandemic or the measures countries have taken (and are still taking) to contain it. Does this imply that the parties’ presupposition as per the absence of a pandemic is tantamount to a basic assumption? The answer varies, as we’ve just noted, across contract types.\nTake, for example, rental agreements and focus first on residential ones. Renters, especially if they lost their job because of the crisis, may seek to be excused from the obligation to pay rent based on an FBA. We think that people experiencing economic upheaval due to the crisis should be certainly aided by the State; and we do not necessarily rule out the possibility that other contractual doctrines may be applicable (on this subject, see Cathy Hwang and David Hoffman’s recent work). But we think that the FBA doctrines are not.\nThe reason is twofold. First, renters and landlords rarely share assumptions about renters’ financial abilities. Instead, they usually acknowledge the uncertainty and allocate the risk. Second, and more importantly, COVID-19 simply did not trigger an FBA in this type of contract. The joint project in residential rents is housing for money, and the virus did not change people’s need for a place to live or their ability to use the property for that purpose. COVID-19, that is, did not undermine the essence of residential rent agreements.\nCommercial rents – for example, of a restaurant – are different. To be sure, like in residential rents, parties to commercial rents do not usually share assumptions about the renter’s financial ability or profits. But they do share the assumption – namely: they take for granted – that eating at restaurants is a socially acceptable behavior. When this stops being the case – for example, due to an imposed lockdown or because the virus caused our social habits to change – COVID-19 is deemed to have caused an FBA.\nAnother example can further highlight the factual aspect of the inquiry. Consider agreements between students and their colleges or universities. We understand the essence of this contract type to be tuition for education. If this is indeed how most parties understand it and if online teaching can (roughly) accomplish this goal, then COVID-19 did not undermine its essence. It has, however, been quite a while since either of us was a student and we may be wrong about the facts. If most relevant actors – that is, students and universities – consider the cultural and social parts crucial aspects, which are part of the essence of their agreement, then COVID-19 may have indeed caused an FBA.\nThus far, we asked whether COVID-19 undermined a basic assumption. But it is also important to examine whether what was undermined was indeed an assumption. Going into a contract, the parties hold various beliefs; but not all beliefs are shared assumptions. An assumption is something taken for granted by the parties – not something that they contemplated, were aware of its uncertainty, and explicitly or implicitly allocated its risk. When the parties are aware that something is uncertain, then, proceeding with the agreement implies that they made the choice to bind their future selves despite the risk. Enhancing their autonomy in this case requires to respect their choice.\nHere too a conventionalist approach is in order. That is, to understand what risks the parties have implicitly allocated, we need to consider the risks that are salient to actors in the relevant market. One way to perform this task is to examine the insurance policies market actors typically procure and inquire which types of risks are covered and especially which risks that not covered are nonetheless made salient.\nTake again the restaurant example. Commercial renters typically procure “business interruption insurance” that covers renters’ lost profits. Does this indicate that the parties were aware of the relevant risk? As it turns out, business interruption insurance pertains mostly to lost revenue due to physical damage to the property. If this is indeed the case (see Christopher French’s work), then it is only this type of risk – and not lost profits due to the coronavirus crisis – that is made salient by the policy, and that, whether actually covered or not, can be assumed to have been allocated by the parties.\nFinding that an FBA had occurred brings us half way through the analysis. We discuss the second half – the legal consequences of an FBA – in Part II.', 'Coverage for Commotion: Insurance for Businesses Affected By Rioting and Vandalism\nBusinesses can endure substantial property damage and, perhaps more importantly, lost revenue from civil disruptions caused by rioting and vandalism. Fortunately, business owners have a multitude of coverage options for these disruptions and should firmly resist an insurer’s attempt to deny or limit coverage.\nAssume you own the fictional pub “Paddy’s Pub” on Sunny Lane in South Philadelphia. One evening, South Philadelphia residents peacefully march down Sunny Lane in protest of the latest government budgeting decision. At some point, rioting and vandalism ensue and a few residents begin vandalizing businesses on Sunny Lane—including Paddy’s Pub. To contain the situation, the police department shuts down Sunny Lane for the remainder of the day. What coverage may be triggered?\nCommercial Property Coverage: Upon first entering Paddy’s Pub, you notice the storefront glass is shattered ($10,000 to replace), the green door is off its hinges ($500 to replace), the pool table is slashed open ($2,000 to replace), the bar stools are broken ($5,000 to replace), and several food and beverage items are missing ($40,000 to replace).\nPaddy’s Pub likely has coverage under its commercial property policy for its damaged property and stolen merchandise (total damage: $57,500), which is subject to policy limits, sublimits, and deductibles. For example, Paddy’s Pub may have a $5,000 deductible and a $2,500 sublimit for the storefront glass. In that instance, Paddy’s Pub could recover $45,000 of the $57,500 in damages to its property and stolen merchandise.\nIn general, standard commercial property policies cover physical loss or damage to the insured’s building and contents as long as the damage was caused by a covered peril. Civil commotion, rioting, and vandalism usually are covered perils or, if a policy is written on a broader form, they are not usually excluded perils. In contrast, losses as a result of war and military action often are excluded perils, though policy language varies on exactly what is excluded.\nIf Paddy’s Pub has “replacement cost” coverage, it may recover the costs to replace the damaged property with new property, usually limited by the policy to “like kind and quality.” Otherwise, Paddy’s Pub may receive only the “actual cash value” of the property at the time of loss. The current value of any item in Paddy’s Pub is minimal, so “replacement cost” coverage is preferred over “actual cash value.”\nDebris Removal Coverage: You also notice that a significant amount of debris is left in Paddy’s Pub after the rioting—all of which must be cleared before Paddy’s Pub can be cleaned and reopened. You estimate that the removal will take $1,000. Paddy’s Pub likely has coverage under its property policy for the $1,000 in debris removal costs.\nProperty policies generally provide “debris removal” coverage for costs incurred to clean-up debris, as long as the debris resulted from a covered peril. Because rioting and vandalism usually are covered perils, Paddy’s Pub may receive the $1,000 in debris removal costs.\nBusiness Interruption Coverage: You estimate that, after five days of cleaning and restocking, Paddy’s Pub will be ready to reopen. During these five days, you estimate lost revenue at $20,000 per day.\nPaddy’s Pub likely has coverage under its policy for business interruption losses (generally defined as lost revenue less saved expenses) for the five days it is required to shut down due to the damage caused by rioting and vandalism. Many business interruption policies, however, have a “waiting period” requiring the business to be closed for a certain number of hours before this coverage begins. In this instance, if Paddy’s Pub’s policy had a 72 hour waiting period, Paddy’s Pub may receive the lost revenue at $20,000 per day for only two of the five days it is closed. Paddy’s Pub, therefore, may only receive $40,000 of the $100,000 in lost revenue (less saved expenses).\nGenerally, business interruption policies cover loss of revenue due to physical damage of such magnitude that the business must interrupt, suspend, or limit its operations. These policies also include coverage for continuing expenses while the business is inoperative, such as payroll. The amount of loss depends on how long the business operations are suspended or limited. The loss generally does not begin accruing until after the ”waiting” period (many policies set that period at 72 hours) after the damage and typically ends when the property should be repaired, rebuilt, or replaced (with reasonable speed and similar quality, plus any additional time necessary to restore the business to its pre-loss level of operation. These, of course, are often points of contention between the insured and insurer, and the policy language and courts’ interpretation of that language will control.\nContingent Business Interruption Coverage: If the riots had been in Pittsburgh (rather than Philadelphia) and the Pub’s supplier in Pittsburgh suffered damage (rather than Paddy’s Pub), Paddy’s Pub may have “contingent” business interruption coverage for any stoppage or downturn resulting from the damage suffered by its supplier and the supplier’s corresponding inability to make timely shipments. For example, even if Paddy’s Pub experienced no damage, Paddy’s Pub may still have contingent business interruption coverage for the resulting downturn in its business from lack of beer.\nExtra/Expediting Expense Coverage: You determine that Paddy’s Pub can reopen in five days only if you expedite all shipments of replacement items ($10,000 to expedite) and pay your employees overtime to remove graffiti from the walls ($7,000 per day). Paddy’s Pub may have coverage under the business interruption policy for the $10,000 in expedited shipment costs and the $7,000 per day in overtime pay (totaling $45,000).\nMost business interruption policies cover “extra expenses” an insured incurs to expedite a business reopening or otherwise reduce its losses, as these “extra expenses” are intended to reduce the lost revenue from the business being closed. Waiting periods, if in the policy, do not usually apply to “extra expense” or “expediting expense” coverage.\nOrder of Civil Authority Coverage: Then, you learn the City is imposing a weeklong curfew to contain the ongoing rioting and vandalism—all businesses on Sunny Lane must close by 5 pm. You estimate that closing at 5 pm reduces revenue by $15,000 per day. So, although Paddy’s Pub would otherwise be fully operational in five days, it will have limited operations for an additional two days due to order of the City.\nPaddy’s Pub may have coverage under the business interruption policy at $15,000 per day for the two days it would have been fully operational but for the 5 pm curfew (totaling $30,000). As with business interruption coverage, there may be a waiting period for civil authority coverage to begin—typically beginning after the first action of civil authority.\nMost business interruption policies include “order of civil authority” coverage for lost revenue stemming from an order of the government prohibiting or limiting access to the area surrounding a business. Even if Paddy’s Pub had not been physically damaged, this coverage would likely apply if an order of civil authority prohibited or limited access to Paddy’s Pub. There often must be physical damage somewhere in the area, but it need not be at Paddy’s Pub and the damaged property need not be owned by Paddy’s Pub.\nOther Access Issues: You also receive notification that the bridge leading to Paddy’s Pub was burned in the riots and will be closed due to renovations for two weeks. As a result, your patrons must take the longer, alternative route to get to Paddy’s Pub. You estimate that this limit in access will reduce revenue for Paddy’s Pub by $10,000 per day.\nPaddy’s Pub may have coverage under its business interruption policy at $10,000 per day for each of the seven days it experiences reduced income due to the bridge closure (totaling $70,000). Without the bridge closure, Paddy’s Pub would have been fully functional after one week.\nMost business interruption policies cover instances in which access to your business is limited due to physical damage to other property that prevents or limits patrons’ access to your business. Again, there may be a waiting period before this coverage applies.\nOther Coverages: If one of your patrons or employees was injured inside Paddy’s Pub during the rioting (for example, if an employee or patron was struck by a bottle thrown through the window), you should consult your commercial general liability and workers’ compensation policies for additional coverage.\nExclusions: Of course, insurance companies may add specific exclusions for civil commotion, rioting, or vandalism. If this occurs, it is important to consider renegotiating the policy or, at the very least, ensure you have a firm understanding as to these changes.']	['<urn:uuid:4ee8484e-010f-4e5a-b1c3-f4a2b6ae0563>', '<urn:uuid:c69b20b2-86e2-408c-b288-25e54d82b60d>']	factoid	direct	concise-and-natural	distant-from-document	multi-aspect	expert	2025-05-12T21:52:51.670698	8	70	2730
65	What is the recommended AWG size range for different speaker applications in home theater systems?	For smaller speakers, a cable of around 17 AWG is sufficient, while for larger speakers, using up to 12 AWG or more will provide good results.	['AWG means United states Cable Evaluate, the standard program associated with calculating the actual cross-sectional section of the cable. This really is accustomed to figure out how a lot present the cable are designed for. AWG leads to a lot misunderstandings with regard to customers, since the regular could be a small hard to comprehend. Is actually 12 AWG much better than fourteen AWG or even vice versa? The reason 1 cable television appears heavier compared to an additional despite the fact that they’ve similar AWG? Is actually AWG a great sign associated with high quality? Will AWG issue, and when therefore, exactly how?\nThey are just about all great queries, as well as we will reach all of them soon. First of all, let us quickly contact how AWG is really determined.\nExactly how is actually AWG determined?\nIf your cable television had been a good round cable, after that AWG is rather simple in order to determine. Consider the region (pi by radius squared) to find the cross-sectional region, and appear in the AWG graph (example below) to sort out AWG. If your cable television offers several strands, an identical procedure is performed to sort out the actual cross-sectional section of every follicle, that is after that merely increased through the amount of strands to find the complete AWG. Nevertheless be cautious when you compare this particular determine because AWG isn’t linear. For each additional 3 AWG, it’s fifty percent the actual cross-sectional region. Therefore 9 AWG is all about 1 / 2 of 6 AWG, that is fifty percent once again associated with 3 AWG. Therefore 3 AWG is actually quadruple the actual width associated with 9 AWG.\nSo how exactly does AWG impact electric qualities?\nA person would have observed right now how the scaled-down the actual AWG, the larger the actual cable television. Bigger wires may have much less DC opposition, that means much less energy reduction. With regard to programs in order to house theater, this is really accurate as much as a good degree. The guideline is actually which with regard to scaled-down loudspeakers, the cable television associated with close to seventeen AWG is enough, while with regard to bigger loudspeakers something as much as 12 AWG or even more provides you with great results.\nThe reason a few wires from the exact same AWG appear various thick?\n2 elements master right here. First of all, the actual AWG just considers the interior conductors. Consequently, the cable television producer might very easily boost the width from the plastic material coat to create the actual cable television seem heavier. This is not always a poor point, because up to and including stage elevated coat width decreases additional undesirable qualities. Simply ensure that you do not evaluate all of them through view.\nAnother element the reason why 2 exact same AWG wires might appear various thick is actually how a inner strands are made. A few wires possess slimmer strands, while some possess heavier strands. With respect to the dimension as well as keeping these types of strands, wires could be designed to appear slimmer or even heavier compared to they’re.\nIs actually AWG a great sign associated with high quality?\nThe bottom line is, absolutely no. A sizable AWG (small cable) might definitely end up being as well little for any specific software (for instance, don’t be utilizing a twenty-four AWG cable television to operate your own entrance speakers). Nevertheless, AWG is really a way of measuring amount, not really high quality. You need to make sure that all your loudspeaker wires tend to be associated with a minimum of OFC wholesomeness.\nWill AWG issue? Exactly how therefore?\nAWG definitely issues. You have to make sure that the actual cable television you’re utilizing is enough to take care of the ability you are going to subjected to all of them. Furthermore, if you’re carrying out a lengthier operate, after that much more width will be needed. Nevertheless, many people obtain swept up an excessive amount of within AWG as well as overlook the truth that as soon as an adequate width is actually arrived at, additional elements enter into perform. This particular after that gets much more the issue with regard to “audiophile” functions to solve, for example utilizing high quality supplies for example metallic conductors or even enhanced style.\nCable evaluate is unquestionably a great basic sign associated with exactly how adequate the cable television is perfect for the application. Nevertheless, it’s in no way the reasoning upon high quality, or perhaps a standards to check out solely. Typically associated with usb, following regarding 11-12 AWG, width gets a smaller amount of the element, while for many hi-fi programs 18-19 AWG will be the minimal wires to make use of.']	['<urn:uuid:e22e19fd-3d78-478d-8e2e-ff23d96763a5>']	factoid	direct	verbose-and-natural	similar-to-document	single-doc	expert	2025-05-12T21:52:51.670698	15	26	797
66	new quantum computer chip benefits security risks	The semiconductor quantum transistor chip enables fast processing of 10 billion photonic qubits per second and is extremely compact, with about a million transistors fitting in a grain of salt. However, there are security concerns as malicious manufacturers could potentially compromise the security of quantum cryptography systems by embedding hidden memory in devices to store and analyze key exchanges, potentially making them only secure for one-time use.	['Semiconductor quantum transistor opens the door for photon-based computing\nTransistors are tiny switches that form the bedrock of modern computing—billions of them route electrical signals around inside a smartphone, for instance.\nQuantum computers will need analogous hardware to manipulate quantum information. But the design constraints for this new technology are stringent, and today’s most advanced processors can’t be repurposed as quantum devices. That’s because quantum information carriers, dubbed qubits, have to follow different rules laid out by quantum physics.\nScientists can use many kinds of quantum particles as qubits, even the photons that make up light. Photons have added appeal because they can swiftly shuttle information over long distances and they are compatible with fabricated chips. However, making a quantum transistor triggered by light has been challenging because it requires that the photons interact with each other, something that doesn’t ordinarily happen on its own.\nNow, researchers at the Joint Quantum Institute (JQI), led by JQI Fellow Edo Waks have cleared this hurdle and demonstrated the first single-photon transistor using a semiconductor chip. The device, described in the July 6 issue of Science, is compact: Roughly one million of these new transistors could fit inside a single grain of salt. It is also fast, able to process 10 billion photonic qubits every second.\n“Using our transistor, we should be able to perform quantum gates between photons,” says Waks. “Software running on a quantum computer would use a series of such operations to attain exponential speedup for certain computational problems.\nThe photonic chip is made from a semiconductor with numerous holes in it, making it appear much like a honeycomb. Light entering the chip bounces around and gets trapped by the hole pattern; a small crystal called a quantum dot sits inside the area where the light intensity is strongest. Analogous to conventional computer memory, the dot stores information about photons as they enter the device. The dot can effectively tap into that memory to mediate photon interactions—meaning that the actions of one photon affect others that later arrive at the chip.\n“In a single-photon transistor the quantum dot memory must persist long enough to interact with each photonic qubit,” says Shuo Sun, the lead author of the new work who is a Postdoctoral Research Fellow at Stanford University*. “This allows a single photon to switch a bigger stream of photons, which is essential for our device to be considered a transistor.”\nTo test that the chip operated like a transistor, the researchers examined how the device responded to weak light pulses that usually contained only one photon. In a normal environment, such dim light might barely register. However, in this device, a single photon gets trapped for a long time, registering its presence in the nearby dot.\nThe team observed that a single photon could, by interacting with the dot, control the transmission of a second light pulse through the device. The first light pulse acts like a key, opening the door for the second photon to enter the chip. If the first pulse didn’t contain any photons, the dot blocked subsequent photons from getting through. This behavior is similar to a conventional transistor where a small voltage controls the passage of current through it’s terminals. Here, the researchers successfully replaced the voltage with a single photon and demonstrated that their quantum transistor could switch a light pulse containing around 30 photons before the quantum dot’s memory ran out.\nWaks, who is also a professor in the University of Maryland Department of Electrical and Computer Engineering, says that his team had to test different aspects of the device’s performance prior to getting the transistor to work. “Until now, we had the individual components necessary to make a single photon transistor, but here we combined all of the steps into a single chip,” Waks says.\nSun says that with realistic engineering improvements their approach could allow many quantum light transistors to be linked together. The team hopes that such speedy, highly connected devices will eventually lead to compact quantum computers that process large numbers of photonic qubits.\n*Other contributors and affiliations\n- Edo Waks has affiliations with the University of Maryland Department of Electrical and Computer Engineering (ECE), Department of Physics, Joint Quantum Institute, and the Institute for Research in Electronics and Applied Physics (IREAP).\n- Shuo Sun was a UMD graduate student at the time of this research. He is now a postdoctoral research fellow at Stanford University.\n- JQI Fellow Glenn Solomon, a physicist at the National Institute of Standards and Technology, grew the sample used in this research.\n- Hyochul Kim was a postdoctoral research at UMD at the time of the research. He is now at Samsung Advanced Institute of Technology.\n- Zhouchen Luo is currently a UMD ECE graduate student.', '‘Quantum Trojans’ undermine security theory\nCan dodgy vendors compromise ‘uncrackable’ security?\nA group of English and Canadian researchers has cast doubt on the nascent push to develop device-independent quantum cryptography standards, asserting that such schemes could be undermined by malicious vendors.\nTheir paper, Prisoners of their own device: Trojan attacks on device-independent quantum cryptography, is published on Arxiv.org, here.\nThe paper outlines scenarios which the authors say would be undetectable to the user, but would allow the attacker to obtain sufficient information to snoop on supposedly “uncrackable” quantum cryptography.\nThe paper, authored by London University mathematician Jonathan Barrett, Roger Colbeck of Canada’s Perimeter Institute of Theoretical Physics, and Adrian Kent of Cambridge’s Centre for Quantum Information and Foundations, states:\n“A malicious manufacturer who wishes to mislead users or obtain data from them can equip devices with a memory [El Reg – to clarify, in our reading this refers to a memory included in the devices specifically for attack purposes] and use it in programming them.\n“A task is potentially vulnerable to our attacks if it involves secret data generated by devices, and if Eve [El Reg – ie, the attacker] can learn some function of the device outputs.”\nTheir analysis gives rise, for example, to a scenario in which the attacking equipment might store key exchange communications from “day 1”, use this to analyse the key exchange taking place on “day 2”; and use this to extract the “day 1” key.\nThis is supposed to be impossible, since any tampering with the quantum communication channel should be revealed – for example, as (entanglement-destroying) noise on the quantum channel.\nHowever, as the authors point out, all real-world channels contain noise; to overcome this, quantum crypto schemes exchange multiple pairs over a noisy channel, and use a statistical analysis to detect interference in the channel.\nThe malicious manufacturer, however, should be able to conceal its activities below the noise threshold the system uses to decide that the channel remains secure. The attacker could even build systems whose actual noise levels are lower than claimed, and use the gap between specified and real noise to conceal their activity.\nIf not addressed, the authors say the flaws they have identified effectively turn QKD devices into a “use once” proposition: you can only guarantee security for the first exchange, so the device has to be disposed of. ®\nComment: Before the world proclaims “quantum crypto not secure!” in headlines (too late? Oh well…) El Reg would make a couple of observations.\nFirst, the malicious manufacturer is not a quantum-specific threat: backdoors can be just as easily inserted into classical cryptography kit.\nSecond, this paper is presenting a discussion not on any mass-deployed system, but on proposed schemes for device-independent QKD. Device independence has come to the fore chiefly because of prior demonstrations suggesting that today’s implementations have exploitable flaws; as a result, there has been ongoing discussion as to how users might verify the security of a quantum communication without knowing anything about the equipment used to create that channel.\nFor those interested in the kinds of schemes they believe could be compromised, the article cites some key papers on Arxiv, such as:\nThird, the authors do not claim to have actually built a working proof-of-concept: their paper is a discussion of how a malicious system may be designed; it’s been published on Arxiv for review, and El Reg would expect a veritable feast of future papers for quantum crypto enthusiasts. ®']	['<urn:uuid:5cf7fcee-cc17-4a27-89bc-37891ad97d7a>', '<urn:uuid:fbadc5db-0fbc-4af8-a0fe-48e46a4aa67e>']	factoid	with-premise	short-search-query	distant-from-document	multi-aspect	novice	2025-05-12T21:52:51.670698	7	67	1371
67	manual grain mill versus electric mill advantages disadvantages grinding	Electric grain mills are faster and easier to operate but require electricity. Manual grain mills save on electricity costs but require physical effort to grind the grains and are much slower at producing the same amount of ground grains compared to electric models.	"['May 15, 2013· The thing with wheat grinders is that each of them differ slightly – in price, capacity, noise, fineness of flour, etc. So the best advice I can give if you are in the market for a wheat grinder is to do your research. I had a Nutrimill for 5 or so years before buying the KoMo mill.\nElectric grain mills are easier and quicker to operate, though they consume electricity. Manual grain mills save you the cost of electricity but require you to put in the physical effort to grind the grains yourself and can be much slower to produce the same yield of ground grains as their electric counterpart.\nAlthough we feature hand-turned grain mills, we offer this mill for those who want to grind their own flour but prefer to do it with electricity. The Nutrimill® is the finest electric grain mill we\'ve tested. Separate knobs adjust the speed of the variable speed motor and the rate of flow of grain into the surgical quality stainless steel milling teeth. The result is the most fine/coarse ...\nChoosing the Right Countertop Grain Mill Learn how to find a quality grain mill to grind your own flour. This article covers reviews on grain mills and the health benefits of fresh-ground grains.\nBy simply changing the. setting you can make cracked wheat, cereals, or pastry flour fine enough to make an angel food cake! ALL GRAIN stones are precision cut and diamond dressed, permitting you to mill the finest flour you have ever seen without stone-to-stone contact. ALL GRAIN mills grind all grains including rice, as well as soy\nJan 05, 2020· There are plenty more good reasons to grind your own flour – so let\'s sift through the information and separate the wheat from the chaff! Join us now for all the details on manual, electric, and convertible grain mills, burrs and other mechanisms, what grains to use, processing and storage tips, and even where to purchase bulk supplies online.\nEach GrainMaker® mill is designed to allow individuals, families and communities the ability to mill wholesome, nutritious foods without additives or preservatives. Our Flaker Mill is the tool you need to flake your own barley, granola, cereals and other grains; roll grains for cereal or beer making; or crack grains for your animal feed.\nManual Grain Mills With a manual grain mill, your entire family could be eating fresher, tastier, more healthful foods and saving money at the same time. By the MOTHER EARTH NEWS editors\nLehman\'s carries an extensive selection of grain mills, grinders, accessories and grains that will let you get the most out of your food. Our grain grinder mill assortment ranges from the most basic and economical hand mill, for making corn meal and animal feed, to top-of-the-line grinders like the Diamant, which can grind practically anything.\nOthers require heavier equipment such as a flour mill. Other tools that can be used include the old school mortar and pestle, spice mills, manual or electric coffee grinders and small electric grinders made specifically for home milling. What Kinds of Grains Work Best? Traditional flours are made predominantly from wheat, barley, and rye.\nApr 20, 2020· Most kitchens probably do not have a grain mill, but these kitchen appliances can be quite handy. Just think of grinding your wheat flour or even making your peanut butter from fresh wheat and peanuts, those are just two of the things that a grain mill will allow you to do.\nSkippy Grain Mills. A grain mill will produce fresh, whole grain flour & flakes with top nutrients and taste... and it\'s so easy. Whether you feed a busy family, have a special diet, or simply want to get back to whole-food essentials, we have a mill that will suit your needs and budget.\nExcellent flour mill. I grind only mixed whole grains for flour and whether buckwheat, quinoa, amaranth, Kamut wheat, barley or course oatmeal, every grain comes out great. I only bake bread with these grains, so I often have to adjust the grind setting slightly as quinoa and barley are two very different grains of hardness. Excellent product.\nDec 09, 2019· Grind To A Fine Flour. If you will be milling wheat (wheat berries) to make flour for bread, you will want to be sure that the grain mill (flour mill) will grind the wheat berries into fine flour.. Grinding / milling your own flour is \'work\' and it takes awhile to process the wheat berries into flour.\nA few mills are specific about the kinds of nuts and grain they can grind- usually, some mills have the limitation of not grinding oily nuts or grains. Conclusion Reading our in-depth review, you may be convinced to invest in your own grain mill to get the fresh and purest flour that is abundant with health benefits.\nJan 02, 2020· Plus, you\'ve already purchased your grinder – your Vitamix! – so no need to spend extra money on a grain mill. Overall, I find that the cost is a wash, but the flavor and nutrition make it worth the investment to grind my own grains. Types of Berries. Hard Red: Similar to store-bought whole wheat flour. High in fiber and whole bran.\nA quality grain mill transforms whole grains into flour, meal or a cereal-grind texture. Whether you want a flour mill for common wheat, einkorn, or corn, or a flaker for oatmeal, at Pleasant Hill Grain we offer the world\'s best. Our mills can grind almost anything, and are available in a range of prices to fit your needs.\nHow to choose the best grain mill to grind your own flour at home, because really, every home and kitchen should be experiencing the benefits of grinding your own flour. And to do that, you need to make sure you\'re choosing the best flour mill for your house.\nWonderMill. World\'s #1 Rated Electric Grain Mill. The WonderMill Electric Grain Mill is the quietest and fastest grain mill available. You can create super-fine flour or coarse flour at temperatures that preserve nutrients, ensuring that you will always have the perfect flour …\nMake Offer - Electric Mill Dry Grinder Machine Corn Grain Rice Wheat Cereal Feed Swing Type 700g High Speed Electric Herb Grain Grinder Cereal Mill Flour Powder Machine $65.49\nApr 07, 2012· I purchased one of these conical burr coffee grinders recently.. It works well for coffee beans, which it can grind down to a near-powder. Question: Can it mill grain? If not, why not? Is it because the finest grind setting ""espresso grain"" would still be too coarse for flour?. Is it because the friction of a conical burr would expose the wheat to too much heat/friction?\nGrain mills are for more than grinding wheat. You can grind peanuts as well. The first time I made peanut butter I had to use my blender to grind peanuts. It was a task. After learning that a grain mill could grind more than just grains, I\'ve learned that making homemade peanut butter becomes a lot easier with a mill. 4. Self Sufficiency\nJun 12, 2017· Home Grain Mills – Comparison of Manual Grain Grinders for the Home. June 12, 2017 By Laurie Neverman 24 Comments This post may contain affiliate links which won\'t change your price but will share some commission.\nThe Wheat Mill Team from Retsel® Corporation brings quality crafted grain mills to the 21st Century family. Our company was established in 1962, currently being the oldest and the largest grain mill manufacturer and competitors parts inventory …']"	['<urn:uuid:81fcbdeb-76c8-4d8c-8cf0-8dd53df99601>']	factoid	direct	long-search-query	similar-to-document	single-doc	expert	2025-05-12T21:52:51.670698	9	43	1261
68	I'm trying to understand how drugs work in our bodies - how do small organic molecules actually fight diseases?	Small organic molecules fight disease by using oxygen- and nitrogen-containing functional groups to interact specifically with particular enzymes in a pathogen or human cell. The effectiveness of the drug depends on its overall shape and how its functional groups are arranged in space, allowing it to specifically recognize one target enzyme out of thousands.	"['Most small, bioactive organic compounds interact specifically with a particular enzyme through a series of oxygen- and nitrogen-containing ""functional groups."" The specificity and strength of this interaction depends on the overall shape of the molecule and on the arrangement of its functional groups in space (i.e., its ""stereochemistry""). The discovery of new compounds with the ability to fight disease or otherwise modulate the behavior of a biological system thus relies on the existence of methods for the synthesis of densely functionalized, well-defined organic molecules. In this regard, a particularly important goal of synthetic organic chemistry is the discovery of new transformations that convert simple, widely available hydrocarbon starting materials into complex, functionalized molecules with high levels of control over their stereochemistry. We are investigating the utility of small, oxygen- and nitrogen-rich compounds called ""oxaziridines"" in new organic reactions. We hypothesize that (1) transition metal catalysts will increase the reactivity of oxaziridines, (2) fundamentally different reactions will be observed using different catalysts, and (3) these catalysts will have the ability to control the stereochemistry of the resulting value-added products. The proposed research will test these hypotheses in the context of two Specific Aims. First, we will develop copper-catalyzed reactions of oxaziridines for the construction of stereochemically well-defined 1,2- aminoalcohol structures (""aminohydroxylation reactions""). Second, we will develop titanium-catalyzed reactions of oxaziridines for the construction of 1,3-aminoalcohol-containing structures with similarly high levels of stereochemical fidelity (""nitrone cycloadditions""). Successful realization of our project goals will constitute a significant contribution to the field of synthetic organic chemistry and provide a set of powerful tools for the discovery of new drugs, new biological probes, and new materials.\nMost drugs are small organic molecules that fight disease by using a series of oxygen- and nitrogen- containing ""functional groups"" to interact with an enzyme in a pathogen or in a human cell. The ability of a drug to specifically recognize one target enzyme out of thousands and control the progress of a disease depends critically on the overall shape of the molecule and the arrangement of its functional groups in space. We are developing new methods for the rapid, shape-selective construction of well-defined nitrogen- and oxygen-rich molecules, which will enable the discovery and manufacture of the next generation of potent disease-fighting drugs.\n|Williamson, Kevin S; Michaelis, David J; Yoon, Tehshik P (2014) Advances in the chemistry of oxaziridines. Chem Rev 114:8016-36|\n|Deporter, Sandra M; Jacobsen, Ashley C; Partridge, Katherine M et al. (2010) N-Nosyl Oxaziridines as Terminal Oxidants in Copper(II)-Catalyzed Olefin Oxyaminations. Tetrahedron Lett 51:5223-5225|\n|Partridge, Katherine M; Guzei, Ilia A; Yoon, Tehshik P (2010) Carbonyl imines from oxaziridines: generation and cycloaddition of N-O=C dipoles. Angew Chem Int Ed Engl 49:930-4|\n|Benkovics, Tamas; Guzei, Ilia A; Yoon, Tehshik P (2010) Oxaziridine-mediated oxyamination of indoles: an approach to 3-aminoindoles and enantiomerically enriched 3-aminopyrroloindolines. Angew Chem Int Ed Engl 49:9153-7|\n|Williamson, Kevin S; Yoon, Tehshik P (2010) Iron-catalyzed aminohydroxylation of olefins. J Am Chem Soc 132:4570-1|\n|Benkovics, Tamas; Du, Juana; Guzei, Ilia A et al. (2009) Anionic halocuprate(II) complexes as catalysts for the oxaziridine-mediated aminohydroxylation of olefins. J Org Chem 74:5545-52|\n|Allen, Charles P; Benkovics, Tamas; Turek, Amanda K et al. (2009) Oxaziridine-mediated intramolecular amination of sp(3)-hybridized C-H bonds. J Am Chem Soc 131:12560-1|\n|Michaelis, David J; Williamson, Kevin S; Yoon, Tehshik P (2009) Oxaziridine-mediated enantioselective aminohydroxylation of styrenes catalyzed by copper(II) bis(oxazoline) complexes. Tetrahedron 65:5118-5124|']"	['<urn:uuid:3a85428e-e004-4c97-bf31-78aa3242535c>']	factoid	with-premise	verbose-and-natural	similar-to-document	single-doc	novice	2025-05-12T21:52:51.670698	19	54	556
69	I'm wondering about the origins and evolution of the groomsmen role - what were their historical duties in primitive weddings, and how do those compare to their responsibilities in modern ceremonies?	Historically, groomsmen were called bride-knights and had a crucial role in primitive marriage by capture, where they helped the groom 'lift' or kidnap the bride and defended her from potential kidnappers. In modern ceremonies, their role is much less dramatic - they help guests find their seats before the ceremony, stand near the groom during the service, assist with gifts and luggage, dance with unaccompanied guests, and may participate in local traditions like decorating the newlywed's car.	"[""The Bridal Party: Roles, Responsibilities, and Choosing the Right People\nChoose Your Wedding Attendants Wisely\nOne of the first things that many people do upon becoming engaged is to start inviting their close friends and family members to be in their wedding party. Being a member of a bridal party is a great honor and a great responsibility. Therefore it is important to choose wisely because it is virtually impossible to rescind an invitation to be a bridesmaid or groomsman. Before you begin selecting your bridal party, make sure you are well versed in the various roles, their responsibilities, and how to choose the right people for each position.\nChoosing the Maid or Matron of Honor\nThe maid of honor is very often the first wedding attendant selected. She will likely be the bride's sister or best friend, and may well have been asked to serve as maid of honor at the very same moment the elated bride called to announce her engagement. The conversation probably sounded a lot like this: “Can you believe it – I just got engaged! You have to be my maid of honor of course, and just wait until you see the ring!” And so on and so on, until the recently engaged woman finally stops to take a breath! By definition, the maid of honor should be a maiden; in other words, never married. If your sister or best friend is a married woman (or for that matter, was at some time in the past, if no longer), she would serve as the matron of honor. A bride can have both a maid of honor and a matron of honor sharing the role of lead bridesmaid. This can work out very well when the bride is torn between choosing her sister or her best friend to be her maid of honor. If one is married and the other is single, designate them matron and maid of honor, sparing you the difficult position of having to choose. By the way, if the bride does find it necessary to choose between a friend and sister for her maid of honor, blood should win out.\nThe maid of honor has some special duties in addition to those of the regular bridesmaids. Above all, she should be the bride's main source of support: someone who will give honest feedback on the wedding plans (only if asked!), listen to the bride vent about her mother taking over the wedding plans, and will help the bride to stay relaxed in the days leading up to the wedding. The maid of honor will frequently assist the bride in shopping for her wedding gown, bridal jewelry, and veil, whenever it is geographically practical. The maid of honor almost always helps with the selection of the bridesmaid dresses and bridesmaid jewelry, and many honor attendants also go with the bride to the florist, stationer, or other wedding shop, particularly if the mother of the bride lives too far away to do these things.\nWhat the Maid of Honor Does\nAs the bridesmaid-in-chief, the maid of honor has some organizational responsibilities. If the group of bridesmaids elects to give a group gift to the bride and groom, it falls upon the maid of honor to organize the funds and purchase the gift. The maid of honor often plans a bridal shower for the bride about two months before the wedding. It is important for everyone (especially the bride) to realize that the maid of honor and bridesmaids are not obligated to throw a bridal shower, though more often or not, they will choose to do so. If the bride's sister is the maid of honor, she should not act as the official shower host, although she could certainly assist the host (who should not be a close relative of the bride) in planning and even paying for the event. If the bridesmaids get together and decide to throw a bachelorette party in addition to a bridal shower, the maid of honor is the one who plans it.\nOn the day of the wedding, the maid of honor has very important tasks. She will generally help the bride get into her wedding finery, and should also help keep the rest of the bridesmaids on schedule to ensure that the ceremony starts on time. The maid of honor traditionally holds the groom's ring during the ceremony until the ring exchange, although sometimes it is more practical for the best man to hold both rings in one box in his jacket pocket (since the bridesmaids' dresses are likely to lack pockets). The maid of honor also holds the bridal bouquet during the ceremony, and should also take care to keep the bride's train neatly arranged during the wedding service. The maid of honor has one more very important job to do after the ceremony, which is to be a witness at the signing of the marriage license. At the conclusion of the ceremony, she is usually expected to stand in the receiving line to greet guests. The maid of honor is not required to give a toast at the reception, although she may certainly do so if inclined. Customarily, she will also dance with the best man during the spotlight dances, and act as a generally hostess during the reception, making sure that everyone is having a good time. At the end of the evening, the maid of honor will help the bride change into her going away outfit and takes charge of the bridal gown and accessories.\nThe bridesmaids are a group of the bride's good friends, sisters, cousins, and possibly future sisters-in-law. Up to six bridesmaids is acceptable for a wedding with about 150 guests, but for smaller weddings, the bridal party should be scaled down to keep it in proportion to the overall size of the event. Too many bridesmaids ends up looking silly, and by the way, it is perfectly acceptable to have uneven numbers of attendants on the bride's side and the groom's side. Bridesmaids should not be invited hastily, as it is an invitation that cannot be undone without permanently ending the friendship. It should go without saying that the depth of friendship should be the deciding factor in selecting a bridal party, rather than the attractiveness of the candidates!\nThe primary role of the bridesmaids is to attend pre-wedding parties, cheer the bride on as she plans her wedding, and to be a charming addition to the ceremony and reception. The general responsibilities of a bridesmaid include:\n- Paying for her dress and accessories (unless bridesmaid jewelry is given as a gift by the bride).\n- Paying for her own transportation (and sometimes her own lodging, although it is customary for the bride to find people to take in her bridesmaids for the weekend of the wedding).\n- Attending pre-wedding parties and events such as the bridal shower, bachelorette party, ladies' luncheon, rehearsal, and rehearsal dinner.\n- Giving a wedding gift to the bride and groom, either individually or as part of a group gift organized by the maid of honor.\n- Volunteering to help the bride with the occasional wedding related task, if at all possible.\n- Showing up to the wedding on time and appropriately attired, groomed, and coiffed.\n- Being attentive to the wedding guests, chatting with lonely ones, and encouraging wallflowers to join the party.\n- Participating in the bouquet toss, if single.\nBest Man: More Than Just a Ceremonial Role\nHis is the most duty-laden role of the wedding party. The groom will have one best man (there is no married and unmarried option as in the matron/maid of honor). It is usually his brother or best friend, although is is also traditional for the father of the groom to serve as best man (the mother of the bride, however, does not customarily serve as matron of honor). Because the best man needs to be a person with strong organizational skills and good at managing details and people, he needs to be chosen very carefully. The role is far from ceremonial, and the fun-loving fraternity brother who might be a blast to have at your bachelor party is not necessarily the man you want in charge of paying your vendors on the wedding day.\nIn general, the best man is responsible for overseeing the groomsmen or ushers. He should make sure that they all get their tuxedos ordered by the deadline, show up at the wedding on time and sober, and give them their marching orders for how to seat the guests. The best man should also be a rock for the groom; the person he can turn to for pep talks when he gets cold feet, the guy who assists the groom in selecting a wedding gift for his bride, and the one who makes sure the groom does not have too much fun at the bachelor party. On the day of the wedding, the best man should ensure that the groom arrives at the ceremony venue at least twenty minutes before the start of the ceremony, and is also the person charged with keeping the groom calm.\nThe Best Man Wears Many Hats\nIf there is to be a bachelor party, it is organized by the best man. The style of the party should take into account the taste of the groom, and should have at least a passing regard for the sensibilities of the bride. The best man may either give an individual gift to the bride and groom, or organize a group gift from all the groomsmen. There are other very important things for which the best man is responsible, foremost of which is holding the bride's wedding ring until the ring exchange during the ceremony. He is also a witness to the signing of the marriage license at the end of the service. Sometimes the best man will also act as chauffeur for the newlyweds, if no limousine has been rented for the wedding. In some cultures, such as Greek weddings, the best man has an even more important role to play in preparing for the wedding.\nThe best man also has key jobs to do during the wedding reception. He offers the first toast to the newlyweds, and has a list of the order of the other speakers. During the spotlight dances, he should take the maid of honor, bride, and the mothers of the bride and groom for a spin on the dance floor. Bonus points are scored for dancing with the flower girl (always a good way to impress the pretty bridesmaids!). The best man serves as sort of an unofficial party manager, in that he should try to see that all the guests are having fun and that their needs are met (bringing a glass of punch to the bride's elderly aunt, that sort of thing). If any of the guests start having a bit too much fun, it may well be the best man who escorts them outside to clear their heads before they make a drunken scene at the reception. At the end of the reception, the best man collects the groom's attire, making sure to return any rental items to the formal wear shop promptly.\nThe Groomsmen / Ushers\nThe male counterparts to the bridesmaids, the groomsmen tend to have a less demanding role. Before the wedding, their main responsibilities are to attend the bachelor party, order their attire in a timely fashion, and give a wedding gift to the bride and groom. They are also expected to attend the wedding rehearsal and the dinner which follows. The main jobs of the groomsmen occur during the wedding ceremony. They seat the guests as they arrive, and must learn the specific instructions for that particular wedding. In the case of an outdoor wedding on a rainy day, the groomsmen shelter arriving guests under golf umbrellas until they reach the covered tent. If wedding programs are to be handed out, the ushers do that as they greet guests and seat them (in many cases, the programs are already placed on the seats before the guests arrive). The groomsmen would also be the ones to roll out the aisle runner before the start of the processional, if a runner is being used. During the reception, the responsibilities of the groomsmen are pretty light. They will most likely dance with the bridesmaids, but beyond that, the ushers are pretty much regular wedding guests, just there to celebrate with the newlyweds.\nJunior Bridesmaids and Flower Girls\nChildren can be a sweet addition to a wedding. If you choose to include them in yours, these are the roles for the junior attendants:\nJunior Bridesmaid: A young girl aged approximately 9 – 13 years old. This role is usually awarded to a favorite niece who has “graduated” from being a flower girl. It is a lovely way to include a girl who is too old to play the role of flower girl, but too young to be a full-fledged bridesmaid. The junior bridesmaid is generally dressed in an outfit which is a more modest and age-appropriate version of what the bridesmaids wear. She carries a bouquet similar to that of the bridesmaids, but on a slightly smaller scale. At the conclusion of the processional, she can stand with the other bridesmaids or sit with her parents in a pew near the front of the church or venue. To make her feel included in the wedding, invite the junior bridesmaid to all of the PG rated pre-wedding parties; in other words, the bridal showe and the rehearsal dinner, but not the bachelorette party. Her parents pay for her attire, transportation, lodging and so on.\nFlower Girl: One of the most darling moments in a wedding ceremony is when the flower girl makes her appearance. She walks right before the bride, either sprinkling rose petals along the path (venue permitting) or carries a small posy of flower. The flower girl should be attired in a sweet dress, either in a color to complement the bridesmaids' dresses or in white. The bride should resist the urge to dress the flower girl as a “mini me” in a tiny full length dress which mimics the bridal gown. Instead, select a classic child's party dress which comes to below the knee, worn with patent leather shoes in the winter or white sandals in the summer. The flower girl may wear a wreath of blossoms and ribbons, or could tuck small spray roses or a sparkly barrette in her hair. The perfect age for a flower girl is 3 – 5 years old, although up to about 8 is acceptable (any older than that and she should be a junior bridesmaid).\nRing Bearers and Pages\nRing Bearer: The ring bearer is a little boy, typically a nephew, aged 3 - 5 years old. He carries a fancy ring pillow up the aisle during the processional. The ring bearer may walk with the flower girl or just before her. Prudent couples may wish to tie fake rings on the pillow, leaving the real jewelry in the more reliable hands of the best man. If the boy is older and fairly responsible, you may wish to tie the actual wedding bands to the ring pillow. The best man removes them when the ring bearer reaches the altar. Fancy attire for small boys can be tricky. The formality of the ring bearer's outfit should be in keeping with the general formality of the rest of the attendants. Many rely on the miniature tuxedo, although it is technically incorrect etiquette to dress a small child in men's formal wear. The most traditional attire for a ring bearer is the Eton suit, which is a dressy suit with short pants and knee socks (very British!), worn in dark colors for the winter or light for the summer. For a very young ring bearer, a sailor suit would be appropriate, and for one at the upper end of the age range, a navy blazer with khaki trousers would be handsome.\nPages: This junior attendant is necessary only if the bridal gown has a very long train (think Princess Diana). Also called a train bearer, the job of the page is to hold the bride's train and keep it straight. Pages generally work in pairs, one at a corner of the end of the bridal train. Because the role requires a decent amount of coordination, pages are generally a bit older than ring bearers. Around ages 6 - 9 is about right for train bearers. Their attire can be similar in style to that of the ring bearer's, but need not match exactly, especially given the age difference (no sailor suits on 8 year olds!).\nTreat Your Bridal Party Well!\nOf course, the bride and groom also have some responsibilities towards their bridal party. They should show respect for their attendants by selecting budget-friendly attire, destinations, and hotels (or may discreetly offer to pick up the tab for a cherished friend who cannot afford the cost of being in a wedding). The bride should also show respect for her bridesmaids' time and lives. While every bride hopes that her good friends will be excited about her wedding, and most truly are, some bridesmaids prefer to have minimal involvement in the wedding planning. It may be due to a lack of free time, or just a lack of knowledge about the ways in which a bridesmaid can be helpful, but the bride should never insist that her attendants become her personal assistants, nor should she scold a bridesmaid for not demonstrating enough enthusiasm. Finally, the bride and groom should show their appreciation for their wonderful bridal party by giving them thoughtful bridesmaid and groomsmen gifts. After all, the members of the wedding party have a lot of responsibilities!"", 'In North America, a groomsman is one of the male attendants to the groom in a wedding ceremony. In Britain, a similar role is performed by an ""usher"". Usually, the groom selects close friends and relatives to serve as groomsmen, and it is considered an honor to be selected. From his groomsmen, the groom usually chooses one to serve as best man.\nFor a wedding with many guests, the groom may also ask other male friends and relatives to act as ushers without otherwise participating in the wedding ceremony; their sole task is ushering guests to their seats before the ceremony. Ushers may also be hired for very large weddings.\nIn a military officer\'s wedding, the roles of groomsmen are replaced by swordsmen of the sword honor guard. They are usually picked as close personal friends of the groom who have served with him. Their role includes forming the traditional saber arch for the married couple and guests to walk through.\nThe most visible duty of the groomsmen is helping guests find their places before the ceremony and to stand near the groom during the wedding ceremony.\nAdditionally, the groom may request other kinds of assistance, such as planning celebratory events such as a bachelor party, also called a stag-do or buck\'s night; helping make the wedding pleasant for guests by talking with people who are alone or dancing with unaccompanied guests or bridesmaids, if there is dancing at a wedding reception; or providing practical assistance with gifts, luggage, or unexpected complications. Groomsmen may also participate in local or regional traditions, such as decorating the newlywed couple\'s car. The other and also very important task of the best man is to give a speech during the wedding. This is usually decided between the bride and groom before the wedding. The best man speech is usually humorous and short.\nBridegroom-men formerly had important duties. The men were called bride-knights, and represented a survival of the primitive days of marriage by capture, when a man called his friends in to assist to ""lift"" or kidnap the bride, or from the need to defend the bride from would-be kidnappers.\nThe best man is the chief assistant to the groom at a wedding and (along with the maid of honour) is the third most important in attendance after the bride and groom. In most modern anglophone countries, the groom extends this honor to someone who is close to him, generally close friend or relative (such as a sibling or cousin). When the groom wishes to give this honor to a woman, she may be termed the best woman or best person, although traditionally she would still be referred to as the \'best man\'. The bride\'s equivalent of the best man is the bridesmaid, or the maid/matron of honor. A gender-neutral term is honor attendant. During a wedding ceremony the best man stands next to the groom, slightly behind him. This means that the four people present at the altar are the celebrant (such as a civil celebrant, priest, rabbi, minister, or other religious figure), the bride, groom, and best man. This is common in some western countries, although in others the best man and bridesmaid participate on an equal footing.\nWhile the best man\'s required duties are only those of a friend, in the context of a western white wedding, the best man will typically:\n- Assist the groom on the wedding day,\n- Be in charge of the ushers\n- Keep the wedding rings safe until needed during the ceremony,\n- Stand next to the groom during the ceremony,\n- Act as a legal witness to the marriage and therefore sign the marriage certificate, and\n- Prepare a ""best man\'s speech"" to be read at the reception\nIn various countries and cultures\nThe best man is not a universal custom. Even in places where a best man is customary, the role may be quite different when compared to other areas of the world.\n- In Britain, it is traditional for the best man to give a short speech.\n- In Zambia, a best man is expected to lead processions both at the wedding, and at preliminary events. This includes pre-wedding dance rehearsals, at which the best man is always expected to be in attendance, and usually is expected to give an outstanding and outlandish dance-performance on the actual wedding-day.\n- In Uganda, a best man is expected to guide the newlyweds in the ways of marriage. This means that ideally a best man must be married, preferably to one wife, and should be in position to give sound, tried and tested advice. A best man must be a confidant and be discreet about the details he shares with the new couple.\n- In Bhutan, the best man presents himself at the wedding as a ceremonial guardian to both bride and groom. Thereafter he entertains the guests, sometimes for several hours.\n- In Eastern Orthodox weddings in Greece, the best man is often also the koumbaros, or religious sponsor. The koumbaros (or koumbara, if a woman) is an honored participant who crowns the couple and participates in circling the altar three times. Sometimes, this person also pays for most of the wedding expenses.\n- In Ukraine, a best man is responsible for guarding the bride during the wedding festivities. When he or the groom steps away, the bride gets ""kidnapped"" or has a shoe stolen. Then the groom or the best man must pay a ransom in exchange for returning the bride, usually by paying money (which is given to the bride) or by doing something embarrassing.\nIn the past, the bachelor party was typically scheduled for a convenient evening during the week before the wedding. A type of farewell dinner, it was always hosted, and therefore organized and paid for, entirely by the groom. The dinner was seen as the groom\'s last chance to entertain his friends as a single man; after the wedding, dinner parties at his home would always be presided over by his wife in her role as hostess.\nCommon slang names for this event are bachelor party, stag do, or bucks\' night in different parts of the world. In many areas, this dinner is now most commonly organized by the best man; the costs can be shared by either all of the participants or all of the participants except for the groom, who becomes the guest of honor.\n- T. Sharper Knowlson (2008) . The Origins of Popular Superstitions and Customs (Forgotten Books). Forgotten Books. pp. 100–102. ISBN 1-60506-458-0.\n- Leopold Wagner (1995). Manners, Customs and Observances. Omnigraphics Inc. pp. 61–62. ISBN 978-1-60506-798-8.\n- Chisholm, Hugh, ed. (1911). ""Bride"". Encyclopædia Britannica (11th ed.). Cambridge University Press.\n- http://www.rhymingspeeches.com/best-man-duties/ Best Man Duties[not in citation given]\n- http://bestman-speech.org/ Best Man Speech Examples\n- ""International Wedding Customs"". Archived from the original on 2008-05-03. Retrieved 2008-06-20.\n- ""How To Give The Worst Best Man Speech Ever (and, Ahem, How Not To)"". www.groovygroomsmengifts.com. Retrieved 15 November 2015.\n- ""Essential Guide to Ukrainian Wedding Traditions!"". What\'s On Kiev. Retrieved 2008.\n- Post, Emily (1922). Etiquette in Society, in Business, in Politics and at Home\'. Funk & Wagnalls Company. pp. 335–337.\n- Post, Peggy (2006). Emily Post\'s Wedding Etiquette (5 ed.). London: Collins. pp. 183–184. ISBN 0-06-074504-5.']"	['<urn:uuid:cfe12765-903c-4c1b-80d6-6151575fb487>', '<urn:uuid:c83e9007-baec-4725-9739-cf54b07ddd68>']	factoid	with-premise	verbose-and-natural	similar-to-document	multi-aspect	expert	2025-05-12T21:52:51.670698	31	77	4185
70	compare rigidity stochastic tessellation foam structure vs falconboard honeycomb arrangement	Foams have highly stochastic and indeterminate geometries with curved walls and edges, while Falconboard uses a more organized honeycomb pattern made of thick paper that makes it far more resistant to bending and snapping.	['Summary: Second of a two-part post on developing a taxonomy of cellular materials. This post focuses on 3D cellular materials. A previous post addressed 2D (prismatic) cellular materials.\nIn the first part of this two-part post, we introduced a classification scheme for cellular materials that was a synthesis of ideas borrowing from communities spanning the fields of cellular solids, geometry, crystallography and design software. The first part also dived deeper into 2D prismatic cellular materials. In this post, we complete the taxonomy by focusing on 3D cellular materials.\nFor the sake of consistency, it would make sense to classify volumetric cellular materials purely in terms of tessellation, as we did for 2D materials. However, unlike in 2D, this approach is not sufficient in 3D since it says nothing of the connectivity within tessellated cells. Therefore, as shown in Figure 1, reproduced from part 1 of this post, for 3D cellular materials we propose a different nomenclature strategy that involves 3 specific decisions a designer has to make, regarding the shape of tessellation, the nature of the elements filling the unit cells, and how they are connected relative to the tessellated outline.\nTessellation is the first of three decisions that a designer has to make when seeking to fill space with 3D cellular structures. As with the prismatic cellular materials, the first level of separation within tessellation is into periodic and stochastic polyhedra. For the former, it is typical to assume only regular or semi-regular polyhedra for reasons of tractability. Within this category, Pearce (1980) identifies 23 different space-filling polyhedra. Discounting the three prismatic polyhedra (cube, triangular prism and hexagonal prism), we are left with 20 space filling systems. These could be composed of one, two, three or four polyhedra and are termed Unary, Binary, Ternary and Quarternary, respectively. Table 1 lists some space filling tessellations that have interesting properties and may form the basis of tessellation for cellular materials with AM for designers. For a more complete study, the reader is referred to the excellent titles by Pearce (Polyhedra Primer and Structure in Nature as a Design Strategy).\nTable 1. Examples of 3D space-filling with non-prismatic polyhedra\nTessellation is merely the first in a 3-step process to defining a 3D unit cell. The next step involves deciding what physical elements will occupy that space. At first glance, this may seem trivial, given our proclivity for lattices in AM. However, there are several options for a given lattice beam, as well as alternative ways to fill space that do not involve beams.\nFor beam-based lattice structures, the length of the beam is governed by the placement of vertices in space, which we discuss in the next section. However, the beam itself has a cross-section that can take a range of shapes, including a variable section from one end to another. Figure 2 shows three possible cross-sections one can assign to a beam. Depending on the scales involved, such nuances may not be resolvable, but if they are, they become a design variable that can influence behavior as well as manufacturability, such as the teardrop shape in Figure 2 (right), so designed to enable overhanging lattice beams to be self-supporting for processes that need support, such as laser powder bed fusion.\nAn alternative to beams is to use surfaces in the construction of the unit cell. Here we have two further possibilities – the use of flat surfaces or curved. A particularly interesting use of curved surfaces is the use of minimal surfaces, that have an average curvature of zero at every point on the surface. An example of a flat surface cellular material and the basic minimal surface are shown in Figure 3. Minimal surfaces are energetically favorable ways of weaving surfaces through space and mathematicians have developed several of these surfaces (Schoen, 2012).\nOnce the type of tessellation and nature of the elements are specified, the last step is to integrate the two. This is done through specifying a series of nodal co-ordinates and establishing connectivity between them, or deploying an equation that assigns these to the unit cell. For lattices, an example is shown in Figure 4 for cubic tessellations using beam elements, but with different nodes and connectivity. This form of connectivity can also then be used to define surfaces between edges, one example of which was shown in Figure 4 (left).\nWith regard to the previously discussed minimal surfaces, the surfaces of interest from a space-filling perspective are called TPMS, or Triply Periodic Minimal Surfaces. Dating back to 1865, there are now several types of TPMS structures, a selection of which are shown in Figure 5. TPMS structures are developed from governing equations that can be implemented in CAD software. TPMS structures also occur in nature and have been found both in living creatures (sea urchins, the scales on butterfly wings and beetle exoskeletons) as well as in zeolite and some liquid crystals (Han and Che, 2018).\nA Note on Nomenclature\nThere is no agreed upon nomenclature for cellular materials in the AM community at the moment, with researchers using the most colloquial name they can identify for a particular shape. While terms like BCC, octet and Gyroid are increasing gaining traction in the AM design community, there remains the need for a formal nomenclature that can extend to the most general case. Inspired by the nomenclature system used in nTopology’s Element software, the following four step nomenclature scheme is proposed:\n- Dimensionality: This involves determination of whether the cell is prismatic (2D) or volumetric (3D)\n- Tessellation: The compartmentalization of space into independent volumes of a certain shape\n- Elements: The use of beams and/or shells/faces within the tessellated space, and\n- Topology: The actual arrangement of the elements within the tessellated space\nFour examples of this nomenclature are shown in Figure 6, with the term lattice being used to represent beam elements, and surface for surface/face elements. While close to a comprehensive naming system, it does have the downside of being quite a mouthful. The bee’s honeycomb would thus be called a Hex Prism Edge Surface cellular material. Therefore, when a simpler term is sufficient and popular in the community, it should be used. For all other cases, this nomenclature is proposed.\nWhat of foams, then?\nThe above classification scheme makes no mention of foams – why is this? There are two reasons: the first has to do with the manufacturing process: foams are perhaps the only cellular material that strongly hint at their method of manufacture in the name itself: most foams are made through foaming processes, i.e. through the generation of gas bubbles in the forming process that creates the void spaces in the foam. Secondly, foams typically have highly stochastic, and somewhat indeterminate geometries with curved walls and edges. Thus, within the constructs of the proposed taxonomy, foams can be considered to be a stochastic tessellation of 3D space with either beam (open cell foam) or surface (closed cell foam) elements.\nHaving said this, the study of foams and their applications are among the most robust in the field of cellular materials, and there is a lot to borrow when developing ideas for cellular materials in AM, particularly with regard to understanding their properties – but this will have to wait for another post.', 'General Comparison of Foamboard, Gatorboard and Falconboard\nWhat are Foamboard, Gatorboard, and Falconboard?\nYou’ve probably seen a number of our signs that seem to be named after animals. What’s that about? What are these different “boards” you keep seeing?\nFoamboard, Gatorboard and Falconboard are all inexpensive, lightweight signage options that are similar in a lot of ways, yet distinctly different in construction.\nLet’s talk about them in a little more detail so you can see the construction and benefits of each type of board. Hopefully, this will give you an idea of which of these is best for your signage needs. Perhaps you’ll find that several of them will work. Perhaps none of them will. Either way, this way you’ll have the information you need to make that decision.\nFacts About Foamboard\nJust as it sounds, foamboard is made of foam. Specifically, the core of the sign is foam and surrounded by printable white paper surfaces. The printing on the surface is done in high-quality UV ink, in full color.\nThe core of foam board signs is rigid, and it has a bit of giving. That said, it’s best not to try bending it too much – it’s prone to creasing or snapping.\nFoamboard weighs about a pound per 7 ½ square inches. That means you can get a reasonably large sign without adding too much weight, though larger signs are more prone to damage from bending or snapping. The standard thickness is 3/16”, but it can be made at a custom thickness․\nThe lightweight makes them versatile and safe for everything from businesses to schools to social gatherings. Better yet, the material is inexpensive.\nOn the downside, foam board doesn’t handle weight particularly well. It is prone to denting and other damage if stored somewhere with weight on top of it. The material is also not waterproof, and its light weight makes it susceptible to moderate winds. As a result, it is not recommended for outdoor use.\nFacts About Gatorboard\nLike foamboard, gatorboard is comprised of a rigid polystyrene core at a standard 3/16” thickness (and also like foamboard, that can be altered if you contact us). Looking at a foamboard and gatorboard sign side-by-side, you might not notice a difference.\nThat’s because the difference between the two isn’t in appearance, it’s in construction quality. Gatorboard is notably more durable and solid in its construction than foamboard, yet maintains its printable surface and relatively light weight.\nThe best way to think about Gatorboard is a much higher-quality foamboard. Foamboard is generally a temporary signage solution, but gatorboard can be used for two years or more if cared for properly.\nOf course, gatorboard is pretty tough, but it isn’t invincible. It’s still a good idea to keep weight off of it and moisture away from it. It is more resistant to both forces than foamboard, but too much of either will cause damage.\nBeing as gatorboard is a stronger, more durable version of foamboard, it’s also a bit more expensive. But it’s still a much more affordable option than aluminum signs or acrylic signage.\nFacts About Falconboard\nAlso lightweight and versatile, falconboard is unlike the other two in that it isn’t made of a polystyrene foam. It’s actually made of thick paper in a honeycomb pattern. You would think that would make it weaker, but it’s actually far more resistant to bending and snapping than either foamboard or gatorboard.\nFalconboard is ½” thick and is frequently cut into custom shapes in order to give it an impressive three-dimensional effect. They can be printed on both sides and uses the same high-quality UV ink that is resistant to scratching and fading.\nFalconboard is another low-cost option that is a great choice if you’re looking for something with a little more character than standard foamboard. It’s printable and looks great when contour cut (foamboard and gatorboard cannot be contour cut, they can only have halo-style cuts). It’s only slightly heavier than foamboard, it is much more rigid.\nWhile a popular option is to get falconboard signs with drilled holes for hanging, the unique three-dimensional nature of falconboard allows it to also be effective as a free-standing sign.\nBecause of the paper core construction, falconboard is highly susceptible to water damage, and should under almost no circumstances be used outdoors. Like foamboard, falconboard is generally best for temporary displays, as it is light and prone to damage.']	['<urn:uuid:ff71bde6-81a2-4acf-8ed6-223aa00e9d94>', '<urn:uuid:d0c49320-d97f-451d-9dbb-69b03826ec16>']	factoid	with-premise	long-search-query	distant-from-document	comparison	expert	2025-05-12T21:52:51.670698	10	34	1946
71	I see news about both regular computer viruses and government hacking - would someone use basic malware tools if they had access to advanced hacking methods?	Interestingly, even sophisticated threat actors may deliberately use basic commodity malware in initial attacks to probe an organization's defenses and establish a foothold. This is because using advanced custom malware early risks exposing both the group and their sophisticated tools before they're confident of accomplishing their objectives. We see this pattern in BLINDINGCAN campaigns, where attackers use common vulnerabilities like CVE-2017-0199 for initial access, betting that some targets will remain unpatched, before deploying their more advanced RAT (Remote Access Tool) capabilities. The advanced tools are typically reserved for high-value attacks once a presence has been established.	"['Cybersecurity is a mature discipline with well-established terminology and procedures. Part of this terminology concerns the identification of threats and threat actors, and of attack frameworks and indicators. You must be able to use threat intelligence and attack frameworks to model likely risks to your organization and perform threat hunting to proactively determine that your systems have not already been compromised. This commitment to proactive defense is at the heart of the best approach to security assurance.\nExplain the importance of threat data and intelligence.\nThreat intelligence helps to focus security monitoring by providing information on new threats and current threat trends. Sources of this information include free online registries and catalogs, commercial registries and monitoring services, and product vendors. Increasingly, these sources are providing threat classification intelligence data in standard formats that are easily processed by automated monitoring systems. In this topic you will review the basic ways of classifying threats and threat actor types.\nHistorically, cybersecurity techniques depended very much on the identification of ""static"" known threats, such as viruses, rootkits, Trojans, and botnets. It is straightforward to identify and scan for this type of threat with automated software by matching the malicious code to a signature in a database of known malware. Unfortunately, many adversaries now have the capability to develop means of circumventing these security systems.\nThe sophisticated nature of modern cybersecurity threats means that when classifying threats, it is important to be able to describe and analyze behaviors as well as enumerate known attack signatures. This type of threat classification underpins tools and procedures that can detect unknown threats. Unknown in this sense means threats that are unlikely to be detected by off-the-shelf tools. Much of the effort in threat modeling has moved to analysis of known unknowns. For example, a research bulletin might reveal the existence of a zero-day vulnerability. The security team will investigate whether their system assets could be affected, and if so, will trigger a heightened alert status, performing scans more frequently. Based on whatever threat intelligence they possess, the security staff will be ready to investigate the type of anomaly they might mark as low priority at another time. This state of alert will persist until the vendor develops an update and the affected systems can be patched. Another example of a known unknown is that malware authors can use various obfuscation techniques to circumvent signature-matching. The exact form that such malware will take is unknown, but its likely use and operation within an attack is predictable, at least to some extent.\n** Another useful category is that of recycled threats. This refers to combining and modifying parts of existing exploit code to create new threats that are not as easily identified by automated scanning. **\nThere are also unknown unknowns. This is the domain of completely new attack vectors and exploits. One of the purposes of security research is to try to discover these, using techniques such as analysis of data collected in honeypots and monitoring of discussion boards used by threat actors.\n** Classifying threats into quadrants (known knowns, unknown knowns, known unknowns, and unknown unknowns) was popularized by a comment made by Donald Rumsfeld as US Secretary of Defense (youtube.com/watch?v=GiPe1OiKQuk), but has a longer history as a personal development analysis framework called the Johari window (businessballs.com/self-awareness/johari-window-model-and-free-diagrams). The Johari window has frequently been adapted for project and risk management purposes. The ""unknown knowns"" quadrant represents risks that are documented or identified but then disregarded or perhaps minimized in importance. **\nTHREAT ACTOR TYPES\nBecause of the need to defend against unknown threats, threat intelligence is not simply a process of identifying malware signatures and technical attack vectors. Threat intelligence must also develop insights into the behaviors of discrete types of adversary groups. You can use threat intelligence reports to monitor nation-state, organized crime, and hacktivist groups and activities that pose relevant threats to your own organization. It is important to identify the level of resources/funding that different adversaries might possess, and whether they can develop sophisticated malware that can evade basic security controls.\nWhen evaluating adversary behaviors, attacks can be characterized as either opportunistic or targeted. Opportunistic attacks might be launched without much sophistication or funding simply by using tools widely available on the Internet. Conversely, a targeted attack might use highly sophisticated tools and be backed by a budget that can allocate resources and skilled professionals to achieving its aims.\nMost nation-states have developed cybersecurity expertise and will use cyber weapons to achieve both military and commercial goals. The security company Mandiant\'s APT1 report into Chinese cyber espionage units (fireeye.com/content/dam/fireeye-www/services/pdfs/mandiant-apt1-report.pdf) was influential in shaping the language and understanding of modern cyberattack life cycles. The term advanced persistent threat (APT) was coined to understand the behavior underpinning modern types of cyber adversaries. Rather than think in terms of systems being infected with a virus or rootkit, an APT refers to the ongoing ability of an adversary to compromise network security—to obtain and maintain access—using a variety of tools and techniques.\nNation-state actors have been implicated in many attacks, particularly on energy and electoral systems. The goals of nation-state actors are primarily espionage and strategic advantage, but it is known that countries—North Korea being a good example—target companies purely for commercial gain. You should also realize that each state may sponsor multiple adversary groups, and that these groups may have different objectives, resources, and degrees of collaboration with one another.\n** Crowdstrike\'s adversary universe app provides an overview of currently identified APTs (https://adversary.crowdstrike.com/en-US/). **\nIn many countries, cybercrime has overtaken physical crime both in terms of number of incidents and losses. An organized crime gang can operate across the Internet from different jurisdictions than its victims, increasing the complexity of prosecution. Organized crime will seek any opportunity for criminal profit, but typical activities are financial fraud (both against individuals and companies) and blackmail. A blog from Security Intelligence (securityintelligence.com/the-business-of-organized-cybercrime-rising-intergang-collaboration-in-2018) discusses some of the strategies and tools used by organized crime gangs.\nA hacktivist such as Anonymous, WikiLeaks, or LulzSec, uses cyber weapons to promote a political agenda. Hacktivists might attempt to obtain and release confidential information to the public domain, perform denial of service (DoS) attacks, or deface websites. Political, media, and financial groups and companies are probably most at risk, but environmental and animal advocacy groups may target companies in a wide range of industries. While international groups gained media attention through the early part of the 2010s, recent research (https://go.recordedfuture.com/hubfs/reports/cta-2019-0821.pdf) suggests that most active hacktivist groups are focused on activities at the regional level—within a single country.\nINSIDER THREAT TYPES\nMany threat actors operate externally from the networks they target. An external actor has to break into the system without having been granted any legitimate permissions. An insider threat arises from an actor who has been identified by the organization and granted some sort of access. Within this group of internal threats, you can distinguish insiders with permanent privileges, such as employees, from insiders with temporary privileges, such as contractors and guests. The Computer Emergency Response Team (CERT) at Carnegie Mellon University\'s definition of a malicious insider is:\nA current or former employee, contractor, or business partner who has or had authorized access to an organization’s network, system, or data and intentionally exceeded or misused that access in a manner that negatively affected the confidentiality, integrity, or availability of the organization’s information or information systems. (insights.sei.cmu.edu/insider-threat/2017/03/cert-definition-of-insider-threat---updated.html)\n** There is the blurred case of former insiders, such as ex-employees now working at another company or who have been dismissed and now harbor a grievance. These can be classified as internal threats or treated as external threats with insider knowledge, and possibly some residual permissions, if effective offboarding controls are not in place. **\nCERT identifies the main motivators for malicious insider threats as sabotage, financial gain, and business advantage. Like external threats, insider threats can be opportunistic or targeted. Again, the key point here is to identify likely motivations, such as employees who might harbor grievances or those likely to perpetrate fraud. An employee who plans and executes a campaign to modify invoices and divert funds is launching a structured attack; an employee who tries to guess the password on the salary database a couple of times, having noticed that the file is available on the network, is perpetrating an opportunistic attack. It is important to realize that an insider threat may be working in collaboration with an external threat actor or group.\nInsider threats can also be categorized as either intentional or unintentional. The examples given previously are intentional threats. An unintentional threat is created by an insider acting with no malicious intent. An unintentional or inadvertent insider threat is a vector for an external actor, or a separate—malicious—internal actor to exploit, rather than a threat actor in its own right.\nUnintentional threats usually arise from lack of awareness or from carelessness, such as users demonstrating poor password management. Another example of unintentional insider threat is the concept of shadow IT, where users purchase or introduce computer hardware or software to the workplace without the sanction of the IT department and without going through a procurement and security analysis process. The problem of shadow IT is exacerbated by the proliferation of cloud services and mobile devices, which are easy for users to obtain. Shadow IT creates a new unmonitored attack surface for malicious adversaries to exploit.\nTechnical controls are less likely to be able to inhibit structured insider threats, as insiders are more likely to be able to bypass them. Implementing operational and management controls, especially secure logging and auditing, is essential. Unintentional insider threats are best tackled via security training and awareness, plus procedural controls to govern critical tasks. Monitoring statistics related to training use and documentation can help to identify employees or departments where there is elevated risk of inadvertent threats.\nCOMMODITY MALWARE AND ZERO-DAY THREATS\nAnother part of threat classification is to describe distinct types of adversary tools, collectively described as malware. While the division of malware into types such as virus, worm, Trojan, rootkit, and ransomware is well known, the development, production, and deployment of malware is highly relevant to threat intelligence, because it reveals clues to the intentions and capabilities of cyber adversaries.\nMalware has existed for almost half a century and over that time its use has become commodified, meaning that it is sold and exchanged just like any other type of software. Commodity malware refers to code that can be used in general circumstances and that is packaged for general sale, typically through dark web marketplaces (csoonline.com/article/3249765/what-is-the-dark-web-how-to-access-it-and-what-youll-find.html). Examples of commodity malware include remote access Trojans (RATs), such as PoisonIvy, Dark Comet, and XtremeRAT. Once such tools are identified as being generally available through online marketplaces or download sites, threat intelligence feeds will tag the process signatures as commodity. Commodity malware can be contrasted with targeted or custom malware, which is developed and deployed with a target in mind, following careful reconnaissance of that target. The difference is similar to that between general phishing campaigns and spear phishing campaigns. Commodity malware depends on unpatched systems vulnerable to known exploits, while targeted malware is more likely to use a zero-day exploit.\nNote that the definition of commodity malware is somewhat fuzzy (crowdstrike.com/blog/blurring-of-commodity-and-targeted-attack-malware). Custom malware may also be available in marketplaces, though sale may be restricted to verified contacts of the group that developed it. Similarly, off-the-shelf or packaged malware can still pose a risk to automated threat detection systems because obfuscation techniques can be used to change the commodity malware code slightly to evade signature detection. From a threat intelligence point-of-view however, identifying malware as commodity versus targeted can help you to determine the severity of an incident because it can help to identify the goals and resources available to the attacker.\nMalware often depends on some sort of software, firmware, or hardware vulnerability, whether it be to achieve initial execution, escalate to higher system privileges, or achieve persistence on the target system. A zero-day is a vulnerability that is discovered or exploited before the vendor can issue a patch to fix it.\n** The term zero-day is usually applied to the vulnerability itself but can also refer to an attack or malware that exploits it. **\nThe most serious zero-day vulnerabilities are those discovered and exploited by adversary groups. Security researchers also discover new vulnerabilities, in which case it is best practice for them to inform the vendor privately and allow time for a fix to be developed before making the vulnerability public. The time allowed is often 90 days by convention, but this may be reduced depending on the status of the vulnerability. An unpatched but discovered vulnerability can be referred to as n-day. For example, if a vulnerability has not been patched in the week following discovery, it is a 7-day vulnerability.\nZero-day vulnerabilities have significant financial value. A zero-day exploit for a mobile OS can be worth millions of dollars. Consequently, an adversary will only use a zero-day vulnerability for high value attacks. State security and law enforcement agencies are known to stockpile zero-days to facilitate the investigation of crimes.\n** Do not allow a classification schema to blind you to potential adversary behaviors. For example, sophisticated threat actors may use commodity malware in initial attacks to probe an organization\'s defensive capabilities and possibly obtain some sort of foothold. Using sophisticated custom malware in the preliminary stages of a campaign risks the exposure of the group and the custom tool, and is likely to be withheld until the group is confident of using it to accomplish their objectives before detection. **\n** The RAND Corporation has produced a fascinating report on the production and marketization of zero-day vulnerabilities (rand.org/content/dam/rand/pubs/research_reports/RR1700/RR1751/RAND_RR1751.pdf). **\nADVANCED PERSISTENT THREAT (APT)\nThe term advanced persistent threat (APT) was coined to understand the behavior underpinning modern types of cyber adversary, such as nation-state and organized crime actors. The term originally referred to the group behind a campaign but has been widened to mean the tools such groups use as well. The concept of an APT is a means of modeling known unknown threats. As well as scanning for virus or Trojan signatures, you can scan for the presence of Command and Control (C2) software or network activity or look for unexplained changes in network activity overall. One of the concepts underpinning APT is that of withdrawal, where the adversary removes evidence of the attack. One way of discovering unknowns is to look for signs of any past attacks that have gone undetected.\nAPTs typically target large organizations, such as financial institutions, companies in healthcare, and other organizations that store large, personally-identifiable information (PII) data sets. APTs have also been known to target governments to carry out political objectives, interfere in elections, or simply to spy on another country.\nThe ""advanced"" part of an APT is an important identifier, as these types of threats are very rarely executed by lone, unskilled attackers using prebaked exploits. An APT will command considerable resources, including staff specializing in different realms of exploit development and execution. APTs spend considerable effort in gathering intelligence on their target and are able to craft highly specific custom exploits. Another characteristic of the advanced nature of APTs is that they often combine many different attack elements into an overall threat architecture.\nAPTs have diverse overall goals, but since a large part of the attack is about stealth, most APTs are interested in maintaining access—or persistence—to networks and systems. There are several techniques that can grant attackers access for months or even years on end without being detected. Because of this, APTs are some of the most insidious and harmful threats to an organization.\nFront of Flashcard 1 of 3\nWhat distinguishes an unknown threat from a known threat?\nBack of Flashcard 1 of 3\nA known threat can be identified by automated detection tools, such as an anti-virus scanner, intrusion detection system (IDS), or vulnerability scanner. Unknown threats are those that cannot be identified from a static signature. You can distinguish between known unknowns, which are threats that may follow some general pattern observable from previous or similar threats, and unknown unknowns, representing completely new threat actors, sources, and techniques.\nFront of Flashcard 2 of 3\nWhat types of controls address risks from unintentional insider threats?\nBack of Flashcard 2 of 3\nTraining and awareness programs reduce the chance that insiders will generate risks from ignorance. Procedural controls help to mitigate risks from carelessness and inattention. The presence of elevated risk from inadvertent threat can be assessed by monitoring training adoption and effectiveness metrics.\nFront of Flashcard 3 of 3\nSecurity monitoring has detected the presence of a remote access tool classified as commodity malware on an employee workstation. Does this allow you to discount the possibility that an APT is involved in the attack?\nBack of Flashcard 3 of 3\nNo. While targeted malware is associated with highly resourced threat actors such as advanced persistent threats (APT), there is nothing to prevent such actors using commodity malware as well, such as during the initial stages of a campaign. You need to evaluate other indicators to identify the threat actor involved and whether the presence of commodity malware is an isolated incident or part of a wider campaign.', 'There has been a great deal of coverage lately around malicious activities attributed to North Korea (and/or adjacent entities). Most recently, this has culminated in the release of MAR (Malware Analysis Report) AR20-232A, which covers activities associated with the BLINDINGCAN RAT. This tool is the latest in a very long line of tools which allow attackers to maintain access to target environments as well as establish ongoing control of infected hosts. In this post, we give an overview of this campaign in context of other related campaigns, describing its infection vector, execution and high-level behavior.\nAs we know, email phishing attacks are still the dominant method of delivering malware when it comes to these types of attacks. The BLINDINGCAN campaigns are no different, but their phishing lure comes with an interesting twist: malicious documents utilized in the campaign masquerade as job offers and postings from high-value defense contractors such as Boeing.\nThis isn’t the first time such a lure has been used. Sophisticated attackers have sought to mimic entities in the defense, military, and government space in the past. This is especially true, historically, with campaigns tied to North Korea. Even early on in 2020, Operation North Star followed a very similar modus operandi, and by some accounts these campaigns may be related.\nCISA maintains a running repository of North Korean / Hidden Cobra related advisories and details. Their alerts cover campaigns from 2017 to present, including (but not limited to):\n- WannaCry – Massively destructive “ransomware” with SMB spreading capabilities.\n- Delta Charlie – Backdoor and Denial-of-Service tool set\n- Volgmer – Backdoor\n- FALLCHILL – Full-function RAT\n- BANKSHOT – RAT and proxy/tunneling tool set\n- HARDRAIN – RAT and proxy tool set w/ Android support\n- SHARPKNOT – MBR Wiper\n- TYPEFRAME – RAT and proxy/tunneling tool set\n- KEYMARBLE – Full-function RAT\n- FASTCash – RAT and proxy/tunneling tool set (Financial attacks)\n- BADCALL – RAT and proxy tool set w/ Android support\n- ELECTRICFISH – proxy/tunneling tool set\n- HOPLIGHT – proxy/tunneling tool set with pseudo-SSL spoofing\n- ARTFULPIE – Downloader and launcher tool set\n- CROWDEDFLOUNDER – Full-function RAT\n- TAINTEDSCRIBE – Downloader and launcher with LFSR (LInear Feedback Shift Register) support\n- COPPERHEDGE – Full-function RAT, cryptocurrency and crypto-exchange focused.\nIn short, the DPRK has a long history of these types of campaigns and it does not appear to be letting up in frequency or aggressiveness. Moreover, North Korea is no stranger to playing the ‘long-game’. Reflecting back on earlier attacks from the region (e.g., Operation Troy, Ten Days of Rain, Dark Seoul, and the Sony attack) we see similar tactics and aggressiveness.\nThe BLINDINGCAN campaign has been specifically focused on defense and aerospace targets, primarily based in Europe and the United States. According to AR20-232a: “The FBI has high confidence that HIDDEN COBRA actors are using malware variants in conjunction with proxy servers” along with “compromised infrastructure from multiple countries to host its command and control (C2) infrastructure”.\nThe objective of these attacks is to gain intelligence and to understand the key technologies that fall under the umbrella of the targeted entity, as well as those adjacent to them (contactors, partners, etc.)\nBLINDINGCAN RAT: Execution and Behavior\nThe malicious documents themselves, upon launch, attempt to exploit CVE-2017-0199. This particular flaw allows for remote code execution via maliciously crafted documents. More specifically, CVE-2017-0199 is a result of the flawed processing of RTF files and elements by way of a potent combination of object links and HTA payloads.\nThis vulnerability is a common vector of attack for malicious actors, and despite the flaw being patched long ago, attackers bet on the fact (often successfully) that at least some of their targets will still be exposed to the flaw, allowing them to achieve their foothold.\nYou can see this behavior immediately upon launching one of the malicious documents.\nThe samples we analyzed reach out to a remote server (C2) for additional components. Once established, a keylogging and clipboard monitoring component is dropped, and additional information is extracted from the targeted hosts. WMI commands are utilized to gleen basic system details:\nstart iwbemservices::execquery - select * from win32_computersystemproduct\nThe RAT component (e.g.,\n58027c80c6502327863ddca28c31d352e5707f5903340b9e6ccc0997fcb9631d) can be found in both 32 and 64 bit varieties. The executable payloads employ multiple levels of obfuscation.\nConfiguration data for the RAT is embedded in the payloads and is both encrypted and encoded. Embedded configuration artifacts are AES-encrypted with a hard-coded key. Upon decrypting, the resulting data is then decoded via XOR. Strings in the malware are RC4 encrypted.\nThe RAT module will initially pull basic system data. The aforementioned WMI command is part of this system reconnaissance process. In this stage, the malware will pull local network data, system name, OS version details, processor/platform details and MAC address details, and then push this data to the C2.\nThe core RAT feature set boils down to the following:\n- Gather and transmit defined set of System features\n- Create, terminate and manipulate processes\n- Create, terminate and manipulate files\n- Self-updating / self-deletion (cleaning of malicious code from the system when necessary)\nWhile the malware and implants discussed here are specific to operations attributed to North Korea, the delivery and weaponization states are common to most other APT groups and non-nation-state backed campaigns.\nThe key takeaways here are 1) it is important to keep abreast of the evolution of malicious attacks generated from this region, but also 2) we can apply what we have learned from other past attacks to improve our posture and reduce overall exposure, along with the potential negative repercussions of suffering from such an attack. Prevention, as always, is key. The SentinelOne Singularity Platform is fully capable of detecting and preventing malicious activity associated with HIDDEN COBRA and BLINDINGCAN.\nIndicators of Compromise\nPhishing: Spearphishing Attachment [T1566]\nCommand and Scripting Interpreter: PowerShell [T1059]\nExploitation for Client Execution [T1203]\nBoot or Logon Autostart Execution: Registry Run Keys / Startup Folder [T1547]\nProcess Injection [T1055]\nDeobfuscate/Decode Files or Information [T1140]\nSystem Time Discovery [T1124]\nAccount Discovery [T1087]\nQuery Registry [T1012]\nProcess Discovery [T1424]\nSystem Owner/User Discovery [T1033]\nAutomated Collection [T1119\nData from Local System [T1533]]\nRemote File Copy [T1544\nAutomated Exfiltration [T1020]]\nExfiltration Over C2 Channel [T1041]\nRead more about Cyber Security\n- Defeating “Doki” Malware and Container Escapes with Advanced Linux Behavioral Detection\n- 5 Cyber Security Challenges Facing CISOs in the Age of COVID-19\n- The World Has Changed. Have You? Capturing Today Through the Lens of Cybersecurity\n- Behavioral AI: An Unbounded Approach to Protecting the Enterprise\n- Feature Spotlight – Enhanced USB & Bluetooth Device Control\n- Four Distinct Families of Lazarus Malware Target Apple’s macOS Platform\n- GDPR Turns Two! Has Anything Really Changed?\n- Kubernetes Security Challenges, Risks, and Attack Vectors\n- Feature Spotlight – Introducing the New Threat Center\n- Look Who’s Back – It’s DDoS!']"	['<urn:uuid:f89a169a-48ed-4f34-af4f-4f5b60023c35>', '<urn:uuid:8353c42c-50d5-4229-a07e-78500558fc4f>']	open-ended	with-premise	verbose-and-natural	distant-from-document	comparison	novice	2025-05-12T21:52:51.670698	26	96	4015
72	I work in market research and need to understand the size of the yeast industry - what was the market value in 2016 and what's the projected growth?	The global Yeast Market was valued at USD 3.04 billion in 2016 and is projected to reach USD 6.44 billion by 2025, growing at a CAGR of 8.7% from 2017 to 2025.	['Global Yeast Market\nThe global Yeast Market was valued at USD 3.04 billion in 2016 and is projected to reach USD 6.44 billion by 2025, growing at a CAGR of 8.7% from 2017 to 2025.\nYeast is one of the most widely used micro-organisms for manufacturing food & beverages. It is used in the process of fermentation and imparts a better texture, taste, and aroma to the food products. This product has witnessed growing demand courtesy an increase in the consumption of functional bakery foods and alcoholic beverages.\n1. Market Drivers\n1.1 Growing demand functional bakery products\n1.2 Growing demand from the alcohol industry\n1.3 Growing demand for bioethanol as a fuel\n1.4 Increasing awareness about the importance of yeast as compared to its alternative Monosodium Glutamate (MSG)\n2. Market Restraints\n2.1 Scarcity of raw materials\n2.2 Stringent food safety standards and regulations\nThe global Yeast Market is segmented on the basis of Product, Type, Application and Region\n1. Global Yeast Market, by Product:\n1.1 Fresh Yeast\n1.2 Dry Yeast\n1.3 Instant Yeast\n2. Global Yeast Market, by Type:\n2.1 Baker’s Yeast\n2.2 Wine Yeast\n2.3 Brewer’s Yeast\n2.4 Bioethanol Yeast\n2.5 Feed Yeast\n3. Global Yeast Market, by Application:\n3.1 Food & Beverages\n3.1.1 Alcoholic Beverages\n3.1.2 Non-alcoholic beverages\n3.1.3 Bakery Products\n3.2 Animal Feed\n4. Global Yeast Market, by Region:\n4.1 North America (U.S., Canada, Mexico)\n4.2 Europe (Germany, UK, France, Rest of Europe)\n4.3 Asia Pacific (China, India, Japan, Rest of Asia Pacific)\n4.4 Latin America (Brazil, Argentina, Rest of Latin America)\n4.5 Middle East & Africa\nThe major players in the market are as follows:\n1. AB Mauri Food\n2. AB Vista\n4. Chr. Hansen\n7. Archer Daniels Midland Company\n8. Kerry Group PLC\n10. Nutreco N.V.\n11. Synergy Flavors\n12. Oriental Yeast Co., Ltd\n13. Koninklijke DSM N.V.\n14. Angelyeast Co., Ltd.\n15. Associated British Foods PLC\nThese major players have adopted various organic as well as inorganic growth strategies such as mergers & acquisitions, new product launches, expansions, agreements, joint ventures, partnerships, and others to strengthen their position in this market.\nRESEARCH METHODOLOGY OF VERIFIED MARKET INTELLIGENCE:\nResearch study on the Yeast Market was performed in five phases which include Secondary research, Primary research, subject matter expert advice, quality check and final review.\nThe market data was analyzed and forecasted using market statistical and coherent models. Also market shares and key trends were taken into consideration while making the report. Apart from this, other data models include Vendor Positioning Grid, Market Time Line Analysis, Market Overview and Guide, Company Positioning Grid, Company Market Share Analysis, Standards of Measurement, Top to Bottom Analysis and Vendor Share Analysis.\nTo know more about the Research Methodology of Verified Market Intelligence and other aspects of the research study, kindly get in touch with our sales team.\nVerified Market Research has been providing Research Reports, with up to date information, and in-depth analysis, for several years now, to individuals and companies alike that are looking for accurate Research Data. It has large database which includes the latest content from renowned authors and publications worldwide. It also provides customized Data and Reports according to the need of the client.']	['<urn:uuid:edda0c5e-c719-489a-87ec-9a545fb117cc>']	factoid	with-premise	verbose-and-natural	distant-from-document	single-doc	expert	2025-05-12T21:52:51.670698	28	32	529
73	nonstick stoneware vs ceramic pan durability green choice	Stoneware and ceramic nonstick cookware have different durability profiles. Stoneware develops a durable nonstick surface through natural seasoning that can outlast most other finishes when properly maintained, though it is fragile and can break from thermal shock. Ceramic nonstick cookware, while free from harmful chemicals like PTFE and PFOA, typically has a shorter lifespan as the coating can wear off over time. From a sustainability perspective, stoneware is made from minimally processed natural clay with low environmental impact, while ceramic nonstick cookware involves more complex manufacturing with mixed materials that are difficult to recycle.	"['What is stoneware?\nA: Stoneware, first discovered by the Chinese for baking, has been around since ancient times and\nis made from natural clay products,hand-finished, and will vary in both color and texture.\nStones develop a durable nonstick surface, that will outlast most other finishes when looked after properly. Chefs have relied on stones for hundreds of years because of its evenly heating and because food maintains the warmth for an extended time when removed from its heat source.\nQ: If I’m not using soap to clean my stone, then how can it be clean?\nA: Soap does not kill bacteria. Soap (and detergent) when combined with water molecules penetrates the pores of food residue to soften it. This softening of food residue just makes it easier for the residue to be removed. Hot water alone makes oil and other food substances more fluid and therefore easier to dislodge, especially when used in combination with the nylon pan scraper. This rubbing action will effectively remove food residue.\nStoneware should never be soaked in water because soaking a stone will weaken it and eventually cause it to break or crack. Before using it for the first time, simply rinse it under warm water (no soap) and dry it completely. Just as you would never soak the stone in water, never, ever wash it with soap or other cleaning products. If you do, the food you cook forward will taste of soap. After your stoneware has cooled you can use a nylon scraper to loosen any food residue and the wash with a nonabrasive sponge or cloth under warm water and then completely towel drive. I have had excellent results cleaning stones with a nonabrasive kitchen brush while it is rather warm but I use hotter water so the stone is not shocked by temperature differences.\nQ: I keep hearing about seasoning but I really don’t understand it. Help!\nA: With use, seasoning will naturally occur in Stoneware. Try cooking cookies and frozen fries on it first. Fats and oils are absorbed onto the surface of the stone. The seasoning forms a non-stick coating, making greasing almost obsolete. With every cooking, the stone will turn from a light tan to darker shades of brown. As the oils soak into the stone, it will become a nonstick surface. Once this occurs, it is not necessary to continue applying oil prior to cooking.\nQ: My Baking Stone is sticky. Why?\nA: If a stone is over-oiled during the seasoning process, a sticky build-up can occur. This build-up can also cause food to stick to the stone. If this occurs, coat stone surface with a baking soda and water paste, let it sit for approximately 30 minutes and clean as usual. Although not recommended by most companies and voids the warranties, people have found by placing it in a self cleaning oven it will bring a stone back to its original state. Because many companies will void their warranties on the stone, I would only suggest this if you have decided you are at the point of tossing it anyway.If you decide to try this method, you will have to begin brushing it with oil again until it becomes seasoned.\nQ: What is the correct seasoning process?\nA: To start the seasoning process, simply brush the stone with vegetable oil for the three to five uses. I would not recommend spraying with cooking sprays. Baking high fat content foods also helps with the natural seasoning process.As mentioned above, begin with frozen fries and cookies.\nQ: Why are there indentations on the bottoms of some stones?\nA: The “indentations” were added to stones by some companies to allow air circulation around the stone. This improves baking results.\nQ: Why are baking stones better than glass or aluminum?\nA: Aluminum simply cannot absorb the heat from the oven and hold it evenly. Aluminum, and glass, does not stop at the temperature of the oven as stoneware does. Aluminum, is actually a good conductor of heat, it continues to rise in temperature. In an oven, heat is not directly applied to the aluminum, as on a stovetop, and therefore does not hold and maintain temperature. This is why underbaking and overbaking occur.\nQ: I’ve seen stoneware in stores and from direct sales companies. What is the difference, other than prices?\nA: Be aware stoneware quality varies among companies. Choose wisely. Cheaper does not mean it is the best buy for your money. Look for companies who offer warranties. Also be sure to read about the stones. Some stones are only to be used for frozen foods such as pizza. These type of stones are just for reheated processed foods. These foods are not meant to cook raw food on. Choose a stone you can cook raw meat on or cookies. Buying two different stones is just money wasted. Choosing wisely you can spend your money wisely.\nQ: There is a baking stone on the market that never breaks and comes with a lifetime guarantee. Why do you think of this?\nA: yes, this type of stoneware is called Kiln Brick (this is the company that makes stones sold through department stores–such as Wal-Mart, etc.) While Kiln Brick is less susceptible to breakage, it has the disadvantage of actually being too porous. When high fat items are baked on a Kiln Brick stone, the grease soaks up into its interior. The next time the stone is heated up, the grease caught in the center of the stone also heats up and starts to smoke, giving off a noxious odor.\nKiln Brick Stones are suited only for baking pizza or bread. Another disadvantage of Kiln Brick is that it must be pre-heated for at least 30 minutes before using it. Also, a 13-inch Kiln Brick weighs well over 2 pounds.\nQ: What about the really low-priced stones sold in mail-order catalogs?\nA: These stones are from the Far East and are very different. Imported stones do not have thermal expansion properties, making them more likely to break.\nQ: Can I use aluminum foil with stones?\nA: Aluminum foil can be used to cover the top surface of foods, such as pizza, or placed over the top of bakers, to act as a cover. However, foil should not cover the baking surface of the stoneware because it will block the beneficial effects of the stoneware. Remember, you can cover the top of foods with foil or use it as a cover, but you do not want direct contact with the baking surface and aluminum foil.If you find foods browning on top before they are done when using a convection oven, use aluminum foil loosely on top to remedy this and as always follow the cooking instructions that come with your convection oven.\nQ: Is it possible for stoneware to become saturated and not absorb any more fat or oil from foods onto its surface? Would this affect the baking quality if the stoneware were saturated?\nA: The stoneware will gradually absorb fats and oils onto its surface to form a non-stick coating. The seasoning on the stoneware will not affect the baking quality. Moisture will still pass through a very seasoned stone and heat will continue to be distributed evenly.\nQ: Can the stones be used on a grill?\nA: Stoneware was intended for oven use only. Stones cannot handle a direct flame and on grills, the flames cannot be controlled. Use of stones on any grill is discouraged and will void the warranties from most companies. I have personally used a bar stone on the warming rack of a gas grill and it did fine but again, I would not recommend it because the warranty could be voided.\nQ: What about a broiler?\nA: A stone should not be used under a broiler or on top of a direct heat source. For this reason, it is not recommended to cook stoneware in an oven above 500 degrees because the broiler element will be turned on at or above 500 degrees.\nQ: Can my stone be stored in the oven?\nA: Yes, your stone can be stored in the oven…however, it is not recommended to leave the stored stone in the oven while baking other foods because of thermal shock. I would store the stone on an oven rack or in a sturdy storage cabinet. Do not store other items on top of the stones.\nQ: Should I preheat Stones before use?\nA:Thermal shock, excessive or sudden temperature changes is the result of most stones breaking or cracking. You can avoid this by following these simple instructions. First, never put a hot stone in cold water and never bake frozen dense/solid foods. Frozen meats should be defrosted first. Pizzas and potatoes evenly distributed can be because they are more porous.Do not preheat a stone in which you are going to cook a frozen pizza. Preheat the oven according to the directions on the package and place the frozen pizza on the stoneware n advance. Give the stone the chance to get adjusted to the temperature of the pizza before placing it in the hot oven. You may have to add a few minutes to the baking time and in other ovens you may have to reduce the cooking time. Appearance of color and a thermometer are the best indicators if food is cooked enough.\nQ: Why does dish soap leave a flavor but garlic and fish don’t?\nA: Soap or detergent works to actually form a bond with fats and oils. Because fats and oils are part of the seasoning of the stone, when you try to rinse the soap away, it has actually bonded to the surface seasoning making it difficult to remove. A soap taste may then exist during your next usage. When you bake food with strong odors and flavors, there is nothing in the food that binds to the stone or seasoning. Garlic, fish, onion, etc., are natural food products and do not adhere to the stone. That’s why you can bake fish one night and cookies the next without having a “fishy” cookie taste.\nQ: Can stoneware be used in a convection oven?\nA: Definitely. A convection oven is an oven with a built-in fan that forces superheated air throughout the oven for a quicker, more even browning and baking. Since the oven temperature is uniform and controlled throughout, convection ovens do require cooking temperatures to be lower. For best results, follow baking time and temperature guidelines that are provided with the oven. And as stated above if browning occurs before the food is done, loosely lay aluminum foil over top.\nQ: What is the correct rack to put a stone in the oven?\nA: The second from the bottom. Some ovens are labeled “A”, “B”, etc. “B” is the correct position-the most “central” part of the oven. Before pre-heating the oven at your shows, always check rack position and adjust if necessary.\nQ: Can I place another pan directly on top of the stoneware to bake?\nA: No. This will create thermal shock and break the stone.\nQ: Why can’t two chicken breasts be baked on the larger stones?\nA: It is very important to evenly spread out foods over surface of the stone before baking. When two chicken breasts are placed on a stone, the area on which the food is placed remains cold while the remaining parts of the stone heat up. This puts a strain on that portion of the stone and could cause a thermal shock. Foods should be baked on stoneware pieces that match the size of the food item being baked. Rule of thumb: 2/3 of the surface of the stone should be covered with food.\nQ: Can you bake with two stones in the oven at the same time?\nA: Yes and no. Two stones can be used for baking at the same time if they are on the same rack. “Stacking” or placing stones on separate racks, one over the other, is not an efficient way to use the stones. The stone on the bottom actually absorbs most of the heat from the element and the upper stone does not heat properly.\nQ: Can stones be used in the microwave?\nA: Yes, food can be baked or reheated in the microwave. Stones actually work best with dry heat, as in a conventional oven, so the results will not be the same in a microwave. Stones with a stone lid does do well in the microwave and there are companies who offer these type of stones. These stones will cook food deliciously in a small window of time.', 'Balancing Utility and Sustainability with Green Cookware\nLike many moms, I am always seeking balance. When it comes to cookware, I try to balance health and safety, impact on the planet, and the overall ease of making a meal. Choosing greener cookware is more than simply avoiding Teflon pans: it’s a choice about avoiding toxins and reducing your footprint while still getting your hands on reliable and usable products. In a market inundated by so many options, not all of them high quality, how do you choose the right cookware for the job that’s also long-lasting, durable, and easier on the planet?\nA note on greenwashing: While terms like “natural”, “eco-friendly”, “sustainable”, “recyclable”, “biodegradable”, and “nontoxic” may appear to signal an environmentally safe product, these words don’t actually mean much. They are intentionally vague descriptions that many brands use to capitalize on the growing demand for more ecologically conscious goods. Known as greenwashing, this marketing tactic uses various techniques to mislead consumers into thinking that a product is more environmentally friendly than it is, rather than investing those advertising funds into making products that match their claims.\nTo confuse matters more, while many types of cookware are lauded as ""the sustainable choice"" based on materials used and longevity, the mining and production of these materials may in fact harmful to the environment, although the precise impact is difficult to measure as a whole and can vary significantly from brand to brand. Nonstick cookware and bamboo kitchen utensils are often the most prone to greenwashing, so it’s crucial to be aware of actual sourcing and manufacturing techniques that may well be associated with the very toxins or unethical practices you’re avoiding.\nCookware Uncovered: Nine Materials to Know\nTeflon is the patented name for the nonstick chemical also known as polytetrafluoroethylene (PTFE), and most of us at one time or another have been the proud owner of a slick-surfaced Teflon pan. Not limited to pan coating, this chemical is also used in everything from carpets to industrial applications.\nPFAS commonly refer to a group called per- and polyfluoroalkyl substances, which includes perfluorooctanoic acid (PFOA, also known as C8), a chemical used in the process of making PTFE. While claims have been made that most of it is burned off during the manufacturing process, exposure to PFOA may have serious health consequences.\nThe American Cancer Society notes that PFOA remains long-term in the body and environment and that exposure is often ongoing. The consequences of long-term exposure are still being studied but there have been noted correlations with testicular, kidney, and thyroid cancers and possible associations including prostate, bladder, and ovarian cancers. In addition, PFOAs have been linked to thyroid hormone disruption, while the chemicals as a group have been connected to low birth weight and immune dysfunction. It should be noted that the greatest risks of this chemical have been linked to manufacturing contamination of the environment in ways such as in drinking water rather than at the scale of periodic cooking, but many researchers feel the risk factor is still too high.\nBy 2015, the Environmental Protection Agency had significantly limited the manufacturing of PFOAS, but cautioned that “existing stocks of PFOA might still be used and there might be PFOA in some imported articles.”1 For more on the hazards of Teflon (and the alternatives!), click here.\nBottom line: Avoid Teflon and PFOAs.\nMost newer nonstick ceramic cookware can go safely from stovetop to oven, and requires less oil to cook with. High-conducting metal, typically aluminum or stainless steel, is covered by a nonstick coating made from chemically altered sand or stone. Thermolon, a patented non-stick coating used by multiple brands, is third-party certified as free from PTFE, PFOA, PFAS, lead, and cadmium.\nAluminum is safe from reacting dangerously to acids as the lightweight metal is coated, or anodized. In terms of sustainability, it rates well as a material that is both highly recyclable and also which producers have taken significant efforts to reduce energy used in manufacturing. However, it is notoriously difficult to recycle mixed material products.\nBottom line: If you need a nonstick pan to cook delicate foods, ceramic nonstick is your best option.\nYou’ve probably seen ceramic items on many wedding registries, as they’re typically deemed to be expensive non-essentials that look gorgeous sitting proudly on tabletops. Ceramic typically refers to hardened, nonmetal materials, and can be commonly found in kitchens as earthenware, stoneware, and porcelain, with stoneware being the most versatile and useful for everyday cookware.\nStoneware is fired at higher temperatures than other types of ceramics and does not require a glaze to be water-resistant. But when glazed, stoneware is about as close as it gets to nonstick (at least in this author’s opinion). The bakeware is great for the oven, and most can also go in the microwave or dishwasher for extra convenience. Stoneware should not be used directly on a stovetop or exposed to extreme temperature shifts.\nCeramic cookware is made from kiln-hardened natural clay. Any toxic materials from the soil are removed in the process as heavy metals like lead, cadmium, and arsenic vapourize below 1500°F, and a ceramic kiln is heated somewhere between 1800°F and 2400°F. Do be cautious with older stoneware, however, as toxic dyes may have been used in many of these glazes.\nIn terms of sourcing, ceramic clay may be the most plentiful and least harmful material in this guide, and requires minimal processing before going to market. Further, ceramics can be recycled into other products—although consumers won’t have an easy time finding a specific ceramic recycler.\nBottom Line: Ceramic cookware works well and can be made from minimal materials, but its price point may place some items out of reach, and its fragility can be intimidating for many families with small children.\nSturdy Cast Iron\nCast iron pots and pans are time-tested kitchen stalwarts. If kept and managed well, they can be passed down through generations, developing a nonstick-style patina from fastidious seasoning and use. While virtually indestructible, cast iron does have an Achilles’ heel: moisture. Rust is the bane of most cast iron owners’ existence, making it a little more finicky than most cookware. Cast iron cookware can’t be tossed in the dishwasher, left to soak for prolonged periods in the sink, or piled together in a place where humidity can take hold. It’s also heavy, doesn’t work well with certain foods, and can result in discoloured meals.\nHowever, cast iron gets high sustainability points for its lengthy lifetime: just ask anyone who’s been handed down a vintage cast iron pot, or has picked up and lovingly restored a junk store cast off! It’s also recyclable. As an added bonus, cast iron pans can actually add iron into your food! How much iron, and how well it’s absorbed, can depend on several factors, so if you are severely deficient in this mineral consult with a medical professional for additional supplementation. For best results, search for cast iron products manufactured in Canada or the United States.\nBottom line: Cast iron provides the longest-lasting cookware and offers the best return on investment in sustainability and versatility.\nSteel has long been used for cookware, but the exact benefits and disadvantages are based on the specific alloy used. The most common mixtures are shiny-looking stainless steel and darker carbon steel.\nStainless steel is a mix of iron with a high chromium content which creates shine, combats rust, and is lighter than cast iron. Like cast iron, stainless steel doesn’t react well to acidic foods, and can cause component minerals to leach into food.\nCarbon steel has a heavier carbon mixture than stainless steel and is known for its lightweight feel. Like cast iron, carbon steel requires some maintenance, as cookware can’t be left to soak and must be seasoned. Chefs love its ability to create a nonstick patina, and its even-heat distribution.\nWith proper care, both stainless and carbon steel products can last many years. Steel alloys are usually highly recyclable, with stainless steel being 100 percent recyclable, and both stainless and carbon steels are regularly recycled and reused.\nBottom Line: Stainless steel is affordable, long lasting, and recyclable. Carbon steel is fantastic for even heat distribution, and a stellar material for woks.\nCopper is pricey and gorgeous, doubling as as high-quality cookware and a statement piece for any kitchen. Valued for its heat conductivity and even heat distribution, copper is more often available in cookware when mixed with other metals, either as a core or bottom for aluminum or stainless steel pans (and to a lesser extent coated in tin or ceramics). The stainless steel combination is thought to make the most of copper’s natural heating conductivity while minimizing the negative aspects: namely, discolouration and leaching into foods.\nThe concern that copper from cookware may leach into foods, potentially affecting the liver, kidneys, and even fetal development, has not been robustly validated and the exact mechanism and need for concern is unclear. Research indicates that our bodies are highly efficient at preventing overexposure to copper, and copper is not listed by the EPA as a known human carcinogen. Keep in mind, however, that there are known health risks associated with metals such as nickel and aluminum, both of which may be used in copper-hybrid cookware.\nCopper ranks as the third least harmful material, behind cast iron and stainless steel, and is highly recyclable. Recycling may be hindered, however, depending on hybrid metal composition. If choosing copper or copper hybrids, look for those manufactured in North America.\nBottom line: Copper is a fantastic tool for chefs, though pricey. Use copper for its heat-conducting advantage and look for stainless pots and pans with a copper core.\nGlass can be used for both cooking and storage, and is generally durable, easy-to clean, and practical. It helps to cook foods faster, which means reduced energy consumption. On the downside, glass cooks unevenly and may crack under quick temperature changes (although the development of temperature-resistant glass has mitigated fear of exploding glass dishes).\nBecause glass is impermeable, it doesn’t absorb foods, flavours, germs, bacteria, mould, or cross-contaminants, making it great for those with food allergies and dietary restrictions. For food storage, glass is preferable to plastic, especially for acidic or fatty foods, which can absorb hormone-disrupting chemicals.\nThe overall sustainability of glass is higher than many metals because its production relies on naturally occurring sand. The primary environmental concern is atmospheric pollution from manufacturing, although responsible companies are taking steps to curb this too. Interestingly, researchers believe that CO2 savings from glass recycling may be ""as large, or larger, than the transportation emissions.”2\nBottom line: Glass is a fantastic and affordable kitchen staple. Search for energy efficient manufacturers.\nBamboo’s quick growth (it’s actually a grass!), drought-tolerance, strength, and flexibility make it a great material for the kitchen. It’s affordable, antimicrobial, and its versatility lends itself to everything from spatulas to cutting boards.\nWhile considered quite safe, don’t forget to review the manufacturing processes of bamboo products you bring into your home. It’s important to know how it has been glued together as some adhesives have been discovered to contain formaldehyde.\nDue to its extremely high growth rate, bamboo ranks as one of the most renewable materials around, although environmental risks increase if toxic finishes or adhesives are used. Bamboo can come from a variety of sources, and may grow in North America, but is more frequent in subtropical climates. Look for bamboo products created through non-toxic manufacturing techniques.\nBottom line: Bamboo is great; toxic glues are not.\nLike bamboo, wooden utensils are lightweight and less abrasive on your expensive cookware, and wood is an inert, non-reactive substance. Wood is also naturally biodegradable and can be sourced from artisans and crafters in your area, which is a great way to support local business. While wood is considered quite safe, make sure to only purchase items treated with food-grade oils such as walnut, coconut, and linseed oil, or beeswax or carnauba wax. Hardwoods like maple, walnut, oak, and cherry have antimicrobial properties. Decomposition of natural wood should have a limited environmental impact, but responsible harvesting of these materials must always be considered. Quality woods like hardwood maple grow naturally in the US and Canada—a carbon footprint win!\nBottom Line: Ethically harvested wood is a safe and sustainable kitchen option.\nThere is no single factor that will make your cookware both green and safe, as most types will have both benefits and drawbacks. It’s important to arm yourself with a little knowledge, and carefully choose the right tool for the cooking job you need, sourcing the most eco-friendly material from a reputable brand and manufacturer to get the safest and greenest cookware available. Your family, your kitchen, and your weeknight dinner will thank you!\nWant to learn more? Check out more EcoParent, including An Eco-Friendly Kitchen for Everyone, Green Up Your Kitchen Essentials, and Creating a Child-Friendly Kitchen.']"	['<urn:uuid:b2739684-ef10-4369-97ca-fc2799a41886>', '<urn:uuid:b0f6d41b-a09f-4ac9-8db3-efd30bad66d4>']	factoid	with-premise	short-search-query	distant-from-document	comparison	expert	2025-05-12T21:52:51.670698	8	94	4260
74	renovation cleanup safety procedure guidelines	Proper cleanup and safety procedures are critical in renovation work. For hardwood floor restoration, post-sanding cleanup requires thorough vacuuming, wiping with damp rags, and using solvents like vinegar and water to ensure all debris is removed before applying finish. When dealing with potentially hazardous dust like silica, a comprehensive protection program is necessary, including proper hazard assessment, worker training, appropriate equipment selection, regular fit testing and maintenance of protective equipment, and continuous monitoring of dust levels. These measures are essential as fine particles can penetrate deep into the lungs and cause serious health conditions.	['Hardwood floors are beautiful and welcoming, but they can become scratched and worn down over time. Restoring hardwood floors yourself isn’t hard, but requires a little attention to detail.Hardwood floors are beautiful and welcoming, but they can become scratched and worn down over time. Restoring hardwood floors yourself isn’t hard, but requires a little attention to detail. There are a couple of ways you can smooth out scratches, even out color, and restore that brand-new sheen to your floors. Take a look at the level of damage to your floors to determine which approach is right for you, then follow a few simple steps.\nMild or Serious Damage?\nIf your floors have scratches that are mainly in the finish and not deep in the wood, or are showing just minor wear, you can likely refinish them without needing to sand the wood. For deep scratches in the wood, major areas of damage, or high traffic areas that have caused uneven wear, a sander is the best choice to get a good, even result.\nIf you’ve determined that your damage is minor, you can use a refinishing kit to create a new finish without the effort or mess involved in sanding floors. These kits use a chemical to smooth out the floor’s surface rather than sanding. You scrub the floor with the chemical before cleaning it, and then you apply a new finish. This method can be used with any hardwood floor to improve the appearance of the wood, but won’t get rid of water stains, deep scratches, or major wear. Keep in mind that you can’t use this sort of kit on any floor that has been waxed.\nSanding Hardwood Floors\nFor floors that need a lot of help, sanding before refinishing is necessary. You can rent a sander from your local home improvement store for the day. Look for easy-to-use sanding equipment (such as a random orbital sander) that is best for do-it-yourself projects if you don’t have a lot of experience. Sanding can be messy work, so clear the area of anything that can be damaged and work in a careful and methodical manner. Be sure to wear protective equipment including dust masks and eye protection. Start with a heavier grit and then work to a finer grit as the floor becomes smoother. Pay careful attention to corners and along baseboards, which you might need to touch up by hand.\nCleaning Up After Sanding\nThe last thing you want is dust, dirt, and hair stuck in your fresh wood finish, so be sure to clean the floors really well after you sand. Start by vacuuming the area thoroughly, and then wipe the floors down with damp rags to get any remaining debris. Wipe the entire floor area with a solvent to finish the cleaning job; vinegar and water are a good all-natural choice.\nApplying the New Finish\nThe type of finish you use depends on what was previously used on the floors. You may need to test a few finish options to find the right one. Look for a finish that doesn’t peel and adheres easily to the wood. Test it in an inconspicuous area before you proceed. As you apply the finish, work from the far corner of the room out towards the doorway; be sure that you don’t box yourself in! If you are using a stain before the polyurethane coating, apply it evenly and allow it to dry completely. Dab to remove excess stain. The polyurethane coating should also be applied in a thin, even coat. A high-density foam roller is a good way to apply the polyurethane.\nTouching Up Mistakes\nIf you find uneven areas or debris stuck in the finish, sand carefully with fine-grit sandpaper, then reapply the finish to create an even look. When the finish dries, it should appear shiny and even.', 'Across numerous industries, silica dust protection is a critical component of workplace safety.\nTypically, this dangerous, even deadly dust is generated during activities such as cutting, grinding, drilling, and crushing materials that contain crystalline silica, such as concrete, stone, and sand.\nSince it’s 100 times smaller than a grain of sand, workers can breathe it in without realizing the dangers.\nBreathing in silica dust can have severe health consequences, including respiratory diseases and lung cancer. And, approx. 2.3 million people in the U.S. are exposed to silica at work every year.\nSo, what are some of the dangers? More importantly, what do you need to know about silica dust protection for you and your team?\nLet’s break it down.\nRelated Article: OSHA Targets Silica Hazards in Cut Stone Industry\nRelated Article: How to Protect Your Team from the Effects of Silica\nHealth Implications of Silica Dust Exposure\nWhen inhaled, silica dust particles can penetrate deep into the lungs and cause various respiratory ailments.\nProlonged exposure to high levels of silica dust can lead to the development of silicosis, a debilitating and irreversible lung disease. Silicosis is characterized by the formation of scar tissue in the lungs, reducing their ability to function effectively.\nSymptoms of silicosis include persistent cough, shortness of breath, chest pain, and fatigue.\nIn addition to silicosis, silica dust exposure is also associated with an increased risk of developing other respiratory conditions. These include chronic obstructive pulmonary disease (COPD), tuberculosis, lung cancer, and kidney disease. Furthermore, silica dust has been linked to an increased susceptibility to respiratory infections and exacerbation of existing respiratory conditions, such as asthma.\nRelated Article: NIOSH Takes Step toward Reducing Silica Dust Exposure in Mines\nThe Importance of a Workplace Respiratory Protection Program\nGiven the serious health risks posed by silica dust, it is crucial for workers to use appropriate respiratory protection measures to minimize their exposure. Respiratory protection can effectively reduce the inhalation of silica dust particles and safeguard the respiratory system. There are various types of respiratory protection equipment available, and the choice of the most suitable option depends on the nature of the work being performed.\nFor proper silica dust protection, your team should be equipped with one of the following.\n1. N95 Respirators\nN95 respirators are a common form of respiratory protection widely used in industries where workers are exposed to silica dust. These respirators filter out at least 95% of airborne particles, including silica dust, with a diameter of 0.3 micrometers or larger.\nN95 respirators create a seal around the nose and mouth, ensuring that the air breathed in is properly filtered. Remember that for silica dust protection, it’s essential to conduct fit testing to ensure a proper fit.\n2. Powered Air-Purifying Respirators (PAPRs)\nPAPRs are another option for respiratory protection, particularly in situations where the concentration of silica dust is high.\nPAPRs use a battery-powered fan to draw air through filters before delivering it to the wearer’s breathing zone. These respirators provide a higher level of protection than N95 respirators, and are more comfortable to wear for extended periods.\nHowever, they require regular maintenance, including filter replacement and battery recharging.\n3. Supplied Air Respirators (SARs)\nSARs, also known as airline respirators, provide a continuous supply of clean air from a remote source.\nThey are suitable for environments with extremely high levels of silica dust or where the oxygen concentration is insufficient. SARs are typically used in confined spaces, such as underground mines or tunnels. These respirators offer excellent silica dust protection, they can sometimes restrict mobility due to the need for an air supply line.\nImplementing an Effective Silica Dust Protection Program at Your Worksite\nTo ensure effective respiratory protection against silica dust, employers should establish comprehensive respiratory protection programs.\nThese programs should include the following elements:\n1. Hazard Assessment\nEmployers must conduct a thorough assessment of workplace hazards to determine the extent of silica dust exposure. This assessment helps identify areas requiring respiratory protection and enables the selection of appropriate equipment.\n2. Silica Dust Protection Training and Education\nWorkers should receive proper training on the risks associated with silica dust exposure, the proper use and maintenance of respiratory protection equipment, and the importance of following safe work practices. Regular refresher training sessions should be conducted to reinforce these principles.\n3. Proper Equipment Selection\nEmployers should select respiratory protection equipment that meets the required standards and is appropriate for the specific work environment. This includes ensuring that respirators have the correct filter type and efficiency level for silica dust particles.\n4. Fit Testing and Maintenance\nFit testing should be conducted to ensure that respirators fit properly and provide an effective seal. Regular maintenance, including inspection, cleaning, and replacement of filters, is essential to keep respiratory protection equipment in good working condition.\n5. Monitoring and Evaluation\nRegular monitoring of silica dust levels in the workplace is necessary to evaluate the effectiveness of control measures and identify any necessary adjustments or improvements to the respiratory protection program.\nSee OSHA’s guidelines on respirable crystalline silica- 1926.1153.\nBringing It Together\nProtecting workers from the adverse effects of silica dust is essential for maintaining occupational health and safety. By understanding the health hazards associated with silica dust exposure and implementing appropriate respiratory protection measures, employers can safeguard the well-being of their workforce.\nRespiratory protection programs, including proper equipment selection, fit testing, training, and maintenance, play a crucial role in mitigating the risks posed by silica dust, enabling workers to breathe safely and maintain their long-term respiratory health.\nAbout Worksite Medical\nIn most cases, OSHA requires medical surveillance testing, and at no cost to employees.\nWorksite Medical makes that program easier with mobile medical testing.\nWe conduct on-site respirator fit tests, as well as audiometric exams, pulmonary function tests and heavy metal lab work, right on your job site. We also keep accurate, easy-to-access medical records for your convenience. You’ll keep your employees at work, and stay ahead of OSHA inspections.']	['<urn:uuid:6d9063d7-a77d-40b6-b8ce-5a3984149cce>', '<urn:uuid:8f4da841-15f6-4f16-b21f-73c3a6c35f3c>']	open-ended	direct	short-search-query	distant-from-document	multi-aspect	expert	2025-05-12T21:52:51.670698	5	94	1628
75	equine supplements transport muscle damage prevention	To prevent muscle damage during transport, horses should be supplemented with vitamin E (1000-2500 IU/day) before long hauls, and vitamin C (7-10 grams) the day of and 2-3 days after transport. Additionally, Natural Vitamin E (RRR-a-tocopherol) in particular is up to 5 times more bioavailable than synthetic sources and provides better antioxidant, immune, cardiovascular, circulatory, and neuromuscular functions.	"[""For high performance horses prone to tying up\nNon oat micronized sweet feed, scientifically formulated to provide the fuel and essential nutrients required for optimum performance in horses prone to the tying up syndrome and other muscle disorders.\n|Crude Protein 15 %||Phosphorous 0.6%|\n|Crude Fat 10%||Vitamin E 1000 IU/kg|\n|Max Crude Fibre 10%||Selenium 0.8 mg/kg|\n|Added Salt 1.25%||Vitamin C 500 mg/kg|\n|Calcium 1%||Magnesium 5 g/kg|\nHYGAIN® RELEASE®is a blend of the following quality ingredients:\nMicronized Lupins, Micronized Beans, Micronized Maize, Micronized Barley, Sunflower Seeds, Rice Bran Oil, Natural Vitamin E, Molasses, Potassium Chloride, Magnesium Oxide, Calcium Carbonate, Di-Calcium Phosphate, Salt, Prebiotic (MOS), Probiotic (live yeast), Bio-Available Selenium, Bio-Available Chromium, Vitamins C & B1, Lysine, Methionine, Biotin, HYGAIN® Vitamin and Chelated Mineral Premix.\nNatural Vitamin E – This unique and highly stable form of Natural Vitamin E known as RRR-a-tocopherol is up to 5 times more bio available and potent than regular synthetic sources of Vitamin E. Research has shown that this form of Natural Vitamin E provides higher retention levels in plasma further aiding antioxidant, immune, cardiovascular, circulatory and neuromuscular functions.\nBio-Available Selenium – Selenium acts as an antioxidant to prevent fatty acid oxidation which can cause tissue damage. Research has shown that an organic form of selenium, is more readily absorbed and retained than selenium in inorganic forms.\nMagnesium & Potassium – Additional magnesium to balance calcium and assist in muscle relaxation and potassium to limit dehydration, increase blood flow to the muscles and prevent potassium depletion.\nVitamin B1 – Vitamin B1 aids in energy utilisation and helps keep horses calm if nervousness is part of the tying up syndrome.\nVitamin C – for immune function.\nPrebiotic – Prebiotics incorporated in HYGAIN® RELEASE® have a defined mode of action, promoting a healthy gut and supporting natural defences essential for optimum health and performance.\nBio-Available Chromium – The addition of chromium to performance horse diets has been shown to reduce the build up of lactic acid at the end of strenuous exercise or the closing stages of a race, and to increase the horse’s ability to store glucose.\nHYGAIN® RBO® – RBO® Equine Performance Oil® is a blend of rice bran oil, omega essential 3,6 & 9 fatty acids and natural anti-oxidants known to improve the health and structural integrity of cell membranes. RBO® oil plays an important role in maximising energy efficiency due to its energy dense nature and is digested efficiently in the small intestine reducing the ‘sugar high’ associated with high grain diets. RBO® promotes growth and development of the muscular system, top line and a healthy skin and coat.\nBiotin – Biotin, a water soluble vitamin is required by the enzymes responsible for protein synthesis and as a result is an essential vitamin for the healthy growth of hair and hooves. Research has shown that biotin improves skin and coat condition, increases hoof integrity and is widely recommended by farriers and veterinarians.\nHYGAIN® RELEASE® should be introduced to your horse’s diet gradually. Feeding amount may be influenced by age, breed, exercise intensity, desired body condition and climate.\nHYGAIN® RELEASE® is a highly palatable, digestible energy source. Care should be taken to match feed intake to workload. Reduce feed intake on rest days, if horses gain weight and/or training regime is interrupted. HYGAIN® RELEASE® is a complete feed and as such the addition of any other grains only serves to dilute the nutritional balance of the feed. Horses in full work on average will require 3-6kg of HYGAIN® RELEASE® per day and between 4-6kg of roughage per day. Do not feed additional selenium with HYGAIN® RELEASE® without consulting a veterinarian.\nDuring periods of heavy sweat loss additional supplementation with an electrolyte such as HYGAIN® REGAIN® is recommended.\nThese tips have been shown to be important in the management of many horses with the tying-up syndrome\n1. Slow days: Replace trotting and cantering with walking and swimming.\n2. Fast days: Warm-up walk only, fast work at 15 seconds per furlong or better every 4 to 5 days.\n3. Equestrian & endurance horses: Commencing the work session with a 800 to 1000 meters strong canter at around 480 meters/min or 28 km/hour.\nWeight per HYGAIN dipper (2ltr)\n- Introduce HYGAIN® RELEASE® to your horse’s diet gradually over a 7-14 day period.\n- Feed by weight and not by volume.\n- Feed a minimum of 1% of your horse’s body weight daily as roughage.\n- Horses have a small stomach, so feed little and often.\n- Ensure horses have access to clean water.\n- Do not add additional vitamins or minerals unless directed by a veterinarian or nutritionist.\n- For dietary recommendations and nutritional solutions contact HYGAIN®.\nComplete Micronized sweet fed for improved digestion\nNatural Vitamin E for antioxidant, circulatory and cardiovascular functions\nBio-available Selenium to assist your horse's muscle metabolism\nBio-Available Chromium assists in lowering lactic acid levels\nProbiotics to support a healthy gut and Vitamin C for immune function\nPure Rice Bran Oil to maximise absorption and feed efficiency for muscle building\nBiotin for rapid hoof development"", 'Carey Williams, Ph.D., Extension Specialist in Equine Management\nWhat is Stress?\nStress is the body’s response to anything it considers threatening. For a horse this could be anything, including trailering and traveling, showing, poor nutrition, feeding at irregular times, changes in other routines, environmental toxins, interactions within their social environment, variations in climate, and illness.\nSome types of stress include various physical stresses that are based on the physical makeup of the animal and its ability to respond to changes in diet, injury, etc. Psychological stresses are based on a horse’s personality and its perception of life. For example, some horses are more stressed than others by being in a stall for long periods of time.\nHow do Horses Cope with Stress?\nThe horse’s basic stress response starts with a change in behavior, either by moving away from a stimulus, swishing its tail, bucking, tensing up, etc. This stress will then cause activation of the sympathetic nervous system, called the “Fight or Flight” response. The sympathetic nervous system will create an involuntary action of the intestines (diarrhea), endocrine glands (production of adrenaline and cortisol), and heart (increase in heart rate). Next the neuroendocrine system will be activated, allowing the horse’s system to increase its energy utilization.\nEach horse deals with stress in a different way depending on their personality.\nDemonstrative, Confident Horse\n• Lets you know when it is stressed!\n• Bucks, kicks, bites, is very curious, mouthy, a troublemaker, etc.\nDemonstrative, Fearful Horse\n• Worries about everything!\n• Shies the first time it sees things and needs time to relax.\nPassive, Confident Horse\n• Usually wonders, “What’s everyone worried about?”\n• Not normally stressed, internalizes stress, shows little change even when stressed.\n• Usually is the last one in the field to take off running if something runs out of the woods.\nPassive, Fearful Horse\n• Wants to please!\n• Seems willing to do anything, but will tighten muscles and lips when stressed.\n• Won’t show fear until pushed over the limit.\nDuring exercise heat production will increase up to 50 %. This can create a problem when exercising under extremely hot and humid conditions. In response to heat stress the horse will increase its sweating rate, move a large portion of blood flow to capillaries under the skin, and increase its rate of respiration (may be over 20 breaths per minute in a resting horse) to help in the cooling process. Some horses will have a condition called “anhidrosis”, which is when the horse lacks the ability to sweat. This makes their coat feel very hot yet dry. The horse’s number one means of cooling down is through evaporation. When it does not sweat it is not able to use this method of cooling and the risk of overheating is even greater! Another way horses will show heat stress is with an increase in resting heart rate. At rest a normal range of heartbeats per minute should be between 28 and 36, however, when heat stressed it can be over 50. Internal rectal temperature will also be elevated to 104°F or higher.\nWhen treating heat stress, the first thing you will need to do is move your horse to a shady area. Provide fans, wind, or other forms of ventilation, which will help with the evaporation process. Spraying the horse with cold water, specifically the legs and abdomen, will also help in this process because the veins and arteries lie just below the skin and can rapidly cool blood flowing to internal organs. Allow the horse to drink small amounts of water frequently and provide electrolytes to replace those lost by excessive sweating. In severe cases you should contact a veterinarian immediately. Cold-water enemas will probably be administered and the horse will be treated for dehydration.\nOnce a horse overheats it is much more likely to overheat again, so preventing heat stress before it is ever a problem is extremely important. One way to help prevent heat stress is to provide regular electrolytes in hot weather, especially if training hard where the horse may lose a large quantity of water through perspiration. Increasing the amount of fat in the diet will allow you to decrease the amount of grain you need to feed, while keeping the energy density of the feed high. Always cool horses down properly after exercise, allowing plenty of time and ventilation. Avoid riding in extreme heat and humidity if at all possible.\nHorses fight cold weather by using energy to stay warm. As a general rule, a 1% increase in energy requirement is needed to replace energy loss from cold weather for each degree the temperature falls below the horse’s critical temperature. Critical temperature is the temperature below which a horse starts to expend additional energy to provide warmth. This temperature for individual horses will vary because of fat cover, hair thickness, acclimatization of the horse to cold, hair/coat wetness, and wind chill. For example, horses clipped during the winter months will have a much higher critical temperature and will have to be blanketed to help thermoregulation. You can tell if a horse is losing too much heat if snow melts on its back instead of accumulating on top of its hair; shivering can also be an indication.\nTo help horses decrease their critical temperature, one needs to precondition them for cold weather. Making sure horses carry enough weight to use as energy and insulation is a good way to go into winter. Provide free choice grass hay during the winter months, which will allow the horses to always produce enough internal heat through gastrointestinal fermentation. Providing shelter from wind, rain, and cold is important. If you have a horse that is not allowed in the shelter with other horses it is a good idea to provide a blanket, especially when there is freezing rain. Be sure blankets are waterproof. A wet blanket may cause a horse to lose more heat than it would if it were not blanketed.\nBelow are some reasons why transporting can be stressful to your horse:\n• Physical factors\n-unnatural sense of confinement\n-withholding food and/or water\n-noise and vibration of a moving vehicle\n-balancing and bouncing during the ride\n-presence of exhaust, dust, molds, manure, and urine\n• Psychological factors\n-separation from the herd\n-exposure to strange animals and environment\n• Climatic factors\n-high internal temperature and humidity\n• Health factors\n-fatigue from constant balancing\nShipping Fever is a respiratory disease that develops during or shortly after a trip. A primary factor in shipping fever is suppression of the immune system due to the stress of the trip. Other than shipping fever, transportation stress can cause dehydration due to reduced fluid intake, loss of appetite, hypocalcaemia (low Calcium) or hypomagnesaemia (low Magnesium), weight loss, dust inhalation and coughing, diarrhea, and also possibly tying-up.\nStudies in the past decade have investigated numerous factors during transport to try and make a horse’s haul more comfortable. Some of these factors include orientation of the horse in the trailer during transport, design of the suspension and ventilation of the trailer, quality of the transport environment and air quality, and generally the amount of stress a horse is under during various lengths of hauls. Most studies have shown that levels of the stress hormone cortisol to increase during transportation, along with greater fluctuations in heart rate (known as heart rate variability). Some studies have reported as much as a 6% weight loss during a 24-hour haul. Half of this weight was replaced over a 24-hour recovery period. Muscle enzymes called creatine kinase and aspartate aminotransferase (CK & AST) leak out of the muscle after a 24-hour haul. This is a potential indicator for muscle damage.\nTo decrease transportation stress it is recommended to keep your total travel time under 12 hours. If this is not possible, plan overnight stops where you can unload your horse and allow it to rest. It is also recommended to stop every 3 to 4 hours to offer the horse’s legs a break from the vibration, and provide water every 6 to 8 hours of the trip. Cleaning the trailer thoroughly after each use will decrease the amount of dust inhaled. It is also a good idea to leave your horse’s head untied or loosely tied to allow them to lower their heads if they need to cough. Always leave a trailer window or vent open to allow for better ventilation. Horses are much more tolerant of the cold then they are of dust. It is always a good idea to give a horse a few days to recover after arriving at the destination. If you are concerned about shipping fever you can monitor daily rectal temperatures, which will help you detect a problem before it gets serious. On long hauls especially, it is a good idea to provide things that are comfortable to the horse, such as their usual hay, water, grain, or bedding.\nAre dietary adjustments necessary before hauling a horse? Many people use bran mashes before transporting horses, however, there is no benefit in doing this. Bran mashes are what nutritionists call a “comfort food”. They make owners feel like they did the right thing for their horse, yet they are not proven to have any real nutritional benefit. Before long hauls, especially during competition, it is a good idea to put your horse on a vitamin E supplement (1000 to 2500 IU/day) to help them cope with the stress. Also, giving them vitamin C (7 to 10 grams) the day of and 2 to 3 days after transport can help reduce the risk of shipping fever. It is a very good idea to vaccinate your horses against respiratory disease at least 2 weeks before traveling. Remember to never ship a sick horse long distances. Also, allow plenty of time for your trip in case of emergencies—this will also decrease stress in the life of the person doing the transporting!\nDuring the metabolism of oxygen, about 98% of the oxygen consumed forms water and carbon dioxide, however, about 1 to 2% of the oxygen is not completely reduced and instead forms reactive oxygen species (ROS—intermediate molecules formed during metabolism of oxygen). These ROS produce harmful effects and can degrade proteins, DNA, and fatty acids that make up equine cells. Some of these ROS include hydrogen peroxide (H2O2), singlet oxygen (1O2), and free radicals (an ROS with an unpaired electron).\nAntioxidants are the body’s way of combating the negative effects of the ROS. Some common antioxidants include: vitamin E, vitamin C, beta-carotene, glutathione, lipoic acid, selenium, cysteine, glutathione peroxidase, etc. 11. What is oxidative stress, and how does it affect my horse? Oxidative stress occurs when the body’s antioxidants are overwhelmed by production of ROS. Antioxidant supplementation may be needed during times of stress that have been associated with an increase in oxidative stress. These include: intense and endurance exercise, rapid growth, reproduction, transportation, illness, or other types of stressful situations described above. When supplementing antioxidants, mixtures usually work best. It is important to avoid over-supplementation due to the fact that some of the antioxidant vitamins and minerals can be toxic to the system in extremely high doses.\nAre you giving your horse an ulcer? Did you know that 80–90% of all racehorses have ulcers? Also, 60% of all performance horses (including eventers, jumpers, and western performance events), and 30–40% of all dressage horses develop ulcers.\nA horse’s stomach secretes acid even when they are not eating, unlike in humans. However, only ½ of their stomach is protected against damage from the acid. When a horse grazes all day the roughage helps absorb the acid and the saliva produced neutralizes acid. Ingestion of a grain meal increases ‘gastrin’, a hormone that stimulates acid secretion. Therefore, it is important to always feed forage along with a grain meal. The main causes of stomach ulcers are changes in eating behavior, changes in management, or an increase in training intensity. Horses that are accustomed to being outside can develop ulcers after only 1 week of being kept in a stall, while others could develop them within 24 hours.\nSome signs that a horse is developing ulcers include a change in attitude, poor appetite, colic, decreased performance, decrease in body condition, weight loss, and a dull or sour attitude. To prevent ulcers, you need to prevent stress and minimize the use of non-steroidal anti-inflammatory agents (NSAID) such as, phenylbutazone (Bute).\nExertional Rhabdomyolysis (ER)\nOther terms for ER are “tying-up”, “azoturia”, or “Monday morning sickness”. This condition is muscle pain and cramping associated with exercise. Its main prevalence is in the middle gluteal or semitendinosus muscles of the hindquarters. There are sporadic or chronic forms of ER, which could be severe enough to decrease performance or end a career. There are different causes of tying-up for different breeds, however, recurrent ER (RER) primarily affects Thoroughbreds, Standardbreds, and Arabians. Nervous two-year-old fillies are most severely affected due to the stress associated with their training regime. With these horses the incidence may increase in severity as fitness increases.\nOne way to test for ER is to take a blood sample after exercise and have the sample analyzed for CK and AST (muscle enzymes described above). These enzymes will be drastically elevated during a bout of ER and may even be elevated at rest in a horse that chronically ties up. The clinical signs that your horse is tying-up include: stiffness of gait in the hind legs, unwillingness to move, and sensitivity of touch to the affected muscles. These muscles will feel very tight or tense. In severe cases the horse’s urine will be a brown or reddish color from the breakdown and excretion of the myoglobin in the muscle.\nTo treat a horse afflicted with ER, immediately move it to a box stall and call a veterinarian. Blanket the horse if weather is cool or hose the horse to remove sweat if weather is warm. Check for signs of dehydration using the skin pinch test and look at mucous membranes. Allow small frequent sips of water when the horse is hot, and free access to water when the horse is cool. Many other factors can contribute to tying-up. However, to help decrease the incidence of the disease attempts should be made to decrease training-induced stress. Dietary changes and vitamin E supplementation may also help in some cases.\nSummary of How to Minimize Stress in Your Horse’s Life:\n• Keep horses turned out as much as possible but if not possible, feed ad libitum hay.\n• Stick to a routine but if you need to make changes, do so slowly.\n• When traveling take items that are familiar (e.g. your own hay, water, grain, etc.).\n• Avoid riding in extreme weather conditions.\n• Feed a well balanced diet.\n• Maintain a good health program.\n• Provide a pleasant environment.\n• Provide regular varied exercise.\n• Allow for play time.\n• Prevent boredom.\n• Allow your horse social activity.\n• Keep yourself happy, healthy, and stress free!!\nReferences and Other Recommended Reading:\nAnon. 2002. Equine Industry Welfare Guidelines Compendium for Horses, Ponies, and Donkeys. (A. Michael, ed.). www.adas.co.uk/equinewelfare/ compendium.pdf.\nAndersen, K. 1996. Winter Care for Horses. Univ. of Nebraska Cooperative Extension Fact Sheet, #G1292.\nGrandin, T. 1997. Assessment of Stress During Handling and Transport. J. Anim. Sci. 75: 249-257.\nValberg, S. 2005. Equine Exertional Rhabdomyolysis: Management of Sporadic and Recurrent Exertional Rhabdomyolysis. Mid-Atlantic Nutrition Conference Proc. p. 197-203.\nWilliams, C.A., D.S. Kronfeld, T.M. Hess, J.N. Waldron, K.M. Crandell, K.E. Saker, R.M. Hoffman, and P.A. Harris. 2004. Antioxidant Supplementation and Subsequent Oxidative Stress of Horses During an 80km Endurance Race. J. Anim. Sci. 82:(588-594).']"	['<urn:uuid:24a00b73-3c69-40fd-9176-24ebf6374b43>', '<urn:uuid:a5936737-1cd0-4695-9f12-78bbad383e52>']	factoid	direct	short-search-query	distant-from-document	three-doc	expert	2025-05-12T21:52:51.670698	6	58	3447
76	What kind of music has Aṣa made throughout her career, and how does it compare to some current Croatian artists?	Aṣa is a Nigerian-French singer who released her debut album in 2007, featuring folk rock, acoustic, and soft rock styles with frequent use of acoustic guitars and melodic passages. In comparison, contemporary Croatian artists show diverse styles - for instance, Nemanja creates instrumental music with Latin American samples, while Kevlar Bikini evolved from punk metal to heavy rock with groove elements.	"['Aṣa - Aṣa FLAC album\nGenre: Rock / Folk and World music\nStyle: Folk Rock, Acoustic, Soft Rock, Folk\nFLAC version ZIP size: 1885 mb\nMP3 version ZIP size: 1708 mb\nWMA version ZIP size: 1670 mb\nOther Formats: ASF MP4 DMF MPC APE XM AC3\nby SH Staff July 8, 2018. Aṣa (Asha) is the debut studio album by Nigerian French singer Aṣa, released by Dramatico on January 1, 2007. Aṣa – Aṣa (Asha) Album Tracklist Below; 1. Jailer DOWNLOAD MP3. 2. 360° DOWNLOAD MP3.\nPerhaps the only new thing about Åsa is the quality. It is top-notch, a far cry from the hiss of older black metal. But even that is not something all that new: other bands have also adopted a higher production sound. So, the question remains, why should one get this album? It\'s definitely different, though it does not exactly bring in techniques new to the black metal scene. One different thing is the very frequent use of acoustic guitars and/or melodic passages.\nAṣa (pronounced ) is a French songwriter and singer of Nigerian origin (born Bukola Elemide January 1st 1970, Paris, France). Nicknamed Aṣa (little hawk) because she often ran away and changed directions as a youngster, she returned to Lagos in Nigeria with her family at the age of two. Her mother was a shopkeeper and her father shot video reports for weddings. Signed to the Naïve label, Aṣa recorded her debut album with producer Christophe Dupouy, which has been hailed as one of the best African albums of 2007.\nBPM Profile Aṣa. Album starts at 101BPM, ends at 144BPM (+43), with tempos within the -BPM range. Try refreshing the page if dots are missing). Recent albums by Aṣa. Bed of Stone. Beautiful Imperfection.\nRedirected from Bukola Elemide). Aṣa (pronounced ) is a Nigerian French singer, songwriter, and recording artist. Her stage name ""Aṣa"" is pronounced ""Asha"" in Yoruba. Aṣa was born in Paris to Nigerian parents who were working and studying cinematography in France\nAlbum Aṣa. Fire On The Mountain Lyrics. There is fire on the mountain And nobody seems to be on the run Oh, there is fire on the mountain top And no one is running I wake up in the morning Tell you what I see on my TV screen I see the blood of an innocent child And everybody\'s watching Now I\'m looking out my window And what. Fire On The Mountain"" Track Info. Written By Aṣa. Aṣa Aṣa.\nOr consider a donation? End of year lists. Consequence of Sound.\nThe song is taken from our new album Åsa and it\'s named ""En Hymne Til Våre Udødelige Forfedre"" This song is accually from our first dem back in 1994 but has now been re-recorded for our new album Åsa. Категория. Автовоспроизведение Если функция включена, то следующий ролик начнет воспроизводиться автоматически.\nTracklist Hide Credits\nGuitar, Keyboards – Cobhams Asuquo\n|5||Fire On The Mountain\nProgrammed By, Performer [All Instruments] – Benjamin Constant\nFlute – Magic Malik\n|7||No One Knows\nKeyboards – Cobhams Asuquo\nAcoustic Guitar – Asa*\nPercussion – Bofi\nAcoustic Guitar – Asa*, Pascal DanaëGuitar – Michaël OhayonPercussion, Drums – Bofi\n- Acoustic Guitar – Cobhams Asuquo (tracks: 6), Michaël Ohayon (tracks: 2, 4, 7, 8, 10)\n- Arranged By [Strings] – Cobhams Asuquo (tracks: 3, 4, 6, 8)\n- Arranged By [Strings], Conductor – Benjamin Constant (tracks: 3, 4, 6 to 8)\n- Artwork By – François Galinier\n- Backing Vocals – Asa* (tracks: 1 to 3, 5 to 10), Cobhams Asuquo (tracks: 1 to 6, 7, 8), Janet Nowse (tracks: 1 to 3, 5 to 7, 9)\n- Bass – Laurent Vernerey (tracks: 2, 8 to 10)\n- Cello – Emmanuelle Cohen (tracks: 3, 4, 6 to 8), Nadine Pierre (tracks: 3, 4, 6 to 8)\n- Double Bass – Laurent Vernerey (tracks: 3, 6)\n- Drums – Régis Cecarelli* (tracks: 3, 8, 9)\n- Engineer – Christophe Dupouy, Sébastien Viguier\n- Executive Producer – Marie Audigier\n- Guitar – Pascal Danaë (tracks: 2, 4, 6, 8, 10)\n- Mastered By [For Avrm Mastering] – Rodolphe Plisson\n- Mastered By [For La Source] – Jean-Pierre Chalbos\n- Mixed By – Christophe Dupouy (tracks: 1, 3, 4, 6 to 10), Renaud Letang\n- Mixed By, Producer [Additional] – Benjamin Constant (tracks: 2, 5), Sébastien Viguier (tracks: 2, 5)\n- Photography – Benoît Peverelli\n- Producer – Cobhams Asuquo\n- Programmed By – Cobhams Asuquo (tracks: 1, 2, 4, 7, 9)\n- Viola – Françoise Gneri (tracks: 3, 4, 6 to 8), Jean-Paul Minali-Bella (tracks: 3, 4, 6 to 8)\n- Violin – Ayako Tanaka (tracks: 3, 4, 6 to 8), Caroline Lasfargues (tracks: 3, 4, 6 to 8), Lyodoh Kaneko (tracks: 3, 4, 6 to 8), Virginie Buscail (tracks: 3, 4, 6 to 8)\n- Written-By, Composed By – Asa* (tracks: 1 to 4, 6 to 10), Cobhams Asuquo (tracks: 5)\nNotes2008 Japan Edition.\nBarcode and Other Identifiers\n- Barcode: 4562132121023\n|WN145125||Aṣa||Aṣa (CD, Album)||Naïve||WN145125||France||2007|\n|DRAMCD0036||Aṣa||Aṣa (CD, Album)||Dramatico||DRAMCD0036||UK & Ireland||2007|\n|DWT70050||Aṣa||Aṣa (CD, Album)||Naïve, Mercer Street Records||DWT70050||US||2008|\n|none||Aṣa||Aṣa (CD, Album)||Iyke The Don Entertainment||none||Nigeria||Unknown|\n|cdjust 208||Aṣa||Aṣa (CD, Album)||Naïve||cdjust 208||South Africa||2007|', ""In this edition of IndieRe, Radio Student from Zagreb, Croatia brings you a fresh selection of artist that have been active through the quarantine period. As a special feature we had an interview with Kornel Šeper, the bassist of the experimental band Šumski, but not only that, he is a promotor, concert organizer and an ex record label manager. He currently works in one of our most prominent clubs in Zagreb called Močvara. We also presented some distinguished artists such as Mark Mrakovčić, Delapse, Kevlar Bikini and Nemanja.\nOnce again we thrived to bring you a range of quality alternative artist active in this hard period.\nWe started the show with the zagreb based producer and artist Mark Mrakovčić. He started performing with his band called Scroll back around 2005. Mark had a big ambition and interest in recording his bands music and started learning about music production. As he got better, more and more local bands startet hiring him for recording. Through years of dedicated work Mark became one of the top underground producers on the scene, recording and mixing many of the bands that we covered through the Radio Muse programme. Even though his own music fell behind, he never stopped writting and after some time. In the beginning of june Mark released an album called Breeding Black Sheep, consisting of 8 songs, the album is just 21 minutes long. It is a raw indie rock album played wholly by him.\nNemanja is a band formed by Luka Šipetić who wrote the album in Pula in Istria, it is an instrumental album that really came to its own in the past few months after selling out a venue in Zagreb for the album promotion. The album is called Tarot Funk and features a lot of different musical styles in an LP package. With many latin american samples and catchy melodies Nemanja became one of our most critically acclaimed alternative artist.\nFor our special feature we recorded an interview with Kornel Šeper and covered his band Šumski. Šumski is an alternative experimental band that started being active through war years in Zagreb. They were active on and off for the best part of the last thirty years but started working more regularly with their last album Ostrvo Ledenog Kita which was almost completely crowdfunded. To have a talk with Kornel was even more interesting as he shared with us his experiences as a promotor and an ex record label manager working in one of our cult alternative clubs called Močvara.\nDelapse (real name Leonard Klaić) is a zagreb based producer who is most active in the ambiental electronic music. As he expressed himself he doesn't look at electronic music in the dance party fashion, he is mostly interested in atmospheres and vibes that comprise a bigger musical image. In his music you can often hear vocal guest appearences from alternative singers who normally don't sing or write electronic music. To mention a few we have Domagoj Šimek from the groove rock heavy band She loves pablo, and Marija Svibanj aka Mary May who mostly sings jazz and blues. Those colaborations put delapse in a very interesting artistic position, one which we will follow in the future.\nKevlar Bikini is a band that started as punkish metal bend around 2010. They've released 4 albums of music during which the band transformed from a 4 piece to a 3 piece and added to their music a lot more groove and punch. From almost a fun punk band they've become a serious band riding the heavy rock wave without losing it's playfulness and manic energy. Their last album called OPT-Outism was released in may 2020 for the croatian label Geenger records.\nFEATURED ARTISTS/PRODUCERS (WITH LABELS) OR FEATURED LABELS:\nŠumski (Geenger records)\nMark Mrakovčić (Dostava Zvuka)\nDelapse (Dostava Zvuka)\nKevlar Bikini (Geenger Records)\nNemanja (YEM agency)\nMark Mrakovčić – Moneymouth (Breeding Black Sheep - Dostava Zvuka – 2020)\nMark Mrakovčić – Tepesh Blues (Breeding Black Sheep - Dostava Zvuka – 2020)\nNemanja – Lovers (Tarot Funk – YEM agency – 2019)\nŠumski – Laguna (Safari - Kekere Aquarium – 1995)\nŠumski – Tito (Ronioci – Zvuk Močvare – 2003)\nŠumski – Ostrvo ledenog kita (Ostrvo ledenog kita – Geenger records – 2017)\nDelapse – Starry Eyes feat Mary May (single – Dostava Zvuka – 2020)\nKevlar Bikini – Ballerina’s toes (Opt-outism – Geenger Records – 2020)\nRADIO (HYPERLINK) & COUNTRY:\nRadio Student from Zagreb, Croatia\nWritten, mixed and produced by Ivan Kolar and Ivan Vlašić""]"	['<urn:uuid:6b1d3545-da6e-4dae-8b12-64a40bbbd284>', '<urn:uuid:1a99ad90-4e8f-4aa7-bce3-75c79d31924b>']	factoid	direct	verbose-and-natural	distant-from-document	three-doc	novice	2025-05-12T21:52:51.670698	20	61	1601
77	how kafka started history	Apache Kafka was originally developed in 2010 by a group of engineers at LinkedIn to handle high amounts of daily data. In 2011, the technology was made open-source. The name Kafka was chosen by co-creator Jay Kreps, who named it after author Franz Kafka because he enjoyed his writings. In 2012, Kafka graduated from the Apache Incubator.	"[""Introduction to Apache Kafka: Fundamentals and Working\nThis article was published as a part of the Data Science Blogathon.\nHave you ever wondered how Instagram recommends similar kinds of reels while you are scrolling through your feed or ad recommendations for similar products that you were browsing on Amazon? All these sites use some event streaming tool to monitor user activities. Kafka is one of the most popular events streaming platforms out there.\nBig companies like Netflix, LinkedIn, Uber, Walmart, and Airbnb use Kafka for a lot of things. Such as,\n- Netflix uses Kafka to provide tv show recommendations in real time.\n- Walmart uses Kafka for the real-time inventory management system.\n- For Uber’s technology stack, Kafka is considered the cornerstone. It uses Kafka for many things, such as computing the cab fare in real time depending on the demand, destination, and availability of cabs.\nApart from these, many other companies use Kafka for large-scale streaming analysis, log analysis, message brokerage services, etc. A real-time event-driven system is essential when every user activity is valuable and every second has a financial cost.\nSo, in this article, we will learn about the fundamentals of Apache Kafka and how it works. We will learn how to set up Kafka servers with Zookeeper and KRaft from the terminal and use the Kafka python client library to read and write events to topics.\nWhat is Apache Kafka?\nApache Kafka is a distributed, real-time streaming platform for large-scale data processing. Organizations use Kafka for real-time analytics and building event-driven architectures and streaming pipelines to process data streams.\nIn 2010, a group of engineers at LinkedIn started working on a tool to handle high amounts of data produced daily. In 2011, considering its usefulness, the technology was open-sourced. Jay Kreps, one of the co-creator, named it Kafka after the author Franz Kafka as he liked Kafka’s writings. Later on, in 2012, Kafka graduated from the Apache Incubator.\nHow does Kafka work?\nKafka has various components like Topics, Producers, Consumers, and Brokers that make Kafka a scalable, fault-tolerant and durable event-streaming platform. So, let us understand how each component works one by one.\nCommon Definition of Terms\nAn event is a real-world phenomenon. A Kafka event has an event key, values, time-stamp and optional metadata.\n- The event key is specific to an event. It can be a number or string value. For example, if we want to store the GPS data of cabs, we may assign the cab ID to the event key.\n- The event value field stores the data we want to write to the topics.\n- A timestamp is added to an event\nThe publish-subscribe messaging is a messaging pattern where the sender sends messages to a destination, and whoever subscribes to that destination receives messages. In this case, the central destination is called the topic.\nEvent streaming is an evolution of pub-sub messaging where messages are stored and transformed as they occur, in addition to typical pub-sub capabilities. An event stream is a continuous flow of events from sender to destination.\nKafka Topics and Partitons\nA Kafka topic is similar to a database table and is used to store data. You can create as many topics as you want, such as a “tweets-data” topic to hold tweets data. Topics are partitioned to allow data to be spread across multiple brokers for scalability. Partitions are log files that hold the actual data. A topic can have many partitions, which can also be configured programmatically.\nWhen a new event is published, it gets appended to one of the partitions. Events with the same event key are appended to the same partition. An event key could be a string or a number, for example, a user id. Kafka guarantees the order of events; consumers will read the events in the same order as they were written on that topic.\nEvery time a message is published on a Topic, an offset is added. An offset is an integer value that identifies a record or event inside a partition. Earlier records will have smaller offsets compared to later ones.\nsource: Kafka log\nApache Kafka Producers\nProducers are the applications that send data to Kafka topics. A producer can be any data source, such as Twitter, Reddit, Logs, GPS data etc. A producer client library can assist in ingesting the data from these sources to respective topics.\nThe records from Applications are written to the partitions. All the messages sharing the same event key are written to the same Kafka partition. If the events lack keys, the messages are written to topics in a round-robin fashion (p0->p1….->p0).\nApache Kafka Consumers\nKafka Consumers are the applications or systems that consume the data from the topics. Applications integrate a Kafka client library to read data from the topics.\nTo read data from Kafka, consumers first need to subscribe to a topic and then can read from single or multiple partitions. Consumer tracks its progress by using the offset of the last message processed.\nThe message order is guaranteed if the consumer consumes messages from a single partition. While reading from multiple partitions may not conserve ordering.\nUnlike traditional messaging platforms, events in topics are not deleted after being read. The lifetime of these events can be configured.\nConsumers identify themselves with a consumer group name, and each message published on a topic is delivered to one consumer instance within each consumer group that is subscribed to it.\nKafka guarantees ordering across multiple consumer instances by assigning partitions to consumers of the group., such that a single consumer in the group reads a single partition. This also ensures load balancing as there are multiple consumers.\nKafka Brokers, Topic Replication, and Controller\nKafka brokers are servers that store data. Kafka usually operates in clusters, which are made up of one or more brokers. Topics are partitioned and stored on multiple brokers. To ensure that Kafka is fault-tolerant, multiple copies of topic partitions are kept on different brokers. The default number of copies is 3, but this can be changed programmatically. So, in case one broker dies, data will persist on other brokers having the replicas.\nTo achieve high throughput, partitions are evenly distributed across brokers. Here’s a diagram to understand the architecture.\nTopic Replication and Controller\nEach active Kafka cluster has special brokers called controller nodes. The Zookeeper/Raft Protocol is responsible for controller selection. We know that each partition has multiple replicas determined by the replication factor. This controller node elects a leader for each of these partitions.\nA leader partition is a partition responsible for writing operations. Producers publish records only to the leader partition, and follower partitions replicate data from the leader. This reduces the overhead of writing to multiple partitions.\nWhenever a new topic is created or deleted, Kafka runs an algorithm to select a leader from the partition. The first replica always gets the preference. The distribution of partition leaders is even across brokers. So if you have 3 partitions and 3 brokers, each broker hosts a leader partition. This will spread the read/write load. When the leader fails, follower brokers take their place.\nTopics with leaders and followers are distributed evenly across brokers.\nNow you have an idea of how Kafka works. Let’s see how to set up a Kafka server from the terminal.\nApache Kafka With Zookeeper\nKafka uses Zookeeper to keep track of all the brokers in a cluster. The Zookeeper is responsible for following actions.\n- Notify Kafka when a broker dies, a new broker joins, a topic is deleted or created, etc.\n- Responsible for identifying and Electing the leader broker of a partition.\n- Metadata management, Permission, and configuration management of topics.\nSetup Kafka with the Zookeeper\nTo set up a Kafka cluster from the shell, you will need to perform the following steps:\n- Download and install the Apache Kafka binary files from the Apache website.\n- Extract the downloaded files and move them to the Kafka installation directory.\n- Start the Zookeeper server by running the\n- Start the Kafka server by running the\n- Create a new Kafka topic by running\nbin/kafka-topics.sh --topic test-events --bootstrap-server localhost:9092.\n- Start a Kafka producer by running\nbin/kafka-console-producer.sh --topic test-events --bootstrap-server. localhost:9092. Now write anything to the Kafka topic you just created.\n- Start a Kafka consumer by running the\nbin/kafka-console-consumer.sh --topic test-events --bootstrap-server localhost:9092. This will show you everything you have published to the test-events topic.\nAfter performing these steps, you will have a single-node Kafka cluster running on your machine. You can test the consumer and producer by running their respective scripts. The best thing about Kafka is both the producer and consumer are independent of each other, failure of one will not impact the other in any way.\nApache Kafka With KRaft\nSome flaws in Zookeeper’s implementation kept developers always wanting more.\n- With Zookeeper, Kafka clusters can only have 200,000 partitions.\n- A high-level leader election, while a broker joins or leaves, overloads the Zookeeper server slowing down the entire process.\n- Zookeeper security lags behind Kafka’s security.\nDue to these reasons, an alternate solution for metadata management and leader elections was created. As Kafka’s metadata are logs only, these logs can be consumed by Kafka brokers as internal metadata topics. In short, Kafka used itself to store metadata. A protocol called Raft was used for controller election, hence the name KRaft.\nSet-up Kafka with KRaft\nTo use Kafka with Kraft, create a cluster UUID.\nFormat Log directories\nbin/kafka-storage.sh format -t $KAFKA_CLUSTER_ID -c config/kraft/server.properties\nStart Kafka server\nThen, you can create topics and publish and consume events as usual.\nWorking with Kafka Using Python\nFirst of all, install Apache Kafka using pip.\npip install kafka-python\nSending messages to a Kafka topic\nThe following example shows how to use the\nKafkaProducer class to send messages to a Kafka topic:\nfrom kafka import KafkaProducer producer = KafkaProducer(bootstrap_servers=['localhost:9092']) # Send a message to the 'test' topic producer.send('test', b'Hello, Kafka!') # Flush the producer to ensure all messages are sent producer.flush()\nReceiving messages from a Kafka topic\nThe following example shows how to use the KafkaConsumer to receive messages from a Kafka topic:\nfrom kafka import KafkaConsumer consumer = KafkaConsumer( 'test', # consume messages from the 'test' topic group_id='my-group', # consumer group to join bootstrap_servers=['localhost:9092'], # Kafka broker address ) # Consume messages from the 'test' topic for message in consumer: print(message.value)\nProcessing messages in real-time with Kafka Streams\nKafka also provides a stream processing library called Kafka Streams, which allows for creating real-time streaming applications that can process data from Kafka topics.\nHere is an example of using Kafka Streams to count the number of messages in a Kafka topic in real-time:\nfrom kafka import KafkaConsumer from kafka.streams import KafkaStreams # Create a Kafka consumer to read from the 'test' topic consumer = KafkaConsumer( 'test', group_id='my-group', bootstrap_servers=['localhost:9092'], ) # Create a KafkaStreams instance to process the 'test' topic streams = KafkaStreams(consumer) # Define a function to process each message in the stream def process_message(message): # Increment a counter for each message received counter += 1 # Consume messages from the 'test' topic and process them with the function defined above streams.foreach(process_message)\nWhere to use Apache Kafka?\nKafka can stream and process website events, e-commerce data, IoT sensor data, and Micro-services generated logs.\nSome use cases of Kafka are\n- Real-time data pipeline: Kafka can help build a real-time data pipeline to ingest a large amount of data and make it available for real-time processing.\n- Streaming Data Analytics: Data from various sources can be ingested and analyzed to drive business and product decisions.\n- Event-Driven Architectures: Kafka can be used to create event-driven architectures that allow for immediate processing and action on data as it is generated.\n- Microservices: Kafka can be used as a communication channel between microservices. It can also retrieve and process large amounts of logs generated by various microservices.\nKafka’s ability to handle large amounts of data and support real-time processing makes Kafka an essential tool for those who need to process large quantities of data quickly and reliably.\nKey takeaways from the article\n- Kafka is a publish-subscribe event streaming tool for real-time data processing.\n- Its distributed nature makes it a low-latency, Highly available, fault-tolerant, robust tool for processing large amounts of real-time data.\n- Kafka has three core components Topics, Producers, and Consumers.\n- In future stable releases, Apache Kafka will no longer need Zookeeper but instead will leverage its capacity to store metadata logs in an internal topic and use the Raft protocol for controller election.\nSo, this was all about the basics of Apache Kafka.\nThe media shown in this article is not owned by Analytics Vidhya and is used at the Author’s discretion.""]"	['<urn:uuid:5a32a411-36f1-40a3-8adb-7604867d5f9c>']	open-ended	direct	short-search-query	similar-to-document	single-doc	novice	2025-05-12T21:52:51.670698	4	57	2123
78	back hurting need rotary machine advice	If you are experiencing back pain, you should not use the rotary torso exercise machine without the guidance of a physical therapist. You should stop the exercise and consult a personal trainer or physical therapist if you experience back pain while doing this exercise.	"[""The rotary torso exercise machine is a weight machine that strengthens the abdominal oblique muscles. Different brands vary in design but the exercise is the same for all brands. This torso rotation is performed while seated, with the fronts of your upper arms or shoulders resting against support pads. You adjust the resistance of a weight stack to your desired weight. Many models also provide a back rest to help you stabilize your pelvis and keep your torso upright.\nThe primary muscles used in this exercise are the internal and external obliques. The obliques are fan-like muscles that wrap around the sides of your waist to your lower back, like a corset. The internal obliques are deep muscles that initiate rotation from the same side -- when you rotate to the right, the right internal oblique muscle engages to create the movement. The external obliques cover the internal obliques and, because of the direction of their fibers, create rotation from the opposite side. When you rotate to the right, the left external oblique muscle engages to create the movement. The quadratus lumborum and erector spinae muscles also assist with this exercise and numerous muscles provide stabilization when you lift the weight.\nPurpose of the Machine\nThe purpose of this machine is to improve range of motion and stabilization through your torso, supporting your spine and assisting with movements such as looking over your shoulder. The rotary torso exercise machine can be used to isolate the oblique muscles, improve posture and, in some cases, treat vertebral joint problems. If you are experiencing back pain, do not use this machine without the guidance of a physical therapist.\nWhat to Focus On\nRegular abdominal strengthening will balance out daily tightening of your lower back muscles and hip flexors. Train your abdominal muscles at least three days a week, daily if possible. Before adding resistance, make sure your technique is correct. If you aren't sure how to do an exercise, ask a personal trainer to teach you. Relax your shoulders and focus on engaging your obliques as you rotate your torso. If you hold the arm pads or handles too tightly, you will use the muscles of your shoulder girdle instead of your abdominals. Stabilize your pelvis as you rotate by pressing the opposite heel down -- if you are turning to the left, press your right heel downward. Keep the movement slow and controlled through a full range of motion.\nWhat to Avoid\nThis is a good exercise to do when you first start training your oblique muscles, but after doing it regularly for a few months, advance to more functional complex exercises that challenge your balance and other muscles and joints at the same time. The machine stabilizes your pelvis, supports your spine and isolates specific muscles but doesn't functionally train your body for the movements of everyday life. Never force torso rotation. Rotation of the spine can only occur in the cervical, or neck, region and in the thoracic, or rib cage, region. Rotation of the lower back is limited by the structure of the bones. If you experience back pain while doing this exercise, stop and consult a personal trainer or physical therapist.\n- University of New Mexico: SuperAbs Resource Manual\n- ACE Advanced Health & Fitness Specialist Manual; American Council on Exercise\n- New York University Langone Medical Center, Spine Center: Muscles\n- Jupiterimages/Brand X Pictures/Getty Images""]"	['<urn:uuid:2852fa22-91f6-4c59-a194-2cfc371fee35>']	factoid	with-premise	short-search-query	distant-from-document	single-doc	novice	2025-05-12T21:52:51.670698	6	44	568
79	How powerful is the MC 25 KW controller and what environment rating does it have?	The MC 25 KW controller operates at 100-550V with 200A continuous motor current, and has an IP65 protection rating, which means it's totally protected against dust and protected against low pressure water jets from any direction with limited ingress permitted.	"['First Motor Controller designed for eVTOL, especially designed to meet aircraft certification standards.\nThe use of electric motors in aviation is no longer restricted to light small unmanned aircraft drones. Electric motors allow better autonomous control due to their short response time in comparison with combustion engines used in larger aircraft (commercial aviation, etc.). Furthermore, efficiency of electric engines is much higher than combustion ones (90% compared to values close to 30%). They also have a higher power-to-weight ratio, something very important to take into account in eVTOL applications.\nInstallation of electric brushless motors has associated specific electronics and components, being the batteries and motor controllers (ESC) the main ones. Its proper design is essential for the correct operation of eVTOL aircraft in order to ensure the maximum flight efficiency and the required robustness for Urban Air Mobility (UAM) operations.\nAn electronic speed controller (ESC), also known as motor controller, is an electronic component capable of regulating and controlling the rotational speed of an electric motor. In addition, it can also act as a dynamic brake as well as reversing the rotation sense. The most suitable ESC configuration for each application is determined by the type of electric motor it intends to control. In this sense, two main variants are distinguished:\nBrushed Motor ESC\nIn this case, the speed is controlled by varying the voltage of the armature. They are cheap but less efficient.\nBrushless Motor ESC\nThe operating principle is different, the speed variation is achieved by adjusting the time between pulses of current that is delivered to the different motor windings. These types of systems create three-phase AC current. These are the most popular for drone applications due to their great efficiency, power and lightness compared to the brushed ones. This is why Embention has focused on the development of this type of ESC for eVTOL applications, developing two high-power variants.\nThis controller is designed for all types of applications where high power is required with a reduced voltage (12-75V), as it is capable of withstanding 280A real continuous current. It also has a regenerative brake, and is optimized both in size and weight, this being only 680 grams. In addition, it offers the possibility to install forced air cooling if the situation requires it. It has been designed to operate in harsh environments as it has an IP68 protection grade.\nMC 25 KW\nThis model stands out mainly for the high power it is capable of controlling. The operating voltage covers a wide range between 100-550V. With a continuous motor current of 200A. Like its little brother, the MC 280, it is suitable to operate in humid environments (IP65) and also has the option of installing forced air cooling. The design stands out for having a reduced power / size ratio, with its maximum dimension being only 150 mm.\nBoth models are optimized for their integration in eVTOL, due to their reduced weight and size, the redundant control, the advanced telemetry and the compliance with the DO178C and DO254 aircraft standards required for eVTOL certification. However, their use is extrapolated to any system driven by electric motors that require high power such as light aircraft or professional drones.', 'Rugged Rating Organizations and Standards\nRatings are set by various government agencies, industry groups and/or independent laboratories and are cited by manufacturers in order to establish a more exact degree of environmental protection and reliability. Some of the more common standards include:\nIP Ratings (Ingress Protection) Rating for Equipment and Enclosures\nA three-digit number established by the International Electrotechnical Commission, is used to provide an IP Rating to a piece of electronic equipment or to an enclosure for electronic equipment. The IP code indicates the level, or amount of the protection.\nThe three digits represent three different forms of environmental influence:\nThe first digit represents protection against ingress of solid objects.\nThe second digit represents protection against ingress of liquids.\nThe third digit represents protection against mechanical impact damage.\n(*The third digit is often omitted, resulting in a 2-digit IP Rating covering ingress against solid objects and liquids only).\nThe larger the value of each digit, the greater the protection from that influence. As an example, a product rated as IP573 would be better protected against environmental factors than another similar product that was only rated as IP432.\n|First Number (Solids)||Second Number (Liquids)||Third Number (Mechanical Impact)|\n|0||No protection||No protection||No protection|\n|1||Protected against solid objects over 50mm e.g. hands, large tools.||Protected against vertically falling drops of water or condensation.||Protected against 0.225 joule impact (150g @ 15cm).|\n|2||Protected against solid objects over 12mm e.g. hands, large tools.||Protected against direct sprays of water up to 15° from vertical.||Protected against 0.375 joule impact (250g @ 15cm).|\n|3||Protected against solid objects over 2.5mm e.g. wire, small tools.||Protected against direct sprays of water up to 60° from vertical.||Protected against 0.5 joule impact (250g @ 20cm).|\n|4||Protected against solid objects over 1.0mm e.g. wires.||Protected against water sprayed from any direction. Limited ingress permitted.|\n|5||Limited protection against dust ingress (no harmful deposit)||Protected against low pressure water jets from any direction. Limited ingress permitted.||Protected against 2.0 joule impact (500g @ 40cm).|\n|6||Totally protected against dust ingress.||Protected against high pressure water jets from any direction. Limited ingress permitted. (Shipdeck)|\n|7||N/A||Protected against the effects of immersion between 15cm and 1M.||Protected against 6.0 joule impact (1.5Kg @ 40cm).|\n|8||N/A||Protected against long periods of immersion under pressure.|\nProtected against 20 joule impact (5Kg @ 40Protected against 0.225 joule impact (150g @ 15cm).cm).\nNEMA stands for the National Electrical Manufacturers Association. They provide a forum for the standardization of electrical equipment, enabling consumers to select from a range of safe, effective, and compatible electrical products. The organization has also made numerous contributions to the electrical industry by shaping public policy development and operating as a central confidential agency for gathering, compiling, and analyzing market statistics and economics data.\nEnvironmental ratings for enclosures based on ""NEMA"" Type designations\n|Enclosure Type Designation||Intended Use and Description|\n|1||Indoor use primarily to provide a degree of protection against limited amounts of falling dirt.|\n|2||Indoor use primarily to provide a degree of protection against limited amounts of falling water and dirt.|\n|3||Outdoor use primarily to provide a degree of protection against rain, sleet, wind blown dust and damage from external ice formation.|\n|3R||Outdoor use primarily to provide a degree of protection against rain, sleet, and damage from external ice formation.|\n|3S||Outdoor use primarily to provide a degree of protection against rain, sleet, windblown dust and to provide for operation of external mechanisms when ice laden.|\n|4||Indoor or outdoor use primarily to provide a degree of protection against windblown dust and rain, splashing water, hose-directed water and damage from external ice formation.|\n|4X||Indoor or outdoor use primarily to provide a degree of protection against corrosion, windblown dust and rain, splashing water, hose-directed water, and damage from external ice formation.|\n|5||Indoor use primarily to provide a degree of protection against settling airborne dust, falling dirt, and dripping noncorrosive liquids.|\n|6||Indoor or outdoor use primarily to provide a degree of protection again hose-directed water, and the entry of water during occasional temporary submersion at a limited depth and damage from external ice formation.|\n|6P||Indoor or outdoor use primarily to provide a degree of protection against hose-directed water, the entry of water during prolonged submersion at a limited depth and damage from external ice formation.|\n|7||Indoor use in locations classified as Class I, Division 1, Groups A, B, C or D hazardous locations as defined in the National Electric Code (NFPA 70) (Commonly referred to as explosion-proof).|\n|8||Indoor or outdoor use in locations classified as Class I, Division 2, Groups A, B, C or D hazardous locations as defined in the National Electric Code (NFPA 70) (commonly referred to as oil immersed).|\n|9||Indoor use in locations classified as Class II, Division 1, Groups E, F and G hazardous locations as defined in the National Electric Code (NFPA 70) (commonly referred to as dust-ignition proof).|\n|10||Intended to meet the applicable requirements of the Mine Safety and Health Administration (MSHA).|\n|12 and 12K||Indoor use primarily to provide a degree of protection against circulating dust, falling dirt, and dripping noncorrosive liquids.|\n|13||Indoor use primarily to provide a degree of protection against dust, spraying of water, oil, and noncorrosive coolant.|\nConversion of ""NEMA"" Type to ""IP"" Code designations*\n|Type Number||IP Designation|\n|4 and 4X||IP56|\n|6 and 6P||IP67|\n|12 and 12K||IP52|\n* Table cannot be used to convert ""IP"" Codes to ""NEMA"" Types. See NEMA 250 for additional details.\nMIL-STD (Military Standard) or MIL SPEC (Military Specification)\nIs a series of performance and manufacturing guidelines set by the US Department of Defense for military and commercial equipment and applications. These guidelines specify allowable parts and environmental condition ranges that a tool or other device must be able to operate in to meet compliance.\nMIL-STD 810 E and F are generally accepted as the highest standard for ruggedized testing by mobile computer manufacturers.\nMIL-STD-810G testing is for a range of extreme conditions including 72"" drops, shocks, vibration, humidity, altitude, rain-, dust- and sand-resistance, temperature extremes and thermal shock. MIL-STD-810G, which was created in October 2008, supersedes MIL-STD-810F.\nMIL-STD 810 test method is used to generate confidence in the environmental worthiness and overall durability of material system design. The testing process follows guidelines which include program documentation, program roles, test standards and laboratory test method guidelines for all categories. The laboratory test methods are broken down into 24 categories. The required compliance test categories include:\n|Low Pressure (Altitude)||Method 500.4|\n|High Temperature||Method 501.4|\n|Low Temperature||Method 502.4|\n|Temperature Shock||Method 503.4|\n|Contamination by Fluids||Method 504|\n|Solar Radiation (Sunshine)||Method 505.4|\n|Salt Fog||Method 509.4|\n|Sand and Dust||Method 510.4|\n|Explosive Atmosphere||Method 511.4|\n|Acoustic Noise||Method 515.5|\n|Acidic Atmosphere||Method 518|\n|Gunfire Vibration||Method 519.5|\n|Temperature, Humidity, Vibration, and Altitude||Method 520.2|\n|Icing/Freezing Rain||Method 521.2|\n|Ballistic Shock||Method 522|\nBefore testing can begin, the item has to have environmental exposure, which means that the test item is run under standard ambient conditions to ensure the item is operating properly and to ensure pretest baseline performance data is collected.\nThe actual tests are carried out according to pre-defined test plans and criteria. The tests can be laboratory or natural environment field tests, or a combination, which ever applies. The test procedure is dependent on the environment tested. The procedure(s) and its execution provide the basis for collecting the necessary information.\nAfter completion of each environmental test, the post-test data is examined and recorded in accordance with material specifications and program guidelines. The results are compared with the pre-test data. The post test record includes: test sequence used, deviations from planned test program, performance data, test conditions and a signature of the test team, to name a few. A final test report will be created for each test which includes an analysis of the test results.\nYou may notice some products will carry a MIL-STD 810E rating and some may state they are MIL-STD 810F compliant. MIL-STD 810F is a revision of MIL-STD 810E. The tests and methods are basically the same but much of the standard has been rewritten to provide clearer direction. Many of the changes that can be found with the MIL-STD 810F are minor or administrative in nature, but technical changes and improvements can be found throughout (i.e. additional technical testing guidelines).\nIS (Intrinsic Safety)\nIS is a term representing the Hazardous Location classifications as described in the National Fire Protection Association\'s (NFPA) National Electrical Code (Article 500). The National Electrical Code (NEC) defines Hazardous Locations as those areas ""where fire or explosion hazards may exist due to flammable gases or vapors, flammable liquids, combustible dust, or ignitable fibers or flyings.""\nAn intrinsic safety rating details the specific Hazardous Location in which an electrical device can be used without fear of electrostatic discharge that may cause an explosion.\nSUMMARY OF CLASS I, II, III HAZARDOUS LOCATIONS\n|CLASSES||GROUPS||DIVISION I||DIVISION 2|\n|I. Gases, vapors and liquids||A: Acetylene\nB: Hydrogen, etc\nC: Ether, etc\nD: Hydrocarbons, fuels, solvents, etc\n|Normally explosive and hazardous||Not normally present in explosive concentrations (but may accidentally exist)|\n|II. Dusts||E: Metal dusts\nF: Carbon dusts\nG: Flour, starch, grain, plastic, chemical dust\n|Ignitable quantities normally or possibly in suspension||Dust not normally suspended in an ignitable concentration (but may exist)|\n|III. Fibers and flyings||Textiles, wood-working etc.||Handled or used in manufacturing||Stored or handled in storage|']"	['<urn:uuid:122774ff-f69b-4592-ba7e-f2ac5f1ce04d>', '<urn:uuid:a15af1e6-fde8-4514-b3ff-f5d192b1a575>']	factoid	direct	concise-and-natural	distant-from-document	multi-aspect	novice	2025-05-12T21:52:51.670698	15	40	2044
80	developing public participation program want learn benefits outcomes citizen science workshops living laboratories	The main outcomes of citizen science workshops in Living Labs include: 1) Knowledge sharing and enhanced citizen engagement in research activities, 2) Practical experience with low-cost sensing technologies, 3) Development of tailored action plans integrating citizen science with Living Lab activities, 4) Creation of opportunities for long-term collaboration between labs and the public, and 5) Establishing methodologies for co-creation and citizen engagement. These workshops help tackle climate-related challenges while fostering public participation in scientific research.	['HOW WOULD YOU INTEGRATE CITIZEN SCIENCE IN YOUR LIVING LAB PROJECTS?\nBACKGROUND/HISTORY AND CONTEXT OF THE WORKSHOP\nThis workshop is a result of the European research and innovation project iSCAPE (Improving the Smart Control of Air Pollution in Europe) which aims to develop an integrated strategy for air pollution control in six European cities. During iSCAPE, low-cost air quality sensors were developed and these sensors have been used by citizens during a series of citizen science workshops run by the iSCAPE living labs, following the citizen science framework, developed by Connected Places Catapult (UK). This framework provided the iSCAPE living labs with guidelines and instructions (including tips and relevant materials) for the workshops.\nHaving run 12 citizen science workshops across the living labs, the iSCAPE team has gained a wealth of learnings and practical experience in how to use citizen science as a tool for Living Lab activities and public engagement. In addition, Tampere from the UNaLab project will share their learnings from citizen science in measuring water quality in their city. This knowledge will be shared and discussed during the interactive, hands-on workshop too.\nMAIN AIM / OBJECTIVE\nThe main objective of this workshop is to share knowledge and discuss the potential of using citizen science as a tool for citizen engagement in Living Lab activities. To achieve this, the workshop is structured around an interactive discussion amongst workshop participants, as well as a demonstration of low-costs sensing technologies. During the workshop, participants will be encouraged to develop a tailored action plan for integrating citizen science best practices with their Living Lab approach.\nThe participants, including those from a variety of living labs, will be invited to share experiences of citizen science initiatives, and contribute to discussions surrounding the challenges and opportunities of using low-cost technologies for citizen engagement.\nDuring the workshop participants will learn about:\n1. What is citizen science and how it can be used across a variety of living labs. A few examples of how citizens science has been used to tackle climate-related challenges by living labs will be provided.\n2. How can living labs incorporate citizen science methodologies in their co-creation and / or citizen engagement activities.\n3. Key steps and considerations to be made when planning citizen science initiatives.\n4. What technologies can be used when undertaking citizen science activities.\n5. First steps towards a citizen science community – the importance of motivation and guidance of the citizen scientists to establish an independent and a long-term collaboration between the living labs and the public.\nBRIEF OUTLINE / METHODOLOGY\nThis collaborative workshop leads to an action plan, integrating citizen science with living lab activities, and includes a hands-on sensor exploration. To guide the process, a pre-structured worksheet (A1) is provided as a tool to encourage discussions and gather knowledge from team members. Workshop participants will work in teams (4-5 teams). Each table will form a team with 4-5 participants.\n1) The session will open with a short introduction to the workshop and expected outcomes. A few examples of how citizen science was used in the H2020 projects (iSCAPE and UNaLab) will be provided. In iSCAPE citizen science has been deployed in exploring the effects of air pollution through the use of Smart Citizen Kit sensors. In UNaLab, the city of Tampere has engaged school children in measuring water quality in their neighbourhoods. A short warm-up exercise will follow the introduction and will not only allow the teams to get to know each other but will also enable the participants to step into the shoes of citizen scientists to gain an empathetic understanding of the task at hand.\n2) The participants will be introduced to the iSCAPE Citizen Science framework. Working on a large worksheet, teams will go through the different stages and elements which should be considered when conducting citizen science initiatives, including value that citizen scientists could bring to the research process.\n3) Upon completion of the exercise, each team will present their work to everyone and gather feedback. We believe that this collaborative exercise will open up new ways of working with the general public/citizens who are interested in researching any given topic of interest.\n4) The final part of the workshop will focus on the Smart Citizen Kit, a hands-on demo of low-cost sensing technologies. This exercise will allow workshop participants to not only have a first-hand experience of using such technologies but also provide an opportunity for the teams to engage in deeper conversations about low-cost technologies. Participants will also have the opportunity to see the water measurement rucksacks that Tampere will showcase from their activities.\nProject Manager at Krakow Technology Park\nAgnieszka Włodarczyk-GebikProject Manager at Krakow Technology Park\nSenior project manager with experience in public, private and financial sector. Since 2013 works for Krakow Technology Park. Project coordinator for regional & international projects in the field of smart city, innovation and circular economy under regional and European programmes (InterregCE, Erasmus+, Horizon2020 & Horizon Europa). Initiator and coordinator of Krakow Living Lab since 2015. Author and co-author of studies and publications in the field of smart city and the use of the living lab methodology in businesses and administrative processes. Evaluator in new living lab selection waves and mentor during virtual learning labs. Enthusiastic about co-creation and involving inhabitants to drive the change. Email - firstname.lastname@example.org']	['<urn:uuid:90873ecd-d8c6-4e7e-9f4c-eb1e1f47dcac>']	open-ended	with-premise	long-search-query	distant-from-document	single-doc	expert	2025-05-12T21:52:51.670698	13	75	885
81	What are the main differences between how culture is learned and passed down in organizations versus general society?	In organizations and society, culture is learned rather than inherited, but they differ in how they're transmitted and changed. In society, culture is acquired as part of being a member of society, including knowledge, beliefs, art, morals, customs and habits. Organizations, however, reflect both the national culture of their location and their own organizational climate. While an organization's climate can be influenced and changed by new ownership (through policies, procedures, and management styles), the national culture aspects cannot be changed. This is because national culture is deeply embedded in shared history and values, while organizational culture can be more deliberately shaped through specific approaches like market culture, clan culture, or hierarchical culture.	['28 Mar Bridging the Culture Gap in Cross-Border M&A\nIt is no secret that cultures change from country to country. They may even change from region to region within the same country. Having an M&A advisor who is intimately familiar with the culture of your target country is a critical component to executing a successful transaction.\nIn this final installment of our Buy-Side-as-a-Service (“BSaaS”) series we will discuss culture from the perspective of a nation and an organization. It is true that individual culture exists and is important; however, it is less significant in our discussion. For that reason we will focus solely on national and organizational culture.\nBefore we jump into the weeds it is important to ensure we are speaking the same language when we discuss culture. Sociologists and anthropologists may argue over the definition of the term culture, but for our purposes we will use the following: culture is the total way of life of a people, composed of their learned and shared behaviour patterns, values, norms, and material objects.\nIn the world of M&A, the culture of the target company is generally acquired in its entirety when the transaction is finalized. Throughout the integration process the culture may evolve to reflect that of the Buyer. When approaching a cross-border transaction management shouldn’t focus on if the culture, at the national or organizational level, is the same or different. Rather, management should approach the culture question from the perspective of the expected synergies. Will the changes necessary to recognize deal synergies cause a conflict in both cultures?\nIt is natural to expect that some bumps will be encountered along the way for both the Buyer and Seller. It is management’s job to evaluate to what degree these bumps may actually become roadblocks to achieving the business goals. When conducting this analysis management can look at culture from the perspective of the nation in which the target is located and the organization itself.\nA national culture connects people who share a common history, beliefs, and values. It represents how these people interact among each other and how they react to someone external to their culture. Companies engaging in cross-border M&A transaction must take care to understand how culture at HQ differs from the culture found in other countries where the target entity is located.\nBuyers approaching an M&A transaction with a foreign company that adopt an “our way or the highway” mentality are likely in for a poor outcome. This is due to the fact that different cultures have different rules and norms for behaviour and procedures. It is unlikely that they will change due to new ownership of their company. Both buyers and sellers should be aware that work will need to be done before, during, and after the transaction to ensure that cultures are respected throughout the process.\nSavvy buyers will try to get an idea as to a country’s culture prior to engaging with a target. Various methods could be utilized in this endeavour from traveling to the country for a first hand experience, speaking with colleagues in the country, or dong some online reading. All can be beneficial, though some are obviously more fun than others. To provide an introduction to various forms of culture we will present Hofstede’s cultural dimensions theory below. This is a theory, and not a law, and should be viewed as a jumping off point and not the finish line in your cultural research.\nHofstede’s Cultural Dimensions Theory\nProfessor Geert Hofstede surveyed IBM employees from around the world from 1967 – 1973. His research led to the creation of Hofstede’s cultural dimensions theory and studies how culture influences values in the workplace. The original study produced the following six dimensions:\nHow does society handle inequality? To what degree do the lower members of society accept and expect that power is unequally distributed?\nIn a high power distance society people tend to accept that a hierarchy is necessary and don’t question the status quo. In a low power distance society people demand a more equal distribution of power. People in a low power distance society will want justifications for any inequalities.\nIndividualism vs. Collectivism\nIndividualistic cultures are defined by people who are obligated to take care of themselves and close family only.\nA collectivist culture is more tight-knit. People in a collectivist culture can rely on support from extended family and even other, unrelated members of the community.\nIndividualism vs. collectivism gets down an “I” vs. “We” mentality.\nMasculinity vs. Feminism\nIf a culture tends to focus on material rewards, assertiveness, and achievement it would be defined as masculine.\nOn the other hand, a feminist culture is more focused on cooperation, modesty and is more consensus-oriented.\nTo avoid gender stereotypes masculine and feminine can also be referred to as tough and tender, respectively.\nThis dimension gets down to how comfortable or uncomfortable members of a society tend to be with uncertainty.\nA country strong in uncertainty avoidance would exhibit unwavering beliefs and not tolerate non-traditional ideas or thoughts. Countries that are low in uncertainty avoidance are more open to different thoughts and experimenting with new ideas.\nLong Term vs. Short Term Orientation\nHow do you respect your past while also preparing for a future that is certain to change? Countries around the world approach this from different perspectives.\nA short term orientation culture adheres to time-honoured traditions and tends to be skeptical of change.\nLong term oriented cultures are more open to change. They encourage efforts to plan today for what changes may come tomorrow.\nIndulgence vs. Restraint\nOf all the dimensions these are probably the easiest to grasp.\nA country that leans towards indulgence accept gratification of basic human needs and other wants and desires. Live life and have fun may be a slogan for these cultures.\nThose that are more restrained suppress gratification and may go so far as to do so via strict social norms or laws.\nNaturally, just because a country tends to lean toward one direction doesn’t mean a company operating in that country is the same. For example, you could have a company that has adopted a culture of experimentation and challenging the status quo that just happens to be located in a country generally defined by strong uncertainty avoidance. Management should use Hofstede’s cultural dimensions as a tool to start understanding other cultures and not as a law written in stone.\nWe’ve reviewed how national culture influences individuals. Now let’s explore how culture influences the companies that operate within a country’s borders. A review of organizational culture and the role it plays in M&A follows.\nBefore you can understand an organization’s culture, you must first understand the national culture. The organization, by operating within a country, will have many similarities with the national culture. However, differences will also be evident.\nAs buyer’s consider the culture of a foreign entity they can divide the organization into two parts:\n1) The organizational climate\n2)The national culture\nA new owner can influence the organizational climate. This includes changing thinking about how the business operates and introducing new practices and technologies.\nThe national culture part of the organization, however, cannot be changed. A company located in the U.S. will have employees who mostly adopt American culture. In more authoritarian regimes aspects of the national culture may be imposed by law on companies.\nIt can be helpful to classify the organizational culture of your company as well as your target. While we don’t have a universally agreed upon set of categories for organizational culture, the following are a good start:\nMarket Culture – Develop a competitive advantage and pursue a market leading position\nClan Culture – Develop strong internal morale among employees to create a more tight-knit and cohesive company\nHierarchical Culture – A bureaucratic structure\nAdhocracy – A flexible organization that encourages innovation and an entrepreneurial mindset\nPower Culture – Focus on power with well-defined rules\nRole Culture – Formal procedures and written rules. Similar to a hierarchical culture\nPerson Culture – Personal growth and development is a focus\nAs two companies plan to merge it is necessary to consider and plan for obstacles arising from conflicting cultures. This will not be isolated to one level of the organization. Incompatibilities in culture can be witnessed from the ground floor employees all the way up to the C-Suite executives.\nWhere is Culture Reflected in the Organization\nWhen examining an organization, culture is reflected in various processes:\n1) Policies & procedures: How are procedures documented, who is responsible for developing the policies and procedures, and how are changes approved?\n2) Systems & controls: What type of relationship do managers have with personnel? Is the objective to manage and control or to coordinate?\n3) Information & communication: What information is shared and how it is distributed to various levels of the organization\n4) Decision making: who makes decisions, who is involved in the process, and what level of the organization is responsible for certain decisions\nApproaches to Connecting Cultures\nVarious theories exist that try to guide companies with different cultures through the integration process. Like many theories, they sound good on paper but may be harder to execute in the real world. The following are a few established theories. Your integration may be able to incorporate pieces from the following or may require an entirely different approach. Motives for the transaction, the geographies involved, the human resources and other factors will all play a role in deciding upon a strategy when connecting cultures.\nImpose Your Culture\nThis method is very straightforward. The buyer, as the new owner, will impose their culture on the newly acquired company. If you decide to go this route ensure it is done with careful planning and execution. Employees will want to understand the motives behind the decision and will seek justification as to why your organizational culture should be adopted.\nThis approach allows the buyer and newly acquired entity to retain their existing organizational culture. Pros to this method are that culture shock can be reduced, or eliminated, and employees can continue to perform at a high level as they aren’t distracted by change. This approach could also reduce the risk of M&A failure. Cons include difficulty in communication and limiting potential synergies.\nCreate a New Culture\nIf neither of the above sounds appealing then why not create a brand new culture. This will be a complex task. Management will need to identify the pros and cons of each existing culture and then structure a better culture that works for all parties. This approach starts with a detailed understanding of the various cultures and creating bulletproof reasons as to why certain elements should be included in the new culture. To pull this off management will need to be expert communicators.\nRegardless of which approach you take to connecting two cultures conflicts are bound to arise. In preparation for a transaction management should openly acknowledge that conflicts will occur and have a plan to quickly identify and resolve issues before they get out of hand.\nSources of Cultural Conflict\nUnderstanding the differences of national and organizational culture can help a buyer plan for a cross-border M&A transaction. It is also helpful to understand various sources of conflict. Below we describe a few various sources you should consider throughout the M&A process.\n1) Cultural differences – Misunderstandings that result from differences in point of view and methods of communicating\n2) Assimilation vs. ethnic identity maintenance – The needs to assimilate with a new culture competing with conservation of a cultural and ethnic identity\n3) Power imbalance – The manner in which power is distributed\n4) Competing goals – Different divisions within the organization may have competing goals, resulting in conflicts during interaction\n5) Competition for scarce resources – From capital to raw materials different divisions within the same company will be competing for limited resources\nWhen preparing for a cross-border M&A transaction a buyer will have many important facts to consider. While valuation, deal structure, and integration plans are critical, buyers must also recognize that culture plays an important role. It is important to create a global corporate culture for your business that respects and incorporates the various national cultures.\nIf you are exploring M&A opportunities in the United States our team of licensed investment bankers would love to have a conversation. Our team has worked with foreign buyers before and we can help you navigate some of the cultural aspects of deal making that aren’t easy to pin down in a spreadsheet.\nFill out our contact form and one of our associates will be in touch.\n Rogers, E. M., & Steinfatt, T. M. (2007). Intercultural Communication. Long Grove: Waveland Press. Retrieved May 29, 2019.\n Hofstede Insights. (n.d.). National Culture. Retrieved May 29, 2019, from https://www.hofstede-insights.com/models/national-culture/\n Nahavandi, A., & Malekzadeh, A. R. (1993). Organizational culture in the management of merges. Westport (Connecticut): Quorum Books.\n Schneider, S. C., Barsoux, J., & Stahl, G. K. (2014). Managing across cultures. Harlow, England: Pearson.', 'Culture and Subculture\nCulture is part of the external influences that impact the consumer. That is, culture represents influences that are imposed on the consumer by other individuals.\nThe definition of culture offered in one textbook is “That complex whole which includes knowledge, belief, art, morals, custom, and any other capabilities and habits acquired by man person as a member of society.” From this definition, we make the following observations:\n- Culture, as a “complex whole,” is a system of interdependent components.\n- Knowledge and beliefs are important parts. In the U.S., we know and believe that a person who is skilled and works hard will get ahead. In other countries, it may be believed that differences in outcome result more from luck. “Chunking,” the name for China in Chinese, literally means “The Middle Kingdom.” The belief among ancient Chinese that they were in the center of the universe greatly influenced their thinking.\n- Other issues are relevant. Art, for example, may be reflected in the rather arbitrary practice of wearing ties in some countries and wearing turbans in others. Morality may be exhibited in the view in the United States that one should not be naked in public. In Japan, on the other hand, groups of men and women may take steam baths together without perceived as improper. On the other extreme, women in some Arab countries are not even allowed to reveal their faces. Notice, by the way, that what at least some countries view as moral may in fact be highly immoral by the standards of another country. For example, the law that once banned interracial marriages in South Africa was named the “Immorality Act,” even though in most civilized countries this law, and any degree of explicit racial prejudice, would itself be considered highly immoral.\nCulture has several important characteristics: (1) Culture is comprehensive. This means that all parts must fit together in some logical fashion. For example, bowing and a strong desire to avoid the loss of face are unified in their manifestation of the importance of respect. (2) Culture is learned rather than being something we are born with. We will consider the mechanics of learning later in the course. (3) Culture is manifested within boundaries of acceptable behavior. For example, in American society, one cannot show up to class naked, but wearing anything from a suit and tie to shorts and a T-shirt would usually be acceptable. Failure to behave within the prescribed norms may lead to sanctions, ranging from being hauled off by the police for indecent exposure to being laughed at by others for wearing a suit at the beach. (4) Conscious awareness of cultural standards is limited. One American spy was intercepted by the Germans during World War II simply because of the way he held his knife and fork while eating. (5) Cultures fall somewhere on a continuum between static and dynamic depending on how quickly they accept change. For example, American culture has changed a great deal since the 1950s, while the culture of Saudi Arabia has changed much less.\nDealing with culture. Culture is a problematic issue for many marketers since it is inherently nebulous and often difficult to understand. One may violate the cultural norms of another country without being informed of this, and people from different cultures may feel uncomfortable in each other’s presence without knowing exactly why (for example, two speakers may unconsciously continue to attempt to adjust to reach an incompatible preferred interpersonal distance).\nWarning about stereotyping. When observing a culture, one must be careful not to over-generalize about traits that one sees. Research in social psychology has suggested a strong tendency for people to perceive an “outgroup” as more homogenous than an “ingroup,” even when they knew what members had been assigned to each group purely by chance. When there is often a “grain of truth” to some of the perceived differences, the temptation to over-generalize is often strong. Note that there are often significant individual differences within cultures.\nCultural lessons. We considered several cultural lessons in class; the important thing here is the big picture. For example, within the Muslim tradition, the dog is considered a “dirty” animal, so portraying it as “man’s best friend” in an advertisement is counter-productive. Packaging, seen as a reflection of the quality of the “real” product, is considerably more important in Asia than in the U.S., where there is a tendency to focus on the contents which “really count.” Many cultures observe significantly greater levels of formality than that typical in the U.S., and Japanese negotiator tend to observe long silent pauses as a speaker’s point is considered.\nCultural characteristics as a continuum. There is a tendency to stereotype cultures as being one way or another (e.g., individualistic rather than collectivistic). Note, however, countries fall on a continuum of cultural traits. Hofstede’s research demonstrates a wide range between the most individualistic and collectivistic countries, for example—some fall in the middle.\nHofstede’s Dimensions. Gert Hofstede, a Dutch researcher, was able to interview a large number of IBM executives in various countries, and found that cultural differences tended to center around four key dimensions:\n- Individualism vs. collectivism: To what extent do people believe in individual responsibility and reward rather than having these measures aimed at the larger group? Contrary to the stereotype, Japan actually ranks in the middle of this dimension, while Indonesia and West Africa rank toward the collectivistic side. The U.S., Britain, and the Netherlands rate toward individualism.\n- Power distance: To what extent is there a strong separation of individuals based on rank? Power distance tends to be particularly high in Arab countries and some Latin American ones, while it is more modest in Northern Europe and the U.S.\n- Masculinity vs. femininity involves a somewhat more nebulous concept. “Masculine” values involve competition and “conquering” nature by means such as large construction projects, while “feminine” values involve harmony and environmental protection. Japan is one of the more masculine countries, while the Netherlands rank relatively low. The U.S. is close to the middle, slightly toward the masculine side. ( The fact that these values are thought of as “masculine” or “feminine” does not mean that they are consistently held by members of each respective gender—there are very large “within-group” differences. There is, however, often a large correlation of these cultural values with the status of women.)\n- Uncertainty avoidance involves the extent to which a “structured” situation with clear rules is preferred to a more ambiguous one; in general, countries with lower uncertainty avoidance tend to be more tolerant of risk. Japan ranks very high. Few countries are very low in any absolute sense, but relatively speaking, Britain and Hong Kong are lower, and the U.S. is in the lower range of the distribution.\nAlthough Hofstede’s original work did not address this, a fifth dimension of long term vs. short term orientation has been proposed. In the U.S., managers like to see quick results, while Japanese managers are known for take a long term view, often accepting long periods before profitability is obtained.\nHigh vs. low context cultures: In some cultures, “what you see is what you get”—the speaker is expected to make his or her points clear and limit ambiguity. This is the case in the U.S.—if you have something on your mind, you are expected to say it directly, subject to some reasonable standards of diplomacy. In Japan, in contrast, facial expressions and what is not said may be an important clue to understanding a speaker’s meaning. Thus, it may be very difficult for Japanese speakers to understand another’s written communication. The nature of languages may exacerbate this phenomenon—while the German language is very precise, Chinese lacks many grammatical features, and the meaning of words may be somewhat less precise. English ranks somewhere in the middle of this continuum.\nEthnocentrism and the self-reference criterion. The self-reference criterion refers to the tendency of individuals, often unconsciously, to use the standards of one’s own culture to evaluate others. For example, Americans may perceive more traditional societies to be “backward” and “unmotivated” because they fail to adopt new technologies or social customs, seeking instead to preserve traditional values. In the 1960s, a supposedly well read American psychology professor referred to India’s culture of “sick” because, despite severe food shortages, the Hindu religion did not allow the eating of cows. The psychologist expressed disgust that the cows were allowed to roam free in villages, although it turns out that they provided valuable functions by offering milk and fertilizing fields. Ethnocentrism is the tendency to view one’s culture to be superior to others. The important thing here is to consider how these biases may come in the way in dealing with members of other cultures.\nIt should be noted that there is a tendency of outsiders to a culture to overstate the similarity of members of that culture to each other. In the United States, we are well aware that there is a great deal of heterogeneity within our culture; however, we often underestimate the diversity within other cultures. For example, in Latin America, there are great differences between people who live in coastal and mountainous areas; there are also great differences between social classes.\nLanguage issues. Language is an important element of culture. It should be realized that regional differences may be subtle. For example, one word may mean one thing in one Latin American country, but something off-color in another. It should also be kept in mind that much information is carried in non-verbal communication. In some cultures, we nod to signify “yes” and shake our heads to signify “no;” in other cultures, the practice is reversed. Within the context of language:\n- There are often large variations in regional dialects of a given language. The differences between U.S., Australian, and British English are actually modest compared to differences between dialects of Spanish and German.\n- Idioms involve “figures of speech” that may not be used, literally translated, in other languages. For example, baseball is a predominantly North and South American sport, so the notion of “in the ball park” makes sense here, but the term does not carry the same meaning in cultures where the sport is less popular.\n- Neologisms involve terms that have come into language relatively recently as technology or society involved. With the proliferation of computer technology, for example, the idea of an “add-on” became widely known. It may take longer for such terms to “diffuse” into other regions of the world. In parts of the World where English is heavily studied in schools, the emphasis is often on grammar and traditional language rather than on current terminology, so neologisms have a wide potential not to be understood.\n- Slang exists within most languages. Again, regional variations are common and not all people in a region where slang is used will necessarily understand this. There are often significant generation gaps in the use of slang.\nWriting patterns, or the socially accepted ways of writing, will differs significantly between cultures.\nIn English and Northern European languages, there is an emphasis on organization and conciseness. Here, a point is made by building up to it through background. An introduction will often foreshadow what is to be said. In Romance languages such as Spanish, French, and Portuguese, this style is often considered “boring” and “inelegant.” Detours are expected and are considered a sign of class, not of poor organization. In Asian languages, there is often a great deal of circularity. Because of concerns about potential loss of face, opinions may not be expressed directly. Instead, speakers may hint at ideas or indicate what others have said, waiting for feedback from the other speaker before committing to a point of view.\nBecause of differences in values, assumptions, and language structure, it is not possible to meaningfully translate “word-for-word” from one language to another. A translator must keep “unspoken understandings” and assumptions in mind in translating. The intended meaning of a word may also differ from its literal translation. For example, the Japanese word hai is literally translated as “yes.” To Americans, that would imply “Yes, I agree.” To the Japanese speaker, however, the word may mean “Yes, I hear what you are saying” (without any agreement expressed) or even “Yes, I hear you are saying something even though I am not sure exactly what you are saying.”\nDifferences in cultural values result in different preferred methods of speech. In American English, where the individual is assumed to be more in control of his or her destiny than is the case in many other cultures, there is a preference for the “active” tense (e.g., “I wrote the marketing plan”) as opposed to the passive (e.g., “The marketing plan was written by me.”)\nBecause of the potential for misunderstandings in translations, it is dangerous to rely on a translation from one language to another made by one person. In the “decentering” method, multiple translators are used.\nThe text is first translated by one translator—say, from German to Mandarin Chinese. A second translator, who does not know what the original German text said, will then translate back to German from Mandarin Chinese translation. The text is then compared. If the meaning is not similar, a third translator, keeping in mind this feedback, will then translate from German to Mandarin. The process is continued until the translated meaning appears to be satisfactory.\nDifferent perspectives exist in different cultures on several issues; e.g.:\n- Monochronic cultures tend to value precise scheduling and doing one thing at a time; in polychronic cultures, in contrast, promptness is valued less, and multiple tasks may be performed simultaneously. (See text for more detail).\n- Space is perceived differently. Americans will feel crowded where people from more densely populated countries will be comfortable.\n- Symbols differ in meaning. For example, while white symbols purity in the U.S., it is a symbol of death in China. Colors that are considered masculine and feminine also differ by culture.\n- Americans have a lot of quite shallow friends toward whom little obligation is felt; people in European and some Asian cultures have fewer, but more significant friends. For example, one Ph.D. student from India, with limited income, felt obligated to try buy an airline ticket for a friend to go back to India when a relative had died.\n- In the U.S. and much of Europe, agreements are typically rather precise and contractual in nature; in Asia, there is a greater tendency to settle issues as they come up. As a result, building a relationship of trust is more important in Asia, since you must be able to count on your partner being reasonable.\n- In terms of etiquette, some cultures have more rigid procedures than others. In some countries, for example, there are explicit standards as to how a gift should be presented. In some cultures, gifts should be presented in private to avoid embarrassing the recipient; in others, the gift should be made publicly to ensure that no perception of secret bribery could be made.']	['<urn:uuid:a339b028-9f12-4531-8ef8-6dcff21dcfa3>', '<urn:uuid:cbfb9446-7b6c-4c2c-9d6f-c11d52280bc4>']	open-ended	direct	verbose-and-natural	similar-to-document	comparison	novice	2025-05-12T21:52:51.670698	18	112	4673
82	nature spots riverside illinois birds	The Riverside Des Plaines River Corridor is an important bird area that serves as a critical spring and fall stopover site for migratory birds. The area hosts various species including Wood Ducks, Cooper's Hawks, Great Horned Owls, Chickadees, Nuthatches, Baltimore Orioles, Blue-gray Gnatcatchers, Indigo Buntings, five species of woodpeckers including Red-headed Woodpeckers, Eastern Wood-pewees, Great Crested Flycatchers, and various Warblers.	"[""The Riverside Des Plaines River Corridor Important Bird Area is a 3.92 km contiguous path along Des Plaines River in Riverside, IL, from the Riverside Swim Club on Bloomingbank Road to Miller Road/Ogden Avenue. It is across the river from and surrounded by the existing 107 hectare Salt Creek Woods Nature Preserve - Bob Mann Woods IBA.\nIn 1970, the Village of Riverside was designated a National Historic Landmark Architectural District by the National Park Service based on its natural landscape and design by Frederick Law Olmsted. Riverside is also noted as one of the first suburban planned developments in America, dating to 1869. During the design and planning process, Olmsted recognized and was inspired by the beauty of the surrounding landscape. The design of the roadways and parks system are curvilinear and created to mimic the natural contour of the s-curve of the Des Plaines River, which bisects Riverside. Based on Olmsted’s vision and design, Riverside is arguably the first conservation designed community in America with over 50% green areas including the river’s natural landscape interconnected with parkways and open “commons” of its urban forest, a walkable downtown, and train station connecting the village to Chicago’s metro center.\nConsistent with Olmsted’s design preferences, the tree and shrub collections have a strong emphasis on native plantings found along the Des Plaines River. The corporate limits of Riverside are approximately 517 hectares, consisting of both public lands and private property. Public lands occupy approximately forty-four percent (44%) of the total community's land. The living collections consist of 8,400 woody plants on public right-of-way. Of the 107 woody plant species documented in Riverside, about 70% are canopy, 25% understory, and the balance are shrubs. These include bur oak, swamp white oak, red oak, hickory and other hardwoods, which are important to migratory birds.\nThe Village of Riverside is committed to operating and maintaining these public lands to preserve the planning concepts upon which the general character of the village and the historic landmark designation are based. Portions of the nominated IBA, such as the Scout Cabin and the Indian Gardens ballparks and tennis courts, are public parks and recreation areas under the jurisdiction, operation, maintenance, and control of the Riverside Parks and Recreation Board.\nSection 1 (Riverside Swim Club to the Scout Cabin): The area is narrow and contains a mix of flowering shrub and tree species such as honeysuckle, hackberry, linden, elm and ash. There are pre-settlement bur, black, and white oaks in the turf areas. Sub-canopy trees such as ironwood, blue beech, and hawthorn are present as well as gray dogwood and eastern wahoo shrub species.\nSection 2 (Scout Cabin to the Barrypoint Bridge): The forested portion is the largest tract of natural area in Riverside, offering habitat for wildlife. Canopy species such as elm, ash, hackberry, bur oak, red oak, swamp white oak, hickory and linden are present with a hawthorn sub-canopy. Bladdernut shrubs have been observed along with several sub-canopy invasive species such as buckthorn and honeysuckle. From an ecological perspective, this area was protected from fire by the Des Plaines River and receives periodic over-the-bank flooding. Due to the emerald ash borer infestation and Dutch elm disease, a significant amount of canopy has been lost along the path behind Indian Gardens, creating opportunity for the reintroduction of natural regeneration due to gaps created in the canopy. Some ash and elm have been allowed to remain to provide wildlife habitat while others have been removed for public safety. This area contains high quality woodland perennials such as bloodroot, Virginia bluebells, wild ginger, trillium, jack-in-the-pulpit, and Solomon’s seal. Sedge meadows are present in this section, with a wetland sedge meadow directly north and east of the Scout Cabin and just south of the tennis courts.\nSection 3 (Barrypoint Bridge to the Riverside Public Library): Basswood, hackberry, ash, and elm are all present with ironwood, blue beech, and Ohio buckeye in the sub-canopy layer. Few shrubs are present in this section. An upland forest frames Swan Pond, creating a room-like effect for the floodplain area. Remnant pre-settlement oak trees border the turf, highlighting the potential transition from upland forest to oak savanna and eventual prairie prior to development.\nSection 4 (Riverside Public Library to Ogden Avenue): The pre-settlement vegetation present on site would suggest this area is remnant oak-hickory savanna. Both shagbark hickory and bur oak are the dominant canopy species. Linden, hackberry, and elm have also colonized the site in the absence of fire. Sub-canopy species such as ironwood and grey dogwood have also been observed. The steep slope supports both wet conditions near the riverbank and dry conditions at the top of turf grass areas, allowing for a wide range of plant species to colonize the site. The entire Village of Riverside was certified as an arboretum by ArbNet in July 2015. Qualifications include having at least 100 species of trees in the village’s public collection, a paid forester, educational programming related to the collection, public events in the village’s forested areas, and a collections policy. Riverside is the second municipality in Illinois to be named an arboretum.\nThe Riverside Des Plaines River Corridor is part of the inland migratory pathway along the Des Plaines River corridor, including the confluence of the Salt Creek, making it an important feeding area and shelter for migratory birds. Beyond being a critical spring and fall stopover site, the IBA is significant for both resident and migratory birds, with nesting Wood Ducks, Cooper’s Hawks, Great Horned Owls, Chickadees and Nuthatches and woodpeckers, as well as Baltimore Orioles, Blue-gray Gnatcatchers, Indigo Buntings, five species of woodpeckers including Red-headed Woodpeckers, Eastern Wood-pewees, Great Crested Flycatchers, and a diversity of Warblers.\nThe Village of Riverside owns the land along the east side of the Des Plaines River and it is managed by the Riverside Department of Pubic Works.""]"	['<urn:uuid:c03d60db-2e40-45f1-b672-b07812897539>']	open-ended	with-premise	short-search-query	distant-from-document	single-doc	novice	2025-05-12T21:52:51.670698	5	60	973
83	direct cremation no funeral service increase popularity after which famous person chose this	Interest in direct cremation, which involves no funeral service at all, has increased after David Bowie chose this option for himself.	['Traditionally, the “Day of the Dead” is a time when family and friends gather to remember friends and family members who have died. But it’s also a perfect time to think about your own death – if you dare.\nMost people won’t go there because it’s either too scary, too painful, or we just pretend it will never happen to us. But ready or not, there’s a 100% certainty death will eventually get up close and personal with every one of us. Here, from MORTALLS – The Death-Positive Conversation Game, are 10 mind-blowing but worthwhile questions to ponder about your own death… if you are willing to do so:\nWhat scares you the most about dying?\nWhat are some not-so-scary things about dying that don’t really bother you?\nWhen you die, would you rather be awake and aware, or sedated and oblivious? Why?\nWhat would be the best thing about a quick death? The worst thing?\nWhat do you think you will regret the most when you are dying?\nWhat is something you want to be remembered for?\nWhat is something you do not want to be remembered for?\nWho will carry out your final wishes when you die? How will they know what those wishes are?\nDoes the certainty that you are going to die affect the way you live your life right now? If so, how? And if not, why not?\nWhat three words would you use to describe your own attitude about death and dying?\n[J]ust in time for the prayers for the dead on All Souls’ Day next Wednesday, the Vatican has restated its position on what can be done with the ashes of the faithful. In short, no longer can Auntie be kept in a mantelpiece urn or grandad’s ashes scattered on his local team’s football pitch.\nConcerned about the adoption of “new ideas contrary to the church’s faith” suggestive of “pantheism, naturalism or nihilism”, the Vatican document conflates ashes-scattering with a dangerously new age spirituality, stipulating instead that remains should be kept tangibly in a sacred place. The Catholic belief in bodily resurrection at the end of days makes this position unsurprising, and the church clearly has a vested interest in discouraging casual rituals outside their control, but it’s a proscription that doesn’t sit well with current trends in the UK. The Vatican may face a harder battle against creeping modernism in the matter of burial and funeral practices than they bargained for.\nBurial space in the UK is at a premium. The Labour government’s 2007 plan to allow the reuse of graves was given the green light in London, but the toxicity of the topic has seen it languish “under review” ever since for the rest of England and Wales. A Scottish bill to permit such recycling was passed in March. But such measures won’t make a significant dent in the 75% cremation rate, and the scattering of ashes is still a huge trend – the Mountaineering Council of Scotland warns that the sheer volume of ashes on the most popular summits is such that it is causing dangerous chemical changes in the soil.\nThe Vatican rejects the idea of death as “the moment of fusion with Mother Nature or the universe, or as a stage in the cycle of regeneration” that scattering in such natural environments represents; it also bans the use of ashes in memorial trinkets. In recent years, ashes have been used to make everything from records to tattoo ink, and such gung-ho going-ons have become associated with rock’n’roll abandon, from Keith Richards snorting his father’s remains, to the metal fan whose ashes were scattered in the mosh pit earlier this year. US experimental act Negativland went so far as to issue their new album this month with a small bag of the ashes of band member Don Joyce. Irreverent stuff, but the modern history of cremation in the UK started in no less paganistic style, with the failed prosecution of druid William Price for burning the body of his baby son on a pyre in 1884, setting a legal precedent that saw the practice legalised in 1902.\nBut cremation may not be where the individualism and valorisation of the natural world the church so fears is really thriving. Alternative trends in the disposal of bodies are moving towards burial. The Association of Natural Burial Grounds (ANBG) represents more than 270 woodlands and meadows run as natural cemeteries in the UK; 20 years ago there was only one such facility. It is in natural burial that the idea of an unmediated return to the earth that the church has denounced is writ large, with bodies often buried without a coffin and the landscape managed sustainably to preserve its natural beauty.\nRosie Inman-Cook, head of the ANBG and of the Natural Death Centre (NDC), a charity that puts choice, family and respect for the environment at the centre of their funeral advice service, has written inspiringly about the wide range of funeral and burial options available in the UK today. In the words of Leedam Natural Heritage, which operates eight natural burial grounds, these alternatives “offer something gentler”. Indeed, this is all in a context of the rejection of the staid funerals of old, which belonged to a more emotionally buttoned-up past, with British Humanist Association-trained celebrants now conducting more than 7,000 funerals a year.\nBut more and more people are doing away with formal ceremony and professional celebrant altogether, instead taking the “direct-it-yourself” approach championed by Inman-Cook, or going for direct cremation, which involves no funeral at all. The fact that David Bowie chose this option cemented his image as the ultimate individualist, and the NDC has reported a rise in interest in this possibility.\nWith adherence to a faith’s doctrines always being on a sliding scale, and the Catholic faithful hardly being immune to changing fashions, the church perceives these new approaches to marking the end of our lives as a threat. But if they are worried about greater freedom and a more individualistic approach to death and burial, scattering of ashes is old news.']	['<urn:uuid:ce22c22e-17ec-4dfc-a69a-ca29509113ba>']	factoid	with-premise	long-search-query	similar-to-document	single-doc	expert	2025-05-12T21:52:51.670698	13	21	1016
84	I'm new to photography and want to take close-up shots of flowers in my garden. What are three key things I should keep in mind about aperture, flash, and distance when taking these photos?	When taking close-up flower photos, you should consider: 1) Aperture settings - using a wider aperture like f/2.8 will blur the background nicely while keeping the flower sharp, though you may want f/4 to keep more of the flower in focus. 2) Fill flash can be helpful outdoors to eliminate shadows and add sparkle to your photos - look for the lightning bolt icon on your camera to enable it. 3) While getting close to your subject is important, closer isn't always better - you need to find the right balance of distance that produces a pleasing picture while maintaining important compositional elements.	['travel & recreation\nselect it via a scene mode). DSLR users can use special macro lenses, which are made for close-up photography, as well as inexpensive yet high-quality close-up diopter lenses (they screw onto the threads of the lens, just like a protective filter). The key to good close-ups is keeping the camera and subject still; a tripod or even a beanbag or pillow placed on a sturdy surface can work. In many cases, you can also zoom in to capture close-up details—as long as the camera can keep the image in focus, you’re good to go. Another tip is to add a clear filter (often called a sky filter) to your camera’s front filter thread and apply petroleum jelly or a similar product very lightly to create a painterly and reamy look, as seen in the flower photo (below center). To clean the filter after use, just rinse it with water under a faucet while rubbing it with a very smooth cloth (a 100 percent cotton T-shirt or similar fabric cloth works well). Then wipe it dry with a clean, smooth cloth.\nBy Andrew Darlow HAVING SEEN SO MANY inspiring pho- tographs over the years in The Costco Connection’s annual photo contest, I know many talented shutterbugs are out there. Whether you are one of them or aspire to be, the following tips are meant to help you better capture all of summer’s sights and activities—no matter if it’s with a fancy digi- tal SLR (DSLR) camera, a point-and-shoot or even a camera phone.\nCapturing the best\nshot takes focus\ncan occur when shadows are cast under the\neyes because the sun is high in the sky. “Fill\nflash” describes the use of flash to fill in\nunderexposed areas. It can be enabled in several ways: by using the small flash found on\nmany point-and-shoot cameras; by using a\ncamera’s pop-up flash; with a detachable flash\nunit (e.g., Speedlites, made by Canon, or\nSpeedlights, made by Nikon); or with an off-camera flash unit.\nThe icon used by most point-and-shoot\ncameras to turn on fill flash is a lightning\nbolt—press the flash button until the lightning bolt stays on. Also, fill flash can add\n“catch lights” to your subjects’ eyes, adding\nsparkle, and illuminate features in a way similar to how pros light models for magazine\ncovers. At night, most cameras automatically\nfire the flash, but to record some of the background nicely, use the “Night Portrait” mode\nif your camera has that option.\nChange your perspective\nIf you get very close and really low to the\nground, you can create very different (and\noften very dramatic) looks. I took this photo o i h\nZoom in on creativity\nWhether your camera has a built-in\nzoom lens or an interchangeable one , the way\nyou use the zoom can make for much more\ninteresting images. To start, consider taking\nphotos of a subject at different zoom levels.\nThe differences in framing and impact can be\namazing. Also, try zooming in to photograph\npeople, instead of using the default wide-angle settings. To do that, just zoom in a bit\n(for more advanced users, aim for about 80\nmm in 35 mm film terms) and, if necessary,\nmove back a few feet. You should notice that\npeople’s features (especially faces) will appear\nless distorted in your photos and more like\nthe way you see them with your eyes from a\nfew feet away.\nFlash in the daytime\nElectronic flash has many uses for pictures taken outdoors. For starters, flash can\nreduce or eliminate “raccoon eyes,” a look that\nUp close and personal\nClose-up photography opens up so many\nopportunities for creative photographs. Many\ncompact cameras have a “Macro” mode (look\nfor the icon of a flower on one of the dials, or\nThe Costco Connection\nCostco and Costco.com offer compact\nand DSLR cameras and accessories plus\na variety of printing options at 1-Hour\nPhoto or online at Costco.com.\nMAY 2010 ;e Costco Connection 61\nof the Eiffel Tower just after midnight, and I\nhad to lie on the ground to capture the image.\nThis technique can also be used when photographing your friends or family. Just lie on\nyour back, aim your camera up and ask everyone to carefully huddle around you.\nIncrease your exposure\nLong-exposure photography can create\nspectacular results. On compact cameras without “Manual” modes, you can try using the\n“Night Scene” mode or “Fireworks” mode—\nboth should make the shutter speed longer\nPHOTOS BY ANDREW DARLOW PHOTOGRAPHY', 'In this article, I will demonstrate how critically important three specific elements are when doing close-up photography. Aperture, background, and the general principles of composition will make or break your close-up photos, and learning how to work with these parameters will dramatically improve your pictures.\nLet’s look at a step-by-step walkthrough of the process.\nStep #1 – Choose your gear\nThe first step in taking a close-up photo is to make sure you have the right gear. Most lenses have a minimum focusing distance that isn’t close enough for macro-style shots, and as a result, do not work the best for this type of photography.\nYou can either get a true macro lens or, for a much smaller investment, a set of close-up filters that screw onto your lens. A macro lens will give you maximum sharpness, flexibility, and color fidelity but it will also cost hundreds of dollars.\nClose-up filters are far more limited but are less than 10% of the cost of a macro lens. So it’s a great option if you’re just getting started with this type of photography. Buy a set of filters for your lens that has the largest filter size (look at the back of your lens cap for the size you need), then get inexpensive step-up rings to adapt them to fit your other lenses.\nFor example: If you have several lenses and one takes a 58mm filter, another uses a 67mm and your last one is 77mm – buy a set that is 77mm. Then get one set of step-up adapters to make them fit onto the other ones as well.\nYou may need one or more to attach your favorite filter to your lens but a set like this is very inexpensive and handy to have in your bag. The set above is only $16. For a full list of recommended gear you can use to do macro photography, including close-up filters: CLICK HERE.\nNOTE: Another option that provides great image quality and a lower cost is extension tubes. Read: How to Use Extension Tubes for Macro Photography.\nStep #2 – Choose a subject\nAfter selecting your gear the next thing you need to do is choose a subject. I like to use the natural world, especially flowers, because of their bright colors and beautiful contrast but nearly anything around you will suffice when you examine it up close.\nHere you can see a yellow coneflower on a rainy, overcast day. One of my favorite conditions for taking pictures. This is a great subject for close-up photography because the flower itself is actually quite large so you don’t even need to be especially close for a good shot. Still, a macro lens or set of close-up filters will definitely come in handy.\nRefine the image\nAfter choosing your subject, it’s time to start thinking about the aperture, the background, and other compositional elements.\nHere you can see the effect that the aperture had on this flower. This first picture (below) was shot at f/4 and the background is slightly blurry, almost like a portrait effect that you might find on a mobile phone.\nThe image is decent but could be significantly improved by adjusting the aperture. Increasing the opening all the way to f/1.8 results in a much better image where the background is much blurrier and, as a result, the viewer’s attention goes directly to the flower.\nThe image above is much better, but getting closer can work wonders too. It’s also important to think beyond blur and look at other compositional elements as well.\nThe background in the picture above is boring and uninteresting, with muted browns and greens that add a sense of context but not much else. Watch what happens when I use a +4 close-up filter to get even closer to the flower while also adjusting the composition of the image.\nNow we’re getting somewhere! Besides the physical distance to the subject, there are some important elements in this shot that help to elevate it above the earlier ones.\n- The yellow flower is now framed between two vertical stems on either side, which helps the flower stand out against the other elements of the image.\n- The bright yellows of other flowers now stand out against the dark greens, whereas before they were mixed in with the brown earthy tones.\n- Best of all, the light coming through the trees creates a brilliant pop of light behind the flower which adds a sense of energy and excitement to the image.\nAll of these are intentional choices that show how important these compositional elements are when photographing subjects up close. But we’re not done yet! Some additional tweaks will make this photo even better.\nWidening the aperture from f/4 to f/2.8 nearly obliterates the background, but still leaves the key compositional elements visible. You could go even wider depending on your lens, but at this point you start to get diminishing returns. Shooting close-up photos at ultra-wide apertures results in depth of field that is far too shallow for most practical situations, and you can quickly lose all sense of place and context in your images.\nJust for fun let’s go even closer to see what happens. Here is the same flower shot with a +10 close-up filter and an aperture of f/4.\nThis picture is interesting but it has now lost some of the elements that made the earlier version so compelling. Gone are the vertical stems for framing the subject, and the brilliant mass of blurry white light is not nearly as interesting. While you can see some details on the flower that were difficult to make out before, it’s also not as sharp.\nThis illustrates a trap that you can easily fall for: closer isn’t always better. Just because you can take pictures with mere centimeters separating your subject and your camera doesn’t mean you should. You have to find a balance that produces a pleasing picture, even if it means backing off just a bit.\nHere’s another example of how aperture, background, and composition elements affect close-up photography. This is an iris flower shot with a 50mm lens at f/2.8.\nThere is so much going on in this photo that the subject is barely discernible from all the other elements. You can see the flower in the center but there are other flowers just to the right, a weird grown mass on the left, and a bright blue sky drawing your eye away from the main subject.\nFrom a compositional standpoint, this image is a mess! Getting closer will help, but the distance to the subject is not the only thing to consider.\nMoving toward the flower with the assistance of a +4 close-up filter helps to isolate the subject, but it’s not enough to create a good photo. Look at the background. It’s pleasantly blurry, but it’s also full of purple spots from other irises.\nAt first glance, it’s still somewhat difficult to separate the main iris from the purple elements in the background, especially on a small screen like a mobile phone. But there’s also another problem. That vertical white post creating a line on the right side of the flower. It’s artificial and unnatural and serves as a distraction (unlike the green flower stems in the coneflower example).\nSo if you want to get good results in conditions like this it’s not enough to just get closer to your subject. You have to look at other things in the frame as well.\nThis final image is much better, but not just because I got even closer to the flower. I used a +10 close-up filter but that’s not all I did to improve the shot. The other purple flowers are still present in the background, but I adjusted the camera position and angle of view such that they are not distracting from the main subject. I also chose a shooting angle that eliminated the white beam.\nFinally, I kept my aperture relatively large at f/2.8, which created a nice combination of background blur and subject sharpness. In retrospect, f/4 might have been preferable since it would have kept the very top of the flower in focus as well, but f/2.8 gave me a result that I am perfectly happy sharing.\nLook close to home\nYou don’t have to go to far-off exotic locations to practice these principles, you can often find things to photograph literally in your own backyard.\nThe image below is of a salvia plant that is along the fence at the edge of my property. Every year it produces brilliant violet flowers that attract all manner of bees, butterflies, and hummingbirds. It’s a great opportunity for a photograph, but capturing the essence of this plant requires some thought, preparation, and practice.\nGetting a good picture of this member of the mint family takes more than just pointing your camera and clicking the shutter button. Look at the difference between the image above and the one below.\nBoth are the same plant, in the same location, but the one below was shot much closer with a better eye for composition. I also used a +4 close-up filter in order to close the gap between my lens and the flowers and to create a nice blurry background too.\nThe myriad of improvements go a long way toward getting a much better picture, but I’m not finished yet. The plant is relatively sharp with the depth of field mostly under control, but some things could still be better.\nThe background looks nice but there are a few simple, easy steps that could dramatically improve the shot. Moving even closer with a +10 close-filter helps, but doesn’t solve everything.\nNow the flower is much more prominent and isolated in the frame. But when you’re shooting at close range like this, you have to take into account everything in the frame, not just your subject. Look at how the white spots of light, coming from the sun poking through my neighbor’s tree, sit high above the flower and leave an empty yellow gap that feels strange and unfinished.\nThis final image is markedly better in virtually every way possible.\nThe flower itself is sharp, the background elements complement the subject and do not distract the viewer, and the gap between the flower and the spots of white light has been minimized. It looks like the purple flowers are being cradled by the bright bokeh spots, and the whole image feels much more cohesive and complete.\nBut this image was no accident! It took me 43 attempts to get this shot! If you are shooting photos of close subjects, you will need every principle discussed in this article along with a great deal of patience as well.\nPay attention to the background\nThis final example shows how one simple trick can have an enormous impact on your close-up images. It’s related to many things I have already covered but shows a simple, clear example of the importance of paying attention to the background.\nThese chamomile flowers are beautiful but very small, which means you have to get close to get a good shot.\nBut remember to also consider both the aperture and background! Use an aperture that blurs the background and isolates your subject, and select a background that gives you the right mood and feeling for the shot.\nFor this shot, I used a +4 close-up filter on my 50mm lens and shot with an aperture of f/4. That was a great combination for isolating one single flower but something isn’t quite right here. Brick pavement fills the background and makes the shot feel strange, distant, and otherworldly as opposed to warm and comforting.\nFixing this takes fewer than five seconds and doesn’t require any special gear or photographic trickery.\nI simply scooted over so the background was green instead of dull orange, and took a picture of the exact same flower. It’s an incredibly simple solution but the difference is enormous. The single flower now feels much more natural, normal, and comforting to look at.\nTaking pictures at close distances doesn’t have to be difficult, but getting good shots does require a keen eye for detail as well as lots of patience and practice.\nLook closely at the background, consider the aperture you are using, and keep your eyes open for any elements in your shot that you can use to enhance your subject. As well, watch for things that might detract from the quality of your photos but can be easily fixed.\nYour pictures will be much better by keeping these simple things in mind. Please share some of your close-up photos in the comment area below and if you have any questions, please ask.']	['<urn:uuid:7877203d-9aa0-4062-ac6d-d255c2585ecd>', '<urn:uuid:c419bd4d-ff18-42f7-a82c-143718fafe7a>']	factoid	with-premise	verbose-and-natural	similar-to-document	three-doc	novice	2025-05-12T21:52:51.670698	34	103	2874
85	How do both the Tokyo National Stadium and modern wind farms balance their massive structural requirements with environmental considerations in their construction?	The Tokyo National Stadium combines steel and timber in its roof structure, creating a 20,000-ton canopy while maintaining environmental sustainability through the use of 70,000 m3 of certified timber. Similarly, wind farms address environmental impact by carefully selecting locations away from Important Bird Areas and experimenting with structural modifications like larger turbine heights and shorter blades to reduce wildlife impacts while maintaining power generation efficiency.	"[""New stadium: Here comes Tokyo's timber stadium\nsource: StadiumDB.com; author: michał\nAlthough we didn't manage to get inside during our visit to Tokyo, finally the new national stadium joins StadiumDB as one of the largest candidates to become Stadium of the Year.\nConstruction of a new national stadium in Tokyo was officially on the agenda since 2008, due to ongoing deterioration of the historical Olympic Stadium of 1964. The successor was initially considered as a football-specific stadium. However, following the fiasco of Tokyo's 2016 Olympics bid, the city went for one more attempt, this time for 2020. That bid, later proven successful, became crucial for the stadium's future.\nThe plan became massively impressive, envisioning a stadium of 80,000 seats with retractable roof and air conditioning. International design competition brought 46 entries from around the world and Zaha Hadid's vision was selected as the best one. Her concept suggested basing the futuristic shape on two 370-meter arches.\nWhile initial estimates suggested it would be built for some ¥130 billion but this quickly proved far too optimistic. Already in 2013 it was decided that floor space should fall by nearly 70,000 m2 (from 290,000 to 220,500 m2). Later further cuts included the moving roof and air conditioning, even dropping capacity to 60,000. And even then Hadid's concept was evaluated to cost massive ¥300 billion ($2.7b) and over a trillion by the end of its life cycle.\nFor this dramatic reason in July of 2015 the design was scrapped entirely and a new call for entries was issued, this time from joint ventures ready to design and build the stadium – a procedure selected for its time efficiency. By December of that year the bid by Taisei + Kengo Kuma + Azusa Sekkei was selected.\nAlong new guidelines issues by the Japan Sport Council it was proposed that the stadium would have a significantly reduced scale, with height not exceeding 50 meters (final figure 47.4m). Still, with 194,000 m2 of floor space, width of nearly 260 and length of some 330 meters it's the largest stadium across Japan in terms of size.\nThe selected concept is characterised by harmony and simplicity and was marketed as one welcoming residents throughout the week as a leisure destination. Opaque facade was scrapped in favour of open concourses with shrubs and trees planted on their edges. Significant plantings were also envisioned around the stadium.\nAll concourses extend outwards and are equipped with eaves, emulating a pagoda-styled temple. Each of the eaves is covered with thin timber latticework, making it seem more natural and cosy. Lack of a more built-up cladding ensures good ventilation and if there's no wind the stadium has 185 fans to create air movement.\nWood follows fans inside the stadium, covering parts of the inner concourses and hospitality areas. But by far the most interesting use of timber is in the roof. While timber truss or frame structures are no rarity, this is the largest one worldwide to combine steel and timber. Weighing 20,000 tons, the canopy is indeed massive, even though its surface of roughly 45,000 m2 (with entire footprint of 61,000 m2) isn't among the world's largest.\nIn total the Japanese contractors have used 70,000 m3 of timber, mostly cedar and larch. Interestingly, the material has its own symbolism, having been sourced from certified locations across all 47 prefectures of Japan. This way the nationwide effort and unifying meaning of the national stadium are acknowledged.\nThe auditorium has been created in a three-tier layout, starting with rake of 20º on the lowermost one, through 29º in the middle, to 34º on top. Two lower levels also offer up to 500 places for wheelchair users, giving them a decent choice of locations. Behind the stands up to 7 floors of facilities are created, with 5-7 floors above ground level. The difference comes from the west side having street level two floors lower than the rest.\nThough selection of the winning bid happened late, within a year the documentation was complete and construction could begin in December of 2016. The deadline of just under 3 years was kept to the day, ending on November 30, 2019. Despite growing prices it was even delivered slightly below the estimated price, at ¥156.9 billion ($1.43 billion upon opening).\nAlong guidelines of the Japan Sport Council the stadium is to enable conversion from athletic to football use by reconfiguration of the lower tier. With seats moved closer to the field it could just exceed 80,000 seats. However, during construction decision was made not to remove the running track as its size would be helpful in organising concerts. Finally, a retractable roof is once more considered in order to weatherproof the stadium in the future."", 'Vultures, eagles, and bats are dealing with an enormous drawback. The very same areas they get pleasure from, the place air currents alongside huge peaks elevate them as they soar, are nice locations for wind farm builders.\n(Photograph : Christian C. Voigt)\nWind generators are a helpful supply of environmentally pleasant, renewable electrical energy. They’re exceeding even pure gasoline as one of many fastest-growing electrical energy sources in america. However, sadly, in addition they kill birds and bats once in a while.\nThis may occasionally look like a Catch-22 state of affairs by way of the surroundings, however it is not. Many American wind farms are experimenting with numerous methods to make their generators safer for flying species, from improved designs and higher placement to high-tech monitoring methods and ultrasonic “increase bins.”\nListed here are 5 methods to maintain bats and birds secure whereas guaranteeing that the wind energy trade thrives:\nDiscovering Safer Areas\n(Photograph : Luo Lei on Unsplash)\nProbably the most easy technique to hold birds and bats away from wind generators is to keep away from putting them in areas the place they’re recognized to fly. Nevertheless it’s not at all times so easy as a result of lots of the huge, treeless stretches that appeal to birds and bats are additionally splendid for wind harvesting.\nIn response to the American Fowl Conservancy, already-altered ecosystems like meals farms supply acceptable turbine websites for wildlife, however any surroundings designated as an “Necessary Fowl Space” needs to be prevented. These embrace wetlands and ridge margins, migratory bottlenecks and flight strains utilized by endangered or lowering species, and migratory bottlenecks and flight pathways utilized by endangered or lowering species.\nUtilizing Totally different Colours\n(Photograph : Getty Photos)\nNearly all of wind generators are painted white or grey to make them as unobtrusive as attainable. Nonetheless, in a 2010 examine, researchers found that white paint can not directly appeal to birds and bats by attracting the flying bugs they search. In response to the examine, white and grey generators had been second solely to yellow generators in attracting bugs akin to flies, moths, butterflies, and beetles.\nPeople are blind to UV radiation, however many different animals, together with bats, can see it. Migrating bats could confuse wind turbine poles for bushes, based on some research. So researchers are taking a look at how low-powered UV lights atop wind generators may warn bats of impending hazard from afar. Earlier than bats method too shut, the lights may promote, “that is horrifying.”\nPutting in Extremely Sonic Audio system\n(Photograph : Wikimedia Commons)\nBirds are principally visible creatures, however the sound could present a way to maintain them away from wind farms as a result of bats journey by way of echolocation. Ultrasonic “increase bins,” which can be mounted to generators and create steady, high-frequency noises between 20 and 100 kilohertz, are based mostly on this idea.\nResearchers said in a 2013 examine that bats’ sonar is nice sufficient to work previous such interference. Increase-box generators killed between 21% and 51% fewer bats than generators that didn’t embrace the function. Nonetheless, the examine’s authors said that a number of technological challenges should be overcome earlier than the methodology might be extensively used.\nUsing New Designs\n(Photograph : Pixabay/pexels.com)\nWind generators’ development may be tweaked to drastically reduce the hurt they trigger to birds and bats, along with recent paint and terrifying lights. Lately, engineers have developed a broad vary of wildlife-friendly designs, starting from minor tweaks to overhauls that scarcely resemble a normal wind turbine.\nResearchers found that the scale of the turbine and the size of the blades may need a major impact on the Vitality Science examine. In response to the examine’s authors, simply placing the generators increased and the blades shorter minimizes the impression on birds. Due to this fact, they suggest that wind-energy coverage ought to promote bigger turbine heights and shorter blades to guard birds along with limiting turbine websites.\n(Photograph : Felice Frankel)\nOregon State College researchers create sensors that may detect when something touches a wind turbine blade, permitting operators to forestall extra collisions by shutting down generators. With such sensors, which researchers are placing to the take a look at by capturing tennis balls at turbine blades, cameras may be fitted on generators to indicate operators if birds or bats are there.\nFor extra animal associated information, remember to comply with Nature World Information!\n© 2021 NatureWorldNews.com All rights reserved. Don’t reproduce with out permission.']"	['<urn:uuid:f7d815e8-2631-4274-880e-7c0b2867a6b6>', '<urn:uuid:31708682-2109-4ed3-826a-14172fac91bc>']	factoid	direct	verbose-and-natural	distant-from-document	multi-aspect	expert	2025-05-12T21:52:51.670698	22	65	1535
86	Is freezing better than humidity control for preservation?	The methods serve different purposes. Humidity control is described as the 'most positive' approach for preventing salt damage in artifacts, but requires precise analysis and constant monitoring. Freezing, on the other hand, is more versatile as it stabilizes paper objects for months, stops dye transfer, ink running, mold growth, and swelling, and can handle large quantities of damaged materials. Freezing is particularly useful during emergency situations when immediate treatment isn't possible.	['Salt can have devastating effects on ancient artefacts. It also can be a tricky process to deal with these salts to protect an artefact over time. Such is the case with one particular process taken on an artefact in the Abbey Museum Collection, the circular plaque of Apollo.\nWe got on a call with Holly Jones-Amin to take us through the process. She is undertaking a PhD at the University of Monash, and is a Senior Conservator at The University of Melbourne where she manages the objects consultancy laboratory. Holly has specialist skills in the treatment of archaeological and ethnographic materials.\nAbbey Museum: How does the story of salts within an artefact begin?\nHolly Jones-Amin: The story can go back to either manufacture or it can go back to use. Some pottery, or some clay objects, are made with the addition of salt. If it’s a utilitarian object, such as a cooking vessel they have some salt present from use and therefore it’s integral to the object’s story. With the tablets they’re never fired, by firing an object you’re closing the pores in the object and therefore you’re making it less porous. The highest degree of pottery not being porous is porcelain, the lowest degree is not being fired at all, and not far from that is something like a 5,000 year old ceramic that’s been fired in a bonfire.\nThe object has either been purposefully put in a tomb, or it’s been purposefully buried, or it’s been discarded.\n“With the roman tablet – there’s cracks and surface loss on the object – and that’s from the salts because they move to the surface and as they get closer to the surface they push harder and harder until you get to the point where it starts flaking.”\n– Holly Jones-Amin\nSo what happens to these artefact underground?\nWhatever goes through the soil goes through the object. Because the object is porous, the water goes through the objects so the object is changed by deposition. By being changed it takes on the salts…the salts themselves are going to be what’s in the ground, they could be nitrates, they could be chlorides, it could be what’s present naturally, it could be added through farming activity.\nWhen the object comes out of that environment, and it could be something that happens immediately or something that happens much further down the track which is the case with the roman tablet, the salts dry out if they’re in a different level of humidity. When they dry out they go back from being something that’s soluble to something that is dry and when it dries it becomes much bigger than it was…it puts a lot of force on the object, and it can cause the surface of the object to come off.\n…You start to lose what’s important to the surface of the object, the decoration, the diagnostic features that make that object so. With the roman tablet – there’s cracks and surface loss on the object – and that’s from the salts because they move to the surface and as they get closer to the surface they push harder and harder until you get to the point where it starts flaking.\nHow do you get rid of these salts?\nThere’s several things you can do – one is that you don’t do anything but control the humidity (in the space) the object’s in and that’s the most positive thing you can do. However, it’s hard to do because you have to understand which salts are present, which means you need analysis and then you need to be keeping the object at that level of humidity – so there needs to be some moisture to stop the salt from drying out.\nMost typically what’s done is that we desalinate it. That could be done one of two ways – either immersing the object in water so that the salts move from the object into the water and you change the water and you measure the level of salt – which is a conductivity reading of the water and you keep doing that until you get to a point where you’re not removing salts anymore, all you’re finding is very little difference from one change of water to the next. Typically you would change the water once every week when you’re doing that (this is the process used in the circular plaque of Apollo).\nAnother way that it could be done when the object is to big or too dense or you’re too concerned about the surface is you can actually do desalination by poulticing the surface – so a little bit like a woman putting a face mask on – so you’re drawing the material into the poultice – the poultice is usually paper pulp.\nCheck out Grimwade Conservation on Instagram @grimwadeconservation\n©Abbey Museum 2020', 'COLLECTIBLES — Column #674 Copyright © Rinker Enterprises, Inc. 1999\nWater Damage with Special Emphasis on Flood Damage – Part III\nThis column is the third of a four-part series on dealing with water damage with special emphasis on flood damage. Part I presented information about Heritage Preservation (www.heritagepreservation.org), “The Emergency Response and Salvage Wheel,” planning ahead, and weathering the immediate emergency. Part II focused on the actions required to clean up a site and the initial steps in recovering water-damaged property. This column deals specifically with water damage recovery techniques relating to paper, everything from books to family photographs.\nBasic Steps That Apply To All Materials\nWhen objects have been heavily water-damaged, there are a few basic steps that apply to all objects whether books, ceramics, glass, furniture, paper, etc.\nEstablish a work area. Ideally, it will be on site. If not, you will have to move objects to it. The site needs plenty of workspace, an adequate water supply, and an indoor drying area that can be humidity controlled. You may have to build the latter.\nWhile an object is still wet, rinse it with clear water. A very fine, low-volume hose spray is ideal. Use soft brushes or damp cloths to clean off silt and debris. Be gentle. Avoid grinding the dirt and silt into the object. Dirt and silt is an abrasive. Over energetic cleaning will scratch an object. Dry the object with a clean, soft cloth or paper towel.\nDo not, repeat, not dry objects in sunlight or other high temperature environments. Drying an object too quickly can cause it to buckle, split, or warp. Air dry objects indoors where you have some control over the temperature and humidity.\nIf you transported the objects from the site to the work area in plastic bags, remove them from the bags as quickly as possible. In fact, do not seal them at all. Keep the bags open to allow air to circulate. These steps are designed to prevent mold from forming.\nThe development of mold and mildew are major concerns. Use these steps to delay or hopefully to prevent their occurrence: (1) after determining which tact is needed, increase airflow using fans, open windows, air conditioners, or dehumidifiers and (2) provide moderate exposure to light (which kills some mildew) by opening window shades or leaving lights on.\nWet paper tears easily. If possible use a plastic or other form of rust proof screen to support paper during the removal and recovery process. Carefully rinse the paper item to remove as much dirt and silt as possible. Be extremely careful with folded paper. It may make more sense to dry the object first before trying to unfold it.\nIf space permits, dry each sheet individually. If space is limited, dry documents in one-quarter inch or less piles, placing absorbent material between the sheets. Remove and replace when the sheets become wet.\nIf the number of documents is large, place a piece of wax or freezer paper between each document, create stacks (ranging between one and three inches), and wrap in freezer paper. Place these stacks supported in an upright position in a plastic or wire container in preparation for freezing. Never pack a container more than ninety percent full. Allow room for expansion due to freezing.\nFreezing stabilizes paper objects for months. It stops dye transfer, ink running, mold growth, and swelling. A sub-zero commercial freezer is the best choice. A home freezer is an adequate second choice. For extremely large paper collections, consider a refrigerated truck.\nFocus your efforts on your most important books. Assign the lowest priority to books that can be easily replaced or have little meaning to you.\nWater-damaged books can be very heavy and fragile. Use both hands when picking up one. Transport books to the recovery site in a plastic crate (milk crates are great) or wire basket. Do not use a paper container, e.g., cardboard box.\nKeep books closed until you are ready to work on them. The goal is to create a slow drying process that allows recovery. Temperature and relative humidity are critical. Work on books only in areas that are dry, cool and comfortable (warm or hot is very bad), and have plenty of air circulation. Never attempt to dry books in an oven or microwave or with a hair dryer or iron.\nDry large volumes flat, smaller volumes upright. Begin by placing sheets of absorbent material (blotters, plain newsprint, paper towels, etc.) between the pages. If the page count is high, you have to do this part of the drying process in several steps. Beware of adding too many sheets and destroying a book’s binding. Change the absorbent material as it becomes wet.\nWhen wet has been reduced to damp, stand the book upright on its driest edge and fan the pages. If you have fans, make certain the book’s spine or binding, not the open pages, is facing the breeze. When the book is dry but still cool to the touch, lay it flat and put a small weight on it. Check it twice a day for mold growth. Remove any mold you find immediately.\nBooks printed on coated paper, e.g., most coffee table and illustrated volumes, need to be treated differently. The coated, smooth, shiny paper will stick together when wet. In order to avoid this, insert a piece of wax paper instead of absorbent paper between each page in the drying process.\nFreeze any books that you cannot air dry within the first two days of your recovery operation. Wrap the books in wax paper and pack them spine down in a sturdy plastic or wire container. Defrost and work on the books as time permits.\nChances are you will not have the time or energy to save every photograph. You have tough decisions to make. Once you make them, do not look back. Remember, saving some photographs is far better than losing them all.\nDamaged photographs for which there are no negatives have the highest priority. If photographs stick together or become moldy, they are extremely difficult, if not impossible, to save. Immediately remove those photographs you wish to save from albums and frames.\nAs with water-damaged books, photographs become fragile when wet. Handle them carefully. Immediately rinse with clean water. Cut wax paper and place it between the photographs. Place the photographs in small stacks, eight to ten per stack, into a Zip-Lock type plastic bag and put them in a freezer to be worked on later. This step allows you to deal with other immediate preservation concerns. When the initial crisis is over, the photographs can be defrosted, separated, and air-dried. Never freeze glass plate negatives.\nIf you have the space and time, you can avoid the freezing process. Dry the photographs face up on a large table, non-rust window screen, or large piece of plastic laid on the ground and covered with absorbent paper if available. Do not dry them in direct sunlight. The photographs will curl as they dry. You can contact a photograph expert later about flattening them.\nFollow the same steps for negatives.\nPart IV, the final installment in this series, completes the steps necessary to recover water-damaged objects by concentrating on ceramics, furniture, glass, metal, and textiles.']	['<urn:uuid:1e6a093d-dd14-4f8a-acc2-65d5999bd43a>', '<urn:uuid:1b21e7ae-e572-4bba-b81d-5f9def877810>']	factoid	direct	concise-and-natural	distant-from-document	comparison	novice	2025-05-12T21:52:51.670698	8	71	2020
87	mars albedo measurement methods limitations	The measurement of Mars' albedo involves infrared spectroscopy techniques like FTIR, but faces two main limitations. From a technical perspective, the instruments have lower sensitivities compared to other equipment like GC-MS. Additionally, aeolian-deposited dust coatings on rock and soil surfaces create a masking effect that impairs wavelengths from reaching the equipment, as observed at Viking, Mars Pathfinder, and Mars Exploration Rover sites. This dust cover gives the illusion of high reflectivity even in volcanic regions like the Tharsis bulge that are actually basaltic in composition.	"['Is it Really Sedimentary?\nIn-Class Activity 2_Life Hosting Rocks & Minerals\nJulia Kahmann-Robinson PhD and Marjorie Chan PhD, University of Utah Department of Geology & Geophysics\nRecognize sedimentary rocks on both Earth and Mars using the albedo effect.\n- Last Chance Canyon, Guadalupe Mountains, NM image in Image File\n- Interactive Earth Surface Albedo Map (NASA-CERES): http://www-surf.larc.nasa.gov/surf/pages/sce_type.html\nPresent the following along with an image of Last Chance Canyon, New Mexico (Figure 1 or see Image File for Life Hosting Rocks (PowerPoint 2007 (.pptx) 22.2MB Jan20 14)):\nScenario: You are planning to hike Last Chance Canyon in the Guadalupe Mountains National Park. It is arid, no winds, and about 95F. If you had the following options for attire which would you choose and why:\n- Sleeveless cotton medium blue colored shirt\n- White long-sleeve cotton shirt\n- Black long-sleeve cotton shirt\nDiscuss student response and their reasoning.\n- Use the interactive Earth Surface Albedo Map (produced by NASA-CERES in Resources) and ask students to interpret the colors for albedo effect.\n- Probe students as to why some ""surfaces"" have a higher albedo than others (ocean, desert, forest cover etc.)\n- Display hand samples of sandstone, andesite, and basalt (or use the Image File for Life Hosting Rocks (PowerPoint 2007 (.pptx) 22.2MB Jan20 14)). Ask students to rank the samples according to their albedo effect.\n- If students were to picture an albedo map of Mars, do they think the surface would be as variable as Earth? Overall, would Mars have a higher albedo than Earth, why or why not?\n- Explain the terms of \'reflectance\' and \'albedo\' using the definitions below. Relate them to the discussion in the Engage section.\n- Albedo- The proportion of the incident light or radiation reflected by a surface, typically that of a planet or moon.\n- In Elaboration use the TES Dust Cover Index layer in JMARS to account for the fact that volcanic regions such as the Tharsis bulge appear to have high reflectivity. While much of the terrain is actually basaltic in composition, dust cover gives the illusion of a highly reflective surface.\nUse the TES Dust Cover Index layer in JMARS to account for the fact that volcanic regions such as the Tharsis bulge appear to have high reflectivity. While much of the terrain is actually basaltic in composition, dust cover gives the illusion of a highly reflective surface.\nExplore TES imagery in JMARS and understand the reflectance.\n- Add the MOLA colorized elevation map for use as context if desired.\n- Add New Layer –> Maps By Instrument –> TES –> TES-Albedo –> View graphic data.\n- Zoom to a window (2 or 4) that allows you to differentiate familiar terrain. You can change the transparency of the TES-Albedo map to see the underlying MOLA colorized map to find major geographic regions of interest.\n- Discuss with your students the results of the albedo map. Is anything surprising to them (i.e. Why are basalt/volcanic regions having high reflectivity? See Explanation)? What could distort the results?\n- Do students find that albedo maps are a good indicator of lithology?\nGo to the ""Kepler: Light Grapher"" web page: http://kepler.nasa.gov/education/ModelsandSimulations/lightgrapher/\nUsing the Light Grapher software and a webcam, measure the amount of light reflected by basalt vs sandstone. This can be done by the following steps:\n- Go to the Kepler: Light Grapher web page and click on ""Run LightGrapher"".\n- Hold the basalt rock in the camera field of view within the circle of LightGrapher.\n- Make sure that the data capture time is set at 30 seconds then click on ""Capture Data"". This will begin measuring the amount of light reflected by the basalt.\n- Keep the basalt in the field of view for ~15 seconds then switch to the sandstone for the next 15 seconds. Make sure that the measurement distance for each rock is the same.\n- This will generate a graph that shows the light detected over the 30 second interval.\n- See how students can correlate this activity with albedo as an analogy.\n- You may use other objects with varying color/reflectivity.\nAn example of results obtained from this procedure can be found below as well as in the module Image File (PowerPoint 2007 (.pptx) 22.2MB Jan20 14)', 'Article updated on 11 November 2020.\nJurik Peter / Shutterstock\nObserving and learning about Mars is incredibly important as it will help us to understand our own planet. Mars and Earth are surprisingly similar in many ways; Mars has wind, clouds, weather, an atmosphere, and dust storms. It also has snowbanks, glaciers and polar ice caps similar to our own. You’ll also find volcanoes, dry river channels, great rift valleys, and sand seas.\nTechnological Advances Help Study Neighboring Planets\nMars also has evidence of the existence of both liquid water and habitable environments. There are also many aspects about the planet that are distinctly different to Earth, and therefore in studying the similarities and differences of Mars, we can make significant conclusions about Earth.\nUp until around 50 years ago, we knew almost nothing about our neighboring planets, but the development of technology to allow us to study them has yielded much valuable information.\nInfrared spectroscopy plays a key role in the continual exploration of Mars. The current strategy of data collection relies heavily on its use to gather data and interpret findings on chemical information. It can answer a number of Mars’s most pressing science questions, it is essential for mineralogical analysis, truth testing of remote sensing measurements, nitrogen compound recognition, and detection of organic compounds.\nAt the beginning of space exploration, back in the 1960s, NASA flew two spacecraft up near Mars. The Mariner 6 and 7 carried infrared (IR) spectrometers, which were able to collect revolutionary findings relating to the composition of Mars’s atmosphere, confirming that there were trace amounts of water vapor and solid CO2. Findings also revealed that solid water and water hydrates existed on the planet’s surface, along with an oxidized oxyhydrate of iron which is significant, as it is evidence of aqueous weathering processes. This was the very first evidence to confirm liquid water had to have existed on the planet.\nFourier Infrared Spectroscopy (FTIR)\nTo this day, infrared spectroscopy is still an important tool used to explore Mars. It is used to investigate both past and present conditions on the planet, to do this it examines the sample depositional environment which can indicate past conditions, as well as exploring the organic matter that currently exists, indicating actual habitation.\nA number of space missions have used a type of infrared spectroscopy called Fourier infrared spectroscopy (FTIR). The benefits of this method are that it is cheap to run as it uses a fast and simple technique that is able to collect an abundance of data on the chemical composition of samples. Mars landers carry FTIR equipment to sense rocks remotely, but the future could see them being used for assessing drilled samples too.\nChallenges of Infrared Spectroscopy\nWhile it is a fundamental component of Mars exploration, infrared spectroscopy does have its challenges. The instruments are heavily relied on because their resource and analytical requirements are few, but they do have lower sensitivities in comparison with other equipment such as GC-MS. In addition, aeolian-deposited dust coatings on rock and soil surfaces results in a masking effect, which can hinder the remote sensing capabilities of infrared by impairing the ability of the wavelengths to reach the equipment through the dust.\nDust coatings occur at land sites of the Viking, Mars Pathfinder, and Mars Exploration Rover, hindering the process of data collection and analysis. In order to overcome this obstacle, more information is needed on the impact of dust coatings on rock surfaces.\nFinally, with the increasing frequency of missions to Mars, data collection methods are being relied on more heavily, but currently, there are not enough experts to process the incoming information. The last hurdle to overcome will be encouraging the growth of a community of spectroscopists worldwide.']"	['<urn:uuid:521fda32-92bb-4eaf-9628-4ad26a0aac90>', '<urn:uuid:a4dc7435-5dd2-4663-9373-ca248a42ea2e>']	factoid	direct	short-search-query	distant-from-document	multi-aspect	expert	2025-05-12T21:52:51.670698	5	85	1333
88	what caused decline powerful port city east africa portuguese arrival impact	Kilwa's decline began with internal dynastic disputes that weakened the city before the Portuguese arrival. In 1505 CE, the Portuguese attacked Kilwa, leaving many buildings in ruins. The Portuguese established control over the Indian Ocean trade through fortresses at strategic points like Sofala and Mozambique Island. As a result, inland traders moved their business to more northerly Swahili ports such as Mombasa. Kilwa faced additional challenges, including attacks from interior tribes. By the 18th century, under French control, Kilwa had transformed into a major port for the East African slave trade and ivory export.	"['Kilwa, an island located off the coast of East Africa in modern-day southern Tanzania, was the most southern of the major Swahili Coast trading cities that dominated goods coming into and out of Africa from and to Arabia, Persia, and India. Kilwa flourished as an independent city-state from the 12th to 15th century CE largely thanks to the great quantity of gold coming from the kingdom of Great Zimbabwe to Kilwa’s southern outpost of Sofala. Kilwa boasted a huge palace complex, a large mosque, and many fine stone buildings at its peak in the 14th century CE. The arrival of the Portuguese in the early 16th century CE spelt the beginning of the end of Kilwa’s independence as trade declined and merchants moved elsewhere.\nThe Swahili Coast\nThe term Swahili Coast refers the stretch of coastline along East Africa from Mogadishu in Somalia at its northern end to the Kilwa in the south. Major ports and towns in between, over 35 in total, included Vrava, Pate, Kismayu, Malindi, Mombassa, Pemba, Zanzibar, and Mafia. In addition to these main sites, there are some 400 other smaller ancient sites dotted along the eastern coastal area. The term Swahili derives from the Arabic word sahil (\'coast\') and so means \'people of the coast\'. It not only refers to the coastal region but also to the language spoken there, a form of Bantu which emerged in the middle of the 1st millennium CE. Later, many Arabic terms were mixed in and Swahili became the lingua franca of East Africa, even if different dialects did develop. The language is still spoken today in East Africa and is the national language of Kenya and Tanzania.\nSwahili coast peoples prospered thanks to agriculture and animal husbandry, aided by a regular annual rainfall and shallow coastal waters plentiful in seafood. Trade, conducted by sailing vessels, first began up and down this coast between the Bantu farming peoples living there in the first centuries of the 1st millennium CE during the region’s Iron Age. Sea travel was aided by the long lines of coral reefs which protect the shallow calmer waters between them and the coastline, as well as coastal islands which provided both shelter and handy stopping-off points en route. In addition, the coast of East Africa provides many natural harbours formed by submerged former river estuaries.\nInitially inhabiting the interior, Bantu people had gradually moved to the coast as the second half of the first millennium CE wore on, creating new settlements and using stone - typically coral blocks held together with mortar - instead of, or in addition to, mud and wood for their homes. They profitably traded coastal commodities such as shell jewellery for agricultural products from the more fertile interior. When trade networks spread along the coast, so too ideas in art and architecture went with them, as did language, spreading Swahili further afield.\nA Meeting of Two Worlds\nFrom the 7th century CE, trade networks expanded to include the Red Sea (and so Cairo in Egypt), and then Arabia and the Persian Gulf. Arab dhows with their distinctive triangular sails filled the ports of the Swahili coast. Trade was even carried on across the Indian Ocean with India and Sri Lanka, as well as China and Southeast Asia. The long-distance sea voyages were made possible by the alternation of winds which blew to the northeast in the summer months and reversed in the winter months.\nFrom the mid-8th century CE, Muslim traders from Arabia and Egypt began to permanently settle in towns and trading centres along the Swahili coast. The Bantu and Arabs mixed, as did their languages, with intermarrying being common and a blending of cultural practices which led to their evolution into a unique Swahili culture.\nShirazi merchants from the Persian Gulf arrived at Kilwa from the 12th century CE and so the influence of the Sunni Islamic religion and Muslim architecture was further strengthened. The Shirazi established their rule over Kilwa by around 1200 CE - by peaceful means according to medieval Arab sources - although the city-state did not exert any form of wider political control or even cultural influence over the mainland interior. As Kilwa could not produce its own food, though, there must have been some arrangement with local tribes on the mainland.\nThe social structure of Kilwa and other Swahili ports is here summarised by the historian H. Neville Chittick:\nThe inhabitants of the towns can be considered as falling into three groups. The ruling class was usually of mixed Arab and African ancestry…such also were probably the landowners, merchants, most of the religious functionaries and the artisans. Inferior to them in status were the pure-blooded Africans, probably mostly captured in raids on the mainland and in a state of slavery, who cultivated the fields and no doubt carried out other menial tasks. Distinct from both these classes were the transient or recently settled Arabs, and perhaps Persians, still incompletely assimilated into the society. (Fage, 209)\nThe population of Kilwa at its peak was likely at least 10,000 residents, perhaps double that figure. It was governed by a single ruler, but details of how he was chosen are lacking besides some cases of one ruler nominating his successor. Assisting the ruler or sultan were various officials, such as a council of advisors and a judge, who were all likely selected from the most powerful merchant families.\nKilwa’s ability to attract foreign interest continued into the 15th century CE. In 1417 CE the famed Chinese admiral Zheng He (1371-1433 CE) made it to East Africa in one of his celebrated seven voyages of exploration. Zheng He took back to China such exotica as giraffes, gems, and spices. Foreign travellers and the accounts they wrote can be added to local documents to flesh out the history of Kilwa, notably the Kilwa Chronicle, a history of the area and its ruling dynasties which was likely written between 1520 and 1530 CE.\nTrade - Kilwa & Sofala\nIn order to reach the resources of southern Africa’s interior, Kilwa needed a trading post further south. This would be Sofala (in modern Mozambique), founded perhaps around 1300 CE. This southern Swahili outpost was important to such cultures as Great Zimbabwe (c. 1100 - c. 1550 CE) in modern-day Zimbabwe and vice-versa. Indeed, gold from Zimbabwe that reached Sofala helped make Kilwa the most prosperous of all the Swahili coast cities, overtaking Mogadishu. Sofala was also a manufacturing centre, too, producing large numbers of pottery and, to a lesser degree, smelting iron and copper before exporting these metals. Meanwhile at Kilwa, cotton cloth was manufactured and there were workshops producing goods made from ivory, glass, and copper.\nBesides gold, Kilwa was able to gather and export ivory, tortoise shells, copper (often cast in x-shaped ingots), timber (especially mangrove poles), incense (e.g. frankincense and myrrh), rock crystal, grain, and rhino horns, which were then exchanged for such exotic luxury goods as Chinese Ming porcelain, precious metal jewellery, fine cloth and glass beads from India, silk, glassware and carved faience from Persia. Many of these items would have been traded on into Africa’s interior along the coast and, of course, consumed within Kilwa itself. As wealth poured into Kilwa - via both exchange and duties on the movement of goods - the city was able to mint its own copper coinage from the 11th or 12th century CE. Great Zimbabwe’s successor in that region, the kingdom of Mutapa (c. 1450 - c. 1650 CE) on the Zambezi River, also traded with Sofala and exchanged gold, ivory, animal hides, and slaves for imported luxury goods.\nKilwa Architecture - Husuni Kubwa\nKilwa had many fine and imposing buildings. The Husuni Kubwa Palace (‘Large Fort’ in Swahili) was located on a sandstone promontory just outside the city and was reached by a monumental staircase cut into the rock. The buildings were mostly of one story and constructed using dressed blocks cut from the natural rock faces of Kilwa. It was a large complex covering almost 10,000 square metres (1 hectare) and included a spacious audience hall, courtyard with tiered seating or steps, domed ceilings, storerooms (covering half of the palace’s area), and a pool. Although the architecture is similar to buildings seen in Aden with its domes, pavilions, and barrel vaults, the Kilwa architects added their own unique twist by embedding pieces of Chinese porcelain into the white lime plaster of the exterior walls for decorative effect. The roofing was made from flat pieces of coral supported by a dense framework of mangrove poles. The palace and other buildings for the ruling elite and wealthy included such luxuries as indoor plumbing.\nThe palace walls and those of the mosque and one house have another point of interest, several examples of ancient graffiti which show both Arab and local trading ships. A different kind of wall writing, and just as important, is an inscription which reveals the name of the ruler who commissioned it, al Hasan ibn Suleiman (r. 1320-1333 CE), and thus we have an approximate date of construction.\nThe Great Mosque\nThe Great Mosque, also known as the Friday Mosque, was, like the palace, constructed from coral rock blocks with a mangrove and coral roof. All mosques on the Swahili Coast were relatively small and almost always without minarets but the Great Mosque at Kilwa, as its name would suggest, was grander than most. Again begun by al Hasan ibn Suleiman in the 14th century CE and then completed under Suleiman al Adil (r. 1412-1442 CE), the structure incorporated parts of an earlier mosque of the 10th-11th century CE. It has impressive monolithic coral columns which support a high vaulted ceiling, octagonal columns creating 30 arched bays, and a 4-metre (13 ft.) square room with a domed roof. There was, too, a small domed chamber separate from the rest of the building for the sultan to worship privately. Minimalist in decoration like other mosques of the region, it did have many carved coral bosses with very intricate geometrical designs.\nAnother impressive structure is the Husuni Ndogo or ‘Small Fort’, which consists of a large rectangular courtyard with a stone well completely encircled by a sandstone circuit wall and only one entrance gate. Its precise purpose is unknown, but it may have functioned as a place for travellers to stay, a barracks, or even a market. The city boasted additional mosques as well as many small and well-watered gardens, some with orchards. Large warehouses were built of coral rock, too. Domestic housing typically consisted of a stone building with two very long rooms, smaller private chambers with many wall niches, an inner courtyard, and large windows. Decoration was achieved by adding carved wooden window and door frames, window grilles, or even setting rows of porcelain bowls into the ceilings. Buildings were constructed very close together, often sharing a wall, and so the city had very narrow and maze-like streets. The palace, Great Mosque, and general attention to architecture led the Moroccan explorer and traveller Ibn Battuta (1304 - c. 1368 CE), who visited c. 1331 CE, to famously describe Kilwa as ""one of the most beautiful towns in the world"" (quoted in Spielvogel, 233).\nThe Portuguese & Decline\nThe decline of Kilwa began with its own internal dynastic disputes and, consequently, the city was already weakened and in no way prepared for the threatening arrival of the Portuguese. These Europeans with their large sailing ships sought to establish a presence and then total control of the lucrative regional trade following the voyage of Vasco da Gama in 1498-9 CE when he went around the Cape of Good Hope and up the east coast of Africa. Another motivation besides trade for Portuguese intervention was the conversion of Muslim communities to Christianity.\nKilwa was attacked by the Portuguese in 1505 CE, leaving many of its buildings in ruins. The Portuguese, with their base at Goa, India, eventually gained control of the Indian Ocean and built fortresses to make sure they kept it, notably at Sofala in 1505 CE and Mozambique Island in 1507 CE. As a consequence of this presence, inland traders now conducted their business with more northerly Swahili ports such as Mombasa. Kilwa had other problems, too, such as the odd uprising from interior tribes like the Zimba cannibals who attacked the island in 1587 CE, killing 3,000 residents (how many they ate is unknown).\nHalf a century later, around 1633 CE, the Portuguese then chose a more aggressive policy to control the region’s resources at the source and cut out their trade rivals. They attacked and conquered one of the chief sources of gold, the kingdom of Mutapa in Zimbabwe, which was already weakened by damaging civil wars, causing its internal collapse. In general, though, trade networks merely moved northwards and, in any case, the Europeans were quickly disillusioned with how little gold was available in East Africa compared to West Africa and Inca Peru. In the 18th century CE Kilwa, now under French control, became a major port of the East African slave trade as well as a significant exporter of ivory. Although Kilwa has survived in part, Sofala fared much worse and was destroyed by the incursion of the sea in the early 20th century CE.']"	['<urn:uuid:67a069ab-ff24-4792-9b7f-dc440efa17b0>']	open-ended	with-premise	long-search-query	distant-from-document	single-doc	novice	2025-05-12T21:52:51.670698	11	94	2211
89	What are the distinguishing characteristics of the Senlac sword's pommel and guard design compared to other medieval swords?	The Senlac features an Oakeshott Type G disc-shaped pommel that, while simple in form, has subtle rounding from the center to the edges, and is slightly asymmetrical where it meets the grip. The cross-guard is a basic Oakeshott Style 1a, notably thinner than those found on lower-priced swords. It has a rectangular cross-section at the blade slot, tapering to a thinner square toward the ends. The guard is expertly fitted to the blade with extremely tight tolerances, featuring a lean, utilitarian design without excess steel.	"[""A Resource for Historic Arms and Armour Collectors\nAlbion Armorers Next Generation Senlac Sword\nA hands-on review by Bryan Heff\nIn the year 1066, during the twilight of the Viking Age, a desperate struggle for control of England came to a head. Duke William of Normandy, with claims to the throne of England, landed his Norman army consisting of mixed troops that included a large contingent of heavy cavalry on the Southeastern coast of England. King Harold II of England and his Saxon army, fresh from a hard fought battle and difficult march from the north, had quickly returned south in an effort to stop this Norman challenge to his throne. The Saxon took up a defensive formation with a traditional shield wall, high atop a hill along the London road in an effort to block the Normans' march north and bring them to decisive battle. Several weeks before, this same Saxon army with its Huscarl core of heavy infantry had successfully smashed an invading Viking army at Stamford Bridge. The fate of the island now lay at the feet of these two armies. By the end of battle a victor would emerge and history would be set. The Battle of Hastings was about to unfold and its outcome would help set the stage for European battle tactics for years to come. The battle was fought on a hill called Senlac, and swords similar to the subject of this review would have been used on both sides. Whether being swung from horseback or from behind large shields, these slashing cutting blades designed to oppose mail and textile armour were the order of the day.\nAlbion Armorers Next Generation Senlac is a long bladed single-hand medieval sword best described using Ewart Oakeshott's typology as a Type Xa, a cousin to the broad cutting blades of Type X. Being slightly longer, having a fuller that is not quite as wide and with a more pronounced profile taper is what essentially differentiates the Type Xa subclass. While potentially more suited to a thrust than a Type X, its primary purpose would be in the cut. Albion produces three swords all using the same blade. Those swords are the Senlac, Norman and Knud. The Norman and Senlac are very similar, differing most significantly in the cross-guard only. The riser patterns on the grip are also different between the two but not greatly so. The Senlac is a no-nonsense sword with little adornment. It is a sword that soldiers could have worn at their hip from Hastings all the way to the gates of Jerusalem during the First Crusade. This is a sword a man could rely on with confidence.\nMeasurements and Specifications:\nReplica created by Albion Armorers of Wisconsin.\nThe Senlac is a long-bladed weapon with a fairly short grip intended solely for single-hand use. These combined factors could create a sword that is sluggish. This is not the case here. The handling is really quite nice. No cutting was done with the sword prior to this review, just basic dry handling movements to gauge how it feels; smooth would be the word most adept at describing its overall characteristics. To borrow from boxing terminology, the blade could be described as a middle-weight. The point of balance is not particularly close to the hilt so it displays nice blade presence. It is a weapon that allows quick attacks and recovery but still has enough heft to deliver vicious slashing and hewing cuts as well.\nOften swords like this would have been used as side arms, not necessarily the primary weapon. The handling speaks volumes to this kind of usage. It is a sword agile enough to be used for a long period of time if need be while being light enough to wear on long campaign marches; it has enough punch to get its wielder out of a jam.\nFit and Finish\nOne aspect of the blade that really stands out is the slight convex curvature of the profile the final 1/3 of the blade. Many production swords available today have very straight lines along the blade's profile until the final tip is formed. The ever so slight convex curve of the Senlac's profile really gives it a beautiful blade presence and is one of the subtle details that helps make what seems like a simple sword much more.\nThe pommel is a disc-shaped Oakeshott Type G. It is simple in form, not overly complex, and could be easily described as quite plain. It does however have some rounding of the faces from the center out to the edges which gives it an element of complexity that differs from a flat disc. It is also not a perfectly round circle in shape but comes out of round ever so slightly where the pommel meets the grip. The peen is blended into the pommel so well that you simply cannot see it unless the light hits it in just the right way. The convex sides of the pommel are not perfect in terms of symmetry, but come very close to it and I think provide a good example of the hand work on the sword. This is not a criticism so much as an observation. It adds a warm handmade look and feel.\nThe guard is a straight, very basic Oakeshott Style 1a. The guard slot is expertly cut and fitted to the blade with extremely tight tolerances. The guard is rectangular in cross-section where the blade slot is located and then tapers down to a thinner square as it moves away from the blade. What struck me right away is how thin the cross-guard is compared to heavier/bulkier cross-guards more common on lower-priced swords. It's almost elegant in its utilitarian and lean design. There is not a lot of extra unneeded steel in the guard, just enough to accomplish its purpose.\nThe Senlac is unquestionably a simple and plain sword at first glance. This may be a deterrent to some sword enthusiasts especially at its price point. I think the argument could be made that along with their Bayeux and Hospitaller models, it is the least adorned medieval sword Albion Armorers produces in terms of hilt components. With all that in mind this reviewer has come to the conclusion that the Senlac is simply and clearly a classic medieval cruciform sword. It is perhaps the classic sword type for this post-Viking Age time period and it makes perfect sense that Albion would want such an iconic design as part of its Next Generation line. The straight guard and disc pommel are function over form in its purest sense. There is beauty in that simplicity. Its clean and straight lines are pleasing to the eye. It has an overall balance in both looks as well as handling that hit the mark on stylistic sensibilities that subscribes to the less is more ideal and it delivers on that extremely well. The Senlac is a beautifully understated but classic sword design.\nAbout the Author\nBryan Heff is a Systems Analyst living in the Philadelphia suburbs with his wife and 2 sons. He has always been intrigued by the European sword as a work of art, symbol and weapon of war. His main area of interest is rooted in the 8th to 14th centuries but he has interests in other time periods as well. In addition to adding to his small but growing collection of swords, he enjoys modifying and customizing swords in his compact basement workshop, hiking and exploring historic sites and trying to get his boys to do their homework.\nPhotographer: Bryan Heff""]"	['<urn:uuid:9a7aa7bd-203d-45b3-bab1-63a32baf3d37>']	open-ended	direct	verbose-and-natural	distant-from-document	single-doc	expert	2025-05-12T21:52:51.670698	18	85	1267
90	What are the three main parts of the Paris Agreement's purpose, and what issues exist with the way climate money is being provided to developing countries?	The Paris Agreement has three main components: 1) keeping temperature rise well below 2°C, 2) increasing ability to adapt to climate impacts while ensuring food production isn't threatened, and 3) making finance flows support low greenhouse gas emissions development. Regarding financial support, the OECD report highlights concerns about the prevalence of loans in climate finance, which could cause debt stress in poorer countries, and notes issues with the lack of a clear definition of 'climate finance' leading to problems like double-counting.	"[""Adoption of the Paris agreement\nOn Saturday 12 December, to a standing ovation, and some tears of relief, the 196 Parties (195 States and 1 regional economic integration organisation) to the United Nations Framework Convention on Climate Change (UNFCCC) adopted the Paris Agreement.\nHow did we get here?\nThe UNFCCC COP in 2011 in Durban, South Africa, established the Ad Hoc Working Group on the Durban Platform for Enhanced Action (ADP)1 as a subsidiary body under the convention, and launched a process to develop a protocol, another legal instrument or an agreed outcome with legal force under the Convention applicable to all Parties, to be adopted at COP 21 and to come into effect and be implemented from 2020.2 Parties to the UNFCCC first started working on draft ‘elements’ of a negotiating text in Lima at COP 203 and followed up in Geneva at the 8th part of the ADP’s second session. The outcome of which this session was the Geneva Text4 a compilation of all Parties’ views on the key components of a draft agreement.\nOver the course of 2015, the ADP met 3 times in Bonn, Germany before COP 21 in Paris and worked to “streamline” the Geneva Text down from 90 pages to a more acceptable size, and to flesh out the COP decision that would adopt the Agreement.5 However, coming into Paris, many core issues, including finance, differentiation, reporting and international review remained unresolved. These political hot potatoes could not be resolved by the UNFCCC negotiators alone, and would require the involvement of the many government ministers attending the COP.\nFinally, after 2 weeks of negotiations, bilateral talks, back-room deals, 3 all-night negotiating sessions, and a last-minute panic around the wording of Article 4, the Paris Agreement was finally adopted early Saturday evening.\nWhat does the Paris Agreement say?\nThe Purpose/objective of the Paris Agreement is to “strengthen the global response to the threat of climate change”, enhancing the implementation of the (UNFCCC) Convention, including its objective.6 The objective acknowledges the needs of developing countries and qualifies the objective of the Agreement with the phrase “in the context of sustainable development and efforts to eradicate poverty”. This provision aims to address concerns from developing countries that climate action should not stifle or slow their (sustainable) development. The objective of the Paris agreement is translated into three broad overarching components:7\n• A Mitigation component, which states that the increase in global average temperature needs to be held to “well below 2 °C above pre-industrial levels” and efforts to limit the temperature increase to 1.5 °C above pre-industrial levels, be “pursued”\n• An Adaptation component, which aims to increase “the ability to adapt to the adverse impacts of climate change and foster climate resilience and low greenhouse gas emissions development, in a manner that does not threaten food production”; and\n• A finance component, which aims to make finance flows consistent with a pathway towards low greenhouse gas emissions and climate- resilient development.\nThe broad wording of this purpose/objective is the result of a need to find compromise language that could reconcile the (often competing or even opposing) interests of the 196 Parties to the UNFCCC.\nHow will it work?\nIn order to achieve the long-term temperature goal, set out in Article 2, Parties aim to reach global peaking of greenhouse gas emissions “as soon as possible”. This is to be achieved under the Agreement through the preparation, communication and maintenance of successive nationally determined contributions (NDCs).8\nAt its 19th meeting in Warsaw, the COP invited Parties to “initiate or intensify” domestic preparations for their Intended Nationally Determined Contributions (INDCs) towards achieving the objective of the Convention as set out in its Article 2.9 Information on what could be included in the INDCs was elaborated (albeit in a very general manner) in Lima at COP 20.10 Once the Paris Agreement comes into force, these INDCs will become NDCs, that Parties commit to implement.\nThe specific targets and measures that each country11 sets for their contribution to addressing climate change is entirely self-determined. The rationale behind such a bottom-up approach is that universal participation is more likely achieved than through a top-down approach with centrally determined targets. Achieving universal participation in a new agreement was prioritised above all other considerations, with the intention that gaps in ambition12 could be addressed through the design of a mechanism for regular review and upward revision of contributions.\nThis is recognised in Article 3 of the Agreement, which states that the efforts of all Parties (the INDCs) “will represent a progression over time.” In order to facilitate this process the COP has mandated the Ad Hoc Working Group on the Paris Agreement (APA) to develop further guidance for the information to be provided by Parties in order to facilitate “clarity, transparency and understanding” of nationally determined contributions.13\nHowever, the way in which NDCs are treated in the Agreement and accompanying Decision is that only mitigation contributions are explicitly required to be included in the INDCs and subsequent NDCs. This is reflected in the objective, as a long-term temperature goal is set (mitigation), but no specific targets are set for adaptation or finance. These two components will be dealt with below in their respective sections.\nHow regularly must NDCs be submitted/revised?\nThe first NDC is expected to be submitted on or before the date at which Parties ratify the Agreement.14 Most Parties have already submitted their INDCs, which, in 2020, will become their nationally determined contribution (NDC). Due to differing views among Parties, the agreement does not include a requirement for Parties to upwardly revise their current INDCs before 2020, meaning that the current INDCs submitted by Parties will be the ones they are expected to implement starting in 2020.\nFollowing 2020 however, each Party’s successive NDC is expected to “represent a progression beyond the Party’s then current nationally determined contribution and reflect its highest possible ambition”.15 Each Party is required (shall) to communicate a NDC every five years, meaning that in 2025, Parties will submit their new NDCs, which should show a progression from their current effort.\nAlthough the cycle for communication of NDCs is every 5 years, the Agreement does not explicitly specify that each NDC should have a time frame for implementation of 5 years. This is intentional to accommodate those Parties that have a ten-year time-frame for their NDCs. Instead, the Decision “urges” Parties whose INDCs contain a timeframe up to 2025 or 2030, to communicate a new INDC by 2020 and a further one every five years after that.16\nWhat about adaptation and finance?\nAlthough not explicitly linked to the preparation, implementation and communication of NDCs, action in the areas of adaptation and finance are still part of the overall purpose of the Agreement.17\nThe Agreement establishes a global goal on adaptation of “enhancing adaptive capacity, strengthening resilience and reducing vulnerability to climate change”.18 For adaptation measures taken, the Agreement states that adaptation efforts “shall be recognized, in accordance with the modalities to be adopted by the Conference of the Parties serving as the meeting of the Parties to the Paris Agreement at its first session.”19\nThis can be read to mean that INDCs and subsequent NDCs are not explicitly required to have an adaptation component, although the importance of “support for and international cooperation on adaptation efforts” is “recognized” 20 and a process has been established to ensure that adaptation efforts by developing countries are recognised.21\nAlthough finance is not explicitly linked to the NDCs, the provision of financial resources by developed country Parties to assist developing country Parties with respect to both mitigation and adaptation is a requirement under the Agreement, and an integral part of its purpose.22 While Parties are “encouraged” to provide or continue to provide such support voluntarily, 23 developed country Parties are expected (should) to continue to take the lead in “mobilizing” climate finance from a wide variety of sources, instruments and channels.24 This should represent a progression beyond previous efforts and should aim to achieve a balance between adaptation and mitigation.25\nFurthermore, the accompanying COP decision establishes that prior to 2025, the Conference of the Parties serving as the meeting of the Parties to the Paris Agreement (CMA) will set a new collective quantified finance goal “from a floor of USD 100 billion per year” taking into account the needs and priorities of developing countries.26 The importance of adequate and predictable finance for REDD+ has also been recognised.27 Developed country Parties are also expected to communicate information on the finance provided, on a biannual basis28 and in accordance with common modalities, procedures and guidelines to be developed for the transparency of action and support.29 These will be considered at COP 24 and should be adopted at the CMA’s first meeting.30\nThe Agreement also establishes a process to assess the collective progress towards achieving the purpose of the Agreement and its long-term goal, which should be undertaken “in a comprehensive and facilitative manner” and cover mitigation, adaptation and the means of implementation and support.31 The first global stock-take is scheduled for 2023 and every five years thereafter.32 Its outcome “shall inform” Parties in updating and enhancing, in a nationally determined manner, their actions and support.33\nAlthough the Paris Agreement represents the most significant global diplomatic achievement to coordinate global governmental action on climate change to date, there is still much to be done. The broad framework is now in place. The next five years before the Agreement’s entry into force will be spent elaborating the variety of modalities, rules and procedures required by the agreement including but not limited to: the features of NDCs, on adaptation actions, on finance and on transparency of action and support.\n1. Decision 1/CP.17 paragraph 2\n2. Ibid paragraph 4\n3. Decision 1/CP.20 Annex\n5. Making an analogy with domestic law, the Paris Agreement represents the general framework for coordinated action on climate change and serves the same role as primary legislation, while the COP decision fleshes out the details on how the framework agreement (principles, objectives, thematic areas of action) can be operationalized (mandates to develop rules on features of mitigation actions, on reporting) in a way that is similar to secondary legislation.\n6. Paris Agreement Article 2.1\n7. Paris Agreement Article 4.2\n8. Paris Agreement Article 4.2\n9. Decision 1/CP.19\n10. Decision 1/CP.20\n11. Article 4 of the Agreement clarifies that least developed countries and small island developing States may prepare and communicate strategies, plans and actions for low greenhouse gas emissions development reflecting their special circumstances, rather than requiring fully fleshed out INDC's\n12. It is acknowledged in draft Decision-/CP.21 accompanying the Paris Agreement (para 17) the estimated aggregate greenhouse gas emmision levels in 2025 and 2030 resulting from the intended nationally determined contributions do not fall within least cost 2 C scenarios.\n13. Draft Decision -/CP.21 Adoption of the Paris Agreement paragraph 28\n14. Draft Decision -/CP.21 Adoption of the Paris Agreement paragraph 22\n15. Ibid paragraph 4.3\n16. Ibid paragraphs 23 and 24\n17. Paris Agreement Article 2.1(b) and (c)\n18. Article 7.1\n19. Paris Agreement Article 7.3\n20. Ibid Article 7.8\n21. Draft Decision -/CP.21 Adoption of the Paris Agreement paragraph 42\n22. Paris Agreement Articles 2.1 (c) and 9.1\n23. Paris Agreement Article 9.2\n24. Paris Agreement Article 9.3\n25. Paris Agreement Article 9.4\n26. Draft Decision -/CP.21 paragraph 54\n27. Draft Decision -/CP.21 paragraph 55\n28. Paris Agreement Article 9.5\n29. Paris Agreement article 9.7\n30. Decision -/CP.21 paragraph 92\n31. Paris Agreement article 14.1\n32. Ibid Article 14.2\n33. Ibid Article 14.3"", 'OECD’s Insights for Global Action\nThe recent report by the Organization for Economic Cooperation and Development (OECD) brings forth critical insights into the climate finance commitments of economically developed countries, particularly in the context of their promise to mobilize $100 billion annually towards climate mitigation and adaptation needs of developing nations.\nGS – 3 (Environmental Pollution & Degradation)\nClimate Finance, COP 26, Kyoto Protocol, Paris Agreement, United Nations Framework Convention on Climate Change (UNFCCC), Green Climate Fund (GCF)\nEvaluate the significance of climate finance in the global climate change discourse and analyze the challenges in meeting the financial commitments as per the Paris Agreement, emphasizing the responsibilities of developed countries towards developing nations. (250 words)\nWhat is Climate Finance:\n- Climate finance pertains to the funding acquired from local, national, or transnational sources, encompassing public, private, and alternative financial channels to enhance actions addressing climate change by supporting both mitigation and adaptation endeavors.\n- The UNFCCC, Kyoto Protocol, and Paris Agreement advocate for financial support from those Developed Countries that has more financial resources to aid Developing Countries that are susceptible to the impacts of climate change.\n- This principle aligns with the concept of “Common but Differentiated Responsibility and Respective Capabilities” (CBDR).\n- COP26 : Fresh commitments were made to provide financial support for developing nations in achieving global goals for adapting to climate change effects.\nDimensions of the Article:\n- OECD Report Overview\n- Developing Countries’ Financial Needs\nOECD Report Overview:\n- The report underscores the failure of economically developed countries to meet their commitment of jointly mobilizing $100 billion annually for climate mitigation and adaptation in 2021, marking a year beyond the 2020 deadline.\n- Analyzing the consequences of the financial gap, the report sheds light on the challenges faced by developing nations in addressing climate mitigation (e.g., renewable energy adoption) and adaptation needs (e.g., resilient agriculture).\n- The shortfall in climate finance impacts trust among developing countries regarding the commitment of developed nations in addressing the climate crisis.\n- The report delves into the composition of climate finance, emphasizing the prevalence of loans in the financial support provided by developed nations. This analysis highlights potential debt stress in poorer countries, raising concerns about the conditions attached to such financial support.\n- The concept of additionality in climate finance is explored, focusing on the UNFCCC stipulation that developed countries must provide new and additional financial resources.\n- The lack of a universally agreed-upon definition of ‘climate finance’ is discussed, pointing out the intentional ambiguity maintained by developed countries. The consequences of this ambiguity, such as double-counting and questionable project classifications, are emphasized.\n- The report also highlights the stagnation of private financing for climate action over the past decade and addresses the challenges faced in scaling up private sector involvement, especially in climate adaptation initiatives.\n- The OECD report’s recommendations for course correction, including de-risking strategies and the role of multilateral development banks in mobilizing private finance, are also discussed.\nDeveloping Countries’ Financial Needs:\n- The article scrutinizes the adequacy of the $100 billion goal, questioning its origin and relevance in comparison to the actual climate investment needs of developing countries.\n- The report suggests estimations indicating a significant gap between the committed amount and the required funds for climate investments in the coming years.\n- While acknowledging the potential contribution of the private sector, the governments and multilateral development banks should also remain indispensable in enabling climate action.\n- The need for a more transparent, accountable, and universally agreed-upon definition of climate finance is important for effective global collaboration.\n- The OECD report serves as a catalyst for reevaluating current approaches, prompting nations to reassess their climate finance commitments and strategies for a more sustainable and inclusive future.\nMeanwhile , the COP 28 in Dubai shall offer a vital opportunity to reinvigorate global efforts and commitments, emphasizing the urgency of addressing the climate finance crisis to secure a sustainable and equitable climate future.']"	['<urn:uuid:1d019641-3d2c-465c-ad79-b12634d3b14a>', '<urn:uuid:b80c14db-f85f-43ac-afaf-595ca9c5f7ef>']	factoid	direct	verbose-and-natural	distant-from-document	multi-aspect	novice	2025-05-12T21:52:51.670698	26	81	2580
91	factors creating arctic security community changes political economic technological external threat	According to work by Adler and Barnett, security communities are typically created by either changes in the political, economic and/or technological status of a region, or the appearance of an external threat. In the Arctic's case, while external threats are less relevant, the region has experienced rapid changes on various fronts.	['Analysis | There are plenty of hurdles to developing a forum to address Arctic security issues, but that does not eliminate the need to do so\nOver the past few weeks, there has been an uptick in the longstanding debate about whether the time is ripe for security issues to be added to the growing array of Arctic regional organisations, either connected to or co-operating with the Arctic Council. One recent article was highly critical of the idea of an Arctic security forum, particularly one which would address military security concerns.\nAmong the notable arguments against such a move are the stable geopolitical situation in the region, as well as the benefits offered by international law, including the UN Convention on the Law of the Sea (Unclos) and new issue-specific regional initiatives such as the Arctic Coast Guard Forum (ACGF), created in 2015 to better co-ordinate far northern maritime safety issues. This view is also based on the assumption that the introduction of a military dimension to Arctic co-operation would actually foster increased divisions and distract from more pressing regional issues.\nThe poor relations between Russia and the West have begun to spill over into the region, amid concerns about Moscow’s military build-up in its Arctic territories and the increased number of incursion incidents in the Baltic-Nordic region by Russian ships and planes. As well, the Ukraine conflict acts as ‘Banquo’s ghost’, an issue which has haunted governmental and non-governmental Arctic meetings. The possibility of Russian-Western rivalries migrating to the Arctic is a valid concern, and has been one of the catalysts of the developing debate about an Arctic security mechanism.\nHowever, the idea of creating a hard security organisation for the region remains unworkable for a variety of reasons. Firstly, there is no common definition of Arctic boundaries, thus creating a problem of jurisdiction. Secondly, the distribution of power among the Arctic governments would also discourage any formal military organisation. The ‘Arctic eight’ includes Nato members and neutrals, two great powers and small states, as well as semi-independent entities, such as Greenland. There would be apprehension among some Arctic states, as well as indigenous groups, about a potential loss of sovereignty, and many non-Arctic states would react with dismay at such an organisation, especially if they were denied the chance to participate.\nThese formidable obstacles should not discourage the possibility of creating a security mechanism in the region through different means. It would be equally counterproductive, and reflective of short-term thinking, to continue to treat the Arctic as embedded within a cordon sanitaire and a place where security issues are kept out indefinitely. Instead, discussion about security co-operation in the region should begin with the creation of an Arctic security community (ASC), which would focus on non-traditional security issues of importance to the Arctic eight as well as non-Arctic states with developing interests in the region, notably the observer governments in the Arctic Council.\nAs noted in the work by Adler and Barnett, security communities are often born of changes in the political, economic and/or technological status of a given region, or the appearance of an external threat. While the latter is less valid in the case of the Arctic, it can certainly be argued that overall conditions in the region have changed rapidly on a variety of fronts.\nThen, should conditions appear which promote knowledge sharing, an increase in trust-building, and the feeling of a collective identity, a security community may be created which does not align against a third part but rather focuses on comprehensive security development. These conditions are appearing in the Arctic, and should an ASC be ‘anarchic’ in nature, meaning that decisions are made by consensus with every member entitled to a veto, concerns about power blocs would be muted.\nThe proliferation of new Arctic agencies and proposals designed to address maritime safety, pollution and codes of conduct has raised concerns about what has been called a ‘spaghetti bowl’ problem, the overlapping of too many organisations with too little co-ordination, leading to confusion and policy duplication. As more Arctic agencies are created to address various strategic areas, an ASC could act as a nexus for these agencies to communicate and suggest new initiatives in a congenial atmosphere.\nAs well, an Arctic security community would address concerns by non-Arctic states, especially China, which are sensitive to being shut out of Arctic affairs as the circumpolar north continues to open economically. An ASC would work to bolster communication and confidence-building between Arctic and non-Arctic entities on the governmental and non-governmental levels, essential links as non-Arctic states and companies seek to deepen their presence in the region.\nWhile the timing for creating an ASC would not be optimal due to the toxic state of relations between Russia and the West right now, in many other ways the geopolitical situation in the region would be very favourable to creating such an organisation. The United States, as current chair of the Arctic Council, would be in an excellent position to put forward such a proposal, especially if it could provide a conduit for communications with Moscow. Canada has a new government that has promised to re-examine its Arctic policies, including in the areas of regional development.\nSeveral Arctic as well as non-Arctic governments, including, most recently, France, have published policy papers detailing their circumpolar policies, many of which include specific references to the need to address traditional and non-traditional security in the Arctic. Finally, due to lower fossil-fuel and commodity prices since 2014, the spectre of a zero-sum global scramble for Arctic resources has greatly faded, but has not completely disappeared given the unpredictability of future raw-material prices (and demands).\nOne needs to ask whether it is time to take advantage of this window of opportunity to bring together the various Arctic security-related initiatives created over the past few years, in the same vein as the events which led to the creation of the Arctic Council 20 years ago, and to create a comprehensive, inclusive and effective security mechanism for the region.\nAlong with the environment in the Arctic, the political situation there is also changing rapidly, and it would be judicious to prepare for these changes rather than simply react to them. It is much better to build the firehouse before having to fight the fire.\nThe author is a senior research fellow at the Norwegian Institute of International Affairs (NUPI) in Oslo.\nOriginally published by The Arctic Journal. Re-published here with the permission of the author.\nGet full-length articles delivered directly to your inbox. Subscribe to The Rasmussen’s newsletter.']	['<urn:uuid:77ccf727-ce8f-472d-9a46-2e9223015cd0>']	factoid	with-premise	long-search-query	similar-to-document	single-doc	expert	2025-05-12T21:52:51.670698	11	51	1092
92	Who usually helps with defibrillators during emergencies?	Before emergency services arrived, defibrillation was attempted in 25% of cases. The first shock was delivered by members of the public in 23% of cases, doctors in 18%, Basic Life Support trained individuals or Cardiac First Responders in 17%, local fire services in 12%, voluntary services in 12%, and nurses and others (including occupational first aiders and Gardaí) in 7% of cases.	"[""Footballer’s cardiac arrest highlights importance of CPR\nSpeed is of the essence with CPRRead More\nThe latest OHCAR report shows improvement in rate of bystander CPR\nLatest figures show that there has been an improvement in the number of people who survived a cardiac arrest in the community in 2019 compared to 2018, and in the rate of bystander CPR, where someone has witnessed or come across a cardiac arrest and stepped in to attempt CPR.\nAccording to the 2019 annual report from the Out of Hospital Cardiac Arrest Register (OHCAR) which collects information on the number of and outcomes for people who suffer a cardiac arrest outside of a hospital setting, between 2012 and 2019, there was a 17 per cent increase in bystander CPR and the number of people who survived to hospital discharge increased from 176 in 2018 to 190 in 2019. The report also found that the vast majority of people who survived had good neurological function on hospital discharge.\nIn 2018 the rate of bystander CPR was 81 per cent and this increased to 84 per cent last year.\nCPR is a lifesaving skill and involves pressing hard and fast on the centre of a person’s chest. If you witness a cardiac arrest and perform CPR, you can double, if not triple, the person’s chance of survival.\n“ The Out of hospital cardiac arrest report 2019 issued today shows some positive increases in survival and bystander CPR, these increases are small but for every life saved this has a huge impact on the survivor's family.”\nThe report, also found that the vast majority of people who survive consistently have good neurological function on hospital discharge.\nCommenting Brigid Sinnott, Resuscitation Manager at the Irish Heart Foundation said, “The Out of hospital cardiac arrest report 2019 issued today shows some positive increases in survival and bystander CPR, these increases are small but for every life saved this has a huge impact on the survivors family.”\nOverall, there were 2,564 out-of-hospital cardiac arrests recorded by OHCAR in 2019 and more than half of these or 69 per cent occurred in an urban area.\nThe vast majority of these cardiac arrests or 86 per cent had a presumed medical cause that caused the heart to stop beating while a small proportion of cases were due to non-medical causes such as trauma, choking, drug overdose or drowning.\nThe report also found that 68 per cent of these cardiac arrests happened in the home with 67 per cent occurring in men and the median age was 68. It found that 18 per cent of patients who collapsed in a public location survived, compared to 4 per cent of patients who collapsed in a private location\nThe 2019 OHCAR report found that cardiac arrest patients ranged in age from less than one to 103 years of age\nThe 2019 OHCAR report found that cardiac arrest patients ranged in age from less than one to 103 years of age with women being more likely to collapse in a private setting i.e. at home or in residential setting like a nursing home.\nWhile the majority of cardiac arrests occur in older age groups, 8 per cent of cardiac arrests reported by OHCAR occurred in 2019 were in patients under the age of 35. Of these, 46 per cent were caused by a presumed medical reason, 12 per cent were caused by trauma (road traffic accident, gunshot, stabbing, crush injuries or fall), 19 per cent of cases resulted from a drug overdose. More than one in 10 or 11 per cent of cardiac arrests in this age group were initially shockable and 9.5 per cent or 20 survived to hospital discharge. This is a significant increase on the 2018 figure where 5 per cent or 11 in this cohort survived according to OCHAR.\nIn relation to defibrillation, the report found that defibrillation was attempted in 25 per cent of cases (187) before the arrival of the emergency services. Members of the public used the defibrillator to deliver the first shock to the patient in 23 per cent of cases, doctors in 18 per cent of cases, someone who was trained in Basic Life Support (BLS) or a trained Cardiac First Responder 17 per cent, member of the local fire services 12 per cent, voluntary services 12 per cent, nurses and “others” including occupational first aiders and members of An Garda Síochána 7 per cent. The report also found that almost half of the patients who survived a cardiac arrest or 49 per cent, received defibrillation before the ambulance arrived.\nThe OHCAR is hosted by the Department of Public Health Medicine in the HSE North West and is funded by the National Ambulance Service. It is administered and supported by the Department of General Practice at the National University of Ireland Galway.\nThe Irish Heart Foundation has recently launched an innovative online campaign to teach the public how to save a life with CPR.Learn More\nSpeed is of the essence with CPRRead More\nSeven Schools awarded CPR special recognition awardsRead More\nThe latest report on out of hospital cardiac arrests published by OHCARRead More\nThe Irish Heart Foundation has launched an innovative online campaign to teach the public how to save a life with CPR.Read More""]"	['<urn:uuid:1a4cb8f1-cd88-4ee5-bc67-6c580931df07>']	open-ended	direct	concise-and-natural	distant-from-document	single-doc	novice	2025-05-12T21:52:51.670698	7	62	880
93	how soldering iron works temperature	A soldering iron consists of a handle with a mounted heating element, ending in a 'bit' that heats the joint. The bit reaches temperatures over 250 degrees Centigrade, while solder melts at around 190 degrees Centigrade. This temperature is hot enough to cause serious burns, so care must be taken when using the iron.	"['Soldering is the process of a making a sound electrical and mechanical joint between certain metals by joining them with a soft solder. This is a low temperature melting point alloy of lead and tin. The joint is heated to the correct temperature by soldering iron. For most electronic work miniature mains powered soldering irons are used. These consist of a handle onto which is mounted the heating element. On the end of the heating element is what is known as the ""bit"", so called because it is the bit that heats the joint up. Solder melts at around 190 degrees Centigrade, and the bit reaches a temperature of over 250 degrees Centigrade. This temperature is plenty hot enough to inflict a nasty burn, consequently care should be taken.\nIt is also easy to burn through the PVC insulation on the soldering iron lead if you were to lay the hot bit on it. It is prudent, therefore, to use a specially designed soldering iron stand. These usually incorporate a sponge for keeping the bit clean.\nSoldering irons come with various ratings from 15W to over 100W. The advantage of a high wattage iron is that heat can flow quickly into a joint, so it can be rapidly made. This is important when soldering connectors as often there is a quite a large volume of metal to be heated. A smaller iron would take a longer time to heat the joint up to the correct temperature, during which time there is a danger of the insulation becoming damaged. A small iron is used to make joints with small electronic components which are easily damaged by excess heat.\nAlways use a good quality multicore solder. A standard 60% tin, 40% lead alloy solder with cores of non-corrosive flux will be found easiest to use. The flux contained in the longitudinal cores of multicore solder is a chemical designed to clean the surfaces to be joined of deposited oxides, and to exclude air during the soldering process, which would otherwise prevent these metals coming together. Consequently, don\'t expect to be able to complete a joint by using the application of the tip of the iron loaded with molten solder alone, as this usually will not work. Having said that, there is a process called tinning where conductors are first coated in fresh, new solder prior to joining by a hot iron. Solder comes in gauges like wire. The two commonest are 18 swg, used for general work, and the thinner 22 swg, used for fine work on printed circuit boards.\nGood soldering is a skill that is learnt by practice. The most important point in soldering is that both parts of the joint to be made must be at the same temperature. The solder will flow evenly and make a good electrical and mechanical joint only if both parts of the joint are at an equal high temperature. Even though it appears that there is a metal to metal contact in a joint to be made, very often there exists a film of oxide on the surface that insulates the two parts. For this reason it is no good applying the soldering iron tip to one half of the joint only and expecting this to heat the other half of the joint as well.\nWhen the iron is hot, apply some solder to the flattened working end at the end of the bit, and wipe it on a piece of damp cloth or sponge so that the solder forms a thin film on the bit. This is tinning the bit.\nMelt a little more solder on to the tip of the soldering iron, and put the tip so it contacts both parts of the joint. It is the molten solder on the tip of the iron that allows the heat to flow quickly from the iron into both parts of the joint. If the iron has the right amount of solder on it and is positioned correctly, then the two parts to be joined will reach the solder\'s melting temperature in a couple of seconds. Now apply the end of the solder to the point where both parts of the joint and the soldering iron are all touching one another. The solder will melt immediately and flow around all the parts that are at, or over, the melting part temperature. After a few seconds remove the iron from the joint. Make sure that no parts of the joint move after the soldering iron is removed until the solder is completely hard. This can take quite a few seconds with large joints. If the joint is disturbed during this cooling period it may become seriously weakened.\nThe hard cold solder on a properly made joint should have a smooth shiny appearance and if the wire is pulled it should not pull out of the joint. In a properly made joint the solder will bond the components very strongly indeed, since the process of soldering is similarly to brazing, and to a lesser degree welding, in that the solder actually forms a molecular bond with the surfaces of the joint.\nIt is important to use the right amount of solder, both on the iron and on the joint. Too little solder on the iron will result in poor heat transfer to the joint, too much and you will suffer from the solder forming strings as the iron is removed, causing splashes and bridges to other contacts. Too little solder applied to the joint will give the joint a half finished appearance: a good bond where the soldering iron has been, and no solder at all on the other part of the joint.\nRemember it is much more difficult to correct a poorly made joint than it is to make the joint properly in the first place. Anyone can learn to solder, it just takes practice.\n[ How to be a Community Radio Station Home Page | Introduction to Community Radio Station Electronics ]\nLast updated 23 August 2003']"	['<urn:uuid:19b197e2-0938-45a2-809b-adf3f0b97001>']	open-ended	direct	short-search-query	similar-to-document	single-doc	novice	2025-05-12T21:52:51.670698	5	54	1004
94	multispectral imaging modern ancient writing reveal	Multispectral imaging is used to reveal hidden text and figures in ancient documents. At the British Library, this technology was used to uncover a hidden figure in Leonardo da Vinci's notebook by exposing it to different wavelengths of light. Similarly, this technique has been crucial in deciphering palimpsests - ancient manuscripts where text was scraped off and rewritten - helping scholars recover more than 80% of previously unreadable texts in works like the Archimedes Palimpsest.	"['29 Ene Revelan una figura oculta en uno de los cuadernos de Leonard Da Vinci\nMultispectral imaging at the British Library has revealed a figure, in previously unseen detail, on a folio of a notebook belonging to Leonardo da Vinci. Da Vinci expert Professor Martin Kemp believes the sketch may be part of a series of ‘fugitive images’ occasionally unearthed on da Vinci’s work.\nLeonardo da Vinci (1452 – 1519) was a prolific note-taker with over 7,000 pages of his thoughts surviving today. The British Library is custodian of a notebook known as Codex Arundel 263 after its English collector Thomas Howard, 14th Earl of Arundel. These notes and sketches come from different periods in da Vinci’s life, though most date to 1508, and cover a range of topics including mechanics, astronomy, optics, architecture and the flight of birds.\nCodex Arundel 263 was not originally a bound volume, but was put together after his death. The variation in folio type and size show how many of da Vinci’s ideas, studies and inventions were observed outdoors as he went about his day. The notes are written in Italian showcasing his famous left-handed mirror writing. Folios 137v and 136r, housed together and currently on display in the British Library Treasures Gallery, were taken for multispectral imaging analysis to enhance and potentially reveal a hidden sketch in a small area of discolouration visible in the lower half of folio 137v.\nThe smudge measures no more than 6 x 3 cm and has been suspected by scholars to contain an elusive sketch of a figure by da Vinci – possibly erased by himself.\nThe analysis took place at the British Library Centre for Conservation where high resolution images of the folios and region of interest were captured. Multispectral imaging is one of the many tools our Conservation Science team use to non-invasively and non-destructively increase the body of knowledge on collection items for scholars, curators and conservators. The da Vinci sketch was placed underneath the monochrome sensor camera and exposed to light of various wavelengths ranging from the ultraviolet at 365 nm to the near infrared at 1050 nm.\nThese wavelengths reside on what is known as the Electromagnetic Spectrum – a wide spectrum encompassing radio and X-rays. The human eye can only detect light within the visible region of this spectrum limiting our ability to see potentially faded or invisible information. Multispectral imaging therefore enables the capture of detail which we cannot see with the naked eye.\nFilters placed underneath the camera’s lens were also used in combination with the lights to capture images of fluorescence resulting in the generation of vivid images highlighting the fugitive figure on folio 137v. The images raise fascinating questions about why the figure was drawn here, and why great efforts were made to erase it.\nMultispectral imaging is an incredibly exciting process and has revealed many secrets from our collections to date, including recovering once thought lost text from the ‘Burnt Magna Carta’ last year. It is an incredible privilege to work with some of the world’s most valued treasures and subject experts. Everyday brings new discoveries to light (quite literally!) and the prospect of unlocking more secrets from the British Library’s vast and varied collections is thrilling.\nThe da Vinci folio can be viewed for free in the Treasures Gallery until the end of March. The entire notebook has been digitised and is available to view online.\nDr Christina Duffy (@DuffyChristina)\nFuente original: Britishlibrary.typepad.co.uk', '|This article needs additional citations for verification. (August 2014) (Learn how and when to remove this template message)|\nA palimpsest (//) is a manuscript page, either from a scroll or a book, from which the text has been scraped or washed off so that the page can be reused for another document. Pergamene (now known as Parchment) was made of baby lamb or kid skin (best made in ancient Pergamos) and was expensive and not readily available, so, in the interest of economy a pergamene often was re-used by scraping the previous writing. In colloquial usage, the term palimpsest is also used in architecture, archaeology, and geomorphology, to denote an object made or worked upon for one purpose and later reused for another, for example a monumental brass the reverse blank side of which has been re-engraved.\nThe word ""palimpsest"" derives from the Latin palimpsestus, which derives from the Ancient Greek παλίμψηστος (palímpsēstos, ""again scraped""),a compound word that literary means ""scraped clean and ready to be used again"". The Ancient Greeks used wax-coated tablets, like scratch-pads, to write on with a stylus, and to erase the writing by smoothing the wax surface and write again; this practice was adopted by Ancient Romans, who wrote (literally scratched on letters) on wax-coated tablets, which were reuseable; Cicero\'s use of the term ""palimpsest"" confirms such a practice.\nBecause parchment prepared from animal hides is far more durable than paper or papyrus, most palimpsests known to modern scholars are parchment, which rose in popularity in Western Europe after the 6th century. Where papyrus was in common use, reuse of writing media was less common because papyrus was cheaper and more expendable than costly parchment. Some papyrus palimpsests do survive, and Romans referred to this custom of washing papyrus.\nThe writing was washed from parchment or vellum using milk and oat bran. With the passing of time, the faint remains of the former writing would reappear enough so that scholars can discern the text (called the scriptio inferior, the ""underwriting"") and decipher it. In the later Middle Ages the surface of the vellum was usually scraped away with powdered pumice, irretrievably losing the writing, hence the most valuable palimpsests are those that were overwritten in the early Middle Ages.\nMedieval codices are constructed in ""gathers"" which are folded (compare ""folio"", ""leaf, page"" ablative case of Latin folium), then stacked together like a newspaper and sewn together at the fold. Prepared parchment sheets retained their original central fold, so each was ordinarily cut in half, making a quarto volume of the original folio, with the overwritten text running perpendicular to the effaced text.\nFaint legible remains were read by eye before 20th-century techniques helped make lost texts readable. To read palimpsests, scholars of the 19th century used chemical means that were sometimes very destructive, using tincture of gall or, later, ammonium bisulfate. Modern methods of reading palimpsests using ultraviolet light and photography are less damaging.\nInnovative digitized images aid scholars in deciphering unreadable palimpsests. Superexposed photographs exposed in various light spectra, a technique called ""multispectral filming"", can increase the contrast of faded ink on parchment that is too indistinct to be read by eye in normal light. For example, multispectral imaging undertaken by researchers at the Rochester Institute of Technology and Johns Hopkins University recovered much of the undertext (estimated to be more than 80%) from the Archimedes Palimpsest. At the Walters Art Museum where the palimpsest is now conserved, the project has focused on experimental techniques to retrieve the remaining text, some of which was obscured by overpainted icons. One of the most successful techniques for reading through the paint proved to be X-ray fluorescence imaging, through which the iron in the ink is revealed. A team of imaging scientists and scholars from the USA and Europe is currently using spectral imaging techniques developed for imaging the Archimedes Palimpsest to study more than one hundred palimpsests in the library of Saint Catherine\'s Monastery in the Sinai Peninsula in Egypt.\nA number of ancient works have survived only as palimpsests. Vellum manuscripts were over-written on purpose mostly due to the dearth or cost of the material. In the case of Greek manuscripts, the consumption of old codices for the sake of the material was so great that a synodal decree of the year 691 forbade the destruction of manuscripts of the Scriptures or the church fathers, except for imperfect or injured volumes. Such a decree put added pressure on retrieving the vellum on which secular manuscripts were written. The decline of the vellum trade with the introduction of paper exacerbated the scarcity, increasing pressure to reuse material.\nCultural considerations also motivated the creation of palimpsests. The demand for new texts might outstrip the availability of parchment in some centers, yet the existence of cleaned parchment that was never overwritten suggests that there was also a spiritual motivation, to sanctify pagan text by overlaying it with the word of God, somewhat as pagan sites were overlaid with Christian churches to hallow pagan ground. Or the pagan texts may have merely appeared irrelevant.\nTexts most susceptible to being overwritten included obsolete legal and liturgical ones, sometimes of intense interest to the historian. Early Latin translations of Scripture were rendered obsolete by Jerome\'s Vulgate. Texts might be in foreign languages or written in unfamiliar scripts that had become illegible over time. The codices themselves might be already damaged or incomplete. Heretical texts were dangerous to harbor – there were compelling political and religious reasons to destroy texts viewed as heresy, and to reuse the media was less wasteful than simply to burn the books.\nVast destruction of the broad quartos of the early centuries took place in the period which followed the fall of the Western Roman Empire, but palimpsests were also created as new texts were required during the Carolingian Renaissance. The most valuable Latin palimpsests are found in the codices which were remade from the early large folios in the 7th to the 9th centuries. It has been noticed that no entire work is generally found in any instance in the original text of a palimpsest, but that portions of many works have been taken to make up a single volume. An exception is the Archimedes palimpsest (see below). On the whole, Early Medieval scribes were thus not indiscriminate in supplying themselves with material from any old volumes that happened to be at hand.\n- The best-known palimpsest in the legal world was discovered in 1816 by Niebuhr and Savigny in the library of Verona cathedral. Underneath letters by St. Jerome and Gennadius was the almost complete text of the Institutes of Gaius, probably the first student\'s textbook on Roman law.\n- The Codex Ephraemi Rescriptus, Bibliothèque Nationale de France, Paris: portions of the Old and New Testaments in Greek, attributed to the 5th century, are covered with works of Ephraem the Syrian in a hand of the 12th century.\n- The Sana\'a palimpsest is one of the oldest Qur\'anic manuscripts in existence. Carbon dating indicates that the undertext (the scriptio inferior) was written probably within 15 years before the death of the Islamic prophet Muhammad. The undertext differs from the standard Qur\'anic text and is therefore the most important documentary evidence for the existence of variant Qur\'anic readings.\n- Among the Syriac manuscripts obtained from the Nitrian desert in Egypt, British Museum, London: important Greek texts, Add. Ms. 17212 with Syriac translation of St. Chrysostom\'s Homilies, of the 9th/10th century, covers a Latin grammatical treatise from the 6th century.\n- Codex Nitriensis, a volume containing a work of Severus of Antioch of the beginning of the 9th century, is written on palimpsest leaves taken from 6th-century manuscripts of the Iliad and the Gospel of Luke, both of the 6th century, and the Euclid\'s Elements of the 7th or 8th century, British Museum.\n- A double palimpsest, in which a text of St. John Chrysostom, in Syriac, of the 9th or 10th century, covers a Latin grammatical treatise in a cursive hand of the 6th century, which in its turn covers the Latin annals of the historian Granius Licinianus, of the 5th century, British Museum.\n- The only known hyper-palimpsest: the Novgorod Codex, where potentially hundreds of texts have left their traces on the wooden back wall of a wax tablet.\n- The Ambrosian Plautus, in rustic capitals, of the 4th or 5th century, re-written with portions of the Bible in the 9th century, Ambrosian Library.\n- Cicero, De republica in uncials, of the 4th century, the sole surviving copy, covered by St. Augustine on the Psalms, of the 7th century, Vatican Library.\n- Seneca, On the Maintenance of Friendship, the sole surviving fragment, overwritten by a late-6th century Old Testament.\n- The Codex Theodosianus of Turin, of the 5th or 6th century.\n- The Fasti Consulares of Verona, of 486.\n- The Arian fragment of the Vatican, of the 5th century.\n- The letters of Cornelius Fronto, overwritten by the Acts of the Council of Chalcedon.\n- The Archimedes Palimpsest, a work of the great Syracusan mathematician copied onto parchment in the 10th century and overwritten by a liturgical text in the 12th century.\n- The Sinaitic Palimpsest, the oldest Syriac copy of the gospels, from the 4th century.\n- The unique copy of a Greek grammatical text composed by Herodian for the emperor Marcus Aurelius in the 2nd century, preserved in the Österreichische Nationalbibliothek, Vienna.\n- Codex Zacynthius – Greek palimpsest fragments of the gospel of Saint Luke, obtained in the island of Zante, by General Colin Macaulay, deciphered, transcribed and edited by Tregelles.\n- The Codex Dublinensis (Codex Z) of St. Matthew\'s Gospel, at Trinity College, Dublin, also deciphered by Tregelles.\n- The Codex Guelferbytanus 64 Weissenburgensis, with text of Origins of Isidore, partly palimpsest, with texts of earlier codices Guelferbytanus A, Guelferbytanus B, Codex Carolinus, and several other texts Greek and Latin.\nAbout sixty palimpsest manuscripts of the Greek New Testament have survived to the present day. Uncial codices include:\nPorphyrianus, Vaticanus 2061 (double palimpsest), Uncial 064, 065, 066, 067, 068 (double palimpsest), 072, 078, 079, 086, 088, 093, 094, 096, 097, 098, 0103, 0104, 0116, 0120, 0130, 0132, 0133, 0135, 0208, 0209.\n- Petroglyphs of Arpa-Uzen – rock art from the Bronze and Iron Ages later covered by Saka pictorials\n- Lyons, Martyn (2011). Books: A Living History. California: J. Paul Getty Museum. p. 215. ISBN 978-1-60606-083-4.\n- According to Suetonius, Augustus, ""though he began a tragedy with great zest, becoming dissatisfied with the style, he obliterated the whole; and his friends saying to him, What is your Ajax doing? He answered, My Ajax met with a sponge."" (Augustus, 85). Cf. a letter of the future emperor Marcus Aurelius to his friend and teacher Fronto (ad M. Caesarem, 4.5), in which the former, dissatisfied with a piece of his own writing, facetiously exclaims that he will ""consecrate it to water (lymphis) or fire (Volcano),"" i.e. that he will rub out or burn what he has written.\n- ""In the Sinai, a global team is revolutionizing the preservation of ancient manuscripts"". Washington POST Magazine. September 8, 2012. Retrieved 2012-09-07.\n- The most accessible overviews of the transmission of texts through the cultural bottleneck are Leighton D. Reynolds (editor), in Texts and Transmission: A Survey of the Latin Classics, where the texts that survived, fortuitously, only in palimpsest may be enumerated, and in his general introduction to textual transmission, Scribes and Scholars: A Guide to the Transmission of Greek and Latin Literature (with N.G. Wilson).\n- The Institutes of Gaius, ed W.M. Gordon and O.F. Robinson, 1988\n- Sadeghi, Behnam; Goudarzi, Mohsen (March 2012). ""Ṣan\'ā\' 1 and the Origins of the Qur\'ān"". Der Islam. Retrieved 2012-03-26.\n|Wikisource has the text of the 1911 Encyclopædia Britannica article Palimpsest.|\n- OPIB Virtual Renaissance Network activities in digitizing European palimpsests\n- Brief note on economic and cultural considerations in production of palimpsests\n- PBS NOVA: ""The Archimedes Palimpsest"" Click on ""What is a Palimpsest?""\n- Rinascimento virtuale a project for the census, description, study and digital reproduction of Greek palimpsests\n- Ángel Escobar, El palimpsesto grecolatino como fenómeno librario y textual, Zaragoza 2006']"	['<urn:uuid:758d860c-c972-4f3f-b64f-ea861c5075a5>', '<urn:uuid:9ce8a3f8-409a-4196-ab49-cd6f73315114>']	factoid	direct	short-search-query	distant-from-document	multi-aspect	novice	2025-05-12T21:52:51.670698	6	75	2604
95	Between ankle strengthening and knee strengthening workouts, which one requires more equipment to do a full routine at home?	Ankle strengthening requires less equipment, needing only a jump rope, resistance bands, and a kettlebell with a bench. In contrast, knee strengthening exercises require more equipment including resistance bands, medicine balls, foam rollers, chairs or tables for support, ankle weights, and specialized equipment or a partner for exercises like the Norwegian hamstring curl.	"['Ankle injuries are common in many different sports and often keep athletes sidelined and out of action. In today’s video, I’m going to show you some simple but very effective ways to strengthen your foot and ankle to help prevent injuries from happening\nOne of the first steps you can take to protect yourself from ankle injury is to strengthen and improve elasticity of the entire ankle joint complex. You can do this by simply using a jump rope.\nPrevent Ankle Injuries By Jumping Rope\nBegin the warm up barefoot. Low-level barefoot hopping will make a big difference in strengthening your ankles and it’s part of the reason you see so few MMA athletes with ankle problems; they train barefoot all the time.\nA few pointers regarding technique:\nYou should be jumping on the balls of your feet\nBe quick to jump; don’t spend a lot of time on the ground between jumps\nFocus on having a fluid rebound between jumps\nOnce you’ve got the basic technique down, you can start to mix things up…\nYou can do single-leg jumps of 60 sec/leg or shorter interval hops\nYou can add forward/backward movement to your jumps\nYou can do lateral hops or dot drills\nThere are lots of variations you can try, each of which will help to strengthen the ankle in different ways to prevent injury.\nGenerally, we recommend using some sort of jump rope variation over a 3-5 minute period during the warm up.\nMake sure you are on some sort of rubberized or field-turf surface that will minimize the pounding on your joints.\nNext, let’s take a look at the bands…\nUsing Bands to Stop Ankle Injuries\nBand exercises are another great way to improve ankle strength during warm up or cool down and the bands themselves are just a few dollars.\nIt’s important to make sure that you have a band that will provide sufficient tension, otherwise you won’t strengthen the ankle very much.\nMake sure you wrap the band around the top of the foot in a complete circle.\nPull the band to give quite a bit of tension and conduct dorsiflexion. Make sure there is even tension throughout the band and that you’re going through the full range of motion.\nThis exercise should feel fatiguing; it’s not an easy exercise to perform.\nWe generally do 15-20 of these dorsiflexion exercises before moving onto other banded movements.\nNext, release the tension from the outside of the band and pull hard on the inside of the band; you should focus on rotating your foot away from you.\nYou should feel this exercise working on the outer part of your calf.\nNext you can pull on the outside of the band and release tension on the inside of the band and begin rotating your foot toward you.\nYou’re trying to work as many muscles and ranges of motion throughout the ankle/foot complex as possible.\nAnother exercise involves pulling on both bands, bringing the kneed up to 90 degrees, and pushing the foot straight down and back through a full range of motion.\nShoot for anywhere from 15-20 reps.\nAs you first start out, you’ll generally do only 1 set of each of these banded exercises as you warm up or cool down.\nAs your ankles get stronger, you can progress to 2-3 sets of each exercise. You can also increase the size of the band to provide more tension.\nOur last exercise will be the kettlebell foot lift…\nUsing Kettlebells To Prevent Ankle Injuries\nThis is an old school Russian exercise, one of the original exercises the Russians used kettlebells for.\nThe idea is that you’ll be getting ankle work on two levels:\n1.) work on the foot that is actually lifting the kettlebell up\n2.) stability work on the ankle that is on the ground keeping you upright\nTo perform the exercise, lift the kettlebell up onto a bench using one foot and then lower the kettlebell back down to the floor.\nTo make things more challenging, you can increase the height of the bench, increase the weight of the kettlebell, or increase the distance from the bench.\nThis is an exercise you can incorporate into your warm up with 2-3 sets of 10-20 reps each leg.\nIf you perform all of these exercises together, they will go a long way toward injury prevention and force generation.\nThis doesn’t have to take up a lot of time; it can be part of your warm up, cool down, or accessory lifts. Yet this is something that you can incorporate into your training almost daily to see stronger ankles, injury prevention, better mobility, and better strength/power.', ""Knee, Thigh & Hamstring Exercises\nWe have ankle exercises for rehabilitation of specific injuries. We have also categorized them into early, mid and late stage exercises although this is only a guide and we recommend seeking professional advice.\nExercises for specific injuries\nThese exercises are often done as soon as possible after injury if pain will allow. The aim is to restore range of motion without putting any damaged tissues under stress. The exact exercises and how quickly you progress through will depend on the type and severity of injury. Active mobility exercises where the athlete physically attempts to move the joint through a range of motion are often the first step.\nThis is a knee mobility exercise to increase the range of knee flexion at the joint. It is suitable for early stage rehabilitation after injuries and surgery. The athlete lies on their back on a hard surface. The heel is slowly moved up towards the buttocks, as far as is comfortable(socks can be worn to ensure that the foot slides). After a minute or so, further movement may be possible. A towel or strap wrapped around the ankle can be used to help in the very early stages.\nAssisted knee flexion\nThis exercise helps to increase the range of knee flexion available at the joint. It is designed for the early stages of rehabilitation after a knee injury or surgery. The athlete uses the other leg to gently push back on the lower leg, increasing knee flexion as far as possible.\nProlonged knee extension\nThis exercise is used to help regain full knee extension. Often after a severe knee injury or after surgery it is not possible to fully straighten the knee. The athlete may sit with the foot rested and the knee unsupported. Gravity will help encourage extension, or a weight can be placed just above the knee to add extra force. The position is held for a few minutes as long as it isn’t painful.\nProlonged knee flexion\nThis exercise is used to increase knee flexion. Sometimes after a knee or thigh injury or after surgery on this area it is not possible to fully bend the knee. The athlete is seated, with padding on the lower leg and a strap around the lower leg, wrapped around the back of the chair and the end held in the hands. The athlete pulls the strap until a tight feeling is felt on the knee/thigh. This position is held for a few minutes before attempting to increase the stretch.\nEarly strengthening exercises\nThese exercises are done as soon as pain allows. In some cases within a day or so of injury after the acute stage.\nIsometric quad exercises aim to strengthen the quads by contracting the muscle, with no, or very little movement of the knee joint. The athlete can be sitting or supine depending on the degree of injury. Being seated increases the difficulty. Keeping the uninvolved knee in place, the athlete tightens the involved knee pushing it into the table.\nIsometric quad prone\nThis exercise strengthens the quads at the front of the thigh. It is for the very early stages of a knee injury or quad strain. To begin strengthening the quad muscles at the front of the thigh the athlete lies on their front with a rolled up towel under the ankle so the knee is very slightly bent. They then push down on the towel to attempt to straighten the leg and contract the quads.\nIsometric hamstring exercises\nStatic or isometric hamstring exercises can be used in the early stages of rehabilitation for a knee injury or a hamstring strain to help prevent muscle wasting. The athlete lieson their front with the knee slightly bent. The therapist grasps around the back of the ankle. The athlete tries to bend their knee against the therapists resistance. Start with a gentle contraction and gradually increase force as pain allows. The knee should not move.\nSit to stand exercise\nThis is a simple exercise that works the quadriceps in the early stages of rehabilitation after a knee injury. It is also helpful for the elderly to maintain quad strength. The athlete sits with the knees bent and feet directly under the knees. In a slow and controlled manner, the athlete moves from seated to standing and then back to seated as shown. Ensure the knees do not fall inwards.\nMid stage knee exercises\nDuring the mid stage exercises progress to gentle strengthening, gradually increasing the load on the joint and through the recovering tissues. Balance and proprioception training usually begins.\nThe wall squat exercise is an slightly easier alternative exercise to the squat. By using the wall some of the body weight is supported. The athlete stands with their back against a wall and the feet moved forwards. They perform a squat by sliding the back down the wall and ensuring that the knees do not move forwards past the toes. The squat position can be held for added difficulty, or performed on a single leg only.\nTerminal knee extension\nThis exercise increases the weight-bearing strength of the quadriceps. A resistance band is wrapped around the knee and anchored to a table leg or similar upright object. The athlete starts with the knee slightly bent and body weight on the involved leg. The knee is then straightened backwards, against the resistance of the band. The knee should not be locked straight.\nStanding hamstring curl\nStanding single leg hamstring curl (leg curl). Start slowly then get faster as your gain in confidence. The athlete stands and flexes the involved knee. Ankle weights can be used to increase difficulty or offer resistance with the hands, or incorporate a resistance band. The athlete may use the hands on stable object to support the body.\nSquat with band\nThe resistance band provides lateral or sideways resistance to add another dimension to the squat exercise. Starting with the feet shoulder width apart, the athlete squats down to no more than a right angle at the knee. The knees should not fall inwards and the back should remain straight throughout. This can be performed with a bar over the shoulders or dumbbells in the hands.\nThe squat is a great exercise to work most of the leg muscles, especially the quads and glutes. Stand with your feet shoulder width apart and toes pointing straight forwards. Keep the back straight as you initiate movement at your hips. Push your buttocks out behind you and bend your knees. Do not let your knees move in front of your toes or squat deeper than a 90 degree (right angle) at the knee. Start with shallow squats and increase gradually.\nStraight leg ball pick up\nThe straight leg ball pick up strengthens the hamstring muscles in a stretched position. The athlete stands with the heel of the involved leg raised. The uninvolved leg is moved back to provide balance. Ensuring that the back is kept straight the athlete bends to pick up the medicine ball. All motion involves the pelvis moving around the femurs rather than lumbar flexion.\nThe Plie is a wide squat exercise with the knees pointing outwards. The back should remain straight during the exercise and the pelvis should not til backwards. The athlete stands with the feet turned out. The knees should be bent as if performing a squat, ensuring they do not move forwards past the toes.\nKnee extension with band\nKnee extension exercise (or leg extension) using a resistance band to strengthen the thigh muscles. The athlete sits on the edge of the table (or on a chair) with the knees over the edge. The resistance band is placed around the ankle and anchored under the furthest table/chair leg on the side of the leg being worked on. The athlete lifts the foot upwards to straighten the knee, then returns to the starting position. If pain is felt do not continue with this exercise.\nSeated hamstring curl\nSeated or supine hamstring curl exercise using a resistance band. The athlete sits with a resistance band around the ankle with both legs straight. A partner holds the band in both hands until it is taut. The partner must not move the band from the starting position. The athlete draws the ankle in towards the buttocks increasing the resistance of the band, then returns to the starting position.\nThis exercise works the hamstring muscles and can be progressed to use weights depending on the state of the injury. Lying on their front with the foot pointing down over the edge of the couch, the athlete fully bends the knee. Provided this is pain free, a resistance band or ankle weight can be used to increase difficulty.\nSingle leg catch exercises for hamstrings. This starts to strengthen the hamstrings eccentrically or as they lengthen. In a prone position the athlete lifts both legs to a 90 degree angle. Ensuring that the leg and the foot are not turned outwards the athlete drops the leg attempting to stop or 'catch' the lower leg reaching full extension.\nLate stage knee exercises\nThese exercises are more functional and sports specific. The aim is to restore full strength and mobility to the joint and return the athlete to full training and competition.\nLunge with ball\nA ball can be used with a lunge to help with balance and to add extra weight. The athlete stands with the injured leg a wide stance in front of the other. The athlete holds a medicine ball close to the chest With the weight shifted onto the front leg, the back knee is slowly bent and dropped down towards the floor.\nLunge on step\nBy raising the uninvolved leg on a step the athlete adds more weight to the leg being worked on. The athlete stands with the injured leg a wide stance in front of the other. The uninvolved leg is raised on a step with weight on the toes. With the weight shifted onto the front leg, the back knee is slowly bent and dropped down towards the floor.\nThis exercise is ideal for not only strengthening the muscles of the lower extremity but also for burning calories! The athlete stands with the injured leg a wide stance in front of the other. With the weight shifted onto the front leg, the back knee is slowly bent and dropped down towards the floor. The lunge position may be held to increase difficulty.\nBy using a medicine ball in the lateral lunge (side lunge) the athlete is able to add weight to the exercise as well as using it to aid balance. The athlete steps to the side keeping the toes forwards and the feet flat. Whilst keeping the involved leg straight, squat through the hip of the involved leg ensuring that the knee is in line with the foot. The athlete holds the ball out to help maintain balance. Squat as low as possible and hold for 2 seconds. Push back to the starting position.\nEccentric squat knee exercises target the hamstrings, glutes and quads. The athlete raises the heels using half a foam roller. Keeping the back straight the athlete lowers themselves down slowly. The athlete returns to the starting position then repeats.\nNorwegian hamstring curl\nThe Norwegian hamstring curl (or nordic curl) requires either a partner or gym equipment to lock the lower legs securely. This is a very advanced exercise isolating hamstring muscles. A partner anchors the athlete's calves. A straight line must be maintained from knee to shoulder. The athlete lowers the body as controlled as possible to the floor. At the point whereby the move becomes uncomfortable the athlete lets the body fall to the floor using the hands to control their landing.""]"	['<urn:uuid:c79bd842-6f8c-46ae-8503-b682e863f272>', '<urn:uuid:525cb522-3094-4781-901c-059e74022a71>']	factoid	with-premise	verbose-and-natural	distant-from-document	comparison	novice	2025-05-12T21:52:51.670698	19	53	2735
96	Why do we need the medulla to stay alive?	The medulla oblongata oversees essential involuntary functions including respiration, heart rhythm, blood vessel function, digestion, sneezing and swallowing. Without it, we wouldn't be able to breathe automatically and our hearts would stop when we went to sleep.	['Our brains are arguably the single most complex system known to man. Together with the spinal cord, the brain makes up the central nervous system, which controls every aspect of our lives; from breathing to solving math problems.\n3 main sections comprise the brain: the forebrain, midbrain and the hindbrain. Each of these sections works together as well as independently to ensure we stay alive, know when we are hungry and help up stand up.\nWe will highlight these sections and explain their functions.\nThe forebrain is the largest part of the brain. It further divides into the cerebrum, thalamus and the hypothalamus. This section of our brain controls higher brain functions such as thought and action.\nAlso called the cerebral cortex, when you think about what a brain looks like, you usually picture the cerebrum. The cerebral cortex is again, further divided into 4 parts, or lobes:\n- Frontal lobe – which gives us reasoning, problem-solving, emotions, speech and problem solving/planning.\n- Occipital lobe – This gives us visual processing.\n- Parietal lobe – This lobe controls our motor skills so we can move, equilibrium and our perception of external stimuli.\n- Temporal lobe – The temporal lobe is associated with hearing, memories and speech.\nCortical folding (the wrinkles and folds in the gray matter of these lobes) allow us to have thoughts, be able to talk and understand language, use fine motor skills and use reason and logic. The more folds, the larger the available surface area to have and use these functions.\nThe cerebrum divides into 2 halves: the left and right hemispheres. The right hemisphere controls our creative and artistic abilities. Without this hemisphere, we wouldn’t have art, music, dance or poetry.\nThe left hemisphere controls reason and logic. This allows us to have reason, make decisions and solve problems. Without this hemisphere, we wouldn’t be able to recall facts and figures or understand mathematics and science.\nThe thalamus is the sensory never center of our brain. It receives sensory input from almost every part of our body. These signals travel through our nervous system to the thalamus which then directs the signal to the correct section of the cerebral cortex.\nThe thalamus allows us to know when we touch something and how it feels, pain and pleasure, as well as temperature.\nSmell is the only sensory stimulus that is not passed through the thalamus.\nWithout the thalamus, we wouldn’t know if a stove was hot or if ice was cold. We wouldn’t be able to understand what our eyes see or what our ears hear.\nThe hypothalamus is mainly involved in motivational behaviors. We understand that we are hungry, thirsty or tired because of the hypothalamus. It controls our circadian rhythm (sleep cycles) as well as emotions.\nThe hypothalamus houses the pituitary gland. With this pituitary gland and the hypothalamus, the body produces hormones. These hormones control emotions and sex drive, our mood and temperature regulation.\nThe hypothalamus is responsible for regulating our body temperature and creating homeostasis. This homeostatic pathway allows us to survive by making small changes that prevent us from freezing or starving. Regardless of changes to external stimulus such as weather, external temperature or diet.\nThe midbrain is made up of the tectum and the tegmentum. Together these work with the thalamus and hypothalamus to control alertness, hearing, vision, temperature control, emotion and sleep cycles.\nThe tectum has two primary functions: processing visual stimuli and auditory processing stimuli.\nThe tectum controls eye movement and processes the optical signals, routing them to the cerebral cortex for interpretation. It also processes the signals sent in by our ears and allows us to understand and recognize auditory signals into sounds and words.\nThe tegmentum works in conjunction with the hypothalamus to maintain homeostasis and reflexive pathways. These paths allow us to regulate our body temperature and redirect changes in our body to ensure we survive.\nAlong the reflexive pathways, the tegmentum ensures we do not have any unwanted muscle movements by sending inhibitory signals to the thalamus.\nThe hindbrain is made up of 3 main sections: cerebellum, pons and the medulla oblongata. The primary function of the hindbrain is to regulate and control involuntary actions such as breathing and heartbeat.\nThe cerebellum is responsible for the coordination of movement, balance and posture. The cerebellum takes in signals sent from the spinal cord and allows us to fine tune motor skills. We can grasp and sit and stand because of the cerebellum.\nThe cerebellum does not initiate movement but instead controls the precision of fine movements, compound movements and timing.\nThe pons is a signal super-highway. It connects the upper and lower portions of the brain and acts as a traffic cop for the incoming and outgoing signals.\nAside from its main attribute of brain communication, it also allows us to dream. The pons is responsible for our rapid eye movement (REM) sleep cycle, where we subconsciously dream.\nWithout the pons, our brains wouldn’t function as no signals would be sent or received.\nThe medulla oblongata (also called simply the medulla) is responsible for the clear majority of involuntary functions.\nThe medulla helps signals pass to and from the spinal cord and the thalamus. Moreover, it oversees our respiration, heart rhythm, blood vessel function, digestion, and even sneezing and swallowing.\nWithout the medulla oblongata, we wouldn’t live. We wouldn’t be able to breathe without thinking about it and our hearts would stop when we went to sleep.\nIt Just Works\nEvery section of the brain has a job to do. While each section is an independent work horse allowing us to do everything we need to do as well as understanding the world around us, they also work together.\nOne section couldn’t function without the other. Working in tandem as well as individually allows us to function, reason, think, learn, eat, digest and breathe. Our brain is an amazing and complex system. We couldn’t live without it.']	['<urn:uuid:f628aadf-94f8-4f25-8132-c5bbcb2c7bc4>']	factoid	direct	concise-and-natural	distant-from-document	single-doc	novice	2025-05-12T21:52:51.670698	9	37	992
97	I've heard about some mental condition that makes people afraid to leave their homes - what are the main signs that someone might have this, and what should they do first to get help?	The condition is called agoraphobia, and its main signs include fear of open spaces, crowded places, and situations where escape might be difficult. People may experience physical symptoms like chest pain, racing heart, shortness of breath, and feelings of detachment from others. They may become overly dependent on others or stay housebound for long periods. The first step to get help is to talk to a primary care doctor, who can evaluate the symptoms and potentially refer to a mental health provider. It's important to prepare for the appointment by making a list of symptoms, triggers, and any major life stresses or changes.	"[""If you've made the choice to seek help for a phobia, you've taken a huge first step. Start by talking to your primary care doctor. Depending on your situation, you may be referred to a mental health provider for evaluation and treatment.\nWhat you can do\nThese suggestions can help you get the most from your appointment.\n- Make a list of the symptoms you're experiencing, even if they seem unrelated to your anxiety. Phobias generally cause both physical and psychological distress. Note what triggers your anxiety, how you've tried to deal with it, and factors that make it better or worse.\n- List key personal information, including any major stresses or recent life changes.\n- Make a list of all medications, vitamins, herbal products or other supplements that you're taking, as well as the dosages.\nPrepare questions to ask your doctor ahead of time, such as:\n- What might have caused me to develop this fear?\n- Is this something that will go away on its own? Or is there anything I can do on my own to improve my symptoms? For instance, if I'm afraid to fly, should I force myself to fly anyway?\n- What treatments do you recommend for this disorder?\n- What are the side effects of medications commonly used for this condition?\n- If I decide to take medications, how long will it take for my symptoms to improve?\n- If the first medication I try isn't effective, what will you recommend next?\n- Would talk therapy help me?\n- How much improvement can I expect if I follow your recommended treatment plan?\n- I have other health conditions. How can I best manage them together?\nDon't hesitate to ask questions at any time during your appointment.\nWhat to expect from your doctor\nBe ready to answer your doctor's questions so you have time to focus on points you want to talk about in-depth. Your doctor may ask these questions:\nFeb. 08, 2014\n- Have you recently had a spell or an attack when all of a sudden you felt frightened, anxious or very uneasy?\n- Would you say that you have recently been feeling nervous, anxious or on edge?\n- During these attacks of fear and anxiety, have you ever felt like you couldn't breathe or like you were having a heart attack?\n- What other symptoms do you have?\n- When did you first notice these symptoms?\n- When are your symptoms most likely to occur?\n- Does anything seem to make your symptoms better or worse?\n- Do you avoid any situations or places because you fear they'll trigger your symptoms?\n- How are your symptoms affecting your life and the people closest to you?\n- What else concerns you that we haven't yet talked about?\n- Have you been diagnosed with any medical conditions?\n- Have you been treated for other psychiatric symptoms or mental illness in the past? If yes, what type of therapy was most beneficial?\n- Have you ever thought about harming yourself?\n- Do you drink alcohol or use illegal drugs? How often?\n- Anxiety disorders. In: Diagnostic and Statistical Manual of Mental Disorders DSM-5. 5th ed. Arlington, Va.: American Psychiatric Association, 2013. http://www.psychiatryonline.com. Accessed July 25, 2013.\n- Phobias. American Psychiatric Association. http://www.psychiatry.org/phobias. Accessed July 22, 2013.\n- Phobic disorders. The Merck Manuals: The Merck Manual for Health Care Professionals. http://www.merckmanuals.com/professional/psychiatric_disorders/anxiety_disorders/phobic_disorders.html?qt=phobic%20disorders&alt=sh. Accessed July 29, 2013.\n- Anxiety disorders. National Institute of Mental Health. http://www.nimh.nih.gov/health/publications/anxiety-disorders/index.shtml. Accessed July 29, 2013.\n- Augustyn M. Overview of fears and specific phobias in children. http://www.uptodate.com/home. Accessed July 29, 2013.\n- Whiteside SP (expert opinion). Mayo Clinic, Rochester, Minn. Aug. 27, 2013.\n- Hall-Flavin DK (expert opinion). Mayo Clinic, Rochester, Minn. Aug. 29, 2013.\nYou Are ... The Campaign for Mayo Clinic\nMayo Clinic is a not-for-profit organization. Make a difference today."", 'The word agoraphobia means ""a fear of wide, open spaces."" The word originates from the ancient Greek word ""agora,"" referring to a place of assembly or market place.\nAgoraphobia is an anxiety disorder involving anxiety and intense fear of any situation where escape may be difficult, or where help may not be available. It often involves a fear of crowds, bridges or of being outside alone.1\nAbout 1.8 million American adults aged over 18 (about 0.8% of adults) have agoraphobia without a history of panic disorder.2 This is about 0.8 % of people in this age group in a given year. The average age of onset is 20 years.3\nContents of this article:\nYou will also see introductions at the end of some sections to any recent developments that have been covered by MNT\'s news stories. Also look out for links to information about related conditions.\nFast facts on agoraphobia\nHere are some key points about agoraphobia. More detail and supporting information is in the main article.\n- It is not known precisely what causes agoraphobia.\n- Agoraphobia often develops after having one or more panic attacks.\n- Agoraphobia can make it extremely difficult for a person to leave their house.\n- Agoraphobia can lead to various fears, such as the fear of open spaces and the fear of places where escape is difficult, such as elevators.\n- People with agoraphobia can experience feelings of helplessness and loss of control.\n- People with agoraphobia may experience physical symptoms of a panic attack such as chest pains, dizziness and shortness of breath.\n- Agoraphobia is usually diagnosed following an interview with a health care provider.\n- Agoraphobia is often treated medically with antidepressants or anxiety-reducing medicine.\n- Agoraphobia is also commonly treated with psychotherapy such as cognitive behavioral therapy.\n- Most people with agoraphobia can get better through treatment.\nWhat is agoraphobia?\nAn anxiety disorder is when a feeling of anxiety does not go away and tends to grow worse over time.4 One type of anxiety disorder is a panic disorder, where panic attacks and sudden feelings of terror can occur without warning.5\nAgoraphobia can lead to a feeling of detachment from others, as well as fear of open spaces.\nAgoraphobia is one such panic disorder. Agoraphobia panic attacks are linked with a fear of places where it is hard to escape or where help may not be available. Places that can induce agoraphobia include those that can make a person feel embarrassed, helpless or trapped, such as crowded areas, bridges, public transport and remote areas.\nMost people develop agoraphobia after having had one or more panic attacks, causing them to fear further attacks and avoid the situation in which the attack occurred.6\nPeople with agoraphobia may need help from a companion in order to go to public places, and may at times feel unable to leave home.\nWhat causes agoraphobia?\nThe cause or causes of agoraphobia and other panic disorders remain unknown, but it is thought that areas of the brain that control the fear response may play a role, as could environmental factors. As there is evidence that anxiety disorders run within families, it also seems likely that genetic factors play a role in agoraphobia and other panic disorders.7\nAgoraphobia also sometimes occurs after a person has had one or more panic attacks and begins to fear situations that could potentially lead to panic attacks in the future. Other panic disorders or phobias can play a developmental role.8\nRecent developments on agoraphobia causes from MNT news\nResearchers from the University of Missouri and the University of Nebraska-Lincoln discovered that people with the genotype 5-HTTLPR - linked to higher social anxiety - were less likely to engage in prosocial behavior, compared with those missing this genotype.\nSigns and symptoms15910\nAgoraphobia can manifest itself as a combination of fears, feelings and bodily symptoms.\nPeople with agoraphobia can become housebound for long periods of time.\nCommon fears associated with agoraphobia are:\n- Fear of spending time alone\n- Fear of being in crowded places\n- Fear of open spaces\n- Fear of being in places where escape might be hard, such as public transport and elevators\n- Fear of losing control in a public place\n- Fear of death.\nA person with agoraphobia may experience the following feelings:\n- Detachment from others\n- Loss of control\n- As though the body is not real\n- As though the environment is not real.\nPeople with agoraphobia may become overly dependent on others, or remain housebound for long periods of time. In addition to these symptoms, people with agoraphobia can also experience the physical symptoms of panic attacks, such as:\n- Chest pain or discomfort\n- Racing heart\n- Shortness of breath\n- Upset stomach, nausea and diarrhea\n- Flushing and chills\nPeople who experience panic attacks may change their behavior and how they function at home, school or work. They may try to avoid situations that could trigger off further attacks. They may become sad, depressed or suicidal, and in some cases may abuse alcohol and other drugs.\nA Swiss study recently found that compared to similar people without agoraphobia, people with agoraphobia had significantly higher levels of markers for inflammation (including C-reactive protein and tumor-necrosis-factor-α), and lower levels of adiponectin (a marker for cardioprotection).16\nLevels of low-grade inflammation also appeared to increase over time in people with agoraphobia, suggesting that those suffering from the condition may have a higher risk of atherosclerosis and coronary heart disease.\nTests and diagnosis111\nAgoraphobia is usually diagnosed following an interview with a health care provider, who assesses signs and symptoms. Descriptions of behavior may be requested from family or friends. In addition, a physical exam may be suggested to rule out other conditions that could potentially be causing the symptoms.\nThe Diagnostic and Statistical Manual of Mental Disorders (DSM-5) published by the American Psychiatric Association is used to diagnose agoraphobia and other mental health conditions in the US. It is used by health care providers for this purpose, as well as by insurance companies to reimburse any treatment.\nExtreme fear regarding being on public transport is one of the diagnostic criteria for agoraphobia.\nThe diagnostic criteria for agoraphobia within DSM-5 include anxiety or extreme fear regarding being in at least two of the following situations, characterized by being difficult to escape from or find help in:\n- On public transport\n- In an open space\n- In an enclosed space\n- In a crowd or queue\n- Out of home alone.\nAdditional diagnostic criteria for agoraphobia include the following:\n- Fear or anxiety caused almost always by exposure to a particular situation\n- Fear or anxiety out of proportion to the actual danger of a particular situation\n- Avoidance of a particular situation or requiring a companion in order to deal with it\n- Endurance of a situation with extreme distress\n- Distress or problems in areas of life caused by fear, anxiety or avoidance\n- Long-term persistent phobia and avoidance.\nAgoraphobia is usually treated with a combination of medication and psychotherapy. Most people with agoraphobia can get better through treatment, though it can become more difficult to treat if people do not get early, effective help.\nHealth care providers can prescribe either one or both of the following types of medication.\nA group of antidepressants called selective serotonin reuptake inhibitors (SSRIs) can be prescribed to treat panic disorder with agoraphobia. Other types of antidepressants can also be prescribed, but they have a greater risk of side effects than SSRIs.\nAnti-anxiety medications, also known as benzodiazepines, are sedatives that can relieve the symptoms of anxiety on a short-term basis. Benzodiazepines are a habit-forming medication.\nA health care provider is likely to increase the dosage of prescribed antidepressants when treatment begins, and slowly decrease the dosage when the treatment is ready to finish. This caution is because starting and ending a course of antidepressants can sometimes lead to side effects that are similar to a panic attack.\nPsychotherapy involves working with a therapist in order to reduce symptoms of anxiety and make a person with agoraphobia feel and function better.\nOne of the most common and effective forms of psychotherapy for anxiety disorders, including agoraphobia, is cognitive-behavioral therapy (CBT). It focuses on changing the thoughts that cause the condition. It can involve learning the following:\nPsychotherapy is often used to treat agoraphobia. Sessions can sometimes be carried out at the patient\'s home to help.\n- That it is unlikely that fears will come true\n- That anxiety decreases over time and that symptoms can be managed until it does\n- How to cope with the symptoms of agoraphobia\n- How to understand and control a distorted view of stress-inducing situations\n- How to recognize and replace thoughts that cause panic\n- How to manage stress\n- How to imagine the situations that cause anxiety, working from the least to the most fearful (referred to as systematic desensitization and exposure therapy).\nTherapists who treat agoraphobia are often able to offer initial treatment without the need for the person with agoraphobia to visit the therapist\'s office. Therapy may be provided over the phone or via email, the therapist may conduct a home visit, or treatment sessions may be arranged in a place that the patient considers safe.\nResearch has also found that family support can aid the treatment of agoraphobia.13 Hostility towards agoraphobic family members increases the risk of them not completing their prescribed treatment plan. Equally, relatives that are too overbearing can also hinder therapy.\nIn addition to these treatment methods, there are several steps of self-care that can be utilized by a person with agoraphobia in order to help care for themselves and to cope with their condition:14\n- Sticking to a plan of treatment\n- Learning relaxation skills and how to achieve and maintain a sense of calm\n- Trying not to avoid feared situations, making them less frightening\n- Avoiding alcohol and illegal drugs\n- Staying healthy with physical activity, a healthy balanced diet and getting enough sleep.\nDue to the effect that agoraphobia can have on a person\'s ability to function day to day, it is recommended that anyone experiencing symptoms of agoraphobia contacts their health care provider. Treatment of the condition is easier the earlier that it begins.\nUnfortunately, changes in the DSM-V definition of agoraphobia as distinct from panic disorder may affect the likelihood of some people obtaining a diagnosis of agoraphobia, which would then limit their ability to access treatment.\nIn particular, researchers found that in an analysis of 151 anxious youth seeking treatment for their anxiety, 25% of those who met the criteria for agoraphobia in the DSM-IV no longer met the criteria for agoraphobia in the DSM-V, despite the severity and impairment being the same for those youth who did meet the new criteria for diagnosis.17\nRecent research suggests that a combination of medication, psychodynamic therapy and virtual reality may help people with agoraphobia to confront phobic stimuli.15\nRecent developments on agoraphobia treatment from MNT news\nNew research suggests that cognitive behavioral therapy is more effective at reducing health anxiety in medical patients, compared with standard care. This is according to a study published in The Lancet.']"	['<urn:uuid:8f1fe8b1-5bb8-40ab-84e2-851d4d2f32af>', '<urn:uuid:bf954ee4-75b9-4ebf-bba2-f108df2799e0>']	factoid	with-premise	verbose-and-natural	distant-from-document	comparison	novice	2025-05-12T21:52:51.670698	34	103	2509
98	How do modern submarines defend against helicopters and adapt radar?	Submarines can defend against helicopters using the IDAS weapon system, which allows them to launch missiles while staying submerged, with operators maintaining control through fiber optic connections. Against adaptive radars, electronic warfare systems are being developed using machine learning and intelligent algorithms to detect and counter these threats in real time, moving beyond traditional database-dependent systems.	"['Nowadays, anti-submarine warfare (ASW) is mainly carried out by airborne units – mostly helicopters equipped with dipping sonar and lightweight torpedoes. Submarines remain vulnerable to these threats. Usually, the only available option is to hide at greater depths or even leave the area of operations. Especially when restricted by coastal or shallow waters, differing salt content or temperatures, this typical avoidance of detection is frequently not an option. The mere presence of an airborne ASW unit already limits the submarine\'s operational options. That is why Diehl Defence and thyssenkrupp Marine Systems cooperated in the IDAS Consortium to develop the IDAS weapon system – Interactive Defense and Attack System for Submarines – to enable a submerged submarine to actively defend itself against airborne ASW threats.\nThe presence of an ASW helicopter in the area of operations is most likely to be detected by optical or electro-optical sensors or through receiving acoustic indicators, such as sonar intercepts of the dipping sonar. As soon as the crew detects such a threat, IDAS is prepared for operation. Range and bearing are determined by the submarine\'s sensor and sensor data fusion system. As soon as the operational situation is assessed as a critical defense situation, the effector is launched. For engagement purposes it is not necessary for the submarine to risk further detection by showing mast-mounted optical or electronic support measure (ESM) sensors or even by surfacing.\nFig. 1: Operational concept behind IDAS\nDuring the underwater flight phase, the missile maneuvers in the direction of the target, breaks the surface, accelerates to cruising speed, and flies towards the target area. The operator onboard the submarine stays in full control of the missile – “human in the loop” – while the submarine remains hidden. This is to give to the operator the chance to change the target or abort the mission. In the unlikely event of the connection being loss (optical fiber rupture) the missile will continue to operate in accordance with the operational settings, i.e. engagement of the last selected or the most probable target or mission-abort if this is in line with the rules of engagement.\nAdditionally, the “human in the loop” enables the system to fulfill a secondary role: engagement of surface targets where a heavyweight torpedo is not appropriate. This also provides the submarine’s commander with a means of escalation that is appropriate to the concrete operational situation. Furthermore, the integration of a GPS sensor would even incorporate land targets (e.g. to support SOF operations) into the IDAS target set.\nAfter the IDAS program was halted in 2010, the German industry partners decided to continue with industry funding. In order to initiate the industrial development program the IDAS Consortium needed a set of requirements, which were established in cooperation with submariners of the German and other navies and laid down in a document forming part of the consortium agreement. The top-level requirements included:\nThe IDAS system uses an ejection container to store four missiles and eject them separately out of a torpedo tube. A thrust piston system, which is fully integrated into the container, is used to eject the missile. The ejection container has the main dimensions and weight of a typical heavyweight torpedo. This allows for easy integration into new submarine building projects as well as refits of existing submarines.\n2: Embarking of an IDAS ejection container\nEvery IDAS missile is equipped with an optical fiber bobbin located in the missile\'s aft section and connecting the effector with the operator. Wings and fins are folded alongside the missile when stored and are unfolded at a safe distance from the submarine after ejection. With a 20 kg warhead and a cruising speed of more than 200 m/s the IDAS missile is capable of very effectively engaging an ASW helicopter. The whole system is pressure-proof at depths significantly deeper than periscope depth.\nFig. 3: IDAS missile components\nThe IDAS Consortium was founded in 2012 by Diehl Defence (formerly Diehl BGT Defence) and thyssenkrupp Marine Systems. Together with our partners Nammo and Roketsan we commenced the initial development phase in 2012 to develop a missile system that complies with those top-level requirements.\nFig. 4: IDAS Consortium and partners\nThe initial ejection tests were performed in May 2015 at the thyssenkrupp Marine Systems dockyard. After numerous unwinding tests with the optical fiber bobbin under varying conditions, including an ignited rocket motor, preparations were made for further tests with Norwegian submarines. In 2016 first system loading and unloading tests and then missile ejection tests from HNoMS Uredd were accomplished successfully. The initial development phase was completed by an engineering development firing test carried out in cooperation with the Royal Norwegian Navy. Thus the entire operational concept had been confirmed.\nThe IDAS Consortium and its international partners will achieve series production maturity for the IDAS system during the system qualification phase, in which the remaining detail development, verification and qualification work will be carried out. This phase is due to be completed and IDAS placed on the market in 2022.\nFor the first time in submarine operations, IDAS provides submarines with the capability to defend themselves effectively against airborne threats – without risking self-exposure. IDAS can be integrated into new submarine designs as well as existing systems. The principle of permanent missile control vi the ""human in the loop"" concept in combination with a submarine operational depth significantly deeper than periscope depth is an entirely new development in the execution of maritime operations. On the one hand, the operating unit does not need to risk exposure if it decides to defend itself; on the other hand, the risk of interference to the system is reduced to a minimum. IDAS will massively change the paradigms of submarine and antisubmarine operations.', 'Cognitive electronic warfare: Countering threats posed by adaptive radars\nThreats posed by adaptive radars to electronic warfare systems are a colossal challenge for the U.S. Navy, but a combo of advanced signal processing, intelligent algorithms, and machine learning techniques are being developed to help warfighters detect and counter them.\nElectronic warfare (EW) systems – whether on land or aboard U.S. military ships and aircraft – tap the electromagnetic spectrum to sense, protect, and communicate. But, when necessary, these same systems can be turned against adversaries to deny their ability to disrupt or use radio, infrared, or radar signals.\nToday’s EW systems tend to rely on databases of known threats with predefined countermeasures, which can limit their ability to quickly adapt and respond to new advanced threats. Soon, these systems may increasingly be tasked with isolating unknown hostile radar signals within dense electromagnetic environments and responding quickly with effective electronic countermeasures.\nThreats from adaptive radars aren’t necessarily new, according to Joe Ottaviano, director, Electronic Warfare, for Lockheed Martin (www.lockheedmartin.com), but are challenging for EW systems to detect.\nYears ago, back when the U.S. Navy’s Surface Electronic Warfare Improvement Program (SEWIP) Block 1, 2, and 3 were formed, “it was a key requirement to be able to detect and counter adaptive radars to provide warfighters with the best information about the radar and its mission,” Ottaviano explains. “We need to do this while performing our mission – before the enemy knows we’re there. Block 2 is designed to deal with adaptive radars.”\nBAE Systems (www.baesystems.com), another defense contractor working within the realm of adaptive radars, was recently awarded a $13.3 million contract by the U.S. Defense Advanced Research Projects Agency (DARPA) to extend its work on the Adaptive Radar Countermeasures (ARC) project, with the goal of enabling airborne EW systems to counter new, unknown, and adaptive radars in real time.\nCognitive EW technologies developed by BAE Systems for the ARC project rely on advanced signal processing, intelligent algorithms, and machine-learning techniques.\nAdaptive radar vs. “unknown” radar\nAdaptive radars, for starters, shouldn’t be confused with “unknown radars,” both of which cognitive EW addresses, as John Tranquilli, technical director for signals and communications processing at BAE Systems, points out.\nBoth radar types do pose challenges for EW systems and warfighters, albeit with differences. “For the class of unknown radars, current systems rely on a database of threat knowledge to compare observed characteristics to a list of known radars,” Tranquilli explains. “When a radar comes up as unknown or is misclassified as some other radar – because it’s either a really new radar or an existing one that’s behaving outside prior known bounds – today’s systems have very limited ability to characterize the threat or turn around an appropriate response.”\nRadars are currently evolving away from fixed analog systems – which aren’t easy to add new capabilities to – toward digitally programmable variants that can easily add or change to unknown behaviors with agile waveform characteristics. “This challenge will increase the prevalence of ‘unknown radars’ and lead to truly adaptive radars,” Tranquilli continues.\nAdaptive radars, which pose an even greater challenge, can sense their environment and design transmission characteristics on the fly to maximize radar performance. “This allows a radar to have new waveforms – or new pulse processing – for every transmission, whether it’s improving target resolution or mitigating interference effects,” Tranquilli says.\nThe objective of cognitive EW “is to enable EW systems to autonomously characterize these threats, automatically generate effective countermeasures against them, and monitor the effectiveness of our countermeasures so we can adapt our techniques if they’re either ineffective or if an adaptive radar attempts to adapt around our responses,” he adds.\nRecognizing and countering adaptive radars\nHow complex is recognizing and countering hostile adaptive radars? It falls under the category of “very complex.”\nFor SEWIP, a significant amount of software was developed as part of Block 2 to specifically address these challenges. “While development is a challenge, testing is becoming even more of a challenge,” Ottaviano notes. “Defining what’s effective and how effective the detection and response are requires a tremendous effort. We’re working with our Navy partners to continue to improve how we define success within a highly adaptive environment … and right now it’s our biggest challenge.”\nFrom BAE Systems’ perspective, “adaptive radars challenge our ability to isolate the pulses from threatening radars from other hostile, friendly, and neutral signals, our ability to deduce the threat posed by the radars, and our ability to select and configure an appropriate response to achieve our desired effect,” Tranquilli says. “A host of adaptive and machine-learning algorithms are necessary because these challenges are too complex to design a simple series of rules around.”\nRole of machine learning and artificial intelligence\nWhat role are machine learning and artificial intelligence (AI) playing in adaptive radars? There’s already a “fair amount of intelligence built into the new operational systems such as SEWIP,” Ottaviano says.\nIs there a role for a fully autonomous system? “It depends on the application,” Ottaviano notes. “I can easily see small packages being fully autonomous within a five-year window. For systems that deal with weapons, you can view it as another tool within the toolbox to allow warfighters to own the electromagnetic battlespace.”\nFor its part, AI can be “a loaded term,” Tranquilli cautions. “Typically, it refers to systems that attempt to have algorithms replace the functions a human would perform in an operation,” he adds.\nBut Tranquilli thinks there’s likely “a place for AI in the near future of cognitive EW, because as threat environments become more contested and complex, it can help reduce the mental load on pilots and planners.” This is “autonomy,” a field in which BAE Systems is an active player.\nMachine-learning techniques, on the other hand, “involve data-driven processing that allows a system to not be beholden to a static set of features, databases, or rules,” Tranquilli points out. “I can’t speak to the level of adaptation and autonomy allowed by today’s systems, but as threat systems continue to advance they’ll adapt more quickly and change their behavior faster … so algorithms and decision engines that can outpace our adversaries are key to future success. This is the central tenant of Bob Work’s Third Offset Strategy – pursuing next-gen technologies and concepts – and is funding advances within the cognitive EW arena.”\nFuture cognitive EW\nInterestingly, in terms of the hardware involved, Lockheed Martin is finding that a commercial off-the-shelf (COTS) open-architecture approach allows a rapid insertion of new capability to continually improve the ability and performance of the SEWIP Block 2 system.\n“These systems can and have been upgraded in an immediate fashion while underway to continue to provide the warfighter with new tools,” Ottaviano says. “We’ve seen a resurgence of this capability now beyond the surface EW community … even moving into decoy and off-board platforms. This provides a quick time to solution because the approaches we’re taking aren’t hardware specific and don’t require long development cycles.”\nTranquilli sees a place for advanced hardware in the future of cognitive EW “because systems will need faster response times, more prevalent RF and spatial coverage, and broader sets of available responses,” he explains. “The great thing about cognitive EW is that it can be a scalable capability so portions of the technology can be implemented into systems that exist today, while advances in the hardware, firmware, and software resources will unlock the ability to get more power out of the technology.”\nLockheed Martin is also currently working on a new U.S. Navy development contract to provide MH-60 helicopters with enhanced EW surveillance and countermeasure capabilities against anti-ship missile threats. (Figure 1.)\nAs part of it, they’ve developed a system – known as the Advanced Off-Board Electronic Warfare Active Mission Payload (AOEW AMP AN/ALQ-248) – that is essentially a self-contained EW pod hosted by an MH-60R or MH-60S to provide the Navy with advanced anti-ship missile detection and response capabilities. It’s designed to work independently or with the ship’s onboard electronic surveillance sensor, SEWIP Block 2 AN/SLQ-32(V)6, to detect incoming missiles and evaluate where they’re going. AOEW then uses RF countermeasures to deter any incoming missiles.\n“Our system will help create a coordinated attack against these threats to keep our warfighters safe by controlling the electromagnetic spectrum and disrupting adversaries,” Ottaviano says.']"	['<urn:uuid:17cd3f7d-9962-4184-a07b-f11bfaa6822b>', '<urn:uuid:345175d3-8e63-4c8c-ba23-6dff23f5adbf>']	factoid	direct	concise-and-natural	distant-from-document	multi-aspect	novice	2025-05-12T21:52:51.670698	10	56	2337
99	emotions sculpture greek art hellenistic compared neolithic	In Neolithic art (4000-2000 BC), sculptures began showing basic human emotions and character, as seen in pieces like the Thinker of Cernavoda which represented introspection. In contrast, Hellenistic Greek sculpture (after Alexander the Great's death) showed much more sophisticated emotional expression, focusing on excitement and strong feelings rather than peaceful depictions. Greek art developed detailed human anatomy and features, with dramatic postures, expressive gestures, and detailed textures. While Neolithic art marked the beginning of representing human emotion in sculpture, Hellenistic Greek art significantly advanced emotional expression through more sophisticated techniques.	['Bryn Mawr Classical Review 2014.08.26\nAngelos Chaniotis, Pierre Ducrey (ed.), Unveiling Emotions II. Emotions in Greece and Rome: Texts, Images, Material Culture. Heidelberger althistorische Beiträge und epigraphische Studien (HABES), Bd 55. Stuttgart: Franz Steiner Verlag, 2013. Pp. 387. ISBN 9783515106375. €62.00 (pb).\nReviewed by Donald Lateiner, Ohio Wesleyan University (firstname.lastname@example.org)\n[The Table of Contents is listed below.]\nHow did ancient humans experience, express, and later represent (or imagine) emotions—theirs and their interactants—and how do these jibe and differ from current experiences and interpretations? How does visual representation differ from literary in the range of affects? How do onomastics, texts, and images reflect perceptions of emotions and their relative standing (e.g., active anger and passive shame)? Chaniotis and Ducrey edited this second volume of case studies of archaeological, literary, and documentary evidence for ancient emotions. Further, they examine affect display in public, private, and divine relationships, but few continuities and discontinuities with current expression or academic analyses. The editors present fourteen essays (listed below). The editors chose not to group the essays as in volume I. In fact, there are six on Greek literature and history, three on the Roman world, and five on imperial culture in the East (including two on Greek art). Five indexes complement the collection.\nThe volume’s authors avoid facile comparisons between ancient and contemporary “feelings,” recognizing (as social constructionists) that the collection of this delicate data on ancient perceptions must avoid facile assumptions of co-extensive equivalents. On the other hand, as the more biologically inclined Cairns observes (87), were we entirely different personalities or non-mammals (such as lobsters, orchids, or granite), we could hardly understand horripilated Priam’s shudder and amazed Akhilleus’ (thambos), stunned surprise in his bivouac (Il. 24.359, 482-4). Same display of symptoms, however, never insures the same affect. Fear and grief snag three essays each. Sadness, pride, and trust focus three others, but the rest of the emotions share the stage either in three general treatments (two notably in historiography, one examining art) or packaged with others (love, hate, anger, pity, and envy). Certain recognized emotions, e.g., surprise and disgust, barely appear. Tamiolaki examines emotions in Xenophon’s Hellenika. She rightly discounts Aristotle’s “reductionist” analysis of emotions skewed towards problems of persuasion. “Tragic history,” as we term it, receives only passing mention here, as a later development (compare Chaniotis’ contribution). Tamiolaki analyzes Xenophon’s vocabulary, emotional episodes, and his “innovations” such as notations (or inventions) of facial expressions—not entirely absent from Herodotos. She touches on ‘focalised emotions,’ admitting that Thucydides too ascribes perceptions to one or another character about another party (e.g., 6.3.23). She usefully notes that Xenophon reveals his own emotions (Hell. 2.3.56, 7.2.16, 3.4.17-18). Some points seem self-evident, and sometimes she underrates Thucydides as well as Herodotos’ descriptions of emotions.\nChaniotis’ important essay starts from Cavafy’s “Alexandrian Kings” 1 and via Cavafy’s obsession with Plutarch and Polybian episodes, he demonstrates the importance of “emotional display” in Hellenistic historiography (Phylarchus, Posidonius, Diodorus). Theatrical costuming, theatrical gestures (such as affable diadem doffing), and strikingly, politics practiced in Mediterranean theaters suggest that contemporary dramatics inspired the historians (66). Through extensive quotations Chaniotis provides epigraphic (summaries in Hellenistic decrees) and historical evidence of such practices (e.g., mass suicide in Abydos 16.32-4), echoed in self-promoting Polybius, the authority who complains of the emotionalism of his competitors in historiography (2.56, 4.54, for example). Demetrios Poliorketes takes the Oscar for theatricality (see DeLacy’s 1952 AJP essay “Biography and Tragedy”), for whom Plutarch describes so many events as tragic spectacles. Doric Kytenian weeping speeches might provoke Xanthian compassion (pity and fear), even shared grieving, without producing “honorable” reactions: here soldiers and drachmas (SEG 38.1476). In brief, “the reality of theatrical behaviour and illusion” (79) produces their inclusion in the “tragic historians.”\nCairns contributes an exemplary study of the semantics of “shudders,” not indeed a “short history” with delightful alliteration, since one of his points is the continuity of both horrific phrike and its instinctive, autonomic symptom, horripilation. He distinguishes the external ‘leakage’ of self-consciously vulnerable creatures from the theory of mind behind the observer’s report and “our own subjective experience” of shuddering and trembling, and the attitudes (fear, awe) that produce it (86). He brilliantly distinguishes, thereafter, the shorthand synecdoche (a form of metonymy) of one lone symptom for many subjective currents behind it. The vivid somatic embodiment expresses the subjective reaction, the cognitive “appraisal and evaluation.” Awe and extreme fear combine to produce phrike felt in the presence of the supernatural, divinities, their miracles and omens, and frightening, oath-breaking, god-enforced consequences.\nPatera shifts us to the extensive philosophical discussion of fear (Homer, Plato, Aristotle, the Stoics), its connections with shame, reason, and the law. Central is its cultivable power to influence, manipulate, and control the masses, like children frighted by their nannies’ invented bogeys (mormolykeia). The cultic dimensions of Phobos (venerated in Sparta), anthropomorphic or formless, invite discussion: does it inculcate a habit to be encouraged (Thucydides, Polybius, Plutarch). Patera discusses both rational fear (often deos, associated with bravery) and irrational, empty fear (kenos phobos). Like all basic emotions (despite Aristotle and David Konstan, n. 58), fear is both instinctive and cognitive. Martial training habituated hoplites to suppress fear, while the community tries to instil it in potential law-breakers. Constructive uses of fear produce civic obedience (130) and religious awe.\nRubinstein extends the rhetorical theorists’ outlines to discussions of dicastic theatricality. She demonstrates that Attic litigants appealed to courtroom audiences’ emotions (“Be very angry!”). The logographers next complained about their opponents’ evocation of the same anger, pity, gratitude, and resentment. She examines the “triggers of particular emotions” (136). Apollodoros stirred up anger and disgust for Neaira, the abused non-citizen prostitute, at the center of his nasty case. Rubinstein, however, trawls for a limited group, vulnerable third parties who had been victims of opponents’ behaviors, such as orphans, widows, and the elderly, to fuel character assassinations that increase hatred and resentment toward parties at trial (140). She mentions in passing, beyond the forensic script, litigants’ visual effects, gaze, gestures and facial expressions, vocal tone and pace.\nKanavou explores the contributions of ‘negative’ emotions (moral and psychological) to Greek naming praxis: anger, enmity, envy, fear, hatred, and shame (cf. index, p. 379). Somewhat less obviously, she includes emotions of grief/sorrow. Her sources include searchable onomastic websites comprehending inscriptions, papyri, and literature. The meaning of many Greek names is obscure or ambiguously active/passsive. Does Euphemus mean “Well Spoken Of” or “Good Speaker”? Aristophanes’ Wasps provides comical emotion names with “Bdelukleon” and “Philokleon,” but enmity names are rare, not surprisingly. Shame names like Aischines or Aiskhylos are puzzling. Alpha-privatives, reversing the negative valence, precede many negative emotional names, understandably, such as Atrometos, Alypetos, and Atarbides.\nMorgan bites off more mentalité than her twenty pages can chew. Might pistis and fides have been experienced as an emotion (rather than cognition) by the Greeks, Roman, Roman Greeks, and early Christian church? The answer: yes, confidently, sometimes—but mostly no, perhaps too compartmentalized by the philosophers (194). Trust involves committing emotional energy for the superstitious, the theological, and even the medically challenged (206). The sometime philosopher Seneca de clem. mixes together illogically trust as emotion and virtue. The early Christians give pistis a special, but widely understandable (emotional) valence.\nBaraz pursues Roman pride (superbia), again not a typical topic in emotion studies, although a favorite of moralists who variously view it as virtue, vice, or sin. Self-regard arising from status, character, brains, or looks can produce an inward inflation that sometimes resists outward demonstration, hybris or verbal (presumption, ridicule, contempt) and nonverbal aggression (haughty postures and gestures, sneering tone, violent assaults like Caligula’s). The Romans censored pride when they thought the self-assessment mistaken (218), that is, often. Baraz starts with Phaedrus’ Jackdaw (1.3) and ends with Ovid’s Niobe. She concludes that the Roman conception and constituent features of superbia remained stable.\nMustakallio, building on six earlier studies, surveys Roman female lamentation and emotional control. The Romans strictly policed displays at funerals, starting from Table Ten of the XII, even forbidding any in the case of enemies of the Senate (e.g., both the Gracchi). Mustakallio shows that the comitatus muliebris or “crowd of women” on extreme occasions exerted political agency (e.g., the decemviri, Livy 3.47.4). Mourning by matrons became tightly controlled, therefore exceptional, in Imperial Rome (248).\nKing’s paper requires extended acquaintance with Galen’s heterogeneous oeuvre. The analysis of how this imperial doctor conceptualized “grief” starts with the bookish Galen’s mention of a literary man (grammatikos Philippides) who died of wasting grief when he lost his library in a house-fire (251). He combines his interest in philosophical and medical matters to explain the experience of grief, a member of the family of pain (lupe, algos, pathos). King concludes (263) that Galen reframes philosophical grief in “decidedly medical terms”—recording life-threatening psychic and somatic symptoms and suffering. One could apply this Galenic analysis to wasting “love-sicknesses” in the erotic novels.\nBobou’s article on ‘emotionality in Greek art’ is perhaps over-ambitious, but it supports Masséglia’s infra. She notes how often Greek vase-paintings and statues lack visible [therefore, any] emotions (273) The same situation is true of dedications, votive reliefs, and funerary monuments. Women’s emotions and unruly behaviors, so prominent in tragedy, are rare in visual art (306). Sometimes one thinks a scene is emotional because we know the story behind it (275). Agony is the main emotion shown in Greek art (Marsyas, Pergamon Altar, Laokoon Group), and even that is rare before the Hellenistic Age. Gods usually show no emotion (cf. Stoic apatheia), while types representing various “Others” show an emotion evoked (drunken old woman, poor fishermen, pained centaurs, barbarians-Dying Gauls). Thus, the emotions shown are usually negative: fear, sadness, anger, or joy (293), and they concede weakness or dependency.\nMasséglia dissociates social status from what many imagine to be “lower class” emotional display. Recognizing the high value on elite displays of sophrosyne, at least from late fourth century BCE memorials (315), she does not expect to find emotional expression in commemorative and honorific public milieux. Statesmen’s and women’s limbs usually appear modestly drawn in, such sculptures are more numerous, better preserved and therefore more frequent in publications. Nevertheless, “Others,” and Anonymi will be shown sad, drunk, pained, or ecstatic, in different contexts, or attached to non-specific personalities (terracotta statuettes) or a local fashion.\nThe contribution of Bourbou on recent discoveries of infant burials better belongs elsewhere. She realizes that the archaeology of grief—from osteological data—runs severe risks of over-interpretation. Notably large numbers may suggest a “special place for the [marginal] dead?”. Many graves were excavated in one area of present Cretan Chania (341)—along with alleged misfits or outcasts. Baby twins, deformed newborns, and infants with cleft palate may have died prematurely or may have been killed because of parental feelings of guilt (divine punishments?), shame, or for more economic and practical reasons (344).\nvan Nijf selected a vast, promising topic, how different classes of the Imperial Greek city expressed, suppressed, or devalued their emotions in assembly. Staged performances populated a world of “citizenship without sovereignty” (353). “Emotion Talk” included philotimia (2000 times, alone) and cognate terms. Kinship honorifics of sons, fathers, mothers, of the boule also flourished, implying sympathy and mutual obligation (361), both duties and rights: deference and infantilization amidst the loss of politics. Public emotions and their metaphors deserve this attention, since riots broke out in Hellenic assemblies from Iliadic Thersites on Troy’s beach to the Christian Paul’s Ephesian theater lynch mob.\nSome authors need no aid; less experienced hands could benefit from direction encouraging tight focus on slippery emotions. In sum, this volume intelligently confronts a helpful handful of emotions.\nTable of Contents\n1. Angelos Chaniotis and Pierre Ducrey: Approaching emotions in Greek and Roman history and culture: Introduction\n2. Melina Tamiolaki: Emotions and historical representation in Xenophon’s Hellenika\n3. Angelos Chaniotis: Empathy, emotional display, theatricality, and illusion in Hellenistic historiography\n4. Douglas Cairns: A short history of shudders\n5. Maria Patera: Reflections on the discourse of fear in Greek sources\n6. Lene Rubinstein: Evoking anger through pity: portraits of the vulnerable and defenceless in Attic oratory\n7. Nikoletta Kanavou: ‘Negative’ emotions and Greek names\n8. Teresa Morgan: Is pistis/fides experienced as an emotion in the Late Roman Republic, early Principate, and early Church?\n9. Yelena Baraz: Pride in the Roman world\n10. Katariina Mustakallio: Grief and mourning in Roman context\n11. Daniel King: Galen and grief: The construction of grief in Galen’s clinical work\n12. Olympia Bobou: Emotionality in Greek art\n13. Jane Masséglia: Feeling low: Social status and emotional display in Hellenistic art\n14. Chryssa Bourbou: The imprint of emotions surrounding the death of children in antiquity\n15. Onno M. van Nijf: Affective politics: the emotional regime in the Imperial Greek city Indices\n1. “The Alexandrians knew of course/ that this was all mere words, all theater. / But the day was warm and poetic...”', 'After thinking more about the brief and researching other methods of recording information and creating art outside of photography, I have decided to look into sculpture and it’s history.\nSculpture – BC\nThe earliest recorded sculptures are from prehistoric times. One of the earliest sculptures was named The Venus of Tan Tan and was found in Morocco. It is around two and half inches long and is dated at 500,000 – 300,000 BC. Another sculpture, dated around 230,000 BC is called The Venus of Berekhat and is around an inch and a half long. Both sculptures vaguely represent the human form.\nThe Venus of Berekhat (left) and The Venus of Tan Tan (right)\n(Image sources: http://fromaltamiratoandy.blogspot.co.uk/2012/11/upper-paleolithic-sculptures-venus.html & http://donsmaps.com/ukrainevenus.html)\nSmall, human like sculptures like this have been donned ‘Venuses’ and have been found mainly in areas of settlement rather than burial grounds, leading archeologists and historians to believe that they were used in rituals. These rituals could be fertility rituals – there are later dating Venuses that show larger women, which could symbolise pregnancy, however that is speculation and it us not clear what the definite use of these Venus sculptures was.\nThe Mesolithic Era of Sculpture 10,000 BC – 4,000BC (approximately)\nThe Mesolithic Era began at the end Palaeolithic Ice Age up to the beginning of agricultural farming – the Neolithic Era, which varied depending on the region of the world. The primary difference is that during the Palaeolithic Ice Age humans were hunter gatherers and lived primarily on instinct, whereas humans from the Neolithic Era lived in ‘settlements’, cultivated crops and bred animals for food and materials. The Palaeolithic Ice Age saw the creation of portable art work – such as ivory or stone carvings that were small. The Neolithic Era saw the start of agriculturally related art such as pottery and ceramics, made from clay.\nThe time between these Eras is called the Mesolithic Era and this acts as the bridge between the two time periods. This time brought more bas-relief art – sculpture where a material is carved away to create an image that is often three dimensional. This time also showed more art that represented the human form, as humans began to recognise the self within the world around them. This led to art such as the Thinker of Cernavoda, one of the Mesolithic Era’s most recognised pieces of art.\n(Image source: http://www.capodopere2019.ro/ganditorul-i-femeie-ezand-ro.html)\nThe Thinker of Cernavoda is a piece that shows introspection, and is one of the oldest pieces of artwork to show this. This was a turning point during the Mesolithic Era at which humans began to examine themselves as well as the world around them, and artwork represented personal reflection of the self instead of solely just figures of fertility or hunting. The figure was made from terracotta and around the same time as it was discovered similar figures, both with and withouts heads, were found in similar positions, all relating to introspection and self reflection.\nAs humans settled there became a demand for decorative art, as well as decoration for functional items. Pots, vases and various utensils were decorated and sealed with fire, having been carved, painted or stamped.\nThe Neolithic Era of Sculpture 4,000 BC – 2,000 BC (approximately)\nIt was during this era that recognisable sculpture such as the Egyptian pyramids were first thought out. Originally North and South Egypt had adopted different burial rituals as North Egypt was wet and therefore buried their dead below their family houses, whereas South Egypt was dry and tended to bury their dead away from their settlements. As the two halves of the country came together their rituals became more organised, and they decided to bury their dead below ground with a flat roofed sculpture above. This eventually, after much change and failure during building, became the base for the famous Egyptian Pyramids.\nThis era also witnessed the first Bronze statues; the Indus Valley Civilisation created a well known sculpture called the Dancing Girl of Mohenjo-Daro which is dated to 2500 BC. This sculpture depicts a long-limbed female, with well defined features.\nThe Indus Valley Civilisation were also well known from their terracotta and stone statues that depicted human forms as well as animal forms.\nThe Eastern Mediterranean Era of Sculpture 2,000 BC – 1100 BC (approximately)\nIn Crete during this time the Minoan Culture flourished and developed sculpture further and began working with metal. However due to an earthquake in 1500 BC the Minoan Culture collapsed and was proceeded by the Mycenaeans.\nThe time that followed this era was stagnant in terms of new development, but instead was a time of slow, continuous experimentation. The creation of new types of sculpture became prominent during the era of Greek Sculpture, from around 650 BC – 27 BC.\nThe Era of Greek Sculpture\nThis era is split into many smaller eras that include Early Classical, High Classical and Late Classical eras and later on Hellenistic Greek Sculpture. The Hellenistic era ran from the death of Alexander the Great to the death of Cleopatra in 31 BC.\nDuring this time Greek culture globalised and brought with it the change from classic realism within art to expressionism and heroicism. Alongside this, knowledge of human anatomy and features developed, leading sculptors to create more finely detailed human forms.\nDonned the ‘greatest example’ of Hellenistic Greek Sculpture, the The Altar of Zeus at Pergamon was created between 166 and 156 BC. It is a 370 foot long altar the depicts the fight between the Gods and the giants in Greek mythology.\nThe Hellenistic Era of Sculpture focussed less of peaceful depictions of the human (or God or Giant) form and more on excitement and strong feeling within the characters. By using a bas-relief technique to create this piece the artist – King Emends II – was able to relieve the material so well that it looked almost as if the characters stood free from the marble. This monumental piece allowed for the development of Greek sculpture within art and architecture, making way for Greek Sculpture to be among the greatest in the world, copied and admired for centuries afterward.\nKing Emends II was among other artists as part of the Pergamene School of Hellenistic Sculpture. Other artists included in the Pergamene School were Antigonus, Epigonus, Phyromachus, Sogonus and Stratonicus. Pergamon was a Greek colony that resided in what is now known as Turkey, near the Caicus River.\nGreek Sculpture was characterised by the new type of expressionism it showed; using dramatic posture and poses, expressive gestures and detailed textures. This was a big corner to turn as it paved the way for more expressive artwork throughout the following centuries, up to the present day. However, Greek sculpture was not properly ‘discovered’ until Early Renaissance artists in Italy came across it around 1700 years later, at which point it became the main inspiration for artworks created across the next four centuries.\nThe Celtic Era of Sculpture 400 BC – 100 BC (approximately)\nThe Celts’ talents lay within goldsmithing and blacksmithing. They were able to create highly detailed metalwork such as one of their most well known pieces – the Broighter Collar, created around 100-50 BC. Named after the location it was later discovered, the collar was created with a matching crown named the Petrie Crown, which was named after it’s former owner – an antiques collector.\n(Image source for both images: https://arthistoryleavingcert.com/pre-christian-ireland-2/iron-age-la-tene-in-ireland/)\nThese two pieces, particularly the Petrie Crown, shows the characteristics of Celtic Sculpture in terms of the detailed circular metal discs connected by rivets, and the bronze bands and adornments. The design includes symbols and characters such as the sun symbol, alongside spirals and animal depictions such as birds. This type of symbolism used in combination with the metal work is symbolic to Celtic Sculpture, however it was not until later on when the continent became more Romanized that these pieces became more popular and widespread, leading them to be traded and imitated around the world.\nIt is interesting to look at how the eras BC influenced each other, and consequently influenced modern day art and all that came between. I will be exploring work in the AD eras soon.\nWhat inspiration am I taking from this information?\nAfter having spend time researching the different eras of BC sculpture I am interested primarily in Neolithic sculpture and Greek sculpture.\nThe Neolithic era saw the beginning of human character and emotion and the Greek era showed the development of gestures, textures and emotion within the human character to create a narrative.\nThese elements are what interest me most as I feel that they relate to the boundary between art and emotion; how by creating a piece of art that represents human emotion and character the viewers of this work can resonate with the feelings depicted and the story being told.']	['<urn:uuid:c013c6ba-be67-4a41-baa9-c82df20729c7>', '<urn:uuid:ce11abe1-b013-4374-bb9d-2081fb8a69b2>']	open-ended	direct	short-search-query	similar-to-document	comparison	novice	2025-05-12T21:52:51.670698	7	90	3610
100	safe timing application process agricultural chemicals before harvesting	Pre-harvest herbicide applications must strictly follow product label recommendations and withholding periods to avoid unacceptable chemical residues in delivered cereal, pulse and oilseed grains. Individual labels should be checked before application as withholding periods and maximum application rates vary by crop type and product. Not following label directions is illegal and can result in MRL violations, including using higher rates or not following timing of application directions and withholding periods.	"['WHEN carefully managed, pre-harvest herbicide applications can be a useful tool in weed control and crop desiccation.\nHowever, it\'s critical to always follow the product label and withholding periods and seek advice from grain buyers concerning any specific market requirements prior to use.\nPlus, pre-harvest herbicide use should be considered as a part of a wider integrated weed management plan.\nGrains Research and Development Corporation (GRDC) crop protection manager west Georgia Megirian said growers planning pre-harvest herbicide applications were being urged to arm themselves with the required knowledge to avoid grain rejection from domestic or international markets.\n""The risks associated with pre-harvest herbicide use for late season weed management or crop desiccation need to be weighed against potential grain market requirements,"" Ms Megirian said.\n""By adhering to the product label directions for use, growers will ensure that any detectable residues will meet Australian Maximum Residue Limits (MRLs), however, importing country MRLs may differ to the Australian MRLs.\n""Growers are encouraged to seek advice from their grain buyers regarding any additional market requirements that might impact on their management choices prior to herbicide application.""\nTo guide growers through their decisions, the GRDC has published the Pre-Harvest Herbicide Use Fact Sheet.\nApplication of herbicides close to harvest comes with the risk of detectable residues being present on harvested grain.\nHerbicides can be used late in the season to manage weeds, prevent weed seed set or to desiccate crops to accelerate or even-up ripening.\n""It is the responsibility of growers to adhere to herbicide label recommendations and withholding periods to avoid the presence of unacceptable chemical residues in delivered cereal, pulse and oilseed grains,"" Ms Megirian said.\n""The Pre-Harvest Herbicide Use Fact Sheet contains basic information regarding chemical registration by crop type, but it is recommended to always check chemical labels before use.""\n""MRLs exist for each crop and market and it is important to take into account MRLs for Australia and the country importing the grain, as these restrictions can often vary.""\nIn-crop spray topping with paraquat or glyphosate is often used as a control for a range of annual grasses in pulse crops and pastures.\nChemical use should be considered as a part of a wider integrated weed management (IWM) plan, allowing for alternative weed control techniques.\nMs Megirian said pre-harvest herbicide use in cereals varied by crop type and product label.\n""Individual labels should be checked before application as withholding periods and maximum application rates do vary,"" she said.\n""Product acceptance can vary between grain buyers\' policies, and growers should consult with their buyer before any application, this is particularly the case for malt barley crops.\n""Harvest weed seed control methods such as the use of chaff carts and seed impact mills can be employed as long-term options for control of weeds such as ryegrass.""\nBaling a particularly infested crop can also assist in cost recovery and weed seed control, while windrowing is also an alternative for growers planning on using desiccation techniques to prepare a canola crop for harvest.\nIn Australia, the Australian Pesticides and Veterinary Medicines Authority (APVMA) determines MRLs as part of the registration process.\n""The MRL is set based on the application of the pesticide under Australian use patterns (label directions),"" Ms Megirian said.\n""Not following label directions is illegal and can result in MRL violations, this includes using higher rates or not following timing of application directions and withholding periods.\n""It is important to remember, the importing country MRLs may differ to the Australian MRL and that breaches of MRLs can result in grain being rejected by both domestic and export markets.""\nGrain handlers and marketers regularly conduct testing on grain receivals for residues, while the National Residue Survey also conducts ongoing residue testing of grain.']"	['<urn:uuid:c7b6fbf5-605a-447e-9509-91179d2a941a>']	open-ended	direct	long-search-query	distant-from-document	single-doc	expert	2025-05-12T21:52:51.670698	8	70	621
